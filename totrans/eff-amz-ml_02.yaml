- en: Machine Learning Definitions and Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter offers a high-level definition and explanation of the machine
    learning concepts needed to use the Amazon Machine Learning (Amazon ML) service
    and fully understand how it works. The chapter has three specific goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing the main techniques to improve the quality of predictions used when
    dealing with raw data. You will learn how to deal with the most common types of
    data problems. Some of these techniques are available in Amazon ML, while others
    aren't.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presenting the predictive analytics workflow and introducing the concept of
    cross validation or how to split your data to train and test your models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Showing how to detect poor performance of your model and presenting strategies
    to improve these performances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reader will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to spot common problems and anomalies within a given dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to extract the most information out of a dataset in order to build robust
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect and improve upon poor predictive performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What's an algorithm? What's a model?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into data munging, let's take a moment to explain the difference
    between an algorithm and a model, two terms we've been using up until now without
    a formal definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the simple linear regression example we saw in [Chapter 1](767f8b14-c3f2-4e45-bfcd-bb45ae2b9e65.xhtml),
    *Introduction to Machine Learning and Predictive Analytics —* the linear regression
    equation with one predictor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *x* is the variable, *ŷ* the prediction, not the real value, and *(a,b)*
    the parameters of the linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: The conceptual or theoretical model is the representation of the data that is
    the most adapted to the actual dataset. It is chosen at the beginning by the data
    scientist. In this case, the conceptual model is the linear regression model,
    where the prediction is a linear combination of a variable. Other conceptual models
    include decision trees, naive bayes, neural networks, and so on. All these models
    have parameters that need to be tuned to the actual data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is the computational process that will calculate the optimal parameters
    of the conceptual model. In our simple linear regression case, the algorithm will
    calculate the optimal parameters *a* and *b*. Here optimal means that it gives
    the best predictions given the available dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the predictive model corresponds to the conceptual model associated
    with the optimal parameters found for the available dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In reality, no one explicitly distinguishes between the conceptual model and
    the predictive model. Both are called the model.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the algorithm is the method of learning, and the model is what results
    form the learning phase. The model is the conceptual model (trees, svm, linear)
    trained by the algorithm on your training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with messy data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the dataset grows, so do inconsistencies and errors. Whether as a result
    of human error, system failure, or data structure evolutions, real-world data
    is rife with invalid, absurd, or missing values. Even when the dataset is spotless,
    the nature of some variables need to be adapted to the model. We look at the most
    common data anomalies and characteristics that need to be corrected in the context of Amazon
    ML linear models.
  prefs: []
  type: TYPE_NORMAL
- en: Classic datasets versus real-world datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data scientists and machine-learning practitioners often use classic datasets
    to demonstrate the behavior of certain models. The **Iris** dataset, composed
    of 150 samples of three types of iris flowers, is one of the most commonly used
    to demonstrate or to teach predictive analytics. It has been around since 1936!
  prefs: []
  type: TYPE_NORMAL
- en: The **Boston housing** dataset and the **Titanic** dataset are other very popular
    datasets for predictive analytics. For text classification, the **Reuters** or
    the **20 newsgroups** text datasets are very common, while image recognition datasets
    are used to benchmark deep learning models. These classic datasets are used to
    establish baselines when evaluating the performances of algorithms and models.
    Their characteristics are well known, and data scientists know what performances
    to expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'These classic datasets can be downloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iris**: [http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boston housing**: [https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Titanic dataset**: [https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)
    or [http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reuters**: [https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**20 newsgroups**: [http://scikit-learn.org/stable/datasets/twenty_newsgroups.html](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image recognition and deep learning**: [http://deeplearning.net/datasets/](http://deeplearning.net/datasets/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, classic datasets can be weak equivalents of real datasets, which have
    been extracted and aggregated from a diverse set of sources: databases, APIs,
    free form documents, social networks, spreadsheets, and so on. In a real-life
    situation, the data scientist must often deal with messy data that has missing
    values, absurd outliers, human errors, weird formatting, strange inputs, and skewed
    distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: The first task in a predictive analytics project is to clean up the data. In
    the following section, we will look at the main issues with raw data and what
    strategies can be applied. Since we will ultimately be using a linear model for
    our predictions, we will process the data with that in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions for multiclass linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a linear model to offer reliable predictions, predictors must satisfy a
    certain number of conditions. These conditions are known as the *Assumptions of
    Multiple Linear Regression* ([http://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/](http://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear relationship**: The predictors should have some level of linear relationship
    with the outcome'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multivariate normality**: The predictors should follow a Gaussian distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No or little multicollinearity**: The predictors should not be correlated
    to one another'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Homoscedasticity**: The variance of each predictor should remain more or
    less constant across the whole range of values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, these assumptions are seldom verified. But there are ways to transform
    the data to approach these optimal conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data aggregation, extraction, and consolidation is often not perfect and sometimes
    results in missing values. There are several common strategies to deal with missing
    values in datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing all the rows with missing values from the dataset. This is simple to
    apply, but you may end up throwing away a big chunk of information that would
    have been valuable to your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using models that are, by nature, not impacted by missing values such as decision
    tree-based models: random forests, boosted trees. Unfortunately, the linear regression
    model, and by extension the SGD algorithm, does not work with missing values ([http://facweb.cs.depaul.edu/sjost/csc423/documents/missing_values.pdf](http://facweb.cs.depaul.edu/sjost/csc423/documents/missing_values.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputing the missing data with replacement values; for example, replacing missing
    values with the median, the average, or the harmonic mean of all the existing
    values, or using clustering or linear regression to predict the missing values.
    It may be interesting to add the information that these values were missing in
    the first place to the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, the right strategy will depend on the type of missing data and of
    course, the context. While replacing missing blood pressure numbers in a patient
    medical record by some average may not be acceptable in a healthcare context, replacing
    missing age values by the average age in the Titanic dataset is definitely adapted
    to a data science competition.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Amazon ML''s documentation is not 100% clear on the strategy used
    to deal with missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: If the target attribute is present in the record, but a value for another numeric
    attribute is missing, then Amazon ML overlooks the missing value. In this case,
    Amazon ML creates a substitute attribute and sets it to 1 to indicate that this
    attribute is missing.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of missing values, a new column is created with a Boolean flag to
    indicate that the value was missing in the first place. But it is not clear whether
    the whole row or sample is dismissed or overlooked or if just the cell is removed.
    There is no mention of any type of imputation.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning algorithms incrementally update the model parameters by minimizing
    the error between the real value and the one predicted with the last iteration''s
    parameters. To measure this prediction error we introduce the concept of loss
    functions. A loss function is a measure of the prediction error. For a certain
    algorithm, using different loss functions will create variants of the algorithm. Most
    common loss functions use the **L2** or the **L1** norm to measure the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '**●  ****L2 norm:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_2_02_eqn.png)'
  prefs: []
  type: TYPE_IMG
- en: '**●  ****L1 norm:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_2_03_eqn.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *y[i]* and *ŷ* are the real and predicted values of the samples.
  prefs: []
  type: TYPE_NORMAL
- en: The measure of the prediction error can end up being skewed when the different
    predictors differ by an order of magnitude. The large predictors obfuscate the
    importance of the smaller valued ones, thus making it difficult to infer the relative
    importance of each predictor in the model. This impacts how the respective weights
    of the linear model converge to their optimal value and as a consequence the performance
    of the algorithm. Predictors with the highest magnitude will end up dominating
    the model even if the predictor has little predictive power with regard to the
    real outcome value. Normalizing the data is a way to mitigate that problem by
    forcing the predictors to all be on the same scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common types of normalization; data can be normalized or standardized:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **min-max normalization**, or **normalization**, which sets all values
    between *[0,1]*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/image_02_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **z-score normalization**, or **standardization**, which normalizes with
    respect to the standard deviation. All predictors will have a mean of 0 and a
    standard deviation of 1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/image_02_007.png)'
  prefs: []
  type: TYPE_IMG
- en: The tree-based methods (decision trees, random forests, boosted trees) are the
    only machine learning models whose performance is not improved by normalization
    or standardization. All other distance/variance-based predictive algorithms may
    benefit from normalization. It has been shown that standardization is particularly
    useful for SGD, as it ensures that all the weights will be adapted at the same
    speed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Efficient BackProp Yann A. LeCun et al. in Neural Networks*: *Tricks of the
    Trade pp. 9-48, Springer Verlag*'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ML offers z-score standardization as part of the available data transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Imbalanced datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dealing with imbalanced datasets is a very common classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a Binary classification problem. Your goal is to predict a positive
    versus a negative class. The ratio between the two classes is highly skewed in
    favor of the positive class. This situation is frequently encountered in the following
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: In a medical context where the positive class corresponds to the presence of
    cancerous cells in people out of a large random population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a marketing context where the positive class corresponds to prospects buying
    an insurance while the majority of people are not buying it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both these cases, we want to detect the samples in the minority class, but
    they are overwhelmingly outnumbered by the samples in the majority (negative)
    class. Most predictive models will be highly biased toward the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: In the presence of highly imbalanced classes, a very simplistic model that always
    predicts the majority class and never the minority one will have excellent accuracy but
    would never detect the important and valuable class. Consider for instance a dataset
    composed of 1,000 samples, with 50 positive samples that we want to detect or
    predict and 950 negative ones of little interest. That simplistic model has an
    accuracy rate of 95% which is obviously a decent accuracy even though that model
    is totally useless. This problem is known as the **Accuracy paradox** ([https://en.wikipedia.org/wiki/Accuracy_paradox](https://en.wikipedia.org/wiki/Accuracy_paradox)).
  prefs: []
  type: TYPE_NORMAL
- en: A straightforward solution would be to gather more data, with a focus on collecting
    samples of the minority class and in order to balance out the two classes. But
    that's not always a possibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other strategies to deal with imbalanced datasets. We will briefly
    look at some of the most common ones. One approach is to resample the data by
    under sampling or oversampling the available data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Undersampling** consists in discarding most samples in the majority class
    in order to tilt back the minority/majority class ratio toward *50/50*. The obvious
    problem with that strategy is that a lot of data is discarded and along with that,
    meaningful signal for the model. This technique can be useful in the presence
    of large enough datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Oversampling** consists in duplicating samples that belong to the minority
    class. Contrary to under sampling, there is no loss of data with that strategy.
    However, oversampling adds extra weight to certain patterns from the minority
    class, which may not bring useful information to the model. Oversampling adds
    noise to the model. Oversampling is useful when the dataset is small and you can''t
    afford to leave some data out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under sampling and oversampling are two simple and easy-to-implement methods
    that are useful in establishing a baseline. Another widely-used method consists
    in creating synthetic samples from the existing data. A popular sample creation
    technique is the **SMOTE** method, which stands for **Synthetic Minority Over-Sampling
    Technique**. SMOTE works by selecting similar samples (with respect to some distance
    measure) from the minority class and adding perturbations on the selected attributes.
    SMOTE then creates new minority samples within clusters of existing minority samples.
    SMOTE is less of a solution in the presence of high-dimensional datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The **imbalanced library** in Python ([http://](http://github.com/scikit-learn-contrib/imbalanced-learn)[github.com/scikit-learn-contrib/imbalanced-learn](http://github.com/scikit-learn-contrib/imbalanced-learn))
    or the **unbalanced package** in R ([https://cran.r-project.org/web/packages/unbalanced/index.html](https://cran.r-project.org/web/packages/unbalanced/index.html))
    both offer a large set of advanced techniques on top of the ones mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the choice of the metric used to assess the performances of the model
    is particularly important in the context of an imbalanced dataset. The accuracy
    rate, which is defined as the ratio of correctly predicted samples to the total
    number of samples is the most straightforward metric in classification problems.
    But as we have seen, this accuracy rate is not a good indicator of the model's
    predictive power in the presence of a highly skewed class distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In such a context, two metrics are recommended:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cohen''s kappa:** A robust measure of the agreement between real and predicted
    classes ([https://en.wikipedia.org/wiki/Cohen%27s_kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The F1 score**: The harmonic mean between Precision and Recall ([https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The F1 score is the metric used by Amazon ML to assess the quality of a classification
    model. We give the definition of the F1-score under the *Evaluating the performance
    of your model* section at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing multicollinearity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the definition of multicollinearity according to Wikipedia
    ([https://en.wikipedia.org/wiki/Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multicollinearity** is a phenomenon in which two or more predictor variables
    in a multiple regression model are highly correlated, meaning that one can be
    linearly predicted from the others with a substantial degree of accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, let''s say you have a model with three predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_2_04_eqn.png)'
  prefs: []
  type: TYPE_IMG
- en: And one of the predictors is a linear combination (perfect multicollinearity)
    or is approximated by a linear combination (near multicollinearity) of two other
    predictors.
  prefs: []
  type: TYPE_NORMAL
- en: For instance: ![](img/B05028_2_05_eqn.png)
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![](img/B05028_2_06_eqn.png) is some noise variable.
  prefs: []
  type: TYPE_NORMAL
- en: In that case, changes in ![](img/B05028_2_07_eqn.png) and ![](img/B05028_2_08_eqn.png)
    will drive changes in ![](img/B05028_2_09_eqn.png), and as a consequence, ![](img/B05028_2_10_eqn.png)
    will be tied to ![](img/B05028_2_11_eqn.png) and ![](img/B05028_2_12_eqn.png).
    The information already contained in ![](img/B05028_2_07_eqn.png) and ![](img/B05028_2_08_eqn.png)
    will be shared with the third predictor ![](img/B05028_2_09_eqn.png), which will
    cause high uncertainty and instability in the model. Small changes in the predictors'
    values will bring large variations in the coefficients. The regression may no
    longer be reliable. In more technical terms, the standard errors of the coefficient
    would increase, which would lower the significance of otherwise important predictors.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to detect multicollinearity. Calculating the correlation
    matrix of the predictors is a first step, but that would only detect collinearity
    between pairs of predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 'A widely used detection method for multicollinearity is to calculate the **variance
    inflation factor** or **VIF** for each of the predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_2_13_eqn.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B05028_2_14_eqn.png) is the coefficient of determination of the
    regression equation in step one, with ![](img/B05028_2_15_eqn.png) on the left-hand
    side and all other predictor variables (all the other ![](img/B05028_2_16_eqn.png)
    variables) on the right-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: A large value for one of the VIFs is an indication that the variance (the square
    of the standard error) of a particular coefficient is larger than it would be
    if that predictor was completely uncorrelated with all the other predictors. For
    instance, a VIF of 1.8 indicates that the variance of a predictor is 80% larger
    than what it would be in the uncorrelated case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the attributes with high collinearity have been identified, the following
    options will reduce collinearity:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing the high collinearity predictors from the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **Partial Least Squares Regression (PLS)** or **Principal Components Analysis
    (PCA)**, regression methods ([https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis));
    PLS and PCA will reduce the number of predictors to a smaller set of uncorrelated
    variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, detection and removal of multicollinear variables is not available
    from the Amazon ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a variable, outliers are values that are very distant from other values
    of that variable. Outliers are quite common, and often caused by human or measurement
    errors. Outliers can strongly derail a model.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate, let's look at two simple datasets and see how their mean is
    influenced by the presence of an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the two datasets with few samples each: *A = [1,2,3,4]* and *B = [1,2,3,4,
    100]*. The 5^(th) value in the B dataset,  100, is obviously an outlier: *mean(A)
    = 2.5*, while *mean(B) = 22*. An outlier can have a large impact on a metric.
    Since most machine learning algorithms are based on distance or variance measurements,
    outliers can have a high impact on the performance of a model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple linear regression is sensitive to outlier effects, as shown in the
    following graph where adding a single outlier point derails the solid regression
    line into the dashed one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_024.png)'
  prefs: []
  type: TYPE_IMG
- en: Removing the samples associated with the outliers is the simplest solution.
  prefs: []
  type: TYPE_NORMAL
- en: Another solution can be to apply **quantile binning** to the predictor by splitting
    the values into *N* ordered intervals or bins, each approximately containing an
    equal number of samples. This will transform a numeric (continuous) predictor
    into a categorical one. For example, [1,2,3,4,5,6,7,8,9,10,11,100] split into
    three equally sized bins becomes [*1,1,1,1,2,2,2,2,3,3,3,3*]; the outlier value
    100 has been included in the third bin and hidden.
  prefs: []
  type: TYPE_NORMAL
- en: The downside of quantile binning is that some granularity of information is
    lost in the process, which may degrade the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantile binning** is available as a data transformation process in Amazon
    ML and is also used to quantify non-linearities in the original dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, **Quantile Binning** (**QB**) is applied by default by Amazon ML to
    all continuous variables that do not exhibit a straightforward linear relation
    to the outcome. In all our trials, and contrary to our prior assumptions, we have
    found that QB is a very efficient data transformation in the Amazon ML context.
  prefs: []
  type: TYPE_NORMAL
- en: Accepting non-linear patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A linear regression model implies that the outcome can be estimated by a linear
    combination of the predictors. This, of course, is not always the case, as features
    often exhibit nonlinear patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following graph, where *Y* axis depends on *X* axis but the relationship
    displays an obvious quadratic pattern. Fitting a line (*y = aX + b*) as a prediction
    model of *Y* as a function of *X* does not work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_025.png)'
  prefs: []
  type: TYPE_IMG
- en: Some models and algorithms are able to naturally handle non-linearities, for
    example, tree-based models or support vector machines with non-linear kernels.
    Linear regression and SGD are not.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformations**: One way to deal with these nonlinear patterns in the context
    of linear regression is to transform the predictors. In the preceding simple example,
    adding the square of the predictor X to the model would give a much better result.
    The model would now be of the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_2_28_eqn.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And as shown in the following diagram, the new quadratic model fits the data
    much better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_027.png)'
  prefs: []
  type: TYPE_IMG
- en: We are not restricted to the quadratic case, and a power function of higher
    order can be used to transform existing attributes and create new predictors.
    Other useful transformations could include taking the logarithm, exponential,
    sine and cosine, and so on. The **Boxcox** **transformation** ([http://onlinestatbook.com/2/transformations/box-cox.html](http://onlinestatbook.com/2/transformations/box-cox.html))
    is worth citing at this point. It's an efficient data transformation that reduces
    skewness and kurtosis of a variable distribution. It reshapes the variable distribution
    into one closer to a Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Splines** are an excellent and more powerful alternative to polynomial interpolation.
    Splines are piece-wise polynomials that join smoothly. At their simplest level,
    splines consists of lines that are connected together at different points. Splines
    are not available in Amazon ML.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantile binning** is the Amazon ML solution to non-linearities. By splitting
    the data into N bins, you remove any non-linearities in the bin''s intervals.
    Although binning has several drawbacks ([http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous](http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous)),
    the main one being that information is discarded in the process, it has been shown
    to generate excellent prediction performance in the Amazon ML platform.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding features?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, adding new features that are correlated in some ways to the outcome
    brings information and improves a model. However, adding too many features with
    little predictive power may end up bringing confusion to that same model and in
    the end degrading its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection by removal of the least interesting features is worth trying
    when the sample size is small compared to the number of features; it leads to
    too few observations or too many features. There are different strategies ([http://machinelearningmastery.com/an-introduction-to-feature-selection/](http://machinelearningmastery.com/an-introduction-to-feature-selection/))
    to identify and remove weak features. Selecting features based on their correlation
    with the outcome and discarding features with little or no correlation with the
    outcome will usually improve your model.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing recapitulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table recapitulates the different issues that one can find in
    raw data and whether Amazon ML offers ways to deal with them:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Linear model sensitivity** | **Available on Amazon ML** |'
  prefs: []
  type: TYPE_TB
- en: '| **Missing values** | Yes | Dealt with automatically |'
  prefs: []
  type: TYPE_TB
- en: '| **Standardization** | Yes | z-score standardization |'
  prefs: []
  type: TYPE_TB
- en: '| **Outliers** | Yes | Quantile binning |'
  prefs: []
  type: TYPE_TB
- en: '| **Multicollinearity** | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| **Imbalanced datasets** | Yes | Uses the right metric F1 Score No sampling strategy
    (may exist in background) |'
  prefs: []
  type: TYPE_TB
- en: '| **Non linearities** | Yes | Quantile binning |'
  prefs: []
  type: TYPE_TB
- en: The predictive analytics workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have been talking about training the model. What does that mean in practice?
  prefs: []
  type: TYPE_NORMAL
- en: 'In supervised learning, the dataset is usually split into three non-equal parts:
    training, validation, and test:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_028.png)'
  prefs: []
  type: TYPE_IMG
- en: The **training** set on which you train your model. It has to be big enough
    to give the model as much information on the data as possible. This subset of
    the data is used by the algorithm to estimate the best parameters of the model.
    In our case, the SGD algorithm will use that training subset to find the optimal
    weights of the linear regression model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **validation** set is used to assess the performance of a trained model.
    By measuring the performance of the trained model on a subset that has not been
    used in its training, we have an objective assessment of its performance. That
    way we can train different models with different meta parameters and see which
    one is performing the best on the validation set. This is also called model selection. Note
    that this creates a feedback loop, since the validation dataset now has an influence
    on your model selection. Another model may have performed worse on that particular
    validation subset but overall better on new data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **test** set corresponds to data that is set aside until you have fully
    optimized your features and model. The test subset is also called the **held-out**
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In real life, your model will face previously unforeseen data, since the ultimate
    *raison d'etre* for a model is to predict unseen data. Therefore, it is important
    to assess the performance of the model on data it has never encountered before.
    The held-out dataset is a proxy for yet unseen data. It is paramount to leave
    this dataset aside until the end. It should never be used to optimize the model
    or the data attributes.
  prefs: []
  type: TYPE_NORMAL
- en: These three subsets should be large enough to represent the real data accurately.
    More precisely, the distribution of all the variables should be equivalent in
    the three subsets. If the original dataset is ordered in some way, it is important
    to make sure that the data is shuffled prior to the train, validation, test split.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, the model you choose based on its performance on the
    validation set may have had a positive bias toward that particular dataset. In
    order to minimize such a dependency, it is common to train and evaluate several
    models with the same parameter settings and to average the performance of the
    model over several training validation dataset pairs. This reduces the model selection
    dependence with regard to the specific distribution of variables in the validation
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This third-split method is basic and as we've seen, the model could end up being
    dependent on some specificities of the validation subset. Cross-validation is
    a standard method to reduce that dependency and improve our model selection. Cross
    validation consists in carrying out several training/validation split and averaging
    the model performance on the different validation subsets. The most frequent cross
    validation technique is k-fold cross validation which consists in splitting the
    dataset in K-chunks, recursively using each part as validation and the k-1 other
    parts as training. Other cross validation techniques include Monte-Carlo cross
    validation where the different training and validation sets are randomly sampled
    from the initial dataset. We will implement Monte Carlo cross validation in a
    later chapter. Cross validation is not a feature included in the Amazon ML service
    and needs to be implemented programatically. In Amazon ML, the training and evaluation
    of a model is done on one training-validation split only.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluation in Amazon ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of Amazon ML, the model is linear regression and the algorithm
    the **Stochastic Gradient Descent** (**SGD**) algorithm. This algorithm has one
    main meta parameter called the learning rate and often noted ![](img/B05028_2_17_eqn.png),
    which dictates how much of a new sample is taken into account for each iterative
    update of the weights. A larger learning rate makes the algorithm converge faster
    but stabilizes further from the optimal weights, while a smaller learning induces
    a slower convergence but a more precise set of regression coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a training and a validation dataset, this is how Amazon ML tunes and
    select the best model:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon trains several models, each with a different learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a given a learning rate:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training dataset allows the SGD to train the model by finding the best regression
    coefficients
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is used on the validation dataset to make predictions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By comparing the quality of the predictions of the different models on that
    validation set, Amazon ML is able to select the best model and the associated
    best learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The held-out set is used as final confirmation that the model is reliable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usual splitting ratios for the training, validation, and held-out subsets are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training : *70%* validation and held-out 15% each'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training : *60%* validation and held-out 20% each'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shuffling**: It is important to make sure that the predictors and the outcome
    follow the same distribution in all three subsets. Shuffling the data before splitting
    it is an important part of creating reliable training, validation, and held-out subsets.'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to define the data transformations on the training dataset and
    apply the transformation parameters on the validation and held-out subsets so
    that the validation and held-out subsets do not leak information back in the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take standardization as an example: the standard deviation and the mean of
    the predictors should be calculated on the training dataset. And these values
    then applied to standardize the validation and held-out sets. If you use the whole
    original dataset to calculate the mean and SGD, you leak information from the held-out
    set into the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A common Supervised Predictive Analytics workflow follows these steps - Let''s
    assume we have an already extracted dataset and that we have chosen a metric to
    assess the quality of our predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: Building the dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cleaning up and transforming the data to handle noisy data issues
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating new predictors
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuffling and splitting the data into a training, a validation and a held-out
    set
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the best model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choosing a model (linear, tree-based, bayesian , ...)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Repeat for several values of the meta parameters:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model on the training set
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess the model performance on the validation set
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat steps 1 and 2 with new data, new predictors, and other model parameters
    until you are satisfied with the performances of your model. Keep the best model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final test of the model on the held-out subset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the context of Amazon ML, there is no possibility to choose a model (step
    2) other than a linear regression one (logistic regression for classification).
  prefs: []
  type: TYPE_NORMAL
- en: Identifying and correcting poor performances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A performant predictive model is one that produces reliable and satisfying predictions
    on new data. There are two situations where the model will fail to consistently
    produce good predictions, and both depend on how the model is trained. A poorly
    trained model will result in underfitting, while an overly trained model will
    result in overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Underfitting** means that the model was poorly trained. Either the training
    dataset did not have enough information to infer strong predictions, or the algorithm
    that trained the model on the training dataset was not adequate for the context.
    The algorithm was not well parameterized or simply inadequate for the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we measure the prediction error not only on the validation set but also
    on the training set, the prediction error will be large if the model is underfitting.
    Which makes sense: if the model cannot predict the training, it won''t be able
    to predict the outcomes in the validation set it has not seen before. Underfitting
    basically means your model is not working.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Common strategies to palliate this problem include:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting more data samples – If the problem comes from a dataset that is too
    small or does not contain sufficient information, getting more data may improve
    the model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding more features, raw or via feature engineering – by taking the log, squaring,
    binning, using splines or power functions. Adding many features and seeing how
    that improves the predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing another model – Support Vector Machine, Random Forest, Boosted trees,
    Bayes classifiers all have different strengths in different contexts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Overfitting** occurs when the model was so well trained that it fits the
    training data too perfectly and cannot handle new data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say you have a unique predictor of an outcome and that the data follows a quadratic
    pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: You fit a linear regression on that data ![](img/B05028_2_18_eqn.png), the predictions
    are weak. Your model is underfitting the data. There is a high error level on
    both the training error and the validation dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You add the square of the predictor in the model ![](img/B05028_2_19_eqn.png)
    and find that your model makes good predictions. The error on both the training
    and the validation datasets are equivalent and lower than for the simpler model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you increase the number and power of polynomial features so that the model
    is now ![](img/B05028_2_20_eqn.png), you end up fitting the training data too
    closely. The model has a very low prediction error on the training dataset but
    is unable to predict anything on new data. The prediction error on the validation
    dataset remains high.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a case of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows an example of an overfitting model with regard to
    the previous quadratic dataset, by setting a high order for the polynomial regression
    (*n = 16*). The polynomial regression fits the training data so well it would
    be incapable of any predictions on new data whereas the quadratic model (*n =
    2*) would be more robust:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_035.png)'
  prefs: []
  type: TYPE_IMG
- en: The best way to detect overfitting is, therefore, to compare the prediction
    errors on the training and validation sets. A significant gap between the two
    errors implies overfitting. A way to prevent this overfitting from happening is
    to add constraints on the model. In machine learning, we use regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization on linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Stochastic Gradient Descent algorithm (SGD) finds the optimal weights *{w[i]}*
    of the model by minimizing the error between the true and the predicted values
    on the N training samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_2_22_eqn.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/image_02_038.png) are the predicted values, *ŷ[i]* the real values
    to be predicted; we have *N* samples, and each sample has *n* dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regularization consists of adding a term to the previous equation and to minimize
    the regularized error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_2_24_eqn.png)'
  prefs: []
  type: TYPE_IMG
- en: The ![](img/image_02_042.png) parameter helps quantify the amount of regularization,
    while *R(w)* is the regularization term dependent on the regression coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of weight constraints usually considered:'
  prefs: []
  type: TYPE_NORMAL
- en: 'L2 regularization as the sum of the squares of the coefficients:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B05028_2_25_eqn.png)'
  prefs: []
  type: TYPE_IMG
- en: 'L1 regularization as the sum of the absolute value of the coefficients:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B05028_2_26_eqn.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The constraint on the coefficients introduced by the regularization term *R(w)*
    prevents the model from overfitting the training data. The coefficients become
    tied together by the regularization and can no longer be tightly leashed to the
    predictors. Each type of regularization has its characteristic and gives rise
    to different variations on the SGD algorithm, which we now introduce:'
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization and Ridge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: L2 regularization prevents the weights *{w[i]}* from being too spread. The smaller
    weights that rise up for non-correlated though potentially meaningful, features
    will not become insignificant when compared to the weights associated to the important
    correlated features. L2 regularization will enforce similar scaling of the weights.
    A direct consequence of L2 regularization is to reduce the negative impact of
    collinearity, since the weights can no longer diverge from one another.
  prefs: []
  type: TYPE_NORMAL
- en: The Stochastic Gradient Descent algorithm with L2 regularization is known as
    the **Ridge algorithm**.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization and Lasso
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: L1 regularization usually entails some loss of predictive power of the model.
  prefs: []
  type: TYPE_NORMAL
- en: One of the properties of L1 regularization is to force the smallest weights
    to 0 and thereby reduce the number of features taken into account in the model.
    This is a desired behavior when the number of features (*n*) is large compared
    to the number of samples (*N*). L1 is better suited for datasets with many features.
  prefs: []
  type: TYPE_NORMAL
- en: The Stochastic Gradient Descent algorithm with L1 regularization is known as
    the **Least Absolute Shrinkage and Selection Operator** **(Lasso)** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both cases the hyper-parameters of the model are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate ![](img/B05028_2_17_eqn.png) of the SGD algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A parameter ![](img/image_02_042.png) to tune the amount of regularization added
    to the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A third type of regularization called **ElasticNet** consists in adding both
    a L2 and a L1 regularization term to the model. This brings up the best of both
    regularization schemas at the expense of an extra hyper-parameter.
  prefs: []
  type: TYPE_NORMAL
- en: In other contexts, although experts have different opinions ([https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization))
    on which type of regularization is more effective, the consensus seems to favor
    L2 over L1 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'L2 and L1 regularization are both available in Amazon ML while ElasticNet is
    not. The amount of regularization available is limited to three values for  ![](img/image_02_042.png):
    mild (1*0^(-6)*), medium (*10^(-4)*), and aggressive (*10^(-2)*).'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performance of your model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Evaluating the predictive performance of a model requires defining a measure
    of the quality of its predictions. There are several available metrics both for
    regression and classification. The metrics used in the context of Amazon ML are
    the following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RMSE for regression**: The root mean squared error is defined by the square
    of the difference between the true outcome values and their predictions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B05028_2_27_eqn.png)'
  prefs: []
  type: TYPE_IMG
- en: '**F-1 Score and ROC-AUC for classification**: Amazon ML uses logistic regression
    for binary classification problems. For each prediction, logistic regression returns
    a value between 0 and 1\. This value is interpreted as a probability of the sample
    belonging to one of the two classes. A probability lower than 0.5 indicates belonging to
    the first class, while a probability higher than 0.5 indicates a belonging to
    the second class. The decision is therefore highly dependent on the value of the
    threshold. A value which we can modify.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Denoting one class positive and the other negative, we have four possibilities
    depicted in the following table:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  | **Predicted Yes** | **Predicted No** |'
  prefs: []
  type: TYPE_TB
- en: '| **Real value: Yes** | True Positive (TP) | False Negative (FN) (or type II
    error) |'
  prefs: []
  type: TYPE_TB
- en: '| **Real value: No** | False Positive (FP) | True Negative |'
  prefs: []
  type: TYPE_TB
- en: 'This matrix is called a **confusion matrix** ([https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix))
    . It defines four indicators of the performance of a classification model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TP: How many Yes were correctly predicted Yes'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FP: How many No were wrongly predicted Yes'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FN: How many Yes were wrongly predicted No'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TN: How many No were correctly predicted No'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From these four indicators, we can define the following metrics:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall:** This denotes the amount of predicted positives actually positive.
    Recall is also called **True Positive Rate** (**TPR**) or sensitivity. It is the
    probability of detection:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recall = (TP / TP + FN)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision** as the fraction of the real positives over all the positive predicted
    values:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Precision = (TP / TP + FP)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**False Positive Rate** is the number of falsely predicted positives over all
    the true negatives. It''s the probability of false alarm:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '*FPR = FP / FP + TN*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the **F1-score** is defined as the weighted average of the recall
    and the precision, and is given by the following:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '*F1-score = 2 TP / ( 2 TP + FP + FN)*'
  prefs: []
  type: TYPE_NORMAL
- en: A F1 score is always between 0 and 1, with 1 the best value and 0 the worst
    one.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: As noted previously, these scores are all dependent on the initial threshold
    used to interpret the result of the logistic regression in order to decide when
    a prediction belongs to one class or the other. We can choose to vary that threshold.
    This is where the ROC-AUC comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you plot the True Positive Rate (Recall) against the False Positive Rate
    for different values of the decision threshold, you obtain a graph like the following,
    called the **Receiver Operating Characteristic** or **ROC curve**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_054-1.png)'
  prefs: []
  type: TYPE_IMG
- en: The diagonal line indicates an equal probability of belonging to one class or
    another. The closer the curve is to the upper-left corner, the better your model
    performances are.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ROC curve has been widely used since WWII, when it was first invented to
    detect enemy planes in radar signals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have the ROC curve, you can calculate the **Area Under the Curve**
    or **AUC**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AUC will give you a unique score for your model taking into account all
    the possible values for the probability threshold from 0 to 1\. The higher the
    AUC the better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we focused on two important elements of a predictive analytics
    project: the data and the evaluation of the predictive power of the model. We
    first listed the most common problems encountered with raw data, their impact
    on the linear regression model, and ways to solve them. The reader should now
    be able to identify and deal with missing values, outliers, imbalanced datasets,
    and normalization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also introduced the two most frequent problems in predictive analytics:
    underfitting and overfitting. L1 and L2 regularization is an important element
    in the Amazon ML platform, which helps overcome overfitting and make models more
    robust and able to handle previously unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to dive into the Amazon Machine Learning platform in the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
