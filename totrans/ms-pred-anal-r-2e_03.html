<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;3.&#xA0;Linear Regression"><div class="book" id="OPEK2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03" class="calibre1"/>Chapter 3. Linear Regression</h1></div></div></div><p class="calibre8">We've learned from previous chapters that regression problems <span class="strong"><em class="calibre9">involve predicting a numerical output</em></span>. The simplest but most common type of regression is linear regression. In this chapter, we'll explore why linear regression is so commonly used, its limitations, and extensions, and then touch on <span class="strong"><em class="calibre9">polynomial regression</em></span>, which you may consider when a linear relationship isn't a best fit for your circumstances.</p></div>

<div class="book" title="Chapter&#xA0;3.&#xA0;Linear Regression">
<div class="book" title="Introduction to linear regression"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch03lvl1sec19" class="calibre1"/>Introduction to linear regression</h1></div></div></div><p class="calibre8">In <span class="strong"><strong class="calibre2">linear regression</strong></span>, the output variable is predicted by a linearly weighted combination of input <a id="id179" class="calibre1"/>features. Here is an example of a simple linear model:</p><div class="mediaobject"><img src="../images/00029.jpeg" alt="Introduction to linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The preceding model essentially says that we are estimating one output, and this is a linear function of a single predictor variable (that is, a feature) denoted by the letter <span class="strong"><em class="calibre9">x</em></span>. The terms <a id="id180" class="calibre1"/>involving the Greek letter <span class="strong"><em class="calibre9">β</em></span> are the parameters of the model and are known as <span class="strong"><strong class="calibre2">regression coefficients</strong></span>. Once we train the model and settle on values for these parameters, we can make a prediction on the output variable for any value of <span class="strong"><em class="calibre9">x</em></span> by a simple substitution in our equation. Another example of a linear model, this time with three features and with values assigned to the regression coefficients, is given by the following equation:</p><div class="mediaobject"><img src="../images/00030.jpeg" alt="Introduction to linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In this equation, just as with the previous one, we can observe that we have one more coefficient than the number of features. This additional coefficient, <span class="strong"><em class="calibre9">β<sub class="calibre14">0</sub></em></span>, is known as the <span class="strong"><strong class="calibre2">intercept</strong></span> and is the expected value of the model when the value of all input features is zero. The other <span class="strong"><em class="calibre9">β</em></span> coefficients can be interpreted as the expected change in the value of the output <a id="id181" class="calibre1"/>per unit increase of a feature. For example, in the preceding equation, if the value of the feature <span class="strong"><em class="calibre9">x<sub class="calibre14">1</sub></em></span> rises by one unit, the expected value of the output will rise <a id="id182" class="calibre1"/>by 1.91 units. Similarly, a unit increase in the feature <span class="strong"><em class="calibre9">x<sub class="calibre14">3</sub></em></span> results in a decrease in the output by 7.56 units. In a simple one-dimensional regression problem, we can plot the output on the <span class="strong"><em class="calibre9">y </em></span>axis of a graph and the input feature on the <span class="strong"><em class="calibre9">x</em></span> axis. In this case, the model predicts a straight-line relationship between these two, where <span class="strong"><em class="calibre9">β<sub class="calibre14">0</sub></em></span> represents the point at which the straight line crosses or intercepts the <span class="strong"><em class="calibre9">y</em></span> axis and <span class="strong"><em class="calibre9">β<sub class="calibre14">1</sub></em></span> represents <a id="id183" class="calibre1"/>the slope of the line. We often refer to the case of a single feature (hence, two regression coefficients) as <span class="strong"><strong class="calibre2">simple linear regression</strong></span> and the <a id="id184" class="calibre1"/>case of two or more features as <span class="strong"><strong class="calibre2">multiple linear regression</strong></span>.</p></div></div>

<div class="book" title="Chapter&#xA0;3.&#xA0;Linear Regression">
<div class="book" title="Introduction to linear regression">
<div class="book" title="Assumptions of linear regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec28" class="calibre1"/>Assumptions of linear regression</h2></div></div></div><p class="calibre8">Before we <a id="id185" class="calibre1"/>delve into the details of how to train a linear regression model and how it performs, we'll look at model assumptions. Model assumptions essentially describe what the model believes about the output variable <span class="strong"><em class="calibre9">y</em></span> that we are trying to predict. Specifically, linear regression models assume that the output variable is a weighted linear function of a set of feature variables. Additionally, the model assumes that for fixed values of the feature variables, the output is normally distributed with a constant variance. This is the same as saying that the model assumes that the true output variable <span class="strong"><em class="calibre9">y</em></span> can be represented by an equation such as the following one, shown for two input features:</p><div class="mediaobject"><img src="../images/00031.jpeg" alt="Assumptions of linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, <span class="strong"><em class="calibre9">ε</em></span> represents an error term, which is normally distributed with a zero mean and a constant variance <span class="strong"><em class="calibre9">σ<sup class="calibre15">2</sup></em></span>:</p><div class="mediaobject"><img src="../images/00032.jpeg" alt="Assumptions of linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We might come across the term <span class="strong"><strong class="calibre2">homoscedasticity</strong></span> as a more formal way of describing the notion of constant <a id="id186" class="calibre1"/>variance. By homoscedasticity or constant variance, we are referring to the fact that the variance in the error component does not vary <a id="id187" class="calibre1"/>with the values or levels of the input features. In the following plot, we are visualizing a hypothetical example of a linear relationship with <span class="strong"><strong class="calibre2">heteroskedastic</strong></span> errors, which <a id="id188" class="calibre1"/>are errors that do not have a constant variance. The data points lie close to the line at low values of the input feature, because the variance is low in this region of the plot, but lie farther away from the line at higher values of the input feature because of the higher variance:</p><div class="mediaobject"><img src="../images/00033.jpeg" alt="Assumptions of linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The <span class="strong"><em class="calibre9">ε</em></span> term is an irreducible error component of the true function <span class="strong"><em class="calibre9">y</em></span> and can be used to represent random errors, such as measurement errors in the feature values. When training a linear regression model, we always expect to observe some amount of error in our estimate of the output, even if we have all the right features, enough data, and the system being modeled really is linear. </p><p class="calibre8">Put differently, even with a true function that is linear, we still expect that once we find a line of best fit through our training examples, our line will not go through all, or even any, of our data points because of this inherent variance exhibited by the error component. The critical thing to remember, though, is that in this ideal scenario, because our error component has zero mean and constant variance, our training criterion will allow us to come close to the true values of the regression coefficients given a sufficiently large sample, as the errors will cancel out.</p><p class="calibre8">Another <a id="id189" class="calibre1"/>important assumption relates to the independence of the error terms. This means that we do not expect the <span class="strong"><strong class="calibre2">residual</strong></span> or error term associated <a id="id190" class="calibre1"/>with one particular observation to be somehow correlated with that of another observation. This assumption can be violated if observations are functions of each other, which is typically the result of an error in the measurement. If we were to take a portion of our training data, double all the values of the features and outputs, and add these new data points to our training data, we could create the illusion of having a larger dataset; however, there will be pairs of observations whose error terms will depend on each other as a result, and hence our model assumption would be violated. Incidentally, artificially growing our dataset in such a manner is never acceptable for any model. Similarly, correlated error terms may occur if observations are related in some way by an unmeasured variable. For example, if we are measuring the malfunction rate of parts from an assembly line, then parts from the same factory might have a correlation in the error: for example, due to different standards and protocols used in the assembly process. Therefore, if we don't use the factory as a feature, we may see correlated errors in our sample <a id="id191" class="calibre1"/>among observations that correspond to parts from the same factory. The study of <span class="strong"><strong class="calibre2">experimental design</strong></span> is concerned with identifying and reducing correlations in error terms, but this is beyond the scope of this book.</p><p class="calibre8">Finally, another important assumption concerns the notion that the features themselves are statistically independent of each other. It is worth clarifying here that in linear models, although the input features must be linearly weighted, they themselves may be the output of another function. To illustrate this, one might be surprised to see that the following is a linear model of three features, <span class="strong"><em class="calibre9">sin(z<sub class="calibre14">1</sub></em></span><span class="strong"><em class="calibre9">)</em></span>, <span class="strong"><em class="calibre9">ln(z<sub class="calibre14">2</sub></em></span>
<span class="strong"><em class="calibre9">)</em></span>, and <span class="strong"><em class="calibre9">exp(z</em></span><sub class="calibre14">3</sub><span class="strong"><em class="calibre9">)</em></span>:</p><div class="mediaobject"><img src="../images/00034.jpeg" alt="Assumptions of linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We can see that this is a linear model by making a few transformations on the input features and then making the replacements in our model:</p><div class="mediaobject"><img src="../images/00035.jpeg" alt="Assumptions of linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, we have an equation that is more recognizable as a linear regression model. If the previous example made us believe that nearly everything could be transformed into a linear model, then the following two examples will emphatically convince us that this is not in fact the case:</p><div class="mediaobject"><img src="../images/00036.jpeg" alt="Assumptions of linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Both models <a id="id192" class="calibre1"/>are not linear models because of the first regression coefficient (<span class="strong"><em class="calibre9">β<sub class="calibre14">1</sub></em></span>). The first model is not a linear model because <span class="strong"><em class="calibre9">β<sub class="calibre14">1</sub></em></span> is acting as the exponent of the first input feature. In the second model, <span class="strong"><em class="calibre9">β<sub class="calibre14">1</sub></em></span> is inside a <span class="strong"><em class="calibre9">sine</em></span> function. The important lesson to take away from these examples is that there are cases where we can apply transformations on our input features in order to fit our data to a linear model; however, we need to be careful that our regression coefficients are always the linear weights of the resulting new features.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Simple linear regression"><div class="book" id="PNV62-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec20" class="calibre1"/>Simple linear regression</h1></div></div></div><p class="calibre8">Before looking <a id="id193" class="calibre1"/>at some real-world datasets, it is very helpful to try to train a model on artificially generated data. In an artificial scenario such as this, we know what the true output function is beforehand, something that as a rule is not the case when it comes to real-world data. The advantage of performing this exercise is that it gives us a good idea of how our model works under the ideal scenario when all of our assumptions are fully satisfied, and it helps visualize what happens when we have a good linear fit. We'll begin by simulating a simple linear regression model. The following R snippet is used to create a data frame with 100 simulated observations of the following linear model with a single input feature:</p><div class="mediaobject"><img src="../images/00037.jpeg" alt="Simple linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here is the code for the simple linear regression model:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(5427395)
&gt; nObs = 100
&gt; x1minrange = 5
&gt; x1maxrange = 25
&gt; x1 = runif(nObs, x1minrange, x1maxrange)
&gt; e = rnorm(nObs, mean = 0, sd = 2.0)
&gt; y = 1.67 * x1 - 2.93 + e
&gt; df = data.frame(y, x1)</pre></div><p class="calibre8">For our <a id="id194" class="calibre1"/>input feature, we randomly sample points from a uniform distribution. We used a uniform distribution to get a good spread of data points. Note that our final <code class="email">df</code> data frame is meant to simulate a data frame that we would obtain in practice; as a result, we do not include the error terms, as these would be unavailable to us in a real-world setting.</p><p class="calibre8">When we <a id="id195" class="calibre1"/>train a linear model using some data such as the data in our data frame, we are essentially hoping to produce a linear model with the same coefficients as the ones from the underlying model of the data. Put differently, the original coefficients define a <span class="strong"><strong class="calibre2">population regression line</strong></span>. In this case, the population regression line represents the true underlying model of the data. In general, we will find ourselves attempting to model a function that is not necessarily linear. In this case, we can still define the population regression line as the best possible linear regression line, but a linear regression model will obviously not perform equally well.</p></div>

<div class="book" title="Simple linear regression">
<div class="book" title="Estimating the regression coefficients"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec29" class="calibre1"/>Estimating the regression coefficients</h2></div></div></div><p class="calibre8">For our <a id="id196" class="calibre1"/>simple linear regression model, the process of training the model amounts to an estimation of our two regression <a id="id197" class="calibre1"/>coefficients from our dataset. As we can see from our previously constructed data frame, our data is effectively a series of observations, each of which is a pair of values (<span class="strong"><em class="calibre9">x<sub class="calibre14">i</sub></em></span>, <span class="strong"><em class="calibre9">y<sub class="calibre14">i</sub></em></span>) where the first element of the pair is the input feature value and the second element of the pair is its output label. It turns out that for the case of simple linear regression, it is possible to write down two equations that can be used to compute our two regression coefficients. Instead of merely presenting these equations, we'll first take a brief moment to review some very basic statistical quantities that the reader has most likely encountered previously, as they will be featured very shortly.</p><p class="calibre8">The <span class="strong"><strong class="calibre2">mean</strong></span> of a set of values is just the average of these values and is often described as a measure <a id="id198" class="calibre1"/>of location, giving a sense of where the <a id="id199" class="calibre1"/>values are centered on the scale in which they are measured. In statistical literature, the average <a id="id200" class="calibre1"/>value of a random variable is often known as the <span class="strong"><strong class="calibre2">expectation</strong></span>, so we often find that the mean of a random <a id="id201" class="calibre1"/>variable <span class="strong"><em class="calibre9">X</em></span> is denoted as <span class="strong"><em class="calibre9">E(X)</em></span>. Another notation that is commonly used is bar notation, where we can represent the notion of taking the average of a variable by placing a bar over that variable. To illustrate this, the following two equations show the mean of the output variable <span class="strong"><em class="calibre9">y</em></span> and input feature <span class="strong"><em class="calibre9">x</em></span>:</p><div class="mediaobject"><img src="../images/00038.jpeg" alt="Estimating the regression coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">A second <a id="id202" class="calibre1"/>very common quantity, which should also be familiar, is the <span class="strong"><strong class="calibre2">variance</strong></span> of a variable. The variance measures the average square distance that individual values have from the mean. In this way, it is a measure of dispersion, so that a low variance implies that most of the values are bunched up close to the mean, whereas a higher variance results in values that are spread out. Note that the definition of variance involves the definition of the mean, and for this reason we'll see the use of the <span class="strong"><em class="calibre9">x</em></span> variable with a bar on <a id="id203" class="calibre1"/>it in the following equation, which shows the variance of our input feature <span class="strong"><em class="calibre9">x</em></span>:</p><div class="mediaobject"><img src="../images/00039.jpeg" alt="Estimating the regression coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Finally, we'll define the <span class="strong"><strong class="calibre2">covariance</strong></span> between two random variables, <span class="strong"><em class="calibre9">x</em></span> and <span class="strong"><em class="calibre9">y</em></span>, using the following equation:</p><div class="mediaobject"><img src="../images/00040.jpeg" alt="Estimating the regression coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">From the previous equation, it should be clear that the variance, which we just defined previously, is actually a special case of the covariance where the two variables are the same. The covariance measures how strongly two variables are correlated with each other and can be positive or negative. A positive covariance implies a positive correlation; that is, when one variable increases, the other will increase as well. A negative covariance suggests the opposite; when one variable increases, the other will tend to decrease. When two <a id="id204" class="calibre1"/>variables are statistically independent of each other and hence uncorrelated, their covariance will be zero (although it should be noted that a zero covariance does not necessarily imply statistical independence).</p><p class="calibre8">Armed with these basic concepts, we can now present equations for the estimates of the two regression coefficients for the case of simple linear regression:</p><div class="mediaobject"><img src="../images/00041.jpeg" alt="Estimating the regression coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The first regression coefficient can be computed as the ratio of the covariance between the output and the input feature, and the variance of the input feature. Note that if the output feature were to be independent of the input feature, the covariance would be zero and therefore, our linear model would consist of a horizontal line with no slope. In practice, it should be noted that even when two variables are statistically independent, we will still typically see a small degree of covariance due to the random nature of the errors; thus, if we were to train a linear regression model to describe their relationship, our first regression coefficient would be nonzero in general. Later, we'll see how significance tests can be used to detect features we should not include in our models.</p><p class="calibre8">To implement linear regression in R, it is not necessary to perform these calculations as R provides us with the <code class="email">lm()</code> function, which builds a linear regression model for us. The following code sample uses the <code class="email">df</code> data frame we created previously and calculates the regression coefficients:</p><div class="informalexample"><pre class="programlisting">&gt; myfit &lt;- lm(y~x1, df)
&gt; myfit

Call:
lm(formula = y ~ x1, data = df)

Coefficients:
(Intercept)           x1  
     -2.380        1.641  </pre></div><p class="calibre8">In the first line, we see that the usage of the <code class="email">lm()</code> function involves first specifying a formula and then following up with the <code class="email">data</code> parameter, which in our case is our data frame. In the case of simple linear regression, the syntax of the formula that we specify for the <code class="email">lm()</code> function is the name of the output variable, followed by a tilde (<span class="strong"><em class="calibre9">~</em></span>) and then by the name of the single input feature. We'll see how to specify more complex formulas when we <a id="id205" class="calibre1"/>look at multiple linear regression further along in this chapter. Finally, the output shows us the values for the two regression coefficients. Note that the <span class="strong"><em class="calibre9">β<sub class="calibre14">0</sub></em></span> coefficient is labeled as the intercept, and the <span class="strong"><em class="calibre9">β<sub class="calibre14">1</sub></em></span> coefficient is labeled by the name of the corresponding feature (in this case, <code class="email">x<sub class="calibre14">1</sub></code>) in the equation of the linear model:</p><p class="calibre8">The following graph shows the population line and the estimated line on the same plot:</p><div class="mediaobject"><img src="../images/00042.jpeg" alt="Estimating the regression coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As we can see, the two lines are so close to each other that they are barely distinguishable, showing that the model has estimated the true population line very closely. From <a class="calibre1" title="Chapter 1. Gearing Up for Predictive Modeling" href="part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7">Chapter 1</a>, <span class="strong"><em class="calibre9">Gearing Up for Predictive Modeling</em></span>, we know that we can formalize how closely our model matches our dataset, as well as how closely it would match an analogous test set using the mean square error. We'll examine this as well as several other metrics <a id="id206" class="calibre1"/>of model performance and quality in this chapter, but first we'll generalize our regression model to deal with more than one input feature.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Multiple linear regression"><div class="book" id="QMFO2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec21" class="calibre1"/>Multiple linear regression</h1></div></div></div><p class="calibre8">Whenever <a id="id207" class="calibre1"/>we have more than one input feature and want to build a linear regression model, we are in the realm of multiple linear regression. The general equation for a multiple linear regression model with <span class="strong"><em class="calibre9">k</em></span> input features is:</p><div class="mediaobject"><img src="../images/00043.jpeg" alt="Multiple linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Our assumptions about the model and about the error component <span class="strong"><em class="calibre9">ε</em></span> remain the same as with simple linear regression, remembering that, as we now have more than one input feature, we assume that these are independent of each other. Instead of using simulated data to demonstrate multiple linear regression, we will analyze two real-world datasets.</p></div>

<div class="book" title="Multiple linear regression">
<div class="book" title="Predicting CPU performance"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec30" class="calibre1"/>Predicting CPU performance</h2></div></div></div><p class="calibre8">Our first <a id="id208" class="calibre1"/>real-world dataset was presented by the researchers <span class="strong"><em class="calibre9">Dennis F. Kibler</em></span>, <span class="strong"><em class="calibre9">David W. Aha</em></span>, and <span class="strong"><em class="calibre9">Marc K. Albert</em></span> in a 1989 paper entitled <span class="strong"><em class="calibre9">Instance-based prediction of real-valued attributes</em></span> and published in the <span class="strong"><em class="calibre9">Journal of Computational Intelligence</em></span>. The data contains the characteristics of different CPU models, such as the cycle time and the amount of cache memory. When deciding between processors, we would like to take all of these things into account, but ideally, we'd like to compare processors on a single numerical scale. For this reason, we often develop programs to benchmark the relative performance of a CPU. Our dataset also comes with the published relative performance of our CPUs and our objective will be to use the available CPU characteristics as features to predict this. The dataset can be obtained <a id="id209" class="calibre1"/>online from the UCI Machine Learning Repository via this link: <a class="calibre1" href="http://archive.ics.uci.edu/ml/datasets/Computer+Hardware">http://archive.ics.uci.edu/ml/datasets/Computer+Hardware</a>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip08" class="calibre1"/>Tip</h3><p class="calibre8">The UCI Machine Learning Repository is a wonderful online resource that hosts a large number of datasets, many of which are often cited by authors of books and tutorials. It is well worth the effort to familiarize yourself with this website and its datasets. A very good way to learn predictive analytics is to practice using the techniques you learn in this book on different datasets, and the UCI repository provides many of these for exactly this purpose.</p></div><p class="calibre8">The <code class="email">machine.data</code> file contains all our data in a comma-separated format, with one line per CPU model. We'll import this in R and label all the columns. Note that there are 10 columns <a id="id210" class="calibre1"/>in total, but we don't need the first two for our analysis, as these are just the brand and model name of the CPU. Similarly, the final column is a predicted estimate of the relative performance that was produced by the researchers themselves; our actual output variable, PRP, is in column <span class="strong"><em class="calibre9">9</em></span>. We'll store the data that we need in a data frame called <code class="email">machine</code>:</p><div class="informalexample"><pre class="programlisting">&gt; machine &lt;- read.csv("machine.data", header = F)
&gt; names(machine) &lt;- c("VENDOR", "MODEL", "MYCT", "MMIN", "MMAX", "CACH", "CHMIN", "CHMAX", "PRP", "ERP")
&gt; machine &lt;- machine[, 3:9]
&gt; head(machine, n = 3)
  MYCT MMIN  MMAX CACH CHMIN CHMAX PRP
1  125  256  6000  256    16   128 198
2   29 8000 32000   32     8    32 269
3   29 8000 32000   32     8    32 220</pre></div><p class="calibre8">The dataset also comes with definitions of the data columns:</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Column name</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Definition</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">MYCT</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The machine cycle time in nanoseconds</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">MMIN</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The minimum main memory in kilobytes</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">MMAX</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The maximum main memory in kilobytes</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CACH</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The cache memory in kilobytes</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CHMIN</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The minimum channels in units</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CHMAX</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The maximum channels in units</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">PRP</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The published relative performance (our output variable)</p>
</td></tr></tbody></table></div><p class="calibre8">The dataset contains no missing values, so no observations need to be removed or modified. One thing that we'll notice is that we only have roughly 200 data points, which is generally considered a very small sample. Nonetheless, we will proceed with splitting our data into a training set and a test set, with an 85-15 split, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt; set.seed(4352345)
&gt; machine_sampling_vector &lt;- createDataPartition(machine$PRP, p = 0.85, list = FALSE)
&gt; machine_train &lt;- machine[machine_sampling_vector,]
&gt; machine_train_features &lt;- machine[, 1:6]
&gt; machine_train_labels &lt;- machine$PRP[machine_sampling_vector]
&gt; machine_test &lt;- machine[-machine_sampling_vector,]
&gt; machine_test_labels &lt;- machine$PRP[-machine_sampling_vector]</pre></div><p class="calibre8">Now that we have our data set up-and-running, we'll usually want to investigate further and check <a id="id211" class="calibre1"/>whether some of our assumptions for linear regression are valid. For example, we would like to know whether we have any highly correlated features. To do this, we can construct a correlation matrix with the <code class="email">cor()</code>function and use the <code class="email">findCorrelation()</code> function from the <code class="email">caret</code> package to get suggestions for which features to remove:</p><div class="informalexample"><pre class="programlisting">&gt; machine_correlations &lt;- cor(machine_train_features)
&gt; findCorrelation(machine_correlations)
integer(0)
&gt; findCorrelation(machine_correlations, cutoff = 0.75)
[1] 3
&gt; cor(machine_train$MMIN, machine_train$MMAX)
[1] 0.7679307</pre></div><p class="calibre8">Using the default cutoff of <code class="email">0.9</code> for a high degree of correlation, we found that none of our features should be removed. When we reduce this cutoff to <code class="email">0.75</code>, we see that <code class="email">caret</code> recommends that we remove the third feature (MMAX). As the final line of the preceding code shows, the degree of correlation between this feature and MMIN is <code class="email">0.768</code>. While the value is not very high, it is still high enough to cause us a certain degree of concern that this will affect our model. Intuitively, of course, if we look at the definitions of our input features, we will certainly tend to expect that a model with a relatively high value for the minimum main memory will also be likely to have a relatively high value for the maximum main memory. Linear regression can sometimes still give us a good model with correlated variables, but we would expect to get better results if our variables were uncorrelated. For now, we've decided to keep all our features for this dataset.</p></div></div>

<div class="book" title="Multiple linear regression">
<div class="book" title="Predicting the price of used cars"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec31" class="calibre1"/>Predicting the price of used cars</h2></div></div></div><p class="calibre8">Our second <a id="id212" class="calibre1"/>dataset is in the cars data frame included in the <code class="email">caret</code> package and was collected by <span class="strong"><em class="calibre9">Shonda Kuiper </em></span>in 2008 from the <span class="strong"><em class="calibre9">Kelly Blue Book</em></span> website, <a class="calibre1" href="http://www.kbb.com">www.kbb.com</a>. This is an online resource to obtain reliable prices for used cars. The dataset comprises 804 GM cars, all with the model year 2005. It includes a number of car attributes, such as the mileage and engine size as well as the suggested selling price. Many features are binary indicator variables, such as the Buick feature, which represents whether a particular car's make is Buick. The cars were <a id="id213" class="calibre1"/>all in excellent condition and less than one year old when priced, so the car condition is not included as a feature. Our objective for this dataset is to build a model that will predict the selling price of a car using the values of these attributes. The definitions of the features are as follows:</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Column name</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Definition</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Price</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The suggested retail price in USD (our output variable)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Mileage</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The number of miles the car has been driven</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Cylinder</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The number of cylinders in the car's engine</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Doors</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The number of doors</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Cruise</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the car has cruise control</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Sound</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the car has upgraded speakers</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Leather</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the car has leather seats</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Buick</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the make of the car is Buick</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Cadillac</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the make of the car is Cadillac</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Chevy</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the make of the car is Chevy</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Pontiac</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the make of the car is Pontiac</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Saab</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the make of the car is Saab</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Saturn</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the make of the car is Saturn</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">convertible</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the type of the car is a convertible</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">coupe</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the type of the car is a coupe</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">hatchback</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the type of the car is a hatchback</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">sedan</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the type of the car is a sedan</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">wagon</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The indicator variable representing whether the type of the car is a wagon</p>
</td></tr></tbody></table></div><p class="calibre8">As with <a id="id214" class="calibre1"/>the machine dataset, we should investigate the correlation between the input features:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt; data(cars)
&gt; cars_cor &lt;- cor(cars_train_features)
&gt; findCorrelation(cars_cor)
integer(0)
&gt; findCorrelation(cars_cor, cutoff = 0.75)
[1] 3
&gt; cor(cars$Doors,cars$coupe)
[1] -0.8254435
&gt; table(cars$coupe,cars$Doors)
   
      2   4
  0  50 614
  1 140   0</pre></div><p class="calibre8">Just as with the machine dataset, we have a correlation that shows up when we set <code class="email">cutoff</code> to <code class="email">0.75</code> in the <code class="email">findCorrelation()</code> function of <code class="email">caret</code>. By directly examining the correlation matrix, we found that there is a relatively high degree of correlation between the <code class="email">Doors</code> feature and the <code class="email">coupe</code> feature. By cross-tabulating these two, we can see why this is the case. If we know that the type of a car is a coupe, then the number of doors is always two. If the car is not a coupe, then it most likely has four doors.</p><p class="calibre8">Another problematic aspect of the cars data is that some features are exact linear combinations of other features. This is discovered using the <code class="email">findLinearCombos()</code> function in the caret package:</p><div class="informalexample"><pre class="programlisting">&gt; findLinearCombos(cars)
$linearCombos
$linearCombos[[1]]
[1] 15  4  8  9 10 11 12 13 14

$linearCombos[[2]]
 [1] 18  4  8  9 10 11 12 13 16 17


$remove
[1] 15 18</pre></div><p class="calibre8">Here, we <a id="id215" class="calibre1"/>are advised to drop the <code class="email">coupe</code> and <code class="email">wagon</code> columns, which are the 15<sup class="calibre15">th</sup> and 18<sup class="calibre15">th</sup> features, respectively, because they are exact linear combinations of other features. We will remove both of these from our data frame, thus also eliminating the correlation problem we saw previously.</p><p class="calibre8">Next, we'll split our data into training and test sets:</p><div class="informalexample"><pre class="programlisting">&gt; cars &lt;- cars[,c(-15, -18)]
&gt; set.seed(232455)
&gt; cars_sampling_vector &lt;- createDataPartition(cars$Price, p = 
  0.85, list = FALSE)
&gt; cars_train &lt;- cars[cars_sampling_vector,]
&gt; cars_train_features &lt;- cars[,-1]
&gt; cars_train_labels &lt;- cars$Price[cars_sampling_vector]
&gt; cars_test &lt;- cars[-cars_sampling_vector,]
&gt; cars_test_labels &lt;- cars$Price[-cars_sampling_vector]</pre></div><p class="calibre8">Now that we have our data ready, we'll build some models.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Assessing linear regression models"><div class="book" id="RL0A2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec22" class="calibre1"/>Assessing linear regression models</h1></div></div></div><p class="calibre8">We'll <a id="id216" class="calibre1"/>once again use the <code class="email">lm()</code> function to fit linear regression models to our data. For both of our datasets, we'll want to use all the input features that remain in our respective data frames. R provides us with a shorthand to write formulas that include all the columns of a data frame as features, excluding the one chosen as the output. This is done using a single period, as the following code snippets show:</p><div class="informalexample"><pre class="programlisting">&gt; machine_model1 &lt;- lm(PRP ~ ., data = machine_train)
&gt; cars_model1 &lt;- lm(Price ~ ., data = cars_train)</pre></div><p class="calibre8">Training a linear regression model may be a one-line affair once we have all our data prepared, but the important work comes straight after, when we study our model in order to determine how well we did. Fortunately, we can instantly obtain some important information about our model using the <code class="email">summary()</code> function. The output of this function for our CPU dataset is shown here:</p><div class="informalexample"><pre class="programlisting">&gt; summary(machine_model1)

Call:
lm(formula = PRP ~ ., data = machine_train)

Residuals:
    Min      1Q  Median      3Q     Max 
-199.29  -24.15    6.91   26.26  377.47 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -5.963e+01  8.861e+00  -6.730 2.43e-10 ***
MYCT         5.210e-02  1.885e-02   2.764 0.006335 ** 
MMIN         1.543e-02  2.025e-03   7.621 1.62e-12 ***
MMAX         5.852e-03  6.867e-04   8.522 7.68e-15 ***
CACH         5.311e-01  1.494e-01   3.555 0.000488 ***
CHMIN        7.761e-02  1.055e+00   0.074 0.941450    
CHMAX        1.498e+00  2.304e-01   6.504 8.20e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 61.31 on 172 degrees of freedom
Multiple R-squared:  0.874,	Adjusted R-squared:  0.8696 
F-statistic: 198.8 on 6 and 172 DF,  p-value: &lt; 2.2e-16</pre></div><p class="calibre8">Following a <a id="id217" class="calibre1"/>repeat of the call we made to the <code class="email">lm()</code> function itself, the information provided by the <code class="email">summary()</code> function is organized into three distinct sections. The first section is a summary of the model residuals, which are the errors that our model makes on the observations in the data on which it was trained. The second section is a table containing the predicted values of the model coefficients as well as the results of their significance tests. The final few lines display overall performance metrics for our model. If we repeat the same process on our cars dataset, we will notice the following line in our model summary:</p><div class="informalexample"><pre class="programlisting">Coefficients: (1 not defined because of singularities)</pre></div><p class="calibre8">This occurs because we still have a feature whose effect on the output is indiscernible from other features due to underlying dependencies. This phenomenon is known as <span class="strong"><strong class="calibre2">aliasing</strong></span>. The <code class="email">alias()</code> command shows the features we need to remove from the model:</p><div class="informalexample"><pre class="programlisting">&gt; alias(cars_model1)
Model :
Price ~ Mileage + Cylinder + Doors + Cruise + Sound + Leather + 
    Buick + Cadillac + Chevy + Pontiac + Saab + Saturn + convertible + hatchback + sedan

Complete :
       (Intercept) Mileage Cylinder Doors Cruise Sound
Saturn  1           0       0        0     0      0   
       Leather Buick Cadillac Chevy Pontiac Saab convertible
Saturn  0      -1    -1       -1    -1      -1    0         
       hatchback sedan
Saturn  0         0   </pre></div><p class="calibre8">As we can see, the problematic feature is the <code class="email">Saturn</code> feature, so we will remove this feature <a id="id218" class="calibre1"/>and retrain the model. To exclude a feature from a linear regression model, we include it in the formula after the period and prefix it with a minus sign:</p><div class="informalexample"><pre class="programlisting">&gt; cars_model2 &lt;- lm(Price ~. -Saturn, data = cars_train)
&gt; summary(cars_model2)

Call:
lm(formula = Price ~ . - Saturn, data = cars_train)

Residuals:
    Min      1Q  Median      3Q     Max 
-9324.8 -1606.7   150.5  1444.6 13461.0 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -954.1919  1071.2553  -0.891  0.37340    
Mileage        -0.1877     0.0137 -13.693  &lt; 2e-16 ***
Cylinder     3640.5417   123.5788  29.459  &lt; 2e-16 ***
Doors        1552.4008   284.3939   5.459 6.77e-08 ***
Cruise        330.0989   324.8880   1.016  0.30998    
Sound         388.4549   256.3885   1.515  0.13022    
Leather       851.3683   274.5213   3.101  0.00201 ** 
Buick        1104.4670   595.0681   1.856  0.06389 .  
Cadillac    13288.4889   673.6959  19.725  &lt; 2e-16 ***
Chevy        -553.1553   468.0745  -1.182  0.23772    
Pontiac     -1450.8865   524.9950  -2.764  0.00587 ** 
Saab        12199.2093   600.4454  20.317  &lt; 2e-16 ***
convertible 11270.4878   597.5162  18.862  &lt; 2e-16 ***
hatchback   -6375.4970   669.6840  -9.520  &lt; 2e-16 ***
sedan       -4441.9152   490.8347  -9.050  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2947 on 669 degrees of freedom
Multiple R-squared:  0.912,	Adjusted R-squared:  0.9101 
F-statistic: 495.1 on 14 and 669 DF,  p-value: &lt; 2.2e-16</pre></div></div>

<div class="book" title="Assessing linear regression models">
<div class="book" title="Residual analysis"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec32" class="calibre1"/>Residual analysis</h2></div></div></div><p class="calibre8">A residual <a id="id219" class="calibre1"/>is simply the error our model makes for a particular observation. Put differently, it is the difference between the actual value of the output and our prediction:</p><div class="mediaobject"><img src="../images/00044.jpeg" alt="Residual analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Analyzing residuals is very important when building a good regression model, as residuals reveal various aspects of our model, from violated assumptions and the quality of the fit to other problems, such as outliers. To understand the metrics in the residual summary, imagine <a id="id220" class="calibre1"/>ordering residuals from the smallest to the largest. Besides the minimum and maximum values that occur at the extremes of this sequence, the summary shows the first and third quartiles, which are the values a quarter along the way in <a id="id221" class="calibre1"/>this sequence and three quarters, respectively. The <span class="strong"><strong class="calibre2">median</strong></span> is the value in the middle of the sequence. The <span class="strong"><strong class="calibre2">interquartile range</strong></span> is the portion of the sequence between the first and third quartiles, and by definition contains half of the data. Looking first at the residual summary from our CPU model, it is interesting to note that the first and third quartiles are quite small in value compared to the minimum and maximum value. This is a first indication that there might be a few points that have a large residual error. In an ideal scenario, our residuals will have a median of zero and will have small values for the quartiles. We can reproduce the residuals summary from the summary function by noting that the model produced by the <code class="email">lm()</code> function has a <code class="email">residuals</code> attribute:</p><div class="informalexample"><pre class="programlisting">&gt; summary(cars_model2$residuals)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-9325.0 -1607.0   150.5     0.0  1445.0 13460.0 
&gt; mean(cars_train$Price)
[1] 21320.2</pre></div><p class="calibre8">Note that, in the preceding example for our cars model, we need to compare the value of the residuals against the average value of the output variable, in order to get a sense of whether the residuals are large or not. Thus, the previous results show that the average selling price of a car in our training data is around $21 k, and 50% of our predictions are roughly within ± $1.6 k of the correct value, which seems fairly reasonable. Obviously, the residuals for our CPU model are all much smaller in the absolute value because the values of the output variable for that model, namely the published relative performance, are <a id="id222" class="calibre1"/>much smaller than the values for <code class="email">Price</code> in the cars model.</p><p class="calibre8">In linear <a id="id223" class="calibre1"/>regression, we assume that the irreducible errors in our model are randomly distributed with a normal distribution. A diagnostic plot, known as the <span class="strong"><strong class="calibre2">Quantile-Quantile plot</strong></span> (<span class="strong"><strong class="calibre2">Q-Q plot</strong></span>), is useful in helping us visually gauge the extent to which this assumption holds. The key idea behind this plot is that we <a id="id224" class="calibre1"/>can compare two distributions by comparing the values at their <span class="strong"><strong class="calibre2">quantiles</strong></span>. The quantiles of a distribution are essentially evenly spaced intervals of a random variable, such that each interval has the same probability; for example, quartiles are four-quantiles because they split up a distribution into four equally probable parts. If the two distributions are the same, then the graph should be a plot of the line <span class="strong"><em class="calibre9">y = x</em></span>. To check whether our residuals are normally distributed, we can compare their distribution against a normal distribution and see how close to the <span class="strong"><em class="calibre9">y = x</em></span> line we land.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip09" class="calibre1"/>Tip</h3><p class="calibre8">There are many other ways to check whether the model residuals are normally distributed. A good place to look is the <code class="email">nortest</code> R package, which implements a number of well-known tests for normality, including the Anderson-Darling test and the Lilliefors test. In addition, the <code class="email">stats</code> package contains the <code class="email">shapiro.test()</code> function for performing the Shapiro-Wilk normality test.</p></div><p class="calibre8">The following code generates Q-Q plots for our two datasets:</p><div class="informalexample"><pre class="programlisting">&gt; par(mfrow = c(2, 1))
&gt; machine_residuals &lt;- machine_model1$residuals
&gt; qqnorm(machine_residuals, main = "Normal Q-Q Plot for CPU data set")
&gt; qqline(machine_residuals)
&gt; cars_residuals &lt;- cars_model2$residuals
&gt; qqnorm(cars_residuals, main = "Normal Q-Q Plot for Cars \data set")
&gt; qqline(cars_residuals)</pre></div><p class="calibre8">The following diagram displays the Q-Q plots:</p><div class="mediaobject"><img src="../images/00045.jpeg" alt="Residual analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The residuals <a id="id225" class="calibre1"/>from both models seem to lie reasonably close to the theoretical quantiles of a normal distribution, although the fit isn't perfect, as is typical with most real-world data. A second very useful diagnostic plot for <a id="id226" class="calibre1"/>a linear regression is the so-called <span class="strong"><strong class="calibre2">residual plot</strong></span>. This is a plot of residuals against corresponding fitted values for the observations in the training data. In other words, this is a plot of the pairs (<span class="strong"><em class="calibre9">i</em></span>, <span class="strong"><em class="calibre9">e<sub class="calibre14">i</sub></em></span>). There are two important properties of the residual plot that interest us in particular. Firstly, we would like to confirm our assumption of constant variance by checking whether the residuals are not larger on average for a range of fitted values and smaller in a different range. Secondly, we should verify that there isn't some sort of pattern in the residuals. If a pattern is observed, however, it may be an indication that the underlying model is nonlinear in terms of the features involved or that there are additional features missing from our model that we have not included. In fact, one way of discovering new features that might be useful for our model is to look for new features that are correlated with our model's residuals.</p><div class="mediaobject"><img src="../images/00046.jpeg" alt="Residual analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Both plots <a id="id227" class="calibre1"/>show a slight pattern of decreasing residuals in the left part of the graph. Slightly more worrying is the fact that the variance of the residuals seems to be a little higher for higher values of both output variables, which could indicate that the errors are not homoscedastic. This is more pronounced in the second plot for the cars datasets. In the preceding two residual plots, we have also labeled some of the larger residuals (in absolute magnitude). We'll see shortly that these are potential candidates for outliers. Another way to obtain a residual plot is to use the <code class="email">plot()</code> function on the model produced by the <code class="email">lm()</code> function itself. This generates four <a id="id228" class="calibre1"/>diagnostic plots, including the residual plot and the Q-Q plot.</p></div></div>

<div class="book" title="Assessing linear regression models">
<div class="book" title="Significance tests for linear regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec33" class="calibre1"/>Significance tests for linear regression</h2></div></div></div><p class="calibre8">After <a id="id229" class="calibre1"/>scrutinizing the residual summaries, the next thing we should focus on is the table of coefficients that our models have produced. Here, every estimated coefficient is accompanied by an additional set of numbers, as well as a number of stars or a dot at the end. At first, this may seem confusing because of the barrage of numbers, but there is a good reason why all this information is included. When we collect measurements on some data and specify a set of features to build a linear regression model, it is often the case that one or more of these features are not actually related to the output we are trying to predict. Of course, this is something we are generally not aware of beforehand when we are collecting data. Ideally, we would want our model to not only find the best values for the coefficients that correspond to the features that our output does actually depend on, but also to tell us which of the features we don't need.</p><p class="calibre8">One possible <a id="id230" class="calibre1"/>approach for determining whether a particular feature is needed in our model is to train two models instead of one. The second model will have all the features of the first model, excluding the specific feature whose significance we are trying to ascertain. We can then test whether the two models are different by looking at their distributions of residuals. This is actually what R does for all of the features that we have specified in each model. For each coefficient, a <span class="strong"><strong class="calibre2">confidence interval</strong></span> is constructed for the null hypothesis that its corresponding feature is unrelated to the output variable. Specifically, for each coefficient, we consider a linear model with all the other features included, except the feature that corresponds to this coefficient. Then, we test whether adding this particular feature to the model significantly changes the distribution of residual errors, which would be evidence of a linear relationship between this feature and the output and that its coefficient should be nonzero. R's <code class="email">lm()</code> function automatically runs these tests for us.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note09" class="calibre1"/>Note</h3><p class="calibre8">In statistics, a confidence interval combines a point estimate with the precision of that estimate. This is done by specifying an interval in which the true value of the parameter that is being estimated is expected to lie under a certain degree of confidence. A 95% globally confidence interval for a parameter essentially tells us that, if we were to collect 100 samples of data from the same experiment and construct a 95 percent confidence interval for the estimated parameter in each sample, the real value of the target parameter would lie within its corresponding confidence interval for 95 of these data samples. Confidence intervals that are constructed for point estimates with high variance, such as when the estimate is being made with very few data points, will tend to define a wider interval for the same degree of confidence than estimates made with low variance.</p></div><p class="calibre8">Let's look at a snapshot of the summary output for the CPU model, which shows the coefficient for the intercept and the MYCT feature in the CPU model:</p><div class="informalexample"><pre class="programlisting">              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -5.963e+01  8.861e+00  -6.730 2.43e-10 ***
MYCT         5.210e-02  1.885e-02   2.764 0.006335 ** </pre></div><p class="calibre8">Focusing on the MYCT feature for the moment, the first number in its row is the estimate of its <a id="id231" class="calibre1"/>coefficient, and this number is roughly 0.05 (<span class="strong"><em class="calibre9">5.210×10<sup class="calibre15">-2</sup></em></span>). The <span class="strong"><strong class="calibre2">standard error</strong></span> is the standard deviation of this estimate, and this <a id="id232" class="calibre1"/>is given next as 0.01885. We can gauge our confidence as to whether the value of our coefficient is really zero (indicating no linear relationship for this feature) by counting the number of standard errors between zero and our coefficient estimate. To do this, we can divide our coefficient estimate by our standard error, and this is precisely <a id="id233" class="calibre1"/>the definition of the <span class="strong"><strong class="calibre2">t-value</strong></span>, the third value in our row:</p><div class="informalexample"><pre class="programlisting">&gt; (q &lt;- 5.210e-02 / 1.885e-02)
[1] 2.763926</pre></div><p class="calibre8">So, our MYCT coefficient is almost three standard errors away from zero, which is a fairly good indicator that this coefficient is not likely to be zero. The higher the t-value, the more likely we should be including our feature in our linear model with a nonzero coefficient. We can turn this absolute value into a probability that tells us how likely it is that our coefficient should really be zero. This probability is obtained from Student's t-distribution <a id="id234" class="calibre1"/>and is known as the <span class="strong"><strong class="calibre2">p-value</strong></span>. For the MYCT feature, this probability is 0.006335, which is small. We can obtain this value for ourselves using the <code class="email">pt()</code> function:</p><div class="informalexample"><pre class="programlisting">&gt; pt(q, df = 172, lower.tail = F) * 2
[1] 0.006333496</pre></div><p class="calibre8">The <code class="email">pt()</code> function is the distribution function for the t-distribution, which is symmetric. To understand why our p-value is computed this way, note that we are interested in the probability of the absolute value of the t-value being larger than the value we computed. To obtain this, we first obtain the probability of the upper or right tail of the t-distribution and multiply this by 2 in order to include the lower tail as well. Working with basic distribution functions is a very important skill in R, and we have included examples in our online tutorial chapter on R if this example seems overly difficult. The t-distribution is parameterized by the degrees of freedom.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note10" class="calibre1"/>Note</h3><p class="calibre8">The number of degrees of freedom is essentially the number of variables that we can freely change when calculating a particular statistic, such as a coefficient estimate. In our linear regression context, this amounts to the number of observations in our training data minus the number of parameters in the model (the number of regression coefficients). For our CPU model, this number is <span class="strong"><em class="calibre9">179 – 7 = 172</em></span>. For the cars model where we have more data points, this number is 664. The name comes from its relation to the number of independent dimensions or pieces of information that are applied as input to a system, and hence reflects the extent to which the system can be freely configured without violating any constraints on the input.</p></div><p class="calibre8">As a general rule of thumb, we would like our p-values to be less than 0.05, which is the same as saying that we would like to have 95 percent confidence intervals for our coefficient estimates that do not include zero. The number of stars next to each coefficient provides us <a id="id235" class="calibre1"/>with a quick visual aid for what the confidence level is, and a single star corresponds to our 95 percent rule of thumb while two stars represent a 99 percent confidence interval. Consequently, every coefficient in our model summary that does not have any stars corresponds to a feature that, we are not confident we should include in our model using our rule of thumb. In the CPU model, the CHMIN feature is the only feature that is suspect, with the other p-values being very small. The situation is different with the cars model. Here, we have four features that are suspect as well as the intercept.</p><p class="calibre8">It is important to properly understand the interpretation of p-values in the context of our linear regression model. Firstly, we cannot and should not compare p-values against each other in order to gauge which feature is the most important. Secondly, a high p-value does not necessarily indicate that there is no linear relationship between a feature and the output; it only suggests that in the presence of all the other model features, this feature does not provide any new information about the output variable. Finally, we should always remember that the 95 percent rule of thumb is not infallible and is only really useful when the number of features and hence coefficients is not very large. Under 95 percent confidence, if we have 1,000 features in our model, we can expect to get the wrong result for 50 coefficients on average. Consequently, linear regression coefficient significance tests aren't as useful for problems in high dimensions.</p><p class="calibre8">The final <a id="id236" class="calibre1"/>test of significance actually appears at the very bottom of the summary of the <code class="email">lm()</code> output and is on the last line. This line provides us with the <span class="strong"><strong class="calibre2">F statistic</strong></span>, which gets its name from the F test, which checks whether there is a statistical significance between the variances of two (ideally normal) distributions. The F statistic in this case tries to assess whether the variance of the residuals from a model in which all coefficients are zero is significantly different from the variance of the residuals from our trained model.</p><p class="calibre8">Put differently, the F test will tell us whether the trained model explains some of the variance in the output, and hence we know that at least one of the coefficients must be nonzero. While not as useful when we have many coefficients, this tests the significance of coefficients together and doesn't suffer from the same problem as the t-test on the individual coefficients. The summary shows a tiny p-value for this, so we know that at least one of <a id="id237" class="calibre1"/>our coefficients is nonzero. We can reproduce the F test that was run using the <code class="email">anova()</code> function, which stands for <span class="strong"><strong class="calibre2">analysis of variance</strong></span>. This test compares the <span class="strong"><strong class="calibre2">null model</strong></span>, which is the model built with just an intercept <a id="id238" class="calibre1"/>and none of the features, with our trained model. We'll show <a id="id239" class="calibre1"/>this here for the CPU dataset:</p><div class="informalexample"><pre class="programlisting">&gt; machine_model_null &lt;- lm(PRP ~ 1, data = machine_train)
&gt; anova(machine_model_null, machine_model1)
Analysis of Variance Table

Model 1: PRP ~ 1
Model 2: PRP ~ MYCT + MMIN + MMAX + CACH + CHMIN + CHMAX
  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    
1    178 5130399                                  
2    172  646479  6   4483919 198.83 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</pre></div><p class="calibre8">Note that the formula of the null model is <code class="email">PRP ~ 1</code>, where the 1 represents the intercept.</p></div></div>

<div class="book" title="Assessing linear regression models">
<div class="book" title="Performance metrics for linear regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec34" class="calibre1"/>Performance metrics for linear regression</h2></div></div></div><p class="calibre8">The final <a id="id240" class="calibre1"/>details in our summary are concerned with the performance of the model as a whole and the degree to which the linear model fits the data. To understand how we assess a linear regression fit, we should first point out that the training criterion of the linear regression model is to minimize the MSE on the data. In other words, fitting a linear model to a set of data points amounts to finding a <a id="id241" class="calibre1"/>line whose slope and position minimize the sum (or average) of the squared distances from these points. As we refer to the error between a data point and its predicted value on the line as the residual, we can define the <span class="strong"><strong class="calibre2">Residual Sum of Squares</strong></span> (<span class="strong"><strong class="calibre2">RSS</strong></span>) as the sum of all the squared residuals:</p><div class="mediaobject"><img src="../images/00047.jpeg" alt="Performance metrics for linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In other <a id="id242" class="calibre1"/>words, RSS is just the <span class="strong"><strong class="calibre2">Sum of Squared Errors</strong></span> (<span class="strong"><strong class="calibre2">SSE</strong></span>), so we can relate to the MSE with which we are familiar via this simple equation:</p><div class="mediaobject"><img src="../images/00048.jpeg" alt="Performance metrics for linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Beyond certain historic reasons, RSS is an important metric to be aware of because it is related to another important metric, known as the RSE, which we will talk about next. For this, we'll need to first build up an intuition about what happens when we train linear regression models. If we run our simple linear regression experiment with artificial data a number of times, each time changing the random seed so that we get a different random sample, we'll see that we will get a number of regression lines that are likely to be very close to <a id="id243" class="calibre1"/>the true population line, just as our single run showed us. This illustrates the fact that linear models are characterized by low variance in general. Of course, the unknown function we are trying to approximate may very well be nonlinear and as a result, even the population regression line is not likely to be a good fit to the data for nonlinear functions. This is because the linearity assumption is very strict, and consequently, linear regression is a method with high bias.</p><p class="calibre8">We define a <a id="id244" class="calibre1"/>metric known as the <span class="strong"><strong class="calibre2">Residual Standard Error</strong></span> (<span class="strong"><strong class="calibre2">RSE</strong></span>), which estimates the standard deviation of our model compared to the target function. That is to say, it measures roughly how far away from the population regression line on average our model will be. This is measured in the units of the output variable and is an absolute value. Consequently, it needs to be compared against the values of <span class="strong"><em class="calibre9">y</em></span> in order to gauge whether it is high or not for a particular sample. The general RSE for a model with <span class="strong"><em class="calibre9">k</em></span> input features is computed as follows:</p><div class="mediaobject"><img src="../images/00049.jpeg" alt="Performance metrics for linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">For simple linear regression, this is just with <span class="strong"><em class="calibre9">k = 1</em></span>:</p><div class="mediaobject"><img src="../images/00050.jpeg" alt="Performance metrics for linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We can compute the RSE for our two models using the preceding formula, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; n_machine &lt;- nrow(machine_train)
&gt; k_machine &lt;- length(machine_model1$coefficients) - 1
&gt; sqrt(sum(machine_model1$residuals ^ 2) / (n_machine - k_machine - 1))
[1] 61.30743

&gt; n_cars &lt;- nrow(cars_train)
&gt; k_cars &lt;- length(cars_model2$coefficients) - 1
&gt; sqrt(sum(cars_model2$residuals ^ 2) / (n_cars - k_cars - 1))
[1] 2946.98</pre></div><p class="calibre8">To interpret the RSE values for our two models, we need to compare them with the mean of our output variables:</p><div class="informalexample"><pre class="programlisting">&gt; mean(machine_train$PRP)
[1] 109.4804
&gt; mean(cars_train$Price)
[1] 21320.2</pre></div><p class="calibre8">Note that, in the car model, the RSE of 61.3 is quite small compared to the RSE of the cars model, which is roughly 2,947. When we look at these numbers in terms of how close they are to the means of their respective output variables, however, we learn that actually <a id="id245" class="calibre1"/>it is the cars model RSE that shows a better fit.</p><p class="calibre8">Now, although the RSE is useful as an absolute value in that one can compare it to the mean of the output variable, we often want a relative value that we can use to compare across <a id="id246" class="calibre1"/>different training scenarios. To this end, when evaluating the fit of linear regression models, we often also look at the <span class="strong"><strong class="calibre2">R2 statistic</strong></span>. In the summary, this is denoted as multiple R-squared. Before we provide the equation, we'll first present the notion of the <span class="strong"><strong class="calibre2">Total Sum of Squares</strong></span> (<span class="strong"><strong class="calibre2">TSS</strong></span>). The total sum of squares is proportional to the total variance in <a id="id247" class="calibre1"/>the output variable, and is designed to measure the amount of variability intrinsic to this variable before we perform our regression. The formula for TSS is:</p><div class="mediaobject"><img src="../images/00051.jpeg" alt="Performance metrics for linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The idea behind the R<sup class="calibre15">2</sup> statistic is that if a linear regression model is a close fit to the true population model, it should be able to completely capture all the variance in the output. In fact, we often refer to the R<sup class="calibre15">2</sup> statistic as the relative amount that shows us what proportion of the output variance is explained by the regression. When we apply our regression model to obtain an estimate of the output variable, we see that the errors in our observations are called residuals and the RSS is essentially proportional to the variance that is left between our prediction and the true values of the output function. Consequently, we can define the R<sup class="calibre15">2</sup> statistic, which is the amount of variance in our output <span class="strong"><em class="calibre9">y</em></span> that our linear regression model explains, as the difference between our starting variance (TSS) and our ending variance (RSS) relative to our starting variance (TSS). As a formula, this is just:</p><div class="mediaobject"><img src="../images/00052.jpeg" alt="Performance metrics for linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">From this equation, we can see that R<sup class="calibre15">2</sup> ranges between 0 and 1. A value close to 1 is indicative of a good fit as it means that most of the variance in the output variable has been explained <a id="id248" class="calibre1"/>by the regression model. A low value, on the other hand, indicates that there is still significant variance in the errors in the model, indicating that our model is not a good fit. Let's see how the R2 statistic can be computed manually for our two models:</p><div class="informalexample"><pre class="programlisting">compute_rsquared &lt;- function(x, y) {
     rss &lt;- sum((x - y) ^ 2)
     tss &lt;- sum((y - mean(y)) ^ 2)
     return(1 - (rss / tss))
 }
 
&gt; compute_rsquared(machine_model1$fitted.values, machine_train$PRP)
[1] 0.8739904
&gt; compute_rsquared(cars_model2$fitted.values, cars_train$Price)
[1] 0.9119826</pre></div><p class="calibre8">We used the <code class="email">fitted.values</code> attribute of the model trained by <code class="email">lm()</code>, which is the predictions the model makes on the training data. Both values are quite high, with the cars model again indicating a slightly better fit. We've now seen two important metrics to assess a linear regression model, namely RSE and the R<sup class="calibre15">2</sup> statistic. At this point, we might consider <a id="id249" class="calibre1"/>whether there is a more general measure of the linear relationship between two variables that we could also apply to our case. From statistics, we might recall that the notion of correlation describes exactly that.</p><p class="calibre8">The <span class="strong"><strong class="calibre2">correlation</strong></span> between two random variables, <span class="strong"><em class="calibre9">X</em></span> and <span class="strong"><em class="calibre9">Y</em></span>, is given by:</p><div class="mediaobject"><img src="../images/00053.jpeg" alt="Performance metrics for linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">It turns out that in the case of simple regression, the square of the correlation between the output <a id="id250" class="calibre1"/>variable and the input feature is the same as the R2 statistic, a result that further bolsters the importance of the latter as a useful metric.</p></div></div>

<div class="book" title="Assessing linear regression models">
<div class="book" title="Comparing different regression models"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch03lvl2sec35" class="calibre1"/>Comparing different regression models</h2></div></div></div><p class="calibre8">When we <a id="id251" class="calibre1"/>want to compare two different regression models that have been trained on the same set of input features, the R<sup class="calibre15">2</sup> statistic can be very useful. Often, however, we want to compare two models that don't have the same number of input features. For example, during the process of feature selection, we may want to know whether including a particular feature in our model is a good idea. One of the limitations of the R<sup class="calibre15">2</sup> statistic is that it tends to be higher for models with more input parameters.</p><p class="calibre8">The <span class="strong"><strong class="calibre2">adjusted R<sup class="calibre15">2</sup></strong></span>
<a id="id252" class="calibre1"/> attempts to correct the fact that R<sup class="calibre15">2</sup> always tends to be higher for models with more input features and hence is susceptible to overfitting. The adjusted R<sup class="calibre15">2</sup> is generally lower than R<sup class="calibre15">2</sup> itself, as we can verify by checking the values in our model summaries. The formula for the adjusted R<sup class="calibre15">2</sup> is:</p><div class="mediaobject"><img src="../images/00054.jpeg" alt="Comparing different regression models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The <a id="id253" class="calibre1"/>definitions of <span class="strong"><em class="calibre9">n</em></span> and <span class="strong"><em class="calibre9">k</em></span> are the same as those <a id="id254" class="calibre1"/>for the R<sup class="calibre15">2</sup> statistic. Now, let's implement <a id="id255" class="calibre1"/>this function in R and compute the adjusted R<sup class="calibre15">2</sup> for our two models:</p><div class="informalexample"><pre class="programlisting">compute_adjusted_rsquared &lt;- function(x, y, k) {
     n &lt;- length(y)
     r2 &lt;- compute_rsquared(x, y)
     return(1 - ((1 - r2) * (n - 1) / (n - k - 1)))
 }

&gt; compute_adjusted_rsquared(machine_model1$fitted.values, 
                            machine_train$PRP, k_machine)
[1] 0.8695947
&gt; compute_adjusted_rsquared(cars_model2$fitted.values, 
                            cars_train$Price, k_cars)
[1] 0.9101407</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note11" class="calibre1"/>Note</h3><p class="calibre8">There are several other commonly used performance metrics designed to compare models with a different number of features. The <span class="strong"><strong class="calibre2">Akaike Information Criterion</strong></span> (<span class="strong"><strong class="calibre2">AIC</strong></span>) uses an information theoretic approach to assess the relative quality of a model by balancing model complexity and accuracy. For linear regression models trained by minimizing the squared error, this is proportional to another well-known statistic, <span class="strong"><strong class="calibre2">Mallow's Cp</strong></span>, so these can be used interchangeably. A third metric is the <span class="strong"><strong class="calibre2">Bayesian Information Criterion</strong></span> (<span class="strong"><strong class="calibre2">BIC</strong></span>). This tends to penalize models with more variables more heavily, compared to the previous metrics.</p></div></div></div>

<div class="book" title="Assessing linear regression models">
<div class="book" title="Test set performance"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch03lvl2sec36" class="calibre1"/>Test set performance</h2></div></div></div><p class="calibre8">So far, we've looked at the performance of our models in terms of the training data. This is important <a id="id256" class="calibre1"/>in order to gauge whether a linear model can fit the data well, but doesn't give us a good sense of predictive accuracy over unseen data. For this, we turn to our test datasets. To use our model to make predictions, we can use the <code class="email">predict()</code> function. This is a general function in R that many packages extend. With models trained with <code class="email">lm()</code>, we simply need to provide the model and a data frame with the observations that we want to predict:</p><div class="informalexample"><pre class="programlisting">&gt; machine_model1_predictions &lt;- predict(machine_model1, 
                                        machine_test)
&gt; cars_model2_predictions &lt;- predict(cars_model2, cars_test)</pre></div><p class="calibre8">Next, we'll define our own function for computing the MSE:</p><div class="informalexample"><pre class="programlisting">compute_mse &lt;- function(predictions, actual) { 
     mean( (predictions - actual) ^ 2 ) 
}
&gt; compute_mse(machine_model1$fitted.values, machine_train$PRP)
[1] 3611.616
&gt; compute_mse(machine_model1_predictions, machine_test$PRP)
[1] 2814.048
&gt; 
&gt; compute_mse(cars_model2$fitted.values, cars_train$Price)
[1] 8494240
&gt; compute_mse(cars_model2_predictions, cars_test$Price)
[1] 7180150</pre></div><p class="calibre8">For each model, we've used our <code class="email">compute_mse()</code> function to return the training and test MSE. It happens that in this case both test MSE values are smaller than the train MSE values. Whether the test MSE is slightly larger or smaller than the train MSE is not particularly important. The important issue is that the test MSE is not significantly larger than the train MSE as this would indicate that our model is overfitting the data. Note that, especially for the CPU model, the number of observations in the original data set is very small and this has resulted in a test set size that is also very small. Consequently, we should <a id="id257" class="calibre1"/>be conservative with our confidence in the accuracy of these estimates for the predictive performance of our models on unseen data, because predictions made using a small test set size will have a higher variance.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Problems with linear regression" id="SJGS1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec23" class="calibre1"/>Problems with linear regression</h1></div></div></div><p class="calibre8">In this <a id="id258" class="calibre1"/>chapter, we've already seen some examples where trying to build a linear regression model might run into problems. One big class of problems that we've talked about is related to our model assumptions of linearity, feature independence, and the homoscedasticity and normality of errors. In particular we saw methods of diagnosing these problems either via plots, such as the residual plot, or by using functions that identify dependent components. In this section, we'll investigate a few more issues that can arise with linear regression.</p></div>

<div class="book" title="Problems with linear regression" id="SJGS1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Multicollinearity"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec37" class="calibre1"/>Multicollinearity</h2></div></div></div><p class="calibre8">As part of <a id="id259" class="calibre1"/>our preprocessing steps, we were diligent in removing features that were linearly related to each other. In doing this, we were looking <a id="id260" class="calibre1"/>for an exact linear relationship and this is an example of <span class="strong"><strong class="calibre2">perfect collinearity</strong></span>. <span class="strong"><strong class="calibre2">Collinearity</strong></span> is the property that describes when two features are approximately in a linear relationship. This creates a problem for linear regression as we are trying <a id="id261" class="calibre1"/>to assign separate coefficients to variables that are almost linear functions of each other. This can result in a situation where the coefficients of two highly <a id="id262" class="calibre1"/>collinear features have high p-values that indicate they are not related to the output variable, but if we remove one of these and retrain a model, the one <a id="id263" class="calibre1"/>left in has a low p-value. Another classic indication of collinearity is an unusual sign on one of the coefficients; for example, a negative coefficient on educational background for a linear model that predicts income. Collinearity between two features can be detected through pairwise correlation. One way to deal with collinearity is to combine two features into a new one (for example, by averaging); another is by simply discarding one of the features.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Multicollinearity</strong></span> occurs when the linear relationship involves more than two features. A standard method for detecting this is to calculate the <span class="strong"><strong class="calibre2">variance inflation factor</strong></span> (<span class="strong"><strong class="calibre2">VIF</strong></span>) for every input <a id="id264" class="calibre1"/>feature in a linear model. In a nutshell, the VIF tries to estimate the increase in variance that is observed in the estimate of a particular coefficient that is a direct result of that feature being collinear with other features. This is typically done by fitting a linear regression model in which we treat one of the features as the output feature and the remaining features as regular input features. We then compute the R2 statistic for this linear model and from this, the VIF for our chosen feature using the formula <span class="strong"><em class="calibre9">1 / (1 – R<sup class="calibre15">2</sup>)</em></span>. In R, the <code class="email">car</code> package contains the <code class="email">vif()</code> function, which <a id="id265" class="calibre1"/>conveniently calculates the VIF value for every feature in a linear regression model. A rule of thumb here is that a VIF score of 4 or more for a feature is suspect, and a score in excess of 10 indicates a strong likelihood of multicollinearity. Since we saw that our cars data had linearly dependent features that we had to remove, let's investigate whether we have multicollinearity in those that remain:</p><div class="informalexample"><pre class="programlisting">&gt; library("car")
&gt; vif(cars_model2)
    Mileage    Cylinder       Doors      Cruise       Sound 
   1.010779    2.305737    4.663813    1.527898    1.137607 
    Leather       Buick    Cadillac       Chevy     Pontiac 
   1.205977    2.464238    3.158473    4.138318    3.201605 
       Saab convertible   hatchback       sedan 
   3.515018    1.620590    2.481131    4.550556</pre></div><p class="calibre8">We see three values here that are slightly above <code class="email">4</code> but no values above that. As an example, the following code shows how the VIF value for <code class="email">sedan</code> was calculated:</p><div class="informalexample"><pre class="programlisting">&gt; sedan_model &lt;- lm(sedan ~ .-Price -Saturn, data = cars_train)
&gt; sedan_r2 &lt;- compute_rsquared(sedan_model$fitted.values, cars_train$sedan)
&gt; 1 / (1-sedan_r2)
[1] 4.550556</pre></div></div></div>

<div class="book" title="Problems with linear regression" id="SJGS1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Outliers"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec38" class="calibre1"/>Outliers</h2></div></div></div><p class="calibre8">When we <a id="id266" class="calibre1"/>looked at the residuals of our two models, we saw that there were certain observations that had a significantly higher residual than others. For <a id="id267" class="calibre1"/>example, referring to the residual plot for the CPU model, we can see that the observation 200 has a very high residual. This is an example of an <span class="strong"><strong class="calibre2">outlier</strong></span>, an observation whose predicted value is very far from its actual value. Due to the squaring of residuals, outliers tend to have a significant impact on the RSS, giving us a sense that we don't have a good model fit. Outliers can occur due to measurement errors and detecting them may be important, as they may denote data that is inaccurate or invalid. </p><p class="calibre8">On the other hand, outliers may simply be the result of not having the right features or building the wrong kind of model.</p><p class="calibre8">As we generally won't know whether an outlier is an error or a genuine observation during data collection, handling outliers can be very tricky. Sometimes, especially when we have very few outliers, a common recourse is to remove them, because including them frequently has the effect of changing the predicted model coefficients significantly. We say that <a id="id268" class="calibre1"/>outliers are often points with high <span class="strong"><strong class="calibre2">influence</strong></span>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note12" class="calibre1"/>Note</h3><p class="calibre8">Outliers are not the only observations that can have high influence. <span class="strong"><strong class="calibre2">High leverage points</strong></span> are observations that have an extreme value for at least one of their features and thus, lie far away from most other observations. <span class="strong"><strong class="calibre2">Cook's distance</strong></span> is a typical metric that combines the notions of outlier and high leverage to identify points that have high influence on the data. For a more thorough exploration of linear regression diagnostics, a wonderful reference is <span class="strong"><em class="calibre9">An R Companion to Applied Regression</em></span>, <span class="strong"><em class="calibre9">John Fox</em></span>, <span class="strong"><em class="calibre9">Sage Publications</em></span>.</p></div><p class="calibre8">To illustrate <a id="id269" class="calibre1"/>the effect of removing an outlier, we will create a new CPU model by using our training data without observation number 200. Then, we will see <a id="id270" class="calibre1"/>whether our model has an improved fit on the training data. Here, we've shown the steps taken and a truncated model summary with only the final three lines:</p><div class="informalexample"><pre class="programlisting">&gt; machine_model2 &lt;- lm(PRP ~ ., data = machine_ train[!(rownames(machine_train)) %in% c(200),])
&gt; summary(machine_model2)
...
Residual standard error: 51.37 on 171 degrees of freedom
Multiple R-squared:  0.8884,	Adjusted R-squared:  0.8844
F-statistic: 226.8 on 6 and 171 DF,  p-value: &lt; 2.2e-16</pre></div><p class="calibre8">As we <a id="id271" class="calibre1"/>can see from the reduced RSE and improved R2, we have a better fit on our training data. Of course, the real measure of model accuracy is the performance on the test data, and there are no guarantees that our decision to label observation 200 as a spurious outlier was the right one.</p><div class="informalexample"><pre class="programlisting">&gt; machine_model2_predictions &lt;- predict(machine_model2, 
                                        machine_test)
&gt; compute_mse(machine_model2_predictions, machine_test$PRP)
[1] 2555.355</pre></div><p class="calibre8">We have a lower test MSE than before, which is usually a good sign that we made the right choice. Again, because we have a small test set, we cannot be certain of this fact despite the positive indication from the MSE.</p></div></div>
<div class="book" title="Feature selection"><div class="book" id="TI1E2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec24" class="calibre1"/>Feature selection</h1></div></div></div><p class="calibre8">Our CPU model <a id="id272" class="calibre1"/>only came with six features. Often, we encounter real-world datasets that have a very large number of features arising from a diverse array of measurements. Alternatively, we may have to come up with a large number of features when we aren't really sure what features will be important in influencing our output variable. Moreover, we might have categorical variables with many possible levels from which we are forced to create a large number of new indicator variables, as we saw in <a class="calibre1" title="Chapter 1. Gearing Up for Predictive Modeling" href="part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7">Chapter 1</a>, <span class="strong"><em class="calibre9">Gearing Up for Predictive Modeling</em></span>. When our scenario involves a large number of features, we often find that our output only depends on a subset of these. Given <span class="strong"><em class="calibre9">k</em></span> input features, there are <span class="strong"><em class="calibre9">2<sup class="calibre15">k</sup></em></span> distinct subsets that we can form, so for even a moderate number <a id="id273" class="calibre1"/>of features the space of subsets is too large for us to fully explore by fitting a model on each subset.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip10" class="calibre1"/>Tip</h3><p class="calibre8">One easy way to understand why there are <span class="strong"><em class="calibre9">2<sup class="calibre15">k</sup></em></span> possible feature subsets is this: we can assign a unique identifying code to every subset as a string of binary digits of length <span class="strong"><em class="calibre9">k</em></span>, where the digit at a certain position <span class="strong"><em class="calibre9">i</em></span> is 1 if we chose to include the <span class="strong"><em class="calibre9">i<sup class="calibre15">th</sup></em></span> feature (features can be ordered arbitrarily) in the subset. For example, if we have three features, the string 101 corresponds to the subset that only includes the first and third features. In this way, we have formed all possible binary strings from a string of <span class="strong"><em class="calibre9">k</em></span> zeros to a string of <span class="strong"><em class="calibre9">k</em></span> ones; thus we have all the numbers from 0 to <span class="strong"><em class="calibre9">2<sup class="calibre15">k-1</sup></em></span> and <span class="strong"><em class="calibre9">2<sup class="calibre15">k</sup></em></span> total subsets.</p></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Feature selection</strong></span> refers to the process by which a subset of features in a model is chosen in order to form a new model with fewer features. This removes features that we deem unrelated to the output variable and consequently results in a simpler model, which is easier to <a id="id274" class="calibre1"/>train as well as interpret. There are a number of methods designed to do this, and they generally do not involve exhaustively searching the space of <a id="id275" class="calibre1"/>possible subsets, but perform a guided search through this space instead.</p><p class="calibre8">One such method is <span class="strong"><strong class="calibre2">forward selection</strong></span>, which is an example of <span class="strong"><strong class="calibre2">stepwise regression</strong></span> that performs feature selection in a series of steps. With forward selection, the idea is to start out with an empty model that has no features selected. We then perform <span class="strong"><em class="calibre9">k</em></span> simple linear regressions (one for every feature that we have) and pick the best one. Here, we are comparing models that have the same number of features so that we can use the R<sup class="calibre15">2</sup> statistic to guide our choice, although we can use metrics such as AIC as well. Once we have chosen our first feature to add, we then pick another feature to add from the remaining <span class="strong"><em class="calibre9">k-1</em></span> features. Therefore, we now run <span class="strong"><em class="calibre9">k-1</em></span> multiple regressions for every possible pair of features, where one of the features in the pair is the feature that we picked in the first step. We continue adding in features like this until we have evaluated the model with all the features included and stop. Note that, in every step, we make a hard choice about which feature to include for all future steps. </p><p class="calibre8">For example, models that have more than one feature in them and do not include the feature we chose in the first step of this process are never considered. Therefore, we do not exhaustively search our space. In fact, if we take into account that we also assess the null model, we can compute the total number of models we perform a linear regression on as follows:</p><div class="mediaobject"><img src="../images/00055.jpeg" alt="Feature selection" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The order of magnitude of this computation is on the scale of <span class="strong"><em class="calibre9">k<sup class="calibre15">2</sup></em></span>, which for even small values of <span class="strong"><em class="calibre9">k</em></span> is already considerably less than <span class="strong"><em class="calibre9">2<sup class="calibre15">k</sup></em></span>. At the end of the forward selection process, we have to choose between <span class="strong"><em class="calibre9">k+1</em></span> models, corresponding to the subsets we obtained at the <a id="id276" class="calibre1"/>end of every step of the process. As the final part of the process involves comparing models with different numbers of features, we usually use a criterion such as the AIC or the adjusted R2 to make our final choice of model. We can demonstrate this process for our CPU dataset by running the following commands:</p><div class="informalexample"><pre class="programlisting">&gt; machine_model3 &lt;- step(machine_model_null, scope = list(lower = machine_model_null, upper = machine_model1), direction = "forward")</pre></div><p class="calibre8">The <code class="email">step()</code> function implements the process of forward selection. We first provide it with the null model obtained by fitting a linear model with no features on our training data. For the <code class="email">scope</code> parameter, we specify that we want our algorithm to step through from the null model all the way to our full model consisting of all six features. The effect of issuing these commands in R is an output that demonstrates which feature subset is specified at every step of the iteration. To conserve space, we present the results in the following table, along with the value of the AIC for each model. Note that the lower the AIC value, the better the model.</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Step</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Features in subset</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">AIC value</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">{}</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">1839.13</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">1</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">{MMAX}</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">1583.38</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">2</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">{MMAX, CACH}</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">1547.21</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">3</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">{MMAX, CACH, MMIN}</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">1522.06</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">4</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">{MMAX, CACH, MMIN, CHMAX}</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">1484.14</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">5</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">{MMAX, CACH, MMIN, CHMAX, MYCT}</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">1478.36</p>
</td></tr></tbody></table></div><p class="calibre8">The <code class="email">step()</code> function uses an alternative specification for forward selection, which is to terminate when there is no feature from those remaining that can be added to the current feature subset and would improve our score. For our dataset, only one feature was left out from the final model, as adding it did not improve the overall score. It is interesting and somewhat reassuring that this feature was CHMIN, which was the only variable whose relatively high p-value indicated that we weren't confident that our output variable is related to this feature in the presence of the other features.</p><p class="calibre8">One might <a id="id277" class="calibre1"/>wonder whether we could perform variable selection in the opposite direction by starting off with a full model and removing features one by one <a id="id278" class="calibre1"/>based on which feature, when removed, will make the biggest improvement in the model score. This is indeed possible, and the process is known either as <span class="strong"><strong class="calibre2">backward selection</strong></span> or <span class="strong"><strong class="calibre2">backward elimination</strong></span>. This can be done in R with the <code class="email">step()</code> function by specifying <code class="email">backward</code> as the direction and starting from the full model. We'll show this on our cars dataset and save the result into a new cars model:</p><div class="informalexample"><pre class="programlisting">&gt; cars_model_null &lt;- lm(Price ~ 1, data = cars_train)
&gt; cars_model3 &lt;- step(cars_model2, scope = list( 
  lower=cars_model_null, upper=cars_model2), direction = "backward")</pre></div><p class="calibre8">The formula for the final linear regression model on the cars dataset is:</p><div class="informalexample"><pre class="programlisting">Call:
lm(formula = Price ~ Mileage + Cylinder + Doors + Leather + Buick + Cadillac + Pontiac + Saab + convertible + hatchback + sedan,
    data = cars_train)</pre></div><p class="calibre8">As we can see, the final model has thrown away the <code class="email">Cruise</code>, <code class="email">Sound</code>, and <code class="email">Chevy</code> features. Looking at <a id="id279" class="calibre1"/>our previous model summary, we can see that these three features had high p-values. The previous two approaches are examples of a <span class="strong"><strong class="calibre2">greedy algorithm</strong></span>. This is <a id="id280" class="calibre1"/>to say that, once a choice about whether to include a variable has been made, it becomes final and cannot be undone later. To remedy this, a third <a id="id281" class="calibre1"/>method of variable selection known as <span class="strong"><strong class="calibre2">mixed selection</strong></span> or <span class="strong"><strong class="calibre2">bidirectional elimination</strong></span> starts as forward selection with forward steps to add variables, but also includes backward steps when these can improve the AIC. Predictably, the <code class="email">step()</code> function does this when the <code class="email">direction</code> is specified as <code class="email">both</code>.</p><p class="calibre8">Now that we have two new models, we can see how they perform on the test sets:</p><div class="informalexample"><pre class="programlisting">&gt; machine_model3_predictions &lt;- predict(machine_model3, machine_test)
&gt; compute_mse(machine_model3_predictions, machine_test$PRP)
[1] 2805.762
&gt; 
&gt; cars_model3_predictions &lt;- predict(cars_model3, cars_test)
&gt; compute_mse(cars_model3_predictions, cars_test$Price)
[1] 7262383</pre></div><p class="calibre8">For the CPU model, we perform marginally better on the test set than our original model. A suitable next step might be to investigate whether this reduced set of features works better in combination with the removal of our outlier; this is left as an exercise for the reader. In contrast, for the cars model, we see that the test MSE has increased slightly as a result <a id="id282" class="calibre1"/>of removing all these features.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Regularization"><div class="book" id="UGI02-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec25" class="calibre1"/>Regularization</h1></div></div></div><p class="calibre8">Variable <a id="id283" class="calibre1"/>selection is an important process, as it tries to make models simpler to interpret, easier to train, and free of spurious associations by eliminating variables unrelated to the output. This is one possible approach to dealing with the problem of overfitting. In general, we don't expect a model to completely fit our training data; in fact, the problem of overfitting often means that it may be detrimental to our predictive model's accuracy on unseen data if we fit our training data too well. In this section on <span class="strong"><strong class="calibre2">regularization</strong></span>, we'll study an alternative to reducing the number of variables in order to deal with overfitting. Regularization is essentially the process of introducing an intentional <a id="id284" class="calibre1"/>bias or constraint in our training procedure that prevents our coefficients from taking large values. As this is a process that tries to shrink the coefficients, the methods we'll look at are also known as <span class="strong"><strong class="calibre2">shrinkage methods</strong></span>.</p></div>

<div class="book" title="Regularization">
<div class="book" title="Ridge regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec39" class="calibre1"/>Ridge regression</h2></div></div></div><p class="calibre8">When the <a id="id285" class="calibre1"/>number of parameters is very large, particularly compared to the number of available observations, linear regression tends to exhibit <a id="id286" class="calibre1"/>very high variance. This is to say that small changes in a few of the observations will cause the coefficients to change substantially. <span class="strong"><strong class="calibre2">Ridge regression</strong></span> is a method that introduces bias through its constraint but is effective at reducing the model's variance. Ridge regression tries to minimize the sum of the residual sum of squares and uses a term that involves the sum of the squares of the coefficients multiplied by a constant for which we'll use the Greek letter <span class="strong"><em class="calibre9">λ</em></span>. For a model with <span class="strong"><em class="calibre9">k</em></span> parameters, not counting the constant term <span class="strong"><em class="calibre9">β<sub class="calibre14">0</sub></em></span>, and a dataset with <span class="strong"><em class="calibre9">n</em></span> observations, ridge regression minimizes the following quantity:</p><div class="mediaobject"><img src="../images/00056.jpeg" alt="Ridge regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We are <a id="id287" class="calibre1"/>still minimizing the RSS but the second term is the penalty term, which is high when any of the coefficients is high. Thus, when minimizing, we are effectively pushing the coefficients to smaller values. The <span class="strong"><em class="calibre9">λ</em></span> parameter is known as a <span class="strong"><strong class="calibre2">meta parameter</strong></span>, which we need to select or tune. A very large value of <span class="strong"><em class="calibre9">λ</em></span> will mask the RSS term and just push the coefficients to zero. An overly small value of <span class="strong"><em class="calibre9">λ</em></span> will not be as effective against overfitting and a <span class="strong"><em class="calibre9">λ</em></span> parameter of 0 just performs regular linear regression.</p><p class="calibre8">When <a id="id288" class="calibre1"/>performing ridge regression, we often want to scale by dividing the values of all our features by their variance. This was not the case with regular linear regression because, if one feature is scaled by a factor of 10, then the coefficient will simply be scaled by a factor of a tenth to compensate. With ridge regression, the scale of a feature affects the computation of all other features through the penalty term.</p></div></div>

<div class="book" title="Regularization">
<div class="book" title="Least absolute shrinkage and selection operator (lasso)"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec40" class="calibre1"/>Least absolute shrinkage and selection operator (lasso)</h2></div></div></div><p class="calibre8">The <span class="strong"><strong class="calibre2">lasso</strong></span> is <a id="id289" class="calibre1"/>an alternative regularization method to ridge regression. The difference <a id="id290" class="calibre1"/>appears only in the penalty term, which involves minimizing the sum of the absolute values of the coefficients.</p><div class="mediaobject"><img src="../images/00057.jpeg" alt="Least absolute shrinkage and selection operator (lasso)" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">It turns <a id="id291" class="calibre1"/>out that this difference in the penalty term is very significant, as the lasso combines both shrinkage and selection because it shrinks some coefficients to exactly zero, which is not the case with ridge regression. Despite this, there is no clear winner between these two. Models that depend on a subset of the input <a id="id292" class="calibre1"/>features will tend to perform better with lasso; models that have a large <a id="id293" class="calibre1"/>spread in coefficients across many different variables will tend to perform better with ridge regression. It is usually worth trying both.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note13" class="calibre1"/>Note</h3><p class="calibre8">The penalty in ridge regression is often referred to as an <span class="strong"><em class="calibre9">l<sub class="calibre14">2</sub></em></span> penalty, whereas the penalty term in lasso is known as an <span class="strong"><em class="calibre9">l<sub class="calibre14">1</sub></em></span> penalty. This arises from the mathematical notion of a <span class="strong"><strong class="calibre2">norm</strong></span> of a vector. A norm of a vector is a function that assigns a positive number to that vector to represent its length or size. There are many different types of norms. Both the <span class="strong"><em class="calibre9">l<sub class="calibre14">1</sub></em></span> and <span class="strong"><em class="calibre9">l<sub class="calibre14">2</sub></em></span> norms are examples of a family of norms known as <span class="strong"><strong class="calibre2">p-norms</strong></span> and have the following general form for a vector <span class="strong"><em class="calibre9">v</em></span> with <span class="strong"><em class="calibre9">n</em></span> components:</p><div class="mediaobject1"><img src="../images/00058.jpeg" alt="Least absolute shrinkage and selection operator (lasso)" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div class="book" title="Regularization">
<div class="book" title="Implementing regularization in R"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec41" class="calibre1"/>Implementing regularization in R</h2></div></div></div><p class="calibre8">There are <a id="id294" class="calibre1"/>a number of different functions and packages that implement ridge regression, such as <code class="email">lm.ridge()</code> from the <code class="email">MASS</code> package and <code class="email">ridge()</code> from the <code class="email">genridge</code> package. For the lasso there is also the <code class="email">lars</code> package. In this chapter, we are going to work with the <code class="email">glmnet()</code> function from the <code class="email">glmnet</code> package due to its consistent and friendly interface. The key to working with regularization is to determine an appropriate value of <span class="strong"><em class="calibre9">λ</em></span> to use. The approach that the <code class="email">glmnet()</code> function uses is to use a grid of different <span class="strong"><em class="calibre9">λ</em></span> values and train a regression model for each value. Then, one can either pick a value manually or use a technique to estimate the best lambda. We can specify the sequence of <span class="strong"><em class="calibre9">λ</em></span> values to try via the <code class="email">lambda</code> parameter; otherwise, a default sequence with 100 values will be used. The first parameter to the <code class="email">glmnet()</code> function must be a matrix of features, which we can build using the <code class="email">model.matrix()</code> function.</p><p class="calibre8">The second parameter is a vector with the output variable. Finally, the <code class="email">alpha</code> parameter is a switch between ridge regression (0) and lasso (1). We're now ready to train some models on the cars dataset:</p><div class="informalexample"><pre class="programlisting">&gt; library(glmnet)
&gt; cars_train_mat &lt;- model.matrix(Price ~ .-Saturn, cars_train)[,-1]
&gt; lambdas &lt;- 10 ^ seq(8, -4, length = 250)
&gt; cars_models_ridge &lt;- 
  glmnet(cars_train_mat, cars_train$Price, alpha = 0, lambda = lambdas)
&gt; cars_models_lasso &lt;- 
  glmnet(cars_train_mat, cars_train$Price, alpha = 1, lambda = lambdas)</pre></div><p class="calibre8">As we provided a sequence of 250 <span class="strong"><em class="calibre9">λ</em></span> values, we've actually trained 250 ridge regression models and another 250 lasso models. We can see the value of <span class="strong"><em class="calibre9">λ</em></span> from the <code class="email">lambda</code> attribute of the object that is produced by <code class="email">glmnet()</code> and apply the <code class="email">coef()</code> function on this object to retrieve the corresponding coefficients for the 100th model, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; cars_models_ridge$lambda[100]
[1] 1694.009
&gt; coef(cars_models_ridge)[,100]
  (Intercept)       Mileage      Cylinder         Doors 
 6217.5498831    -0.1574441  2757.9937160   371.2268405 
       Cruise         Sound       Leather         Buick 
 1694.6023651   100.2323812  1326.7744321  -358.8397493 
     Cadillac         Chevy       Pontiac          Saab 
11160.4861489 -2370.3268837 -2256.7482905  8416.9209564 
  convertible     hatchback         sedan 
10576.9050477 -3263.4869674 -2058.0627013</pre></div><p class="calibre8">We can use the <code class="email">plot()</code> function to obtain a plot showing how the values of the coefficients change as the logarithm of <span class="strong"><em class="calibre9">λ</em></span> changes.</p><p class="calibre8">As shown <a id="id295" class="calibre1"/>below, it is very helpful to show the corresponding plot for ridge regression and lasso side by side:</p><div class="informalexample"><pre class="programlisting">&gt; layout(matrix(c(1, 2), 1, 2))
&gt; plot(cars_models_ridge, xvar = "lambda", main = "Ridge 
  Regression\n")
&gt; plot(cars_models_lasso, xvar = "lambda", main = "Lasso\n")</pre></div><div class="mediaobject"><img src="../images/00059.jpeg" alt="Implementing regularization in R" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The key difference between these two graphs is that lasso forces many coefficients to fall to zero exactly, whereas in ridge regression they tend to drop off smoothly and only become zero altogether at extreme values of <span class="strong"><em class="calibre9">λ</em></span>. This is further evident by reading the values of the numbers on the top horizontal axis of both graphs, which show the number of non-zero coefficients as <span class="strong"><em class="calibre9">λ</em></span> varies. In this way, lasso has a significant advantage in that it can often be used to perform feature selection (because a feature with a zero coefficient is essentially not included in the model) as well as providing regularization to minimize the issue of overfitting. We can obtain other useful plots by changing the value supplied to the <code class="email">xvar</code> parameter. The value <code class="email">norm</code> plots the <span class="strong"><em class="calibre9">l<sub class="calibre14">1</sub></em></span> norm of the coefficients on the x-axis and <code class="email">dev</code> plots the percentage deviance explained. We will learn about deviance in the next chapter.</p><p class="calibre8">To deal <a id="id296" class="calibre1"/>with the issue of finding a good value for <span class="strong"><em class="calibre9">λ</em></span>, the <code class="email">glmnet()</code> package offers the <code class="email">cv.glmnet()</code> function. This uses a technique known as cross-validation (we'll study this in <a class="calibre1" title="Chapter 5. Neural Networks" href="part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7">Chapter 5</a>, <span class="strong"><em class="calibre9">Support Vector Machines</em></span>) on the training data to find an appropriate <span class="strong"><em class="calibre9">λ</em></span> that minimizes the MSE:</p><div class="informalexample"><pre class="programlisting">&gt; ridge.cv &lt;- cv.glmnet(cars_train_mat, cars_train$Price, alpha = 0)
&gt; lambda_ridge &lt;- ridge.cv$lambda.min
&gt; lambda_ridge
[1] 641.6408

&gt; lasso.cv &lt;- cv.glmnet(cars_train_mat, cars_train$Price, alpha = 1)
&gt; lambda_lasso &lt;- lasso.cv$lambda.min
&gt; lambda_lasso
[1] 10.45715</pre></div><p class="calibre8">If we plot the result produced by the <code class="email">cv.glmnet()</code> function, we can see how the MSE changes over the different values of lambda:</p><div class="mediaobject"><img src="../images/00060.jpeg" alt="Implementing regularization in R" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The bars <a id="id297" class="calibre1"/>shown above and below each dot are the error bars showing one standard deviation above and below the estimate of the MSE for each plotted value of lambda. The plots also show two vertical dotted lines. The first vertical line shown corresponds to the value of <code class="email">lambda.min</code>, which is the optimal value proposed by cross-validation. The second vertical line to the right is the value in the attribute <code class="email">lambda.1se</code>. This corresponds to a value that is one standard error away from <code class="email">lambda.min</code> and produces a more regularized model.</p><p class="calibre8">With the <code class="email">glmnet</code> package, the <code class="email">predict()</code> function now operates in a variety of contexts. We can, for example, obtain the coefficients of a model for a lambda value that was not in our original list.</p><p class="calibre8">For example, we have this:</p><div class="informalexample"><pre class="programlisting">&gt; predict(cars_models_lasso, type = "coefficients", s = lambda_lasso)
15 x 1 sparse Matrix of class "dgCMatrix"
                        1
(Intercept)  -521.3516739
Mileage        -0.1861493
Cylinder     3619.3006985
Doors        1400.7484461
Cruise        310.9153455
Sound         340.7585158
Leather       830.7770461
Buick        1139.9522370
Cadillac    13377.3244020
Chevy        -501.7213442
Pontiac     -1327.8094954
Saab        12306.0915679
convertible 11160.6987522
hatchback   -6072.0031626
sedan       -4179.9112364</pre></div><p class="calibre8">Note that <a id="id298" class="calibre1"/>it seems that lasso has not forced any coefficients to zero in this case, indicating that, based on the MSE, it is not suggesting removing any of them for the cars dataset. Finally, using the <code class="email">predict()</code> function again, we can make predictions with a regularized model using the <code class="email">newx</code> parameter to provide a matrix of features for observations on which we want to make predictions:</p><div class="informalexample"><pre class="programlisting">&gt; cars_test_mat &lt;- model.matrix(Price ~ . -Saturn, cars_test)[,-1]
&gt; cars_ridge_predictions &lt;- predict(cars_models_ridge, s = 
                            lambda_ridge, newx = cars_test_mat)
&gt; compute_mse(cars_ridge_predictions, cars_test$Price)
[1] 7609538
&gt; cars_lasso_predictions &lt;- predict(cars_models_lasso, s = 
                            lambda_lasso, newx = cars_test_mat)
&gt; compute_mse(cars_lasso_predictions, cars_test$Price)
[1] 7173997</pre></div><p class="calibre8">The lasso model performs best and, unlike ridge regression, in this case also slightly outperforms the regular model on the test data.</p></div></div>
<div class="book" title="Polynomial regression" id="VF2I1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec26" class="calibre1"/>Polynomial regression</h1></div></div></div><p class="calibre8">Polynomial <a id="id299" class="calibre1"/>regression is a <span class="strong"><em class="calibre9">kind</em></span> of linear regression.</p><p class="calibre8">While linear regression is when both the predictor and the response are each continuous and linearly-related, causing the response to increase or decrease at a constant ratio to the predictor (that is, in a straight line), with polynomial regression, <span class="strong"><em class="calibre9">different powers</em></span> of the predictor are successively added to see if they adjust the response significantly. As these increases are added to the equation, the line of data points will change its shape, turning the linear regression model from a best fitted line into a best fitted curve.</p><p class="calibre8">So, why should you bother with polynomial regression? The generally accepted answer or thought process is: when a linear model doesn't seem to be the best model for your data.</p><p class="calibre8">There are <a id="id300" class="calibre1"/>three main conditions that indicate a linear relationship may not be a good model for a use:</p><div class="book"><ul class="itemizedlist"><li class="listitem">There will be some variable relationships in your data that you <span class="strong"><em class="calibre9">assume</em></span> are curvilinear</li><li class="listitem">During visual inspection of your variables, you <span class="strong"><em class="calibre9">establish</em></span> (using a scatter plot is the most common method for this) a curvilinear relationship</li><li class="listitem">After you have actually created a linear regression model, an examination of residuals (using a scatterplot) shows numerous positive residual values in the middle, but patches of negative residual values at either end (or vice versa)</li></ul></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note14" class="calibre1"/>Note</h3><p class="calibre8">Note: In curvilinear relationships, values increase together up to a certain level (like a positive relationship) and then, as one value increases, the other decreases (negative relationship) or vice versa.</p></div><p class="calibre8">So, here we consider an example like the one just mentioned involving user cars. In our vehicle data, we have information listing many attributes, including the number of options each vehicle has. Suppose we are interested in the relationship between the number of options a car has (such as air conditioning or heated seats) and the resell price.</p><p class="calibre8">One might assume that the more options a vehicle has, the higher the price the vehicle would sell for.</p><p class="calibre8">However, upon closer analysis of this data, we see that is not the case:</p><div class="mediaobject"><img src="../images/00061.jpeg" alt="Polynomial regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">If we plot the data, we can see perhaps a text book example of a scenario that would benefit from a polynomial regression:</p><div class="mediaobject"><img src="../images/00062.jpeg" alt="Polynomial regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In this <a id="id301" class="calibre1"/>hypothetical scenario, the relationship between the independent variable <span class="strong"><em class="calibre9">x</em></span> (the percentage increase in the resell price over the blue book value) and the dependent variable <span class="strong"><em class="calibre9">y</em></span> (the number of options a vehicle has) can be modeled as an <span class="strong"><em class="calibre9">n<sup class="calibre15">th</sup></em></span> degree polynomial in <span class="strong"><em class="calibre9">x</em></span>.</p></div>
<div class="book" title="Summary" id="10DJ41-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec27" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we studied linear regression, a method that allows us to fit a linear model in a supervised learning setting where we have a number of input features and a single numeric output. Simple linear regression is the name given to the scenario where we have only one input feature, and multiple linear regression describes the case where we have multiple input features. Linear regression is very commonly used as a first approach to solving a regression problem. It assumes that the output is a linear weighted combination of the input features in the presence of an irreducible error component that is normally distributed and has zero mean and constant variance. The model also assumes that the features are independent. The performance of linear regression can be assessed by a number of different metrics from the more standard MSE to others, such as the R<sup class="calibre15">2</sup> statistic. We explored several model diagnostics and significance tests designed to detect problems from violated assumptions to outliers. Finally, we also discussed how to perform feature selection with stepwise regression and perform regularization using ridge regression and lasso.</p><p class="calibre8">Linear regression is a model with several advantages, which include fast and cheap parameter computation and a model that, by virtue of its simple form, is very easy to interpret and draw inferences from. There is a plethora of tests available to diagnose problems with the model fit and perform hypothesis testing to check the significance of the coefficients. In general, as a method, it is considered to be low variance because it is robust to small errors in the data. On the negative side, because it makes very strict assumptions, notably that the output function must be linear in the model parameters, it introduces a high degree of bias, and for general functions that are complex or highly nonlinear this approach tends to fare poorly. In addition, we saw that we cannot really rely on significance testing for coefficients when we move to a high number of input features. This fact, coupled with the independence assumption between features, renders linear regression a relatively poor choice to make when working in a higher dimensional feature space.</p><p class="calibre8">We also mentioned polynomial regression as an option for fitting data once a linear regression falls short, based upon the relationship of your data point values or when residuals from a linear regression model show certain positive-negative relationships.</p><p class="calibre8">In the next chapter, we will study logistic regression, which is an important method used in classification problems.</p></div></body></html>