<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Learning Using TensorFlow on the IBM Cloud</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we will provide an introduction to the concepts of deep learning and neural networks on the IBM Cloud. An overview of how to use the TensorFlow framework to implement deep learning models on the cloud will be provided as well. This chapter is designed to be a balance between theory and practical implementation.</span></p>
<p><span>We will cover the following topics in this chapter:</span></p>
<ul>
<li><span>Introduction to deep learning</span></li>
<li><span>TensorFlow basics</span></li>
<li><span>Neural networks using TensorFlow</span></li>
<li><span>An example</span></li>
<li>TensorFlow and image classifications</li>
<li>Additional preparation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to deep learning </h1>
                </header>
            
            <article>
                
<p><span>Deep learning (also known as <strong>deep structured learning</strong> or <strong>hierarchical learning</strong>) is part of a larger group of machine learning approaches based on learning data representations, as opposed to task-specific algorithms.<span class="Apple-converted-space"> </span></span></p>
<p><span>Learning can be supervised (which we covered in <a href="b2822c69-13f0-4943-9e66-f9ef04898b60.xhtml">Chapter 3</a>, <em>Supervised Machine Learning Models and Your Data</em>), semi-supervised, or unsupervised (covered in <a href="f131f753-1d77-478c-9c0d-1e799330eed8.xhtml">Chapter 4</a>, <em>Implementing Unsupervised Algorithms</em>).</span></p>
<p><span>Deep learning algorithms are at work in exciting areas such as image classification (</span><span>categorizing every pixels in a digital </span><span>image</span><span> into one of several land cover classes, or themes</span><span>)</span><span>, object detection (</span><span>the process of finding instances of real-world objects such as faces, cars, and buildings in images or videos)</span><span>, image restoration (</span><span>to compensate for, or undo, defects caused by motion blur, noise, and camera misfocus, which degrade an image</span><span>) and image segmentation (the process of </span><span>partitioning a digital </span><span>image</span><span> into multiple segments of pixels, also known as <strong>super-pixels</strong>, to simplify and/or change the representation of an </span><span>image</span><span> into something that is more meaningful and easier to analyze</span><span>).<span class="Apple-converted-space"> </span></span></p>
<p><span>Deep learning using enormous neural networks is teaching machines to automate the tasks performed by human visual systems.</span><span><span class="Apple-converted-space"> </span></span></p>
<p><span>Deep learning models are vaguely inspired by information processing and communication patterns in biological nervous systems, yet they do differ from the structural and functional properties of biological brains, which make them incompatible with neuroscience evidences.</span></p>
<p><span>Enough theory. While the preceding explanation of machine/deep learning may be high-level, it is sufficient enough for us to move on to the next section, where we start thinking about the means of deep learning implementation, specifically using a toolset developed by the Google Brain team for internal Google use, under the Apache 2.0 open source license on November 9, 2015: TensorFlow.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow basics </h1>
                </header>
            
            <article>
                
<p><span>Tensors can be thought to be generalized matrixes or, more specifically, mathematical</span> <span>entities living in structures and interacting with other mathematical entities. If the other entities in the structure are transformed</span> <span>in any way, then the tensor must also be transformed by that transformation rule.</span></p>
<p><span>What does the preceding definition mean? Perhaps thinking of a <strong>Tensor</strong> as multidimensional array is easier to grasp, or consider the following, comparing <strong>Scalar</strong>, <strong>Vector</strong>, <strong>Matrix</strong>, and <strong>Tensor</strong></span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6aa4e189-4fb1-4e5d-84b3-2ad53319d2b2.png" style=""/></div>
<p class="mce-root"/>
<p><span>Building on the topic of TensorFlow, TensorFlow is an</span> <span>open source software library (also called a</span> <span><strong>framework</strong>) originally created by Google for creating deep learning models.</span></p>
<p><span>You can visit</span> <a href="https://www.tensorflow.org/"><span>https://www.tensorflow.org/</span></a> <span>for more details</span> <span>on TensorFlow.</span></p>
<p><span>In the next section, we will talk about the connection between deep learning, neural networks, and TensorFlow.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural networks and TensorFlow </h1>
                </header>
            
            <article>
                
<p><span>Deep learning models typically employ algorithms known as <strong>neural networks</strong>, which are said to be inspired by the way actual biological nervous systems (such as the brain) process information. It enables computers to recognize all data points as to what each represents and learn patterns.<span class="Apple-converted-space"> </span></span></p>
<p><span>Today, the principal software tool for deep learning models is TensorFlow as it</span> <span>permits developers to create large-scale neural networks with numerous layers.<span class="Apple-converted-space"> </span></span></p>
<p><span>TensorFlow is mainly used for the following purposes:<span class="Apple-converted-space"> </span></span></p>
<ul>
<li><span>Classification</span></li>
<li><span>Perception</span></li>
<li><span>Understanding<span class="Apple-converted-space"> </span></span></li>
<li><span>Discovering<span class="Apple-converted-space"> </span></span></li>
<li><span>Prediction <span class="Apple-converted-space"> </span></span></li>
<li><span>Creation</span></li>
</ul>
<p><span>As noted in the Watson documentation, the challenge with deploying complex machine learning models such as a TensorFlow model is that these models are very computationally expensive and time-consuming to train. Some solutions (to this challenge) include GPU acceleration, distributed computing, or a combination of both. The</span> <span>IBM</span> <span>Cloud</span> <span>platform and Watson Studio offers both of these.</span></p>
<p><span>It also points: IBM Watson Studio permits one to leverage the computational power available on the cloud to speed up the training time of the more complex machine learning models, and thus reduce the time from hours or days, down to minutes.</span></p>
<p><span>In the next sections, we will explore several exercises demonstrating various ways of using TensorFlow with IBM Watson Studio.<span class="Apple-converted-space"> </span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An example </h1>
                </header>
            
            <article>
                
<p><span>In this section, we will start by stepping through a Watson Community (<a href="https://dataplatform.cloud.ibm.com/community">https://dataplatform.cloud.ibm.com/community</a>) tutorial, designed to demonstrate how easy it is to deploy a deep neural network using the TensorFlow libraries on</span> <span>IBM Watson Studio.<span class="Apple-converted-space"> </span></span></p>
<p><span>The tutorial is available on GitHub for download, but we won't provide the URL here because we will demonstrate how easy it is to simply import content from external sources (such as GitHub) from directly within a IBM Watson Studio project.</span></p>
<p><span>This exercise's key point is that complex machine learning models can be computationally thirsty, but IBM Watson Studio gives you the opportunity to easily and efficiently (pay as you go) use</span> <span>the computational power available on the cloud to speed up processing time and reduce the time it takes to learn from hours, or days, down to minutes.<span class="Apple-converted-space"> </span></span></p>
<p><span>Additionally, IBM Watson Studio provides all the tools essential to develop a data-centric solution in the cloud. It makes use of Apache Spark clusters (for computational power) and lets you create assets in Python, Scala, and R, and leverage open source frameworks (such as TensorFlow), all of which are already installed on Watson Studio.</span></p>
<p><span>If you take the time to read through the details of the tutorial, you will see that it explains how to create a new IBM Cloud account and sign up for IBM Watson Studio (which we already covered in <a href="07c92a06-635f-41ef-b2be-3654ba90b790.xhtml">Chapter 1</a>, <em>Introduction to IBM Cloud</em>).</span></p>
<p><span>The tutorial then goes on to show how to navigate to IBM's Watson Studio (once on the IBM Cloud platform), create a new project, and then import a notebook to the project.<span class="Apple-converted-space"> </span></span></p>
<p><span>Although, in earlier chapters, we showed how to create a new project in Watson Studio and create new notebooks, this will be the first time we do a notebook import (directly from an external URL), so the next sections will focus on how that process works.</span></p>
<div class="packt_infobox"><span>The notebook that is to be imported will already contain the TensorFlow libraries and example code, so this exercise should be both quick and super easy for us, so let's not waste any more time!</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the new project</h1>
                </header>
            
            <article>
                
<p><span>Taking the same steps that we followed in earlier chapters, we can create a new deep learning IBM Watson Studio project (see the following screenshot) and give it a name:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ab396f17-1f49-4b85-aa27-166db71fb636.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Notebook asset type</h1>
                </header>
            
            <article>
                
<p><span>Once the new project is created, from the project dashboard, you can click the</span> <span><span class="packt_screen">Add to project</span></span> <span>option and</span> <span>select</span> <span><span class="packt_screen">Notebook</span></span>, <span>as shown in the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e4ae4435-6b3f-47e4-a4f5-ef618a9f6793.png"/></div>
<p><span>As we have done in preceding chapters, we could create a new, empty notebook. But for this exercise we want to import an existing notebook.</span></p>
<p><span>IBM Watson Studio allows you to import notebooks from either a file or directly from a known and accessible URL.<span class="Apple-converted-space"> </span></span></p>
<p><span>In this case, we will choose to import from a URL. To do that, you select the</span> <span><span class="packt_screen">From URL</span></span> <span>option</span><span> </span><span>and type or paste in the following line</span><span>:</span></p>
<p><span><a href="https://github.com/aounlutfi/building-first-dl-model/blob/master/first-dl-model.ipynb">https://github.com/aounlutfi/building-first-dl-model/blob/master/first-dl-model.ipynb</a></span></p>
<p><span>The preceding link will be the (external) URL of the notebook to be imported into your Watson Studio project.<span class="Apple-converted-space"> </span></span></p>
<p><span>Next, click on</span> <span><span class="packt_screen">Create Notebook</span></span> <span>to begin the import (it should only take a few seconds, providing you have access to the URL), as shown in the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0793073b-1eea-4389-b611-fe74e691b61f.png" style=""/></div>
<p><span>After a few seconds, the notebook opens and is ready for review and execution:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/01d88983-414f-4ffd-9100-de56f3c23d4d.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the imported notebook</h1>
                </header>
            
            <article>
                
<p><span>To run the imported notebook, click</span> <span>on</span> <span><span class="packt_screen">Cell</span></span> <span>from the</span> commands <span>ribbon, and then click</span> <span><span class="packt_screen">Run All</span></span><span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-434 image-border" src="assets/4e080b2d-d4f3-44a1-b2b7-e0c7c1e3cec4.png" style=""/></div>
<p><span>After clicking</span> <span><span class="packt_screen">Run All</span>,</span> <span>IBM Watson Studio will then run all the cells within the notebook, which should take (about) fifteen minutes or so to complete, since the data set is made up of 20,000 images.<span class="Apple-converted-space"> </span></span></p>
<div class="packt_infobox"><span>You can also run each cell individually, if you want to better understand what is going on.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reviewing the notebook</h1>
                </header>
            
            <article>
                
<p><span>If you take the time (and you should) to look through the cells contained in the notebook, you will notice that there are plenty of markdown cells that explicitly describe the steps within the notebook.<span class="Apple-converted-space"> </span></span></p>
<p><span>For example, you should take note of the markdown cell labeled <span class="packt_screen">Imports</span> (as shown in the following screenshot) where it clearly states </span><span><span class="packt_screen">In order to be able to build, test, and run a NN in TensorFlow, the following imports have to be used. This also imports the MNIST data set (each point in the data set is a handwritten representation of the digits 0-9 in 784 pixels)</span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-435 image-border" src="assets/338b3dd5-be69-473b-9969-6823826befeb.png" style=""/></div>
<p><span>The tutorial also makes a point by challenging you to attempt to set up Python and TensorFlow on a local computer (not on the cloud using IBM Watson Studio) and run</span> <span>the example so that you can</span> <span>compare the results, noting that it may take hours, maybe even days to train, depending on the performance of the machine, and that is after you have assembled the required environment!</span></p>
<p><span>In the next example, we will cover using IBM Watson Studio with Watson services and the TensorFlow API to perform image classifications and object detection.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow and image classifications</h1>
                </header>
            
            <article>
                
<p><span>The IBM Watson Visual Recognition service uses deep learning algorithms to identify features like scenes, objects, and faces within images you upload to the service. You can also create and train custom classifiers to identify subjects that meet your requirements, using the Visual Recognition service, IBM Watson Studio, and related Python modules.</span></p>
<p><span>To get started with Visual Recognition, we'll need to use the usual procedure to create a Watson Studio project and define a new Python 3.5 Notebook, but we will also need to associate an IBM Watson Visual Recognition service instance with the project (a pretty easy thing to do as it turns out).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding the service</h1>
                </header>
            
            <article>
                
<p><span>To add the Watson Visual Recognition service, you need to go to the IBM Cloud <span class="packt_screen">Dashboard</span> and select <span class="packt_screen">Watson Services</span>, then <span class="packt_screen">Browse Services</span>, where you can then find and select the <span class="packt_screen">Visual Recognition</span> service:<span class="Apple-converted-space"> </span></span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-436 image-border" src="assets/2f3c736d-6d73-4ee5-9dd4-925f4fa61f6c.png" style=""/></div>
<p><span>Next, from the <span class="packt_screen">Visual Recognition</span> page (which is shown in the following screenshot), you can choose a location and resource group for the service instance and then click on <span class="packt_screen">Create</span> to actually create the instance of the service that you can use in your project:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-437 image-border" src="assets/7fac7444-0a3b-4002-acee-a22511d01131.png" style=""/></div>
<div class="packt_infobox"><span>A Visual Recognition service instance may only be associated with one project at a time.</span></div>
<p><span>Once you have created the Visual Recognition service instance, it will be listed on your cloud dashboard:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-438 image-border" src="assets/76a53786-2282-468a-8ca4-61171883a0a6.png" style=""/></div>
<p><span>After creating the service instance, you should be able to create what is referred to as <span class="packt_screen">Service Credentials</span> (or the API key) by clicking on <span class="packt_screen">New Credentials</span> or </span><span><span class="packt_screen">View Credentials</span> </span><span>in the </span><span><span class="packt_screen">Service credentials</span></span><span> section</span><span>.<span class="Apple-converted-space"> </span></span></p>
<div class="packt_infobox"><span>You will need this API key to refer to and use the instance within the project.</span></div>
<p class="mce-root"/>
<p><span>The following screenshot shows the</span> <span class="packt_screen"><span>S</span><span>ervice credentials</span></span> <span>page:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-439 image-border" src="assets/e779fd2e-0867-413b-8977-8719feab630c.png" style=""/></div>
<p><span>We will need to refer to this API key in the next few sections of this chapter, so keep it handy!</span></p>
<p><span>Now we are ready to get going with the actual Visual Recognition project. In this example (a version is available on GitHub), the Watson Visual Recognition service is used to perform</span> o<span>bject detection using the TensorFlow object detection API. <span class="Apple-converted-space"> </span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Required modules</h1>
                </header>
            
            <article>
                
<p><span>Although</span> <span>many </span><span>pre-installed libraries and modules</span><span> are already included as part of an IBM Watson Notebook environment (depending upon the type of notebook selected),</span> <span>various other modules and frameworks may still need to be installed for a particular project to work correctly, so the first thing we should do in our notebook is to type the following command to</span> <span>see the list of pre-installed libraries:</span><span><span class="Apple-converted-space"> </span></span></p>
<pre><strong><span>!pip list –isolated</span></strong></pre>
<div class="packt_tip"><span>It is a very good idea to always begin a new project by using this command to gain an understanding of your notebook's environment; it may save time later.</span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>You can then use the Python</span> <kbd><span>!pip install</span></kbd> <span>command to install each of this project's required modules line by line, as follows:</span></p>
<pre><strong><span>!pip install asn1crypto==0.23.0<br/></span><span>!pip install bleach==1.5.0<br/></span><span>!pip install certifi==2017.11.5<br/></span><span>!pip install cffi==1.11.2<br/></span><span>!pip install chardet==3.0.4<br/></span><span>!pip install cryptography==2.1.3<br/></span><span>!pip install cycler==0.10.0<br/></span><span>!pip install enum34==1.1.6<br/></span><span>!pip install html5lib==0.9999999<br/></span><span>!pip install idna==2.6<br/></span><span>!pip install Markdown==2.6.9<br/></span><span>!pip install matplotlib==2.1.0<br/></span><span>!pip install numpy==1.13.3<br/></span><span>!pip install olefile==0.44<br/></span><span>!pip install Pillow==4.3.0<br/></span><span>!pip install protobuf==3.5.0.post1<br/></span><span>!pip install pycparser==2.18<br/></span><span>!pip install pyOpenSSL==17.4.0<br/></span><span>!pip install pyparsing==2.2.0<br/></span><span>!pip install pysolr==3.6.0<br/></span><span>!pip install python-dateutil==2.6.1<br/></span><span>!pip install pytz==2017.3<br/></span><span>!pip install requests==2.18.4<br/></span><span>!pip install six==1.11.0<br/></span><span>!pip install tensorflow==1.4.0<br/></span><span>!pip install tensorflow-tensorboard==0.4.0rc3<br/></span><span>!pip install urllib3==1.22<br/></span><span>!pip install watson-developer-cloud==1.0.0<br/></span><span>!pip install Werkzeug==0.12.2</span></strong></pre>
<p><span>An alternative and more efficient way to install the required modules with one command is to reference the provided <kbd>requirements.txt</kbd> file:</span></p>
<pre><strong><span>pip3 install -r requirements.txt</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the API key in code</h1>
                </header>
            
            <article>
                
<p><span>The IBM Watson Visual Recognition service comes with built-in models that you can use to analyze images for scenes, objects, faces, and many other categories without any training. We have already created an instance of the Visual Recognition service, so it is available to our project. For this to work, you need to have that valid key (our actual API key).<span class="Apple-converted-space"> </span></span></p>
<p class="mce-root"/>
<p><span>Even though you have working Python code in the Watson Notebook, you need to now use your established API key with it, so it will validate with the Watson service (and actually work).<span class="Apple-converted-space"> </span></span></p>
<p><span>If you look through the Python project code (which we will load in the next section), you will find the following code statement:</span></p>
<pre><span>Visual_recognition = VisualRecognitionV3('2016-05-20', api_key='API_KEY')</span></pre>
<p><span>This line of code is where the Watson Visual Recognition service is initialized as an object we can use within our code.</span> <span>You will replace the</span> <kbd><span>API_KEY</span></kbd><span> phrase with your actual API key (the one you created in the previous section).<span class="Apple-converted-space"> </span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Additional preparation</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will take care of a bit more housekeeping that is required before we can successfully run the project.</span></p>
<p><span>Pillow</span> <span>is a <strong>Python Imaging Library</strong> (<strong>PIL</strong>), which provides support for opening, manipulating, and saving images. The current version identifies and reads a large number of formats.<span class="Apple-converted-space"> </span></span></p>
<p><span>This project utilizes <kbd>pillow</kbd> and requires at least version 5.3.0 to be installed. To ensure that the notebook uses this version, we need to run the following commands:</span></p>
<pre><span># --- we need pillow version of 5.3.0 to run this project<br/></span><span># --- this code will uninstall the current version:<br/></span><span>!pip uninstall -y Pillow<br/></span><span># --- install the 5.3.0 version<br/></span><span>!pip install Pillow==5.3.0<br/></span><span># --- import the new one<br/></span><span>import PIL<br/></span><span># --- Should print 5.3.0. If it doesn't, then restart the kernel<span class="Apple-converted-space"> <br/></span></span><span>print(PIL.PILLOW_VERSION)</span></pre>
<p><span>The code uninstalls the currently installed version of Pillow, installs version 5.3.0, imports it into the project, and then prints the (now) currently installed version.</span></p>
<p class="mce-root"/>
<p><span>As the final line of code indicates, if the output of the <kbd>print</kbd> command does not indicate that the <kbd>pillow</kbd> version installed is 5.3.0, you will need to stop and then restart your kernel (click on <span class="packt_screen">Kernel</span>, then restart within your notebook), then again execute the <kbd>print</kbd> command, and you should be ready to go:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a1874dbc-70a5-4042-b240-6dfb3913ccf3.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upgrading Watson</h1>
                </header>
            
            <article>
                
<p><span>When I first started experimenting with this project I ran into several difficulties with using the Visual Recognition service. After much debugging and some much appreciated help from IBM Cloud support, it was determined that the project code that I was using was using an older version of the cloud API.<span class="Apple-converted-space"> </span></span></p>
<p><span>To resolve the issues I was seeing, it was necessary to upgrade the Watson service to the latest version using the following command:</span></p>
<pre><span># --- Upgrade Watson Developer<br/></span><span>!pip install --upgrade "watson-developer-cloud&gt;-2.8.0"</span></pre>
<p>The preceding command generates the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9cbeb948-5a03-4c36-a9b0-8d8f44a23ecb.png"/></div>
<p class="mce-root"/>
<p><span>To validate that the service was upgraded, you should see the following message:</span></p>
<pre><span>Successfully installed watson-developer-cloud-2.8.0 websocket-client-0.48.0</span></pre>
<p><span>Once the service was upgraded, all of the issues I was previously experiencing were resolved.<span class="Apple-converted-space"> </span></span></p>
<div class="packt_infobox"><span>At the time of writing, 2.8.0 is the latest release. It is advisable to always check for and use the latest available version.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Images</h1>
                </header>
            
            <article>
                
<p><span>One last setup task to perform is to provide an image file</span> for our solution <span>to detect objects within.<span class="Apple-converted-space"> </span></span></p>
<p><span>The sample project code offers a picture of four dogs that you can use, but it'</span><span>s more fun to provide one or more of your own. The README notes of this project indicate that the code will expect the file to be located in</span> <kbd><span>test_image/image1.jpg</span></kbd><span>, but you can simply upload it as a data asset using the same steps we did in previous chapters and then update the code, so it finds the file (you can change the filename as well).<span class="Apple-converted-space"> </span></span></p>
<p><span>I chose to use the following three different images:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-442 image-border" src="assets/f2450e5f-fefe-4018-991d-603cdc52aad6.png" style=""/></div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code examination</h1>
                </header>
            
            <article>
                
<p><span>At this point, our environment should be ready for the main code section. Let's now look at each section of that code to understand its purpose.<span class="Apple-converted-space"> </span></span></p>
<p><span>The first section performs various additional imports. Take specific note of the line of code that imports the Visual Recognition service (<kbd>VisualRecognitionV3</kbd>) from the (now upgraded) <kbd>watson_developer_cloud</kbd>:</span></p>
<pre><span>from watson_developer_cloud import VisualRecognitionV3</span></pre>
<p><span>The following are the commands:</span></p>
<pre><span>import numpy as np<br/></span><span>import os<br/></span><span>import six.moves.urllib as urllib<br/></span><span>import sys<br/></span><span>import tarfile<br/></span><span>import tensorflow as tf<br/></span><span>import zipfile<br/></span><span>import json<br/></span><span>from io import StringIO<br/></span><span>from PIL import Image<br/></span><span>from watson_developer_cloud import VisualRecognitionV3<br/></span><span>import matplotlib.pyplot as plt<br/></span><span>import matplotlib.patches as patches</span></pre>
<p><span>The next line of code uses our previously mentioned API key (you'll use your own):</span></p>
<pre><span># --- Replace with your api key<br/></span><span>visual_recognition = VisualRecognitionV3('2016-05-20', api_key='r-</span><span>1m0OdmBy9khRHJvujylJoLRJIqjwS6Bqwb6VMBfeCE</span><span>')</span></pre>
<p><span>The next section contains variables that you can experiment with when you run the notebook. Look at the results and adjust the variables to see the effects:</span></p>
<pre><span>MAX_NUMBER_OF_BOXES = 9<br/></span><span>MINIMUM_CONFIDENCE = .9<br/></span><span>COLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'b', 'w']</span></pre>
<p>From the preceding commands, let's explore each of the three variables defined:</p>
<ul>
<li><kbd><span>MAX_NUMBER_OF_BOXES</span></kbd>:<span><span> This variable represents the maximum number of objects to locate within you test image; I used <kbd>9</kbd> because it can get ugly if there are a lot of them.</span></span></li>
<li><kbd>MINIMUM_CONFIDENCE</kbd>:<span> This variable represents the minimum confidence score that a box can have. If this value is too low, you may end up with boxes around nothing.</span></li>
<li><kbd>COLORS</kbd>: <span>This variable</span> <span>sets the resulting identification boxes' attributes.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accessing the model</h1>
                </header>
            
            <article>
                
<p><span>The next sections of the code will download the Visual Recognition model to be used and then load it into memory. Downloading the model may take a few minutes the first time, but it only needs to be downloaded on the first run:</span></p>
<pre><span># --- define What model to download.<br/></span><span>MODEL_NAME = 'ssd_mobilenet_v1_coco_11_06_2017'<br/></span><span>MODEL_FILE = MODEL_NAME + '.tar.gz'<br/></span><span>DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'<br/></span><span># --- Path to frozen detection graph. This is the actual model that<span class="Apple-converted-space"> </span> # --- is used for the object detection.<br/></span><span>PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'<br/></span><span>print('Downloading model... (This may take over 5 minutes)')<br/></span><span># --- Download model if not already downloaded<br/></span><span>if not os.path.exists(PATH_TO_CKPT):<br/></span><span><span class="Apple-converted-space">  </span> opener = urllib.request.URLopener()<br/></span><span>opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)<br/></span><span> print('Extracting...')<br/></span><span>tar_file = tarfile.open(MODEL_FILE)<br/></span><span>for file in tar_file.getmembers():<br/></span><span>    file_name = os.path.basename(file.name)<br/></span><span>if 'frozen_inference_graph.pb' in file_name:<br/></span><span>tar_file.extract(file, os.getcwd())<br/></span><span>else:<br/></span><span><span class="Apple-converted-space">  </span> print('Model already downloaded............')<br/></span><span># --- Load model into memory<br/></span><span>print('Loading da model...')<br/></span><span>detection_graph = tf.Graph()<br/></span><span>with detection_graph.as_default():<br/></span><span><span class="Apple-converted-space">   </span> od_graph_def = tf.GraphDef()<br/></span><span><span class="Apple-converted-space">   </span> with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:<br/></span><span><span class="Apple-converted-space">     </span> serialized_graph = fid.read()<br/></span><span><span class="Apple-converted-space">       </span> od_graph_def.ParseFromString(serialized_graph)<br/></span><span><span class="Apple-converted-space">      </span> tf.import_graph_def(od_graph_def, name='')</span></pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detection</h1>
                </header>
            
            <article>
                
<p><span>The next part of the code runs the image using the TensorFlow object detection API. It will provide the coordinates of the box as an array of the edge positions (top, left, bottom, and right). It will then crop and save the images based on the boxes. In order for us to crop the correct area we need to transform the coordinates from percentages to pixels by multiplying the values by the width and height:</span></p>
<pre><span>def load_image_into_numpy_array(image):<br/></span><span><span class="Apple-converted-space">  </span> (im_width, im_height) = image.size<br/></span><span><span class="Apple-converted-space">   </span> return np.array(image.getdata()).reshape(<br/></span><span><span class="Apple-converted-space">  </span> (im_height, im_width, 3)).astype(np.uint8)<br/></span><span># --- Path to image to test, was: "test_image/image1.jpg"<br/></span><span>TEST_IMAGE_PATH = 'image1.jpg'</span></pre>
<p><span>After we have saved the image portions, we can pass each of them to Watson for classification.<span class="Apple-converted-space"> </span></span></p>
<p><span>Notice the use of the variables we set previously (</span><span><kbd>MAX_NUMBER_OF_BOXES</kbd> and <kbd>MINIMUM_CONFIDENCE</kbd>) in the following code:</span></p>
<pre><span>print('detecting...')<br/></span><span>with detection_graph.as_default():<br/></span><span><span class="Apple-converted-space">  </span> with tf.Session(graph=detection_graph) as sess:<span class="Apple-converted-space"> <br/></span></span><span><span class="Apple-converted-space">       </span> image = Image.open(TEST_IMAGE_PATH)<br/></span><span><span class="Apple-converted-space">       </span> image_np = load_image_into_numpy_array(image)<br/></span><span><span class="Apple-converted-space">       </span> image_np_expanded = np.expand_dims(image_np, axis=0)<br/></span><span><span class="Apple-converted-space">       </span> image_tensor = detection_graph.<br/>        get_tensor_by_name('image_tensor:0')<br/></span><span><span class="Apple-converted-space">       </span> boxes = detection_graph.get_tensor_by_name('detection_boxes:0')<br/></span><span><span class="Apple-converted-space">       </span> scores = detection_graph.<br/>        get_tensor_by_name('detection_scores:0')<br/></span><span><span class="Apple-converted-space">       </span></span> <span>num_detections = detection_graph.<br/>        get_tensor_by_name('num_detections:0')<br/></span><span><span class="Apple-converted-space">      </span>  # --- Actual detection.<br/></span><span><span class="Apple-converted-space">    </span>(boxes, scores, num_detections) = sess.run([boxes, scores, num_detections], feed_dict={image_tensor: image_np_expanded})<br/></span><span><span class="Apple-converted-space"> </span> # --- Create figure and axes and display the image<br/></span><span><span class="Apple-converted-space">       </span> fig, ax = plt.subplots(1)<br/></span><span><span class="Apple-converted-space">       </span> ax.imshow(image_np)<br/></span><span><span class="Apple-converted-space">       </span> (height, width, x) = image_np.shape<br/></span><span>for i in range(0, int(min(num_detections,<span class="Apple-converted-space">         </span> MAX_NUMBER_OF_BOXES))):<br/></span><span><span class="Apple-converted-space">          </span> score = np.squeeze(scores)[i]<br/></span><span><span class="Apple-converted-space">          </span> # --- if the score is not greater than<br/></span><span><span class="Apple-converted-space">           </span># --- what we set the minimun score to be then<br/></span><span><span class="Apple-converted-space">           </span># --- exit the loop<br/></span><span><span class="Apple-converted-space">          </span> if score &lt; MINIMUM_CONFIDENCE:<br/></span><span><span class="Apple-converted-space">            </span> break<br/></span><span><span class="Apple-converted-space">         </span>   box = np.squeeze(boxes)[i]<br/></span><span><span class="Apple-converted-space">           </span> box_x = box[1] * width<br/></span><span><span class="Apple-converted-space">           </span> box_y = box[0] * height<br/></span><span><span class="Apple-converted-space">           </span> box_width = (box[3] - box[1]) * width<br/></span><span><span class="Apple-converted-space">           </span> box_height = (box[2] - box[0]) * height<br/></span><span><span class="Apple-converted-space">           </span> box_x2 = box[3] * width<br/></span><span><span class="Apple-converted-space">           </span> box_y2 = box[2] * height<br/></span><span><span class="Apple-converted-space">           </span> img2 = image.crop((box_x, box_y, box_x2, box_y2))<br/></span><span><span class="Apple-converted-space">           </span> path = 'cropped/image1'<br/></span><span><span class="Apple-converted-space">           </span> os.makedirs(path, exist_ok=True)<br/></span><span><span class="Apple-converted-space">           </span> full_path = os.path.join(path, 'img{}.jpg'.format(i))<br/></span><span><span class="Apple-converted-space">           </span> img2.save(full_path)</span><span><span class="Apple-converted-space">        </span></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification and output</h1>
                </header>
            
            <article>
                
<p><span>Assuming you perform the previously outlined environmental checks and setups outlined earlier in this chapter, all of the code up to this point should run flawlessly without producing any errors. The next section of code was updated from the original version of the project offered on GitHub due to changes IBM made to the newer version of the API.</span></p>
<div class="packt_infobox"><span>If you use the notebook code provided with this book, the updates have already been made for you.</span></div>
<p><span>Finally, the next code section receives the results or the classifications back from the Watson Visual Recognition service into an object named</span> <kbd><span>results</span></kbd><span>. <span class="Apple-converted-space"> </span></span></p>
<p><span>From that information, a label is then constructed, and a rectangle shape is drawn around each object that was detected within the source image file:</span></p>
<pre><span># --- Classify images with Watson visual recognition<br/></span><span><span class="Apple-converted-space">        </span> with open(full_path, 'rb') as images_file:<br/></span><span><span class="Apple-converted-space">            </span> parameters = json.dumps({'threshold': 0.7,<span class="Apple-converted-space">         </span> 'classifier_ids': ['default']})<br/></span><span><span class="Apple-converted-space">              </span> results = visual_recognition.classify(images_file=images_file, parameters=parameters).get_result()<span class="Apple-converted-space"> <br/></span></span><span><span class="Apple-converted-space">               </span> label = results['images'][0]['classifiers'][0]['classes'][0]['class']<br/></span><span><span class="Apple-converted-space">               </span> ax.text(box_x + 5, box_y - 5, label, fontsize=10, color='white', bbox={'facecolor':COLORS[i % 8], 'edgecolor':'none'})<br/></span><span><span class="Apple-converted-space">           </span> # --- Create a Rectangle patch<br/></span><span><span class="Apple-converted-space">           </span> rect = patches.Rectangle((box_x, box_y), box_width, box_height, linewidth=2, edgecolor=COLORS[i % 8], facecolor='none')<br/></span><span><span class="Apple-converted-space">           </span> ax.add_patch(rect)</span></pre>
<p><span>If you examine the <kbd>results</kbd> object more closely (try using the Python</span> <kbd><span>type</span></kbd> <span>command on it), you will see that the results object is a Python dictionary object.</span></p>
<div class="packt_infobox"><span>A Python dictionary object is</span> <span>similar to a list in that it is a collection of objects.<span class="Apple-converted-space"> </span></span></div>
<p><span>Now try adding</span> <kbd><span>print(results)</span></kbd> <span>to the notebook code and you'll get a glimpse of the raw output returned in</span> <span>results</span><span>.<span class="Apple-converted-space"> </span></span></p>
<p><span>Using the</span> <kbd><span>print(results)</span></kbd> <span>command, the actual output is shown in the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-443 image-border" src="assets/b7b06b27-feff-47a6-82bc-e7ae5082476b.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Objects detected</h1>
                </header>
            
            <article>
                
<p><span>Finally we are ready to use, <kbd>matplotlib</kbd>,</span> <kbd><span>plt.show()</span></kbd> <span>to display the current image that we are working on:</span></p>
<pre><span><span class="Apple-converted-space"> </span></span><span># --- use matplotlib to show the result!<br/></span><span><span class="Apple-converted-space"> </span>plt.show()</span></pre>
<p><span>We can now finally see the output of the project. In this example, our image was of a horse, and we can see that the Watson Visual Recognition service correctly detected and labeled the object as a pinto horse:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-444 image-border" src="assets/71d2361b-c83c-4c62-add9-cee100a4e2ac.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Now the fun part</h1>
                </header>
            
            <article>
                
<p><span>Now comes the fun part. You can download any number of files from almost anywhere to test the application (or create your own). I used several images and found that Watson was pretty accurate.<span class="Apple-converted-space"> </span></span></p>
<p><span>The first image used was correctly detected as a motorcycle, the second (an image showing two vintage cars) was close in that Watson detected one of the cars as a car but the other was detected as a light truck.<span class="Apple-converted-space"> </span></span></p>
<p><span>The results of the final image we already mentioned: Watson not only correctly detected a horse but also labeled its breed: pinto.</span></p>
<p><span>The following screenshot shows the three different images and their respective results:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-445 image-border" src="assets/11d3605d-8eed-4e7f-af0e-79b35559d38c.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Save and share your work</h1>
                </header>
            
            <article>
                
<p><span>As always, once you have a working project notebook, you can click on <span class="packt_screen">File</span> then <span class="packt_screen">Save</span> to save your work. The next step is to share your notebook by clicking on the <span class="packt_screen">Share</span> icon, as shown in the following screenshot):</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-446 image-border" src="assets/175a785c-318a-4faa-9a0a-8f43d52c38ca.png" style=""/></div>
<p><span>From there, you can select the way you want to share your notebook:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-447 image-border" src="assets/426faef0-dc94-41ef-90b1-1cde9e209233.png" style=""/></div>
<div class="packt_infobox"><span>You should get into the habit of documenting your notebooks with Markdown cell content before sharing it. You will be surprised how much better your work will be received if you add commentary and perspective to each cell.<span class="Apple-converted-space"> </span></span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>Finally, when you view your project notebooks (see the following screenshot), note the <span class="packt_screen">SHARED</span> and <span class="packt_screen">STATUS</span> icons. You can publish your notebook to various target environments, such as GitHub:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-448 image-border" src="assets/d53062b6-b401-4d3e-ba45-7f0b8e9f37d8.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we started with an introduction to the concepts of deep learning and looked at the basics of using TensorFlow libraries and neural networks.<span class="Apple-converted-space"> </span></span></p>
<p><span>We then walked through two IBM Watson Studio projects to illustrate how to build both a neural network and an object detection project using the tools and services provided on IBM Cloud.</span></p>
<p><span>In the next chapter, we will c</span><span>reate a facial expression platform on IBM Cloud.</span></p>


            </article>

            
        </section>
    </body></html>