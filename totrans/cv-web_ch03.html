<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" style="font-size:1.200rem;">
<head><title>Chapter&#160;3.&#160;Easy Object Detection for Everyone</title>
<link rel="stylesheet" href="../Styles/style0001.css" type="text/css"/>
<meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
</head>
<body id="page">
<div class="chapter" title="Chapter&#160;3.&#160;Easy Object Detection for Everyone" id="aid-MSDG1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"></a>Chapter&#160;3.&#160;Easy Object Detection for Everyone</h1>
</div>
</div>
</div>
<p>In the last chapter, We discussed fundamental topics such as matrix operations and matrix convolutions. Moreover, we saw how to apply various image filters and how to use them in our applications. But those topics are mostly about image processing, not about Computer Vision! What do Computer Vision methods do by themselves? They provide the ability to understand an image by analyzing it in such a manner that the computer can provide the information about objects of an image scene. Libraries, which we discussed previously, provide various functionalities to find different objects in an image. In this chapter, we will mainly<a id="id99" class="indexterm"></a> discuss methods that are included in the tracking.js (<a class="ulink" href="http://trackingjs.com">http://trackingjs.com</a>) and JSFeat (<a class="ulink" href="http://inspirit.github.io/jsfeat/">http://inspirit.github.io/jsfeat/</a>) libraries<a id="id100" class="indexterm"></a> to get objects from an image. We will see how to find a colored object, and how to find an object using a template. Further, we will create our own object detector. These techniques can be implemented not only for an image, but for a video too! Finally, we will move on to the object tracking topic.</p>
<p>We will cover the following topics in this chapter:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Color object detection</li>
<li class="listitem">Digging into the tracking.js API</li>
<li class="listitem">Image features</li>
<li class="listitem">Descriptors and object matching</li>
</ul>
</div>
<div class="section" title="Detecting color objects"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"></a>Detecting color objects</h1>
</div>
</div>
</div>
<p>In the previous<a id="id101" class="indexterm"></a> chapters, we worked mainly with grayscale images. Of course, the shape and intensity parts of objects are important, but what about the color information? Why don't we use that too? For example, a red apple on a table can be easily detected with just the color information. Actually, that is why color object detection sometimes performs much better than other detector methods. In addition, it is much faster to implement these algorithms, and a computer usually consumes less resources for these types of operation.</p>
<p>The tracking.js library <a id="id102" class="indexterm"></a>provides an outstanding functionality to create a color detection application. We will start from a basic color tracking example. It is relatively simple, but you need to keep in mind that it performs best only when a colored object can be easily separated from the background.</p>
<div class="section" title="Using predefined colors with the tracking.js library"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec21"></a>Using predefined colors with the tracking.js library</h2>
</div>
</div>
</div>
<p>Color detection is one of the methods provided by tracking.js. To use it properly, we need to learn <a id="id103" class="indexterm"></a>some background <a id="id104" class="indexterm"></a>first.</p>
<p>We will start<a id="id105" class="indexterm"></a> with the intuitive steps:</p>
<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we find all connected regions with the specified color. This is the most computation consuming part of the algorithm. The smaller the color regions present on an image, the faster the algorithm works.</li>
<li class="listitem">Next, we define bounding boxes or rectangles around each of those regions. Finally, we merge overlapping boxes to find the main object. Merging is done by just one pass. So, if an image has overlapping boxes that are produced after the first merge, then they will still overlap each other.</li>
</ol>
<div style="height:10px; width: 1px"></div>
</div>
<p>We will run the algorithm using three colors: yellow, magenta, and cyan. Here is an example of the color detection algorithm:</p>
<div class="mediaobject"><img src="../Images/image00112.jpeg" alt="Using predefined colors with the tracking.js library"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>As you can see, it is really hard to get color objects from the first image. It is a bit easier to do so for the second, but it can be easily done only for the center of the flower, since it can be separated from the leaves and flower petals.</p>
<p>How <a id="id106" class="indexterm"></a>can we do that with the tracking.js library? In this example, we will use the canvas, as we did in the previous <a id="id107" class="indexterm"></a>chapters. It can be done for other tags<a id="id108" class="indexterm"></a> too, which we will see a bit in the following.</p>
<p>First, we define a <code class="literal">ColorTracker</code> object and add prebuilt colors to it. For now, there are only three colors available: <code class="literal">magenta</code>, <code class="literal">cyan</code>, and <code class="literal">yellow</code>.</p>
<div class="informalexample"><pre class="programlisting">var canvas = document.getElementById('initCanvas');
var tracker = new tracking.ColorTracker(['magenta', 'cyan', 'yellow']);</pre>
</div>
<p>The tracker variable is just a holder for various parameters, based on which we will track an object.</p>
<p>Then, we need to define a function which will be called after the <code class="literal">track</code> event. In the example, we just want to show all our bounding boxes over the canvas and we execute the <code class="literal">draw</code> function for each rectangle here. Since the algorithm returns a color for a box, it will be easier to see the difference between results:</p>
<div class="informalexample"><pre class="programlisting">tracker.on('track', function (event) {
    event.data.forEach(function (rect) {
        draw(rect.x, rect.y, rect.width, rect.height, rect.color);
    });
});</pre>
</div>
<p>As we did in the first chapter, we create a <code class="literal">&lt;div&gt;</code> element for our canvas:</p>
<div class="informalexample"><pre class="programlisting">&lt;div id="images" class="canvas-parent"&gt;
    &lt;canvas id="initCanvas" class="canvas-img"&gt;&lt;/canvas&gt;
&lt;/div&gt;</pre>
</div>
<p>There are many ways of printing a result, for now, we take an example of the draw function from the tracking.js examples section. It will create a <code class="literal">&lt;div&gt;</code> element for each bounding box and append it to the <code class="literal">&lt;div&gt;</code> tag around the canvas:</p>
<div class="informalexample"><pre class="programlisting">var canvasParent = document.querySelector('.canvas-parent');
function draw(x, y, w, h, color) {
    var rect = document.createElement('div');
    canvasParent.appendChild(rect);
    rect.classList.add('rect');
    rect.style.border = '8px solid ' + color;
    rect.style.width = w + 'px';
    rect.style.height = h + 'px';
    rect.style.left = (canvas.offsetLeft + x) + 'px';
    rect.style.top = (canvas.offsetTop + y) + 'px';
    rect.style.position = 'absolute';
}</pre>
</div>
<p>Finally, we<a id="id109" class="indexterm"></a> only need to start<a id="id110" class="indexterm"></a> the tracker by calling the <code class="literal">track</code> function. The first parameter defines the element that contains graphical information <a id="id111" class="indexterm"></a>about where we need to detect a color. The second parameter holds the <code class="literal">tracker</code> variable:</p>
<div class="informalexample"><pre class="programlisting">tracking.track('#initCanvas', tracker);</pre>
</div>
<p>It was simple, wasn't it? We saw how the tracker works with not-so-easy examples. It can be applied for those cases, but it will not be smart to do that. Let's see examples where the color detection works really well.</p>
</div>
<div class="section" title="Using your own colors"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec22"></a>Using your own colors</h2>
</div>
</div>
</div>
<p>Only three colors?! Is that it? Of course not. If you want, you can register your own colors for object detection. It is not much harder than setting predefined colors. We first need to register<a id="id112" class="indexterm"></a> a new color for the <code class="literal">ColorTracker</code> function. You create a mapping between a string and a function, where the function should return a Boolean condition based on three channels: R, G, and B. The <code class="literal">true</code> is returned when the color matches our condition and <code class="literal">false</code> if not. Here, we want to get all colors where the red channel prevails. Since we start from really dark pixels, we will call it the <code class="literal">darkRed</code> color:</p>
<div class="informalexample"><pre class="programlisting">tracking.ColorTracker.registerColor('darkRed', function (r, g, b) {
    return r &gt; 100 &amp;&amp; g &lt; 90 &amp;&amp; b &lt; 90;
});</pre>
</div>
<p>By doing that, we register the darkRed color for all color trackers which we create. Now, we need to define a new color tracker with the newly registered color:</p>
<div class="informalexample"><pre class="programlisting">var tracker = new tracking.ColorTracker(['darkRed']);</pre>
</div>
<p>All other parts of the code are the same as they were in the previous example. The good thing is that the tracking.js library itself finds the color for a bounding box, we do not need to specify it. For example, we have picked a new image—two beautiful apples and the result looks something like the following screenshot:</p>
<div class="mediaobject"><img src="../Images/image00113.jpeg" alt="Using your own colors"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>Do you see how<a id="id113" class="indexterm"></a> the apples stand out from the cloth? That is an example of an image where color tracker shows its best performance. Use the color detection when an object can be easily separated from the background and you will not be disappointed. Furthermore, the basic advantages are that color detection is fast in terms of computation, and it is very easy to implement.</p>
</div>
</div>
</div>


<div class="section" title="Digging into the tracking.js API" id="aid-NQU21"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"></a>Digging into the tracking.js API</h1>
</div>
</div>
</div>
<p>We saw a <a id="id114" class="indexterm"></a>color tracker and added our own color matcher. The tracking.js library provides an excellent functionality to add a new object detector. It has a clear <a id="id115" class="indexterm"></a>API and good documentation to follow (<a class="ulink" href="http://trackingjs.com/docs.html">http://trackingjs.com/docs.html</a>). But first, we will see how to use a tracker with different HTML tags and dig a bit into the tracker API.</p>
<div class="section" title="Using the &lt;img&gt; and &lt;video&gt; tags"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec23"></a>Using the &lt;img&gt; and &lt;video&gt; tags</h2>
</div>
</div>
</div>
<p>The library <a id="id116" class="indexterm"></a>uses a <code class="literal">&lt;canvas&gt;</code> tag to operate with images. If<a id="id117" class="indexterm"></a> you run a tracker on a different tag, then the library <a id="id118" class="indexterm"></a>will convert the information from it to the canvas <a id="id119" class="indexterm"></a>automatically.</p>
<p>First of all, tracking can be applied to an <code class="literal">&lt;img&gt;</code> tag:</p>
<div class="informalexample"><pre class="programlisting">&lt;img id="img" src="/path/to/your/image.jpg"/&gt;</pre>
</div>
<p>In that case, we can specify the image path not in a JavaScript code, but in the tag itself. To run a tracker, we just need to set the tag <code class="literal">id</code> as a first parameter:</p>
<div class="informalexample"><pre class="programlisting">tracking.track('#img', tracker);</pre>
</div>
<p>Next comes the <code class="literal">&lt;video&gt;</code> tag. In our <code class="literal">&lt;div&gt;</code> element, which wraps the canvas, we need to add a <code class="literal">&lt;video&gt;</code> tag with the path to a video file:</p>
<div class="informalexample"><pre class="programlisting">&lt;div id="images" class="canvas-parent"&gt;
    
    &lt;video id="video" preload autoplay loop muted controls&gt;
        &lt;source src="/path/to/your/video"&gt;
    &lt;/video&gt;
    &lt;canvas id="initCanvas" class="canvas-img"&gt;&lt;/canvas&gt;
&lt;/div&gt;</pre>
</div>
<p>The library will<a id="id120" class="indexterm"></a> take each frame and process it separately. If <a id="id121" class="indexterm"></a>we want to print the result on a canvas, we need to <a id="id122" class="indexterm"></a>clear the canvas of the previous tracking results. We can do<a id="id123" class="indexterm"></a> that using the <code class="literal">context.clearRect</code> function and add it to the postprocessing part of the tracker functionality:</p>
<div class="informalexample"><pre class="programlisting">var context = canvas.getContext('2d');
tracker.on('track', function (event) {
    context.clearRect(0, 0, canvas.width, canvas.height);
    event.data.forEach(function (rect) {
        draw(rect.x, rect.y, rect.width, rect.height, rect.color);
    });
});</pre>
</div>
<p>We can draw new elements not only with the <code class="literal">&lt;div&gt;</code> tags, but also using the context itself. It is easier to use and the speed will be a bit faster. Here, in addition to just displaying a bounding box, we place a rectangle parameter around it:</p>
<div class="informalexample"><pre class="programlisting">function draw(x, y, w, h, color) {
    context.strokeStyle = color;
    context.strokeRect(x, y, w, h);
    context.fillStyle = "#fff";
    context.fillText('x: ' + x + 'px', x + w + 5, y + 11);
}</pre>
</div>
<p>To run the tracker with a video file, we place the <code class="literal">id</code> parameters of a <code class="literal">&lt;video&gt;</code> tag, as we did previously with other content tags:</p>
<div class="informalexample"><pre class="programlisting">tracking.track('#video', tracker);</pre>
</div>
<p>If you want to work with a camera instead of a video file, you need to remove the source from a <code class="literal">&lt;video&gt;</code> tag and add a new, third, parameter to the <code class="literal">track</code> call:</p>
<div class="informalexample"><pre class="programlisting">tracking.track('#video', tracker, { camera: true });</pre>
</div>
<p>What if you have a long-running video? In that case, you probably need to have full control over your application, as you may want to stop and rerun the tracker. First, you need to get a reference to it:</p>
<div class="informalexample"><pre class="programlisting">var trackingTask = tracking.track(...);</pre>
</div>
<p>You can stop or run a tracking task any time with those functions:</p>
<div class="informalexample"><pre class="programlisting">trackingTask.stop();
trackingTask.run();</pre>
</div>
<p>If you are<a id="id124" class="indexterm"></a> still not satisfied with your control over the<a id="id125" class="indexterm"></a> tracking methods, there are various functions that <a id="id126" class="indexterm"></a>can<a id="id127" class="indexterm"></a> help you while you do the tracking:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">setMinDimension</code>: This<a id="id128" class="indexterm"></a> sets a bounding box of an object with minimum width and height; by default, it is 20 pixels. It helps to avoid noisy objects and focuses on objects that hold a larger space.</li>
<li class="listitem"><code class="literal">setMaxDimension</code>: This sets the maximum dimensions of a bounding box, it is infinity by <a id="id129" class="indexterm"></a>default. In some cases, it helps to remove an image background which is of the same color as the object.</li>
<li class="listitem"><code class="literal">setMinGroupSize</code>: This sets the minimum number of pixels for a colored object to<a id="id130" class="indexterm"></a> be classified as a rectangle; the default is 30. This also helps to reduce the noise significantly.</li>
</ul>
</div>
</div>
<div class="section" title="Building a custom tracker"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec24"></a>Building a custom tracker</h2>
</div>
</div>
</div>
<p>It is time to build your own tracker! The tracking.js library provides an abstract interface for that, but you need to write the core of the tracker—its brains. We will create a tracker that will determine <a id="id131" class="indexterm"></a>edge pixels using the knowledge that you <a id="id132" class="indexterm"></a>gained from the previous chapter.</p>
<p>As a first step, we will create a constructor of our new <code class="literal">CornerTracker</code> variable. We will use the Sobel filter for our example, so we only need one threshold parameter for it, which we define here as a field of our object:</p>
<div class="informalexample"><pre class="programlisting">var CornerTracker = function (thres) {
    CornerTracker.base(this, 'constructor');
    this.thres = thres;
};</pre>
</div>
<p>Our tracker must inherit from the basic tracker of the tracking.js library, which can be done using the following function:</p>
<div class="informalexample"><pre class="programlisting">tracking.inherits(CornerTracker, tracking.Tracker);</pre>
</div>
<p>The most important part of our tracker is a <code class="literal">track</code> function. It contains the tracker logic. As parameters, it takes an array of pixels from an image and the dimensions of that image. On the inside, we run the Sobel filter, and if you remember, it returns a 4-channel array for a grayscale image, but we need only one. We check whether the value exceeds the threshold and if so, we add a new edge pixel there. After all, we need to emit the computed data using the <code class="literal">emit</code> function. It sends the output data through the <code class="literal">track</code> event. The output for our example is the coordinates of pixels that passed the condition:</p>
<div class="informalexample"><pre class="programlisting">CornerTracker.prototype.track = function (pixels, width, height) {
    var sobel = tracking.Image.sobel(pixels, width, height);
    var edges = [];
    var pos = 0;
    for (var i = 0; i &lt; height; i++) {
        for (var j = 0; j &lt; width; j++) {
            var w = i * width * 4 + j * 4;
            if (sobel[w] &gt; this.thres)
                edges[pos++] = {x: j, y: i};
        }
    }
    this.emit('track', {
        data: edges
    });
};</pre>
</div>
<p>To create a<a id="id133" class="indexterm"></a> new tracker, we call the constructor <a id="id134" class="indexterm"></a>with a threshold parameter and set the threshold to 400:</p>
<div class="informalexample"><pre class="programlisting">var tracker = new CornerTracker(400);</pre>
</div>
<p>At the end of tracking process, we plot the result. The plot function simply puts the pixels on a canvas:</p>
<div class="informalexample"><pre class="programlisting">tracker.on('track', function (event) {
    event.data.forEach(function (point) {
        plot(point.x, point.y);
    });
});

var context = canvas.getContext('2d');
function plot(x, y) {
    context.fillStyle = '#FF0000';
    context.fillRect(x, y, 3, 3);
}</pre>
</div>
<p>To start our tracker, we need to initiate the tracking function as we did previously:</p>
<div class="informalexample"><pre class="programlisting">tracking.track('#initCanvas', tracker);</pre>
</div>
<p>Let's match the result from the previous chapter with our tracker:</p>
<div class="mediaobject"><img src="../Images/image00114.jpeg" alt="Building a custom tracker"/><div class="caption"><p>From left to right: an image after the Sobel filter, Sobel filter thresholding and the result after our tracker.</p>
</div>
</div>
<p style="clear:both; height: 1em;"> </p>
<p>As we see, the<a id="id135" class="indexterm"></a> results match together, so we have done <a id="id136" class="indexterm"></a>everything right. Good job!</p>
<p>The tracking.js API provides a very good abstraction level for creating your own object tracker. There are not many trackers there yet, but you can always extend the functionality. The main advantage of this abstraction is that you can focus on the implementation of algorithms without wasting your time thinking about how to apply your algorithm to an image.</p>
</div>
</div>


<div class="section" title="Image features" id="aid-OPEK1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"></a>Image features</h1>
</div>
</div>
</div>
<p>Color object detection and detection of changes in intensity of an image, is a simple Computer Vision method. It is a fundamental thing which every Computer Vision enthusiast should know. To get a better picture of Computer Vision capabilities, we will see how to find an object on a scene using a template. This topic includes several parts: feature extraction and descriptor matching. In this part, we will discuss feature detection and its application in Computer Vision.</p>
<div class="section" title="Detecting key points"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec25"></a>Detecting key points</h2>
</div>
</div>
</div>
<p>What information do we get when we see an object on an image? An object usually consists of some<a id="id137" class="indexterm"></a> regular parts or unique points, which represent the particular object. Of course, we can compare each pixel of an image, but it is not a good idea in terms of computational speed. We can probably take unique points randomly, thus reducing the computation cost significantly. However, we will still not get much information from random points. Using the whole information, we can get too much noise and lose the important parts of an object representation. Eventually, we need to consider that both ideas—getting all pixels and selecting random pixels—are really bad. So, what can we do instead?</p>
<p>Since we are <a id="id138" class="indexterm"></a>working with a grayscale image and we need to get a unique point, we need to focus on intensity information, for example, getting object edges like we did with the Canny edge detector or Sobel filter. We are closer to the solution! But still, not close enough. What if we have a long edge? Don't you think that it is a bit bad that we have too many unique pixels which lay on this edge? An edge of an object has end points or corners. If we reduce our edge to those corners, we will get enough unique pixels and remove unnecessary information.</p>
<p>There are various methods of getting keypoints from an image, many of them extract corners as keypoints. To get them, we will use the <span class="strong"><strong>Features from Accelerated Segment Test</strong></span> (<span class="strong"><strong>FAST</strong></span>) algorithm. It is really simple and you can easily implement it by yourself if you want. But<a id="id139" class="indexterm"></a> you do not need to. The algorithm implementation is provided both by the tracking.js and JSFeat libraries.</p>
<p>The idea of the FAST algorithm can be captured from the following image:</p>
<div class="mediaobject"><img src="../Images/image00115.jpeg" alt="Detecting key points"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>Suppose we want to check whether the pixel <span class="strong"><strong>P</strong></span> is a corner, we will check 16 pixels around it. And if at least 9 pixels in an arc around <span class="strong"><strong>P</strong></span> are much darker or brighter than the <span class="strong"><strong>P</strong></span> value, then we say that <span class="strong"><strong>P</strong></span> is a corner. How much darker or brighter the <span class="strong"><strong>P</strong></span> pixels be? The decision is made by applying a threshold for the difference between the value of <span class="strong"><strong>P</strong></span> and the value of pixels around <span class="strong"><strong>P</strong></span>.</p>
</div>
<div class="section" title="A practical example"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec26"></a>A practical example</h2>
</div>
</div>
</div>
<p>First, we will start with an example of FAST corner detection for the tracking.js library. Before we do something, we can set the detector threshold. The threshold defines the minimum difference between a tested corner and points around it:</p>
<div class="informalexample"><pre class="programlisting">tracking.Fast.THRESHOLD = 30;</pre>
</div>
<p>It is usually<a id="id140" class="indexterm"></a> good practice to apply a Gaussian blur to an image before we start the method. It significantly reduces the noise of an image:</p>
<div class="informalexample"><pre class="programlisting">var imageData = context.getImageData(0, 0, cols, rows);
var gray = tracking.Image.grayscale(imageData.data, cols, rows, true);
var blurred4 = tracking.Image.blur(gray, cols, rows, 3);</pre>
</div>
<p>Remember, that the blur function returns a 4 channel array—RGBA. In that case, we need to convert it to a 1-channel array. Since we can easily skip other channels, it should not be a problem:</p>
<div class="informalexample"><pre class="programlisting">var blurred1 = new Array(blurred4.length / 4);
for (var i = 0, j = 0; i &lt; blurred4.length; i += 4, ++j) {
    blurred1[j] = blurred4[i];
}</pre>
</div>
<p>Next, we run a corner detection function on our image array:</p>
<div class="informalexample"><pre class="programlisting">var corners = tracking.Fast.findCorners(blurred1, cols, rows);</pre>
</div>
<p>The result returns an array where length is twice that of the corners number. The array is returned in a format: <code class="literal">[x0,y0,x1,y1,...]</code>, where <code class="literal">[xn, yn]</code> are coordinates of a detected corner. To print the result on a canvas, we will use the <code class="literal">fillRect</code> function. Since the number of points is usually around several hundreds, we cannot efficiently use <code class="literal">&lt;div&gt;</code> tag for that:</p>
<div class="informalexample"><pre class="programlisting">for (i = 0; i &lt; corners.length; i += 2) {
    context.fillStyle = '#0f0';
    context.fillRect(corners[i], corners[i + 1], 3, 3);
}</pre>
</div>
<p>Now we will see an example with the JSFeat library, the steps for which are very similar to what we saw with tracking.js. First, we set the global threshold with a function:</p>
<div class="informalexample"><pre class="programlisting">jsfeat.fast_corners.set_threshold(30);</pre>
</div>
<p>Then, we apply a Gaussian blur to the image matrix and run corner detection:</p>
<div class="informalexample"><pre class="programlisting">jsfeat.imgproc.gaussian_blur(matGray, matBlurred, 3);</pre>
</div>
<p>We need to preallocate keypoints for a corner's result. The <code class="literal">keypoint_t</code> function is just a new type, which is useful for key points of an image. The first two parameters represent coordinates of a point, and the other parameters represent point score (is that point good enough to be a key point?) point level (which you can use in an image pyramid, for example), and point angle (which is usually used for the gradient orientation):</p>
<div class="informalexample"><pre class="programlisting">var corners = [];
var i = cols * rows;
while (--i &gt;= 0) {
    corners[i] = new jsfeat.keypoint_t(0, 0, 0, 0, -1);
}</pre>
</div>
<p>After all this, we<a id="id141" class="indexterm"></a> execute the FAST corner detection method. As a last parameter of the detection function, we define a border size. The border is used to constrain circles around each possible corner. For example, you cannot precisely say if the point is a corner for the <code class="literal">[0,0]</code> pixel. There is no <code class="literal">[0, -3]</code> pixel in our matrix:</p>
<div class="informalexample"><pre class="programlisting">var count = jsfeat.fast_corners.detect(matBlurred, corners, 3);</pre>
</div>
<p>Since we preallocated the corners, the function returns the number of calculated corners for us. The result returns an array of structures with the <code class="literal">x</code> and <code class="literal">y</code> fields, so we can print it using those fields:</p>
<div class="informalexample"><pre class="programlisting">for (var i = 0; i &lt; count; i++) {
    context.fillStyle = '#0f0';
    context.fillRect(corners[i].x, corners[i].y, 3, 3);
}</pre>
</div>
<p>The result is nearly the same for both algorithms. The difference is in some parts of realization. Let's look at the following examples:</p>
<div class="mediaobject"><img src="../Images/image00116.jpeg" alt="A practical example"/><div class="caption"><p>From left to right: tracking.js without blur, JSFeat without blur, tracking.js and JSFeat with blur.</p>
</div>
</div>
<p style="clear:both; height: 1em;"> </p>
<p>If you look closely, you can see the difference between tracking.js and JSFeat results, but it is not easy to spot. Look at how much noise was reduced by applying just a small 3 x 3 Gaussian filter! A lot of noisy points were removed from the background. And now the algorithm can focus on points that represent flowers and the pattern of the vase.</p>
<p>We extracted key<a id="id142" class="indexterm"></a> points from our image, and we successfully reached the goal of reducing the number of keypoints and focusing on the unique points of an image. Now we need to compare or match those points somehow. How we can do that? We will cover this in the next chapter.</p>
</div>
</div>


<div class="section" title="Descriptors and object matching"><div class="titlepage" id="aid-PNV62"><div><div><h1 class="title"><a id="ch03lvl1sec25"></a>Descriptors and object matching</h1>
</div>
</div>
</div>
<p>Image features by themselves are a bit useless. Yes, we have found unique points on an image. But what<a id="id143" class="indexterm"></a> did we get? Only pixels values and that's it. If we try to compare those values it will not give us much information. Moreover, if we change the overall image brightness, we will not find the same keypoints on the same image! Taking into account all of this, we need the information which surrounds our key points. Moreover, we need a method to efficiently compare this information. First, we need to describe the image features, which comes from image descriptors. In this part, we will see how those descriptors can be extracted and matched. The tracking.js and JSFeat libraries provide different methods for image descriptors. We will discuss both.</p>
<div class="section" title="The BRIEF and ORB descriptors"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec27"></a>The BRIEF and ORB descriptors</h2>
</div>
</div>
</div>
<p>The<a id="id144" class="indexterm"></a> descriptors theory is focused on changes in an image <a id="id145" class="indexterm"></a>pixels' intensities. The tracking.js library provides<a id="id146" class="indexterm"></a> the <span class="strong"><strong>Binary Robust Independent Elementary Features</strong></span> (<span class="strong"><strong>BRIEF</strong></span>) descriptors<a id="id147" class="indexterm"></a> and the its JSFeat extension <span class="strong"><strong>Oriented FAST and Rotated BRIEF</strong></span> (<span class="strong"><strong>ORB</strong></span>). As we can see from the ORB naming, it is rotation invariant. This means that even if you rotate an object, the algorithm can still detect it. Moreover, the authors of the JSFeat library provide an example using the image pyramid, which is scale invariant too.</p>
<p>Let's start by explaining BRIEF, since it is the source for ORB descriptors. As a first step, the algorithm takes computed image features, and it takes the unique pairs of elements around each feature. Based on these pairs' intensities, it forms a binary string. For example, if we have a positions pair <code class="literal">i</code> and <code class="literal">j</code>, and if <code class="literal">I(i) &lt; I(j)</code> (where <code class="literal">I(pos)</code> indicates the value of the image at the position <code class="literal">pos</code>), then the result is 1, otherwise 0. We add this result to the binary string. We do this for <code class="literal">N</code> pairs, where <code class="literal">N</code> is taken as a power of 2 <code class="literal">(128, 256, 512)</code>. Since descriptors are just binary strings, we can compare them in an efficient manner. To match these strings, the Hamming distance is usually used. It shows the minimum number of substitutions required to change one string to another. For example, if we have two binary strings—<code class="literal">10011</code> and <code class="literal">11001</code>, the Hamming distance between them is <code class="literal">2</code>, since we need to change two bits of information to change the first string to the second.</p>
<p>The JSFeat library<a id="id148" class="indexterm"></a> provides the functionality to apply the<a id="id149" class="indexterm"></a> ORB descriptors. The core idea is very similar to<a id="id150" class="indexterm"></a> that <a id="id151" class="indexterm"></a>of BRIEF. There are two major differences:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The implementation is scale invariant, since the descriptors are computed for an image pyramid.</li>
<li class="listitem">The descriptors are rotation invariant; the direction is computed using intensity of the patch around a feature. Using this orientation, ORB manages to compute a BRIEF descriptor in a rotation invariant manner.</li>
</ul>
</div>
</div>
<div class="section" title="Descriptors implementation and their matching"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec28"></a>Descriptors implementation and their matching</h2>
</div>
</div>
</div>
<p>Our goal is to find <a id="id152" class="indexterm"></a>an object from a template on a scene image. We<a id="id153" class="indexterm"></a> can do that by finding features and descriptors on both images and by matching descriptors from a template to an image.</p>
<p>We start from the tracking.js library and BRIEF descriptors. The first thing that you can do is set the number of location pairs:</p>
<div class="informalexample"><pre class="programlisting">tracking.Brief.N = 512</pre>
</div>
<p>By default, it is <code class="literal">256</code>, but you can choose a higher value. The larger the value the more information you will get and the more memory and computational cost it requires.</p>
<p>Before starting the computation, do not forget to apply the Gaussian blur. Next, we find the FAST corners and compute descriptors on both images. In the following code, we use the suffix <code class="literal">Object</code> for a template image and <code class="literal">Scene</code> for a scene image:</p>
<div class="informalexample"><pre class="programlisting">var cornersObject = tracking.Fast.findCorners(grayObject, colsObject, rowsObject);
var cornersScene = tracking.Fast.findCorners(grayScene, colsScene, rowsScene);
var descriptorsObject = tracking.Brief.getDescriptors(grayObject, colsObject, cornersObject);
var descriptorsScene = tracking.Brief.getDescriptors(grayScene, colsScene, cornersScene);</pre>
</div>
<p>Then, we do the matching:</p>
<div class="informalexample"><pre class="programlisting">var matches = tracking.Brief.reciprocalMatch(cornersObject, descriptorsObject, cornersScene, descriptorsScene);</pre>
</div>
<p>We need to pass both the corners and descriptors information to the function, since it returns coordinate information as a result.</p>
<p>Next, we<a id="id154" class="indexterm"></a> print both images on one canvas. To draw the matches<a id="id155" class="indexterm"></a> using this trick, we need to shift our scene keypoints for the width of a template image. As a <code class="literal">keypoint1</code> the matching function returns points on a template image and <code class="literal">keypoint2</code> matched points from a scene image. The keypoint1/2 are arrays with <code class="literal">x</code> and <code class="literal">y</code> coordinates at <code class="literal">0</code> and <code class="literal">1</code> indexes, respectively:</p>
<div class="informalexample"><pre class="programlisting">for (var i = 0; i &lt; matches.length; i++) {
    var color = '#' + Math.floor(Math.random() * 16777215).toString(16);
    context.fillStyle = color;
    context.strokeStyle = color;
    context.fillRect(matches[i].keypoint1[0], matches[i].keypoint1[1], 5, 5);
    context.fillRect(matches[i].keypoint2[0] + colsObject, matches[i].keypoint2[1], 5, 5);
    context.beginPath();
    context.moveTo(matches[i].keypoint1[0], matches[i].keypoint1[1]);
    context.lineTo(matches[i].keypoint2[0] + colsObject, matches[i].keypoint2[1]);
    context.stroke();
}</pre>
</div>
<p>The JSFeat library provides most of the code for pyramids and scale invariant features not in the library, but in the examples, which are available on <a class="ulink" href="https://github.com/inspirit/jsfeat/blob/gh-pages/">https://github.com/inspirit/jsfeat/blob/gh-pages/</a>
<a class="ulink" href="http://sample_orb.html">sample_orb.html</a>. We will not provide the full code here, because it requires too much space. But do not worry; we will highlight the main topics.</p>
<p>Let's start from functions that are included in the library. First, we need to preallocate the descriptors matrix, where 32 is the length of a descriptor and 500 is the maximum number of descriptors. Again 32 is a power of two:</p>
<div class="informalexample"><pre class="programlisting">var descriptors = new jsfeat.matrix_t(32, 500, jsfeat.U8C1_t);</pre>
</div>
<p>Then, we compute the ORB descriptors for each corner, we need to do this for both template and scene images:</p>
<div class="informalexample"><pre class="programlisting">jsfeat.orb.describe(matBlurred, corners, num_corners, descriptors);</pre>
</div>
<p>JSFeat does not provide a matching function in the library but it does in the examples section. The function uses global variables, which mainly define input descriptors and output matching:</p>
<div class="informalexample"><pre class="programlisting">function match_pattern()</pre>
</div>
<p>The resulting <code class="literal">match_t</code> function contains the following fields:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">screen_idx</code>: This is the index of a scene descriptor</li>
<li class="listitem"><code class="literal">pattern_lev</code>: This is the index of a pyramid level</li>
<li class="listitem"><code class="literal">pattern_idx</code>: This is the index of a template descriptor</li>
</ul>
</div>
<p>Since <a id="id156" class="indexterm"></a>ORB works with the image pyramid, it returns corners and <a id="id157" class="indexterm"></a>matches for each level the pyramid:</p>
<div class="informalexample"><pre class="programlisting">var s_kp = screen_corners[m.screen_idx];
var p_kp = pattern_corners[m.pattern_lev][m.pattern_idx];</pre>
</div>
<p>We can print each matching as follows. Again, we use <span class="emphasis"><em>Shift</em></span>, since we computed descriptors on separate images, but print the result on one canvas:</p>
<div class="informalexample"><pre class="programlisting">context.fillRect(p_kp.x, p_kp.y, 4, 4);
context.fillRect(s_kp.x + shift, s_kp.y, 4, 4);</pre>
</div>
</div>
<div class="section" title="Finding an object location"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec29"></a>Finding an object location</h2>
</div>
</div>
</div>
<p>We found a <a id="id158" class="indexterm"></a>match. That is great. But what we did not <a id="id159" class="indexterm"></a>do is find the object location. There is no function for that in the tracking.js library but JSFeat provides such a functionality in the examples section.</p>
<p>First, we need to compute a perspective transform matrix. Remember the first chapter? We have points from two images, but we do not have a transformation for the whole image.</p>
<p>First, we define a transform matrix:</p>
<div class="informalexample"><pre class="programlisting">var homo3x3 = new jsfeat.matrix_t(3, 3, jsfeat.F32C1_t);</pre>
</div>
<p>To compute the homography, we need only four points. But after the matching, we get too many. In addition, there can be noisy points, which we want to skip somehow. For this, we use a <span class="strong"><strong>Random sample consensus</strong></span> (<span class="strong"><strong>RANSAC</strong></span>) algorithm. It is an iterative method for<a id="id160" class="indexterm"></a> estimating a mathematical model from a dataset that contains outliers (noise). It estimates outliers and generates a model that is computed without the noisy data.</p>
<p>Before we start, we need to define the algorithm parameters. The first parameter is a match mask, where all matches will be marked as good (<code class="literal">1</code>) or bad (<code class="literal">0</code>).</p>
<div class="informalexample"><pre class="programlisting">var match_mask = new jsfeat.matrix_t(500, 1, jsfeat.U8C1_t);</pre>
</div>
<p>Next parameter is a mathematical model which we want to obtain using the RANSAC algorithm:</p>
<div class="informalexample"><pre class="programlisting">var mm_kernel = new jsfeat.motion_model.homography2d();</pre>
</div>
<p>Third, we need minimum number of points to estimate a model (4 points to get a homography), this can be defined as follows:</p>
<div class="informalexample"><pre class="programlisting">var num_model_points = 4;</pre>
</div>
<p>Then, it is useful to have a maximum threshold to classify a data point as an inlier or a good match:</p>
<div class="informalexample"><pre class="programlisting">var reproj_threshold = 3;</pre>
</div>
<p>Finally, the variable which holds the main parameters, that is, the last two arguments define the maximum ratio of outliers and probability of success. The algorithm stops when the number of inliers is 99 percent:</p>
<div class="informalexample"><pre class="programlisting">var ransac_param = new jsfeat.ransac_params_t(num_model_points,
        reproj_threshold, 0.5, 0.99);</pre>
</div>
<p>Then we run <a id="id161" class="indexterm"></a>the RANSAC algorithm. The last parameter <a id="id162" class="indexterm"></a>represents the number of maximum iterations for the algorithm.</p>
<div class="informalexample"><pre class="programlisting">jsfeat.motion_estimator.ransac(ransac_param, mm_kernel,
        object_xy, screen_xy, count, homo3x3, match_mask, 1000);</pre>
</div>
<p>The shape finding can be applied for both tracking.js and JSFeat libraries, you just need to set matches as <code class="literal">object_xy</code> and <code class="literal">screen_xy</code>, where those arguments must hold an array of objects with the <code class="literal">x</code> and <code class="literal">y</code> fields. After we find the transformation matrix, we compute the projected shape of an object to a new image:</p>
<div class="informalexample"><pre class="programlisting">var shape_pts = tCorners(homo3x3.data, colsObject, rowsObject);</pre>
</div>
<p>After the computation is done, we draw the computed shapes on our images:</p>
<div class="mediaobject"><img src="../Images/image00117.jpeg" alt="Finding an object location"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>As we see, our program successfully found an object in both cases. Actually, both methods can show different performance, it is mainly based on the thresholds you set.</p>
</div>
</div>


<div class="section" title="Summary" id="aid-QMFO1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec26"></a>Summary</h1>
</div>
</div>
</div>
<p>We completed one of the hardest chapters in this book. Congratulations! We saw how to find and track a basic colored object and plunged into the depths of library APIs. Oh, and don't forget, we have completed our own object detector! The applications of Computer Vision methods vary. What we cannot accomplish with the simple color detection, we achieve with powerful feature detection and descriptor matching algorithms. Both libraries provide different functionalities to match the objects and some functions are not included in the libraries. But it should not stop you from using those excellent methods. To know how and, probably the more important part, when to use those algorithms are the most crucial things you need to know.</p>
<p>One of the most commonly seen objects in our world is a person's face. We interact with people everywhere. However, we did not see how to detect such objects in an application. The algorithms we covered in this chapter are not so useful for face detection, which is why we need to introduce new methods for that. This is our topic of the next chapter. See you there!</p>
</div>
</body>
</html>