<html><head></head><body>
		<div id="_idContainer156">
			<h1 id="_idParaDest-115"><em class="italic"><a id="_idTextAnchor117"/>Chapter 6</em>: Alerting on ML Analysis </h1>
			<p>The previous chapter (<a href="B17040_05_Epub_AM.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a><em class="italic">, Interpreting Results</em>) explained in depth how anomaly detection and forecasting results are stored in Elasticsearch indices. This gives us the proper background to now create proactive, actionable, and informative alerts on those results.</p>
			<p>At the time of writing this book, we find ourselves at an inflection point. For several years, Elastic ML has relied on the alerting capabilities of Watcher (a component of Elasticsearch) as this was the exclusive mechanism to alert on data. However, a new platform of alerting has been designed as part of Kibana (and was deemed GA in v7.11) and this new approach will be the primary mechanism of alerting moving forward. </p>
			<p>There are still some interesting pieces of functionality that Watcher can provide that are not yet available in Kibana alerting. As such, this chapter will showcase the usage of alerts using both Kibana alerting and Watcher. Depending on your needs, you can decide which approach you would like to use.</p>
			<p>Specifically, this chapter will cover the following topics:</p>
			<ul>
				<li>Understanding alerting concepts</li>
				<li>Building alerts from the ML UI</li>
				<li>Creating alerts with a watch</li>
			</ul>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor118"/>Technical requirements</h1>
			<p>The information in this chapter will use the Elastic Stack as it exists in v7.12. </p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor119"/>Understanding alerting concepts</h1>
			<p>Hopefully, without <a id="_idIndexMarker384"/>running the risk of being overly pedantic, a few declarations can be made here about alerting and how certain aspects of alerting (especially with respect to anomaly detection) are extremely important to understand before we get <a id="_idIndexMarker385"/>into the mechanics of configuring those alerts. </p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor120"/>Anomalies are not necessarily alerts</h2>
			<p>This needs to be <a id="_idIndexMarker386"/>explicitly said. Often, users who first embrace anomaly detection feel compelled to alert on everything once they realize that you can alert on anomalies. This is potentially a really challenging situation if anomaly detection is deployed across hundreds, thousands, or even tens of thousands of entities. Anomaly detection, while certainly liberating users from having to define specific, rule-driven exceptions or hardcoded thresholds from alerts, also has the potential to be deployed broadly across a lot of data. We need to be cognizant that detailed alerting on every little anomaly could be potentially quite noisy if we're not careful.</p>
			<p>Fortunately, there are a few mechanisms that we've already learned about in <a href="B17040_05_Epub_AM.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a><em class="italic">, Interpreting Results</em>, that help us mitigate such a situation:</p>
			<ul>
				<li><strong class="bold">Summarization</strong>: We learned that anomalousness is not only reported for individual anomalies (at the "record level") but is also summarized at the bucket level and influencer level. These summary scores can facilitate alerting at a higher level of abstraction if we so desire.</li>
				<li><strong class="bold">Normalized</strong> <strong class="bold">scoring</strong>: Because every anomaly detection job has a custom normalization scale that is purpose-built for the specific detector configuration and dataset being analyzed, it means that we can leverage the normalized scoring that comes out of Elastic ML to rate-limit the typical alerting cadence. Perhaps for a specific job that you create, alerting at a minimum anomaly score of 10 will typically give about a dozen alerts per day, a score of 50 will give about one per day, and a score of 90 will give about one alert per week. In other words, you can effectively tune the alerting to your own tolerance for the number of alerts you'd prefer to get per unit of time (of course, except for the case of an unexpected system-wide outage, which may create more alerts than usual).</li>
				<li><strong class="bold">Correlation</strong>/<strong class="bold">combination</strong>: Perhaps alerting on a single metric anomaly (a host's CPU being abnormally high) is not as compelling as a group of related anomalies (CPU is high, free memory is low, and response time is also high). Alerting on compound events or sequences may be more meaningful for some situations.</li>
			</ul>
			<p>The bottom line is that even though there isn't a one-size-fits-all philosophy about the best way to <a id="_idIndexMarker387"/>structure alerting and increase the effectiveness of alerts, there are some options available to the user in order to choose what may be right for you.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor121"/>In real-time alerting, timing matters</h2>
			<p>Back in <a href="B17040_02_Epub_AM.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a><em class="italic">, Enabling and Operationalization</em>, we learned that anomaly detection<a id="_idIndexMarker388"/> jobs <a id="_idIndexMarker389"/>are a relatively complex orchestration of querying of raw data, analyzing that data, and reporting of the results as an ongoing process that can run in near real time. As such, there were a few key aspects of the job's configuration that determined the cadence of that process, namely the <strong class="source-inline">bucket_span</strong>, <strong class="source-inline">frequency</strong>, and <strong class="source-inline">query_delay</strong> parameters. These parameters define when results are "available" and what timestamp the values will have. This is extremely important because alerting on anomaly detection jobs will involve a subsequent query to the results indices (<strong class="source-inline">.ml-anomalies-*</strong>), and clearly, when that query is run and what time range is used matters whether or not you actually find the anomalies you are looking for.</p>
			<p>To illustrate, let's look at the following: </p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B17040_06_1.jpg" alt="Figure 6.1 – A representation of the bucket span, query delay, and frequency with respect to now&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – A representation of the bucket span, query delay, and frequency with respect to now</p>
			<p>In <em class="italic">Figure 6.1</em>, we see that a particular bucket of time (represented by the width of time equal to <em class="italic">t2-t1</em>), lags the current system time (<strong class="bold">now</strong>) by an amount equal to <strong class="source-inline">query_delay</strong>. Within the bucket, there may be subdivisions of time, as defined by the <strong class="source-inline">frequency</strong> parameter. With respect to how the results of this bucket are written to the results indices (<strong class="source-inline">.ml-anomalies-*</strong>), we should remember that the timestamp of the documents written for this bucket will all have a <strong class="source-inline">timestamp</strong> value equal to the time at <em class="italic">t1</em>, the leading edge of the bucket. </p>
			<p>To make a practical example for discussion, let's imagine the following:</p>
			<ul>
				<li><strong class="source-inline">bucket_span</strong> = 15 minutes</li>
				<li><strong class="source-inline">frequency</strong> = 15 minutes</li>
				<li><strong class="source-inline">query_delay</strong> = 2 minutes</li>
			</ul>
			<p>If <strong class="bold">now</strong> is 12:05 P.M., then <a id="_idIndexMarker390"/>the bucket corresponding to 11:45 A.M.-12:00 P.M. was <a id="_idIndexMarker391"/>queried and processed by Elastic ML sometime around 12:02 P.M. (due to the lag of <strong class="source-inline">query_delay</strong>) and the results document was written into <strong class="source-inline">.ml-anomalies-*</strong> soon after (but written with a timestamp equal to 11:45 A.M.). Therefore, if at 12:05 P.M. we looked into <strong class="source-inline">.ml-anomalies-*</strong> to see whether results were there for 11:45 A.M., we would be pretty confident they would exist, and we could inspect the content. However, if <strong class="bold">now</strong> were only 12:01 P.M., the results documents for the bucket corresponding to 11:45 A.M.-12:00 P.M. would not yet exist and wouldn't be written for another minute or so. We can see that the timing of things is very important.</p>
			<p>If in our example scenario, we instead had reduced the value of <strong class="source-inline">frequency</strong> to 7.5 minutes or 5 minutes, then we would indeed have access to the results of the bucket "sooner," but the results would be marked as <strong class="bold">interim</strong> and are subject to change when the bucket is finalized.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Interim results are created within a bucket if the frequency is a sub-multiple of the bucket span, but not all detectors make interim results. For example, if you have a <strong class="source-inline">max</strong> or <strong class="source-inline">high_count</strong> detector, then an interim result that shows a higher-than-expected value over the typical value is possible and sensible – you don't need to see the contents of the entire bucket to know that you've already exceeded expectations. However, if you have a <strong class="source-inline">mean</strong> detector, you really do need to see that entire bucket's worth of observations before determining the average value – therefore, interim results are not produced because they are not sensible.</p>
			<p>So, with that said, if we now take the diagram from <em class="italic">Figure 6.1</em> and advance time a little, but <a id="_idIndexMarker392"/>also draw the buckets before and after this one, it will look like<a id="_idIndexMarker393"/> the following:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B17040_06_2.jpg" alt="Figure 6.2 – A representation of consecutive buckets&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – A representation of consecutive buckets</p>
			<p>Here in <em class="italic">Figure 6.2</em>, we see that the current system time (again, denoted by <strong class="bold">now</strong>) is in the middle of <strong class="bold">bucket t2</strong> – therefore, <strong class="bold">bucket t2</strong> is not yet finalized and if there are any results written for <strong class="bold">bucket t2</strong> by the anomaly detection job, they will be marked with the <strong class="source-inline">is_interim:true</strong> flag as first shown in <a href="B17040_05_Epub_AM.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a><em class="italic">, Interpreting Results</em>. </p>
			<p>If we wanted to invoke an alert search that basically asked the question, "Are there any new anomalies created since last time I looked?" and the time that we invoked that search was done at the time that is <strong class="bold">now</strong> in <em class="italic">Figure 6.2</em>, then we should notice the following: </p>
			<ul>
				<li>The "look back" period of time should be about twice the width of <strong class="source-inline">bucket_span</strong>. This is because this guarantees that we will see any interim results that may be published for the current bucket (here <strong class="bold">bucket t2</strong>) and any finalized results for the previous bucket (here <strong class="bold">bucket t1</strong>). Results from <strong class="bold">bucket t0</strong> will not be matched because the timestamp for <strong class="bold">bucket t0</strong> is outside of the window of time queried – this is okay as long as we get the alert query to repeat on a proper schedule (see the following point).</li>
				<li>The time chosen to run this query could fall practically anywhere within bucket <em class="italic">t2</em>'s window of time and this will still work as described. This is important because the schedule at which the alert query runs will likely be asynchronous to the schedule that the anomaly detection job is operating (and writing results).</li>
				<li>We would likely schedule our alert search to repeat its operation at most at an interval equal to <strong class="source-inline">bucket_span</strong>, but it could be executed more frequently if we're interested in catching interim anomalies in the current, not-yet-fiinalized bucket.</li>
				<li>If we didn't want to consider interim results, we would need to modify the query such that <strong class="source-inline">is_interim:false</strong> was part of the query logic to not match them.</li>
			</ul>
			<p>Given all of<a id="_idIndexMarker394"/> these conditions, you might think that there is some type of<a id="_idIndexMarker395"/> dark magic that is required to get this working correctly and reliably. Fortunately, when we build the alerts using Kibana from the Elastic ML UI, these considerations are taken care of for you. However, if you feel like you are a wizard and fully understand how this all works, then you may not be too intimidated by the prospect of building very custom alert conditions using Watcher, where you will have complete control.</p>
			<p>In the following main sections, we'll do some examples using each method so that you can compare and contrast how they work.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor122"/>Building alerts from the ML UI</h1>
			<p>With the<a id="_idIndexMarker396"/> release<a id="_idIndexMarker397"/> of v7.12, Elastic ML changed its default alert handler from Watcher to Kibana alerting. Prior to v7.12, the user had a choice of accepting a default <strong class="bold">watch</strong> (an instance of a script for Watcher) if alerting was selected from the ML UI, or the user could create a watch from scratch. This section will focus on the new workflow using Kibana alerting as of v7.12, which offers a nice balance of flexibility and ease of use. </p>
			<p>To create a working, illustrative example of real-time alerting, we will contrive a scenario using the Kibana sample web logs dataset that we first used in <a href="B17040_03_Epub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a><em class="italic">, Anomaly Detection. </em></p>
			<p>The process outlined in this section will be as follows:</p>
			<ol>
				<li>Define some sample anomaly detection jobs on the sample data.</li>
				<li>Define two alerts on two of the anomaly detection jobs.</li>
				<li>Run a simulation of anomalous behavior, to catch that behavior in an alert.</li>
			</ol>
			<p>Let's <a id="_idIndexMarker398"/>first <a id="_idIndexMarker399"/>define the sample anomaly detection jobs.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor123"/>Defining sample anomaly detection jobs</h2>
			<p>Of course, before<a id="_idIndexMarker400"/> we can build alerts, we need jobs running in real time. We can leverage the sample ML jobs that come with the same Kibana web logs dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you still have this dataset loaded in your cluster, you should delete it and re-add it. This will reset the timestamps on the dataset so that about half of the data will be in the past and the rest will be in the future. Having some data in the future will allow us to pretend that data is appearing in real time and therefore our real-time anomaly detection jobs and our alerts on those jobs will act like they are truly real time. </p>
			<p>To get started, let's reload the sample data and build some sample jobs:</p>
			<ol>
				<li value="1">From the Kibana home screen, click on <strong class="bold">Try our sample data</strong>:<div id="_idContainer137" class="IMG---Figure"><img src="image/B17040_06_3.jpg" alt="Figure 6.3 – Kibana home screen&#13;&#10;"/></div><p class="figure-caption">Figure 6.3 – Kibana home screen</p></li>
				<li>Click <a id="_idIndexMarker401"/>on <strong class="bold">Index Patterns </strong>in the <strong class="bold">Sample web logs</strong> section (if already loaded, please remove and re-add):<div id="_idContainer138" class="IMG---Figure"><img src="image/B17040_06_4.jpg" alt="Figure 6.4 – Add sample web logs data&#13;&#10;"/></div><p class="figure-caption">Figure 6.4 – Add sample web logs data</p></li>
				<li>Under the <strong class="bold">View data</strong> menu, select <strong class="bold">ML jobs</strong> to create some sample jobs:<div id="_idContainer139" class="IMG---Figure"><img src="image/B17040_06_5.jpg" alt="Figure 6.5 – Selecting to create some sample ML jobs&#13;&#10;"/></div><p class="figure-caption">Figure 6.5 – Selecting to create some sample ML jobs</p></li>
				<li>Give the <a id="_idIndexMarker402"/>three sample jobs a job ID prefix (here <strong class="source-inline">alert-demo-</strong> was chosen) and make sure you de-select Use full <strong class="source-inline">kibana_sample_data_logs</strong> data and pick the end time to be the closest 15 minutes to your current system time (in your time zone):<div id="_idContainer140" class="IMG---Figure"><img src="image/B17040_06_6.jpg" alt="Figure 6.6 – Naming the sample jobs with a prefix and selecting now as the end time&#13;&#10;"/></div><p class="figure-caption">Figure 6.6 – Naming the sample jobs with a prefix and selecting now as the end time</p></li>
				<li>Notice<a id="_idIndexMarker403"/> in <em class="italic">Figure 6.6</em> that <strong class="bold">Apr 8, 2021 @ 11:00:00.00</strong> was chosen as the end time and that a date of 11 days earlier (<strong class="bold">Mar 28, 2021 @ 00:00:00.00</strong>) was chosen as the start time (the sample data goes back about 11 days from when you install it). The current local time at the time of this screenshot was 11:10 A.M. on April 8th. This is important in the spirit of trying to make this sample data seem real time. Click the <strong class="bold">Create Jobs</strong> button to set the job creation in motion. Once the jobs are created, you will see the following screen:<div id="_idContainer141" class="IMG---Figure"><img src="image/B17040_06_7.jpg" alt="Figure 6.7 – Sample jobs completed initial run&#13;&#10;"/></div><p class="figure-caption">Figure 6.7 – Sample jobs completed initial run</p></li>
				<li>We don't need<a id="_idIndexMarker404"/> to view the results just yet. Instead, we need to make sure these three jobs are running in real time. Let's click <strong class="bold">Anomaly Detection</strong> at the top to return us to the <strong class="bold">Job</strong> <strong class="bold">Management</strong> page. There we can see our three jobs have analyzed some data but are now in the closed state with the data feeds currently stopped:<div id="_idContainer142" class="IMG---Figure"><img src="image/B17040_06_8.jpg" alt="Figure 6.8 – Sample jobs in the Jobs Management screen&#13;&#10;"/></div><p class="figure-caption">Figure 6.8 – Sample jobs in the Jobs Management screen</p></li>
				<li>Now we need to enable these three jobs to run in real time. Click the boxes next to each job, and then select the gear icon to bring up the menu to choose <strong class="bold">Start data feeds</strong> for<a id="_idIndexMarker405"/> all three jobs:<div id="_idContainer143" class="IMG---Figure"><img src="image/B17040_06_9.jpg" alt="Figure 6.9 – Starting the datafeed for all three sample jobs&#13;&#10;"/></div><p class="figure-caption">Figure 6.9 – Starting the datafeed for all three sample jobs</p></li>
				<li>In the pop-up window, choose the top option for both <strong class="bold">Search start time</strong> and <strong class="bold">Search end time</strong>, ensuring that the job will continue to run in real time. For now, we will leave <strong class="bold">Create alert after datafeed has started</strong> unchecked as we will create our own alerts in just a moment:<div id="_idContainer144" class="IMG---Figure"><img src="image/B17040_06_10.jpg" alt="Figure 6.10 – Starting the data feeds of the three sample jobs to run in real time&#13;&#10;"/></div><p class="figure-caption">Figure 6.10 – Starting the data feeds of the three sample jobs to run in real time</p></li>
				<li>After<a id="_idIndexMarker406"/> clicking the <strong class="bold">Start</strong> button, we will see that our three jobs are now in the opened/started state:</li>
			</ol>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B17040_06_11.jpg" alt="Figure 6.11 – Sample jobs now running in real time&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11 – Sample jobs now running in real time</p>
			<p>Now that we have our jobs up and running, let's now define a few alerts against them.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor124"/>Creating alerts against the sample jobs</h2>
			<p>With our<a id="_idIndexMarker407"/> jobs running in real time, we can now define some alerts for our jobs: </p>
			<ol>
				<li value="1">For the <strong class="source-inline">alert-demo-response_code_rates</strong> job, click the <strong class="bold">…</strong> icon and select <strong class="bold">Create alert</strong>:<div id="_idContainer146" class="IMG---Figure"><img src="image/B17040_06_12.jpg" alt="Figure 6.12 – Creating an alert for a sample job&#13;&#10;"/></div><p class="figure-caption">Figure 6.12 – Creating an alert for a sample job</p></li>
				<li>Now the <strong class="bold">Create alert</strong> flyout window appears, and we can now begin to fill in our desired alert configuration:<div id="_idContainer147" class="IMG---Figure"><img src="image/B17040_06_13.jpg" alt="Figure 6.13 – Creating an alert configuration&#13;&#10;"/></div><p class="figure-caption">Figure 6.13 – Creating an alert configuration</p></li>
				<li>In <em class="italic">Figure 6.13</em>, we <a id="_idIndexMarker408"/>will name our alert, but will also define that we wish to have this alert check for anomalies every 10 minutes. This job's <strong class="source-inline">bucket_span</strong> is set for 1 hour, but the frequency is set to 10 minutes – therefore interim results will be available much sooner than the full bucket time. This is also why we chose to include interim results in our alert configuration, so that we can get notified as soon as possible. We also set <strong class="bold">Result type</strong> to be of type <strong class="bold">Bucket</strong> to give us a summarized treatment of the anomalousness, as previously discussed. Finally, we set the severity threshold to <strong class="bold">51</strong> to have alerts be generated only for anomalies of a score exceeding that value.</li>
				<li>Before we continue too far, we can check the alert configuration on past data. Putting <strong class="source-inline">30d</strong> into the test box, we can see that there was only one other alert in the <a id="_idIndexMarker409"/>last 30 days' worth of data that matched this alert condition:<div id="_idContainer148" class="IMG---Figure"><img src="image/B17040_06_14.jpg" alt="Figure 6.14 – Testing alert configuration on past data&#13;&#10;"/></div><p class="figure-caption">Figure 6.14 – Testing alert configuration on past data</p></li>
				<li>Lastly, we can configure an action to invoke on an alert being fired. In this case, our system was pre-configured to use Slack as an alert action, so we will choose that here, but there are many other options available for the user to consider (please see <a href="https://www.elastic.co/guide/en/kibana/current/action-types.html">https://www.elastic.co/guide/en/kibana/current/action-types.html</a> to explore all options available and how to customize the alert messaging):<div id="_idContainer149" class="IMG---Figure"><img src="image/B17040_06_15.jpg" alt="Figure 6.15 – Configuring alert action&#13;&#10;"/></div><p class="figure-caption">Figure 6.15 – Configuring alert action</p></li>
				<li>Clicking <a id="_idIndexMarker410"/>on the <strong class="bold">Save</strong> button will obviously save the alert, which is then viewable and modifiable via the <strong class="bold">Stack Management | Alerts and Actions</strong> area of Kibana:<div id="_idContainer150" class="IMG---Figure"><img src="image/B17040_06_16.jpg" alt="Figure 6.16 – Alerts management&#13;&#10;"/></div><p class="figure-caption">Figure 6.16 – Alerts management</p></li>
				<li>We are<a id="_idIndexMarker411"/> going to create one more alert, for the <strong class="source-inline">alert-demo-url_scanning</strong> job. This time, we'll create a <strong class="bold">Record</strong> alert, but with the other configuration parameters similar to the prior example:</li>
			</ol>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B17040_06_17.jpg" alt="Figure 6.17 – Configuring another alert on the URL scanning job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17 – Configuring another alert on the URL scanning job</p>
			<p>Now that we <a id="_idIndexMarker412"/>have our two alerts configured, let's move on to simulating an actually anomalous situation in real time to trigger our alerts.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor125"/>Simulating some real-time anomalous behavior</h2>
			<p>Triggering <a id="_idIndexMarker413"/>simulated anomalous behavior in the context of these sample web logs is a little tricky, but not too hard. It will involve some usage of the Elasticsearch APIs, executing a few<a id="_idIndexMarker414"/> commands via <strong class="bold">the Dev Tools console</strong> in Kibana. Console is where you can issue API calls to Elasticsearch and see the output (response) of those API calls. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you are unfamiliar with Console, please consult <a href="https://www.elastic.co/guide/en/kibana/current/console-kibana.html">https://www.elastic.co/guide/en/kibana/current/console-kibana.html</a>.</p>
			<p>What we will be simulating is twofold – we'll inject several fake documents into the index that the anomaly detection job is monitoring, and then wait for the alert to fire. These documents will show a spike in requests from a fictitious IP address of <strong class="source-inline">0.0.0.0</strong> that will result in <a id="_idIndexMarker415"/>a response code of <strong class="source-inline">404</strong> and will also be requesting random URL paths.</p>
			<p>Let's get started:</p>
			<ol>
				<li value="1">We need to determine the current time for you in UTC. We must know the UTC time (as opposed to your local time zone's time) because the documents stored in the Elasticsearch index are stored in UTC. To determine this, you can simply use an online tool (such as Googling <strong class="source-inline">current time utc</strong>). At the time of writing, the current UTC time is 4:41 P.M. on April 8, 2021. Converted into the format that Elasticsearch expects for the <strong class="source-inline">kibana_sample_data_logs</strong> index, this will take the form of this:<p class="source-code">  "timestamp": "2021-04-08T16:41:00.000Z"</p></li>
				<li>Let's now insert some new bogus documents into the <strong class="source-inline">kibana_sample_data_logs</strong> index at the current time (perhaps with a little buffer – rounding up to the next half hour, in this case to 17:00). Replace the <strong class="source-inline">timestamp</strong> field value accordingly and invoke the following command <em class="italic">at least 20 times</em> in the Dev Tools console to insert:<p class="source-code">   POST kibana_sample_data_logs/_doc</p><p class="source-code">   {</p><p class="source-code">     "timestamp": "2021-04-08T17:00:00.000Z",</p><p class="source-code">     "event.dataset" : "sample_web_logs",</p><p class="source-code">     "clientip": "0.0.0.0",</p><p class="source-code">     "response" : "404",</p><p class="source-code">     "url": ""</p><p class="source-code">   }</p></li>
				<li>We can then dynamically modify only the documents we just inserted (in particular, the <strong class="source-inline">url</strong> field) to simulate that the URLs are all unique by using a little script to <a id="_idIndexMarker416"/>randomize the field value in an <strong class="source-inline">_update_by_query</strong> API call:<p class="source-code">   POST kibana_sample_data_logs/_update_by_query</p><p class="source-code">   {</p><p class="source-code">     "query": {</p><p class="source-code">       "term": {</p><p class="source-code">         "clientip": {</p><p class="source-code">           "value": "0.0.0.0"</p><p class="source-code">         }</p><p class="source-code">       }</p><p class="source-code">     },</p><p class="source-code">     "script": {</p><p class="source-code">       "lang": "painless",</p><p class="source-code">       "source": "ctx._source.url = '/path/to/'+ UUID.randomUUID().toString();"</p><p class="source-code">     }</p><p class="source-code">   }</p></li>
				<li>We can validate that we have correctly created a bunch of unique, random requests from our bogus IP address by looking that the appropriate time in Kibana Discover:<div id="_idContainer152" class="IMG---Figure"><img src="image/B17040_06_18.jpg" alt="Figure 6.18 – Our contrived burst of anomalous events shown in Discover&#13;&#10;"/></div><p class="figure-caption">Figure 6.18 – Our contrived burst of anomalous events shown in Discover</p></li>
				<li>Notice<a id="_idIndexMarker417"/> in <em class="italic">Figure 6.18</em> that we had to peek into the future a little to see the documents we artificially inserted (as the red vertical line in the timeline near 12:45 P.M. is the actual current system time in the local time zone). Also notice that our inserted documents have a nice-looking random <strong class="source-inline">url</strong> field as well. Now that we have "laid the trap" for anomaly detection to find, and we have our alerts ready, we must now sit back and patiently wait for the alerts to trigger.</li>
			</ol>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor126"/>Receiving and reviewing the alerts</h2>
			<p>Since the<a id="_idIndexMarker418"/> anomalous <a id="_idIndexMarker419"/>behavior we inserted is now waiting to be found by our anomaly detection job and our alerts, we can contemplate when we should expect to see that alert. We should recognize that given our jobs have bucket spans of 1 hour, frequencies of 10 minutes, and query delays on the order of 1-2 minutes (and that our alerts will indeed look for interim results – and that our alerts are running with a 10-minute frequency that is asynchronous from the anomaly detection job), we should expect to see our alerts between 1:12 P.M. and 1:20 P.M. local time. </p>
			<p>Right on cue, the alert messages for the two jobs surface in Slack at 1:16 P.M. and 1:18 P.M. local time:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B17040_06_19.jpg" alt="Figure 6.19 – Alerts received in the Slack client&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.19 – Alerts received in the Slack client</p>
			<p>The<a id="_idIndexMarker420"/> top<a id="_idIndexMarker421"/> alert in <em class="italic">Figure 6.19</em>, of course, is for the anomaly detection job that was counting the number of events for each <strong class="source-inline">response.keyword</strong> (and thus seeing the spike of 404 documents exceeds expectations) and the bottom alert is for the other job that notices the high distinct count of unique URLs that were being requested. Notice that both jobs correctly identify <strong class="source-inline">clientip = 0.0.0.0</strong> as an influencer into the anomalies. Included in the alert text is the ability to follow the link to directly view the information in <strong class="bold">Anomaly</strong> <strong class="bold">Explorer</strong>. In <em class="italic">Figure</em> <em class="italic">6.20</em>, we can see that by following the link in the second alert, we arrive at a familiar place to investigate the anomaly further:</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B17040_06_20.jpg" alt="Figure 6.20 – Anomaly Explorer from the alert drill-down link&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.20 – Anomaly Explorer from the alert drill-down link</p>
			<p>Hopefully, through <a id="_idIndexMarker422"/>this example you can see not only how to use the alerts<a id="_idIndexMarker423"/> using the Kibana alerting framework on anomaly detection jobs but can now also appreciate the intricacies of the real-time operation of both the job and the alert. The settings within the job's datafeed and alert sampling interval truly affect how real-time the alerts can be. We could have, for example, reduced both the datafeed <strong class="source-inline">frequency</strong> and the alert's <strong class="bold">Check every</strong> setting to shave a few minutes off. </p>
			<p>In the next section, we won't attempt to replicate a real-time alert detection with Watcher, but we will work to understand the equivalent settings within a watch to accomplish what we need to interface Watcher to an anomaly detection job and to also showcase some interesting example watches.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor127"/>Creating an alert with a watch</h1>
			<p>Prior to <a id="_idIndexMarker424"/>version 7.12, Watcher was used as the mechanism to <a id="_idIndexMarker425"/>alert on anomalies found by Elastic ML. <strong class="bold">Watcher</strong> is a very flexible native plugin for Elasticsearch that can handle a number of automation tasks and alerting is certainly one of them. In versions 7.11 and earlier, users could either create their<a id="_idIndexMarker426"/> own <strong class="bold">watch</strong> (an instance of an automation task in Watcher) from scratch to alert on anomaly detection job results or opt to use a default watch template that was created for them by the Elastic ML UI. We will first look at the default watch that was provided and then will discuss some ideas around custom watches.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor128"/>Understanding the anatomy of the legacy default ML watch </h2>
			<p>Now that alerting<a id="_idIndexMarker427"/> on anomaly detection jobs is handled by the new Kibana alerting framework, the legacy watch default template (plus a few other examples) are memorialized in a GitHub repository here: <a href="https://github.com/elastic/examples/tree/master/Alerting/Sample%20Watches/ml_examples">https://github.com/elastic/examples/tree/master/Alerting/Sample%20Watches/ml_examples</a>.</p>
			<p>In dissecting the default ML watch (<strong class="source-inline">default_ml_watch.json</strong>) and the companion version, which has an email action (<strong class="source-inline">default_ml_watch_email.json</strong>), we see that there are four main sections:</p>
			<ul>
				<li><strong class="bold">trigger</strong>: Defines the scheduling of the watch </li>
				<li><strong class="bold">input</strong>: Specifies the input data to be evaluated</li>
				<li><strong class="bold">condition</strong>: Assesses whether or not the <strong class="source-inline">actions</strong> section is executed</li>
				<li><strong class="bold">actions</strong>: Lists the desired actions to take if the watch's condition is met<p class="callout-heading">Note</p><p class="callout">For a full explanation of all of the options of Watcher, please consult the Elastic documentation at <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/how-watcher-works.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/how-watcher-works.html</a>.</p></li>
			</ul>
			<p>Let's discuss each section in depth.</p>
			<h3>The trigger section</h3>
			<p>In the <a id="_idIndexMarker428"/>default ML watch, the <strong class="source-inline">trigger</strong> section<a id="_idIndexMarker429"/> is defined as follows:</p>
			<p class="source-code">    "trigger": {</p>
			<p class="source-code">      "schedule": {</p>
			<p class="source-code">        "interval": "82s"</p>
			<p class="source-code">      }</p>
			<p class="source-code">    },</p>
			<p>Here, we can see that the interval at which the watch will fire in real time is every 82 seconds. This usually should be a random value between 60 and 120 seconds so that if a node restarts, all of the watches will not be synchronized, and they will have their execution times more evenly spread out to reduce any potential load on the cluster. It is also important that this interval value is less than or equal to the bucket span of the job. As explained earlier in this chapter, having it larger than the bucket span may cause recently written anomaly records to be missed by the watch. With the interval being less (or even much less) than the bucket span of the job, you can also take advantage of the advanced notification that is available when there are interim results, anomalies that can still be determined despite not having seen all of the data within a bucket span. </p>
			<h3>The input section</h3>
			<p>The <strong class="source-inline">input</strong> section<a id="_idIndexMarker430"/> starts with a <strong class="source-inline">search</strong> section <a id="_idIndexMarker431"/>in which the following <strong class="source-inline">query</strong> is defined against the <strong class="source-inline">.ml-anomalies-*</strong> index pattern:</p>
			<p class="source-code">            "query": {</p>
			<p class="source-code">              "bool": {</p>
			<p class="source-code">                "filter": [</p>
			<p class="source-code">                  {</p>
			<p class="source-code">                    "term": {</p>
			<p class="source-code">                      "job_id": "&lt;job_id&gt;"</p>
			<p class="source-code">                    }</p>
			<p class="source-code">                  },</p>
			<p class="source-code">                  {</p>
			<p class="source-code">                    "range": {</p>
			<p class="source-code">                      "timestamp": {</p>
			<p class="source-code">                        "gte": "now-30m"</p>
			<p class="source-code">                      }</p>
			<p class="source-code">                    }</p>
			<p class="source-code">                  },</p>
			<p class="source-code">                  {</p>
			<p class="source-code">                    "terms": {</p>
			<p class="source-code">                      "result_type": [</p>
			<p class="source-code">                        "bucket",</p>
			<p class="source-code">                        "record",</p>
			<p class="source-code">                        "influencer"</p>
			<p class="source-code">                      ]</p>
			<p class="source-code">                    }</p>
			<p class="source-code">                  }</p>
			<p class="source-code">                ]</p>
			<p class="source-code">              }</p>
			<p class="source-code">            },</p>
			<p>Here, we <a id="_idIndexMarker432"/>are<a id="_idIndexMarker433"/> asking Watcher to query for <strong class="source-inline">bucket</strong>, <strong class="source-inline">record</strong>, and <strong class="source-inline">influencer</strong> result documents for a job (you would replace <strong class="source-inline">&lt;job_id&gt;</strong> with the actual <strong class="source-inline">job_id</strong> for the anomaly detection job of interest) in the last 30 minutes. As we know from earlier in the chapter, this look-back window should be twice the <strong class="source-inline">bucket_span</strong> value of the ML job (this template must assume that the job's bucket span is 15 minutes). While all result types were asked for, we will later see that only the bucket-level results are used to evaluate whether or not to create an alert.</p>
			<p>Next comes a series of three aggregations. When they're collapsed, they look as follows:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B17040_06_21.jpg" alt="Figure 6.21 – Query aggregations in the watch input&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.21 – Query aggregations in the watch input</p>
			<p>The <strong class="source-inline">bucket_results</strong> aggregation<a id="_idIndexMarker434"/> first filters for buckets<a id="_idIndexMarker435"/> where the anomaly score is greater than or equal to 75:</p>
			<p class="source-code">             "aggs": {</p>
			<p class="source-code">               "bucket_results": {</p>
			<p class="source-code">                 "filter": {</p>
			<p class="source-code">                   "range": {</p>
			<p class="source-code">                     "anomaly_score": {</p>
			<p class="source-code">                       "gte": 75</p>
			<p class="source-code">                      }</p>
			<p class="source-code">                  }</p>
			<p class="source-code">               },</p>
			<p>Then, a sub-aggregation asks for the top 1 bucket sorted by <strong class="source-inline">anomaly_score</strong>:</p>
			<p class="source-code">                "aggs": {</p>
			<p class="source-code">                  "top_bucket_hits": {</p>
			<p class="source-code">                    "top_hits": {</p>
			<p class="source-code">                      "sort": [</p>
			<p class="source-code">                        {</p>
			<p class="source-code">                          "anomaly_score": {</p>
			<p class="source-code">                            "order": "desc"</p>
			<p class="source-code">                          }</p>
			<p class="source-code">                        }</p>
			<p class="source-code">                      ],</p>
			<p class="source-code">                      "_source": {</p>
			<p class="source-code">                        "includes": [</p>
			<p class="source-code">                          "job_id",</p>
			<p class="source-code">                          "result_type",</p>
			<p class="source-code">                          "timestamp",</p>
			<p class="source-code">                          "anomaly_score",</p>
			<p class="source-code">                          "is_interim"</p>
			<p class="source-code">                        ]</p>
			<p class="source-code">                      },</p>
			<p class="source-code">                      "size": 1,</p>
			<p class="source-code">                      </p>
			<p>Next, still<a id="_idIndexMarker436"/> within <a id="_idIndexMarker437"/>the <strong class="source-inline">top_bucket_hits</strong> sub-aggregation, there are a series of defined <strong class="source-inline">script_fields</strong>:</p>
			<p class="source-code">                      "script_fields": {</p>
			<p class="source-code">                        "start": {</p>
			<p class="source-code">                          "script": {</p>
			<p class="source-code">                            "lang": "painless",</p>
			<p class="source-code">                            "source": "LocalDateTime.ofEpochSecond((doc[\"timestamp\"].value.getMillis()-((doc[\"bucket_span\"].value * 1000)\n * params.padding)) / 1000, 0, ZoneOffset.UTC).toString()+\":00.000Z\"",</p>
			<p class="source-code">                            "params": {</p>
			<p class="source-code">                              "padding": 10</p>
			<p class="source-code">                            }</p>
			<p class="source-code">                          }</p>
			<p class="source-code">                        },</p>
			<p class="source-code">                        "end": {</p>
			<p class="source-code">                          "script": {</p>
			<p class="source-code">                            "lang": "painless",</p>
			<p class="source-code">                            "source": "LocalDateTime.ofEpochSecond((doc[\"timestamp\"].value.getMillis()+((doc[\"bucket_span\"].value * 1000)\n * params.padding)) / 1000, 0, ZoneOffset.UTC).toString()+\":00.000Z\"",</p>
			<p class="source-code">                            "params": {</p>
			<p class="source-code">                              "padding": 10</p>
			<p class="source-code">                            }</p>
			<p class="source-code">                          }</p>
			<p class="source-code">                        },</p>
			<p class="source-code">                        "timestamp_epoch": {</p>
			<p class="source-code">                          "script": {</p>
			<p class="source-code">                            "lang": "painless",</p>
			<p class="source-code">                            "source": """doc["timestamp"].value.getMillis()/1000"""</p>
			<p class="source-code">                          }</p>
			<p class="source-code">                        },</p>
			<p class="source-code">                        "timestamp_iso8601": {</p>
			<p class="source-code">                          "script": {</p>
			<p class="source-code">                            "lang": "painless",</p>
			<p class="source-code">                            "source": """doc["timestamp"].value"""</p>
			<p class="source-code">                          }</p>
			<p class="source-code">                        },</p>
			<p class="source-code">                        "score": {</p>
			<p class="source-code">                          "script": {</p>
			<p class="source-code">                            "lang": "painless",</p>
			<p class="source-code">                            "source": """Math.round(doc["anomaly_score"].value)"""</p>
			<p class="source-code">                          }</p>
			<p class="source-code">                        }</p>
			<p class="source-code">                      }</p>
			<p>These <a id="_idIndexMarker438"/>newly <a id="_idIndexMarker439"/>defined variables will be used by the watch to provide more functionality and context. Some of the variables are merely reformatting values (<strong class="source-inline">score</strong> is just a rounded version of <strong class="source-inline">anomaly_score</strong>), while <strong class="source-inline">start</strong> and <strong class="source-inline">end</strong> will later fill a functional role by defining a start and end time that is equal to +/- 10 bucket spans from the time of the anomalous bucket. This is later used by the UI to show an appropriate contextual time range before and after the anomalous bucket so that the user can see things more clearly.</p>
			<p>The <strong class="source-inline">influencer_results</strong> and <strong class="source-inline">record_results</strong> aggregations ask for the top three influencer scores and record scores, but only the output of the <strong class="source-inline">record_results</strong> aggregation is used in subsequent parts of the watch (and only in the <strong class="source-inline">action</strong> section of <strong class="source-inline">default_ml_watch_email.json</strong>, which contains some default email text).</p>
			<h3>The condition section</h3>
			<p>The <strong class="source-inline">condition</strong> section<a id="_idIndexMarker440"/> is where <strong class="source-inline">input</strong> is evaluated<a id="_idIndexMarker441"/> to see whether or not the <strong class="source-inline">action</strong> section is executed. In this case, the <strong class="source-inline">condition</strong> section is as follows:</p>
			<p class="source-code">    "condition": {</p>
			<p class="source-code">      "compare": {</p>
			<p class="source-code">        "ctx.payload.aggregations.bucket_results.doc_count": {</p>
			<p class="source-code">          "gt": 0</p>
			<p class="source-code">        }</p>
			<p class="source-code">      }</p>
			<p class="source-code">    },</p>
			<p>We are using this to check whether the <strong class="source-inline">bucket_results</strong> aggregation returned any documents (where <strong class="source-inline">doc_count</strong> is greater than 0). In other words, if the <strong class="source-inline">bucket_results</strong> aggregation did indeed return non-zero results, that indicates that there were indeed documents where <strong class="source-inline">anomaly_score</strong> was greater than 75. If true, then the <strong class="source-inline">action</strong> section will be invoked.</p>
			<h3>The action section</h3>
			<p>The <strong class="source-inline">action</strong> section <a id="_idIndexMarker442"/>of our default watches has two <a id="_idIndexMarker443"/>parts in our case: one <strong class="source-inline">log</strong> action for logging information to a file and a <strong class="source-inline">send_email</strong> action for sending an email. The text of the watch won't be repeated here for brevity (it is a lot of text). The <strong class="source-inline">log</strong> action will print a message to an output file, which by default is the Elasticsearch log file. Notice that the syntax of the message is using the templating language called <strong class="bold">Mustache</strong> (named because of its prolific usage of curly braces). Simply put, variables contained in Mustache's double curly braces will be substituted with their actual values. As a result, for one of the sample jobs we created earlier in the chapter, the logging text written out to the file may look as follows:</p>
			<p class="source-code">Alert for job [alert-demo-response_code_rates] at [2021-04-08T17:00:00.000Z] score [91] </p>
			<p>This alert should look familiar to what we saw in our Slack message earlier in the chapter – of course, because it is derived from the same information. The email version of the <a id="_idIndexMarker444"/>action <a id="_idIndexMarker445"/>may look as follows:</p>
			<p class="source-code">Elastic Stack Machine Learning Alert</p>
			<p class="source-code">    Job: alert-demo-response_code_rates</p>
			<p class="source-code">    Time: 2021-04-08T17:00:00.000Z</p>
			<p class="source-code">    Anomaly score: 91</p>
			<p class="source-code">    Click here to open in Anomaly Explorer.</p>
			<p class="source-code">    Top records:</p>
			<p class="source-code">    count() [91]</p>
			<p>It is obvious that the format of the alert HTML is really oriented not around getting the user a summary of the information but enticing the user to investigate further by clicking on the link within the email. </p>
			<p>Also, it is notable that the top three records are reported in the text of the email response. In our example case, there is only one record (a <strong class="source-inline">count</strong> detector with a score of 91). This section of information came from the <strong class="source-inline">record_results</strong> aggregation we described previously in the <strong class="source-inline">input</strong> section of the watch.</p>
			<p>This default watch is a good, usable alert that provides summarized information about the unusualness of the dataset over time, but it is also good to understand the implications of using this:</p>
			<ul>
				<li>The main condition for firing the alert is a bucket anomaly score above a certain value. Therefore, it would not alert on individual anomalous records within a bucket in the case where their score does not lift the overall bucket score above the stated threshold.</li>
				<li>By default, only a maximum of the top three record scores in the bucket are reported in the output, and only in the email version.</li>
				<li>The only action in these examples is logging and email. Adding other actions (Slack message, webhook, and so on) would require manually editing the watch.</li>
			</ul>
			<p>Knowing this<a id="_idIndexMarker446"/> information, it may become necessary <a id="_idIndexMarker447"/>at some point to create a more full- featured, complex watch to fully customize the behavior and output of the watch. In the next section, we'll discuss some more examples of creating a watch from scratch.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor129"/>Custom watches can offer some unique functionality</h2>
			<p>For those who<a id="_idIndexMarker448"/> feel emboldened and want to dig deeper into some advanced aspects of Watcher, let's look at some highlights from a few of the other samples in the GitHub repository. These include examples of querying the results of multiple jobs at once, programmatically combining the anomaly scores, and dynamically gathering additional potential root-cause evidence of other anomalies correlated in time.</p>
			<h3>Chained inputs and scripted conditions</h3>
			<p>A nice<a id="_idIndexMarker449"/> example <a id="_idIndexMarker450"/>of an interesting watch is <strong class="source-inline">multiple_jobs_watch.json</strong>, which shows the ability to do a chained input (doing multiple queries against the results of multiple jobs) but also executing a more dynamic condition using a script:</p>
			<p class="source-code">  "condition" : {</p>
			<p class="source-code">    "script" : {</p>
			<p class="source-code">// return true only if the combined weighted scores are greater than 75</p>
			<p class="source-code">      "source" : "return ((ctx.payload.job1.aggregations.max_anomaly_score.value * 0.5) + (ctx.payload.job2.aggregations.max_anomaly_score.value * 0.2) + (ctx.payload.job3.aggregations.max_anomaly_score.value * 0.1)) &gt; 75"</p>
			<p class="source-code">    }</p>
			<p class="source-code">  },</p>
			<p>This is basically saying that the alert only gets triggered if the combined weighted anomaly <a id="_idIndexMarker451"/>scores of the three different jobs are greater <a id="_idIndexMarker452"/>than a value of 75. In other words, not every job is considered equally important, and the weighting takes that into account.</p>
			<h3>Passing information between chained inputs</h3>
			<p>Another<a id="_idIndexMarker453"/> unique aspect of chained inputs is that information gleaned from one input chain can be passed along to another. As shown in <strong class="source-inline">chained_watch.json</strong>, the second and third input chains use the <strong class="source-inline">timestamp</strong> value learned from the first query as part of the <strong class="source-inline">range</strong> filter:</p>
			<p class="source-code">{ "range": { "timestamp": {"gte": "{{ctx.payload.job1.hits.hits.0._source.timestamp}}||-{{ctx.metadata.lookback_window}}", "lte": "{{ctx.payload.job1.hits.hits.0._source.timestamp}}"}}},</p>
			<p>This effectively means that the watch is gathering anomalies as evidence culled from a window of time prior to a presumably important anomaly from the first job. This kind of alert aligns nicely to the situation we'll discuss in <a href="B17040_07_Epub_AM.xhtml#_idTextAnchor131"><em class="italic">Chapter 7</em></a><em class="italic">, AIOps and Root Cause Analysis</em>, in which a real application problem is solved by looking for correlated anomalies in a window of time around the anomaly of a KPI. Therefore, a sample output of this watch could look like this:</p>
			<p class="source-code">[CRITICAL] Anomaly Alert for job it_ops_kpi: score=85.4309 at 2021-02-08 15:15:00 UTC</p>
			<p class="source-code">Possibly influenced by these other anomalous metrics (within the prior 10 minutes):</p>
			<p class="source-code">job:it_ops_network: (anomalies with at least a record score of 10):</p>
			<p class="source-code">field=In_Octets: score=11.217614808972602, value=13610.62255859375 (typical=855553.8944717721) at 2021-02-08 15:15:00 UTC</p>
			<p class="source-code">field=Out_Octets: score=17.00518, value=1.9079535783333334E8 (typical=1116062.402864764) at 2021-02-08 15:15:00 UTC</p>
			<p class="source-code">field=Out_Discards: score=72.99199, value=137.04444376627606 (typical=0.012289061361553099) at 2021-02-08 15:15:00 UTC</p>
			<p class="source-code">job:it_ops_sql: (anomalies with at least a record score of 5):</p>
			<p class="source-code">hostname=dbserver.acme.com field=SQLServer_Buffer_Manager_Page_life_expectancy: score=6.023424, value=846.0000000000005 (typical=12.609336298838242) at 2021-02-08 15:10:00 UTC</p>
			<p class="source-code">hostname=dbserver.acme.com field=SQLServer_Buffer_Manager_Buffer_cache_hit_ratio: score=8.337633, value=96.93249340057375 (typical=98.93088463835487) at 2021-02-08 15:10:00 UTC</p>
			<p class="source-code">hostname=dbserver.acme.com field=SQLServer_General_Statistics_User_Connections: score=27.97728, value=168.15000000000006 (typical=196.1486370757187) at 2021-02-08 15:10:00 UTC</p>
			<p>Here, the<a id="_idIndexMarker454"/> formatting of the output that collates results from each of the three payloads is managed with a hefty <strong class="bold">transform</strong> script that leverages the Java-like <strong class="bold">Painless</strong> scripting language. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information on the Painless scripting language, please consult the Elastic documentation at <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html</a>.</p>
			<p>If you're not intimidated by the code-heavy format of Watcher, you can wield it as a very <a id="_idIndexMarker455"/>powerful tool to implement some very interesting and useful alert schemes.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor130"/>Summary</h1>
			<p>Anomaly detection jobs are certainly useful on their own, but when combined with near real-time alerting, users can really harness the power of automated analysis – while also being confident about getting only alerts that are meaningful. </p>
			<p>After a practical study of how to effectively capture the results of anomaly detection jobs with real-time alerts, we went through a comprehensive example of using the new Kibana alerting framework to easily define some intuitive alerts and we tested them with a realistic alerting scenario. We then witnessed how an expert user can leverage the full power of Watcher for advanced alerting techniques if Kibana alerting cannot satisfy the complex alerting requirements.</p>
			<p>In the next chapter, we'll see how anomaly detection jobs can assist not only with alerting on important key performance indicators but also how Elastic ML's automated analysis of a broad set of data within a specific application context is the means to achieving some "AI" on tracking down an application problem and determining its root cause.</p>
		</div>
	</body></html>