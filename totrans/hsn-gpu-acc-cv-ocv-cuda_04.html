<html><head></head><body><div><div><h1 class="header-title">Advanced Concepts in CUDA</h1>
                
            
            
                
<p>In the last chapter, we looked at memory architecture in CUDA and saw how it can be used efficiently to accelerate applications. Up until now, we have not seen a method to measure the performance of CUDA programs. In this chapter, we will discuss how we can do that using CUDA events. The Nvidia Visual Profiler will also be discussed, as well as how to resolve errors in CUDA programs from within the CUDA code and using debugging tools. How we can improve the performance of CUDA programs will also be discussed. This chapter will describe how CUDA streams can be used for multitasking and how we can use them to accelerate applications. You will also learn how array-sorting algorithms can be accelerated using CUDA. Image processing is an application where we need to process a large amount of data in a very small amount of time, so CUDA can be an ideal choice for this kind of application to manipulate pixel values in an image. This chapter describes the acceleration of a simple and widely used image-processing function a histogram calculation, using CUDA.</p>
<p class="mce-root"/>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Performance measurement in CUDA</li>
<li>Error handling in CUDA</li>
<li>Performance improvement of CUDA programs</li>
<li>CUDA streams and how they can be used to accelerate applications</li>
<li>Acceleration of sorting algorithms using CUDA</li>
<li>Introduction to image processing applications with CUDA</li>
</ul>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>This chapter requires familiarity with the basic C or C++ programming language and all the code examples explained in the previous chapters. All the code used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA</a>. The code can be executed on any operating system, although it has only been tested on Windows 10 and Ubuntu. Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2Nt4DEy">http://bit.ly/2Nt4DEy</a></p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Performance measurement of CUDA programs</h1>
                
            
            
                
<p>Up until now, we have not determined the performance of the CUDA programs explicitly. In this section, we will see how to measure the performance of CUDA programs using CUDA Events and also visualize the performance using the Nvidia Visual Profiler. This is a very important concept in CUDA because it will allow you to choose the best-performing algorithms for a particular application from many options. First, we will measure performance using CUDA Events.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">CUDA Events</h1>
                
            
            
                
<p>We can use a CPU timer for measuring the performance of CUDA programs, but it will not give accurate results. It will include thread latency overhead and scheduling in the OS, among many other factors. The time measured using the CPU will also depend on the availability of a high precision CPU timer. Many times, the host is performing asynchronous computation while the GPU kernel is running, hence CPU timers may not give the correct time for kernel executions. So, to measure the GPU kernel computation time, CUDA provides an event API.</p>
<p>A CUDA event is a GPU timestamp that's recorded at a specified point from your CUDA program. In this API, the GPU records the timestamp, which eliminates the issues that were present when using CPU timers for measuring performance. There are two steps to measure time using CUDA Events: creating an event and recording an event. We will record two events, one at the start of our code and one at the end. Then, we will try to calculate the difference in time between these two events, and that will give the overall performance of our code. </p>
<p>In your CUDA code, you can include the following lines to measure performance using the CUDA event API:</p>
<pre>cudaEvent_t e_start, e_stop;<br/>cudaEventCreate(&amp;e_start);<br/>cudaEventCreate(&amp;e_stop);<br/>cudaEventRecord(e_start, 0);<br/>//All GPU code for which performance needs to be measured allocate the memory<br/>cudaMalloc((void**)&amp;d_a, N * sizeof(int));<br/>cudaMalloc((void**)&amp;d_b, N * sizeof(int));<br/>cudaMalloc((void**)&amp;d_c, N * sizeof(int));<br/>  <br/>  //Copy input arrays from host to device memory<br/>cudaMemcpy(d_a, h_a, N * sizeof(int), cudaMemcpyHostToDevice);<br/>cudaMemcpy(d_b, h_b, N * sizeof(int), cudaMemcpyHostToDevice);<br/>  <br/>gpuAdd &lt;&lt; &lt;512, 512 &gt;&gt; &gt;(d_a, d_b, d_c);<br/>//Copy result back to host memory from device memory<br/>cudaMemcpy(h_c, d_c, N * sizeof(int), cudaMemcpyDeviceToHost);<br/>cudaDeviceSynchronize();<br/>cudaEventRecord(e_stop, 0);<br/>cudaEventSynchronize(e_stop);<br/>float elapsedTime;<br/>cudaEventElapsedTime(&amp;elapsedTime, e_start, e_stop);<br/>printf("Time to add %d numbers: %3.1f ms\n",N, elapsedTime);</pre>
<p>We will create two events, <kbd>e_start</kbd> and <kbd>e_stop</kbd>, for starting and ending the code. <kbd>cudaEvent_t</kbd> is used to define event objects. To create an event, we will use the <kbd>cudaEventCreate</kbd> API. We can pass in event objects as arguments to this API. At the beginning of the code, we will record the GPU timestamp in the <kbd>e_start</kbd> event; this will be done using the <kbd>cudaEventRecord</kbd> API. The second argument to this function is zero, which indicates the CUDA stream number, which we will discuss later in this chapter.</p>
<p>After recording the timestamp in the beginning, you can start writing your GPU code. After the code ends, we will again record the time in the <kbd>e_stop</kbd> event. This will be done with the <kbd>cudaEventRecord(e_stop, 0)</kbd> line. Once we have recorded both the start and end times, the difference between them should give us the actual performance of the code. But there's still one issue with directly calculating the difference in time between these two events. </p>
<p>As we have discussed in previous chapters, execution in CUDA C can be asynchronous. When the GPU is executing the kernel, the CPU might be executing the next lines of our code until the GPU finishes its execution. So, measuring time directly without synchronizing the GPU and CPU may give incorrect results. <kbd>CudaEventRecord()</kbd> will record a timestamp when all GPU instructions prior to its call finish. We should not read the <kbd>e_stop</kbd> event until this point, when prior work on the GPU is finished. So, to synchronize CPU operations with the GPU, we will use <kbd>cudaEventSynchronize(e_stop)</kbd>. It ensures that the correct timestamp is recorded in the <kbd>e_stop</kbd> event.</p>
<p>Now, to calculate the difference between these two timestamps, CUDA provides an API called <kbd>cudaEventElapsedTime</kbd>. It has three arguments. The first is the variable in which we want to store the difference, the second is the start event, and the third is the end event. After calculating this time, we will print it on the console in the next line. We added this performance measurement code to the vector addition code seen in the previous chapter, using multiple threads and blocks. The output after adding these lines is as follows:</p>
<div><img class="alignnone size-full wp-image-251 image-border" src="img/07822ae7-a24a-4f50-b732-a4468731e6d8.png" style="" width="328" height="145"/></div>
<p>The time taken to add 50,000 elements on the GPU is around 0.9 ms. This output will depend on your system configurations, hence you might get a different output in the red box. So, you can include this performance measurement code in all the code examples we have seen in this book to measure their performance. You can also quantify performance gains using constant and texture memories by using this event API.</p>
<p>It should be kept in mind that CUDA Events can only be used to measure the timing of device code blocks. This only includes memory allocation, memory copies, and kernel execution. It should not be used to measure the timings of the host code. As the GPU is recording time in the event API, using it to measure the performance of the host code may give incorrect results.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">The Nvidia Visual Profiler</h1>
                
            
            
                
<p>We now know that CUDA provides an efficient way to improve the performance of parallel computing applications. However, sometimes, it may happen that even after incorporating CUDA in your application, the performance of the code does not improve. In this kind of scenario, it is very useful to visualize which part of the code is taking the most time to complete. This is called <strong>profiling of kernel execution code</strong>. Nvidia provides a tool for this and it comes with the standard CUDA installation. This tool is called the <strong>Nvidia Visual Profiler</strong>. In the standard CUDA 9.0 installation on Windows 10, it can be found on the following path: <kbd>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp</kbd>. You can run the <kbd>nvvp</kbd> application available on this path, which will open the Nvidia Visual Profile tool as follows:</p>
<div><img class="alignnone size-full wp-image-252 image-border" src="img/fb1a410f-a1dd-4536-bec1-67a77db0610b.png" style="" width="1181" height="617"/></div>
<p>This tool will execute your code and, based on the performance of your GPU, give you a detailed report on the execution time of each kernel, detailed timestamps for each operation in your code, the memory used in your code, and the memory bandwidth, among others. To visualize and get detailed reports for any applications you have developed, you can go to File -&gt; New Session. Select the <kbd>.exe</kbd> file of the application. We have selected the vector addition example seen in the previous chapter. The result will be as follows:</p>
<div><img class="alignnone size-full wp-image-253 image-border" src="img/ac336aad-6c71-4343-a7ef-fb8ae154a36f.png" style="" width="1184" height="658"/></div>
<p>The result displays the timing of all operations in the program. It can be seen that the <kbd>cudaMalloc</kbd> operation takes the most time to complete. It also displays the order in which each operation is performed in your code. It shows that the kernel is called only once and it needs an average of 192.041 microseconds to execute. The details of memory copy operations can also be visualized. The properties of memory copy operations from the host to the device are shown as follows:</p>
<div><img class="alignnone size-full wp-image-254 image-border" src="img/dfaaa8a7-df80-4009-9164-ac73eebdee5a.png" style="" width="255" height="208"/></div>
<p>It can be seen that, as we are copying two arrays from the host to the device, the memory copy operation is invoked twice. The total number of bytes copied are 400 KB with a throughput of 1.693 GB/s. This tool is very important in the analysis of kernel execution. It can also be used to compare the performance of two kernels. It will show you the exact operation that is slowing down the performance of your code.</p>
<p>To summarize, in this section, we have seen two methods to measure and analyze CUDA code. CUDA Events is an efficient API for measuring the timing of device code. The Nvidia Visual Profiler gives a detailed analysis and profiling of CUDA code, which can be used to analyze performance. In the next section, we will see how to handle errors in CUDA code.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Error handling in CUDA </h1>
                
            
            
                
<p>We have not checked the availability of GPU devices or memory for our CUDA programs. It may happen that, when you run your CUDA program, the GPU device is not available or is out of memory. In that case, you may find it difficult to understand the reason for the termination of your program. Therefore, it is a good practice to add error handling code in CUDA programs. In this section, we will try to understand how we can add this error handling code to CUDA functions. When the code is not giving the intended output, it is useful to check the functionality of the code line-by-line or by adding a breakpoint in the program. This is called <strong>debugging</strong>. CUDA provides debugging tools that can help. So, in the following section, we will see some debugging tools that are provided by Nvidia with CUDA. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Error handling from within the code</h1>
                
            
            
                
<p>When we discussed CUDA API functions in <a href="bf5e2281-2978-4e37-89d8-8c4b781a34cd.xhtml">Chapter 2</a>, <em>Parallel Programming using CUDA C</em>, we saw that they also return the flag that indicates whether the operation has finished successfully or not. This can be used to handle errors from within CUDA programs. Of course, this will not help in resolving errors, but it will indicate which CUDA operation is causing errors. It is a very good practice to wrap CUDA functions with this error handling code. The sample error handling code for a <kbd>cudaMalloc</kbd> function is as follows:</p>
<pre>cudaError_t cudaStatus;<br/>cudaStatus = cudaMalloc((void**)&amp;d_a, sizeof(int));<br/>if (cudaStatus != cudaSuccess) {<br/>        fprintf(stderr, "cudaMalloc failed!");<br/>        goto Error;<br/>}</pre>
<p>The <kbd>cudaError_t</kbd> API is used to create an error object that will store the return value of all CUDA operations. So the output of a <kbd>cudaMalloc</kbd> function is assigned to this error object. If the error object is not equal to <kbd>cudaSuccess</kbd>, then there was some error in assigning memory on the device. This is handled by an <kbd>if</kbd> statement. It will print the error on the console and jump to the end of the program. The wrapper code for error handling during a memory copy operation is shown as follows:</p>
<pre>cudaStatus = cudaMemcpy(d_a,&amp;h_a, sizeof(int), cudaMemcpyHostToDevice);<br/>if (cudaStatus != cudaSuccess) {<br/>  fprintf(stderr, "cudaMemcpy failed!");<br/>  goto Error;<br/>  }</pre>
<p>Again, it has a similar structure to the error handling code for <kbd>cudaMalloc</kbd>. The wrapper code for the kernel call is shown as follows: </p>
<pre>gpuAdd&lt;&lt;&lt;1, 1&gt;&gt;&gt;(d_a, d_b, d_c);<br/>// Check for any errors launching the kernel<br/>cudaStatus = cudaGetLastError();<br/>if (cudaStatus != cudaSuccess) {<br/>  fprintf(stderr, "addKernel launch failed: %s\n", cudaGetErrorString(cudaStatus));<br/>  goto Error;<br/>}</pre>
<p>The kernel call does not return a flag that indicates success or failure, so it is not directly assigned to an error object. Instead, if there is any error during the kernel's launch, then we can fetch it with the <kbd>cudaGetLastError()</kbd> API, which is used to handle an error during kernel calls. It is assigned to the <kbd>cudaStatus</kbd> error object and, if it is not equal to <kbd>cudaSuccess</kbd>, it prints the error on the console and jumps to the end of the program. All the error handling code jumps to the code section defined by the <kbd>Error</kbd> label. It can be defined as follows:</p>
<pre>Error:<br/>    cudaFree(d_a);</pre>
<p>Whenever any error is encountered in a program, we jump to this section. We free up memory allocated on the device and then exit the <kbd>main</kbd> function. This is a very efficient way of writing CUDA programs. We suggest that you use this method for writing your CUDA code. It was not explained earlier to avoid unnecessary complexity in the code examples. The addition of error handling code in CUDA programs will make them longer, but it will be able to pinpoint which CUDA operation is causing problems in the code.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Debugging tools</h1>
                
            
            
                
<p>There are always two types of errors that we may encounter in programming: <strong>syntax</strong> errors and <strong>semantic</strong> errors. Syntax errors can be handled by compilers, but semantic errors are difficult to find and debug. Semantic errors cause the program to work unexpectedly. When your CUDA program is not working as intended, then there is a need to execute your code line-by-line to visualize the output after each line. This is called <strong>debugging</strong>. It is a very important operation for any kind of programming. CUDA provides debugging tools that help resolve this kind of error.</p>
<p>For Linux-based systems, Nvidia provides a very helpful debugger known as <strong>CUDA-GDB</strong>. It has a similar interface to the normal GDB debugger used for C code. It helps you in debugging your kernel directly on the GPU with features such as setting breakpoints, inspecting the GPU memory, and inspecting blocks and threads. It also provides a memory checker to check illegal memory accesses. </p>
<p>For Windows-based systems, Nvidia provides the Nsight debugger integrated with Microsoft Visual Studio. Again, it has features for adding breakpoints in the program and inspecting blocks or thread execution. The device's global memory can be viewed from a Visual Studio memory interface.</p>
<p>To summarize, in this section, we have seen two methods for handling errors in CUDA. One method helps in solving GPU hardware-related errors, such as the device or memory not being available, from CUDA programs. The second method of using debugging helps when the program is not working as per expectations. In the next section, we will see some advanced concepts that can help improve the performance of CUDA programs. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Performance improvement of CUDA programs</h1>
                
            
            
                
<p>In this section, we will see some basic guidelines that we can follow to improve the performance of CUDA programs. These are explained one by one.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using an optimum number of blocks and threads</h1>
                
            
            
                
<p>We have seen two parameters that need to be specified during a kernel call: the number of blocks and the number of threads per block. GPU resources should not be idle during a kernel call; only then it will give the optimum performance. If resources remain idle, then it may degrade the performance of the program. The number of blocks and threads per block help in keeping GPU resources busy. It has been researched that if the number of blocks are double the number of multiprocessors on the GPU, it will give the best performance. The total number of multiprocessors on the GPU can be found out by using device properties, as seen in <a href="0cea3d6f-76ab-449e-8264-85dbdb57de9c.xhtml">Chapter 2</a>, <em>Parallel Programming Using CUDA C</em>. In the same way, the maximum number of threads per block should be equal to the <kbd>maxThreadperblock</kbd> device property. These values are just for guidance. You can play around with these two parameters to get optimum performance in your application.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Maximizing arithmetic efficiency</h1>
                
            
            
                
<p><strong>Arithmetic efficiency</strong> is defined as the ratio of the number of mathematical computations to the number of memory access operations. The value of arithmetic efficiency should be as high as possible for good performance. It can be increased by maximizing computations per thread and minimizing the time spent on the memory per thread. Sometimes, the opportunity to maximize computations per thread is limited, but certainly, you can reduce the time spent on the memory. You can minimize it by storing frequently accessed data in fast memory.</p>
<p>We saw in the last chapter that the local memory and register files are the fastest memory types available on the GPU. So, they can be used to store data that needs frequent access. We also saw the use of shared memory, constant memory, and texture memory for performance improvement. Caching also helps in reducing memory access time. Ultimately, if we reduce the bandwidth of global memory, we can reduce the time spent on the memory. Efficient memory usage is very important in improving the performance of CUDA programs, as memory bandwidth is the biggest bottleneck in fast execution.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using coalesced or strided memory access</h1>
                
            
            
                
<p>Coalesced memory access means that every thread reads or writes into contiguous memory locations. GPU is most efficient when this memory access method is used. If threads use memory locations that are offset by a constant value, then this is called <strong>strided memory access</strong>. It still gives better performance than random memory access. So, if you try to use coalesced memory access in the program, it can drastically improve performance. Here are examples of these memory access patterns:</p>
<pre>Coalesce Memory Access: d_a[i] = a<br/>Strided Memory Access: d_a[i*2] = a </pre>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Avoiding thread divergence</h1>
                
            
            
                
<p>Thread divergence happens when all threads in a kernel call follow different paths of execution. It can happen in the following kernel code scenarios:</p>
<pre>Thread divergence by way of branching<br/>tid = ThreadId<br/>if (tid%2 == 0)<br/>{ <br/>  Some Branch code;<br/>}<br/>else<br/>{<br/>  Some other code; <br/>}
Thread divergence by way of looping <br/>Pre-loop code<br/>for (i=0; i&lt;tid;i++)<br/>{<br/>  Some loop code;<br/>}<br/>Post loop code;</pre>
<p>In the first code snippet, there is separate code for odd and even number threads because of the condition in the <kbd>if</kbd> statement. This makes odd and even number threads follow different paths for execution. After the <kbd>if</kbd> statement, these threads will again merge. This will incur time overhead because fast threads will have to wait for slow threads.</p>
<p>In the second example, using the <kbd>for</kbd> loop, each thread runs the <kbd>for</kbd> loop for a different number of iterations, hence all threads will take different amounts of time to finish. <kbd>Post loop code</kbd> has to wait for all these threads to finish. It will incur time overhead. So, as much as possible, avoid this kind of thread divergence in your code.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using page-locked host memory</h1>
                
            
            
                
<p>In every example until this point, we have used the <kbd>malloc</kbd> function to allocate memory on the host, which allocates standard pageable memory on the host. CUDA provides another API called <kbd>cudaHostAlloc()</kbd>, which allocates page-locked host memory or what is sometimes referred to as pinned memory. It guarantees that the operating system will never page this memory out of this disk and that it will remain in physical memory. So, any application can access the physical address of the buffer. This property helps the GPU copy data to and from the host via <strong>Direct Memory Access (DMA)</strong> without CPU intervention. This helps improve the performance of memory transfer operations. But page-locked memory should be used with proper care because this memory is not swapped out of disk; your system may run out of memory. It may effect the performance of other applications running on the system. You can use this API to allocate memory that is used to transfer data to a device, using the <kbd>Memcpy</kbd> operation. The syntax of using this API is shown as follows:</p>
<pre>Allocate Memory: cudaHostAlloc ( (void **) &amp;h_a, sizeof(*h_a), cudaHostAllocDefault);<br/>Free Memory: cudaFreeHost(h_a); <br/><br/></pre>
<p>The syntax of <kbd>cudaHostAlloc</kbd> is similar to a simple <kbd>malloc</kbd> function. The last argument, <kbd>cudaHostAllocDefault</kbd>, which is a flag used to modify the behavior of pinned memory, is added. <kbd>cudaFreeHost</kbd> is used to free memory allocated using a <kbd>cudaHostAlloc</kbd> function.</p>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">CUDA streams</h1>
                
            
            
                
<p>We have seen that the GPU provides a great performance improvement in data parallelism when a single instruction operates on multiple data items. We have not seen task parallelism where more than one kernel function, which are independent of each other, operate in parallel. For example, one function may be computing pixel values while another function is downloading something from the internet. We know that the CPU provides a very flexible method for this kind of task parallelism. The GPU also provides this capability, but it is not as flexible as the CPU. This task parallelism is achieved by using CUDA streams, which we will see in detail in this section. </p>
<p>A CUDA stream is nothing but a queue of GPU operations that execute in a specific order. These functions include kernel functions, memory copy operations, and CUDA event operations. The order in which they are added to the queue will determine the order of their execution. Each CUDA stream can be considered a single task, so we can start multiple streams to do multiple tasks in parallel. We will look at how multiple streams work in CUDA in the next section.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using multiple CUDA streams</h1>
                
            
            
                
<p>We will understand the working of CUDA streams by using multiple CUDA streams in the vector addition program that we developed in the previous chapter. The kernel function for this is as follows:</p>
<pre>#include "stdio.h"<br/>#include&lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/>//Defining number of elements in Array<br/>#define N 50000<br/><br/>//Defining Kernel function for vector addition<br/>__global__ void gpuAdd(int *d_a, int *d_b, int *d_c) {<br/>  //Getting block index of current kernel<br/><br/>  int tid = threadIdx.x + blockIdx.x * blockDim.x;<br/>  while (tid &lt; N)<br/>  {<br/>    d_c[tid] = d_a[tid] + d_b[tid];<br/>    tid += blockDim.x * gridDim.x;<br/>  }<br/>}</pre>
<p>The <kbd>kernel</kbd> function is similar to what we developed earlier. It is just that multiple streams will execute this kernel in parallel. It should be noted that not all GPU devices support CUDA streams. GPU devices that support the <kbd>deviceOverlap</kbd> property can perform memory transfer operations and kernel executions simultaneously. This property will be used in CUDA streams for task parallelism. Before proceeding further in this code, please ensure that your GPU device supports this property. You can use code from <a href="0cea3d6f-76ab-449e-8264-85dbdb57de9c.xhtml">Chapter 2</a>, <em>Parallel Programming Using CUDA C</em>, to verify this property. We will use two parallel streams, which will execute this kernel in parallel and operate on half of the input data each. We will start by creating these two streams in the main function, as follows:</p>
<pre>int main(void) {<br/>  //Defining host arrays<br/>  int *h_a, *h_b, *h_c;<br/>  //Defining device pointers for stream 0<br/>  int *d_a0, *d_b0, *d_c0;<br/>  //Defining device pointers for stream 1<br/> int *d_a1, *d_b1, *d_c1;<br/> cudaStream_t stream0, stream1;<br/> cudaStreamCreate(&amp;stream0);<br/> cudaStreamCreate(&amp;stream1);<br/><br/>  <br/>cudaEvent_t e_start, e_stop;<br/> cudaEventCreate(&amp;e_start);<br/>  cudaEventCreate(&amp;e_stop);<br/>  cudaEventRecord(e_start, 0);<br/>  </pre>
<p>Two stream objects, <kbd>stream 0</kbd> and <kbd>stream 1</kbd>, are defined using the <kbd>cudaStream_t</kbd> and <kbd>cudaStreamCreate</kbd> APIs. We have also defined host pointers and two sets of device pointers, which will be used for each stream separately. We defined and created two events for performance measurement of this program. Now, we need to allocate memory for these pointers. The code is as follows:</p>
<pre>  //Allocate memory for host pointers<br/>  cudaHostAlloc((void**)&amp;h_a, 2*N* sizeof(int),cudaHostAllocDefault);<br/> cudaHostAlloc((void**)&amp;h_b, 2*N* sizeof(int), cudaHostAllocDefault);<br/> cudaHostAlloc((void**)&amp;h_c, 2*N* sizeof(int), cudaHostAllocDefault);<br/>  //Allocate memory for device pointers<br/>  cudaMalloc((void**)&amp;d_a0, N * sizeof(int));<br/>  cudaMalloc((void**)&amp;d_b0, N * sizeof(int));<br/>  cudaMalloc((void**)&amp;d_c0, N * sizeof(int));<br/>  cudaMalloc((void**)&amp;d_a1, N * sizeof(int));<br/>  cudaMalloc((void**)&amp;d_b1, N * sizeof(int));<br/>  cudaMalloc((void**)&amp;d_c1, N * sizeof(int));<br/>  for (int i = 0; i &lt; N*2; i++) {<br/>    h_a[i] = 2 * i*i;<br/>    h_b[i] = i;<br/>  }</pre>
<p>CUDA streams need an access to page-locked memory for its memory copy operation, hence we are defining host memory using a <kbd>cudaHostAlloc</kbd> function instead of simple <kbd>malloc</kbd>. We have seen the advantage of page-locked memory in the last section. Two sets of device pointers are allocated memory using <kbd>cudaMalloc</kbd>. It should be noted that host pointers hold the entire data so their size is <kbd>2*N*sizeof(int)</kbd>, whereas each device pointer operates on half the data elements so their size is only <kbd>N*sizeof(int)</kbd>. We have also initialized host arrays with some random values for addition. Now, we will try to enqueue memory copy operations and kernel execution operations in both of the streams. The code for this is as follows:</p>
<pre>//Asynchrnous Memory Copy Operation for both streams<br/>cudaMemcpyAsync(d_a0, h_a , N * sizeof(int), cudaMemcpyHostToDevice, stream0);<br/>cudaMemcpyAsync(d_a1, h_a+ N, N * sizeof(int), cudaMemcpyHostToDevice, stream1);<br/>cudaMemcpyAsync(d_b0, h_b , N * sizeof(int), cudaMemcpyHostToDevice, stream0);<br/>cudaMemcpyAsync(d_b1, h_b + N, N * sizeof(int), cudaMemcpyHostToDevice, stream1);<br/>    <br/>//Kernel Call     <br/>gpuAdd &lt;&lt; &lt;512, 512, 0, stream0 &gt;&gt; &gt; (d_a0, d_b0, d_c0);<br/>gpuAdd &lt;&lt; &lt;512, 512, 0, stream1 &gt;&gt; &gt; (d_a1, d_b1, d_c1);<br/>    <br/>//Copy result back to host memory from device memory<br/>cudaMemcpyAsync(h_c , d_c0, N * sizeof(int), cudaMemcpyDeviceToHost, stream0);<br/>cudaMemcpyAsync(h_c + N, d_c1, N * sizeof(int), cudaMemcpyDeviceToHost, stream0);</pre>
<p>Instead of using a simple <kbd>cudaMemcpy</kbd> API, we are using a <kbd>cudaMemcpyAsync</kbd> API, which is used for asynchronous memory transfer. It enqueues a request of a memory copy operation in a given stream specified as the last argument to the function. When this function returns, the memory copy operation may not have even started, hence it is called an asynchronous operation. It just puts a request of the memory copy in a queue. As we can see in the memory copy operations, <kbd>stream0</kbd> operates on data between <kbd>0 </kbd> to  <kbd>N</kbd> and <kbd>stream 1</kbd> operates on data from <kbd>N+1</kbd> to  <kbd>2N</kbd>. </p>
<p>The order of operation is important in stream operations as we want to overlap memory copy operations with kernel execution operations. So, instead of enqueuing all the <kbd>stream0</kbd> operations and then enqueuing the <kbd>stream 1</kbd> operations, we are first enqueuing memory copy operations in both streams and then enqueuing both kernel computation operations. This will ensure that memory copy and kernel computation overlaps with each other. If both operations take the same amount of time, we can achieve two times the speedup. We can get a better idea about the order of operations by looking at the following diagram:</p>
<div><img class="alignnone size-full wp-image-255 image-border" src="img/e068bb15-3dcf-4826-8563-cb0c96994f04.png" style="" width="452" height="161"/></div>
<p>The time increases from top to bottom. We can see that two memory copy operations and kernel execute operations are performed in the same time period, which will accelerate your program. We have also seen that the memory copy operations, defined by <kbd>cudaMemcpyAsync</kbd>, are asynchronous; so, when one stream returns, the memory copy operation may not have started. If we want to use the result of the last memory copy operation, then we have to wait for both streams to finish their queue operations. This can be ensured by using the following code:</p>
<pre>cudaDeviceSynchronize();<br/>cudaStreamSynchronize(stream0);<br/>cudaStreamSynchronize(stream1);</pre>
<p><kbd>cudaStreamSynchronize</kbd> ensures that all operations in the stream are finished before proceeding to the next line. To measure the performance of the code, the following code is inserted:</p>
<pre>cudaEventRecord(e_stop, 0);<br/>cudaEventSynchronize(e_stop);<br/>float elapsedTime;<br/>cudaEventElapsedTime(&amp;elapsedTime, e_start, e_stop);<br/>printf("Time to add %d numbers: %3.1f ms\n",2* N, elapsedTime);</pre>
<p>It will record the stop time and, based on the difference between the start and stop time, it will calculate the overall execution time for this program and print the output on the console. To check whether the program has calculated the correct output, we will insert the following code for verification:</p>
<pre>int Correct = 1;<br/>printf("Vector addition on GPU \n");<br/>//Printing result on console<br/>for (int i = 0; i &lt; 2*N; i++) <br/>{<br/>  if ((h_a[i] + h_b[i] != h_c[i]))<br/>  {<br/>    Correct = 0;<br/>  }<br/>}<br/>  <br/>if (Correct == 1)<br/>{<br/>  printf("GPU has computed Sum Correctly\n");<br/>}<br/>else<br/>{<br/>  printf("There is an Error in GPU Computation\n");<br/>}<br/>//Free up memory<br/>cudaFree(d_a0);<br/>cudaFree(d_b0);<br/>cudaFree(d_c0);<br/>cudaFree(d_a0);<br/>cudaFree(d_b0);<br/>cudaFree(d_c0);<br/>cudaFreeHost(h_a);<br/>cudaFreeHost(h_b);<br/>cudaFreeHost(h_c);<br/>return 0;<br/>}</pre>
<p>The verification code is similar to what we have seen earlier. Memory allocated on the device is freed up using <kbd>cudaFree</kbd> and memory allocated on the host using <kbd>cudaHostAlloc</kbd> is freed up using the <kbd>cudaFreeHost</kbd> function. This is mandatory, otherwise your system may run out of memory very quickly. The output of the program is shown as follows:</p>
<div><img class="alignnone size-full wp-image-256 image-border" src="img/e2d48765-0845-4e40-8995-8f961ec3fc48.png" style="" width="314" height="133"/></div>
<div><p>As can be seen in the preceding screenshot, 0.9 ms is needed to add 100,000 elements, which is a double increment over code without streams, which needed 0.9 ms to add 50,000 numbers, as seen in the first section of this chapter.</p>
<p>To summarize, we have seen CUDA streams in this section, which helps achieve task parallelism on the GPU. The order in which operations are queued in streams is very important to achieve speedup using CUDA streams.  </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Acceleration of sorting algorithms using CUDA</h1>
                
            
            
                
<p>Sorting algorithms are widely used in many computing applications. There are many sorting algorithms, such as enumeration or rank sort, bubble sort, and merge sort. All algorithms have different levels of complexity, so it takes a different amount of time to sort a given array. For a large array, all algorithms take a long time to complete. If this can be accelerated using CUDA, then it can be of great help in any computing application. </p>
<p>To show an example of how CUDA can accelerate different sorting algorithms, we will implement a rank sort algorithm in CUDA.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Enumeration or rank sort algorithms</h1>
                
            
            
                
<p>In this algorithm, we count every element in an array to find out how many elements in an array are less than the current element. From that, we can get the position of the current element in a sorted array. Then, we put this element in that position. We repeat this process for all elements in an array to get a sorted array. This is implemented as the <kbd>kernel</kbd> function, which is shown as follows:</p>
<pre>#include "device_launch_parameters.h"<br/>#include &lt;stdio.h&gt;<br/><br/>#define arraySize 5<br/>#define threadPerBlock 5<br/>//Kernel Function for Rank sort<br/>__global__ void addKernel(int *d_a, int *d_b)<br/>{<br/>  int count = 0;<br/>  int tid = threadIdx.x;<br/>  int ttid = blockIdx.x * threadPerBlock + tid;<br/>  int val = d_a[ttid];<br/>  __shared__ int cache[threadPerBlock];<br/>  for (int i = tid; i &lt; arraySize; i += threadPerBlock) {<br/>    cache[tid] = d_a[i];<br/>    __syncthreads();<br/>    for (int j = 0; j &lt; threadPerBlock; ++j)<br/>      if (val &gt; cache[j])<br/>        count++;<br/>        __syncthreads();<br/>  }<br/>  d_b[count] = val;<br/>}</pre>
<p>The <kbd>Kernel</kbd> function takes two arrays as parameters. <kbd>d_a</kbd> is an input array and <kbd>d_b</kbd> is the output array. The <kbd>count</kbd> variable is taken, which stores the position of the current element in the sorted array. The current thread index in the block is stored in <kbd>tid</kbd> and the unique thread index among all blocks is stored in <kbd>ttid</kbd>. Shared memory is used to reduce the time in accessing data from global memory. The size of shared memory is equal to the number of threads in a block, as discussed earlier. The <kbd>value</kbd> variable holds the current element. Shared memory is filled up with the values of global memory. These values are compared with the <kbd>value</kbd> variable and the count of the number of values that are less is stored in the <kbd>count</kbd> variable. This continues until all elements in an array are compared with the <kbd>value</kbd> variable. After the loop finishes, the <kbd>count</kbd> variable has the position of the element in a sorted array, and the current element is stored at that position in <kbd>d_b</kbd>, which is an output array.</p>
<p>The <kbd>main</kbd> function for this code is as follows:</p>
<pre>int main()<br/>{<br/>    //Define Host and Device Array<br/>  int h_a[arraySize] = { 5, 9, 3, 4, 8 };<br/>  int h_b[arraySize];<br/>  int *d_a, *d_b;<br/>   <br/>    //Allocate Memory on the device <br/>  cudaMalloc((void**)&amp;d_b, arraySize * sizeof(int));<br/>  cudaMalloc((void**)&amp;d_a, arraySize * sizeof(int));<br/>    <br/>    // Copy input vector from host memory to device memory.<br/>  cudaMemcpy(d_a, h_a, arraySize * sizeof(int), cudaMemcpyHostToDevice);<br/>    <br/>    // Launch a kernel on the GPU with one thread for each element.<br/>  addKernel&lt;&lt;&lt;arraySize/threadPerBlock, threadPerBlock&gt;&gt;&gt;(d_a, d_b);<br/><br/>    //Wait for device to finish operations<br/>  cudaDeviceSynchronize();<br/>    // Copy output vector from GPU buffer to host memory.<br/>  cudaMemcpy(h_b, d_b, arraySize * sizeof(int), cudaMemcpyDeviceToHost);<br/>  printf("The Enumeration sorted Array is: \n");<br/>  for (int i = 0; i &lt; arraySize; i++) <br/>  {<br/>    printf("%d\n", h_b[i]);<br/>  }<br/>    //Free up device memory<br/>  cudaFree(d_a);<br/>  cudaFree(d_b);<br/>  return 0;<br/>}</pre>
<p>The <kbd>main</kbd> function should be very familiar to you by now. We are defining host and device arrays and allocating memory on the device for device arrays. The host array is initialized with some random values and copied to the device's memory. The kernel is launched by passing device pointers as parameters. The kernel computes the sorted array by the rank sort algorithm and returns it to the host. This sorted array is printed on the console as follows:</p>
<div><img class="alignnone size-full wp-image-257 image-border" src="img/a545b18f-26fd-4176-9963-5af2a4752a5e.png" style="" width="283" height="148"/></div>
<p>This is a very trivial case and you might not see any performance improvement between the CPU and GPU. But if you go on increasing the value of <kbd>arraySize</kbd> , then the GPU will drastically improve the performance of this algorithm. It can be a hundredfold improvement for an array size equal to 15,000.</p>
<p>Rank sort is the simplest sorting algorithm available. This discussion will help you in developing code for other sorting algorithms, such as bubble sort and merge sort. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Image processing using CUDA</h1>
                
            
            
                
<p>Today, we live in an age of high definition camera sensors that capture high-resolution images. An image can have a size of up to 1920 x 1920 pixels. So, processing of these pixels on computers in real time involves billions of floating point operations to be performed per second. This is difficult for even the fastest of CPUs. A GPU can help in this kind of situation. It provides high computation power, which can be leveraged using CUDA in your code. </p>
<p>Images are stored as multidimensional arrays in a computer with two dimensions for a grayscale image and three dimensions for a color image. CUDA also supports multidimensional grid blocks and threads. So, we can process an image by launching multidimensional blocks and threads, as seen previously. The number of blocks and threads can vary depending on the size of an image. It will also depend on your GPU specifications. If it supports 1,024 threads per block, then 32 x 32 threads per block can be launched. The number of blocks can be determined by dividing the image size by the number of these threads. As discussed many times previously, the choice of parameters affects the performance of your code. So, they should be chosen properly.</p>
<p>It is very easy to convert a simple image processing code developed in C or C++ to a CUDA code. It can be done by an inexperienced programmer also by following a set pattern. Image processing code has a set pattern, as in the following code:</p>
<pre>for (int i=0; i &lt; image_height; i++)
{
   for (int j=0; j &lt; image_width; j++)
   {
      //Pixel Processing code for pixel located at (i,j)
   }
}</pre>
<p>Images are nothing but multidimensional matrices stored on a computer, so getting a single pixel value from an image involves using nested <kbd>for</kbd> loops to iterate over all pixels. To convert this code to CUDA, we want to launch a number of threads equal to the number of pixels in an image. The pixel value in a thread can be obtained by the following code in the <kbd>kernel</kbd> function:</p>
<pre>int i = blockIdx.y * blockDim.y + threadIdx.y;
int j = blockIdx.x * blockDim.x + threadIdx.x;<br/></pre>
<p>The <kbd>i</kbd> and <kbd>j</kbd> values can be used as an index to an image array to find the pixel values. So, as can be seen in the previous code, with a simple conversion process of converting <kbd>for</kbd> loops into a thread index, we can write device code for a CUDA program. We are going to develop many image processing applications using the <kbd>OpenCV</kbd> library from the next section onward. We will not cover actual image manipulation in this chapter, but we will end this chapter by developing a CUDA program for the very important statistical operation of calculating a histogram. Histogram calculation is very important for image processing applications as well.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Histogram calculation on the GPU using CUDA</h1>
                
            
            
                
<p>A histogram is a very important statistical concept used in a variety of applications such as machine learning, computer vision, data science, and image processing. It represents a count of the frequency of each element in a given dataset. It shows which data items occur the most frequently and which occur the least frequently. You can also get an idea about the distribution of data by just looking at the values of the histogram. In this section, we will develop an algorithm that calculates the histogram of a given data distribution.</p>
<p>We will start by calculating a histogram on the CPU so that you can get an idea of how to calculate a histogram. Let's assume that we have data with 1,000 elements, and each element has a value between 0 to 15. We want to calculate the histogram of this distribution. The sample code for this calculation on the CPU is as follows:</p>
<pre>int h_a[1000] = Random values between 0 and 15<br/><br/>int histogram[16];<br/>for (int i = 0; i&lt;16; i++)<br/>{ <br/>   histogram[i] = 0;<br/>}<br/>for (i=0; i &lt; 1000; i++)<br/>{<br/>   histogram[h_a[i]] +=1;<br/>} </pre>
<p>We have 1,000 data elements and they are stored in <kbd>h_a</kbd>. The <kbd>h_a</kbd> array contains values between <kbd>0</kbd> and <kbd>15</kbd>; it has 16 distinct values. So the number of bins, which indicates the number of distinct values for which histogram needs to be calculated, is 16. So we have defined a histogram array that will store the final histogram with a size equal to the number of bins. This array needs to be initialized to zero as it will be incremented with each occurrence. This is done in the first <kbd>for</kbd> loop that runs from <kbd>0</kbd> to the number of bins. </p>
<p>For the calculation of the histogram, we need to iterate through all of the elements in <kbd>h_a</kbd>. Whichever value is found in <kbd>h_a</kbd>, the particular index in that histogram array needs to be incremented. That is done by the second <kbd>for</kbd> loop, which calculates the final histogram by running from 0 to the size of the array and incrementing the histogram array indexed by the value found in <kbd>h_a</kbd>. The histogram array will contain the frequency of each element between <kbd>0 to 15</kbd> after the <kbd>for</kbd> loop finishes.</p>
<p>Now, we will develop this same code for the GPU. We will try to develop this code using three different methods. The kernel code for the first two methods is as follows:</p>
<pre>#include &lt;stdio.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/><br/>#define SIZE 1000<br/>#define NUM_BIN 16<br/><br/>__global__ void histogram_without_atomic(int *d_b, int *d_a)<br/>{<br/>  int tid = threadIdx.x + blockDim.x * blockIdx.x;<br/>  int item = d_a[tid];<br/>  if (tid &lt; SIZE)<br/>  {<br/>    d_b[item]++;<br/>  }<br/> }<br/><br/>__global__ void histogram_atomic(int *d_b, int *d_a)<br/>{<br/>  int tid = threadIdx.x + blockDim.x * blockIdx.x;<br/>  int item = d_a[tid];<br/>  if (tid &lt; SIZE)<br/>  {<br/>    atomicAdd(&amp;(d_b[item]), 1);<br/>  }<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The first function is the simplest kernel function for histogram computation. Each thread is operating on one data element. The value of the data element is fetched using the thread ID as an index to the input array. This value is used as an index into the <kbd>d_b</kbd> output array, which is incremented. The <kbd>d_b</kbd> array should contain the frequency of each value between <kbd>0</kbd> to  <kbd>15</kbd> in the input data. But if you recall from <a href="0cea3d6f-76ab-449e-8264-85dbdb57de9c.xhtml">Chapter 3</a>, <em>Threads, Synchronization, and Memory</em>, this may not give you a correct answer, as many threads are trying to modify the same memory location simultaneously. In this example, 1,000 threads are trying to modify 16 memory locations simultaneously. We need to use the atomic <kbd>add</kbd> operation for this kind of scenario.</p>
<p>The second device function is developed using the atomic <kbd>add</kbd> operation. This kernel function will give you the correct answer but it will take more time to complete, as the atomic operation is a blocking operation. All other threads have to wait when one thread is using a particular memory location. So, this second kernel function will add overhead time, which makes it even slower than the CPU version. To complete the code, we will try to write the <kbd>main</kbd> function for it as follows:</p>
<pre>int main()<br/>{<br/>  <br/>  int h_a[SIZE];<br/>  for (int i = 0; i &lt; SIZE; i++) {<br/>    <br/>  h_a[i] = i % NUM_BIN;<br/>  }<br/>  int h_b[NUM_BIN];<br/>  for (int i = 0; i &lt; NUM_BIN; i++) {<br/>    h_b[i] = 0;<br/>  }<br/><br/>  // declare GPU memory pointers<br/>  int * d_a;<br/>  int * d_b;<br/><br/>  // allocate GPU memory<br/>  cudaMalloc((void **)&amp;d_a, SIZE * sizeof(int));<br/>  cudaMalloc((void **)&amp;d_b, NUM_BIN * sizeof(int));<br/><br/>  // transfer the arrays to the GPU<br/>  cudaMemcpy(d_a, h_a, SIZE * sizeof(int), cudaMemcpyHostToDevice);<br/>  cudaMemcpy(d_b, h_b, NUM_BIN * sizeof(int), cudaMemcpyHostToDevice);<br/><br/>  <br/>  // launch the kernel<br/>  <br/>  //histogram_without_atomic &lt;&lt; &lt;((SIZE+NUM_BIN-1) / NUM_BIN), NUM_BIN &gt;&gt; &gt;(d_b, d_a);<br/>  histogram_atomic &lt;&lt; &lt;((SIZE+NUM_BIN-1) / NUM_BIN), NUM_BIN &gt;&gt; &gt;(d_b, d_a);<br/>    <br/><br/>  // copy back the sum from GPU<br/>  cudaMemcpy(h_b, d_b, NUM_BIN * sizeof(int), cudaMemcpyDeviceToHost);<br/>  printf("Histogram using 16 bin without shared Memory is: \n");<br/>  for (int i = 0; i &lt; NUM_BIN; i++) {<br/>    printf("bin %d: count %d\n", i, h_b[i]);<br/>  }<br/><br/>  // free GPU memory allocation<br/>  cudaFree(d_a);<br/>  cudaFree(d_b);<br/>  return 0;<br/>}</pre>
<p>We have started the <kbd>main</kbd> function by defining the host and device arrays and allocating memory for it. The <kbd>h_a</kbd> input data array is initialized with values from <kbd>0 to 15</kbd> in the first <kbd>for</kbd> loop. We are using the modulo operation, hence 1,000 elements will be evenly divided between values of <kbd>0 to 15</kbd>. The second array, which will store the histogram, is initialized to zero. These two arrays are copied to the device memory. The kernel will compute the histogram and return it to the host. We will print this histogram on the console. The output is shown here:</p>
<div><img class="alignnone size-full wp-image-258 image-border" src="img/23a248c6-ceb8-47b0-ab44-bb8f91850aca.png" style="" width="513" height="341"/></div>
<p>When we try to measure the performance of this code using the atomic operation and compare it with CPU performance, it is slower than the CPU for large-sized arrays. That begs the question: should we use CUDA for histogram computation, or is it possible to make this computation faster?</p>
<p>The answer to this question is: <em>yes</em>. If we use shared memory for computing a histogram for a given block and then add this block histogram to the overall histogram on global memory, then it can speed up the operation. This is possible because addition is a cumulative operation. The kernel code of using shared memory for histogram computation is shown as follows:</p>
<pre>#include &lt;stdio.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/>#define SIZE 1000<br/>#define NUM_BIN 256<br/>__global__ void histogram_shared_memory(int *d_b, int *d_a)<br/>{<br/>  int tid = threadIdx.x + blockDim.x * blockIdx.x;<br/>  int offset = blockDim.x * gridDim.x;<br/>  __shared__ int cache[256];<br/>  cache[threadIdx.x] = 0;<br/>  __syncthreads();<br/>  <br/>  while (tid &lt; SIZE)<br/>  {<br/>    atomicAdd(&amp;(cache[d_a[tid]]), 1);<br/>    tid += offset;<br/>  }<br/>  __syncthreads();<br/>  atomicAdd(&amp;(d_b[threadIdx.x]), cache[threadIdx.x]);<br/>}</pre>
<p>In this code, the number of bins are 256 instead of 16, for more capacity. We are defining shared memory of a size equal to the number of threads in a block, which is equal to 256 bins. We will calculate a histogram for the current block, so shared memory is initialized to zero and a histogram is computed for this block in the same way as discussed earlier. But, this time, the result is stored in shared memory and not in global memory. In this case, only 256 threads are trying to access 256 memory elements in shared memory, instead of 1,000 elements, as in the previous code. This will help reduce the time overhead in the atomic operation. The final atomic <kbd>add</kbd> operation in the last line will add a histogram of one block to the overall histogram values. As addition is a cumulative operation, we do not have to worry about the order in which each block is executed. The <kbd>main</kbd> function for this is similar to the previous function.</p>
<p class="mce-root"/>
<p>The output for this <kbd>kernel</kbd> function is as follows:</p>
<div><img class="alignnone size-full wp-image-259 image-border" src="img/2bfad7b5-3977-4a99-977d-d7ef4d5e290c.png" style="" width="977" height="516"/></div>
<p>If you measure the performance of the preceding program, it will beat both GPU versions without shared memory and the CPU implementation for large array sizes. You can check whether the histogram computed by the GPU is correct or not by comparing results with the CPU computation. </p>
<p>This section demonstrated the implementation of the histogram on the GPU. It also re-emphasized the use of shared memory and atomic operations in CUDA programs. It also demonstrated how CUDA is helpful in image processing applications and how easy it is to convert existing CPU code into CUDA code.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we saw some advanced concepts in CUDA that can help us to develop a complex application using CUDA. We saw the method for measuring the performance of the device code and how to see a detailed profile of the kernel function using the Nvidia Visual Profiler tool. It helps us in identifying the operation that slows down the performance of our program. We saw the methods to handle errors in hardware operation from the CUDA code itself, and we saw methods of debugging the code using certain tools. The CPU provides efficient task parallelism where two completely different functions execute in parallel. We saw that the GPU also provides this functionality using CUDA streams and achieves a twofold speedup on the same vector addition program using CUDA streams.</p>
<p>Then, we saw an acceleration of sorting algorithms using CUDA, which is an important concept to understand in order to build complex computing applications. Image processing is a computationally intensive task, which needs to be performed in real time. Almost all image processing algorithms can utilize parallelism of the GPU and CUDA. So, in the last section, we saw the use of CUDA in the acceleration of image processing application and how we can convert existing C++ code to CUDA code. We have also developed CUDA code for histogram calculation, which is an important image processing application.</p>
<p>This chapter also marks an end to concepts related to CUDA programming. From the next chapter onward, we will start with the exciting part of developing computer vision applications using the OpenCV library, which utilizes the CUDA acceleration concepts that we have seen up until this point. We will start dealing with real images and not matrices from the next chapter.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li> Why aren't CPU timers used to measure the performance of kernel functions?</li>
<li>Try to visualize the performance of the matrix multiplication code implemented in the previous chapter using the Nvidia Visual Profiler tool.</li>
<li>Give different examples of semantic errors encountered in programs.</li>
<li>What is the drawback of thread divergence in kernel functions? Explain with an example.</li>
<li>What is the drawback of using the <kbd>cudahostAlloc</kbd> function to allocate memory on the host?</li>
<li>Justify the following statement: the order of operations in CUDA streams is very important to improve the performance of a program.</li>
<li>How many blocks and threads should be launched for a 1024 x 1024 image for good performance using CUDA?</li>
</ol>


            

            
        
    </div></div></body></html>