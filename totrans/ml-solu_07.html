<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 7. Text Summarization"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Text Summarization</h1></div></div></div><p>In this chapter, we will be building the summarization application. We will specifically focus on the textual dataset. Our primary goal is to perform the summarization task on medical notes. Basically, the idea is to come up with a good solution to summarize medical transcription documents.</p><p>This kind of <a id="id779" class="indexterm"/>summarization application helps doctors a great manner. You ask how? Let's take an example. Suppose a patient has 10 years of history with a certain disease, and after 10 years, he consults a new doctor for better results. On the first day, the patient needs to hand over their last 10 years of medical prescriptions to this new doctor. After that, the doctor will need to study all these documents. The doctor also relies on the conversation he had with the patient. By using medical notes and conversations with the patient, the doctor can find out the patient's health status. This is quite a lengthy method.</p><p>However, what if we could generate a summary of the patient's medical notes and provide these summarized documents to the doctor? It seems like a promising solution because this way, we can save the doctor's time and efforts. Doctors can understand their patients' issues in an efficient and accurate way. Patients can start getting treatment from their first meeting with the doctor. This is a win-win situation for both parties, and this kind of solution is what we are trying to build here. So, in this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the basics of summarization</li><li class="listitem" style="list-style-type: disc">Introducing the problem statement</li><li class="listitem" style="list-style-type: disc">Understanding datasets</li><li class="listitem" style="list-style-type: disc">Building the baseline approach:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the baseline approach</li><li class="listitem" style="list-style-type: disc">Problems with the baseline approach</li><li class="listitem" style="list-style-type: disc">Optimizing the baseline approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">Building the revised approach:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the revised approach</li><li class="listitem" style="list-style-type: disc">Problems with the revised approach</li><li class="listitem" style="list-style-type: disc">Understanding how to improve the revised approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">The best approach:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the best approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">The best approach: building a summarization application for Amazon reviews</li><li class="listitem" style="list-style-type: disc">Summary</li></ul></div><div class="section" title="Understanding the basics of summarization"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec68"/>Understanding the basics of summarization</h1></div></div></div><p>In this section, we <a id="id780" class="indexterm"/>will be focusing on the basic concepts of summarization. In today's fast-growing information age, text summarization has become an important tool. It will be difficult for humans to generate a summary for large text documents. There are lots of documents available on the web today. So, we need a solution that can automatically generate a summary for documents efficiently, accurately, and intelligently. This task is referred to as automatic text summarization.</p><p>Automatic text summarization is all about finding relevant information from the large text document in a small amount of time. Basically, there are two types of summarization:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Extractive summarization</li><li class="listitem" style="list-style-type: disc">Abstractive summarization</li></ul></div><p>Let's look at the types of summarization one by one.</p><div class="section" title="Extractive summarization"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec114"/>Extractive summarization</h2></div></div></div><p>In the <a id="id781" class="indexterm"/>extractive summarization method, we will <a id="id782" class="indexterm"/>be generating a summary of the document by selecting words, phrases, or sentences from the original document. We will be using concepts such as <span class="strong"><strong>Term-Frequency, Inverse-Document Frequency</strong></span> (<span class="strong"><strong>TF-IDF</strong></span>), Count vectorizers, Cosine similarity, and the ranking algorithm to generate this type of summary.</p><p>We have covered concepts such as TF-IDF, Count vectorizers, and Cosine similarity in <a class="link" href="ch04.xhtml" title="Chapter 4. Recommendation Systems for E-Commerce">Chapter 4</a>, <span class="emphasis"><em>Recommendation Systems for E-Commerce</em></span>, section <span class="emphasis"><em>Understanding TF-IDF</em></span>. We will look at the ranking mechanism when we implement the code for it in this chapter.</p></div><div class="section" title="Abstractive summarization"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec115"/>Abstractive summarization</h2></div></div></div><p>In the <a id="id783" class="indexterm"/>abstractive summarization method, we will try and make the machine learn internal language representation so that it can generate more human-like summaries by paraphrasing.</p><p>In order to <a id="id784" class="indexterm"/>implement this type of summarization, we will be using deep learning algorithms such as a sequence-to-sequence model with an attention mechanism. You will learn about the algorithm and concepts later on in this chapter.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Introducing the problem statement"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec69"/>Introducing the problem statement</h1></div></div></div><p>At the beginning of <a id="id785" class="indexterm"/>the chapter, we already looked at an overview of the problem statement. Here, we will be delving into further details. We want to build an automatic text summarization application. We will be providing a medical transcription document as the input. Our goal is to generate the summary of this document. Note that here, we are going to provide a single document as the input, and as an output, we will be generating the summary of that single document. We want to generate an informative summary for the document. An informative summary is a type of summary where the summarization document is a substitute of the original document as far as the converging of information is concerned. This is because we are dealing with the medical domain.</p><p>Initially, we use extractive summarization methods in our approaches. We will be generating the extractive summary for a medical document. Later on in this chapter, we will be also developing a solution that can generate an abstractive summarization of Amazon reviews.</p><p>Now, it is time to explore the dataset and look at the challenges we have faced in accessing the dataset.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding datasets"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec70"/>Understanding datasets</h1></div></div></div><p>This section is <a id="id786" class="indexterm"/>divided into two parts. In the first part, we need to discuss <a id="id787" class="indexterm"/>the challenges we have faced in order to generate the dataset. In the later section, we will be discussing the attributes of the dataset.</p><div class="section" title="Challenges in obtaining the dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec116"/>Challenges in obtaining the dataset</h2></div></div></div><p>As we all know, the health domain <a id="id788" class="indexterm"/>is a highly regulated domain when it comes to obtaining the dataset. These are some of the challenges I want to highlight:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For summarization, ideally, we need to have a corpus that contains original text as well as a summary of that text. This is called parallel corpus. Unfortunately, there is no good, free parallel corpus available for medical document summarization. We need to obtain this kind of parallel dataset for the English language.</li><li class="listitem" style="list-style-type: disc">There are some free datasets available, such as the MIMIC II and MIMIC III dataset, but they won't contain summaries of the medical transcription. We can access just the medical transcription from this dataset. Gaining access to this dataset is a lengthy and time-consuming process.</li></ul></div><p>In order to solve <a id="id789" class="indexterm"/>the preceding challenges, professionals, researchers, academics, and big tech companies need to come forward and make good quality, freely available datasets for the medical domain. Now let's look at how to get the medical transcription dataset.</p></div><div class="section" title="Understanding the medical transcription dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec117"/>Understanding the medical transcription dataset</h2></div></div></div><p>You might <a id="id790" class="indexterm"/>wonder if we do not have a parallel dataset with us, then how will we build the summarization <a id="id791" class="indexterm"/>application? There is a workaround here. I have a sample medical transcription from the MIMIC – II dataset. We will be using them and generating an extractive summary of the documents. Apart from that, we will be referring to <a class="ulink" href="http://www.mtsamples.com">www.mtsamples.com</a> in order to get an idea about the different kind of medical transcriptions we could possibly have. With the help of a minimum number of documents, we are going to build the summarization application. You can see what these medical transcriptions will look like in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_01.jpg" alt="Understanding the medical transcription dataset" width="661" height="664"/><div class="caption"><p>Figure 7.1: Sample medical transcription</p></div></div><p>Generally, in <a id="id792" class="indexterm"/>medical transcriptions, there are a couple of sections, and they are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Chief complaint</strong></span>: This section describes <a id="id793" class="indexterm"/>the main problem or disease that the patient is facing</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hist</strong></span><span class="strong"><strong>ory of patient's illness</strong></span>: This section has a detailed description of the patient's medical status and<a id="id794" class="indexterm"/> their history of a similar disease or other kinds of diseases</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Past medical history</strong></span>: This <a id="id795" class="indexterm"/>section describes the name of the diseases that the patient had in past</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Past surgical history</strong></span>: If the<a id="id796" class="indexterm"/> patient had any surgeries in the past, then the name of those surgeries is mentioned here</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Family history</strong></span>: If <a id="id797" class="indexterm"/>any family member has the same type of disease or a history of certain kinds of diseases in the family, then those are mentioned in this section</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Medications</strong></span>: This <a id="id798" class="indexterm"/>section describes the medicine names</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Physical examination</strong></span>: This <a id="id799" class="indexterm"/>section has all the descriptions related to physical examinations</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Assessment</strong></span>: This <a id="id800" class="indexterm"/>section contains the details about the potential disease the patient may have after taking all preceding parameters into consideration.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Recommendations</strong></span>: This<a id="id801" class="indexterm"/> section describes the recommended solution for the patient's complaints</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Keywords</strong></span>: This section <a id="id802" class="indexterm"/>has the keywords that can describe the entire document properly so the dataset can be used for the topic modeling task as well</li></ul></div><p>This kind of<a id="id803" class="indexterm"/> transcription is random in certain sections. Some transcriptions contain all the preceding sections, and some do not. So, the number of sections for this kind of document may vary a lot.</p><p>Now let's look at details related to the Amazon review dataset.</p></div><div class="section" title="Understanding Amazon's review dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec118"/>Understanding Amazon's review dataset</h2></div></div></div><p>Later on in this chapter, we <a id="id804" class="indexterm"/>will be using the Amazon review dataset in order to generate the abstractive summary. So, it is <a id="id805" class="indexterm"/>better if you understand basic data attributes for this dataset. First of all, you can download that dataset by using this link: <a class="ulink" href="https://www.kaggle.com/currie32/summarizing-text-with-amazon-reviews/data">https://www.kaggle.com/currie32/summarizing-text-with-amazon-reviews/data</a>. The name of the file you need to download is <code class="literal">Reviews.csv</code>.</p><p>You can look at the <a id="id806" class="indexterm"/>content of this dataset by referring to the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_07_02.jpg" alt="Understanding Amazon's review dataset" width="900" height="272"/><div class="caption"><p>Figure 7.2: Data records from Amazon's review dataset</p></div></div><p>Let's understand each of the <a id="id807" class="indexterm"/>data attributes of this dataset:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">ID</code>: This attribute indicates the serial number for data records.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ProductId</code>: This attribute indicates the unique ID for the particular product.</li><li class="listitem" style="list-style-type: disc"><code class="literal">UserId</code>: This attribute indicates the unique user ID of the user who has shared their review for a particular product.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ProfileName</code>: This data attribute is the user's profile name. Using this profile name, the user will have submitted their review.</li><li class="listitem" style="list-style-type: disc"><code class="literal">HelpfulnessNumerator</code>: This attribute indicates how many other users found this review useful in a positive way.</li><li class="listitem" style="list-style-type: disc"><code class="literal">HelpfulnessDenominator</code>: This attribute indicates the total number of users who voted as to whether this review was useful or not useful.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Score</code>: This is the score for a particular product. Zero means the user didn't like it, and five means the user liked it a lot.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Time</code>: This attribute indicates the timestamp at which the review has been submitted.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Summary</code>: This attribute is quite useful as it indicates the summary for the entire review.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Text</code>: This attribute is the long text review for any given product.</li></ul></div><p>Now we have looked at <a id="id808" class="indexterm"/>both the datasets. Let's move on to the next section.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the baseline approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec71"/>Building the baseline approach</h1></div></div></div><p>In this section, we <a id="id809" class="indexterm"/>will be implementing the baseline approach for the summarization application. We will be using medical transcriptions <a id="id810" class="indexterm"/>to generate the summary. Here we will be using a small trial MIMIC-II dataset which contains a few sample medical documents and <a class="ulink" href="http://www.mtsamples.com">www.mtsamples.com</a> for getting medical transcriptions. You can find the code by using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Base_line_approach">https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Base_line_approach</a>.</p><p>Let's start building the baseline approach.</p><div class="section" title="Implementing the baseline approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec119"/>Implementing the baseline approach</h2></div></div></div><p>Here, we will be <a id="id811" class="indexterm"/>performing the following steps in order to build the baseline approach:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Install python dependencies</li><li class="listitem" style="list-style-type: disc">Write code and generate summary</li></ul></div><div class="section" title="Installing python dependencies"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec84"/>Installing python dependencies</h3></div></div></div><p>We will be using two <a id="id812" class="indexterm"/>python dependencies, which are really easy to use, in order to develop the summarization application. One <a id="id813" class="indexterm"/>is <code class="literal">PyTeaser</code>, and the second one is <code class="literal">Sumy</code>. You <a id="id814" class="indexterm"/>need to execute the following commands in order to install these two dependencies: </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo pip install pyteaser</strong></span>
<span class="strong"><strong>$ sudo pip install sumy or $ sudo pip3 install sumy</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>Note that the <code class="literal">PyTeaser</code> library works only with <code class="literal">python 2.7</code>. Sumy can work with <code class="literal">python 2.7</code> and <code class="literal">python 3.3+</code>.</p></div></div><p>Now let's write the code.</p></div><div class="section" title="Writing the code and generating the summary"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec85"/>Writing the code and generating the summary</h3></div></div></div><p>Both the <code class="literal">PyTeaser</code> and<a id="id815" class="indexterm"/> Sumy libraries have great features. They take any weburl as the input and generate the summary for the given weburl. You can refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_07_03.jpg" alt="Writing the code and generating the summary" width="879" height="113"/><div class="caption"><p>Figure 7.3: Code snippet for generating summarization using PyTeaser</p></div></div><p>As you can see, we <a id="id816" class="indexterm"/>are passing the weburl of the sample medical transcription from <a class="ulink" href="http://www.mtsamples.com">www.mtsamples.com</a>. The <code class="literal">PyTeaser </code>library will generate the top five sentences of the document as the summary. To view the output, you can take a look at the the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_07_04.jpg" alt="Writing the code and generating the summary" width="822" height="930"/><div class="caption"><p>Figure 7.4: Summary for the medical transcription using PyTeaser</p></div></div><p>Now let's try out the <code class="literal">Sumy</code> library. You <a id="id817" class="indexterm"/>can refer to the code given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_07_05.jpg" alt="Writing the code and generating the summary" width="865" height="583"/><div class="caption"><p>Figure 7.5: Code snippet for generating summarization using Sumy</p></div></div><p>In the <code class="literal">Sumy </code>library, we need to pass the weburl as the input, but there is one difference. As you can <a id="id818" class="indexterm"/>see in the preceding code, we have provided <code class="literal">SENTENCES_COUNT = 10</code>, which means our summary or output has 10 sentences. We can control the number of statements by using the <code class="literal">SENTENCES_COUNT</code> parameter. You can refer to the output given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_06.jpg" alt="Writing the code and generating the summary" width="619" height="1000"/><div class="caption"><p>Figure 7.6: Summary for medical transcription using Sumy</p></div></div><p>If you view and compare the output of the <code class="literal">Sumy</code> and <code class="literal">PyTeaser</code> libraries, then you could say that the <code class="literal">Sumy</code> library is performing really well compared to the <code class="literal">PyTeaser</code> library. As you can see, both these libraries obtain a basic summary of the given document. These libraries <a id="id819" class="indexterm"/>are using the ranking algorithm and the frequency of the words in order to obtain the summaries. We don't have control over their internal mechanisms. You might be wondering whether we can make our own summarization so that we can optimize the code as and when needed. The answer is yes; we can develop our code for this task. Before that, let's discuss the shortcomings of this approach, and then we will build our own code with the revised approach.</p></div></div><div class="section" title="Problems with the baseline approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec120"/>Problems with the baseline approach</h2></div></div></div><p>Here, we will be<a id="id820" class="indexterm"/> discussing the shortcomings of the baseline approach so that we can take care of these disadvantages in the next iteration:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">As mentioned earlier, we do not have full ownership over the code of these libraries. So, we cannot change or add functionalities easily.</li><li class="listitem" style="list-style-type: disc">We have obtained a basic kind of summary, so we need to improve the result of the summary.</li><li class="listitem" style="list-style-type: disc">Because of the lack of a parallel corpus, we cannot build a solution that can generate an abstractive summary for medical documents.</li></ul></div><p>These are three main shortcomings of the baseline approach, and we need to solve them. In this chapter, we will be focusing on first and second shortcomings. For the third shortcoming, we cannot do much about it. So, we have to live with that shortcoming.</p><p>Let's discuss how we will be optimizing this approach.</p></div><div class="section" title="Optimizing the baseline approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec121"/>Optimizing the baseline approach</h2></div></div></div><p>In this section, we will be <a id="id821" class="indexterm"/>discussing how to optimize the baseline approach. We will be implementing a simple summarization algorithm. The idea behind this algorithm is simple: This approach is also generating an extractive summary for the medical document. We need to perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we need to determine the frequencies of the words in the given document.</li><li class="listitem">The, we split the document into a series of sentences.</li><li class="listitem">In order to generate the summary, we select the sentences that have more frequent words.</li><li class="listitem">Finally, we reorder summarize sentences so that the generated output is aligned with the original document.</li></ol></div><p>The preceding algorithm can solve our two shortcomings, although we may need help with the third one because right now, there is no availability of the dataset that can be used in the summarization task, especially in the <a id="id822" class="indexterm"/>medical domain. For this chapter, we have to live with this shortcoming (unfortunately, we don't have any other option), but don't worry. This doesn't mean we will not learn how to generate the abstractive summary. In order to learn how to generate abstractive summaries, we will be using the Amazon review dataset later on this chapter.</p><p>Now let's implement the steps of the algorithms that we described in this section.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the revised approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec72"/>Building the revised approach</h1></div></div></div><p>Now we will be coding the algorithm<a id="id823" class="indexterm"/> that we discussed in the previous section. After implementing it, we will<a id="id824" class="indexterm"/> check how well or badly our algorithm is performing. This algorithm is easy to implement, so let's begin with the code. You can find the code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Revised_approach">https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Revised_approach</a>.</p><div class="section" title="Implementing the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec122"/>Implementing the revised approach</h2></div></div></div><p>In this section, we will <a id="id825" class="indexterm"/>be implementing the summarization algorithm step by step. These are the functions that we will be building here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The get_summarized function</li><li class="listitem" style="list-style-type: disc">The reorder_sentences function</li><li class="listitem" style="list-style-type: disc">The summarize function</li></ul></div><p>Let's begin with the first one.</p><div class="section" title="The get_summarized function"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec86"/>The get_summarized function</h3></div></div></div><p>Basically, this<a id="id826" class="indexterm"/> function performs the summarization task. First, it will take the content of the document as input in the<a id="id827" class="indexterm"/> form of string. After that, this function generates the frequency of the words, so we need to tokenize the sentences into words. After that, we will be generating the top 100 most frequent words from the given document. For small of dataset, the top 100 most frequent words can describe the vocabulary of the given dataset really well so we are not considering more words. If you have large dataset, then you can consider the top 1,000 or top 10,000 most frequent words based on the size of the dataset. You can refer to the code given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_07.jpg" alt="The get_summarized function" width="615" height="267"/><div class="caption"><p>Figure 7.7: Code snippet for generating the most frequent words from the given input document</p></div></div><p>Now let's code the <a id="id828" class="indexterm"/>second step. We need to split the documents into sentences. We will convert the sentences into lowercase. We will use the NLTK sentence splitter here. You can refer to the code given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_08.jpg" alt="The get_summarized function" width="564" height="156"/><div class="caption"><p>Figure 7.8: Code snippet for generating sentences from the input document</p></div></div><p>In the third step, we will iterate over the list of the most frequent words and find out the sentences that include a higher amount of frequent words. You can refer to the code shown in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_09.jpg" alt="The get_summarized function" width="543" height="228"/><div class="caption"><p>Figure 7.9: Code snippet for generating the sentence that has a higher amount of frequent words</p></div></div><p>Now it's time to<a id="id829" class="indexterm"/> rearrange the sentences so that the sentence order aligns with the original input document.</p></div><div class="section" title="The reorder_sentences function"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec87"/>The reorder_sentences function</h3></div></div></div><p>This<a id="id830" class="indexterm"/> function basically reorders the summarized sentence so that all the sentences align with the order of the <a id="id831" class="indexterm"/>sentences of the original document. We will take summarized sentences and sentences from the original document into consideration and perform the sorting operation. You can refer to the code given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_10.jpg" alt="The reorder_sentences function" width="522" height="229"/><div class="caption"><p>Figure 7.10: Code snippet for reordering the summarized sentences</p></div></div><p>Now let's move on to the final step.</p></div><div class="section" title="The summarize function"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec88"/>The summarize function</h3></div></div></div><p>This function basically generates the summary. This is the method that we can call from any other file. Here, we need to pass <a id="id832" class="indexterm"/>the input data and the number of sentences we need in the summarized content. You <a id="id833" class="indexterm"/>can refer to the code that is displayed in the following figure: </p><div class="mediaobject"><img src="Images/B08394_07_11.jpg" alt="The summarize function" width="523" height="59"/><div class="caption"><p>Figure 7.11: Code snippet for defining the function that can be called outside of the class</p></div></div></div><div class="section" title="Generating the summary"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec89"/>Generating the summary</h3></div></div></div><p>Now let's look at a <a id="id834" class="indexterm"/>demonstration of this this code and generate the summary for the document. We will pass the textual content from <a class="ulink" href="http://www.mtsamples.com">www.mtsamples.com</a> and then try to generate a summary of the content. You can refer to the code snippet given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_12.jpg" alt="Generating the summary" width="1000" height="500"/><div class="caption"><p>Figure 7.12: Code snippet to call the summarized function</p></div></div><p>The output of the preceding code is given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_13.jpg" alt="Generating the summary" width="819" height="670"/><div class="caption"><p>Figure 7.13: Output for the revised approach</p></div></div><p>As you can see, the <a id="id835" class="indexterm"/>output is more relevant than the baseline approach. We know the approach for the kind of steps we have been performing so far. This approach gives us clarity about how we can generate the extractive summary for the medical transcription. The good part of this approach is that we do not need any parallel summarization corpus.</p><p>Now let's discuss the shortcomings of the revised approach.</p></div></div><div class="section" title="Problems with the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec123"/>Problems with the revised approach</h2></div></div></div><p>In this section, we will be <a id="id836" class="indexterm"/>discussing the shortcomings of the revised approach, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The revised approach does not have the ranking mechanism to rank the sentences based on their importance.</li><li class="listitem" style="list-style-type: disc">We have considered word frequencies so far; we have not considered their importance with respect to the other words. Suppose word <span class="emphasis"><em>a </em></span>appears a thousand times in a document. That doesn't mean it carries more importance.</li></ul></div><p>Now let's see how we can overcome these shortcomings.</p></div><div class="section" title="Understanding how to improve the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec124"/>Understanding how to improve the revised approach</h2></div></div></div><p>In this section, we will be discussing the steps that we should take in order to improve the revised approach. To obtain the best result for extractive summarization, we need to use TF-IDF and the sentence ranking <a id="id837" class="indexterm"/>mechanism to generate the summary. We have covered TF-IDF in <a class="link" href="ch04.xhtml" title="Chapter 4. Recommendation Systems for E-Commerce">Chapter 4</a>, <span class="emphasis"><em>Recommendation Systems for E-Commerce</em></span>, in the <span class="emphasis"><em>Generating features using TF-IDF</em></span> section. We will be building the ranking mechanism by using cosine similarity and LSA (Latent Semantic Analysis). We have already looked at cosine similarity in <a class="link" href="ch04.xhtml" title="Chapter 4. Recommendation Systems for E-Commerce">Chapter 4</a>, <span class="emphasis"><em>Recommendation Systems for E-Commerce</em></span>. Let's explore the LSA algorithm.</p><div class="section" title="The LSA algorithm"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec90"/>The LSA algorithm</h3></div></div></div><p>The LSA algorithm is <a id="id838" class="indexterm"/>similar to <a id="id839" class="indexterm"/>the cosine similarity. We will generate the matrix by using the words present in the paragraphs of the document. The row of the matrix will represent the unique words present in each paragraph, and columns represent each paragraph. You can view the matrix representation for the LSA algorithm in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_14.jpg" alt="The LSA algorithm" width="661" height="335"/><div class="caption"><p>Figure 7.14: Matrix representation for the LSA algorithm</p></div></div><p>The basic assumption for the LSA algorithm is that words that are close in their meaning will occur in a similar piece of text. As you can see from the preceding example, if we say that the word pair (cat, is) occurs more frequently, it means that it carries higher semantic meaning than the (cat, mouse) word pair. This is the meaning of the assumption behind the <a id="id840" class="indexterm"/>algorithm. We generate the matrix that is given in the previous figure and then try to reduce the number of rows of the matrix by using the <span class="strong"><strong>single value decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>) method. SVD is basically a factorization of the matrix.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>You can read more on SVD by using this link: <a class="ulink" href="https://en.wikipedia.org/wiki/Singular-value_decomposition">https://en.wikipedia.org/wiki/Singular-value_decomposition</a>.</p></div></div><p>Here, we are<a id="id841" class="indexterm"/> reducing the number of rows (which means the number of words) while preserving the similarity structure among columns (which means paragraphs). In order to generate the similarity score between word pairs, we are using cosine similarity. This is more than enough to keep in mind in order to build the summarization application.</p><p>Now let's discuss the approach we are taking in order to build the best possible solution for generating an extractive summary for medical documents.</p></div><div class="section" title="The idea behind the best approach"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec91"/>The idea behind the best approach</h3></div></div></div><p>We will perform the <a id="id842" class="indexterm"/>following steps in order to build the <a id="id843" class="indexterm"/>best approach:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First of all, we will take the content of the document in the form of a string.</li><li class="listitem">We will parse the sentence, and after that, we will remove the stop words and special characters. We will be converting the abbreviations into their full forms.</li><li class="listitem">After that, we will generate the lemma of the words and their <span class="strong"><strong>Part-of-Speech</strong></span> (<span class="strong"><strong>POS</strong></span>) tags. Lemma is<a id="id844" class="indexterm"/> nothing but the root form of words and POS tags indicate whether the word is used as a verb, noun, adjective, or adverb. There are many POS tags available. You can find a list of POS tags at this site: <a class="ulink" href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>.</li><li class="listitem">We will generate the matrix of the TF-IDF vectors for the words.</li><li class="listitem">We will generate the SVD matrix using the <code class="literal">SciPy</code> library for the given TF-IDF matrix.</li><li class="listitem">Finally, using cosine-similarity, we can rank the sentences and generate the summary.</li></ol></div><p>Now let's look at the implementation of these steps.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="The best approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec73"/>The best approach</h1></div></div></div><p>In this<a id="id845" class="indexterm"/> section, we will look at the implementation of the best approach. We will also discuss the structure of the code. So, without wasting time, let's begin with the implementation. You can find the code by using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Best_approach">https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Best_approach</a>.</p><div class="section" title="Implementing the best approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec125"/>Implementing the best approach</h2></div></div></div><p>The steps you need to take in order <a id="id846" class="indexterm"/>to implement the code are provided in the following list:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Understanding the structure of the project</li><li class="listitem">Understanding helper functions</li><li class="listitem">Generating the summary</li></ol></div><p>Let's start with the first step.</p><div class="section" title="Understanding the structure of the project"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec92"/>Understanding the structure of the project</h3></div></div></div><p>The structure of<a id="id847" class="indexterm"/> the project is quite important here. There will be four different files in which we will be writing<a id="id848" class="indexterm"/> code. You can see the structure of the project in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_15.jpg" alt="Understanding the structure of the project" width="410" height="283"/><div class="caption"><p>Figure 7.15: Structure of the project's code files</p></div></div><p>There are four code files. I will explain their usage one by one:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Contractions.py</code>: This<a id="id849" class="indexterm"/> file contains an extensive list of all of the abbreviations, especially grammatical abbreviations. You can take a look at the list abbreviations in the following figure:<div class="mediaobject"><img src="Images/B08394_07_16.jpg" alt="Understanding the structure of the project" width="274" height="261"/><div class="caption"><p>Figure 7.16: List of abbreviations and their full forms</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">Normalization.py</code>: This<a id="id850" class="indexterm"/> file contains various helper functions for the preprocessing step</li><li class="listitem" style="list-style-type: disc"><code class="literal">Utils.py</code>: This file<a id="id851" class="indexterm"/> contains the helper function that is used to calculate TF-IDF and obtain the SVD matrix for the given TF-IDF matrix</li><li class="listitem" style="list-style-type: disc"><code class="literal">Document_summarization.py</code>: This file uses the already defined helper function and <a id="id852" class="indexterm"/>generates a summary for the document</li></ul></div><p>Now let's see <a id="id853" class="indexterm"/>what kind of helper functions we have defined in each file.</p></div><div class="section" title="Understanding helper functions"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec93"/>Understanding helper functions</h3></div></div></div><p>We will discuss <a id="id854" class="indexterm"/>the helper function file-wise so you will get an idea as to which helper function is part of which file.</p><div class="section" title="Normalization.py"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec49"/>Normalization.py</h4></div></div></div><p>This file contains <a id="id855" class="indexterm"/>many helper functions. I will explain each helper function based on the sequence of its usage:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">parse_document</code>: This<a id="id856" class="indexterm"/> function takes the content of the document as the input and tokenizes it sentence-wise. This means we are splitting the string sentence by sentence. We will consider only the Unicode string here. You can refer to the code snippet given in the following figure:<div class="mediaobject"><img src="Images/B08394_07_17.jpg" alt="Normalization.py" width="672" height="217"/><div class="caption"><p>Figure 7.17: Code snippet for parsing documents</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">remove_special_characters</code>: This function will remove the special characters from the strings. You <a id="id857" class="indexterm"/>can refer to the code snippet given in the following figure for a better idea:<div class="mediaobject"><img src="Images/B08394_07_18.jpg" alt="Normalization.py" width="678" height="155"/><div class="caption"><p>Figure 7.18: Code snippet for removing special characters from the string</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">remove_stopwords</code>: This function will remove the stop words from the sentences. You can<a id="id858" class="indexterm"/> refer to the code snippet given in the following figure:<div class="mediaobject"><img src="Images/B08394_07_19.jpg" alt="Normalization.py" width="666" height="119"/><div class="caption"><p>Figure 7.19: Code snippet for removing stop words</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">unescape_html</code>: This function removes HTML tags from the sentences. You can refer <a id="id859" class="indexterm"/>to the code snippet given in the following figure:<div class="mediaobject"><img src="Images/B08394_07_20.jpg" alt="Normalization.py" width="288" height="68"/><div class="caption"><p>Figure 7.20: Code snippet for removing HTML tags</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">pos_tag_text</code>: This function tokenizes the sentences into words, after which it will<a id="id860" class="indexterm"/> provide POS tags to these words. You can refer to the code snippet given in the following figure:<div class="mediaobject"><img src="Images/B08394_07_21.jpg" alt="Normalization.py" width="549" height="356"/><div class="caption"><p>Figure 7.21: Code snippet for generating POS tags</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">lemmatize_text</code>: This <a id="id861" class="indexterm"/>function will tokenize the sentence into words and then generate the lemma of the words. You can refer to the code given in the following figure:<div class="mediaobject"><img src="Images/B08394_07_22.jpg" alt="Normalization.py" width="542" height="189"/><div class="caption"><p>Figure 7.22: Code snippet for generating the lemma of the words</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">expand_contractions</code>: This<a id="id862" class="indexterm"/> function looks at the abbreviations. If there is any abbreviation that is present in our list in the given sentence, then we will replace that abbreviation with its full form. You can refer to the code displayed in the following figure:<div class="mediaobject"><img src="Images/B08394_07_23.jpg" alt="Normalization.py" width="746" height="291"/><div class="caption"><p>Figure 7.23: Code snippet for replacing abbreviations with full forms</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">normalize_corpus</code>: This <a id="id863" class="indexterm"/>function calls all the preceding helper functions and generates the preprocessed sentences. You can refer to the code given in the following figure:<div class="mediaobject"><img src="Images/B08394_07_24.jpg" alt="Normalization.py" width="517" height="339"/><div class="caption"><p>Figure 7.24: Code snippet for generating preprocessed sentences</p></div></div></li></ul></div><p>Now let's see what functions we have defined in the <code class="literal">utils.py</code> file.</p></div><div class="section" title="Utils.py"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec50"/>Utils.py</h4></div></div></div><p>In this file, there<a id="id864" class="indexterm"/> are only two helper functions. They are described here.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">build_feature_matrixs</code>: This<a id="id865" class="indexterm"/> function generates the TF-IDF vectors using the scikit-learn <code class="literal">Tfidfvectorizer</code> API. We are providing the preprocessed text as the input, and as the output, we have the matrix. This matrix contains the vectorized value of the given words. You can refer to the code snippet for this, which is provided in the following figure:<div class="mediaobject"><img src="Images/B08394_07_25.jpg" alt="Utils.py" width="843" height="381"/><div class="caption"><p>Figure 7.25: Code snippet for generating TF-IDF vectors</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">low_rank_svd</code>: This<a id="id866" class="indexterm"/> particular function uses the API from python's <code class="literal">SciPy</code> library. It performs the SVD on the TF-IDF matrix, and after that, we obtain the cosine similarity score. Based on the score, we will rank the sentences. Here, we just define the function that can generate the SVD for the TF-IDF matrix. You can refer to the code snippet given in the following figure:<div class="mediaobject"><img src="Images/B08394_07_26.jpg" alt="Utils.py" width="394" height="144"/><div class="caption"><p>Figure 7.26: Code snippet for generating SVD</p></div></div></li></ul></div><p>Now let's use all these helper functions in order to generate the summary.</p></div></div><div class="section" title="Generating the summary"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec94"/>Generating the summary</h3></div></div></div><p>In this section, we will look at<a id="id867" class="indexterm"/> the code that is given in the <code class="literal">document_summarization.py</code> file. There are two methods that are responsible for generating the summary for the given document. They are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">textrank_text_summarizer</code>: This method takes the preprocessed document as the input, and by using the <code class="literal">build_feature_matrix</code> helper function, we<a id="id868" class="indexterm"/> will generate the TF-IDF matrix. After that, we will generate the similarity score. Based on the similarity score, we will sort the sentences and provide them a rank. As an output, we will display these sorted sentences. Here, sentence sequence is aligned with the original document, so we don't need to worry about that. You <a id="id869" class="indexterm"/>can take a look at the code snippet given in the following figure:<div class="mediaobject"><img src="Images/B08394_07_27.jpg" alt="Generating the summary" width="630" height="379"/><div class="caption"><p>Figure 7.27: Code snippet in order to generate the summary using the textrank_text_summarizer method</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">lsa_text_summarizer:</code> This function takes the preprocessed text as the input and <a id="id870" class="indexterm"/>generates the TF-IDF matrix. After that, the <code class="literal">low_rank_svd</code> method is applied on the matrix, and we get our factorized matrix. We will generate the similarity score using these <a id="id871" class="indexterm"/>factorized matrices. After sorting sentences based on this similarity score, we can generate the summary. You can refer to the code snippet displayed in the following figure:<div class="mediaobject"><img src="Images/B08394_07_28.jpg" alt="Generating the summary" width="636" height="384"/><div class="caption"><p>Figure 7.28: Code snippet for generating the summary using lsa_text_summarizer</p></div></div></li></ul></div><p>We will call these <a id="id872" class="indexterm"/>functions and generate the output. The code snippet for that is given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_29.jpg" alt="Generating the summary" width="996" height="728"/><div class="caption"><p>Figure 7.29: Code snippet for generating the output summary</p></div></div><p>You can take <a id="id873" class="indexterm"/>a look at the output shown in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_30.jpg" alt="Generating the summary" width="799" height="240"/><div class="caption"><p>Figure 7.30: Output summary for document_1</p></div></div><p>The output for another document is given in the following figure: </p><div class="mediaobject"><img src="Images/B08394_07_31.jpg" alt="Generating the summary" width="827" height="415"/><div class="caption"><p>Figure 7.31: Output summary for document_2</p></div></div><p>As you <a id="id874" class="indexterm"/>can see, compared to the revised approach, we will get a much more relevant extractive type of summary for the given document. Now let's build the abstractive summarization application using Amazon's product review dataset.</p></div></div><div class="section" title="Building the summarization application using Amazon reviews"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec126"/>Building the summarization application using Amazon reviews</h2></div></div></div><p>We are building this<a id="id875" class="indexterm"/> application so that you can learn how to use parallel corpus in order to generate the abstractive summary for the textual dataset. We have already explained basic stuff related to the dataset earlier in the chapter. Here, we will cover how to build an abstractive summarization application using the Deep Learning (DL) algorithm. You can refer to the code using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Amazon_review_summarization/blob/master/summarize_reviews.ipynb">https://github.com/jalajthanaki/Amazon_review_summarization/blob/master/summarize_reviews.ipynb</a>.</p><p>You can also download the pre-trained model using this link: <a class="ulink" href="https://drive.google.com/open?id=1inExMtqR6Krddv7nHR4ldWTYY7_hMALg">https://drive.google.com/open?id=1inExMtqR6Krddv7nHR4ldWTYY7_hMALg</a>.</p><p>For this application, we will perform the following steps: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Loading the dataset</li><li class="listitem" style="list-style-type: disc">Exploring the dataset</li><li class="listitem" style="list-style-type: disc">Preparing the dataset</li><li class="listitem" style="list-style-type: disc">Building the DL model</li><li class="listitem" style="list-style-type: disc">Training the DL model</li><li class="listitem" style="list-style-type: disc">Testing the DL model</li></ul></div><div class="section" title="Loading the dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec95"/>Loading the dataset</h3></div></div></div><p>In this section, we <a id="id876" class="indexterm"/>will see the code for how we can load the dataset. Our dataset is in the CSV file format. We will be using pandas to read our dataset. You can refer to the code snippet given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_32.jpg" alt="Loading the dataset" width="536" height="381"/><div class="caption"><p>Figure 7.32: Code snippet for loading the dataset</p></div></div></div><div class="section" title="Exploring the dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec96"/>Exploring the dataset</h3></div></div></div><p>In this section, we<a id="id877" class="indexterm"/> will be doing some basic analysis of the dataset. We will check whether any null entries are present. If there are, then we will remove them. You can refer to the code snippet given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_33.jpg" alt="Exploring the dataset" width="876" height="518"/><div class="caption"><p>Figure 7.33: Code snippet for removing null data entries</p></div></div><p>Now let's prepare the dataset that can be used to train the model.</p></div><div class="section" title="Preparing the dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec97"/>Preparing the dataset</h3></div></div></div><p>These are the <a id="id878" class="indexterm"/>steps that we will perform in order to prepare the dataset: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We will replace the abbreviations that appeared in the text with their full forms</li><li class="listitem" style="list-style-type: disc">We will remove special characters, URLs, and HTML tags from the review data column</li><li class="listitem" style="list-style-type: disc">We will remove stop words from the reviews</li></ul></div><p>We have performed all the preceding steps and generated the junk-free review. You can refer to the code snippet given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_34.jpg" alt="Preparing the dataset" width="903" height="475"/><div class="caption"><p>Figure 7.34: Code snippet for performing preprocessing of the reviews</p></div></div><p>Here, there are 132,884 unique words. You can find the size of the vocabulary when you run the code. These <a id="id879" class="indexterm"/>unique words are the vocabulary for this application, and we need to convert these words into a vector format. The vector format of the words is called word embeddings. You can use Word2vec, Numberbatch, or GloVe in order to generate word embeddings. Here, we will be using Numberbatch's embedding pre-trained model in order to generate word embedding for this application. The Numberbatch's pretrained model is more optimize<a id="id880" class="indexterm"/> and faster than GloVe so we are using Numberbatch's model. You can refer to the code snippet given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_35.jpg" alt="Preparing the dataset" width="676" height="215"/><div class="caption"><p>Figure 7.35: Code snippet for generating word embedding using Numberbatch's pre-trained model</p></div></div><p>If you want to learn more about word2vec, then you can refer to my previous book, <span class="emphasis"><em>Python Natural Language Processing</em></span>, particularly <a class="link" href="ch06.xhtml" title="Chapter 6. Job Recommendation Engine">Chapter 6</a>, <span class="emphasis"><em>Advance Feature Engineering and NLP Algorithms</em></span>. The link is <a class="ulink" href="https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/6">https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/6</a>.</p></div><div class="section" title="Building the DL model"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec98"/>Building the DL model</h3></div></div></div><p>In this<a id="id881" class="indexterm"/> section, we will be building the DL algorithm. We are using the seq2seq neural network. Basically, the seq2seq model is used to process the sequential data. Language or sentences are the sequence of words. In this algorithm, there is an encoder that accepts the word embedding and learns the language representation. The output of this layer is fed to the decoding layer. Here, we will also use the attention mechanism. The attention mechanism will focus on the most import part of the sentences. It will store the semantic representation of the sentences. For the attention mechanism, we will use the LSTM cell with the recurrent neural network architecture, which learns the complex semantic representation of the language and stores it in the LSTM network. When we generate the final output, we will be using the weight of the decoder cells as well as the weight of LSTM cells and will generate the final word embedding. Based on the word embedding, we will generate the summary.</p><p>In order <a id="id882" class="indexterm"/>to achieve this, we need to build seq2seq using a <span class="strong"><strong>Recurrent Neural Network</strong></span> (<span class="strong"><strong>RNN</strong></span>) with the attention mechanism. You can refer to the code given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_36.jpg" alt="Building the DL model" width="885" height="669"/><div class="caption"><p>Figure 7.36: Code snippet for building the RNN encoding layer</p></div></div><p>You can refer<a id="id883" class="indexterm"/> to the code snippet given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_37.jpg" alt="Building the DL model" width="886" height="747"/><div class="caption"><p>Figure 7.37: Code snippet for building the RNN decoding layer</p></div></div><p>The code snippet <a id="id884" class="indexterm"/>for building the seq2seq model is given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_38.jpg" alt="Building the DL model" width="820" height="429"/><div class="caption"><p>Figure 7.38: Code snippet for building seq2seq</p></div></div><p>Now let's train the model.</p></div><div class="section" title="Training the DL model"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec99"/>Training the DL model</h3></div></div></div><p>Basically, we <a id="id885" class="indexterm"/>have built the neural network, and now it's time to start the training. In this section, we will define the values for all hyperparameters, such as the learning rate, the batch size, and so on. You can refer to the code given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_39.jpg" alt="Training the DL model" width="861" height="703"/><div class="caption"><p>Figure 7.39: Code snippet for training the model</p></div></div><p>During the<a id="id886" class="indexterm"/> training, we will be tracking the loss function and using the gradient descent algorithm, and we will try to minimize the value of our loss function. You can refer to the code snippet given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_40.jpg" alt="Training the DL model" width="773" height="626"/><div class="caption"><p>Figure 7.40: Code snippet for tracing the loss function</p></div></div><p>Here, we have <a id="id887" class="indexterm"/>the trained the model on CPU for 6 to 8 hours, and we have the loss value 1.413. You can train the model for more amount time as well. Now let's test the trained model.</p></div><div class="section" title="Testing the DL model"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec100"/>Testing the DL model</h3></div></div></div><p>In this section, we<a id="id888" class="indexterm"/> load the trained model and generate the summary for a randomly selected review. You can refer to the code snippet given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_41.jpg" alt="Testing the DL model" width="863" height="811"/><div class="caption"><p>Figure 7.41: Code snippet for generating the summary for the given review</p></div></div><p>The output <a id="id889" class="indexterm"/>for the preceding code is shown in the following figure:</p><div class="mediaobject"><img src="Images/B08394_07_42.jpg" alt="Testing the DL model" width="996" height="548"/><div class="caption"><p>Figure 7.42: Summary for the given review</p></div></div><p>This <a id="id890" class="indexterm"/>approach is great if we want to generate a one-line summary for the given textual data. In future, if we will have the parallel medical transcription dataset for the summarization task, then this approach will work well.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec74"/>Summary</h1></div></div></div><p>In this chapter, we built the summarization application for medical transcriptions. In the beginning, we listed the challenges in order to generate a good parallel corpus for the summarization task in the medical domain. After that, for our baseline approach, we used the already available Python libraries, such as <code class="literal">PyTeaser </code>and <code class="literal">Sumy</code>. In the revised approach, we used word frequencies to generate the summary of the medical document. In the best possible approach, we combined the word frequency-based approach and the ranking mechanism in order to generate a summary for medical notes.</p><p>In the end, we developed a solution, where we used Amazon's review dataset, which is the parallel corpus for the summarization task, and we built the deep learning-based model for summarization. I would recommend that researchers, community members, and everyone else come forward to build high-quality datasets that can be used for building some great data science applications for the health and medical domains.</p><p>In the next chapter, we will be building chatbots. Chatbots, or virtual assistants, have become a hot topic in the data science domain over the last couple of years. So, in the next chapter, we will take into consideration a movie dialog dataset and the Facebook <code class="literal">bAbI</code> dataset. With the help of these datasets and by using deep learning algorithms, we will build chatbots. So, if you want to learn how to build one for yourself, then keep reading!</p></div></div>



  </body></html>