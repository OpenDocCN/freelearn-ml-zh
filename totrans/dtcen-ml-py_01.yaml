- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Data-Centric Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides a foundational understanding of what data-centric **machine
    learning** (**ML**) is. We will also contrast data centricity with model centricity
    and compare the performance of the two approaches, using practical examples to
    illustrate key points. Through these practical examples, you will gain a strong
    appreciation for the potential of data centricity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data-centric ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data-centric versus model-centric ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of quality data in ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data-centric ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data-centric ML** is the discipline of systematically engineering the data
    used to build ML and **artificial intelligence** (**AI**) systems1.'
  prefs: []
  type: TYPE_NORMAL
- en: The data-centric AI and ML movement is grounded in the philosophy that data
    quality is more important than data volume when it comes to building highly informative
    models. Put another way, it is possible to achieve more with a small but high-quality
    dataset than with a large but noisy dataset. For most ML use cases, it is not
    feasible to build models based on very large datasets, say millions of observations,
    simply because the volume of data doesn’t exist. In other words, the potential
    use of ML as a tool to solve certain problems is often ignored on the basis that
    the available dataset is too small.
  prefs: []
  type: TYPE_NORMAL
- en: But what if we can use ML to solve problems based on much smaller datasets,
    even down to less than 100 observations? This is one challenge the data-centric
    movement is attempting to solve through systematic data collection and engineering.
  prefs: []
  type: TYPE_NORMAL
- en: For most ML use cases, the algorithm you need already exists. The quality of
    your input data (*x*) and your dependent variable labels (*y*) is what makes the
    difference. The traditional response to dealing with noise in a dataset is to
    get as much data as possible to average out anomalies. Data centricity tries to
    improve the signal in the data such that more data is not needed.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that data centricity marks the next frontier for larger
    data solutions too. No matter how big or small your dataset is, it is the foundational
    ingredient in your ML solution. Let’s take a closer look at the different aspects
    of data-centric ML.
  prefs: []
  type: TYPE_NORMAL
- en: The origins of data centricity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The push toward a more data-centric approach to ML development has been spearheaded
    by famous data science pioneer, Dr. Andrew Ng.
  prefs: []
  type: TYPE_NORMAL
- en: Dr. Ng is the co-founder of the massive open online course platform Coursera
    and an adjunct professor in computer science at Stanford University. He is also
    the founder and CEO of DeepLearning.AI, an education company, and Landing AI2,
    an AI-driven visual inspection platform for manufacturing. He previously worked
    as chief scientist at Baidu and was the founding lead of the Google Brain team.
    His Coursera courses on various ML topics have been completed by millions of students
    worldwide.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dr. Ng and his team at Landing AI build complex ML solutions, such as computer
    vision systems used to inspect manufacturing quality. Through this work, they
    observed that the following characteristics are typical of most ML opportunities3:'
  prefs: []
  type: TYPE_NORMAL
- en: The majority of potential ML use cases rely on datasets smaller than 10,000
    observations. It is often very difficult or impossible to add more data to reduce
    the effects of noise, so improving data quality is essential to these use cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even in very large datasets, subsets of the data will exhibit the behavior of
    a small dataset. As an example, Google’s search engine generates billions of searches
    every day, but 95% of the searches are based on keyword combinations that occur
    fewer than 10 times per month (in the US). 15% of daily keyword combinations have
    never been searched before4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the dataset is small, it is typically faster and easier to identify and
    remove noise in the data than it is to collect more data. For example, if a dataset
    of 500 observations has 10% mislabeled observations, it is usually easier to improve
    the data quality on this existing data than it is to collect a new set of observations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML solutions are commonly built on pretrained models and packages, with minimal
    tweaking or modification required. Improving model performance by enhancing data
    quality frequently yields better results than changing model parameters or adding
    more data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dr. Ng published a comparison of Landing AI’s outcomes that illustrates the
    last point that we just discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 1**.1*, Landing AI produced three defect detection solutions
    for their clients. In all three cases, the teams created a baseline model and
    then tried to improve upon this model using model-centric and data-centric approaches,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Applying data-centric ML – Landing AI’s results (Source: A Chat
    with Andrew on MLOps: From Model-Centric to Data-Centric AI)](img/B19297_01_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1 – Applying data-centric ML – Landing AI’s results (Source: A Chat
    with Andrew on MLOps: From Model-Centric to Data-Centric AI)'
  prefs: []
  type: TYPE_NORMAL
- en: In all three examples, the Landing AI teams were able to achieve the best results
    by following a data-centric approach over a model-centric approach. In one of
    three examples, model-centric techniques achieved a tiny 0.04% uplift on the baseline
    model performance, and in the other two examples, no improvement was achieved.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, improving data quality consistently led to an improvement in the
    baseline model, and in two out of three cases quite substantially. The Landing
    AI teams spent about 2 weeks iteratively improving the training datasets to achieve
    these results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dr. Ng’s recommendation is clear: if you want to build relevant and impactful
    ML models regardless of the size of your dataset, you must put a lot of effort
    into systematically engineering your input data.'
  prefs: []
  type: TYPE_NORMAL
- en: Logically, it makes sense that better data leads to better models and Landing
    AI’s results provide some empirical evidence for the same. Now, let’s have a look
    at why data centricity is the future of ML development.
  prefs: []
  type: TYPE_NORMAL
- en: The components of ML systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ML systems are comprised of three main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data-centric approach considers systematic data engineering the key to
    the next ML breakthroughs for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, a model’s training data typically carries the most potential for improvement
    because it is the foundational ingredient in any model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Secondly, the code and infrastructure components of ML systems are much further
    advanced than our methods and processes for consistently capturing quality data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Over the last few decades, we have experienced a huge evolution in ML algorithms,
    data science tools, and compute and storage capacity, and our approach to operationalizing
    data science solutions has matured through practices such as **ML** **operations**
    (**MLOps**).
  prefs: []
  type: TYPE_NORMAL
- en: Open source tools such as Python and R make it relatively cheap and accessible
    for almost anyone with a computer to learn how to produce, tune, and validate
    ML models. The popularity of these tools is underpinned by the availability of
    a large number of prebuilt packages that can be installed for free from public
    libraries. These packages allow users to use common ML algorithms with just a
    few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: At the other end of the tooling spectrum, low-code and no-code **automated ML**
    (**AutoML**) tools allow non-experts with limited or no coding experience to use
    ML techniques with a few mouse clicks.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution in cloud computing has provided us with elastic compute and storage
    capacity that can be scaled up or down relatively easily when demand calls for
    it (beware of the variable costs!).
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we have solved a lot of the technical constraints surrounding
    ML models. The biggest opportunity for further upside now lies in improving the
    availability, accuracy, consistency, completeness, validity, and uniqueness of
    input data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at why.
  prefs: []
  type: TYPE_NORMAL
- en: Data is the foundational ingredient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Think of the analogous example of a chef wanting to create a world-renowned
    Michelin Star restaurant. The chef has spent a long time learning how to combine
    flavors and textures into wonderful recipes that will leave patrons delighted.
    After many years of practicing and honing their craft, they are ready to open
    their restaurant. They know what it takes to make their restaurant a success.
  prefs: []
  type: TYPE_NORMAL
- en: At the front of the restaurant, they must have a nicely laid out dining room
    with comfortable furniture, set up in a way that lets their guests enjoy each
    other’s company. To serve the guests, they need great waiters who will attend
    to customers’ every need, making sure orders are taken, glasses are filled, and
    tables are kept clean and tidy.
  prefs: []
  type: TYPE_NORMAL
- en: But that’s not all. A successful restaurant must also have a fully equipped
    commercial kitchen capable of producing many meals quickly and consistently, no
    matter how many orders are put through at the same time. And then, of course,
    there is the food. The chef has created a wonderful menu full of carefully crafted
    recipes that will provide their guests with unique and delightful flavor sensations.
    They are all set to open their soon-to-be award-winning restaurant.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, on opening night, there is a problem. Mold has gone through some of
    the vegetables in the pantry and they must be thrown away. Some herbs and spices
    are out of stock and hard to come by easily. Lastly, the most popular dish on
    the menu contains red cabbage, but only green cabbage was delivered by the supplier.
    As a result, the meals are not delightful flavor sensations, but rather bland
    and average. The chef has built a perfect operation and a wonderful menu but paid
    too little attention to the most important and hardest-to-control element: the
    ingredients.'
  prefs: []
  type: TYPE_NORMAL
- en: The ingredients are produced outside the restaurant and delivered by several
    different suppliers. If one or more parts of the supply chain are not delivering,
    then the final output will suffer, no matter how talented the chef is.
  prefs: []
  type: TYPE_NORMAL
- en: The story of the restaurant illustrates why a more systematic approach to engineering
    high-quality datasets is the key to better models.
  prefs: []
  type: TYPE_NORMAL
- en: Like the superstar chef needing the best ingredients to make their meals exceptional,
    data scientists often fall short of building highly impactful models because the
    input data isn’t as good or accessible as it should be. Instead of rotten vegetables,
    we have mislabeled observations. Instead of out-of-stock ingredients, we have
    missing values. Instead of the wrong kind of cabbage, we have generic or high-level
    labels with limited predictive power. Instead of a network of food suppliers,
    we have a plethora of data sources and technical platforms that are rarely purpose-built
    for ML.
  prefs: []
  type: TYPE_NORMAL
- en: Part of the reason for this lack of maturity in data collection has to do with
    the maturity of ML as a capability relative to other disciplines in the computer
    science sphere. It is common for people with only a superficial understanding
    of ML to view ML systems the same way they understand traditional software applications.
  prefs: []
  type: TYPE_NORMAL
- en: However, unlike traditional software, ML systems produce variable outputs that
    depend on a combinatory set of ever-changing data inputs. In ML, the data is part
    of the code. This is important because the data holds the most potential for varying
    the final model output. The breadth, depth, and accuracy of input features and
    observations are foundational to building impactful and reliable models. If the
    dataset is unrepresentative of the real-world population or scenarios you are
    trying to predict, then the model is unlikely to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, the dataset will determine most of the potential biases of
    the model; that is, whether the model is more likely to produce results that incorrectly
    favor one group over another. In short, the input data is the source of the most
    variability in an ML model and we want to use this variability to our advantage
    rather than it being a risk or a hindrance.
  prefs: []
  type: TYPE_NORMAL
- en: As we move from data to algorithms and on to system infrastructure, we want
    the ML system to become increasingly standardized and unvarying. Following a data-centric
    approach, we want to have lots of the right kind of variability in the data (not
    noise!) while keeping our ML algorithms and overall operational infrastructure
    robust and stable. That way, we can iteratively improve model accuracy by improving
    data quality, while keeping everything else stable.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1**.2* provides an overview of the facets associated with each of the
    three components of ML systems – data, code, and infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – The components of ML systems](img/B19297_01_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – The components of ML systems
  prefs: []
  type: TYPE_NORMAL
- en: Under a data-centric approach, high-quality data is the foundation for robust
    ML systems. The biggest opportunities to improve an ML model are typically found
    in the input data rather than the code.
  prefs: []
  type: TYPE_NORMAL
- en: While it makes a lot of sense to focus on data quality over changes to model
    parameters, data scientists tend to focus on the latter because it is a lot easier
    to implement in the short term. Multiple models and hyperparameters can typically
    be tested within a very short timeframe following a traditional model-centric
    approach, but increasing the signal and reducing the noise in your modeling dataset
    seems like a complex and time-consuming exercise.
  prefs: []
  type: TYPE_NORMAL
- en: In part, this is because systematically improved data collection typically involves
    upstream process changes and the participation of various stakeholders in the
    organization. That is rarely something data scientists can do alone, and it requires
    the overall organization to appreciate the value and potential of data science
    to commit the appropriate time and resources to better data collection. Unfortunately,
    most organizations waste more resources building and implementing suboptimal models
    based on poor data than the resources it would take to collect better data.
  prefs: []
  type: TYPE_NORMAL
- en: As we will learn in the following sections, a well-designed data-centric approach
    can overcome this challenge and usually unlocks many new ML opportunities in an
    organization. This is because data-centric ML requires everyone involved in the
    data pipeline to think more holistically about the structure and purpose of an
    organization’s data.
  prefs: []
  type: TYPE_NORMAL
- en: To further understand and appreciate the potential of a data-centric approach
    to model development, let’s compare data centricity with the more dominant model-centric
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Data-centric versus model-centric ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have established that data centricity is about systematically engineering
    the data used to build ML models. The conventional and more prevalent model-centric
    approach to ML suggests that optimizing the model itself is the key to better
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in *Figure 1**.3*, the central objective of a model-centric
    approach is improving the code underlying the model. Under a data-centric approach,
    the goal is to find a much larger upside in improved data quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Building ML solutions via model-centric and data-centric workflows](img/B19297_01_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Building ML solutions via model-centric and data-centric workflows
  prefs: []
  type: TYPE_NORMAL
- en: ML model development has traditionally focused on improving model performance
    mainly by optimizing the code. Under a data-centric approach, the focus shifts
    to achieving even larger performance enhancements, mainly by iteratively improving
    data quality. It is important to note that the data-centric approach sits on top
    of the principles and techniques that underpin model-centric ML, rather than replacing
    them. Both approaches consider the model and the data critical components of ML
    solutions. A solution will fail if either of the two is misconfigured, buggy,
    biased, or applied incorrectly.
  prefs: []
  type: TYPE_NORMAL
- en: Model configuration is an important step under a data-centric approach and in
    the very short term, it is certainly quicker to seek incremental gains in model
    performance by optimizing the code. However, as we’ve discussed, there is limited
    upside in changing the recipe if you don’t have the right ingredients. In other
    words, the difference between the two approaches lies in where we put our focus
    and efforts into iteratively improving model performance.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in *Figure 1**.4*, a model-centric approach treats the data as
    fixed input and focuses on model selection, parameter tuning, feature engineering,
    and adding more data as the main ways to improve model performance. A data-centric
    approach considers the model somewhat static and focuses on improving performance
    mainly through data quality.
  prefs: []
  type: TYPE_NORMAL
- en: Following a model-centric approach, we attempt to collect as much data as possible
    to crowd out any outliers in the data and reduce bias – the bigger the dataset,
    the better. Then, we engineer our model(s) to be as predictive as possible without
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is in contrast to a data-centric approach, which has better data collection
    and labeling at source, on top of model selection and tuning. Data quality is
    improved even further through outlier detection, programmatic labeling, more systematic
    feature engineering, and synthetic data creation (these techniques are explained
    in depth in subsequent chapters):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Comparing model-centric and data-centric ML approaches](img/B19297_01_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Comparing model-centric and data-centric ML approaches
  prefs: []
  type: TYPE_NORMAL
- en: 'ML model improvement comes from two areas: improving the code and improving
    the data. While data collection and engineering processes might sound like a data
    engineer’s job, they really should be a key part of the data scientist’s toolbox.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at what’s required of data scientists, data engineers, and
    other stakeholders under a data-centric approach.
  prefs: []
  type: TYPE_NORMAL
- en: Data centricity is a team sport
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While it makes a lot of sense to focus on data quality over changes to model
    parameters, data scientists tend to focus on the latter because it is a lot easier
    to implement in the short term. Multiple models and hyperparameters can typically
    be tested within a very short timeframe following a traditional model-centric
    approach, but increasing the signal and reducing the noise in your modeling dataset
    seems like a complex and time-consuming exercise that can’t easily be dealt with
    by a small team. Data-centric ML takes a lot more effort across the organization,
    whereas a model-centric approach largely relies on the data scientist’s skills
    and tools to increase model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Data centricity is a team sport. Data centricity requires data scientists and
    others involved in ML development to acquire a new set of data quality-specific
    skills. The most important of these new data-centric skills and techniques is
    what we will teach you in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Data capture and labeling processes must be designed with data science in mind
    and performed by professionals with at least a foundational understanding of ML
    development. Data engineering processes and ETL layers must be structured to identify
    data quality issues and allow for iterative improvement of ML input data. All
    of this requires continuous collaboration between data scientists, data collectors,
    subject matter experts, data engineers, business leaders, and others involved
    in turning data into insights.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this point, *Figure 1**.5* compares the data-to-model process
    for both approaches. Depending on the size and purpose of your organization, there
    may be a wide range of roles involved in delivering ML solutions, such as data
    architects, ML engineers, data labelers, analysts, model validators, decision
    makers, project managers, and product owners.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in our simplified diagram in *Figure 1**.5*, three types of roles
    are involved in the process – a data scientist, a data engineer, and a subject
    matter expert:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Data-centric versus model-centric roles and responsibilities](img/B19297_01_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Data-centric versus model-centric roles and responsibilities
  prefs: []
  type: TYPE_NORMAL
- en: Stakeholders at the top of the data pipeline must be active participants in
    the process for an organization to be good at data collection and engineering
    for ML purposes. In short, data centricity requires a lot of teamwork.
  prefs: []
  type: TYPE_NORMAL
- en: Under a conventional model-centric approach, data creation typically starts
    with a data collection process, which may be automated, manual, or a mix of both.
    Examples include a customer entering details into a web page, a radiographer performing
    a CT scan, or a call center operator taking a recorded call. At this point, data
    has been captured for its primary operational purpose, but through the work of
    the data engineer, this information can also be transformed into an analytical
    dataset. The typical process requires a data engineer to extract, transform, and
    normalize the data in a database, data lake, data warehouse, or equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: Once a data scientist gets a hold of the data, it typically goes through several
    steps to ensure accuracy, consistency, validity, and integrity are maintained.
    In other words, the data should be ready for use; however, any data scientist
    knows that this is rarely the case.
  prefs: []
  type: TYPE_NORMAL
- en: A common heuristic in data science is that 80% of the time it takes to build
    a new ML model is spent on finding, cleaning, and preparing the modeling data
    for use, while only 20% is spent on analysis and model building. Traditionally,
    this has been seen as a problem because data scientists are paid to work with
    the data to build models and perform analyses, and not spend most of their time
    preparing it.
  prefs: []
  type: TYPE_NORMAL
- en: Following a data-centric approach, data preparation becomes the most important
    part of the model-building process. Instead of asking "*how might we minimize
    the time spent on data prep?",* we instead ask "*how might we systematically optimize
    data collection and preparation?"* The problem is not that data scientists are
    spending a lot of time learning and enhancing their datasets. The problem is a
    lack of connectivity between ML development and other upstream data activities
    that allow data scientists, engineers, and subject matter experts to co-create
    faster and more accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, data centricity is about establishing the processes, tools, and
    techniques to do this systematically. Subject matter experts are actively involved
    in key parts of the ML development process, including identifying outliers, validating
    data labels and model predictions, and developing new features and attributes
    that should be captured in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers and data scientists also gain additional responsibilities under
    a data-centric approach. The data engineer’s responsibilities must expand from
    building and maintaining data pipelines to being more directly involved in developing
    and maintaining high-quality features and labels for specific ML solutions. In
    turn, this requires data engineers and data scientists to understand each other’s
    roles and collaborate towards common goals.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will illustrate, through applied examples, the impact
    a data-centric approach can have on ML opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of quality data in ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have defined what data-centric ML is and how it compares to the conventional
    model-centric approach. In this section, we will examine what good data looks
    like in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a data-centric perspective, good data is as follows5:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Captured consistently**: Independent (*x*) and dependent variables (*y*)
    are labeled unambiguously'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full of signal and free of noise**: Input data covers a wide range of important
    observations and events in the smallest number of observations possible'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Designed for the business problem**: Data is designed and collected specifically
    for solving a business problem with ML, rather than the problem being solved with
    whatever data is already available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timely and relevant**: Independent and dependent variables provide an accurate
    representation of current trends (no data or concept drift)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At first glance, this sort of systematic data collection seems both expensive
    and time-consuming. However, in our experience, highly deliberate data collection
    is often a foundational requirement for getting the desired results with ML.
  prefs: []
  type: TYPE_NORMAL
- en: To appreciate the importance and potential of data centricity, let’s look at
    some applied examples of how data quality and systematic engineering of features
    make all the difference.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying high-value legal cases with natural language processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our first example of the pivotal importance of data quality comes from an ML
    solution built by Jonas and Manmohan at a large Australian legal services firm.
  prefs: []
  type: TYPE_NORMAL
- en: ML is a nascent discipline in legal services relative to comparable service
    industries such as banking, insurance, utilities, and telecommunications. This
    is due to the nature and complexity of the data available in legal services, as
    well as the risks and ethics associated with using ML in a legal setting.
  prefs: []
  type: TYPE_NORMAL
- en: Although the legal services industry is incredibly data-rich, data is often
    collected manually, stored in a textual format, and highly contextual to the particulars
    of the legal case. This textual data may come in a variety of formats, such as
    letters from medical professionals, legal contracts, counterparty communications,
    emails between lawyer and client, case notes, and audio recordings.
  prefs: []
  type: TYPE_NORMAL
- en: On top of that, the legal services industry is a high-stakes environment where
    a mistake or omission made by one party can win or lose the case altogether. Because
    of this, legal professionals tend to spend a lot of time and effort reviewing
    detailed documents and keeping track of key dates and steps in the legal process.
    The devil is in the detail!
  prefs: []
  type: TYPE_NORMAL
- en: The legal services firm is a no-win-no-fee plaintiff law firm representing people
    who have been injured or wronged physically or financially. The company fights
    on behalf of individuals or groups against the more powerful counterparties, such
    as insurance firms, negligent hospitals or doctors, and misbehaving corporations.
    The client only pays a fee if they win – otherwise, the firm bears the loss.
  prefs: []
  type: TYPE_NORMAL
- en: In 2022, the business identified an opportunity to use data science to find
    rare but high-value cases that could then be fast-tracked by specialist lawyers.
    The earlier in the process that these high-value cases could be identified, the
    better. So, the goal was to recognize them in the very first interview with prospective
    clients.
  prefs: []
  type: TYPE_NORMAL
- en: The initial project design followed a conventional model-centric approach. The
    data science team collected 2 years’ worth of case notes from prospective client
    interviews and created a flag for cases that had later turned out to be high-value
    (the dependent variable, *y*). The team also used topic modeling to engineer new
    features to be included in the final input dataset. **Topic modeling** is an unsupervised
    ML technique that’s used to detect patterns across various documents or text snippets
    that can be grouped into *topics*. These topics were then used as direct input
    into the initial model and also as a tool to explain model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial model proved reasonably predictive, but the team faced several
    challenges that could only be solved by taking a data-centric approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Less than a thousand high-value cases were opened on an annual basis, so this
    was a *small data* problem, even after oversampling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main predictors were captured from case notes, which were in a semi-structured
    or unstructured format, and often free text. Although case notes followed some
    standards, each note taker had used their distinct vocabulary, shortenings, and
    formatting, making it difficult to create a standardized modeling dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the input data was largely in free-text format, some very important
    facts were too vague for the model to pick up. For instance, it was important
    whether the legal case involved more than one injured person as this could change
    the case strategy altogether. Sometimes, each injured party would be called out
    explicitly and other times just referred to as *they*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some details were left out of the case notes because they were either assumed
    knowledge by legal professionals or they would be obvious to a human reading the
    document as a whole. Unfortunately, this was not helpful to a learning algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The team decided to take a data-centric approach and formed a cross-functional
    project team comprising a highly skilled lawyer, a data scientist, a data engineer,
    an operations manager, and a call center expert. Everyone on the team was an expert
    in one part of the overall process and together they provided lots of depth and
    breadth across client experience, legal, data, and operational processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than improving model accuracy through feature engineering, the team
    altered the data capture altogether by designing a set of client questions that
    were highly predictive of whether a case was high value. The criteria for new
    questions were as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It must provide very specific details on whether a case was high value or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The format must be easily interpretable by humans and algorithms alike
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must be easy for the prospective client to answer new questions and the call
    center operator to capture the information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must be easy to create a triaging process around the captured data such that
    the call center operator can take the right action immediately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The previously mentioned criteria highlight why it is important to involve a
    wide group of subject matter experts in developing ML solutions. Everyone in the
    cross-functional team had specific knowledge that contributed to the finer details
    of the overall solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The team identified a handful of key questions that would be highly predictive
    of whether a case was high-value. These questions needed to be so specific that
    they could only be answered with a yes, no, or a quantity. For example, rather
    than looking for the word *they* in a free text field, the call center operator
    could simply ask *how many people were involved in the incident?* and record only
    a numeric answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Hypothetical case notes before and after data-centric improvements](img/B19297_01_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Hypothetical case notes before and after data-centric improvements
  prefs: []
  type: TYPE_NORMAL
- en: With these questions answered, every prospective case could be grouped into
    high, medium, and low probability of being a high-value case. The team then built
    a simple process that allowed call center operators to direct high-probability
    cases straight into a fast-track process handled by specialized lawyers. Other
    cases would continue to be monitored using an ML model to detect new facts that
    may push them into high-value territory.
  prefs: []
  type: TYPE_NORMAL
- en: The final solution was a success because it helped identify high-value cases
    faster and more accurately, but the benefits of taking a data-centric approach
    were much broader than that. The focus on improved data collection didn’t just
    create better data for ML purposes. It created a different kind of collaboration
    between people from across the business, ultimately leading to better-defined
    processes and a stronger focus on optimizing key moments in the client journey.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting cardiac arrests in emergency calls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another example comes from an experimental study conducted at the **Emergency
    Medical Dispatch Center** (**EMDC**) in Copenhagen, Denmark6.
  prefs: []
  type: TYPE_NORMAL
- en: A team led by medical researcher *Stig Blomberg* worked to examine whether an
    ML solution could be used to identify out-of-hospital cardiac arrest by listening
    to the calls made to the EMDC.
  prefs: []
  type: TYPE_NORMAL
- en: The team trained and tested an ML model using audio recordings of emergency
    calls generated in 2014, with the primary goal of assisting medical dispatchers
    in the early detection of cardiac arrest calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'The study found the ML solution to be faster and more accurate at identifying
    cases of cardiac arrest as measured by the model’s sensitivity. However, the researchers
    also discovered the following limitations in following a model-centric approach:'
  prefs: []
  type: TYPE_NORMAL
- en: With no ability for structured feedback between ambulance paramedics and dispatchers,
    there was a lack of *learning* in the system. For instance, it would likely be
    possible to improve human and machine predictions of cardiac arrest by asking
    tailored and more structured questions of the caller, such as "*does he look pale?"*
    or "*can* *he move?".*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language barriers of non-native speakers impacted model performance. The ML
    solution worked best with Danish-speaking callers and was worse at identifying
    cardiac arrests in foreign-accent calls than the human dispatchers who might speak
    several languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the solution had a higher sensitivity (detection of true positives)
    than human dispatchers, less than one in five alerts were true positives. This
    created a high risk of alert fatigue among dispatchers, who ultimately bear the
    risk of acting on ML recommendations or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This case study is another prime example of an ML use case that requires a data-centric
    approach to achieve optimal results while managing risks and ethics appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, an ML solution classifying cardiac arrest calls will only ever be based
    on *small data* due to the nature and complexity of the underlying problem. In
    this case, it is not necessarily possible to just add more data to improve model
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: With about 1,000 true cardiac arrests being reported per year from a population
    of circa 1.8 million people in Greater Copenhagen, even years’ worth of call recordings
    would not add up to a large dataset. Once you consider the many subsets in the
    data, such as foreign language speakers and those with non-native accents, the
    data becomes even more fragmented.
  prefs: []
  type: TYPE_NORMAL
- en: The risks and ethical concerns associated with producing wrong predictions (especially
    false negatives) for life-and-death situations mean that data labels must be carefully
    curated until any biases are reduced to an acceptable minimum. This requires an
    iterative process of reviewing data quality and enhancing model features.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying cardiac arrest cases based on a short phone conversation is a complex
    exercise. It requires subject matter expertise, as well as training and experience
    from dispatchers and paramedics alike. Building a quality natural language dataset
    for ML purposes is largely about reducing ambiguity in the interpretation of the
    signal you’re looking for. This, in turn, requires the organization to define
    what matters in the process that is being modeled by involving subject matter
    experts in the design. You will learn how this is done in [*Chapter 4*](B19297_04.xhtml#_idTextAnchor056),
    *Data Labeling is a* *Collaborative Process*.
  prefs: []
  type: TYPE_NORMAL
- en: Being specific in how questions are asked and answered creates clarity for human
    agents (in this case, the dispatchers), as well as ML models. This example highlights
    how data centricity is not just about collecting better data for ML models. It
    is a golden opportunity to be more deliberate in defining and improving how people
    work and collaborate across the organization.
  prefs: []
  type: TYPE_NORMAL
- en: The two case studies you have just read through highlight the importance of
    carefully collecting and curating datasets to be high quality in terms of accuracy,
    validity, and contextual relevance. In some situations, data quality can be a
    matter of life and death!
  prefs: []
  type: TYPE_NORMAL
- en: As you will learn in [*Chapter 2*](B19297_02.xhtml#_idTextAnchor028)*, From
    Model-Centric to Data-Centric – ML’s Evolution*, there is huge potential for ML
    to be a fantastic tool in high-stakes domains such as legal services and healthcare,
    so long as we can manage the risks associated with data quality.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve discussed the different aspects of data-centric ML, let’s summarize
    what we’ve learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the fundamentals of data-centric ML and its origins.
    We also learned how data centricity differs from model centricity, including the
    roles and responsibilities of key stakeholders in a typical organization using
    ML. At this point, you should have a solid understanding of data-centric ML and
    its additional potential compared to a more traditional model-centric approach.
    Hopefully, this will encourage you to use data-centric ML for your next project.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discover why ML development has been mostly model-centric
    until now and explore further why data centricity is the key to the next phase
    of the evolution of AI.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://datacentricai.org/](https://datacentricai.org/), viewed 10 July 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.andrewng.org/](https://www.andrewng.org/) and [https://www.coursera.org/instructor/andrewng](https://www.coursera.org/instructor/andrewng),
    viewed 6 July 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=06-AZXmwHjo](https://www.youtube.com/watch?v=06-AZXmwHjo),
    viewed 2 August 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://ahrefs.com/blog/long-tail-keywords/](https://ahrefs.com/blog/long-tail-keywords/),
    viewed 2 August 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Derived from *A Chat with Andrew on MLOps – From Model-centric to Data-Centric
    AI*: [https://www.youtube.com/watch?v=06-AZXmwHjo](https://www.youtube.com/watch?v=06-AZXmwHjo),
    viewed 2 August 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zicari et al.: *On assessing trustworthy AI in healthcare: Best practice for
    machine learning as a supportive tool to recognize cardiac arrest in emergency
    calls*. Frontiers in Human Dynamics (2021)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
