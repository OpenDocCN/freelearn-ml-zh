- en: Building Deep Learning Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a superset of machine learning incorporating algorithms influenced
    by the design and functionality of the human brain, known as the artificial intelligent
    neural network. It's represented in the form of supervised, semi-supervised, and
    unsupervised algorithms, where architectures profoundly concentrate on deep neural
    networks, deep belief networks, and recurrent neural networks. Deep learning today
    is widely accepted and utilized in industry as well as in R and D sectors in the
    field of computer vision, speech recognition, audio synthesis, image recognition,
    natural language processing, social media content moderation, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of Microsoft CNTK and the MMLSpark framework, along with third-party
    deep learning tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow and Keras, and the steps of deployment on Azure compute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a subclass of traditional machine learning algorithms that
    utilizes a series of non-linear processing layers for feature extraction, transformation,
    and, finally, analysis over the successive layers of output from the previous
    layers of input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first layer of the deep learning neural network consists of an input layer,
    an output layer (the outermost layer), and a hidden layer, which is a complex
    layer in-between the input and output layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e92c8ffa-5be6-43bf-91bc-83648a72fdfc.png)'
  prefs: []
  type: TYPE_IMG
- en: Differences between traditional machine learning and deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The comparison between traditional machine learning and deep learning are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Traditional Machine learning** | **Deep learning** |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional machine learning needs manual features of data extraction/engineering.
    | Deep learning learns automatically from the data features. |'
  prefs: []
  type: TYPE_TB
- en: '| For unstructured data, feature extraction is difficult. | Deep learning updates
    learned network weights and bias in each layer. |'
  prefs: []
  type: TYPE_TB
- en: Common Deep Learning Neural Networks (DNNs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a diverse set of deep learning neural networks available that are
    used to solve deep learning problems in the data science platform. Some of them
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep Convolutional Neural Network **(**DCNN**): Used for the extraction of
    images representation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent Neural Network** (**RNN**): Used for the extraction of sequential
    data representation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Belief Neural Network** (**DBN**): Applied for the extraction of hierarchical
    dataset representation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Reinforcement Learning** (**DRL**): Prediction of agent behaviors to
    maximize the future cumulative reward'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The traditional manner of working on various deep learning frameworks and tools
    comes with a lot of challenges as it consists of various dependencies ...
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Azure Notebook service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Azure Notebook service is a managed service that basically provides easy
    access to Jupyter Notebooks by using the computational power of R, Python, and
    F#, and users can utilize its numerous visual libraries and share the notebooks
    both publicly and in a private manner with a shareable link.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft's **Cognitive Toolkit** (**CNTK**) has native support for Azure Notebook
    services so that Python-based Jupyter Notebooks can be executed with the CNTK
    framework. For execution in other DL frameworks like TensorFlow, Keras, or Theano,
    users need to install the respective framework components by using Miniconda or
    Pip/wheel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Azure Notebook services are available at [https://notebooks.azure.com/](https://notebooks.azure.com/),
    and leverage the features of free, cloud-based, web-based Jupyter Notebook environments,
    including facilities for the creation of libraries and numerous interactive graphics
    that are built using data science languages like Python 2, Python 3, R, and F#.
    You can create your own libraries and build interactive notebooks, and you can
    simply upload your existing Jupyter Notebooks as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2cfe591-4e0c-4560-88fa-6df493d4ce44.png)'
  prefs: []
  type: TYPE_IMG
- en: Microsoft CNTK notebooks have built-in support in Azure Notebooks. All of the
    notebooks in Azure Notebooks can be organized into individual groups known as
    libraries that are shareable but non-editable. Notebooks can be cloned from other
    repositories as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data can be uploaded with ease to Azure Notebooks by using the Data menu and
    loading it into memory with function cells. It can also be downloaded, as demonstrated
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f7c1d77-c8b9-44b2-9f5c-6733a77c69ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Azure Notebook services provide the ability to implement interactive IPython
    notebooks by using libraries like matplotlib, scikit-learn, scipy, numpy, pandas,
    and so on. In the following demo, an interactive IPython notebook on the World''s
    Population Growth rate analytics has been implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1dd41b8b-7895-44b3-b3be-1f74130df022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, data exploration is performed by importing the raw data into a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we implement the filtering in order to build a more concise pivot table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd079e1d-1cac-492a-95a7-9cf5033954f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Pivot table formation with Azure Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Pivot table can be populated using function like `pivot_table()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can build interactive Python-based visuals by using visualization
    libraries, such as `matplotlib`, `seaborn`, `pandas`, `folium` and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Overview of Azure Deep Learning Virtual Machine toolkits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Deep Learning Virtual Machine **(**DLVM**) is a superset variant of the
    traditional Azure data science VM which consists of pre-configured environments
    that are mainly used to develop and deploy deep learning models on top of GPU
    instances (for example, a Azure GPU NC series VM), and is available on two OSes—Windows
    Server 2016 and Ubuntu Linux edition.
  prefs: []
  type: TYPE_NORMAL
- en: The DSVM on Azure contains several AI tools that have been pre-built, including
    CNTK, Keras, Caffe2, and Chainer to pre-process and extract visual data, text,
    audio, or video data. You can perform data science modelling and use implementation
    operations by using tools like Microsoft R server, Anaconda Python, Jupyter Notebooks
    for Python /2.x , R , SQL Server 2017, Azure ML workbench, Julia,  F# SDK and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can provision the Deep Learning VM in the Azure portal from the marketplace
    as an Azure Resource Manager (ARM) and by providing various details like the type
    of OS, user credentials, and the instance size of the GPU that''s been accelerated
    on a deep learning machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96f16dad-8a88-4eea-85fe-404480779449.png)'
  prefs: []
  type: TYPE_IMG
- en: More details on the Azure DLVM from the marketplace can be found at the following
    link:[https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning).
  prefs: []
  type: TYPE_NORMAL
- en: Open source deep learning frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The details of various open source deep learning frameworks that are used in
    enterprise situations can be seen in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Software** | **Innovator** | **Platform** | **Software license** | **Open
    source?** | **CUDA (GPU) support** | **Platform interface** |'
  prefs: []
  type: TYPE_TB
- en: '| CNTK | Microsoft research | Windows and Linux | MIT | Yes | Yes  | Python,
    C++ , C#, and CLI support |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow | Google Brain | Linux, macOS, and Windows | Apache 2.0 | Yes
    | Yes | Python(NumPy) and C/C++ |'
  prefs: []
  type: TYPE_TB
- en: '| Theano | University of Montreal | Cross-platform | Apache 2.0 | BSD license
    | Yes | Python |'
  prefs: []
  type: TYPE_TB
- en: '| Caffe | Berkeley AI  | Linux, macOS, and Windows | BSD license | Yes | Yes
    | Python and Matlab |'
  prefs: []
  type: TYPE_TB
- en: '| Apache MXNet | Distributed ML community | Ubuntu, macOS, Windows, AWS, Android,
    and iOS | Apache 2.0 | Yes | Yes | C++ , Python, Matlab, ... |'
  prefs: []
  type: TYPE_TB
- en: In-depth analysis of Microsoft deep learning tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft has brought out extensive new deep learning toolkits which can be
    utilized to speed up advances in areas like text analysis, speech/voice recognition,
    and image classification by applying the cognitive toolkit known as CNTK, which
    can be run on-premise or in Azure GPU instances. The Azure Cognitive toolkit has
    support for binding to BrainScript and Python (versions 2.7, 3.5, and 3.6 at the
    time of writing), C++, and the .NET managed C# platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the features of CNTK 2.0 in deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: An extension facility for the CNTK function for the extraction, feature engineering,
    and scoring of optimizer ML algorithms in a variety of languages like Python,
    C#, and C++.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The integration of TensorFlow models for visualization in CNTK.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several pre-trained models are available as samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for image recognition via the use of the FAST R-CNN algorithm on GPU
    instances (for example, Nvidia Tesla CUDA, and cuDNN).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The availability of a performance profiler for Python and BrainScript.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling feasibility of deep learning projects on Azure by running on kubernetes
    clusters. The autoscaling facility on Kubernetes provides both pod-level scaling
    (out of the box) as well as node-level scaling. **Horizontal pod scaling** (**HPA**)
    is a major feature of running CNTK models on AKS, as this automatically scales
    the number of pods in the clusters based on your requirements and also takes care
    to specify several node metrics like the percentage of CPU utilization and % of
    memory availability based on being scaled out or in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support of VS tools for AIs, which provides easy local installation for most,
    if not all, deep learning libraries (for example, Tensorflow, MXNet, Keras, Caffe2,
    Theano, Torch, Pytorch, Chainer (with GPU cuda support as cuPy), XG-Boost, Scikit-learn,
    LIBSVM, **Open Neural Network Exchange** (**ONNX**), Core ML Community Tools (coremltools),
    Microsoft ML tools, tf2onnx, Netron, and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More details on AI tools for Visual Studio and its supported ML/DL libraries
    can be found at the following GitHub link: [https://github.com/Microsoft/vs-tools-for-ai/blob/master/docs/](https://github.com/Microsoft/vs-tools-for-ai/blob/master/docs/prepare-localmachine.md).'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Microsoft CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft CNTK is a commercial-grade open source toolkit that's used for deep
    learning and specifies the neural network structure as a series of computational
    directed graphs. It was introduced by Microsoft speech researchers (Dong Yu et
    al.) in 2012, open sourced in 2015, and published on Codeplex. On GitHub, the
    source code base of CNTK has been available under a permissions license from 2016\.
    CNTK provides the flexibility of ease of use, is fast, and composes simple building
    blocks into complex networks. This deep learning toolkit is 100% production ready
    and gives state-of-the-art accuracy, making it efficient and scalable to any CPU/GPU
    processing platform. It incorporates the popular training models of feed-forward
    ...
  prefs: []
  type: TYPE_NORMAL
- en: The architecture building blocks of CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **open neural network exchange** (**ONNX**) format that's supported by CNTK 
    as the first deep learning toolkit has a shared open source model representation
    for framework interoperability and optimization. ONNX also extends support for
    moving trained models between frameworks such as CNTK, Caffe2, Apache MXNet, and
    PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: The top-level command blocks of CNTK, which are CNTK configuration files, define
    what actions are to be carried out with related information. The configuration
    parameter classifies what command blocks are to be implemented, and in what order
    context, if more than one command block is defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Architecture-wise, CNTK configuration parameter command blocks consists of
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input reader block**: Specifies the building concepts of the network from
    the corpus and by loading an existing model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network layer**: Defines the specified training algorithm to use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learner layer**: Specifies the *where* and *how* to load the training modules
    and labels:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/05fb49a2-b118-4b00-98ae-6e43018d402f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The most widely used configuration blocks of CNTK are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Network layer building block:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SimpleNetwork Builder**: Executes one of the network models with constrained
    customization'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BrainScriptNetwork Builder**: Implements a network based on the CNTK network
    description language (BrainScript), which provides benefits in network designs
    and neural network configurations'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learners:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SGD model**: It mainly applies the stochastic gradient descent algorithm
    for the training of the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input readers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CNTK Text format Reader**: Reads input text files which merge multiple input
    text files in the same format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LM Sequence Reader**: Reads input text files containing word sequences for
    predicting word sequences.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LU Sequence Reader**: Accepts input text-based files as word sequences, as
    well as its associated labels. This is mainly used for language understanding
    API building.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTKMLF Reader**: Reads the input files in the format of HTK/MLF for speech
    recognition and voice synthesis applications.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Concepts on CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The input, output, and parameters of CNTK are organized as *tensors*, where
    rank classifies each tensor. A tensor of rank 0 is associated as Scalar, a tensor
    of *rank 1* is specified as a Vector, and a tensor of *rank 2* is defined as a
    Matrix. There are some static and dynamic axes available for every CNTK. Static
    axes have the same length throughout the lifetime of the network. The dynamic
    network's static axes are defined as a meaningful grouping of  tensors where a)
    their axes' lengths can differentiate from instance to instance, b) their axes
    lengths are typically unknown before each minibatch is represented, and c) the
    dynamic axes are ordered. The minibatch is called a tensor, and is called a dynamic
    axis or batch axis if ...
  prefs: []
  type: TYPE_NORMAL
- en: Developing and deploying CNTK layers in the Azure Deep Learning VM to implement
    a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft CNTK is flexible and easy to use, and mainly applies simple building
    blocks to build complex layers quickly. One of the major utilities of CNTK is
    that it can be used as a backend for the Keras framework as well. From a few benchmark
    results, we can see that CNTK is generally faster than Google's TensorFlow and
    up to 5-10 times faster than recurrent/LSTM networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started and build the CNTK building blocks of Azure Deep Learning GPU
    instances, we need to provision the DLVM from an Azure portal that supports GPU
    instances. You can provision the DLVM from the Azure Marketplace by selecting
    Create a Resource | New, and then typing `Deep Learning Virtual Machine` in the
    search bar, as demonstrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ceb43971-110f-4801-998e-28a3ece752ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, by providing the appropriate VM details, such as OS type (Windows/Linux),
    user credentials, and resource group, you may choose the required GPU instance
    size, for example, NV6 or NV12, or, if a sufficient quota is available in your
    Azure subscription, then you can try out the instance sizes such as NC6sv3 (for
    example, 6 core GPU, 112 GB RAM, and 12 data disks) and NC12sv3 (for example,
    12 core GPU, 224 GB of RAM, 24 data disks, and 40k disk IOPS availability).
  prefs: []
  type: TYPE_NORMAL
- en: Azure deep learning is accessible either through the remote desktop (RDP) mode
    (port `3389`), or SSH mode (port `22`).
  prefs: []
  type: TYPE_NORMAL
- en: CNTK inputs and variables declaration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The foremost exceptional thing about the deep learning framework is its ability
    to handle input datasets, declared variables, and performance management on computational
    graphs. In this CNTK demo on the Azure Deep Learning VM, three layers will be
    associated so that they can recognize a MNIST dataset of handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: CNTK variables section
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the MNIST dataset classification problem, a flattened 28 x 28 pixel value
    scale input and its associated ten labels are present for classification. In CNTK,
    the variables could be declared to capture data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: These types of `input_variable` functions are declared, just like the placeholder
    variables in TensorFlow. However, Microsoft CNTK eliminates the necessity to identify
    the number of sample/batch sizes and users can also supply the dimensions for
    each evaluation sample. In the case of a convolution neural network task, users
    can assign `input_dimension = (1,28,28)` for a flattened 28 x 28 = 784 pixel input
    and 10 output labels or classes.
  prefs: []
  type: TYPE_NORMAL
- en: Data readers for CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft CNTK provides a few helper modules to assist in getting training data
    into an acceptable format and having it read into the model in a minibatch context.
    `CTFDeserializer()` is a type of function in CNTK that can read input text files
    in a special CNTK format (where data comes in a sample per line with a pipe/delimiter).
    Another one is the `StreamDef()` function, which acts like a dictionary object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `CTFDeserializer()` function, the CNTK file format is read in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Operations in CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to TensorFlow, Microsoft CNTK allows operations that are nodes in a
    computational graph. These nodes and operations provide support for flows. CNTK
    specifies operations from graph multiplication and division to softmax and convolutional
    operations. There is a need for the explicit evaluation of the operation code
    via the `eval()` method on the operation runtime. Though most of these operations
    are not explicitly evaluated, it's evaluated implicitly during the final layer's
    network execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the MNIST dataset, a simple CNTK operation is performed to
    scale input features. This scaling is achieved by using 1/256 ~ 0.00390625:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, a constant of 0.00390 is declared, as well as the usage of `element_times()`
    operations for multiplying it by the input variable features. The input dataset
    is scaled between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Layers of the Microsoft CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Microsoft Cognitive Toolkit provides us with the capability to provision
    neural network layers, which provides many layer features such as Dense, Convolution,
    MaxPooling, Recurrentm, and LSTM. For example, in an MNIST dataset, the standard
    neural network classifier consists of some densely connected layers such as the
    input layer, the first hidden layer, the second hidden layer, and the final output
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4fe8523-fd91-4c16-9c71-172ad89a5fe7.png)'
  prefs: []
  type: TYPE_IMG
- en: Fundamentally, the input layer consists of 784 flattened pixel input layers
    that are proceeded by two hidden layers of size 200 and a final output layer on
    which a softmax has been activated. The layers ...
  prefs: []
  type: TYPE_NORMAL
- en: CNTK layer provision helpers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make network definitions more streamlined, CNTK provides some helper functions/object
    classes, such as the `Sequential()` module, which is similar to the `Sequential(
    )` paradigm in Keras. It also allows you to sequentially stack layer after layer
    on top without specifying output, which is then passed to the next layer as the
    input of the next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There is the presence of the `layers.default_options()` module in CNTK, which
    can assist in streamlining, and which is used in more complicated networks. Activation
    functions are no longer required here, but `default_option` is used for the output
    layer since it allows us to apply softmax in the loss function. The same initialization
    of the `glorot_uniform()` function is specified in each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: CNTK modules for losses and error handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CNTK library has a full set of loss functions and error handling modules
    to train the model. This range classifies from standard cross entropy and squared
    error to cosine distances such as lambda ranks. For classification purposes, the
    `cross_entropy_with_softmax` option can be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, the output layer, `z`, is supplied with a labelled output variable value
    and the cross entropy loss is calculated with softmax precision on `z`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, for accessing errors on the test set, the training model has to be used.
    For the classification task, the `classification_error()` function has to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Input training models in CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNTK has various options for performing training, such as simply calling a
    dictionary containing input and output training sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `MinibatchSource()` object that's been used here calls `reader_train`, and
    is where you can access the streams/data by using the dot notation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `ProgressPrinter` also needs to be defined, and this is where an object allows
    you to design output metrices such as loss and classification errors. The progress
    writers can be instantiated as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `tag` argument specifies the demonstration of a value in the log that's
    been attached in each update. The total number of epochs during model training
    is counted by a counter, and is declared by `num_epochs`.
  prefs: []
  type: TYPE_NORMAL
- en: Instantiating the Trainer object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to set up the `Trainer` object, we need a module that trains the model
    and feeds it into a number of information layers, such as the output layer and
    the prior layer, which is used to train a computational graph structure. Then,
    we need to utilize the loss function that''s going to be used for computing gradients
    where optimizers such as stochastic descent, and Ada Grad can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Defining the training session object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CNTK library has a marvellous way of expressing the `training_session()`
    object and its minibatch initialization. It associates defining the input data,
    logging, `num_sweeps_to_train`, samples per sweep, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this `training_session()` object, all of the optimization and parameter learning
    is going to occur in the source, and is where we can extract minibatch data that's
    used as the `reader_main` `MinibatchSource` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you execute the training, the output is shown on the progress writer,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0db99f30-730c-431d-895c-e649b42cae03.png)'
  prefs: []
  type: TYPE_IMG
- en: The CNTK testing model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For testing the CNTK training model, we need to load `Test-28x28_cntk__text.txt` in
    the path retrieved from the MNIST dataset. We need to set up `MinibatchSource`
    to read our test data, and we also need to assign input maps to the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The ...
  prefs: []
  type: TYPE_NORMAL
- en: Deploying CNTK tools by using Azure Containers (Docker)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For running CNTK Jupyter Notebooks on Docker containers and pulling CNTK images
    from Docker, make sure that you use a Linux-based VM or Azure Linux Data Science/DLVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The latest build of CNTK can be pulled using a Docker container that''s using
    Azure DSVM via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`docker pull microsoft/cntk:2.6-gpu-python3.6` can be used for a GPU-specific
    version of Python. The Nvidia-docker driver is required for the execution of GPU
    versions of CNTK Jupyter Notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c9a7652-5903-42f5-9c1c-d13be0152867.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To run CNTK Jupyter Notebooks in the Docker container of the Azure Deep Learning
    VM, the CNTK container needs to be created and started with the IP port exposed
    in detached mode in the default port `8888:8888`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the following command starts and activates the CNTK for Jupyter Notebooks.
    You need to expose port `8888` in the **network security group** (**NSG**) configuration
    settings for inbound network rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output screenshot looks like it does in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/026a31dd-ec8a-44c6-870b-ec329890e10e.png)'
  prefs: []
  type: TYPE_IMG
- en: More details on executing CNTK in GPU mode on Docker containers in a Linux environment
    can be found at the following link: [https://docs.microsoft.com/en-us/cognitive-toolkit/CNTK-Docker-Containers](https://docs.microsoft.com/en-us/cognitive-toolkit/CNTK-Docker-Containers).
  prefs: []
  type: TYPE_NORMAL
- en: Keras as a backend for Microsoft CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras is a high-level neural network API that''s written in Python that abstracts
    complex configurations and builds production grade training models using matrix
    algebra. Keras is capable of executing on top of Microsoft CNTK, Google TensorFlow,
    or Theano, and has been developed with aim of enabling fast experimentation in
    a sequence or a graph of standalone, fully configurable modules:'
  prefs: []
  type: TYPE_NORMAL
- en: Keras supports both convolutional and recurrent networks and executes on CPU/GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After CNTK activation, Keras can be simply installed by using `pip`. The `keras.json`
    file can be used as the backend of CNTK.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Update `keras.json` at `%USERPROFILE%/.keras` on Windows, or `$HOME/.keras`
    on Linux:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: An overview of the Microsoft Machine Learning Library for Apache Spark (MMLSpark)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Microsoft **Machine Learning Library for Apache Spark** (**MMLSpark**) assists
    in provisioning scalable machine learning models for large datasets, especially
    for building deep learning problems. MMLSpark works with SparkML pipelines, including
    Microsoft CNTK and the OpenCV library, which provide end-to-end support for the
    ingress and processing of image input data, categorization of images, and text
    analytics using pre-trained deep learning algorithms. They also train and retrieve
    scores from classification and regression models by applying featurization.
  prefs: []
  type: TYPE_NORMAL
- en: Environment setup for MMLSpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following prerequisites are mandatory for setting up MMLSpark library for
    deep learning projects on Azure:'
  prefs: []
  type: TYPE_NORMAL
- en: The `MMLSpark` library can be used with the Azure ML workbench
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MMLSpark` can also be integrated with the Azure HDInsight Spark cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of a Databricks Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of an Azure GPU VM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of the Spark/pyspark/Scala(SBT) package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of a Docker container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execution of MMLSpark notebooks using a Docker container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to execute `MMLSpark` Jupyter Notebooks by using a Docker container,
    you can run the following command in a PowerShell prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The execution output of the MMLSpark Jupyter Notebook running on a Docker container
    appear as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7861d68b-1759-415a-a60c-da45977edac1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the notebook is tagged with the name `mmlsparkbook` and is accepting
    the EULA agreement by default. Next, the Docker container needs to be started
    and activated for `mmlsparkbook`, which opens the MMLSpark notebooks at the following
    URL: `http://localhost:8888`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4a40dce-4e83-47eb-acef-82ad9efa53f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Azure HDInsight Spark cluster setup for MMLSpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MMLSpark library can be installed by using an existing Spark cluster and
    applying the `--packages` options, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`spark-shell --packages Azure:MMLSpark:0.13`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pyspark --packages Azure:MMLSpark:0.13`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark-submit --packages Azure:MMLSpark:0.13 MyMMLSparkApp.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, it can be applied for Spark contexts as well, which can be done by
    using MMLSpark in AZTK in the `.aztk/spark-default.conf` file.
  prefs: []
  type: TYPE_NORMAL
- en: More details on the MMLSpark library can be found at the following GitHub link: [https://github.com/Azure/MMLSpark](https://github.com/Azure/mmlspark).
  prefs: []
  type: TYPE_NORMAL
- en: Overview of TensorFlow on Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is an open source, deep learning library that was introduced by Google
    and is used for solving a range of tasks. TensorFlow was introduced to fulfill
    the requirement of building and training complex neural networks in order to detect
    and decipher patterns, recognitions, and correlations, similar to that of the
    learning process of the human brain. Google introduced the TPU (Tensor Processing
    Unit) cloud platform for running the TensorFlow Python API and utilizing TensorFlow
    graph units.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get started on TensorFlow with Azure, the two easiest options are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using Deep Learning toolkit for Data Science VM (Deep Learning VM)**: Provides
    a Windows GPU version of mxnet, CNTK, TensorFlow, and Keras that''s able to run
    on a GPU-NC, N-series, or FPGA infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using Data Science VM for Azure**: Support for CNTK, TensorFlow, MXNet, Caffe,
    Caffe2, DIGITS, H2O, Keras, Theano, and PyTorch is installed by default, and has
    been configured so that it''s ready to use along with the support of NVidia CUDA,
    and cuDNN. Jupyter Notebooks and VS tools with AI are preconfigured as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple computation graph on TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The TensorFlow library is based on computational graphs, such as *a = d * e*,*d
    = b + c*, and *e = c+ 2*, and so this formula can be written as *a = (b+c) * (c+2)*,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ca8392b-d8f0-4ad7-93fe-8727e2590e80.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding graph computation can be parallelized by executing (*d = b + c*
    and *e = c + 2*) and by splitting the calculations on both CPUs and GPUs. For
    complex deep learning problems, especially in Convolutional Neural Network (CNNs)
    and **Recurrent Neural Network** (**RNNs**) architectures, this is essential.
    The concept behind TensorFlow is to have the capability to provision these computational
    graphs in code and allow ...
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow supports a bunch of operations so that it can initialize the graph''s
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to run the operations between the variables, we need to start a TensorFlow
    session, such as `tensorf.Session`. The TensorFlow session is an object where
    all such operations can run. In the TensorFlow session `run` function, the operation
    initializes variables that need to be initialized. Next is an operation. This
    needs to be run and can be executed with the `tfsess.run(a)` command. We can assign
    the output to `a_graphout` so that it can be printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Declaration of the TensorFlow placeholder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow assigns the basic structure of data by using a placeholder variable
    declaration such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Neural Network Formation using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow specifies the Neural Network Formation for solving complex real-life
    problems, especially on CNNs or RNNS. For example, we can use the MNIST dataset
    TensorFlow package, where the dataset contains a 28 x 28 pixel grayscale image
    with approximately 55k rows, 10k testing rows, and 5k validation handwritten digit
    rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For training data and parameters, the placeholder variables can be provisioned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `x` input data layer consists of 28 x 28 = 784 pixels and y nodes
    like 10 digits. Also, for a neural network, the weight and bias also need to be
    initialized. In TensorFlow, there is the possibility of an L-1 number of weights/bias
    tensors or graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: First, we need to declare some variables for `W1` and `b1` for the weights and
    bias for the connections between the input and hidden layer, where the neural
    network will have 300 nodes in the hidden layer. The size of the weight tensor, `W1`,
    is `[784, 300]`. Similarly, TensorFlow supports the NumPy random normal function,
    which assigns to provision a matrix of a given size that's populated with random
    samples. In a similar manner, the weight variable, `W2`, and the bias variable, `b2`,
    connect the hidden layer to the output of the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the hidden layer is calculated by using the `relu` function (by
    applying the rectified linear unit):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The weight multiplication with the output from the hidden layer, and the addition
    of a `b2` bias value, is applied by using the softmax activation for the output
    layer. This can be found via the TensorFlow softmax function `tf.nn.softmax`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For the optimizer, we need to include a cost or loss function. The cross entropy
    cost function is used for this purpose. Here is how we set up the optimizer in
    TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The gradient descent optimizer is supplied by TensorFlow alongside a learning
    rate that''s used to specify the minimized cross entropy cost operation that has
    been provisioned. The function is going to perform gradient descent and back propagation
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The correct prediction operation provides `correct_prediction`, which utilizes
    TensorFlow. `tensorf.equal` provides a true/false reading depending on the arguments
    of the Boolean value. `tensorf.argmax` works in the same way as the NumPy `argmax`
    function, since it returns the index of the maximum value in a particular tensor
    or vector.
  prefs: []
  type: TYPE_NORMAL
- en: Henceforth, the `correct_prediction` operation assigns a tensor of size (`mx1`)
    true or false, designating whether the neural network is correctly predicting
    the digit value.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For provisioning the TensorFlow training, first, the TensorFlow session needs
    to be set up and initialize the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also provides the details from a minibatch training scheme that can be executed
    for the neural network. It also calculates the number of batches to run through
    in each epoch by calculating each training epoch and initializing an `avg_cost`
    variable. TensorFlow supplies an MNIST dataset that has a utility function, such
    as `next_batch`, which makes it easier to extract batches of training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Execution of TensorFlow on Azure using Docker container services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow can be executed using Docker container services on top of Azure
    Linux virtual machines. The endpoint needs to be exposed in the NSG port `8888`,
    and the following command, which initializes a Docker container running TensorFlow
    inside a Jupyter Notebook, needs to be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a2afbfff-6672-406f-8a1b-d6028b910a37.png)'
  prefs: []
  type: TYPE_IMG
- en: Running TensorFlow containers on an Azure Kubernetes Cluster (AKS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A sample Kubernetes cluster on the **Azure container service** (**AKS**) is
    provisioned by using the open source toolkit DLWorkspace ([https://microsoft.github.io/DLWorkspace/](https://microsoft.github.io/DLWorkspace/)).
    The repository provides standard Azure VMs on CPU/GPU. The sample k8 cluster and
    Alluxio-FUSE-enabled k8 pods can be created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample pod configuration is available at the following GitHub link: [https://github.com/jichang1/TensorFlowonAzure/tree/master/Alluxio](https://github.com/jichang1/TensorFlowonAzure/tree/master/Alluxio).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow jobs can be executed on the parameter server pods and worker pods
    by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Other deep learning libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Microsoft provides samples of deep learning tools across Theano, Caffe, MXNet,
    Chainer, PyTorch, and Keras on datasets such as MNIST and CIFAR10\. The following
    are the prerequisites to run these samples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You need Visual Studio 2017 with VS tools for AI and the MNIST dataset. The
    VS tools for AI are available to download from Extensions and Updates under Tools:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/17aba079-166e-4747-993d-319890d26abb.png)'
  prefs: []
  type: TYPE_IMG
- en: An NVIDIA GPU driver/CUDA 9.0/cuDNN 7.0, as applicable, and Python 3.5/3.6\.
    Python 2.x is still not supported (as of the time of writing).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The deep learning libraries that need to be installed include NumPy, SciPy,
    Matplotlib, ONNX, CNTK, TensorFlow, Caffe2 , MXNet, Keras, theano, and PyTorch:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a2655502-f1b4-43ac-801d-5ef11f740bbd.png)'
  prefs: []
  type: TYPE_IMG
- en: The GitHub link for the AI samples repository is available at [https://github.com/Microsoft/samples-for-ai](https://github.com/Microsoft/samples-for-ai).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache MXNet**: Apache MXNet is a scalable deep learning library that''s
    used to train and deploy deep neural networks  that are available to scale across
    GPU or CPU. MXNet offers support for Azure. More details on MXNet are available
    at [https://mxnet.incubator.apache.org/](https://mxnet.incubator.apache.org/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Caffe**: This deep learning framework provides expressions, speed, scalability,
    modularity, openness, and huge community support for building and training complex
    neural networks. Caffe 2.0 is pre-installed in Azure Deep Learning toolkits and
    DSVM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Theano**: This is a Python-based deep learning library that''s used for the
    evaluation of complex mathematical, statistical expressions by using the NumPy-esque
    syntax and is compiled by using CPU/GPU architectures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pytorch**: Pytorch is again a Python-based scientific computing framework
    that''s used for Numpy executions on GPU and Deep Learning interactive research.
    Pytorch allows for interactive debugging with clean dynamic graphs with a mixture
    of high-level and low-level API support. This works for **Artificial Neural Networks**
    (**ANNs**), Regression, and **Convolution Neural Networks** (**CNNs**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chainer**: An open source deep learning library based on Python that''s used
    for NumPy and CuPy `libraries.supports` CUDA implementations and intuitive, flexible
    DL frameworks that allow the use of feed-forward nets, convnets, recurrent nets,
    and recursive nets. More details can be found at [https://chainer.org/](https://chainer.org/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about deep learning methodology and the tools
    that are supported on the Microsoft Azure AI platform. We have demonstrated various
    AI tools, such as CNTK, MMLSpark, and TensorFlow, as well as its execution process
    on Azure deep learning toolkits/data science VMs, along with other open source
    deep learning libraries and utilities.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be looking at a step-by-step overview of integrating
    other Azure services with the Microsoft AI platform.
  prefs: []
  type: TYPE_NORMAL
