<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>3D Computer Vision</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">3D Computer Vision</h1>
                </header>
            
            <article>
                
<p>In the last few chapters, we have discussed the extraction of an object and semantic information from images. We saw how good feature extraction leads to object detection, segmentation, and tracking. This information explicitly requires the geometry of the scene; in several applications, knowing the exact geometry of a scene plays a vital role.&#160;</p>
<p>In this chapter, we will see a discussion leading to the three-dimensional aspects of an image. Here, we will begin by using a simple camera model to understand how pixel values and real-world points are linked correspondingly. Later, we will study methods for computing depth from images and also methods of computing the motion of a camera from a sequence of images.</p>
<p>We will cover the following topics in the chapter:</p>
<ul>
<li>RGDB dataset</li>
<li>Applications to extract features from images</li>
<li>Image formation</li>
<li>Aligning of images</li>
<li>Visual odometry</li>
<li>Visual SLAM</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dataset and libraries</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be using OpenCV for most of the applications. In the last section, for <strong>Visual Simultaneous Localization and Mapping</strong> (<strong>vSLAM</strong>) techniques, we will see the use of an open source repository; directions for its use are mentioned in the section. The dataset is the&#160;<kbd>RGBD</kbd> dataset, consisting of a sequence of images captured using RGB and a depth camera. To download this dataset, visit the following link and download the&#160;<span class="packt_screen">fr1/xyz&#160;</span>tar file:&#160;<a href="https://vision.in.tum.de/data/datasets/rgbd-dataset/download" target="_blank">https://vision.in.tum.de/data/datasets/rgbd-dataset/download</a>.</p>
<p>Alternatively, use the following <span>(Linux only)&#160;</span>command in a Terminal:</p>
<pre><strong>wget https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_xyz.tgz</strong><br/><strong>tar -xvf rgbd_dataset_freiburg1_xyz.tgz</strong></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Applications</h1>
                </header>
            
            <article>
                
<p>While deep learning can extract good features for high-level applications, there are areas that require pixel level matching to compute geometric information from an image. Some of the applications that use this information are:</p>
<ul>
<li><strong>Drones</strong>: In commercial robots like drones, the image sequence is used to compute the motion of the camera mounted on them. This helps them to make robust motion estimations and, in addition to other <strong>Inertial Measurement Units</strong> (<strong>IMU</strong>) such as gyroscopes, accelerometers, and so on, the overall motion is estimated more accurately.&#160;</li>
<li><strong>Image editing applications</strong>: Smartphones and professional applications for image editing include tools like panorama creation, image stitching, and so on. These apps compute orientation from common pixels across image samples and align the images in one target orientation. The resulting image looks as if it has been stitched by joining the end of one image to another.&#160;&#160;</li>
<li><strong>Satellites or space vehicles</strong>: In the remote operation of satellites or robots, it is hard and erroneous to obtain orientation after a significant motion. If the robot moves along a path on the moon, it might get lost due to an error in its local GPS systems or inertial measurement units. In order to build more robust systems, an image-based orientation of the camera is also computed and fused other sensor data to obtain more robust motion estimates.&#160;</li>
<li><strong>Augmented Reality</strong>: With the boom in smartphones and apps and the availability of better hardware, several computer vision algorithms that use geometry information can now run in real time. <strong>Augmented Reality</strong> (<strong>AR</strong>) apps and games use geometrical properties from a sequence of images. These further combine this information with other sensor data to create a seamless AR experience and we can see a virtual object as if it is a real object placed on the table. Tracking planar objects and computing the relative positions of objects and the camera is crucial in these applications.&#160; &#160;</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image formation</h1>
                </header>
            
            <article>
                
<p>The basic camera model is a pinhole camera, though the real-world cameras that we use are far more complex models. A pinhole camera is made up of a very small slit on a plane that allows the formation of an image as depicted in the following figure:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="318" width="472" class="alignnone size-full wp-image-421 image-border" src="images/326a14a8-9a7f-47ef-8bf8-1b6897d80f00.png"/></div>
<p>This camera converts a point in the physical world, often termed the&#160;<em>real world</em>, to a pixel on an image plane. The conversion follows the transformation of the three-dimensional coordinate to two-dimensional coordinates. Here in the image plane, the coordinates are denoted as where <img height="20" width="86" class="alignnone size-full wp-image-423 image-border" src="images/ef95d229-0447-4c95-922a-d5c95b7d3b06.png"/>, <em>P<sub>i</sub></em> is any point on an image. In the physical world, the same point is denoted by <img height="21" width="101" class="alignnone size-full wp-image-424 image-border" src="images/e2f62f14-34eb-4b77-a17b-627cdf391a4a.png"/>,&#160;where <em>P<sub>w</sub></em> is any point in the physical world with a global reference frame.&#160;&#160;</p>
<p><span><em>P<sub>i</sub>(x', y')</em> and <em>P<sub>w</sub>(x, y, z)</em> can be related as, for an ideal pin hole camera:</span></p>
<p><img height="58" width="568" class="fm-editor-equation" src="images/3b1b711b-8dfc-4770-994b-b61e4e047b74.png"/></p>
<p>Here, <em>f</em> is focal length of the camera.&#160;</p>
<p><span class="md-line md-end-block"><span class="md-expand">For further discussion on geometry of image formation, it is necessary to introduce the homogeneous coordinate system. The physical world coordinate system is referred to as <strong>Euclidean coordinate system</strong>.&#160;</span></span><span class="md-line md-end-block"><span class="">In the image plane, a point <em>P'</em> with <em>(x, y)</em> coordinates is represented in homogeneous coordinate system as <em>(x, y, 1)</em>. Similarly a point <em>P<sub>w</sub></em> with <em>(x, y, z)</em> in world coordinates can be represented in homogeneous coordinate system as (x, y, z, 1) .</span></span></p>
<p><span class="md-line md-end-block"><span class="">To convert back from homogeneous to Euclidean, we need to divide by the last coordinate value. To convert a point on an image plane in homogeneous system as (x,y, w) to Euclidean system as (x/w, y/w) . Similarly, for a 3D point in a homogeneous system given by (x, y, z, w), the Euclidean coordinates are given by (x/w, y/w, z/ w).</span></span> <span class="md-line md-end-block md-focus"><span>In this book, the use of homogeneous&#160;coordinate systems will be explicitly mentioned; otherwise we will see equations in the Euclidean coordinate system.</span></span></p>
<p>Image formation comes from transforming physical world coordinates to image plane coordinates but losing information about an extra dimension. This means that when we construct an image we are losing depth information for each pixel. As a result, converting back from image pixel coordinates to real-world coordinates is not possible. As shown in the following figure, for a point <strong>P<sub>I</sub></strong> in the figure there can be an infinite number of points lying along the line. Points&#160;<strong>P1</strong>, <strong>P2</strong>, and&#160;<strong>P3</strong> have the same image pixel location, and therefore estimations of depth (distance from camera) are lost during image formation:&#160;&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="362" width="576" class="alignnone size-full wp-image-425 image-border" src="images/464d2f09-3ffa-4842-87c5-5a3781d1b612.png"/></div>
<p>Let us observe a point world from two images. If we know the optical center of a camera that constructs an image and the point location of two images, we can get much more information. The following figure explains <strong>Epipolar Geometry</strong> using two images:</p>
<div class="CDPAlignCenter CDPAlign"><img height="362" width="596" class="alignnone size-full wp-image-426 image-border" src="images/15a9a57e-f709-4eba-9dc7-3155994cf8c3.png"/></div>
<p>In the previous figure, the camera centers <strong>O<sub>1</sub></strong> and <strong>O<sub>2</sub></strong> are connected to point <strong>P<sub>w</sub></strong> in the world, and the plane forming the line <strong>P<sub>w</sub></strong>, <strong>O<sub>1</sub></strong>, <strong>O<sub>2</sub></strong> is the epipolar plane. The points where the camera's center line O<sub>1</sub>O<sub>2</sub>&#160; intersects with the image are epipoles for the image. These may or may not lie on the image. In cases where both the image planes are parallel, the epipoles will lie at infinity. Here, we can define an epipolar constraint, as if we know the transformation between camera center <strong>O<sub>1</sub></strong> and <strong>O<sub>2</sub></strong> as translation T and rotation R, we can compute the location of point <strong>P1</strong> in <strong>Image 1</strong> to the corresponding location in <strong>Image 2</strong>. Mathematically, this is written as follows:&#160;</p>
<p style="padding-left: 150px"><img height="23" width="140" class="alignnone size-full wp-image-428 image-border" src="images/8137ecdd-e645-4387-bee2-1613721c4bee.png"/></p>
<p>Inversely, if know the location of corresponding points in two images, we would like to compute the rotation matrix R and translation matrix T between the two camera centers. Here, if the two cameras are different, the camera centers can be at the different distance from the image plane and, therefore, we would require camera intrinsic parameters too. Mathematically, this is written as follows:</p>
<p style="padding-left: 120px"><img height="24" width="306" class="alignnone size-full wp-image-429 image-border" src="images/85ab1610-68d7-4026-8425-da6370aa5def.png"/></p>
<p>Here, <em>F</em> is called the&#160;<strong>fundamental matrix</strong> and <em>K</em> is our&#160;<strong>camera intrinsic matrix</strong> for each camera. Computing <em>F</em>, we can know the correct transformation between the two camera poses and can convert any point on one image plane to another.&#160; &#160;&#160;</p>
<p>In the next section, we will see transformations between images and their applications.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Aligning images&#160;</h1>
                </header>
            
            <article>
                
<p>Image alignment is a problem for computing a transformation matrix so that on applying that transformation to an input image, it can be converted to the target image plane. As a result, the resulting images look like they are stitched together and form a continuous larger image.</p>
<p>Panorama is one such example of aligning images, where we collect images of a scene with changing camera angles and the resulting image is a combination of images aligned. A resulting image is as shown, as follows:&#160;</p>
<div class="CDPAlignCenter CDPAlign">&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; <img height="304" width="569" class="alignnone size-full wp-image-430 image-border" src="images/f75b5df7-a580-47da-bfd8-cae9d1f85a5e.png"/></div>
<p>In the preceding figure, an example of panorama creation is shown. Using a camera, we collect multiple images for the same scene by adding overlapping regions. As the camera is moved, often, the pose changes significantly, so therefore for different poses of the camera a transformation matrix is computed.&#160;</p>
<p>Let's get started with a basic method to compute this transformation matrix. The following code works inside Jupyter notebook too. In the following block of code, we define a function to compute <strong>oriented BRIEF</strong> (<strong>ORB</strong>)&#160;keypoints. There is a descriptor for each keypoint also:</p>
<pre><strong>import numpy as np </strong><br/><strong>import matplotlib.pyplot as plt </strong><br/><strong>import cv2 </strong><br/><strong>print(cv2.__version__)</strong><br/><strong>import glob</strong><br/><strong># With jupyter notebook uncomment below line </strong><br/><strong># %matplotlib inline </strong><br/><strong># This plots figures inside the notebook</strong><br/><br/><br/><br/><strong>def compute_orb_keypoints(filename):</strong><br/><strong>    """</strong><br/><strong>    Reads image from filename and computes ORB keypoints</strong><br/><strong>    Returns image, keypoints and descriptors. </strong><br/><strong>    """</strong><br/><strong>    # load image</strong><br/><strong>    img = cv2.imread(filename)</strong><br/>    <br/><strong>    # create orb object</strong><br/><strong>    orb = cv2.ORB_create()</strong><br/>    <br/><strong>    # set method for extraction orb points </strong><br/><strong>    orb.setScoreType(cv2.FAST_FEATURE_DETECTOR_TYPE_9_16)</strong><br/><strong>    orb.setWTA_K(3)</strong><br/>    <br/><strong>    # detect keypoints</strong><br/><strong>    kp = orb.detect(img,None)</strong><br/><br/><strong>    # for detected keypoints compute descriptors. </strong><br/><strong>    kp, des = orb.compute(img, kp)</strong><br/>    <br/><strong>    return img,kp, des</strong></pre>
<p>Once we have feature keypoints, we match them using a brute force matcher, as follows:</p>
<pre><strong>def brute_force_matcher(des1, des2):</strong><br/><strong>    """</strong><br/><strong>    Brute force matcher to match ORB feature descriptors</strong><br/><strong>    """</strong><br/><strong>    # create BFMatcher object</strong><br/><strong>    bf = cv2.BFMatcher(cv2.NORM_HAMMING2, crossCheck=True)</strong><br/><strong>    # Match descriptors.</strong><br/><strong>    matches = bf.match(des1,des2)</strong><br/><br/><strong>    # Sort them in the order of their distance.</strong><br/><strong>    matches = sorted(matches, key = lambda x:x.distance)</strong><br/><br/><strong>    return matches</strong></pre>
<p>This is our main function for computing the fundamental matrix:</p>
<pre><br/><strong>def compute_fundamental_matrix(filename1, filename2):</strong><br/><strong>    """</strong><br/><strong>    Takes in filenames of two input images </strong><br/><strong>    Return Fundamental matrix computes </strong><br/><strong>    using 8 point algorithm</strong><br/><strong>    """</strong><br/><strong>    # compute ORB keypoints and descriptor for each image</strong><br/><strong>    img1, kp1, des1 = compute_orb_keypoints(filename1)</strong><br/><strong>    img2, kp2, des2 = compute_orb_keypoints(filename2)</strong><br/>    <br/><strong>    # compute keypoint matches using descriptor</strong><br/><strong>    matches = brute_force_matcher(des1, des2)</strong><br/>    <br/><strong>    # extract points</strong><br/><strong>    pts1 = []</strong><br/><strong>    pts2 = []</strong><br/><strong>    for i,(m) in enumerate(matches):</strong><br/><strong>        if m.distance &lt; 20:</strong><br/><strong>            #print(m.distance)</strong><br/><strong>            pts2.append(kp2[m.trainIdx].pt)</strong><br/><strong>            pts1.append(kp1[m.queryIdx].pt)</strong><br/><strong>    pts1 = np.asarray(pts1)</strong><br/><strong>    pts2 = np.asarray(pts2)</strong><br/>    <br/><strong>    # Compute fundamental matrix</strong><br/><strong>    F, mask = cv2.findFundamentalMat(pts1,pts2,cv2.FM_8POINT)</strong><br/><strong>    return F</strong><br/><br/><br/><br/><strong># read list of images form dir in sorted order</strong><br/><strong># change here to path to dataset </strong><br/><strong>image_dir = '/Users/mac/Documents/dinoRing/'  </strong><br/><strong>file_list = sorted(glob.glob(image_dir+'*.png'))</strong><br/><br/><strong>#compute F matrix between two images</strong><br/><strong>print(compute_fundamental_matrix(file_list[0], file_list[2]))</strong><br/><br/></pre>
<p>In the next section, we will extend relative transformation between images to compute camera pose and also estimate the trajectory of the camera.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Visual odometry</h1>
                </header>
            
            <article>
                
<p>Odometry is the process of incrementally estimating the position of a robot or device. In the case of a wheeled robot, it uses wheel motion or inertial measurement using tools such as gyroscopes or accelerometers to estimate the robot's position by summing over wheel rotations. Using <strong>visual odometry</strong> (<strong>VO</strong>), we can estimate the odometry of cameras using only image sequences by continuously estimating camera motion.&#160;</p>
<p>A major use of VO is in autonomous robots like drones, where gyroscopes and accelerometers are not robust enough for motion estimation. However, there are several assumptions and challenges in using VO:</p>
<ul>
<li>Firstly, objects in the scene for the camera should be static. While the camera captures a sequence of the image, the only moving object should be the camera itself.</li>
<li>Moreover, during the estimation of VO, if there are significant illumination changes, like light source appearance, drastic changes to pixel values might occur in subsequent images. As a result, VO suffers from large errors or complete dysfunction. The same case applies to dark environments; due to the lack of illumination, VO is not able to estimate robust motion.</li>
</ul>
<p>The process of VO is described as follows:</p>
<ol>
<li>Initialize the starting position as the origin, for the frame of reference. All the subsequent motion estimation is done with respect to this frame.&#160;</li>
<li>As an image arrives, compute features and match corresponding features with previous frames to get a transformation matrix.&#160;</li>
<li>Use the historical transformation matrix between all subsequent frames to compute the trajectory of the camera.&#160;&#160;</li>
</ol>
<p>&#160;</p>
<p>This process is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="235" width="488" class="alignnone size-full wp-image-431 image-border" src="images/b2d2ae77-0f07-4849-923e-e3bdf08d3532.png"/></div>
<p>Here, <strong>I<sub>i</sub></strong> is the&#160;i<sup>th</sup> image received from the camera and&#160;<strong>T<sub>ij</sub></strong> is the transformation matrix computed using feature matching between <em>i</em> and <em>j</em> images. The trajectory of camera motion is shown with stars, where <em>P<sub>i</sub></em> is the estimated pose of the camera at the&#160;<em>i</em><sup>th</sup> image. This can be a two-dimensional pose with (<em>x</em>, <em>y</em>) angle as well as a three-dimensional pose. Each <em>P<sub>j</sub></em> is computed as applying the transformation <em>T<sub>ij</sub></em> on <em>P<sub>i</sub></em>.</p>
<p>Other than the assumption mentioned earlier, there are a few limitations to VO estimation:&#160;</p>
<ul>
<li>As more images are observed from the sequence, the errors in trajectory estimation are accumulated. This results in an overall drift in the computed track of camera motion.&#160;</li>
<li>In cases of sudden motion in camera,&#160; the image feature match between the corresponding two images will be significantly erroneous. As a result, the estimated transformation between the frames will also have huge errors and, therefore, the overall trajectory of camera motion gets highly distorted.&#160;&#160;</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Visual SLAM</h1>
                </header>
            
            <article>
                
<p>SLAM refers to Simultaneous Localization and Mapping and is one of the most common problems in robot navigation. Since a mobile robot does not have hardcoded information about the environment around itself, it uses sensors onboard to construct a representation of the region. The robot tries to estimate its position with respect to objects around it like trees, building, and so on. This is, therefore, a chicken-egg problem, where&#160;the robot first tries to localize itself using objects around it and then uses its obtained location to map objects around it; hence the term <em>Simultaneous&#160; Localization and Mapping</em>. There are several methods for solving the SLAM problem. In this section, we will discuss special types of SLAM using a single RGB camera.&#160;</p>
<p>Visual SLAM methods extend visual odometry by computing a more robust camera trajectory as well as constructing a robust representation of the environment. An overview of Visual SLAM in action is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="309" width="548" class="alignnone size-full wp-image-432 image-border" src="images/f8abfbd3-a703-44ca-a782-c9728fe0ea75.png"/></div>
<p>This is an overview of a generic SLAM method composed of an undirected graph. Each node of the graph is composed of a keyframe which represents unique information about the world and also contains the camera pose (<em>x</em>,<em>y</em>, angle) for the location. In between, keyframes are frames that overlap significantly with the keyframes scene, however, they help in computing robust estimates of pose for the next frame. Here, a camera starts the process by initializing a keyframe at the origin. As the camera moves along a trajectory, the SLAM system updates the graph by adding keyframes or frames based on criteria. If the camera returns back to a previously seen area, it links up with the old frame, creating a cyclic structure in the graph. This is often called <strong>loop closure</strong> and helps correct the overall graph structure. The edges connecting nodes to another in the graph are usually weighted with a&#160; transformation matrix between the pose of the two nodes. Overall, the graph structure is corrected by improving the position of keyframes. This is done by minimizing overall error. Once a graph is constructed, it can be saved and used for localizing a camera by matching to the nearest keyframe.&#160; &#160;</p>
<p>In this section, we will see a popular robust method, ORB SLAM, using monocular cameras. This method constructs a graph structure similar to that which was shown previously to keep track of camera pose and works on RGB image frames from a simple camera. The steps involved can be summarized as:</p>
<ol>
<li><strong>Input</strong>: In the case of the monocular camera, the input is a single captured frame.&#160;</li>
<li><strong>Initialization</strong>: Once the process starts, a map is initialized with the origin, and the first node of a keyframe graph is constructed.</li>
<li>There are three threads that run in parallel for the system:
<ul>
<li><strong>Tracking</strong>: For each incoming frame, ORB features are extracted for matching. These features are matched with previously seen frames and are then used to compute the relative pose of the current frame. This also decides if the current frame is to be kept as a keyframe or used as a normal frame.&#160;</li>
<li><strong>Local mapping</strong>: If a new keyframe is determined from tracking, the overall map is updated with the insertion of a new node in the graph. While a new connection between neighbourhood keyframes is formed, redundant connections are removed.&#160;</li>
<li><strong>Loop closure</strong>: If there is a previously seen keyframe that matches the current keyframe, a loop is formed. This gives extra information about drift caused by the trajectory of the camera pose and as a result, all node poses in the graph map is corrected by an optimization algorithm.&#160;</li>
</ul>
</li>
</ol>
<p>In the following section, we will use an implementation of ORB SLAM2 from&#160;<a href="https://github.com/raulmur/ORB_SLAM2">https://github.com/raulmur/ORB_SLAM2</a>. This is not a Python-based implementation. The instruction provided there can be used to build the package and can be used to see visual SLAM. However, for demonstration purposes, we will use a Docker container version of it.&#160;</p>
<p>A Docker is a container platform that provides the distributed shipping of an environment <span>as if they are packed inside a ship container,&#160;</span> as well as code to run applications. We need to install the Docker platform and pull an image of the environment, as well as the code. The environment inside the image is independent of the platform we use, as long as the Docker platform is installed. If you want to learn more about Docker and containers, the following website provides more details, as well as installation instructions:&#160;<a href="https://www.docker.com/what-docker">https://www.docker.com/what-docker</a>.&#160;</p>
<p>Once Docker is installed, we can begin with the following steps for ORB SLAM 2. Let's start by pulling a Docker image (this is similar to cloning a repository) for ORB SLAM:</p>
<pre><strong>docker pull resbyte/orb-slam2</strong></pre>
<p>This will download the environment for the package and pre-build the ORB SLAM2 repository so that we don't have to build it again. All the dependencies for this repository are already satisfied inside the Docker image.&#160;&#160;</p>
<p>Once the Docker image is downloaded, let's get started with downloading the dataset. In this section, we will use the <kbd>TUM RGBD</kbd> dataset, which was collected specifically to evaluate SLAM and VO methods. Earlier in this chapter, under dataset and libraries, we saw how to download this dataset. We will use the extracted dataset in the following section.&#160;</p>
<p>Since this implementation of ORB SLAM uses a GUI interface to output the results, we will first add the GUI interface to the Docker image. The following code assumes a Linux environment.&#160;</p>
<p>For the GUI output from ORB SLAM, add this as the first step, otherwise, visual SLAM will run but there will be an error:</p>
<pre><strong>xhost +local:docker</strong></pre>
<p>Now, let's launch the downloaded image using the Docker platform, though with several parameters:</p>
<pre><strong>docker run -ti --rm   -e DISPLAY=$DISPLAY   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /home/rgbd_dataset_freiburg1_xyz:/root/rgbd_dataset_freiburg1_xyz   orb-slam:latest /bin/bash</strong></pre>
<p>Here, the&#160;<kbd>-e</kbd> and <kbd>-v</kbd>&#160;parameters are used to set the display environment for GUI. The dataset downloaded before is shared inside Docker using <kbd>- v&#160;$PATH_TO_DOWNLOADED_DATASET:$PATH_INSIDE_DOCKER</kbd>. Finally, the name of the image is <kbd>orb-slam: latest</kbd><em>,</em> which we downloaded earlier using Docker pull, and we asked it to run bash inside Docker using <kbd>/bin/bash</kbd>.&#160;</p>
<p>On running the previous command, we can see a change in the Terminal, as if we logging in to a new computer Terminal. Let's go and run ORB-SLAM as follows:</p>
<pre><strong>cd ORB_SLAM2</strong><br/><br/><strong># run orb slam</strong><br/><strong>./Examples/Monocular/mono_tum Vocabulary/ORBvoc.txt Examples/Monocular/TUM1.yaml /root/rgbd_dataset_freiburg1_xyz</strong></pre>
<p>Here, the first parameter is to run Monocular Visual SLAM, as there are other methods too. The other parameters are to run the type of dataset that we had downloaded earlier. If there is any change in the dataset, these parameters are to be changed accordingly.&#160;</p>
<p>On this command, after some time there will be two windows, shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/30ce9f93-8e72-400e-a603-0bcf7713d6d2.png"/></div>
<p>Here, the window on the right is the input dataset, with the keypoints detected in each frame. While the window on the left details the Visual SLAM happening. As we can see there are blue boxes that show the keyframe graph creation, with the current state of the position of the camera and its links with the historical position. As the camera in the dataset is moved, the graph is created and adjusted as more observations are found. The result is an accurate trajectory of the camera as well as adjusted keypoints.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, the aim was to view computer vision from a geometrical point of view. Starting with understanding how an image is formed using a pinhole camera, there was a discussion on how to incorporate three-dimensional worlds using multi-image formation. We saw an explanation of Visual Odometry with an introduction to Visual SLAM. The various steps involved in SLAM were explained and a demo of using ORB-SLAM was also shown, so that we could see a SLAM operation as it happened. This is basic motivation to extend the SLAM solution for various other datasets, and so create interesting applications.&#160;&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Sturm Jürgen, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. <em>A Benchmark for the Evaluation of RGB-D SLAM Systems</em>. In&#160;</span>Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on<span>, pp. 573-580. IEEE, 2012.</span></li>
<li><span>Mur-Artal Raul, Jose Maria Martinez Montiel, and Juan D. Tardos. <em>ORB-SLAM: A Versatile and Accurate Monocular SLAM System</em>.&#160;</span>IEEE Transactions on Robotics<span>&#160;31, no. 5 (2015): 1147-1163.</span></li>
<li><span>Rublee Ethan, Vincent Rabaud, Kurt Konolige, and Gary Bradski. <em>ORB: an efficient alternative to SIFT or SURF</em>. In&#160;</span>Computer Vision (ICCV), 2011 IEEE international conference on<span>, pp. 2564-2571. IEEE, 2011.</span></li>
</ul>


            </article>

            
        </section>
    </div>
</body>
</html>