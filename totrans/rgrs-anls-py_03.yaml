- en: Chapter 3. Multiple Regression in Action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced linear regression as a supervised method
    for machine learning rooted in statistics. Such a method forecasts numeric values
    using a combination of predictors, which can be continuous numeric values or binary
    variables, given the assumption that the data we have at hand displays a certain
    relation (a linear one, measurable by a correlation) with the target variable.
    To smoothly introduce many concepts and easily explain how the method works, we
    limited our example models to just a single predictor variable, leaving to it
    all the burden of modeling the response.
  prefs: []
  type: TYPE_NORMAL
- en: However, in real-world applications, there may be some very important causes
    determining the events you want to model but it is indeed rare that a single variable
    could take the stage alone and make a working predictive model. The world is complex
    (and indeed interrelated in a mix of causes and effects) and often it cannot be
    easily explained without considering various causes, influencers, and hints. Usually
    more variables have to work together to achieve better and reliable results from
    a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Such an intuition decisively affects the complexity of our model, which from
    this point forth will no longer be easily represented on a two-dimensional plot.
    Given multiple predictors, each of them will constitute a dimension of its own
    and we will have to consider that our predictors are not just related to the response
    but also related among themselves (sometimes very strictly), a characteristic
    of data called **multicollinearity**.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting, we'd like to write just a few words on the selection of topics
    we are going to deal with. Though in the statistical literature there is a large
    number of publications and books devoted to regression assumptions and diagnostics,
    you'll hardly find anything here because we will leave out such topics. We will
    be limiting ourselves to discussing problems and aspects that could affect the
    results of a regression model, on the basis of a practical data science approach,
    not a purely statistical one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given such premises, in this chapter we are going to:'
  prefs: []
  type: TYPE_NORMAL
- en: Extend the procedures for a single regression to a multiple one, keeping an
    eye on possible sources of trouble such as multicollinearity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the importance of each term in your linear model equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make your variables work together and increase your ability to predict using
    interactions between variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leverage polynomial expansions to increase the fit of your linear model with
    non-linear functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To recap the tools seen in the previous chapter, we reload all the packages
    and the Boston dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are working on the code in an IPython Notebook (as we strongly suggest),
    the following magic command will allow you to visualize plots directly on the
    interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We are still using the Boston dataset, a dataset that tries to explain different
    house prices in the Boston of the 70s, given a series of statistics aggregated
    at the census zone level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will always work by keeping with us a series of informative variables, the
    number of observation and variable names, the input data matrix, and the response
    vector at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Model building with Statsmodels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a first step toward extending to more predictors the previously done analysis
    with Statsmodels, let''s reload the necessary modules from the package (one working
    with matrices and the other with formulas) :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also prepare a suitable input matrix, naming it `Xc` after having it
    incremented by an extra column containing the bias vector (a constant variable
    having the unit value):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After having fitted the preceding specified model, let''s immediately ask for
    a summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Model building with Statsmodels](img/00034.jpeg)![Model building with Statsmodels](img/00035.jpeg)![Model
    building with Statsmodels](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Basically, the enunciations of the various statistical measures, as presented
    in the previous chapter, are still valid. We will just devote a few words to remarking
    on a couple of extra features we couldn't mention before because they are related
    to the presence of multiple predictors.
  prefs: []
  type: TYPE_NORMAL
- en: First, the adjusted R-squared is something to take note of now. When working
    with multiple variables, the standard R-squared can get inflated because of the
    many coefficients inserted into the model. If you are using too many predictors,
    its measure will diverge perceptibly from the plain R-squared. The adjusted R-squared
    considers the complexity of the model and reports a much more realistic R-squared
    measure.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just make a ratio between the plain and the adjusted R-squared measure. Check
    if their difference exceeds 20%. If it does, it means that we have introduced
    some redundant variables inside our model specification. Naturally, the larger
    the ratio difference, the more serious the problem.
  prefs: []
  type: TYPE_NORMAL
- en: This is not the case in our example because the difference is quite slight,
    approximately between 0.741 and 0.734, which translated into a ratio turns out
    to be *0.741/0.734 = 1.01*, that is just 1% over the standard R-squared.
  prefs: []
  type: TYPE_NORMAL
- en: Then, working with so many variables at a time, coefficients should also be
    checked for important warnings. The risk involved is having coefficients picking
    up noisy and non-valuable information. Usually such coefficients will not be far
    from zero and will be noticeable because of their large standard errors. Statistical
    t-tests are the right tool to spot them.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be aware that variables with a low p-value are good candidates for being removed
    from the model because there will probably be little proof that their estimated
    coefficient is different from zero.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, being largely not significant (p-value major of 0.05), the `AGE`
    and `INDUS` variables are represented in the model by coefficients whose usefulness
    could be seriously challenged.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the condition number test (`Cond. No.`) is another previously mentioned
    statistic that now acquires a fresh importance under the light of a system of
    predictors. It signals numeric unstable results when trying an optimization based
    on matrix inversion. The cause of such instability is due to multicollinearity,
    a problem we are going to expand on in the following paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a condition number is over the score of 30, there's a clear signal that
    unstable results are rendering the result less reliable. Predictions may be affected
    by errors and the coefficients may drastically change when rerunning the same
    regression analysis with a subset or a different set of observations.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the condition number is well over 30, and that's a serious warning
    signal.
  prefs: []
  type: TYPE_NORMAL
- en: Using formulas as an alternative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To obtain the same results using the `statsmodels.formula.api` and thereby
    explicating a formula to be interpreted by the Patsy package ([http://patsy.readthedocs.org/en/latest/](http://patsy.readthedocs.org/en/latest/)),
    we use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this case, you have to explicate all the variables to enter into model building
    by naming them on the right side of the formula. After fitting the model, you
    can use all the previously seen Statsmodels methods for reporting the coefficients
    and results.
  prefs: []
  type: TYPE_NORMAL
- en: The correlation matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When trying to model the response using a single predictor, we used Pearson's
    correlation (Pearson was the name of its inventor) to estimate a coefficient of
    linear association between the predictor and the target. Having more variables
    in the analysis now, we are still quite interested in how each predictor relates
    to the response; however, we have to distinguish whether the relation between
    the variance of the predictor and that of the target is due to unique or shared
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: The measurement of the association due to unique variance is called **partial
    correlation** and it expresses what can be guessed of the response thanks to the
    information uniquely present in a variable. It represents the exclusive contribution
    of a variable in predicting the response, its unique impact as a direct cause
    to the target (if you can view it as being a cause though, because, as seen, correlation
    is not causation).
  prefs: []
  type: TYPE_NORMAL
- en: The shared variance is instead the amount of information that is simultaneously
    present in a variable and in other variables in the dataset at hand. Shared variance
    can have many causes; maybe one variable causes or it just interferes with the
    other (as we described in the previous chapter in the *Correlation is not causation*
    section ). Shared variance, otherwise called **collinearity** (between two variables)
    or multicollinearity (among three or more variables), has an important effect,
    worrisome for the classical statistical approach, less menacing for the data science
    one.
  prefs: []
  type: TYPE_NORMAL
- en: For the statistical approach, it has to be said that high or near perfect multicollinearity
    not only often renders coefficient estimations impossible (matrix inversion is
    not working), but also, when it is feasible, it will be affected by imprecision
    in coefficient estimation, leading to large standard errors of the coefficients.
    However, the predictions won't be affected in any way and that leads us to the
    data science points of view.
  prefs: []
  type: TYPE_NORMAL
- en: Having multicollinear variables, in fact, renders it difficult to select the
    correct variables for the analysis (since the variance is shared, it is difficult
    to figure out which variable should be its causal source), leading to sub-optimal
    solutions that could be resolved only by augmenting the number of observations
    involved in the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine the manner and number of predictors affecting each other, the
    right tool is a correlation matrix, which, though a bit difficult to read when
    the number of the features is high, is still the most direct way to ascertain
    the presence of shared variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The correlation matrix](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: At first glance, some high correlations appear to be present in the order of
    the absolute value of 0.70 (highlighted by hand in the matrix) between `TAX`,
    `NOX`, `INDUS`, and `DIS`. That's fairly explainable since `DIS` is the distance
    from employment centers, `NOX` is a pollution indicator, `INDUS` is the quota
    of non-residential or commercial buildings in the area, and `TAX` is the property
    tax rate. The right combination of these variables can well hint at what the productive
    areas are.
  prefs: []
  type: TYPE_NORMAL
- en: 'A faster, but less numerical representation is to build a heat map of the correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The correlation matrix](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Having a cut at 0.5 correlation (which translates into a 25% shared variance),
    the heat map immediately reveals how **PTRATIO** and **B** are not so related
    to other predictors. As a reminder of the meaning of variables, **B** is an indicator
    quantifying the proportion of colored people in the area and **PTRATIO** is the
    pupil-teacher ratio in the schools of the area. Another intuition provided by
    the map is that a cluster of variables, namely **TAX**, **INDUS**, **NOX**, and
    **RAD**, is confirmed to be in strong linear association.
  prefs: []
  type: TYPE_NORMAL
- en: 'An even more automatic way to detect such associations (and figure out numerical
    problems in a matrix inversion) is to use eigenvectors. Explained in layman''s
    terms, eigenvectors are a very smart way to recombine the variance among the variables,
    creating new features accumulating all the shared variance. Such recombination
    can be achieved using the NumPy `linalg.eig` function, resulting in a vector of
    eigenvalues (representing the amount of recombined variance for each new variable)
    and eigenvectors (a matrix telling us how the new variables relate to the old
    ones):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: After extracting the eigenvalues, we print them in descending order and look
    for any element whose value is near to zero or small compared to the others. Near
    zero values can represent a real problem for normal equations and other optimization
    methods based on matrix inversion. Small values represent a high but not critical
    source of multicollinearity. If you spot any of these low values, keep a note
    of their index in the list (Python indexes start from zero).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Using their index position in the list of eigenvalues, you can recall their
    specific vector from eigenvectors, which contains all the variable loadings—that
    is, the level of association with the original variables. In our example, we investigate
    the eigenvector at index `8`. Inside the eigenvector, we notice values at index
    positions `2`, `8`, and `9`, which are indeed outstanding in terms of absolute
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We now print the variables'' names to know which ones contribute so much by
    their values to build the eigenvector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Having found the multicollinearity culprits, what remedy could we use for such
    variables? Removal of some of them is usually the best solution and that will
    be carried out in an automated way when exploring how variable selection works
    in [Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 6. Achieving Generalization"), *Achieving Generalization*.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In continuity with the previous chapter, we carry on our explanation and experimentation
    with gradient descent. As we have already defined both the mathematical formulation
    and their translation into Python code, using matrix notation, we don't need to
    worry if now we have to deal with more than one variable at a time. Having used
    the matrix notation allows us to easily extend our previous introduction and example
    to multiple predictors with just minor changes to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we have to take note that, by introducing more parameters to
    be estimated during the optimization procedure, we are actually introducing more
    dimensions to our line of fit (turning it into a hyperplane, a multidimensional
    surface) and such dimensions have certain communalities and differences to be
    taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Working with different features requires more attention when estimating the
    coefficients because of their similarities which can cause a variance increase
    of the estimates, as we initially discussed. Multicollinearity between variables
    also has other drawbacks because it can also make matrix inversion (the matrix
    operation at the core of the normal equation coefficient estimation) very difficult,
    if not impossible, to achieve; such a problem is due to a mathematical limitation
    of the algorithm. Gradient descent, instead, is not affected at all by reciprocal
    correlation, allowing us to estimate reliable coefficients even in the presence
    of perfect collinearity.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, though being quite resistant to problems that affect other approaches,
    its simplicity also makes it vulnerable to other common problems, such as the
    different scale present in each feature. In fact, some features in your data may
    be represented by measurements in units, some in decimals, and others in thousands,
    depending on what aspect of reality each feature represents. In our real estate
    example, one feature could be the number of rooms, another one could be the percentage
    of certain pollutants in the air, and finally, the average value of a house in
    the neighborhood. When it is the case that the features have a different scale,
    though the algorithm will be processing each of them separately, optimization
    will be dominated by the variables with the more extensive scale. Working in a
    space of dissimilar dimensions will require more iterations before convergence
    to a solution (and sometimes there might be no convergence at all).
  prefs: []
  type: TYPE_NORMAL
- en: The remedy is very easy; it is just necessary to put all the features on the
    same scale. Such an operation is called **feature scaling**. Feature scaling can
    be achieved through standardization or normalization. Normalization rescales all
    the values in the interval between zero and one (usually, but different ranges
    are also possible), whereas standardization operates by removing the mean and
    dividing by standard deviation to obtain a unit variance. In our case, standardization
    is preferable, both because it easily permits retuning the obtained standardized
    coefficients into their original scale and also because, by centering all the
    features at the zero mean, it makes the error surface more tractable by many machine
    learning algorithms, in a much more effective way than just rescaling the maximum
    and minimum of a variable.
  prefs: []
  type: TYPE_NORMAL
- en: An important reminder when applying feature scaling is that changing the scale
    of the features implies that you will have to use rescaled features also for predictions,
    unless you can recalculate the coefficients as if the variables had never been
    rescaled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try the algorithm, first using standardization based on the Scikit-learn
    `preprocessing` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we just standardized the variables using the `StandardScaler`
    class from Scikit-learn. This class can fit a data matrix, record its column means
    and standard deviations, and operate a transformation on itself, as well as on
    any other similar matrix, standardizing the column data. With this method, after
    fitting we keep a track of means and standard deviations that have been used because
    they will come in handy later when we have to recalculate the coefficients using
    the original scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The code we are using is not at all different from the code we used in the previous
    chapter with the exception of its input, which is now made up of multiple standardized
    variables. In this case, the algorithm reaches a convergence in fewer iterations
    and uses a smaller alpha than before because in our previous example our single
    variable actually was unstandardized. Observing the output, we now need a way
    to rescale the coefficients to their variables' characteristics and we will be
    able to report the gradient descent solution in unstandardized form.
  prefs: []
  type: TYPE_NORMAL
- en: Another point to mention is our choice of alpha. After some tests, the value
    of `0.02` has been chosen for its good performance on this very specific problem.
    Alpha is the learning rate and during optimization it can be fixed or changeable,
    accordingly to a line search method, modifying its value in order to minimize
    as far as possible the cost function at each single step of the optimization process.
    In our example, we opted for a fixed learning rate and we had to look for its
    best value by trying a few optimization values and deciding on which minimized
    the cost in the minor number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Unstandardizing coefficients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given a vector of standardized coefficients from a linear regression and its
    bias, we can recall the formulation of the linear regression, which is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unstandardizing coefficients](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous formula, transforming the predictors using their mean and standard
    deviation, is actually equivalent (after a few calculations) to such an expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unstandardizing coefficients](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In our formula, ![Unstandardizing coefficients](img/00041.jpeg) represents the
    original mean and *δ* the original variance of the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'By comparing the different parts of the two formulas (the first parenthesis
    and the second summation), we can calculate the bias and coefficient equivalents
    when transforming a standardized coefficient into an unstandardized one. Without
    replicating all the mathematical formulas, we can quickly implement them into
    Python code and immediately provide an application showing how such calculations
    can transform gradient descent coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Unstandardizing coefficients](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As an output from the previous code snippet, you will get a list of coefficients
    identical to our previous estimations with both Scikit-learn and Statsmodels.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating feature importance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After having confirmed the values of the coefficients of the linear model we
    have built, and after having explored the basic statistics necessary to understand
    if our model is working correctly, we can start auditing our work by first understanding
    how a prediction is made up. We can obtain this by accounting for each variable''s
    role in the constitution of the predicted values. A first check to be done on
    the coefficients is surely on the directionality they express, which is simply
    dictated by their sign. Based on our expertise on the subject (so it is advisable
    to be knowledgeable about the domain we are working on), we can check whether
    all the coefficients correspond to our expectations in terms of directionality.
    Some features may decrease the response as we expect, thereby correctly confirming
    that they have a coefficient with a negative sign, whereas others may increase
    it, so a positive coefficient should be correct. When coefficients do not correspond
    to our expectations, we have **reversals**. Reversals are not uncommon and they
    can actually reveal that things work in a different way than we expected. However,
    if there is much multicollinearity between our predictors, reversals could be
    just due to the higher uncertainty of estimates: some estimates may be so uncertain
    that the optimization processes have allocated them a wrong sign. Consequently,
    when a linear regression doesn''t confirm our expectations, it is better not to
    jump to any quick conclusion; instead, closely inspect all the statistical measures
    pointing toward multicollinearity.'
  prefs: []
  type: TYPE_NORMAL
- en: A second check is done on the impact of the variable on the model—that is, how
    much of the predicted result is dominated by variations in feature. Usually, if
    the impact is low, reversals and other difficulties caused by the variable (it
    could be from an unreliable source, for instance, or very noisy—that is, measured
    imprecisely) are less disruptive for the predictions or can even be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the idea of impact also presents the possibility of making our model
    economic in terms of the number of modeled coefficients. Up to now, we have just
    concentrated on the idea that it is desirable to fit data in the best way possible
    and we didn't check if our predictive formula generalizes well to new data. Starting
    to rank the predictors could help us make new simpler models (by just selecting
    only the most important features in the model) and simpler models are less prone
    to errors when in the production phase.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, if our objective is not simply to fit our present data maximally with
    a formula, but to also fit future data well, it is necessary to apply the principle
    of Occam's razor. This suggests that, given more correct answers, simpler models
    are always preferable to more complex ones. The core idea is not to make an explanation,
    that is a linear model, more complex than it should be, because complexity may
    hide overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting standardized coefficients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Extending our interpretation of the linear model from one single variable to
    a host of them, we can still read each single coefficient as the unit change inducted
    on the response variable by each predictor (keeping the other predictors constant).
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, larger coefficients seem to impact more on the result of the linear
    combination; however, as we noticed while revisiting gradient descent, different
    variables may have different scales and their coefficients may incorporate this.
    Being smaller or larger in terms of coefficient may just be because of the variable's
    relative scale in comparison to the other features involved in the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: By standardizing variables, we place them under a similar scale where a unit
    is the standard deviation of the variable itself. Variables extending from high
    to low values (with a larger range) tend to have a greater standard deviation
    and you should expect them to be reduced. By doing so, most variables with a normal-like
    distribution should be reduced in the range *-3 < x < +3*, thus allowing a comparison
    on their contribution to the prediction. Highly skewed distributions won't be
    standardized in the range *-3 <x <+3*. The transformation will be beneficial anyway
    because their range is going to be largely reduced and after that it will even
    make sense to compare different distributions because then they will all represent
    the same unit measure—that is, the unit variance. After standardization, larger
    coefficients can be interpreted as major contributions to establishing the result
    (a weighted summation, so the result will resemble larger weights more closely).
    Using standardized coefficients, we can therefore confidently rank our variables
    and spot those contributing less.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s proceed to an example using our Boston dataset. This time we will be
    using the `LinearRegression` method from Scikit-learn because we do not need to
    do a linear model for its statistical properties, but just a working model using
    a fast and scalable algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In such an initialization, apart from the `fit_intercept` parameter that explicates
    the insertion of a bias into the design of model, the `normalize` option indicates
    whether we intend to rescale all the variables in the range between zero and one.
    Such a transformation is different from statistical standardization and we will
    omit it for the moment by setting it in a `False` state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides the `StandardScaler` seen before, we also import from Scikit-learn
    the convenient `make_pipeline` wrapper, which allows us to establish a sequence
    of operations to be done automatically on our data before feeding it to the linear
    regression analysis. Now, the `Stand_coef_linear_reg` pipeline will execute a
    statistical standardization on data before regressing it, thus outputting standardized
    coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Inspecting standardized coefficients](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As a first step, we output the coefficients of the regression on unstandardized
    data. As seen before, the output seems dominated by the huge coefficient of the
    NOX variable, which overlooks (with its absolute value of about 17.8) minor coefficients
    of 3.8 and less. However, we may question whether it could be so after also standardizing
    the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Inspecting standardized coefficients](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Having all the predictors on a similar scale now, we can easily provide a more
    realistic interpretation of each coefficient. Clearly, it appears that a unit
    change has more impact when it involves the variables `LSTAT`, `DIS`, `RM`, `RAD`,
    and `TAX`. `LSTAT` is the percentage of lower status population, and this aspect
    explains its relevancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using standardized scales has certainly pointed us at the most important variables
    but it is still not a complete overview of the predictive power of each variable
    because:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Standardized coefficients represent how well the model works out its predictions
    because large coefficients heavily impact the resulting response: though having
    a large coefficient is certainly a hint of how important a variable is, it tells
    us just a part of the role that the variable plays in reducing the error of the
    estimates and in making our predictions more accurate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardized coefficients can be ranked but their unit, though similar in scale,
    is somehow abstract (the standard deviation of each variable) and relative to
    the data at hand (so we shouldn't compare the standardized coefficients of different
    datasets because their standard deviations could be different)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One solution could be to integrate the importance estimate based on standardized
    coefficients, instead using some measure related to the error measure.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing models by R-squared
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a general point of view, we can evaluate a model by comparing how better
    it does in respect of a simple mean, and that's the coefficient of determination,
    R-squared.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared can estimate how good a model is; therefore, by comparing the R-squared
    of our model against alternative models where the variables have been removed,
    we can get an idea of how predictive each removed variable is. All we have to
    do is compute the difference between the coefficients of determination of the
    initial model against the model without that variable. If the difference is large,
    the variable is very important in the determination of a better R-squared and
    of a better model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we have to first record what the R-squared is when we build the
    model with all the variables present. We can name such a value our baseline of
    comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, all we have to do is to remove one variable at a time from the
    set of predictors, to estimate again the regression model recording its coefficient
    of determination, and subtract it from the baseline value we got from the complete
    regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![Comparing models by R-squared](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: After we get all the differences, each one representing the contribution of
    each variable to the R-squared, we just have to rank them and we will then have
    an idea of which variables contribute more in reducing the error of the linear
    model; this is a different point of view from knowing which variable contributed
    the most to the response value. Such a contribution is called the partial R-squared.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from suggesting we use both measures (standardized coefficients and partial
    Rsquared) to separate relevant variables from irrelevant ones, by using the partial
    Rsquared you can actually directly compare the importance of the variables because
    using ratios does make sense here (so you can tell that a variable is twice as
    important as another because it reduces the error twice as much).
  prefs: []
  type: TYPE_NORMAL
- en: Another noticeable point is that partial R-squareds are not really a decomposition
    of the initial R-squared measure. In fact, only if the predictors are uncorrelated
    by summing all the partial scores will you get the precise coefficient of determination
    of the full model. This is due to collinearity between variables. Therefore, when
    you remove a variable from the model, you certainly do not remove all its informative
    variance since correlated variables, containing similar information to the removed
    variable, are kept in the model. It may happen that, if you have two highly correlated
    variables, removing each in turn won't change the R-squared by much because, as
    one is removed, the other one will provide the missing information (thus, the
    double-check with the standardized coefficient is not redundant at all).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are more sophisticated methods to estimate the variable importance, but
    these two methods should provide you with enough insight. Knowing what variables
    impact your results more strongly can provide you with the means to:'
  prefs: []
  type: TYPE_NORMAL
- en: Try to explain the results to management in a reasonable and understandable
    way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prioritize your work, in terms of data cleaning, preparation, and transformation,
    by concentrating first on the features most relevant to the success of your project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conserve resources, in particular memory, as less data is used when building
    and using the regression model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you would like to use importance to exclude irrelevant variables using one
    of the measures we presented, the safest way would be to recalculate the ranking
    every time you decide to exclude a variable from the set. Otherwise, using a single
    ranking may risk hiding the true importance of highly correlated variables (and
    that's true for both the methodologies, though standardized coefficients are a
    bit more revealing).
  prefs: []
  type: TYPE_NORMAL
- en: Such an approach is certainly time-consuming, but it is necessary when you notice
    that your model, though presenting a good fit on your present data, cannot generalize
    well to new observations.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to discuss such circumstances in more depth in [Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 6. Achieving Generalization"), *Achieving Generalization*, when we will
    illustrate the best ways to reduce your predictor set, maintaining (simplifying
    your solution) and even improving your predictive performances (generalizing more
    effectively).
  prefs: []
  type: TYPE_NORMAL
- en: Interaction models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having explained how to build a regression model with multiple variables and
    having touched on the theme of its utilization and interpretation, we start from
    this paragraph to explore how to improve it. As a first step, we will work on
    its fit with present data. In the following chapters, devoted to model selection
    and validation, we will concentrate on how to make it really generalizable—that
    is, capable of correctly predicting on new, previously unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: As we previously reasoned, the beta coefficients in a linear regression represent
    the link between a unit change in the predictors and the response variations.
    The assumptions at the core of such a model are of a constant and unidirectional
    relationship between each predictor and the target. It is the linear relationship
    assumption, having the characteristics of a line where direction and fluctuation
    are determined by the angular coefficient (hence the name linear regression, hinting
    at the operation of regressing, tracing back to the linear form from some data
    evidence). Although a good approximation, a linear relationship is a simplification
    often not true in real data. In fact, most relationships are non-linear, showing
    bends and curves and alternating different fluctuations in increase and decrease.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that we do not have to limit ourselves to the originally provided
    features, but we can modify them in order to *straighten* their relationship with
    the target variable. In fact, the more similarity between the predictor and the
    response, the better the fit and the fewer prediction errors in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, we can say that:'
  prefs: []
  type: TYPE_NORMAL
- en: We can improve our linear regression by transforming predictor variables in
    various ways
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can measure such improvement using the partial R-squared, since every transformation
    should impact on the quantity of residual errors and ultimately on the coefficient
    of determination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering interactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first sources of non-linearity is due to possible interactions between
    predictors. Two predictors interact when the effect of one of them on the response
    variable varies in respect of the values of the other predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematical formulation, interaction terms (the interacting variables)
    have to be multiplied by themselves for our linear model to catch the supplementary
    information of their relation as expressed in this example of a model with two
    interacting predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discovering interactions](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An easy-to-remember example of an interaction in a regression model is the role
    of engine noise in the evaluation of a car. If you are going to model the preference
    for car models, you will immediately notice that the engine noise can either decrease
    or increase consumer preference for the car, depending on the price of the car
    (or its category, which is a proxy of monetary value). In fact, if the car is
    inexpensive being silent is clearly a must-have, but if the car is expensive (such
    as a Ferrari or another sports car) the noise is an outstanding benefit (clearly
    when in it, you want to have everyone notice the cool car you are driving).
  prefs: []
  type: TYPE_NORMAL
- en: It may sound a little bit tricky to deal with interaction, but actually it isn't;
    after all, you are just transforming a variable role in a linear regression based
    on another one. Finding interaction terms can be achieved in two different ways,
    the first one being domain knowledge—that is, knowing directly the problem you
    are modeling and incorporating your expertise in it. When you do not have such
    an expertise, an automatic search over the possible combinations will suffice
    if it is well tested using a revealing measure such as R-squared.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to illustrate the automatic search approach is to show an example
    in Python using the `PolynomialFeatures` from Scikit-learn, a function that allows
    both interactions and polynomial expansions (we are going to talk about them in
    the next paragraph):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: By the `degree` parameter we define how many variables to put into the interaction,
    it being possible to have three or even more variables interact with each other.
    Interactions in statistics are called two-way effects, three-way effects, and
    so on, depending on the number of variables involved (whereas the original variables
    are instead called the main effects).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After recalling the baseline R-squared value, the code creates a new input
    data matrix using the `fit_transform` method, enriching the original data with
    the interaction effects of all the variables. At this point, we create a series
    of new linear regression models, each one containing all the main effects plus
    a single interaction. We measure the improvement, calculate the difference with
    the baseline, and then report only interactions over a certain threshold. We can
    decide on a threshold just above zero or a threshold we determine based on a statistical
    test. In our example we decided on an arbitrary threshold, aiming at reporting
    all R-squared increment above `0.01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Discovering interactions](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Relevant interaction effects are clearly made up by the variable `''RM''` (one
    of the most important ones, as seen before) and the strongest improvement is given
    by its interaction with another key feature, `LSTAT`. An important take away would
    be that we add it to our original data matrix, as a simple multiplication between
    the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Polynomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an extension of interactions, polynomial expansion systematically provides
    an automatic means of creating both interactions and non-linear power transformations
    of the original variables. Power transformations are the bends that the line can
    take in fitting the response. The higher the degree of power, the more bends are
    available to fit the curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you have a simple linear regression of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial regression](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'By a second degree transformation, called **quadratic**, you will get a new
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial regression](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'By a third degree transformation, called **cubic**, your equation will turn
    into:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial regression](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If your regression is a multiple one, the expansion will create additional
    terms (interactions) increasing the number of new features derived from the expansion.
    For instance, a multiple regression made up of two predictors (*x[1]* and *x[2]*),
    expanded using the quadratic transformation, will become:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial regression](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Before proceeding, we have to note two aspects of the expansion procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial expansion rapidly increases the number of predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher-degree polynomials translate into high powers of the predictors, posing
    problems for numeric stability, thus requiring suitable numeric formats or standardizing
    numeric values that are too large
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing linear versus cubic transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By setting the `interaction_only` parameter off in the `PolynomialFeatures`
    function seen before, we can get the full polynomial transformation of our input
    matrix, not just simply the interactions as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'By sending both `PolynomialFeatures` and `LinearRegression` into a pipeline
    we can create a function automatically by a single command, expanding out data
    and regressing it. As an experiment, we try to model the `''LSTAT''` variable
    alone for best clarity, remembering that we could have expanded all the variables
    at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Testing linear versus cubic transformation](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Our first fit is the linear one (a simple linear regression) and from the scatterplot
    we can notice that the line is not representing well the cloud of points relating
    to `''LSTAT''` with the response; most likely we need a curve. Instead of testing
    a second degree transformation that will turn into a parabola, we immediately
    try a cubic transformation: using two bends, we should obtain a better fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![Testing linear versus cubic transformation](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Our graphic check confirms that now we have a more credible representation of
    how `'LSTAT'` and the response relate. We question whether we cannot do better
    using even higher-degree transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Going for higher-degree solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To test a higher degree of polynomial transformations, we prepare a script
    that creates the expansion and reports its R-squared measure. We then try to plot
    the function with the highest degree in the series and have a look at how it fits
    the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![Going for higher-degree solutions](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Noticeably, there is a huge difference in the coefficient of determination
    between the linear model and the quadratic expansion (second-degree polynomial).
    The measure jumps from `0.544` to `0.641`, a difference that increases up to `0.682`
    when reaching the fifth degree. Preceding to even higher degrees, the increment
    is not so astonishing, though it keeps on growing, reaching `0.695` when the degree
    is the fifteenth. As the latter is the best result in terms of coefficient of
    determination, having a look at the plot on the data cloud will reveal a not so
    smooth fit, as we can see with lower degrees of polynomial expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Going for higher-degree solutions](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Observing closely the resulting curve, you will surely notice how, by such a
    high degree, the curve tends strictly to follow the distribution of points, going
    erratic when the density diminishes at the fringes of the range of the predictors'
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing underfitting and overfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Polynomial regression offers us the right occasion for starting to talk about
    model complexity. We have not explicitly tested it, but you may already have got
    the feeling that, by increasing the degree of the polynomial expansion, you are
    going to always reap better fits. We say more: the more variables you have in
    your model, the better, until you will have such a large number of betas, likely
    equal or almost equal to the number of your observations, that your predictions
    will be perfect.'
  prefs: []
  type: TYPE_NORMAL
- en: Decaying performances due to over-parameterization (an excess of parameters
    to be learned by the model) is a problem of linear regression and of many other
    machine learning algorithms. The more parameters you add, the better the fit because
    the model will cease to intercept the rules and regularities of your data but
    will start, in such an embarrassment of riches, to populate the many available
    coefficients with all the erratic and erroneous information present in the data.
    In such a situation, the model won't learn general rules but it will just be memorizing
    the dataset itself in another form.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is called **overfitting**: fitting the data at hand so well that the result
    is far from being an extraction of the form of the data to draw predictions from;
    the result is just a mere memorization. On the other side, another problem is
    **underfitting**—that is, when you are using too few parameters for your prediction.
    The most straightforward example is fitting a non-linear relation using a simple
    linear regression; clearly, it won''t match the curve bends and some of its predictions
    will be misleading.'
  prefs: []
  type: TYPE_NORMAL
- en: There are appropriate tools for verifying if you are underfitting or, more likely,
    overfitting and they will be discussed in [Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 6. Achieving Generalization"), *Achieving Generalization*, (the chapter
    explaining data science); in the meantime, don't overfit too much with high-degree
    polynomials!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have carried on introducing linear regression, extending
    our example from a simple to a multiple one. We have revisited the previous outputs
    from the Statsmodels linear functions (the classical statistical approach) and
    gradient descent (the data science engine).
  prefs: []
  type: TYPE_NORMAL
- en: We started experimenting with models by removing selected predictors and evaluating
    the impact of such a move from the point of view of the R-squared measure. Meanwhile
    we also discovered reciprocal correlations between predictors and how to render
    more linear relations between each predictor and the target variable by intercepting
    the interactions and by means of polynomial expansion of the features.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will progress again and extend the regression model
    to make it viable for classification tasks, turning it into a probabilistic predictor.
    The conceptual jump into the world of probability will allow us to complete the
    range of possible problems where linear models can be successfully applied.
  prefs: []
  type: TYPE_NORMAL
