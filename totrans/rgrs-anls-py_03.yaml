- en: Chapter 3. Multiple Regression in Action
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章：实际操作中的多元回归
- en: In the previous chapter, we introduced linear regression as a supervised method
    for machine learning rooted in statistics. Such a method forecasts numeric values
    using a combination of predictors, which can be continuous numeric values or binary
    variables, given the assumption that the data we have at hand displays a certain
    relation (a linear one, measurable by a correlation) with the target variable.
    To smoothly introduce many concepts and easily explain how the method works, we
    limited our example models to just a single predictor variable, leaving to it
    all the burden of modeling the response.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了线性回归作为基于统计学的机器学习监督方法。这种方法通过预测因子的组合来预测数值，这些预测因子可以是连续的数值或二元变量，假设我们手头的数据显示与目标变量之间存在某种关系（一种线性关系，可以通过相关性来衡量）。为了顺利引入许多概念并轻松解释该方法的工作原理，我们将示例模型限制在只有一个预测变量，将建模响应的所有负担都留给了它。
- en: However, in real-world applications, there may be some very important causes
    determining the events you want to model but it is indeed rare that a single variable
    could take the stage alone and make a working predictive model. The world is complex
    (and indeed interrelated in a mix of causes and effects) and often it cannot be
    easily explained without considering various causes, influencers, and hints. Usually
    more variables have to work together to achieve better and reliable results from
    a prediction.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际应用中，可能存在一些非常重要的原因决定着你想要建模的事件，但确实很少有一个变量能够单独登台并构建一个有效的预测模型。世界是复杂的（并且确实在原因和效果中相互关联），通常不考虑各种原因、影响因素和线索，很难轻易解释。通常需要更多的变量协同工作，才能从预测中获得更好和可靠的结果。
- en: Such an intuition decisively affects the complexity of our model, which from
    this point forth will no longer be easily represented on a two-dimensional plot.
    Given multiple predictors, each of them will constitute a dimension of its own
    and we will have to consider that our predictors are not just related to the response
    but also related among themselves (sometimes very strictly), a characteristic
    of data called **multicollinearity**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这种直觉决定性地影响了我们模型的复杂性，从这一点起，我们的模型将不再容易在二维图上表示。给定多个预测因子，每个预测因子都将构成其自身的维度，我们必须考虑我们的预测因子不仅与响应相关，而且彼此之间（有时非常严格）相关，这是数据的一个特征，称为**多重共线性**。
- en: Before starting, we'd like to write just a few words on the selection of topics
    we are going to deal with. Though in the statistical literature there is a large
    number of publications and books devoted to regression assumptions and diagnostics,
    you'll hardly find anything here because we will leave out such topics. We will
    be limiting ourselves to discussing problems and aspects that could affect the
    results of a regression model, on the basis of a practical data science approach,
    not a purely statistical one.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们想就我们将要处理的主题选择说几句话。尽管在统计文献中有很多关于回归假设和诊断的出版物和书籍，但你在这里几乎找不到任何东西，因为我们将省略这些主题。我们将限制自己讨论可能影响回归模型结果的问题和方面，基于实际的数据科学方法，而不是纯粹统计的方法。
- en: 'Given such premises, in this chapter we are going to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的前提下，在本章中，我们将要：
- en: Extend the procedures for a single regression to a multiple one, keeping an
    eye on possible sources of trouble such as multicollinearity
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将单变量回归的程序扩展到多元回归，同时关注可能的问题来源，如多重共线性
- en: Understand the importance of each term in your linear model equation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解你线性模型方程中每个术语的重要性
- en: Make your variables work together and increase your ability to predict using
    interactions between variables
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让你的变量协同工作，通过变量间的交互作用提高你的预测能力
- en: Leverage polynomial expansions to increase the fit of your linear model with
    non-linear functions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用多项式展开来提高你的线性模型与非线性函数的拟合度
- en: Using multiple features
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多个特征
- en: 'To recap the tools seen in the previous chapter, we reload all the packages
    and the Boston dataset:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾上一章中看到的工具，我们重新加载所有包和波士顿数据集：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you are working on the code in an IPython Notebook (as we strongly suggest),
    the following magic command will allow you to visualize plots directly on the
    interface:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在IPython笔记本中编写代码（我们强烈建议这样做），以下魔法命令将允许你直接在界面上可视化图表：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We are still using the Boston dataset, a dataset that tries to explain different
    house prices in the Boston of the 70s, given a series of statistics aggregated
    at the census zone level:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍在使用波士顿数据集，这是一个试图解释70年代波士顿不同房价的数据集，给定一系列在人口普查区层面的汇总统计数据：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will always work by keeping with us a series of informative variables, the
    number of observation and variable names, the input data matrix, and the response
    vector at hand:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将始终通过保留一系列信息变量来工作，包括观测数量、变量名称、输入数据矩阵和手头的响应向量：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Model building with Statsmodels
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Statsmodels构建模型
- en: 'As a first step toward extending to more predictors the previously done analysis
    with Statsmodels, let''s reload the necessary modules from the package (one working
    with matrices and the other with formulas) :'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为将Statsmodels中之前所做的分析扩展到更多预测变量的第一步，让我们重新加载包中必要的模块（一个用于矩阵，另一个用于公式）：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s also prepare a suitable input matrix, naming it `Xc` after having it
    incremented by an extra column containing the bias vector (a constant variable
    having the unit value):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也准备一个合适的输入矩阵，将其命名为`Xc`，在增加一个包含偏置向量的额外列之后（这是一个具有单位值的常量变量）：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After having fitted the preceding specified model, let''s immediately ask for
    a summary:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合了前面的指定模型后，让我们立即请求一个摘要：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Model building with Statsmodels](img/00034.jpeg)![Model building with Statsmodels](img/00035.jpeg)![Model
    building with Statsmodels](img/00036.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![使用Statsmodels构建模型](img/00034.jpeg)![使用Statsmodels构建模型](img/00035.jpeg)![使用Statsmodels构建模型](img/00036.jpeg)'
- en: Basically, the enunciations of the various statistical measures, as presented
    in the previous chapter, are still valid. We will just devote a few words to remarking
    on a couple of extra features we couldn't mention before because they are related
    to the presence of multiple predictors.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，如前一章所述的各种统计度量陈述仍然有效。我们只是用几句话来强调一些我们之前无法提及的额外功能，因为它们与多个预测变量的存在有关。
- en: First, the adjusted R-squared is something to take note of now. When working
    with multiple variables, the standard R-squared can get inflated because of the
    many coefficients inserted into the model. If you are using too many predictors,
    its measure will diverge perceptibly from the plain R-squared. The adjusted R-squared
    considers the complexity of the model and reports a much more realistic R-squared
    measure.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，现在需要注意调整后的R平方。当与多个变量一起工作时，标准R平方会因为模型中插入的许多系数而膨胀。如果你使用太多的预测变量，其度量将明显偏离普通R平方。调整后的R平方考虑了模型的复杂性，并报告了一个更加现实的R平方度量。
- en: Tip
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Just make a ratio between the plain and the adjusted R-squared measure. Check
    if their difference exceeds 20%. If it does, it means that we have introduced
    some redundant variables inside our model specification. Naturally, the larger
    the ratio difference, the more serious the problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 只需在普通R平方和调整后的R平方度量之间做一个比率。检查它们的差异是否超过20%。如果超过，这意味着我们在模型规范中引入了一些冗余变量。自然地，比率差异越大，问题越严重。
- en: This is not the case in our example because the difference is quite slight,
    approximately between 0.741 and 0.734, which translated into a ratio turns out
    to be *0.741/0.734 = 1.01*, that is just 1% over the standard R-squared.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中并非如此，因为差异相当小，大约在0.741和0.734之间，转换成比率得出*0.741/0.734 = 1.01*，即只是标准R平方的1%以上。
- en: Then, working with so many variables at a time, coefficients should also be
    checked for important warnings. The risk involved is having coefficients picking
    up noisy and non-valuable information. Usually such coefficients will not be far
    from zero and will be noticeable because of their large standard errors. Statistical
    t-tests are the right tool to spot them.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一次处理这么多变量时，也应检查系数的重要警告。涉及的风险是系数获取了嘈杂且无价值的信息。通常这样的系数不会远离零，并且会因为它们的大标准误差而明显。统计t检验是发现它们的正确工具。
- en: Tip
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Be aware that variables with a low p-value are good candidates for being removed
    from the model because there will probably be little proof that their estimated
    coefficient is different from zero.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，具有低p值的变量是模型中移除的好候选，因为可能几乎没有证据表明它们的估计系数与零不同。
- en: In our example, being largely not significant (p-value major of 0.05), the `AGE`
    and `INDUS` variables are represented in the model by coefficients whose usefulness
    could be seriously challenged.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，由于大部分不显著（p值主要大于0.05），`AGE`和`INDUS`变量在模型中由系数表示，其有用性可能受到严重质疑。
- en: Finally, the condition number test (`Cond. No.`) is another previously mentioned
    statistic that now acquires a fresh importance under the light of a system of
    predictors. It signals numeric unstable results when trying an optimization based
    on matrix inversion. The cause of such instability is due to multicollinearity,
    a problem we are going to expand on in the following paragraphs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，条件数测试（`Cond. No.`）是另一个之前提到的统计量，现在在预测因子系统下获得了新的重要性。当尝试基于矩阵求逆进行优化时，它表示数值不稳定的结果。这种不稳定性的原因是多重共线性，我们将在接下来的段落中对其进行扩展。
- en: Tip
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**提示**'
- en: When a condition number is over the score of 30, there's a clear signal that
    unstable results are rendering the result less reliable. Predictions may be affected
    by errors and the coefficients may drastically change when rerunning the same
    regression analysis with a subset or a different set of observations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当条件数超过30的分数时，有一个明确的信号表明不稳定的结果正在使结果变得不那么可靠。预测可能会受到错误的影响，当重新运行相同的回归分析时，系数可能会发生剧烈变化，无论是使用子集还是不同的观察数据集。
- en: In our case, the condition number is well over 30, and that's a serious warning
    signal.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，条件数远远超过30，这是一个严重的警告信号。
- en: Using formulas as an alternative
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用公式作为替代
- en: 'To obtain the same results using the `statsmodels.formula.api` and thereby
    explicating a formula to be interpreted by the Patsy package ([http://patsy.readthedocs.org/en/latest/](http://patsy.readthedocs.org/en/latest/)),
    we use:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`statsmodels.formula.api`获得相同的结果，从而解释一个由Patsy包（[http://patsy.readthedocs.org/en/latest/](http://patsy.readthedocs.org/en/latest/)）解释的公式，我们使用：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this case, you have to explicate all the variables to enter into model building
    by naming them on the right side of the formula. After fitting the model, you
    can use all the previously seen Statsmodels methods for reporting the coefficients
    and results.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你必须通过在公式的右侧命名它们来明确所有变量以进入模型构建。在拟合模型后，你可以使用之前看到的Statsmodels方法来报告系数和结果。
- en: The correlation matrix
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**相关矩阵**'
- en: When trying to model the response using a single predictor, we used Pearson's
    correlation (Pearson was the name of its inventor) to estimate a coefficient of
    linear association between the predictor and the target. Having more variables
    in the analysis now, we are still quite interested in how each predictor relates
    to the response; however, we have to distinguish whether the relation between
    the variance of the predictor and that of the target is due to unique or shared
    variance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试使用单个预测因子来建模响应时，我们使用了皮尔逊相关（皮尔逊是其发明者的名字）来估计预测因子和目标之间的线性关联系数。现在分析中变量更多，我们仍然非常感兴趣的是每个预测因子如何与响应相关；然而，我们必须区分预测因子的方差和目标方差之间的关联是由于独特方差还是相互方差。
- en: The measurement of the association due to unique variance is called **partial
    correlation** and it expresses what can be guessed of the response thanks to the
    information uniquely present in a variable. It represents the exclusive contribution
    of a variable in predicting the response, its unique impact as a direct cause
    to the target (if you can view it as being a cause though, because, as seen, correlation
    is not causation).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于独特方差引起的关联测量被称为**偏相关**，它表达了由于变量中独特存在的信息所能推测出的响应。它代表了变量在预测响应中的独特贡献，其作为直接原因对目标的影响（如果你可以将其视为原因的话，因为，如所见，相关性并不等同于因果关系）。
- en: The shared variance is instead the amount of information that is simultaneously
    present in a variable and in other variables in the dataset at hand. Shared variance
    can have many causes; maybe one variable causes or it just interferes with the
    other (as we described in the previous chapter in the *Correlation is not causation*
    section ). Shared variance, otherwise called **collinearity** (between two variables)
    or multicollinearity (among three or more variables), has an important effect,
    worrisome for the classical statistical approach, less menacing for the data science
    one.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 相互方差则是同时存在于变量和手头数据集中其他变量中的信息量。相互方差可能有多种原因；可能是一个变量引起或只是干扰了另一个（正如我们在上一章的*相关性不等于因果关系*部分所描述的）。相互方差，也称为**共线性**（两个变量之间）或**多重共线性**（三个或更多变量之间），具有重要作用，对经典统计方法来说是个令人担忧的问题，而对数据科学方法来说则不那么令人畏惧。
- en: For the statistical approach, it has to be said that high or near perfect multicollinearity
    not only often renders coefficient estimations impossible (matrix inversion is
    not working), but also, when it is feasible, it will be affected by imprecision
    in coefficient estimation, leading to large standard errors of the coefficients.
    However, the predictions won't be affected in any way and that leads us to the
    data science points of view.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于统计方法，必须说，高度或几乎完美的多重共线性不仅经常使系数估计变得不可能（矩阵求逆不起作用），而且，即使可行，它也会受到系数估计不准确的影响，导致系数的标准误差很大。然而，预测不会受到影响，这使我们转向数据科学观点。
- en: Having multicollinear variables, in fact, renders it difficult to select the
    correct variables for the analysis (since the variance is shared, it is difficult
    to figure out which variable should be its causal source), leading to sub-optimal
    solutions that could be resolved only by augmenting the number of observations
    involved in the analysis.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，存在多重共线性变量会使选择分析中正确的变量变得困难（由于方差是共享的，很难确定哪个变量是其因果来源），导致次优解，这些解只能通过增加分析中涉及的观测数量来解决。
- en: 'To determine the manner and number of predictors affecting each other, the
    right tool is a correlation matrix, which, though a bit difficult to read when
    the number of the features is high, is still the most direct way to ascertain
    the presence of shared variance:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定影响彼此的预测变量的方式和数量，合适的工具是相关矩阵，尽管当特征数量较多时阅读起来有些困难，但它仍然是确定共享方差存在最直接的方式：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will give the following output:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![The correlation matrix](img/00037.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![相关矩阵](img/00037.jpeg)'
- en: At first glance, some high correlations appear to be present in the order of
    the absolute value of 0.70 (highlighted by hand in the matrix) between `TAX`,
    `NOX`, `INDUS`, and `DIS`. That's fairly explainable since `DIS` is the distance
    from employment centers, `NOX` is a pollution indicator, `INDUS` is the quota
    of non-residential or commercial buildings in the area, and `TAX` is the property
    tax rate. The right combination of these variables can well hint at what the productive
    areas are.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，一些高相关性似乎出现在`TAX`、`NOX`、`INDUS`和`DIS`之间的绝对值0.70的顺序（在矩阵中用手标出）。这是可以解释的，因为`DIS`是就业中心的距离，`NOX`是污染指标，`INDUS`是该地区非住宅或商业建筑的比例，而`TAX`是财产税率。这些变量的正确组合可以很好地暗示生产区域。
- en: 'A faster, but less numerical representation is to build a heat map of the correlations:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更快但数值表示较少的方法是构建相关性的热图：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will give the following output:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![The correlation matrix](img/00038.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![相关矩阵](img/00038.jpeg)'
- en: Having a cut at 0.5 correlation (which translates into a 25% shared variance),
    the heat map immediately reveals how **PTRATIO** and **B** are not so related
    to other predictors. As a reminder of the meaning of variables, **B** is an indicator
    quantifying the proportion of colored people in the area and **PTRATIO** is the
    pupil-teacher ratio in the schools of the area. Another intuition provided by
    the map is that a cluster of variables, namely **TAX**, **INDUS**, **NOX**, and
    **RAD**, is confirmed to be in strong linear association.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当相关系数达到0.5（这相当于25%的共享方差）时，热图立即揭示**PTRATIO**和**B**与其他预测变量之间关系不大。作为变量意义的提醒，**B**是一个量化该地区有色人种比例的指标，**PTRATIO**是该地区学校的师生比例。地图提供的另一个直觉是，一组变量，即**TAX**、**INDUS**、**NOX**和**RAD**，被证实存在强烈的线性关联。
- en: 'An even more automatic way to detect such associations (and figure out numerical
    problems in a matrix inversion) is to use eigenvectors. Explained in layman''s
    terms, eigenvectors are a very smart way to recombine the variance among the variables,
    creating new features accumulating all the shared variance. Such recombination
    can be achieved using the NumPy `linalg.eig` function, resulting in a vector of
    eigenvalues (representing the amount of recombined variance for each new variable)
    and eigenvectors (a matrix telling us how the new variables relate to the old
    ones):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更自动化的方式来检测此类关联（并解决矩阵求逆中的数值问题）是使用特征向量。用通俗易懂的话来说，特征向量是一种非常聪明的重新组合变量之间方差的方法，创建新的特征，累积所有共享的方差。这种重新组合可以通过使用NumPy的`linalg.eig`函数实现，从而得到特征值向量（代表每个新变量的重新组合方差量）和特征向量（一个矩阵，告诉我们新变量与旧变量之间的关系）：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After extracting the eigenvalues, we print them in descending order and look
    for any element whose value is near to zero or small compared to the others. Near
    zero values can represent a real problem for normal equations and other optimization
    methods based on matrix inversion. Small values represent a high but not critical
    source of multicollinearity. If you spot any of these low values, keep a note
    of their index in the list (Python indexes start from zero).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取特征值后，我们按降序打印它们，并寻找任何值接近零或与其他值相比较小的元素。接近零的值可能代表正常方程和其他基于矩阵求逆的优化方法的一个真正问题。较小的值代表一个高但非关键的多重共线性来源。如果你发现任何这些低值，请记住它们在列表中的索引（Python索引从零开始）。
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Using their index position in the list of eigenvalues, you can recall their
    specific vector from eigenvectors, which contains all the variable loadings—that
    is, the level of association with the original variables. In our example, we investigate
    the eigenvector at index `8`. Inside the eigenvector, we notice values at index
    positions `2`, `8`, and `9`, which are indeed outstanding in terms of absolute
    value:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过它们在特征值列表中的索引位置，你可以从特征向量中召回它们的特定向量，该向量包含所有变量载荷——即与原始变量的关联程度。在我们的例子中，我们研究了索引为`8`的特征向量。在特征向量内部，我们注意到索引位置`2`、`8`和`9`的值，这些值在绝对值方面确实非常突出：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We now print the variables'' names to know which ones contribute so much by
    their values to build the eigenvector:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们打印变量名称，以了解哪些变量通过它们的值对构建特征向量做出了如此大的贡献：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Having found the multicollinearity culprits, what remedy could we use for such
    variables? Removal of some of them is usually the best solution and that will
    be carried out in an automated way when exploring how variable selection works
    in [Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 6. Achieving Generalization"), *Achieving Generalization*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 找到多重共线性问题后，我们能为这样的变量采取什么补救措施？通常，移除其中一些是最佳解决方案，这将在探索[第6章](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "第6章。实现泛化")中变量选择如何工作的方式时自动执行，*实现泛化*。
- en: Revisiting gradient descent
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视梯度下降法
- en: In continuity with the previous chapter, we carry on our explanation and experimentation
    with gradient descent. As we have already defined both the mathematical formulation
    and their translation into Python code, using matrix notation, we don't need to
    worry if now we have to deal with more than one variable at a time. Having used
    the matrix notation allows us to easily extend our previous introduction and example
    to multiple predictors with just minor changes to the algorithm.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一章的内容相连续，我们继续用梯度下降法进行解释和实验。因为我们已经定义了数学公式及其用矩阵表示的Python代码翻译，所以我们现在不需要担心是否需要同时处理多个变量。使用矩阵表示法使我们能够轻松地将先前的介绍和例子扩展到多个预测变量，只需对算法进行一些小的修改。
- en: In particular, we have to take note that, by introducing more parameters to
    be estimated during the optimization procedure, we are actually introducing more
    dimensions to our line of fit (turning it into a hyperplane, a multidimensional
    surface) and such dimensions have certain communalities and differences to be
    taken into account.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 特别要注意的是，通过在优化过程中引入更多需要估计的参数，我们实际上是在我们的拟合线上引入了更多维度（将其变成一个超平面，一个多维表面），这些维度具有一定的共性和差异需要考虑。
- en: Feature scaling
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征缩放
- en: Working with different features requires more attention when estimating the
    coefficients because of their similarities which can cause a variance increase
    of the estimates, as we initially discussed. Multicollinearity between variables
    also has other drawbacks because it can also make matrix inversion (the matrix
    operation at the core of the normal equation coefficient estimation) very difficult,
    if not impossible, to achieve; such a problem is due to a mathematical limitation
    of the algorithm. Gradient descent, instead, is not affected at all by reciprocal
    correlation, allowing us to estimate reliable coefficients even in the presence
    of perfect collinearity.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在估计系数时，由于特征之间的相似性可能导致估计值的方差增加，正如我们最初讨论的那样，处理不同的特征需要更多的注意。变量之间的多重共线性也有其他缺点，因为它也可能使矩阵求逆（正常方程系数估计的核心矩阵运算）变得非常困难，甚至不可能实现；这样的问题是由于算法的数学限制造成的。相反，梯度下降法根本不受相互相关性影响，即使在完全共线性的情况下，我们也能估计出可靠的系数。
- en: Anyway, though being quite resistant to problems that affect other approaches,
    its simplicity also makes it vulnerable to other common problems, such as the
    different scale present in each feature. In fact, some features in your data may
    be represented by measurements in units, some in decimals, and others in thousands,
    depending on what aspect of reality each feature represents. In our real estate
    example, one feature could be the number of rooms, another one could be the percentage
    of certain pollutants in the air, and finally, the average value of a house in
    the neighborhood. When it is the case that the features have a different scale,
    though the algorithm will be processing each of them separately, optimization
    will be dominated by the variables with the more extensive scale. Working in a
    space of dissimilar dimensions will require more iterations before convergence
    to a solution (and sometimes there might be no convergence at all).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，尽管它对影响其他方法的常见问题具有相当大的抵抗力，但其简单性也使其容易受到其他常见问题的攻击，例如每个特征中存在的不同尺度。事实上，你的数据中的一些特征可能表示的是以单位为单位的测量值，一些是十进制，还有一些是千位数，这取决于每个特征代表现实世界的哪个方面。在我们的房地产例子中，一个特征可能是房间数量，另一个可能是空气中某些污染物的百分比，最后是邻居中房屋的平均价值。当特征具有不同的尺度时，尽管算法将分别处理每个特征，但优化将由尺度更大的变量主导。在一个不同维度的空间中工作将需要更多的迭代才能收敛到解决方案（有时甚至可能根本无法收敛）。
- en: The remedy is very easy; it is just necessary to put all the features on the
    same scale. Such an operation is called **feature scaling**. Feature scaling can
    be achieved through standardization or normalization. Normalization rescales all
    the values in the interval between zero and one (usually, but different ranges
    are also possible), whereas standardization operates by removing the mean and
    dividing by standard deviation to obtain a unit variance. In our case, standardization
    is preferable, both because it easily permits retuning the obtained standardized
    coefficients into their original scale and also because, by centering all the
    features at the zero mean, it makes the error surface more tractable by many machine
    learning algorithms, in a much more effective way than just rescaling the maximum
    and minimum of a variable.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方法非常简单；只需将所有特征置于相同的尺度上即可。这种操作称为**特征缩放**。特征缩放可以通过标准化或归一化实现。归一化将所有值缩放到零和一之间的区间（通常如此，但也可能使用不同的范围），而标准化则是通过减去均值并除以标准差来获得单位方差。在我们的例子中，标准化更可取，因为它可以轻松地将获得的标准化系数重新调整到原始尺度，并且通过将所有特征中心化到零均值，它使得错误表面对于许多机器学习算法来说更容易处理，比仅仅缩放变量的最大值和最小值要有效得多。
- en: An important reminder when applying feature scaling is that changing the scale
    of the features implies that you will have to use rescaled features also for predictions,
    unless you can recalculate the coefficients as if the variables had never been
    rescaled.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用特征缩放时，一个重要的提醒是，改变特征的范围意味着你将不得不使用缩放后的特征来进行预测，除非你能够重新计算系数，就像变量从未被缩放一样。
- en: 'Let''s try the algorithm, first using standardization based on the Scikit-learn
    `preprocessing` module:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试这个算法，首先使用基于Scikit-learn `preprocessing`模块的标准规范化：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding code, we just standardized the variables using the `StandardScaler`
    class from Scikit-learn. This class can fit a data matrix, record its column means
    and standard deviations, and operate a transformation on itself, as well as on
    any other similar matrix, standardizing the column data. With this method, after
    fitting we keep a track of means and standard deviations that have been used because
    they will come in handy later when we have to recalculate the coefficients using
    the original scale:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们只是使用了Scikit-learn的`StandardScaler`类来标准化变量。这个类可以拟合一个数据矩阵，记录其列均值和标准差，并对自身以及任何其他类似矩阵进行转换，标准化列数据。使用这种方法，在拟合后，我们保留均值和标准差，因为它们将在我们稍后需要使用原始尺度重新计算系数时派上用场：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The code we are using is not at all different from the code we used in the previous
    chapter with the exception of its input, which is now made up of multiple standardized
    variables. In this case, the algorithm reaches a convergence in fewer iterations
    and uses a smaller alpha than before because in our previous example our single
    variable actually was unstandardized. Observing the output, we now need a way
    to rescale the coefficients to their variables' characteristics and we will be
    able to report the gradient descent solution in unstandardized form.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用的代码与上一章中使用的代码没有太大区别，除了它的输入现在由多个标准化变量组成。在这种情况下，算法在更少的迭代次数中达到收敛，并且使用了比之前更小的alpha值，因为在我们之前的例子中，我们的单个变量实际上是未标准化的。观察输出，我们现在需要一种方法来重新缩放系数以适应变量的特征，我们就能以非标准化的形式报告梯度下降的解。
- en: Another point to mention is our choice of alpha. After some tests, the value
    of `0.02` has been chosen for its good performance on this very specific problem.
    Alpha is the learning rate and during optimization it can be fixed or changeable,
    accordingly to a line search method, modifying its value in order to minimize
    as far as possible the cost function at each single step of the optimization process.
    In our example, we opted for a fixed learning rate and we had to look for its
    best value by trying a few optimization values and deciding on which minimized
    the cost in the minor number of iterations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要提及的是我们选择alpha的方法。经过一些测试，我们选择了`0.02`这个值，因为它在这个非常具体的问题上表现良好。Alpha是学习率，在优化过程中它可以固定或可变，根据线搜索方法，通过修改其值以尽可能最小化优化过程中的每个单独步骤的成本函数。在我们的例子中，我们选择了固定的学习率，并且我们必须通过尝试几个优化值来确定哪个在更少的迭代次数中最小化了成本。
- en: Unstandardizing coefficients
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非标准化系数
- en: 'Given a vector of standardized coefficients from a linear regression and its
    bias, we can recall the formulation of the linear regression, which is:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个线性回归的标准化系数向量及其偏差，我们可以回忆线性回归的公式，它是：
- en: '![Unstandardizing coefficients](img/00039.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![非标准化系数](img/00039.jpeg)'
- en: 'The previous formula, transforming the predictors using their mean and standard
    deviation, is actually equivalent (after a few calculations) to such an expression:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的公式，使用变量的均值和标准差转换预测变量，实际上（经过一些计算）等同于以下表达式：
- en: '![Unstandardizing coefficients](img/00040.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![非标准化系数](img/00040.jpeg)'
- en: In our formula, ![Unstandardizing coefficients](img/00041.jpeg) represents the
    original mean and *δ* the original variance of the variables.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的公式中，![非标准化系数](img/00041.jpeg)代表变量的原始均值，而*δ*代表原始方差。
- en: 'By comparing the different parts of the two formulas (the first parenthesis
    and the second summation), we can calculate the bias and coefficient equivalents
    when transforming a standardized coefficient into an unstandardized one. Without
    replicating all the mathematical formulas, we can quickly implement them into
    Python code and immediately provide an application showing how such calculations
    can transform gradient descent coefficients:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较两个公式（第一个括号和第二个求和项）的不同部分，我们可以计算出将标准化系数转换为非标准化系数时的偏差和系数等价值。不重复所有数学公式，我们可以快速将它们实现为Python代码，并立即提供一个应用示例，展示这些计算如何转换梯度下降系数：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Unstandardizing coefficients](img/00042.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![非标准化系数](img/00042.jpeg)'
- en: As an output from the previous code snippet, you will get a list of coefficients
    identical to our previous estimations with both Scikit-learn and Statsmodels.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作为前一个代码片段的输出，你将得到一个与我们的先前估计相同的系数列表，无论是使用Scikit-learn还是Statsmodels。
- en: Estimating feature importance
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计特征重要性
- en: 'After having confirmed the values of the coefficients of the linear model we
    have built, and after having explored the basic statistics necessary to understand
    if our model is working correctly, we can start auditing our work by first understanding
    how a prediction is made up. We can obtain this by accounting for each variable''s
    role in the constitution of the predicted values. A first check to be done on
    the coefficients is surely on the directionality they express, which is simply
    dictated by their sign. Based on our expertise on the subject (so it is advisable
    to be knowledgeable about the domain we are working on), we can check whether
    all the coefficients correspond to our expectations in terms of directionality.
    Some features may decrease the response as we expect, thereby correctly confirming
    that they have a coefficient with a negative sign, whereas others may increase
    it, so a positive coefficient should be correct. When coefficients do not correspond
    to our expectations, we have **reversals**. Reversals are not uncommon and they
    can actually reveal that things work in a different way than we expected. However,
    if there is much multicollinearity between our predictors, reversals could be
    just due to the higher uncertainty of estimates: some estimates may be so uncertain
    that the optimization processes have allocated them a wrong sign. Consequently,
    when a linear regression doesn''t confirm our expectations, it is better not to
    jump to any quick conclusion; instead, closely inspect all the statistical measures
    pointing toward multicollinearity.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在确认了我们构建的线性模型的系数值，并探索了理解模型是否正确工作所需的基本统计信息之后，我们可以开始审计我们的工作，首先了解预测是如何构成的。我们可以通过考虑每个变量在构成预测值中的作用来获得这一点。对系数进行的第一次检查无疑是检查它们所表达的方向性，这仅仅由它们的符号决定。基于我们对主题的专长（因此，建议我们对正在工作的领域有所了解），我们可以检查所有系数是否都符合我们对方向性的预期。一些特征可能以我们预期的方式降低响应，从而正确地确认它们具有负号系数，而其他特征可能增加响应，因此正系数应该是正确的。当系数不符合我们的预期时，我们就有**反转**。反转并不罕见，实际上它们可以揭示事物以与我们预期不同的方式运作。然而，如果我们的预测变量之间存在大量多重共线性，反转可能只是由于估计的不确定性较高：一些估计可能如此不确定，以至于优化过程分配给它们错误的符号。因此，当线性回归不符合我们的预期时，最好不要急于得出任何结论；相反，仔细检查所有指向多重共线性的统计指标。
- en: A second check is done on the impact of the variable on the model—that is, how
    much of the predicted result is dominated by variations in feature. Usually, if
    the impact is low, reversals and other difficulties caused by the variable (it
    could be from an unreliable source, for instance, or very noisy—that is, measured
    imprecisely) are less disruptive for the predictions or can even be ignored.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对变量对模型的影响进行第二次检查——也就是说，预测结果中有多少是由特征的变化所主导的。通常情况下，如果影响较小，由变量（例如，可能来自不可靠的来源，或者非常嘈杂——也就是说，测量不准确）引起的变化和反转以及其他困难对预测的破坏性较小，甚至可以忽略不计。
- en: Introducing the idea of impact also presents the possibility of making our model
    economic in terms of the number of modeled coefficients. Up to now, we have just
    concentrated on the idea that it is desirable to fit data in the best way possible
    and we didn't check if our predictive formula generalizes well to new data. Starting
    to rank the predictors could help us make new simpler models (by just selecting
    only the most important features in the model) and simpler models are less prone
    to errors when in the production phase.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 引入影响的概念也提出了使我们的模型在模型系数数量方面经济化的可能性。到目前为止，我们只是专注于这样一个观点，即尽可能好地拟合数据是可取的，我们没有检查我们的预测公式是否很好地推广到新数据。开始对预测变量进行排序可以帮助我们构建新的更简单的模型（只需选择模型中最重要的特征）并且简单的模型在生产阶段更不容易出错。
- en: In fact, if our objective is not simply to fit our present data maximally with
    a formula, but to also fit future data well, it is necessary to apply the principle
    of Occam's razor. This suggests that, given more correct answers, simpler models
    are always preferable to more complex ones. The core idea is not to make an explanation,
    that is a linear model, more complex than it should be, because complexity may
    hide overfitting.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果我们的目标不仅仅是用公式最大限度地拟合我们的现有数据，而且还要很好地拟合未来的数据，那么有必要应用奥卡姆剃刀的原则。这表明，在给出更多正确答案的情况下，简单的模型总是比更复杂的模型更可取。核心思想不是使解释（即线性模型）比应有的更复杂，因为复杂性可能隐藏过拟合。
- en: Inspecting standardized coefficients
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查标准化系数
- en: Extending our interpretation of the linear model from one single variable to
    a host of them, we can still read each single coefficient as the unit change inducted
    on the response variable by each predictor (keeping the other predictors constant).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们对线性模型的解释从单个变量扩展到大量变量，我们仍然可以将每个单个系数视为每个预测变量对响应变量单位变化的诱导（保持其他预测变量不变）。
- en: Intuitively, larger coefficients seem to impact more on the result of the linear
    combination; however, as we noticed while revisiting gradient descent, different
    variables may have different scales and their coefficients may incorporate this.
    Being smaller or larger in terms of coefficient may just be because of the variable's
    relative scale in comparison to the other features involved in the analysis.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上看，较大的系数似乎对线性组合的结果影响更大；然而，正如我们在回顾梯度下降时注意到的，不同的变量可能具有不同的尺度，它们的系数可能包含这一点。系数较小或较大可能只是因为变量相对于分析中涉及的其他特征的相对尺度。
- en: By standardizing variables, we place them under a similar scale where a unit
    is the standard deviation of the variable itself. Variables extending from high
    to low values (with a larger range) tend to have a greater standard deviation
    and you should expect them to be reduced. By doing so, most variables with a normal-like
    distribution should be reduced in the range *-3 < x < +3*, thus allowing a comparison
    on their contribution to the prediction. Highly skewed distributions won't be
    standardized in the range *-3 <x <+3*. The transformation will be beneficial anyway
    because their range is going to be largely reduced and after that it will even
    make sense to compare different distributions because then they will all represent
    the same unit measure—that is, the unit variance. After standardization, larger
    coefficients can be interpreted as major contributions to establishing the result
    (a weighted summation, so the result will resemble larger weights more closely).
    Using standardized coefficients, we can therefore confidently rank our variables
    and spot those contributing less.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标准化变量，我们将它们置于一个相似的尺度上，其中单位是变量的标准差。从高值到低值（范围较大）的变量往往具有更大的标准差，你应该预期它们会被缩小。通过这样做，大多数呈正态分布的变量应该会在范围*-3
    < x < +3*内缩小，从而允许比较它们对预测的贡献。高度偏斜的分布不会在范围*-3 <x <+3*内进行标准化。这种转换无论如何都是有益的，因为它们的范围将被大大缩小，之后甚至可以比较不同的分布，因为那时它们都将代表相同的单位度量——即单位方差。标准化后，较大的系数可以解释为对建立结果（加权求和，因此结果将更接近较大的权重）的主要贡献。因此，我们可以自信地对我们变量进行排序，并找出那些贡献较小的变量。
- en: 'Let''s proceed to an example using our Boston dataset. This time we will be
    using the `LinearRegression` method from Scikit-learn because we do not need to
    do a linear model for its statistical properties, but just a working model using
    a fast and scalable algorithm:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来继续，这次我们将使用波士顿数据集。这次我们将使用Scikit-learn的`LinearRegression`方法，因为我们不需要对其统计属性进行线性模型，而只需要一个使用快速且可扩展算法的工作模型：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In such an initialization, apart from the `fit_intercept` parameter that explicates
    the insertion of a bias into the design of model, the `normalize` option indicates
    whether we intend to rescale all the variables in the range between zero and one.
    Such a transformation is different from statistical standardization and we will
    omit it for the moment by setting it in a `False` state:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的初始化中，除了`fit_intercept`参数明确指出将偏差插入模型设计之外，`normalize`选项表示我们是否打算将所有变量重新缩放到零和一之间。这种转换与统计标准化不同，我们暂时将其设置为`False`状态以省略它：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Besides the `StandardScaler` seen before, we also import from Scikit-learn
    the convenient `make_pipeline` wrapper, which allows us to establish a sequence
    of operations to be done automatically on our data before feeding it to the linear
    regression analysis. Now, the `Stand_coef_linear_reg` pipeline will execute a
    statistical standardization on data before regressing it, thus outputting standardized
    coefficients:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前看到的`StandardScaler`之外，我们还从Scikit-learn导入方便的`make_pipeline`包装器，它允许我们在将数据馈送到线性回归分析之前自动执行一系列操作。现在，`Stand_coef_linear_reg`管道将在回归之前对数据进行统计标准化，从而输出标准化系数：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Inspecting standardized coefficients](img/00043.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![检查标准化系数](img/00043.jpeg)'
- en: 'As a first step, we output the coefficients of the regression on unstandardized
    data. As seen before, the output seems dominated by the huge coefficient of the
    NOX variable, which overlooks (with its absolute value of about 17.8) minor coefficients
    of 3.8 and less. However, we may question whether it could be so after also standardizing
    the variables:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们输出未标准化数据上的回归系数。如前所述，输出似乎主要由NOX变量的巨大系数主导，其绝对值约为17.8，而忽略了3.8和更小的较小系数。然而，我们可能会质疑在标准化变量后是否仍然如此：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Inspecting standardized coefficients](img/00044.jpeg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![检查标准化系数](img/00044.jpeg)'
- en: Having all the predictors on a similar scale now, we can easily provide a more
    realistic interpretation of each coefficient. Clearly, it appears that a unit
    change has more impact when it involves the variables `LSTAT`, `DIS`, `RM`, `RAD`,
    and `TAX`. `LSTAT` is the percentage of lower status population, and this aspect
    explains its relevancy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有预测变量都在相似尺度上，我们可以轻松地提供每个系数的更现实解释。显然，涉及变量`LSTAT`、`DIS`、`RM`、`RAD`和`TAX`的单位变化似乎有更大的影响。`LSTAT`是低地位人口的比例，这一方面解释了其相关性。
- en: 'Using standardized scales has certainly pointed us at the most important variables
    but it is still not a complete overview of the predictive power of each variable
    because:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准化尺度无疑指出了最重要的变量，但它仍然不是每个变量预测能力的完整概述，因为：
- en: 'Standardized coefficients represent how well the model works out its predictions
    because large coefficients heavily impact the resulting response: though having
    a large coefficient is certainly a hint of how important a variable is, it tells
    us just a part of the role that the variable plays in reducing the error of the
    estimates and in making our predictions more accurate'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化系数代表模型预测效果的好坏，因为大的系数对结果响应有重大影响：尽管拥有大的系数确实是一个变量重要性的提示，但它只告诉我们变量在减少估计误差和使我们的预测更准确中所起的部分作用。
- en: Standardized coefficients can be ranked but their unit, though similar in scale,
    is somehow abstract (the standard deviation of each variable) and relative to
    the data at hand (so we shouldn't compare the standardized coefficients of different
    datasets because their standard deviations could be different)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化系数可以排序，但它们的单位，尽管在尺度上相似，但某种程度上是抽象的（每个变量的标准差）并且相对于手头的数据（因此我们不应该比较不同数据集的标准化系数，因为它们的标准差可能不同）
- en: One solution could be to integrate the importance estimate based on standardized
    coefficients, instead using some measure related to the error measure.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案可能是将基于标准化系数的重要性估计整合，而不是使用与误差度量相关的某些度量。
- en: Comparing models by R-squared
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过R-squared比较模型
- en: From a general point of view, we can evaluate a model by comparing how better
    it does in respect of a simple mean, and that's the coefficient of determination,
    R-squared.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从一般的角度来看，我们可以通过比较模型在简单均值方面的表现来评估一个模型，这就是确定系数，R-squared。
- en: R-squared can estimate how good a model is; therefore, by comparing the R-squared
    of our model against alternative models where the variables have been removed,
    we can get an idea of how predictive each removed variable is. All we have to
    do is compute the difference between the coefficients of determination of the
    initial model against the model without that variable. If the difference is large,
    the variable is very important in the determination of a better R-squared and
    of a better model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: R-squared可以估计模型的好坏；因此，通过比较我们的模型与变量已被移除的替代模型的R-squared，我们可以了解每个移除变量的预测能力。我们只需计算初始模型与没有该变量的模型的确定系数之间的差异。如果差异很大，该变量在确定更好的R-squared和更好的模型中起着非常重要的作用。
- en: 'In our case, we have to first record what the R-squared is when we build the
    model with all the variables present. We can name such a value our baseline of
    comparison:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们首先需要记录当我们用所有变量构建模型时的R-squared值。我们可以将这样的值命名为我们的比较基准：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After that, all we have to do is to remove one variable at a time from the
    set of predictors, to estimate again the regression model recording its coefficient
    of determination, and subtract it from the baseline value we got from the complete
    regression model:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们只需从预测变量集中一次移除一个变量，重新估计回归模型并记录其确定系数，然后从完整回归模型得到的基准值中减去：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Comparing models by R-squared](img/00045.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![通过R-squared比较模型](img/00045.jpeg)'
- en: After we get all the differences, each one representing the contribution of
    each variable to the R-squared, we just have to rank them and we will then have
    an idea of which variables contribute more in reducing the error of the linear
    model; this is a different point of view from knowing which variable contributed
    the most to the response value. Such a contribution is called the partial R-squared.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们得到所有差异之后，每个差异代表每个变量对R平方的贡献，我们只需对它们进行排序，然后我们就会对哪些变量在减少线性模型误差方面贡献更多有一个概念；这与知道哪个变量对响应值贡献最大是不同的观点。这种贡献被称为部分R平方。
- en: Apart from suggesting we use both measures (standardized coefficients and partial
    Rsquared) to separate relevant variables from irrelevant ones, by using the partial
    Rsquared you can actually directly compare the importance of the variables because
    using ratios does make sense here (so you can tell that a variable is twice as
    important as another because it reduces the error twice as much).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 除了建议我们使用这两种度量（标准化系数和部分R平方）来区分相关变量和不相关变量之外，通过使用部分R平方，你实际上可以直接比较变量的重要性，因为使用比率在这里是有意义的（因此，你可以知道一个变量的重要性是另一个变量的两倍，因为它减少了两倍多的误差）。
- en: Another noticeable point is that partial R-squareds are not really a decomposition
    of the initial R-squared measure. In fact, only if the predictors are uncorrelated
    by summing all the partial scores will you get the precise coefficient of determination
    of the full model. This is due to collinearity between variables. Therefore, when
    you remove a variable from the model, you certainly do not remove all its informative
    variance since correlated variables, containing similar information to the removed
    variable, are kept in the model. It may happen that, if you have two highly correlated
    variables, removing each in turn won't change the R-squared by much because, as
    one is removed, the other one will provide the missing information (thus, the
    double-check with the standardized coefficient is not redundant at all).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得注意的点是，部分R平方并不是初始R平方测量的真正分解。实际上，只有当预测变量通过求和所有部分得分相互不相关时，你才能得到完整模型的精确确定系数。这是由于变量之间的共线性造成的。因此，当你从模型中移除一个变量时，你肯定不会移除其所有信息方差，因为包含与移除变量相似信息的相关变量仍然保留在模型中。可能发生的情况是，如果你有两个高度相关的变量，依次移除它们并不会显著改变R平方，因为当一个变量被移除时，另一个变量将提供缺失的信息（因此，与标准化系数的双检查并不是多余的）。
- en: 'There are more sophisticated methods to estimate the variable importance, but
    these two methods should provide you with enough insight. Knowing what variables
    impact your results more strongly can provide you with the means to:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有更多复杂的方法可以用来估计变量重要性，但这两种方法应该能为你提供足够的洞察力。了解哪些变量对结果影响更大，可以为你提供以下手段：
- en: Try to explain the results to management in a reasonable and understandable
    way.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽量以合理且易于理解的方式向管理层解释结果。
- en: Prioritize your work, in terms of data cleaning, preparation, and transformation,
    by concentrating first on the features most relevant to the success of your project.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先根据项目成功的关键性，优先处理数据清洗、准备和转换工作，集中精力关注与项目成功最相关的特征。
- en: Conserve resources, in particular memory, as less data is used when building
    and using the regression model.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建和使用回归模型时，尽量节约资源，特别是内存，因为使用更少的数据可以减少资源消耗。
- en: If you would like to use importance to exclude irrelevant variables using one
    of the measures we presented, the safest way would be to recalculate the ranking
    every time you decide to exclude a variable from the set. Otherwise, using a single
    ranking may risk hiding the true importance of highly correlated variables (and
    that's true for both the methodologies, though standardized coefficients are a
    bit more revealing).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望使用重要性来排除不相关变量，并使用我们提出的一种度量，最安全的方法是在每次决定从集合中排除一个变量时重新计算排名。否则，使用单一的排名可能会隐藏高度相关变量的真正重要性（这对于两种方法都是真的，尽管标准化系数稍微更有揭示性）。
- en: Such an approach is certainly time-consuming, but it is necessary when you notice
    that your model, though presenting a good fit on your present data, cannot generalize
    well to new observations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法当然很耗时，但当你注意到你的模型虽然在你当前的数据上呈现良好的拟合，但不能很好地推广到新的观测值时，这是必要的。
- en: We are going to discuss such circumstances in more depth in [Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 6. Achieving Generalization"), *Achieving Generalization*, when we will
    illustrate the best ways to reduce your predictor set, maintaining (simplifying
    your solution) and even improving your predictive performances (generalizing more
    effectively).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第6章](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6 "第6章。实现泛化")“实现泛化”中更深入地讨论这种情形，届时我们将说明减少预测变量集的最佳方法，保持（简化解决方案）甚至提高预测性能（更有效地泛化）。
- en: Interaction models
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交互模型
- en: Having explained how to build a regression model with multiple variables and
    having touched on the theme of its utilization and interpretation, we start from
    this paragraph to explore how to improve it. As a first step, we will work on
    its fit with present data. In the following chapters, devoted to model selection
    and validation, we will concentrate on how to make it really generalizable—that
    is, capable of correctly predicting on new, previously unseen data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 解释了如何构建多变量回归模型，并触及了其利用和解释的主题后，我们从这一段开始探讨如何改进它。作为第一步，我们将从当前数据对它的拟合工作开始。在接下来的章节中，我们将专注于模型选择和验证，我们将集中讨论如何使其真正具有普遍性——也就是说，能够正确预测新的、之前未见过的数据。
- en: As we previously reasoned, the beta coefficients in a linear regression represent
    the link between a unit change in the predictors and the response variations.
    The assumptions at the core of such a model are of a constant and unidirectional
    relationship between each predictor and the target. It is the linear relationship
    assumption, having the characteristics of a line where direction and fluctuation
    are determined by the angular coefficient (hence the name linear regression, hinting
    at the operation of regressing, tracing back to the linear form from some data
    evidence). Although a good approximation, a linear relationship is a simplification
    often not true in real data. In fact, most relationships are non-linear, showing
    bends and curves and alternating different fluctuations in increase and decrease.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前推理的那样，线性回归中的beta系数代表了预测变量单位变化与响应变量变化之间的联系。此类模型的核心假设是每个预测变量与目标变量之间存在着恒定且单向的关系。这是线性关系假设，具有线的特征，其中方向和波动由角度系数（因此得名线性回归，暗示着回归操作，追溯到从某些数据证据中得出的线性形式）。尽管这是一个很好的近似，但线性关系通常是简化的，在真实数据中往往并不成立。事实上，大多数关系都是非线性的，显示出弯曲、曲线以及增加和减少中的不同波动。
- en: The good news is that we do not have to limit ourselves to the originally provided
    features, but we can modify them in order to *straighten* their relationship with
    the target variable. In fact, the more similarity between the predictor and the
    response, the better the fit and the fewer prediction errors in the training set.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我们不必局限于最初提供的特征，但我们可以修改它们，以便*拉直*它们与目标变量之间的关系。事实上，预测变量与响应变量之间的相似性越大，拟合度越好，训练集中的预测误差就越少。
- en: 'Consequently, we can say that:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以这样说：
- en: We can improve our linear regression by transforming predictor variables in
    various ways
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过各种方式变换预测变量来改进我们的线性回归。
- en: We can measure such improvement using the partial R-squared, since every transformation
    should impact on the quantity of residual errors and ultimately on the coefficient
    of determination
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用部分R平方来衡量这种改进，因为每一次变换都应该影响残差误差的数量，并最终影响确定系数。
- en: Discovering interactions
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现交互作用
- en: One of the first sources of non-linearity is due to possible interactions between
    predictors. Two predictors interact when the effect of one of them on the response
    variable varies in respect of the values of the other predictors.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性的第一个来源是由于预测变量之间可能存在的交互作用。当其中一个预测变量对响应变量的影响随着另一个预测变量的值而变化时，这两个预测变量就发生了交互。
- en: 'In mathematical formulation, interaction terms (the interacting variables)
    have to be multiplied by themselves for our linear model to catch the supplementary
    information of their relation as expressed in this example of a model with two
    interacting predictors:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学公式中，交互项（交互变量）必须乘以自身，以便我们的线性模型能够捕捉到它们关系的补充信息，正如以下具有两个交互预测变量的模型示例所示：
- en: '![Discovering interactions](img/00046.jpeg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![发现交互作用](img/00046.jpeg)'
- en: An easy-to-remember example of an interaction in a regression model is the role
    of engine noise in the evaluation of a car. If you are going to model the preference
    for car models, you will immediately notice that the engine noise can either decrease
    or increase consumer preference for the car, depending on the price of the car
    (or its category, which is a proxy of monetary value). In fact, if the car is
    inexpensive being silent is clearly a must-have, but if the car is expensive (such
    as a Ferrari or another sports car) the noise is an outstanding benefit (clearly
    when in it, you want to have everyone notice the cool car you are driving).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归模型中，一个易于记忆的交互示例是发动机噪音在评估汽车中的作用。如果你要建模汽车型号的偏好，你会立即注意到发动机噪音可能会增加或减少消费者对汽车的偏好，这取决于汽车的价格（或其类别，这是货币价值的代理）。事实上，如果汽车价格便宜，安静显然是必不可少的，但如果汽车价格昂贵（如法拉利或其他跑车）噪音则是一个突出的好处（显然当你坐在车里时，你希望每个人都注意到你开的酷车）。
- en: It may sound a little bit tricky to deal with interaction, but actually it isn't;
    after all, you are just transforming a variable role in a linear regression based
    on another one. Finding interaction terms can be achieved in two different ways,
    the first one being domain knowledge—that is, knowing directly the problem you
    are modeling and incorporating your expertise in it. When you do not have such
    an expertise, an automatic search over the possible combinations will suffice
    if it is well tested using a revealing measure such as R-squared.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 处理交互可能听起来有点棘手，但实际上并不难；毕竟，你只是在基于另一个变量转换线性回归中的变量角色。找到交互项可以通过两种不同的方式实现，第一种是领域知识——也就是说，直接了解你正在建模的问题，并在其中融入你的专业知识。当你没有这样的专业知识时，如果使用如R-squared这样的揭示性度量进行良好测试，自动搜索所有可能的组合就足够了。
- en: 'The best way to illustrate the automatic search approach is to show an example
    in Python using the `PolynomialFeatures` from Scikit-learn, a function that allows
    both interactions and polynomial expansions (we are going to talk about them in
    the next paragraph):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的方法是通过一个Python示例来展示自动搜索方法，使用Scikit-learn中的`PolynomialFeatures`函数，这个函数允许交互和多项式展开（我们将在下一段中讨论它们）：
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: By the `degree` parameter we define how many variables to put into the interaction,
    it being possible to have three or even more variables interact with each other.
    Interactions in statistics are called two-way effects, three-way effects, and
    so on, depending on the number of variables involved (whereas the original variables
    are instead called the main effects).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`degree`参数，我们定义了要放入交互中的变量数量，可能有三变量甚至更多变量相互交互。在统计学中，交互被称为双向效应、三向效应等，这取决于涉及的变量数量（而原始变量则被称为主要效应）。
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After recalling the baseline R-squared value, the code creates a new input
    data matrix using the `fit_transform` method, enriching the original data with
    the interaction effects of all the variables. At this point, we create a series
    of new linear regression models, each one containing all the main effects plus
    a single interaction. We measure the improvement, calculate the difference with
    the baseline, and then report only interactions over a certain threshold. We can
    decide on a threshold just above zero or a threshold we determine based on a statistical
    test. In our example we decided on an arbitrary threshold, aiming at reporting
    all R-squared increment above `0.01`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在回忆起基线R-squared值之后，代码使用`fit_transform`方法创建一个新的输入数据矩阵，通过所有变量的交互效应丰富了原始数据。此时，我们创建了一系列新的线性回归模型，每个模型包含所有主要效应和一个单独的交互效应。我们测量改进，计算与基线的差异，然后仅报告超过一定阈值的交互效应。我们可以决定一个略高于零的阈值，或者基于统计测试确定的阈值。在我们的例子中，我们决定了一个任意阈值，目的是报告所有超过`0.01`的R-squared增量：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Discovering interactions](img/00047.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![发现交互作用](img/00047.jpeg)'
- en: 'Relevant interaction effects are clearly made up by the variable `''RM''` (one
    of the most important ones, as seen before) and the strongest improvement is given
    by its interaction with another key feature, `LSTAT`. An important take away would
    be that we add it to our original data matrix, as a simple multiplication between
    the two:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的交互效应显然是由变量`'RM'`（如之前所见，是最重要的变量之一）及其与另一个关键特征`LSTAT`的交互产生的。一个重要的启示是，我们将它添加到我们的原始数据矩阵中，作为两个变量之间的简单乘积：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Polynomial regression
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式回归
- en: As an extension of interactions, polynomial expansion systematically provides
    an automatic means of creating both interactions and non-linear power transformations
    of the original variables. Power transformations are the bends that the line can
    take in fitting the response. The higher the degree of power, the more bends are
    available to fit the curve.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 作为交互作用的扩展，多项式展开系统地提供了一种自动创建交互作用和原始变量的非线性幂变换的方法。幂变换是线在拟合响应时可以采取的弯曲。幂的次数越高，可用的弯曲就越多，从而更好地拟合曲线。
- en: 'For instance, if you have a simple linear regression of the form:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有一个简单线性回归的形式：
- en: '![Polynomial regression](img/00048.jpeg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![多项式回归](img/00048.jpeg)'
- en: 'By a second degree transformation, called **quadratic**, you will get a new
    form:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通过二次变换，称为**二次**，你将得到一个新的形式：
- en: '![Polynomial regression](img/00049.jpeg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![多项式回归](img/00049.jpeg)'
- en: 'By a third degree transformation, called **cubic**, your equation will turn
    into:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通过三次变换，称为**三次**，你的方程将变为：
- en: '![Polynomial regression](img/00050.jpeg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![多项式回归](img/00050.jpeg)'
- en: 'If your regression is a multiple one, the expansion will create additional
    terms (interactions) increasing the number of new features derived from the expansion.
    For instance, a multiple regression made up of two predictors (*x[1]* and *x[2]*),
    expanded using the quadratic transformation, will become:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的回归是多元的，展开将创建额外的项（交互作用），从而增加由展开产生的新特征的数量。例如，由两个预测因子（*x[1]* 和 *x[2]*）组成的多元回归，使用二次变换展开，将变为：
- en: '![Polynomial regression](img/00051.jpeg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![多项式回归](img/00051.jpeg)'
- en: 'Before proceeding, we have to note two aspects of the expansion procedure:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们必须注意展开过程的两点：
- en: Polynomial expansion rapidly increases the number of predictors
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式展开会迅速增加预测因子的数量
- en: Higher-degree polynomials translate into high powers of the predictors, posing
    problems for numeric stability, thus requiring suitable numeric formats or standardizing
    numeric values that are too large
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高次多项式转化为预测因子的高次幂，这会对数值稳定性造成问题，因此需要合适的数值格式或标准化过大的数值
- en: Testing linear versus cubic transformation
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试线性与三次变换
- en: 'By setting the `interaction_only` parameter off in the `PolynomialFeatures`
    function seen before, we can get the full polynomial transformation of our input
    matrix, not just simply the interactions as before:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在之前提到的 `PolynomialFeatures` 函数中将 `interaction_only` 参数关闭，我们可以得到输入矩阵的完整多项式变换，而不仅仅是之前简单的交互作用：
- en: '[PRE27]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'By sending both `PolynomialFeatures` and `LinearRegression` into a pipeline
    we can create a function automatically by a single command, expanding out data
    and regressing it. As an experiment, we try to model the `''LSTAT''` variable
    alone for best clarity, remembering that we could have expanded all the variables
    at once:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `PolynomialFeatures` 和 `LinearRegression` 同时放入管道中，我们可以通过单个命令自动创建一个函数，展开数据并对其进行回归。作为一个实验，我们尝试单独对
    `'LSTAT'` 变量进行建模，以获得最佳清晰度，记住我们本来可以一次性展开所有变量：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Testing linear versus cubic transformation](img/00052.jpeg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![测试线性与三次变换](img/00052.jpeg)'
- en: 'Our first fit is the linear one (a simple linear regression) and from the scatterplot
    we can notice that the line is not representing well the cloud of points relating
    to `''LSTAT''` with the response; most likely we need a curve. Instead of testing
    a second degree transformation that will turn into a parabola, we immediately
    try a cubic transformation: using two bends, we should obtain a better fit:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一次拟合是线性的（简单的线性回归），从散点图中我们可以注意到，这条线并没有很好地代表与 `'LSTAT'` 相关的点云；很可能是我们需要一个曲线。我们立即尝试三次变换：使用两个弯曲，我们应该能得到更好的拟合：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![Testing linear versus cubic transformation](img/00053.jpeg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![测试线性与三次变换](img/00053.jpeg)'
- en: Our graphic check confirms that now we have a more credible representation of
    how `'LSTAT'` and the response relate. We question whether we cannot do better
    using even higher-degree transformations.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图形检查确认，现在我们对 `'LSTAT'` 和响应之间的关系有了更可信的表示。我们质疑是否不能通过使用更高次的变换做得更好。
- en: Going for higher-degree solutions
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻求更高次解
- en: 'To test a higher degree of polynomial transformations, we prepare a script
    that creates the expansion and reports its R-squared measure. We then try to plot
    the function with the highest degree in the series and have a look at how it fits
    the data points:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试更高次的多项式变换，我们准备了一个脚本，该脚本创建展开并报告其 R 平方度量。然后我们尝试绘制系列中最高次的函数，并查看它如何拟合数据点：
- en: '[PRE30]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Going for higher-degree solutions](img/00054.jpeg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![寻求更高次解法](img/00054.jpeg)'
- en: 'Noticeably, there is a huge difference in the coefficient of determination
    between the linear model and the quadratic expansion (second-degree polynomial).
    The measure jumps from `0.544` to `0.641`, a difference that increases up to `0.682`
    when reaching the fifth degree. Preceding to even higher degrees, the increment
    is not so astonishing, though it keeps on growing, reaching `0.695` when the degree
    is the fifteenth. As the latter is the best result in terms of coefficient of
    determination, having a look at the plot on the data cloud will reveal a not so
    smooth fit, as we can see with lower degrees of polynomial expansion:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，线性模型和二次展开（二次多项式）之间的决定系数有很大的差异。这个指标从`0.544`跳到`0.641`，差异增加到达到五次方时为`0.682`。在更高的次数之前，增量并不那么令人惊讶，尽管它还在增长，当次数达到第十五时，达到`0.695`。作为后者在决定系数方面是最好的结果，查看数据云的图将揭示一个不太平滑的拟合，正如我们可以从较低次数的多项式展开中看到的那样：
- en: '[PRE31]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Going for higher-degree solutions](img/00055.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![寻求更高次解法](img/00055.jpeg)'
- en: Observing closely the resulting curve, you will surely notice how, by such a
    high degree, the curve tends strictly to follow the distribution of points, going
    erratic when the density diminishes at the fringes of the range of the predictors'
    values.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察得到的曲线，你一定会注意到，通过如此高的次数，曲线严格遵循点的分布，当预测值的范围边缘密度降低时，曲线变得不规则。
- en: Introducing underfitting and overfitting
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍欠拟合和过拟合
- en: 'Polynomial regression offers us the right occasion for starting to talk about
    model complexity. We have not explicitly tested it, but you may already have got
    the feeling that, by increasing the degree of the polynomial expansion, you are
    going to always reap better fits. We say more: the more variables you have in
    your model, the better, until you will have such a large number of betas, likely
    equal or almost equal to the number of your observations, that your predictions
    will be perfect.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归为我们提供了一个开始讨论模型复杂性的合适机会。我们没有明确测试它，但你可能已经感觉到，通过增加多项式展开的次数，你将始终获得更好的拟合。我们更进一步说：你模型中的变量越多，越好，直到你将有如此多的beta系数，可能等于或几乎等于你的观察数量，以至于你的预测将是完美的。
- en: Decaying performances due to over-parameterization (an excess of parameters
    to be learned by the model) is a problem of linear regression and of many other
    machine learning algorithms. The more parameters you add, the better the fit because
    the model will cease to intercept the rules and regularities of your data but
    will start, in such an embarrassment of riches, to populate the many available
    coefficients with all the erratic and erroneous information present in the data.
    In such a situation, the model won't learn general rules but it will just be memorizing
    the dataset itself in another form.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过参数化（模型需要学习的参数过多）导致的性能下降是线性回归以及许多其他机器学习算法的问题。你添加的参数越多，拟合效果越好，因为模型将不再拦截你数据中的规则和规律，而是在这种丰富的尴尬中，开始用数据中所有不规则和错误的信息填充可用的许多系数。在这种情况下，模型不会学习一般规则，而只是以另一种形式记住数据集本身。
- en: 'This is called **overfitting**: fitting the data at hand so well that the result
    is far from being an extraction of the form of the data to draw predictions from;
    the result is just a mere memorization. On the other side, another problem is
    **underfitting**—that is, when you are using too few parameters for your prediction.
    The most straightforward example is fitting a non-linear relation using a simple
    linear regression; clearly, it won''t match the curve bends and some of its predictions
    will be misleading.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**过拟合**：对现有数据进行如此完美的拟合，以至于结果远非从数据形式中提取以进行预测；结果只是简单的记忆。另一方面，另一个问题是**欠拟合**——即当你使用太少参数进行预测时。最直接的例子是使用简单的线性回归拟合非线性关系；显然，它不会匹配曲线的弯曲，并且其中一些预测将是误导性的。
- en: There are appropriate tools for verifying if you are underfitting or, more likely,
    overfitting and they will be discussed in [Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 6. Achieving Generalization"), *Achieving Generalization*, (the chapter
    explaining data science); in the meantime, don't overfit too much with high-degree
    polynomials!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 有适当的工具可以验证你是否欠拟合，或者更有可能的是过拟合，它们将在[第6章](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "第6章。实现泛化") *实现泛化*（解释数据科学的章节）中进行讨论；同时，不要过度使用高次多项式进行拟合！
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have carried on introducing linear regression, extending
    our example from a simple to a multiple one. We have revisited the previous outputs
    from the Statsmodels linear functions (the classical statistical approach) and
    gradient descent (the data science engine).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续介绍线性回归，将我们的例子从简单扩展到多元。我们回顾了Statsmodels线性函数（经典统计方法）和梯度下降（数据科学引擎）的先前输出。
- en: We started experimenting with models by removing selected predictors and evaluating
    the impact of such a move from the point of view of the R-squared measure. Meanwhile
    we also discovered reciprocal correlations between predictors and how to render
    more linear relations between each predictor and the target variable by intercepting
    the interactions and by means of polynomial expansion of the features.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过移除选定的预测因子并从R平方测量的角度评估这种移动的影响，开始对模型进行实验。同时，我们还发现了预测因子之间的相互关系，以及如何通过截断交互作用和通过特征的多项式扩展来使每个预测因子与目标变量之间形成更线性的关系。
- en: In the next chapter, we will progress again and extend the regression model
    to make it viable for classification tasks, turning it into a probabilistic predictor.
    The conceptual jump into the world of probability will allow us to complete the
    range of possible problems where linear models can be successfully applied.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将再次前进，将回归模型扩展到分类任务，使其成为一个概率预测器。进入概率世界的概念跳跃将使我们能够完成线性模型可以成功应用的潜在问题范围。
