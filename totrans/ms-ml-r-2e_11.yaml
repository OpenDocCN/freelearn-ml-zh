- en: Creating Ensembles and Multiclass Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"This is how you win ML competitions: you take other people''s work and ensemble
    them together."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Vitaly Kuznetsov, NIPS2014'
  prefs: []
  type: TYPE_NORMAL
- en: You may have already realized that we have discussed ensemble learning. It is
    defined by [www.scholarpedia.org](http://www.scholarpedia.org/article/Main_Page)
    as "the process by which multiple models, such as classifiers or experts, are
    strategically generated and combined to solve a particular computational intelligence problem".
    In random forest and gradient boosting, we combined the "votes" of hundreds or
    thousands of trees to make a prediction. Thus, by definition, those models are
    ensembles. This methodology can be extended to any learner to create ensembles,
    which some refer to as meta-ensembles or meta-learners. We will look at one of
    these methods referred to as "stacking". In this methodology, we will produce
    a number of classifiers and use their predicted class probabilities as input features
    to another classifier. This method *can* result in improved predictive accuracy.
    In the previous chapters, we focused on classification problems focused on binary
    outcomes. We will now look at methods to predict those situations where the data
    consists of more than two outcomes, a very common situation in real-world data
    sets. I have to confess that the application of these methods in R is some of
    the most interesting and enjoyable applications I have come across.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The quote at the beginning of this chapter mentions using ensembles to win
    machine learning competitions. However, they do have practical applications. I''ve
    provided a definition of what ensemble modeling is, but why does it work? To demonstrate
    this, I''ve co-opted an example, from the following blog, which goes into depth
    at a number of ensemble methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://mlwave.com/kaggle-ensembling-guide/](http://mlwave.com/kaggle-ensembling-guide/).'
  prefs: []
  type: TYPE_NORMAL
- en: As I write this chapter, we are only a couple of days away from Super Bowl 51,
    the Atlanta Falcons versus the New England Patriots. Let's say we want to review
    our probability of winning a friendly wager where we want to take the Patriots
    minus the points (3 points as of this writing). Assume that we have been following
    three expert prognosticators that all have the same probability of predicting
    that the Patriots will cover the spread (60%). Now, if we favor any one of the
    so-called experts, it is clear we have a 60% chance to win. However, let's see
    what creating an ensemble of their predictions can do to increase our chances
    of profiting and humiliating friends and family.
  prefs: []
  type: TYPE_NORMAL
- en: Start by calculating the probability of each possible outcome for the experts
    picking New England. If all three pick New England, we have 0.6 x 0.6 x 0.6, or
    a 21.6% chance, that all three are correct. If any two of the three pick New England
    then we have (0.6 x 0.6 x 0.3) x 3 for a total of 43.2%. By using majority voting,
    if at least two of the three pick New England, then our probability of winning
    becomes almost 65%.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a rather simplistic example but representative nonetheless. In machine
    learning, it can manifest itself by incorporating the predictions from several
    average or even weak learners to improve overall accuracy. The diagram that follows
    shows how this can be accomplished:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_ens_01.png)'
  prefs: []
  type: TYPE_IMG
- en: In this graphic, we build three different classifiers and use their predicted
    probabilities as inputs to a fourth and different classifier in order to make
    predictions on the test data. Let's see how to apply this with R.
  prefs: []
  type: TYPE_NORMAL
- en: Business and data understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are are going to visit our old nemesis the Pima Diabetes data once again.
    It has proved to be quite a challenge with most classifiers producing accuracy
    rates in the mid-70s. We''ve looked at this data in [Chapter 5](a7511867-5362-4215-a7dd-bbdc162740d1.xhtml), *More
    Classification Techniques - K-Nearest Neighbors and Support Vector Machines *and
    [Chapter 6](4dd3d3b8-f4cc-4ddd-b062-34d9684311cd.xhtml), *Classification and Regression
    Trees* so we can skip over the details. There are a number of R packages to build
    ensembles, and it is not that difficult to build your own code. In this iteration,
    we are going to attack the problem with the `caret` and `caretEnsemble` packages.
     Let''s get the packages loaded and the data prepared, including creating the
    train and test sets using the `createDataPartition()` function from caret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Modeling evaluation and selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we''ve done in prior chapters, the first recommended task when utilizing
    caret functions is to build the object that specifies how model training is going
    to happen. This is done with the `trainControl()` function. We are going to create
    a five-fold cross-validation and save the final predictions (the probabilities).
    It is recommended that you also index the resamples so that each base model trains
    on the same folds. Also, notice in the function that I specified upsampling. Why?
    Well, notice that the ratio of "Yes" versus "No" is 2 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This ratio is not necessarily imbalanced, but I want to demonstrate something
    here. In many data sets, the outcome of interest is a rare event. As such, you
    can end up with a classifier that is highly accurate but does a horrible job at
    predicting the outcome of interest, which is to say it doesn''t predict any true
    positives. To balance the response, you can upsample the minority class, downsample
    the majority class, or create "synthetic data". We will focus on synthetic data
    in the next exercise, but here, let''s try upsampling. In upsampling, for each
    cross-validation fold, the minority class is randomly sampled with replacement
    to match the number of observations in the majority class. Here is our function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train our models using the `caretList()` function. You can use any
    model in the function that is supported by the caret package. A list of models
    is available with their corresponding hyperparameters here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://rdrr.io/cran/caret/man/models.html](https://rdrr.io/cran/caret/man/models.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will train three models:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification tree - `"rpart"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multivariate Adaptive Regression Splines - `"earth"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Nearest Neighbors - `"knn"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s put this all together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Not only are the models built, but the parameters for each model are tuned
    according to caret''s rules. You can create your own tune grids for each model
    by incorporating the `caretModelSpec()` function, but for demonstration purposes,
    we will let the function do that for us. You can examine the results by calling
    the model object. This is the abbreviated output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A trick to effective ensembles is that the base models are not highly correlated.
    This is a subjective statement, and there is no hard rule of correlated predictions.
    One should experiment with the results and substitute a model if deemed necessary.
    Let''s look at our results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The classification tree and earth models are highly correlated. This may be
    an issue, but let''s progress by creating our the fourth classifier, the stacking
    model, and examining the results. To do this, we will capture the predicted probabilities
    for "Yes" on the test set in a dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We now stack these models for a final prediction with `caretStack()`. This
    will be a simple logistic regression based on five bootstrapped samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can examine the final model as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though `rpart` and earth were highly correlated, their coefficients are
    both significant and we can probably keep both in the analysis. We can now compare
    the individual model results with the `ensembled` learner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: What we see with the `colAUC()` function is the individual model AUCs and the
    AUC of the stacked/ensemble. The ensemble has led to a slight improvement over
    only using MARS from the earth package. So in this example, we see how creating
    an ensemble via model stacking can indeed increase predictive power. Can you build
    a better ensemble given this data? What other sampling or classifiers would you
    try? With that, let's move on to multi-class problems.
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of approaches to learning in multiclass problems. Techniques
    such as random forest and discriminant analysis will deal with multiclass while
    some techniques and/or packages will not, for example, generalized linear models,
    `glm()`, in base R. As of this writing, the `caretEnsemble` package, unfortunately,
    will not work with multiclasses. However, the Machine Learning in R (`mlr`) package
    does support multiple classes and also ensemble methods. If you are familiar with
    sci-kit Learn for Python, one could say that `mlr` endeavors to provide the same
    functionality for R. The mlr and the caret-based packages are quickly turning
    into my favorites for almost any business problem. I intend to demonstrate how
    powerful the package is on a multiclass problem, then conclude by showing how
    to do an ensemble on the `Pima` data.
  prefs: []
  type: TYPE_NORMAL
- en: For the multiclass problem, we will look at how to tune a random forest and
    then examine how to take a GLM and turn it into a multiclass learner using the
    "one versus rest" technique. This is where we build a binary probability prediction
    for each class versus all the others, then ensemble them together to predict an
    observation's final class. The technique allows you to extend any classifier method
    to multiclass problems, and it can often outperform multiclass learners.
  prefs: []
  type: TYPE_NORMAL
- en: 'One quick note: don''t confuse the terminology of multiclass and multilabel.
    In the former, an observation can be assigned to one and only one class, while
    in the latter, it can be assigned to multiple classes. An example of this is text
    that could be labeled both politics and humor. We will not cover multilabel problems
    in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Business and data understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are once again going to visit our wine data set that we used in [Chapter
    8](f3f7c511-1b7f-4500-ba78-dd208b227ae0.xhtml), *Cluster Analysis*. If you recall,
    it consists of 13 numeric features and a response of three possible classes of
    wine. Our task is to predict those classes. I will include one interesting twist
    and that is to artificially increase the number of observations. The reasons are
    twofold. First, I want to fully demonstrate the resampling capabilities of the
    `mlr` package, and second, I wish to cover a synthetic sampling technique. We
    utilized upsampling in the prior section, so synthetic is in order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first task is to load the package libraries and bring the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We have 178 observations, plus the response labels are numeric (1, 2 and 3).
    Let's more than double the size of our data. The algorithm used in this example
    is **Synthetic Minority Over-Sampling Technique** (**SMOTE**). In the prior example,
    we used upsampling where the minority class was sampled WITH REPLACEMENT until
    the class size matched the majority. With `SMOTE`, take a random sample of the
    minority class and compute/identify the k-nearest neighbors for each observation
    and randomly generate data based on those neighbors. The default nearest neighbors
    in the `SMOTE()` function from the `DMwR` package is 5 (k = 5).  The other thing
    you need to consider is the percentage of minority oversampling. For instance,
    if we want to create a minority class double its current size, we would specify
    `"percent.over = 100"` in the function. The number of new samples for each case added
    to the current minority class is percent over/100, or one new sample for each
    observation. There is another parameter for percent over, and that controls the
    number of majority classes randomly selected for the new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the application of the technique, first starting by structuring the
    classes to a factor, otherwise the function will not work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Voila! We have created a dataset of 624 observations. Our next endeavor will
    involve a visualization of the number of features by class. I am a big fan of
    boxplots, so let''s create boxplots for the first four inputs by class. They have
    different scales, so putting them into a dataframe with mean 0 and standard deviation
    of 1 will aid the comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_ens_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall from [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml), *Logistic
    Regression and Discriminant Analysis* that a dot on the boxplot is considered
    an outlier. So, what should we do with them? There are a number of things to do:'
  prefs: []
  type: TYPE_NORMAL
- en: Nothing--doing nothing is always an option
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete the outlying observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Truncate the observations either within the current feature or create a separate
    feature of truncated values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an indicator variable per feature that captures whether an observation
    is an outlier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I've always found outliers interesting and usually look at them closely to determine
    why they occur and what to do with them. We don't have that kind of time here,
    so let me propose a simple solution and code around truncating the outliers. Let's
    create a function to identify each outlier and reassign a high value (> 99th percentile)
    to the 75th percentile and a low value (< 1 percentile) to the 25th percentile.
    You could do median or whatever, but I've found this to work well.
  prefs: []
  type: TYPE_NORMAL
- en: You could put these code excerpts into the same function, but I've done in this
    fashion for simplification and understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are our outlier functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we execute the function on the original data and create a new dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple comparison of a truncated feature versus the original is in order.
     Let''s try that with `V3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_ens_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So that worked out well. Now it''s time to look at correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_ens_04.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that V6 and V7 are the highest correlated features, and we see a number
    above 0.5\. In general, this is not a problem with non-linear based learning methods,
    but we will account for this in our GLM by incorporating an L2 penalty (ridge
    regression).
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will begin by creating our training and testing sets, then create a random
    forest classifier as our base model. After evaluating its performance, we will
    move on and try the one-versus-rest classification method and see how it performs.
    We split our data 70/30\. Also, one of the unique things about the `mlr` package
    is its requirement to put your training data into a "task" structure, specifically
    a classification task. Optionally, you can place your test set in a task as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'A full list of models is available here, plus you can also utilize your own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/index.html](https://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our training data task created, you have a number of functions to explore
    it. Here is the abbreviated output that looks at its structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many ways to use `mlr` in your analysis, but I recommend creating
    your resample object. Here we create a resampling object to help us in tuning
    the number of trees for our random forest, consisting of three subsamples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The next object establishes the grid of trees for tuning with the minimum number
    of trees, 750, and the maximum of 2000\. You can also establish a number of multiple
    parameters like we did with the `caret` package. Your options can be explored
    using calling help for the function with `makeParamSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a control object, establishing a numeric grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, go ahead and tune the hyperparameter for the optimal number of trees.
    Then, call up both the optimal number of trees and the associated out-of-sample
    error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The optimal number of trees is 1,250 with a mean misclassification error of
    0.01 percent, almost perfect classification. It is now a simple matter of setting
    this parameter for training as a wrapper around the `makeLearner()` function.
    Notice that I set the predict type to probability as the default is the predicted
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the confusion matrix on the train data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, evaluate its performance on the test set, both error and accuracy (1
    - error). With no test task, you specify `newdata = test`, otherwise if you did
    create a test task, just use `test.task`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Well, that was just too easy as we are able to predict each class with no error.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For demonstration purposes, let''s still try our ridge regression on a one-versus-rest
    method. To do this, create a `MulticlassWrapper` for a binary classification method.
    The `classif.penalized.ridge` method is from the `penalized` package, so make
    sure you have it installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s go ahead and create a wrapper for our classifier that creates a
    bagging resample of 10 iterations (it is the default) with replacement, sampling
    70% of the observations and all of the input features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This can now be used to train our algorithm. Notice in the code I put `mlr::`
    before `train()`. The reason is that caret also has a `train()` function, so we
    are specifying we want `mlr` train function and not caret''s. Sometimes, if this
    is not done when both packages are loaded, you will end up with an egregious error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how it did:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Again, it is just too easy. However, don't focus on the accuracy as much as
    the methodology of creating your classifier, tuning any parameters, and implementing
    a resampling strategy.
  prefs: []
  type: TYPE_NORMAL
- en: MLR's ensemble
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is something we haven''t found too easy: the `Pima` diabetes classification.
    Like caret, you can build ensemble models, so let''s give that a try. I will also
    show how to incorporate `SMOTE` into the learning process instead of creating
    a separate dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: First, make sure you run the code from the beginning of this chapter to create
    the train and test sets. I'll pause here and let you take care of that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Great, now let''s create the training task as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `smote()` function here is a little different from what we did before.
    You just have to specify the rate of minority oversample and the k-nearest neighbors.
    We will double our minority class (Yes) based on the three nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have 533 observations instead of the 400 originally in train. To accomplish
    our ensemble stacking, we will create three base models (random forest, quadratic
    discriminant analysis, and L1 penalized GLM). This code puts them together as
    the base models, learners if you will, and ensures we have probabilities created
    for use as input features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The stacking model will simply be a GLM, with coefficients tuned by cross-validation.
    The package default is five folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the base and stacked models. You can choose to incorporate
    resampling and tuning wrappers as you see fit, just like we did in the prior sections.
    In this case, we will just stick with the defaults. Training and prediction on
    the test set works the same way also:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: All that effort and we just achieved 75% accuracy and a slightly inferior `AUC`
    to the ensemble built using `caretEnsemble`, granted we used different base learners.
    So, that begs the question as before, can you improve on these results? Please
    let me know your results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the very important machine learning methods of
    creating an ensemble model by stacking and then multiclass classification. In
    stacking, we used base models (learners) to create predicted probabilities that
    were used on input features to another model (super learner) to make our final
    predictions. Indeed, the stacked method showed slight improvement over the individual
    base models. As for multiclass methods, we worked on using a multiclass classifier
    as well as taking a binary classification method and applying it to a multiclass
    problem using the one-versus-all technique. As a side task, we also incorporated
    two sampling techniques (upsampling and Synthetic Minority Oversampling Technique)
    to balance the classes. Also significant was the utilization of two very powerful
    R packages, `caretEnsemble` and `mlr`. These methods and packages are powerful
    additions to an R machine learning practitioner.
  prefs: []
  type: TYPE_NORMAL
- en: Up next, we are going to delve into the world of time series and causality.
    In my opinion, time series analysis is one of the most misunderstood and neglected
    areas of machine learning. The next chapter should get you on your way to help
    our profession close that gap.
  prefs: []
  type: TYPE_NORMAL
