- en: Creating Ensembles and Multiclass Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集成和多元分类
- en: '"This is how you win ML competitions: you take other people''s work and ensemble
    them together."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “这就是你赢得机器学习竞赛的方法：你将其他人的工作集成在一起。”
- en: '- Vitaly Kuznetsov, NIPS2014'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 维塔利·库兹涅佐夫，NIPS2014'
- en: You may have already realized that we have discussed ensemble learning. It is
    defined by [www.scholarpedia.org](http://www.scholarpedia.org/article/Main_Page)
    as "the process by which multiple models, such as classifiers or experts, are
    strategically generated and combined to solve a particular computational intelligence problem".
    In random forest and gradient boosting, we combined the "votes" of hundreds or
    thousands of trees to make a prediction. Thus, by definition, those models are
    ensembles. This methodology can be extended to any learner to create ensembles,
    which some refer to as meta-ensembles or meta-learners. We will look at one of
    these methods referred to as "stacking". In this methodology, we will produce
    a number of classifiers and use their predicted class probabilities as input features
    to another classifier. This method *can* result in improved predictive accuracy.
    In the previous chapters, we focused on classification problems focused on binary
    outcomes. We will now look at methods to predict those situations where the data
    consists of more than two outcomes, a very common situation in real-world data
    sets. I have to confess that the application of these methods in R is some of
    the most interesting and enjoyable applications I have come across.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经意识到我们已经讨论了集成学习方法。它在[www.scholarpedia.org](http://www.scholarpedia.org/article/Main_Page)上被定义为“通过战略性地生成和组合多个模型，如分类器或专家，来解决特定计算智能问题的过程”。在随机森林和梯度提升中，我们结合了数百或数千棵树的“投票”来进行预测。因此，根据定义，这些模型是集成模型。这种方法可以扩展到任何学习器以创建集成，有些人称之为元集成或元学习器。我们将探讨其中一种被称为“堆叠”的方法。在这种方法中，我们将生成多个分类器，并使用它们的预测类别概率作为另一个分类器的输入特征。这种方法*可以*提高预测准确性。在前几章中，我们专注于关注二元结果的分类问题。现在，我们将探讨预测数据由两个以上结果组成的情况的方法，这在现实世界的数据集中是非常常见的。我必须承认，这些方法在R中的应用是我遇到的最有趣和最令人愉快的应用之一。
- en: Ensembles
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成
- en: 'The quote at the beginning of this chapter mentions using ensembles to win
    machine learning competitions. However, they do have practical applications. I''ve
    provided a definition of what ensemble modeling is, but why does it work? To demonstrate
    this, I''ve co-opted an example, from the following blog, which goes into depth
    at a number of ensemble methods:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开头引用的引言提到了使用集成来赢得机器学习竞赛。然而，它们确实有实际应用。我已经提供了集成建模的定义，但它为什么有效呢？为了证明这一点，我从以下博客中借用了一个例子，该博客深入探讨了多种集成方法：
- en: '[http://mlwave.com/kaggle-ensembling-guide/](http://mlwave.com/kaggle-ensembling-guide/).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://mlwave.com/kaggle-ensembling-guide/](http://mlwave.com/kaggle-ensembling-guide/)'
- en: As I write this chapter, we are only a couple of days away from Super Bowl 51,
    the Atlanta Falcons versus the New England Patriots. Let's say we want to review
    our probability of winning a friendly wager where we want to take the Patriots
    minus the points (3 points as of this writing). Assume that we have been following
    three expert prognosticators that all have the same probability of predicting
    that the Patriots will cover the spread (60%). Now, if we favor any one of the
    so-called experts, it is clear we have a 60% chance to win. However, let's see
    what creating an ensemble of their predictions can do to increase our chances
    of profiting and humiliating friends and family.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我写这一章时，我们离超级碗51只有几天的时间了，亚特兰大猎鹰队对阵新英格兰爱国者队。假设我们想回顾一下我们赢得一场友好赌注的概率，我们希望选择爱国者队减去分数（截至本文写作时为3分）。假设我们已经跟踪了三位专家预测者，他们预测爱国者队能否覆盖赔率的概率相同（60%）。现在，如果我们偏爱任何一位所谓的专家，那么我们显然有60%的胜率。然而，让我们看看创建他们预测的集成能如何提高我们盈利和羞辱朋友和家人的机会。
- en: Start by calculating the probability of each possible outcome for the experts
    picking New England. If all three pick New England, we have 0.6 x 0.6 x 0.6, or
    a 21.6% chance, that all three are correct. If any two of the three pick New England
    then we have (0.6 x 0.6 x 0.3) x 3 for a total of 43.2%. By using majority voting,
    if at least two of the three pick New England, then our probability of winning
    becomes almost 65%.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先计算专家选择新英格兰的每种可能结果的概率。如果三位专家都选择新英格兰，那么我们就有0.6 x 0.6 x 0.6，即21.6%的概率，三位都是正确的。如果其中两位选择新英格兰，那么我们有(0.6
    x 0.6 x 0.3) x 3，总共43.2%。通过使用多数投票，如果至少有两位选择新英格兰，那么我们赢得比赛的概率几乎达到65%。
- en: 'This is a rather simplistic example but representative nonetheless. In machine
    learning, it can manifest itself by incorporating the predictions from several
    average or even weak learners to improve overall accuracy. The diagram that follows
    shows how this can be accomplished:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简单的例子，但仍然具有代表性。在机器学习中，它可以通过结合几个平均或甚至弱学习者的预测来提高整体准确性。下面的图表显示了如何实现这一点：
- en: '![](img/image_ens_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_ens_01.png)'
- en: In this graphic, we build three different classifiers and use their predicted
    probabilities as inputs to a fourth and different classifier in order to make
    predictions on the test data. Let's see how to apply this with R.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图表中，我们构建了三个不同的分类器，并使用它们的预测概率作为输入到第四个不同分类器，以便对测试数据进行预测。让我们看看如何用R来实现这一点。
- en: Business and data understanding
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业和数据理解
- en: 'We are are going to visit our old nemesis the Pima Diabetes data once again.
    It has proved to be quite a challenge with most classifiers producing accuracy
    rates in the mid-70s. We''ve looked at this data in [Chapter 5](a7511867-5362-4215-a7dd-bbdc162740d1.xhtml), *More
    Classification Techniques - K-Nearest Neighbors and Support Vector Machines *and
    [Chapter 6](4dd3d3b8-f4cc-4ddd-b062-34d9684311cd.xhtml), *Classification and Regression
    Trees* so we can skip over the details. There are a number of R packages to build
    ensembles, and it is not that difficult to build your own code. In this iteration,
    we are going to attack the problem with the `caret` and `caretEnsemble` packages.
     Let''s get the packages loaded and the data prepared, including creating the
    train and test sets using the `createDataPartition()` function from caret:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次访问我们老对手皮马糖尿病数据。它已经证明对大多数分类器来说是一个相当大的挑战，大多数分类器的准确率在70年代中期。我们已经在[第5章](a7511867-5362-4215-a7dd-bbdc162740d1.xhtml)，*更多分类技术
    - K最近邻和支持向量机*和[第6章](4dd3d3b8-f4cc-4ddd-b062-34d9684311cd.xhtml)，*分类和回归树*中查看过这些数据，因此我们可以跳过细节。有许多R包可以构建集成，自己编写代码也不是那么困难。在这个迭代中，我们将使用`caret`和`caretEnsemble`包来解决这个问题。让我们加载这些包并准备数据，包括使用caret中的`createDataPartition()`函数创建训练集和测试集：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Modeling evaluation and selection
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估和选择
- en: 'As we''ve done in prior chapters, the first recommended task when utilizing
    caret functions is to build the object that specifies how model training is going
    to happen. This is done with the `trainControl()` function. We are going to create
    a five-fold cross-validation and save the final predictions (the probabilities).
    It is recommended that you also index the resamples so that each base model trains
    on the same folds. Also, notice in the function that I specified upsampling. Why?
    Well, notice that the ratio of "Yes" versus "No" is 2 to 1:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中所做的那样，在利用caret函数时，第一个推荐的任务是构建一个对象，该对象指定了模型训练将如何进行。这是通过`trainControl()`函数完成的。我们将创建一个五折交叉验证并保存最终的预测（概率）。建议您也索引重采样，以便每个基础模型在相同的折上训练。注意，在函数中，我指定了上采样。为什么？好吧，注意“是”与“否”的比例是2比1：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This ratio is not necessarily imbalanced, but I want to demonstrate something
    here. In many data sets, the outcome of interest is a rare event. As such, you
    can end up with a classifier that is highly accurate but does a horrible job at
    predicting the outcome of interest, which is to say it doesn''t predict any true
    positives. To balance the response, you can upsample the minority class, downsample
    the majority class, or create "synthetic data". We will focus on synthetic data
    in the next exercise, but here, let''s try upsampling. In upsampling, for each
    cross-validation fold, the minority class is randomly sampled with replacement
    to match the number of observations in the majority class. Here is our function:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个比率不一定是不平衡的，但我想在这里展示一些东西。在许多数据集中，感兴趣的结果是一个罕见事件。因此，你可能会得到一个高度准确的分类器，但在预测感兴趣的结果方面做得非常糟糕，也就是说，它没有预测任何真正的阳性。为了平衡响应，你可以增加少数类的样本，减少多数类的样本，或者创建“合成数据”。在下一项练习中，我们将专注于合成数据，但在这里，让我们尝试增加样本。在增加样本时，对于每个交叉验证折，少数类会随机有放回地采样以匹配多数类的观察数。以下是我们的函数：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now train our models using the `caretList()` function. You can use any
    model in the function that is supported by the caret package. A list of models
    is available with their corresponding hyperparameters here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`caretList()`函数来训练我们的模型。你可以使用任何由caret包支持的模型。这里有一个模型列表，以及它们对应的超参数：
- en: '[https://rdrr.io/cran/caret/man/models.html](https://rdrr.io/cran/caret/man/models.html)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://rdrr.io/cran/caret/man/models.html](https://rdrr.io/cran/caret/man/models.html)'
- en: 'In this example, we will train three models:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将训练三个模型：
- en: Classification tree - `"rpart"`
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类树 - `"rpart"`
- en: Multivariate Adaptive Regression Splines - `"earth"`
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多变量自适应回归样条 - `"earth"`
- en: K-Nearest Neighbors - `"knn"`
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-最近邻 - `"knn"`
- en: 'Let''s put this all together:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把所有这些都放在一起：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Not only are the models built, but the parameters for each model are tuned
    according to caret''s rules. You can create your own tune grids for each model
    by incorporating the `caretModelSpec()` function, but for demonstration purposes,
    we will let the function do that for us. You can examine the results by calling
    the model object. This is the abbreviated output:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅模型已经建立，而且每个模型的参数都根据caret的规则进行了调整。你可以通过结合`caretModelSpec()`函数为每个模型创建自己的调整网格，但为了演示目的，我们将让函数为我们做这件事。你可以通过调用模型对象来检查结果。这是简化的输出：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A trick to effective ensembles is that the base models are not highly correlated.
    This is a subjective statement, and there is no hard rule of correlated predictions.
    One should experiment with the results and substitute a model if deemed necessary.
    Let''s look at our results:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有效集成的一个技巧是基础模型之间不高度相关。这是一个主观的陈述，没有关于相关预测的硬性规则。应该通过实验结果来决定是否替换模型。让我们看看我们的结果：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The classification tree and earth models are highly correlated. This may be
    an issue, but let''s progress by creating our the fourth classifier, the stacking
    model, and examining the results. To do this, we will capture the predicted probabilities
    for "Yes" on the test set in a dataframe:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树和earth模型高度相关。这可能会成为一个问题，但让我们通过创建第四个分类器，即堆叠模型，并检查结果来继续前进。为此，我们将捕获测试集中“是”的预测概率到一个数据框中：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now stack these models for a final prediction with `caretStack()`. This
    will be a simple logistic regression based on five bootstrapped samples:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用`caretStack()`将这些模型堆叠起来进行最终预测。这将基于五个自助样本的简单逻辑回归：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can examine the final model as such:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样检查最终模型：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Even though `rpart` and earth were highly correlated, their coefficients are
    both significant and we can probably keep both in the analysis. We can now compare
    the individual model results with the `ensembled` learner:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`rpart`和earth模型高度相关，但它们的系数都是显著的，我们可能可以保留这两个模型在分析中。现在我们可以用`ensembled`学习器比较单个模型的结果：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: What we see with the `colAUC()` function is the individual model AUCs and the
    AUC of the stacked/ensemble. The ensemble has led to a slight improvement over
    only using MARS from the earth package. So in this example, we see how creating
    an ensemble via model stacking can indeed increase predictive power. Can you build
    a better ensemble given this data? What other sampling or classifiers would you
    try? With that, let's move on to multi-class problems.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过`colAUC()`函数看到的是单个模型的AUC和堆叠/集成的AUC。集成在仅使用earth包中的MARS的情况下带来了一点点改进。所以在这个例子中，我们看到通过模型堆叠创建集成确实可以增加预测能力。你能根据这些数据构建一个更好的集成吗？你会尝试哪些其他采样或分类器？有了这些，让我们继续探讨多类问题。
- en: Multiclass classification
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类分类
- en: There are a number of approaches to learning in multiclass problems. Techniques
    such as random forest and discriminant analysis will deal with multiclass while
    some techniques and/or packages will not, for example, generalized linear models,
    `glm()`, in base R. As of this writing, the `caretEnsemble` package, unfortunately,
    will not work with multiclasses. However, the Machine Learning in R (`mlr`) package
    does support multiple classes and also ensemble methods. If you are familiar with
    sci-kit Learn for Python, one could say that `mlr` endeavors to provide the same
    functionality for R. The mlr and the caret-based packages are quickly turning
    into my favorites for almost any business problem. I intend to demonstrate how
    powerful the package is on a multiclass problem, then conclude by showing how
    to do an ensemble on the `Pima` data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类问题中，有许多学习方法。例如，随机森林和判别分析等技术将处理多类问题，而一些技术和/或包则不会，例如，基础R中的广义线性模型`glm()`。截至本文写作时，不幸的是，`caretEnsemble`包将无法与多类一起使用。然而，机器学习在R（`mlr`）包支持多类和集成方法。如果你熟悉Python的sci-kit
    Learn，可以说`mlr`旨在为R提供相同的功能。mlr和基于caret的包正在迅速成为我解决几乎所有商业问题的首选。我打算展示这个包在多类问题上的强大功能，然后通过展示如何在`Pima`数据上执行集成来结束。
- en: For the multiclass problem, we will look at how to tune a random forest and
    then examine how to take a GLM and turn it into a multiclass learner using the
    "one versus rest" technique. This is where we build a binary probability prediction
    for each class versus all the others, then ensemble them together to predict an
    observation's final class. The technique allows you to extend any classifier method
    to multiclass problems, and it can often outperform multiclass learners.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类问题，我们将探讨如何调整随机森林，然后检查如何使用“一对余”技术将GLM转换为多类学习器。这就是我们为每个类别与所有其他类别构建二元概率预测，然后将它们组合在一起来预测观察结果的最终类别的地方。这项技术允许你将任何分类器方法扩展到多类问题，并且它通常可以优于多类学习器。
- en: 'One quick note: don''t confuse the terminology of multiclass and multilabel.
    In the former, an observation can be assigned to one and only one class, while
    in the latter, it can be assigned to multiple classes. An example of this is text
    that could be labeled both politics and humor. We will not cover multilabel problems
    in this chapter.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简短的提醒：不要混淆多类和多标签术语。在前者中，一个观察结果只能被分配给一个且仅一个类别，而在后者中，它可以被分配给多个类别。一个例子是既可以被标记为政治又可以被标记为幽默的文本。在本章中，我们不会涵盖多标签问题。
- en: Business and data understanding
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业和数据理解
- en: We are once again going to visit our wine data set that we used in [Chapter
    8](f3f7c511-1b7f-4500-ba78-dd208b227ae0.xhtml), *Cluster Analysis*. If you recall,
    it consists of 13 numeric features and a response of three possible classes of
    wine. Our task is to predict those classes. I will include one interesting twist
    and that is to artificially increase the number of observations. The reasons are
    twofold. First, I want to fully demonstrate the resampling capabilities of the
    `mlr` package, and second, I wish to cover a synthetic sampling technique. We
    utilized upsampling in the prior section, so synthetic is in order.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次访问我们在[第8章](f3f7c511-1b7f-4500-ba78-dd208b227ae0.xhtml)“聚类分析”中使用过的葡萄酒数据集。如果你还记得，它由13个数值特征和三种可能的葡萄酒类别响应组成。我们的任务是预测这些类别。我将包括一个有趣的转折，那就是人为地增加观察结果的数量。原因有两个。首先，我想充分展示`mlr`包的重采样能力，其次，我希望涵盖一种合成采样技术。在前一节中，我们已经使用了上采样，所以现在是合成采样的时候了。
- en: 'Our first task is to load the package libraries and bring the data:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是加载包库并导入数据：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We have 178 observations, plus the response labels are numeric (1, 2 and 3).
    Let's more than double the size of our data. The algorithm used in this example
    is **Synthetic Minority Over-Sampling Technique** (**SMOTE**). In the prior example,
    we used upsampling where the minority class was sampled WITH REPLACEMENT until
    the class size matched the majority. With `SMOTE`, take a random sample of the
    minority class and compute/identify the k-nearest neighbors for each observation
    and randomly generate data based on those neighbors. The default nearest neighbors
    in the `SMOTE()` function from the `DMwR` package is 5 (k = 5).  The other thing
    you need to consider is the percentage of minority oversampling. For instance,
    if we want to create a minority class double its current size, we would specify
    `"percent.over = 100"` in the function. The number of new samples for each case added
    to the current minority class is percent over/100, or one new sample for each
    observation. There is another parameter for percent over, and that controls the
    number of majority classes randomly selected for the new dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the application of the technique, first starting by structuring the
    classes to a factor, otherwise the function will not work:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Voila! We have created a dataset of 624 observations. Our next endeavor will
    involve a visualization of the number of features by class. I am a big fan of
    boxplots, so let''s create boxplots for the first four inputs by class. They have
    different scales, so putting them into a dataframe with mean 0 and standard deviation
    of 1 will aid the comparison:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_ens_02.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Recall from [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml), *Logistic
    Regression and Discriminant Analysis* that a dot on the boxplot is considered
    an outlier. So, what should we do with them? There are a number of things to do:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Nothing--doing nothing is always an option
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete the outlying observations
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Truncate the observations either within the current feature or create a separate
    feature of truncated values
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an indicator variable per feature that captures whether an observation
    is an outlier
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I've always found outliers interesting and usually look at them closely to determine
    why they occur and what to do with them. We don't have that kind of time here,
    so let me propose a simple solution and code around truncating the outliers. Let's
    create a function to identify each outlier and reassign a high value (> 99th percentile)
    to the 75th percentile and a low value (< 1 percentile) to the 25th percentile.
    You could do median or whatever, but I've found this to work well.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: You could put these code excerpts into the same function, but I've done in this
    fashion for simplification and understanding.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'These are our outlier functions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we execute the function on the original data and create a new dataframe:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A simple comparison of a truncated feature versus the original is in order.
     Let''s try that with `V3`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_ens_03.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: 'So that worked out well. Now it''s time to look at correlations:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_ens_04.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: We see that V6 and V7 are the highest correlated features, and we see a number
    above 0.5\. In general, this is not a problem with non-linear based learning methods,
    but we will account for this in our GLM by incorporating an L2 penalty (ridge
    regression).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and selection
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will begin by creating our training and testing sets, then create a random
    forest classifier as our base model. After evaluating its performance, we will
    move on and try the one-versus-rest classification method and see how it performs.
    We split our data 70/30\. Also, one of the unique things about the `mlr` package
    is its requirement to put your training data into a "task" structure, specifically
    a classification task. Optionally, you can place your test set in a task as well.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'A full list of models is available here, plus you can also utilize your own:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[https://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/index.html](https://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/index.html)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Random forest
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our training data task created, you have a number of functions to explore
    it. Here is the abbreviated output that looks at its structure:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'There are many ways to use `mlr` in your analysis, but I recommend creating
    your resample object. Here we create a resampling object to help us in tuning
    the number of trees for our random forest, consisting of three subsamples:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The next object establishes the grid of trees for tuning with the minimum number
    of trees, 750, and the maximum of 2000\. You can also establish a number of multiple
    parameters like we did with the `caret` package. Your options can be explored
    using calling help for the function with `makeParamSet`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, create a control object, establishing a numeric grid:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, go ahead and tune the hyperparameter for the optimal number of trees.
    Then, call up both the optimal number of trees and the associated out-of-sample
    error:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The optimal number of trees is 1,250 with a mean misclassification error of
    0.01 percent, almost perfect classification. It is now a simple matter of setting
    this parameter for training as a wrapper around the `makeLearner()` function.
    Notice that I set the predict type to probability as the default is the predicted
    class:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we train the model:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can see the confusion matrix on the train data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, evaluate its performance on the test set, both error and accuracy (1
    - error). With no test task, you specify `newdata = test`, otherwise if you did
    create a test task, just use `test.task`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Well, that was just too easy as we are able to predict each class with no error.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For demonstration purposes, let''s still try our ridge regression on a one-versus-rest
    method. To do this, create a `MulticlassWrapper` for a binary classification method.
    The `classif.penalized.ridge` method is from the `penalized` package, so make
    sure you have it installed:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们仍然尝试使用单对余方法进行岭回归。为此，为二元分类方法创建一个 `MulticlassWrapper`。`classif.penalized.ridge`
    方法来自 `penalized` 包，所以请确保你已经安装了它：
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now let''s go ahead and create a wrapper for our classifier that creates a
    bagging resample of 10 iterations (it is the default) with replacement, sampling
    70% of the observations and all of the input features:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续创建一个用于我们的分类器的包装器，该包装器创建一个包含 10 次迭代（默认值）的袋装重采样，采样 70% 的观测值和所有输入特征：
- en: '[PRE28]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This can now be used to train our algorithm. Notice in the code I put `mlr::`
    before `train()`. The reason is that caret also has a `train()` function, so we
    are specifying we want `mlr` train function and not caret''s. Sometimes, if this
    is not done when both packages are loaded, you will end up with an egregious error:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用这个来训练我们的算法。注意在代码中我在 `train()` 前面加了 `mlr::`。原因是 caret 也有一个 `train()` 函数，所以我们指定我们想要
    `mlr` 的 `train()` 函数，而不是 caret 的。有时，如果两个包都加载了但没有这样做，你最终会得到一个严重的错误：
- en: '[PRE29]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s see how it did:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看效果如何：
- en: '[PRE30]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Again, it is just too easy. However, don't focus on the accuracy as much as
    the methodology of creating your classifier, tuning any parameters, and implementing
    a resampling strategy.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这太容易了。然而，不要过分关注准确性，而应该关注创建你的分类器的方法、调整任何参数以及实施重采样策略的方法。
- en: MLR's ensemble
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLR 的集成
- en: 'Here is something we haven''t found too easy: the `Pima` diabetes classification.
    Like caret, you can build ensemble models, so let''s give that a try. I will also
    show how to incorporate `SMOTE` into the learning process instead of creating
    a separate dataset.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些我们觉得不太容易的事情：`Pima` 糖尿病分类。和 caret 一样，你可以构建集成模型，所以让我们试一试。我还会展示如何将 `SMOTE`
    集成到学习过程中，而不是创建一个单独的数据集。
- en: First, make sure you run the code from the beginning of this chapter to create
    the train and test sets. I'll pause here and let you take care of that.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保你从本章的开头运行代码来创建训练和测试集。我会在这里暂停，让你去处理。
- en: 'Great, now let''s create the training task as before:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，现在让我们像以前一样创建训练任务：
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `smote()` function here is a little different from what we did before.
    You just have to specify the rate of minority oversample and the k-nearest neighbors.
    We will double our minority class (Yes) based on the three nearest neighbors:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `smote()` 函数与我们之前做的不太一样。你只需要指定少数类过采样率和 k-最近邻。我们将基于三个最近邻将少数类（是）的数量加倍：
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We now have 533 observations instead of the 400 originally in train. To accomplish
    our ensemble stacking, we will create three base models (random forest, quadratic
    discriminant analysis, and L1 penalized GLM). This code puts them together as
    the base models, learners if you will, and ensures we have probabilities created
    for use as input features:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有 533 个观测值，而不是训练中的原始 400 个。为了完成我们的集成堆叠，我们将创建三个基础模型（随机森林、二次判别分析和 L1 惩罚 GLM）。这段代码将它们组合成基础模型，即学习者，并确保我们创建了用于输入特征的概率：
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The stacking model will simply be a GLM, with coefficients tuned by cross-validation.
    The package default is five folds:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠模型将简单地是一个 GLM，系数通过交叉验证调整。包的默认值是五折：
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can now train the base and stacked models. You can choose to incorporate
    resampling and tuning wrappers as you see fit, just like we did in the prior sections.
    In this case, we will just stick with the defaults. Training and prediction on
    the test set works the same way also:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以训练基础和堆叠模型。你可以选择根据需要合并重采样和调整包装器，就像我们在前面的章节中所做的那样。在这种情况下，我们将坚持默认设置。在测试集上的训练和预测也是同样的方式：
- en: '[PRE35]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: All that effort and we just achieved 75% accuracy and a slightly inferior `AUC`
    to the ensemble built using `caretEnsemble`, granted we used different base learners.
    So, that begs the question as before, can you improve on these results? Please
    let me know your results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 经过所有这些努力，我们只达到了 75% 的准确率，以及略低于使用 `caretEnsemble` 构建的集成模型的 `AUC`，尽管我们使用了不同的基础学习器。所以，就像以前一样，你能提高这些结果吗？请告诉我你的结果。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at the very important machine learning methods of
    creating an ensemble model by stacking and then multiclass classification. In
    stacking, we used base models (learners) to create predicted probabilities that
    were used on input features to another model (super learner) to make our final
    predictions. Indeed, the stacked method showed slight improvement over the individual
    base models. As for multiclass methods, we worked on using a multiclass classifier
    as well as taking a binary classification method and applying it to a multiclass
    problem using the one-versus-all technique. As a side task, we also incorporated
    two sampling techniques (upsampling and Synthetic Minority Oversampling Technique)
    to balance the classes. Also significant was the utilization of two very powerful
    R packages, `caretEnsemble` and `mlr`. These methods and packages are powerful
    additions to an R machine learning practitioner.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了通过堆叠和随后进行多类分类创建集成模型的重要机器学习方法。在堆叠中，我们使用了基础模型（学习器）来创建预测概率，这些概率被用于输入特征到另一个模型（超级学习器）以做出我们的最终预测。实际上，堆叠方法在个别基础模型之上显示出了轻微的改进。至于多类方法，我们研究了使用多类分类器，以及将二元分类方法应用于多类问题，使用一对一技术。作为一个辅助任务，我们还结合了两种采样技术（上采样和合成少数类过采样技术）来平衡类别。同样重要的是，我们利用了两个非常强大的R包，`caretEnsemble`和`mlr`。这些方法和包是R机器学习实践者的强大补充。
- en: Up next, we are going to delve into the world of time series and causality.
    In my opinion, time series analysis is one of the most misunderstood and neglected
    areas of machine learning. The next chapter should get you on your way to help
    our profession close that gap.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨时间序列和因果关系的领域。在我看来，时间序列分析是机器学习中最被误解和忽视的领域之一。下一章应该能帮助你开始帮助我们的行业缩小这一差距。
