- en: '*Appendix*: Anomaly Detection Tips'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we wind down the content for this book, it occurred to us that there's still
    a plethora of good, bite-sized explanations, examples, and pieces of advice that
    didn't quite fit into sections of the other chapters. It therefore made sense
    to give them a home all to themselves here in the [*Appendix*](B17040_14_Epub_AM.xhtml#_idTextAnchor248).
    Enjoy this potpourri of tips, tricks, and advice!
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered here in the [*Appendix*](B17040_14_Epub_AM.xhtml#_idTextAnchor248):'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding influencers in split versus non-split jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using one-sided functions to your advantage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ignoring time periods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using custom rules and filters to your advantage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection job throughput considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding the over-engineering of a use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using anomaly detection on runtime fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The information in this chapter will use the Elastic Stack as it exists in v7.12\.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding influencers in split versus non-split jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might question whether or not it is necessary to split the analysis by a
    field, or merely hope that the use of influencers will give the desired effect
    of identifying the offending entity.
  prefs: []
  type: TYPE_NORMAL
- en: Let's remind ourselves of the difference between the purpose of influencers
    and the purpose of splitting a job. An entity is identified by an anomaly detection
    job as an influencer if it has contributed significantly to the existence of the
    anomaly. This notion of deciding influential entities is completely independent
    of whether or not the job is split. An entity can be deemed influential on an
    anomaly only if an anomaly happens in the first place. If there is no anomaly
    detected, there is no need to figure out whether there is an influencer. However,
    the job may or may not find that something is anomalous, depending on whether
    or not the job is split into multiple time series. When splitting the job, you
    are modeling (creating a separate analysis) for each entity of the field chosen
    for the split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at one of the Elastic ML development team''s favorite demo datasets,
    called `farequote` (available in the GitHub repository for this book as a file
    entitled `farequote-2021.csv` and easily uploaded via Elastic ML''s file upload
    in the **Data Visualizer**). This dataset originated from a real customer, who
    ran a travel portal application. The application''s access log recorded the number
    of times a piece of middleware was called when it reached out to a third-party
    airline for a quote of airline fares. The JSON documents look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The number of events per unit of time corresponds to the number of requests
    being made, and the `responsetime` field is the response time of that individual
    request to the fare quoting web service of that airline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1**: An analysis of count over time, not split on airline, but using
    airline as an influencer. You could accomplish this using the Multi-metric wizard
    configured as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure A.1 – A job with no split, but an influencer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.1 – A job with no split, but an influencer
  prefs: []
  type: TYPE_NORMAL
- en: 'After the analysis runs, the spike seen in the middle of the data (as shown
    in the job configuration preview screen in *Figure A.1*) is indeed flagged as
    anomalous with a modest score of 27, and the airline of AAL was flagged as an
    influencer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.2 – Result of count job with no split, but an influencer found'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.2 – Result of count job with no split, but an influencer found
  prefs: []
  type: TYPE_NORMAL
- en: Let's compare this result to what we see in the next case.
  prefs: []
  type: TYPE_NORMAL
- en: '`airline`, and using `airline` as an influencer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we repeat the configuration in *Figure A.1*, but this time choose to split
    on `airline` (thereby setting `partition_field_name:airline`), we will certainly
    see that airline AAL is still the most unusual, and that the anomaly score for
    it is much higher than in Case 1:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.3 – Result of count job with a split, and an influencer found'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.3 – Result of count job with a split, and an influencer found
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the anomalous behavior of AAL is much more prominent when the
    job was split to model each airline separately. Otherwise, AAL's anomalous behavior
    was being masked somewhat when all the document counts were mixed. This is even
    more prominent when we show the difference of splitting versus not splitting when
    we look at an analysis of the `responsetime` field in the next two cases.
  prefs: []
  type: TYPE_NORMAL
- en: '`responsetime` field, split on `airline`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we see that AAL is also the most unusual airline with respect to an analysis
    of the `responsetime` field:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.4 – Result of response time job with a split, and an influencer
    found'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.4 – Result of response time job with a split, and an influencer found
  prefs: []
  type: TYPE_NORMAL
- en: Let's now compare this result to the next case, where we won't split the job.
  prefs: []
  type: TYPE_NORMAL
- en: '`responsetime` field, no split, but still using `airline` as an influencer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, the results are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.5 – Result of response time job without a split, and an influencer
    found'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.5 – Result of response time job without a split, and an influencer
    found
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that the airline that we know is the most unusual (AAL) is no longer
    found. In this case, all of the airline''s response times are getting averaged
    together each bucket span, because the job is not split. Now, the most prominent
    anomaly (even though it is a relatively minor variation above normal) is shown
    and is deemed to be influenced by `airline=NKS`. However, this may be misleading.
    You see, `airline=NKS` has a very stable response time during this period, but
    note that its normal operating range is much higher than the rest of the group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.6 – Average response times of each airline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.6 – Average response times of each airline
  prefs: []
  type: TYPE_NORMAL
- en: As such, the contribution of NKS to the total aggregate response times of all
    airlines is more significant than the others. So, of course, ML identifies NKS
    as the most prominent influencer.
  prefs: []
  type: TYPE_NORMAL
- en: 'And there you have it: the moral here is that you should be thoughtful if you
    are simply relying on influencers to find unusual entities within a dataset of
    multiple entities. It might be more sensible to individually model each entity
    independently!'
  prefs: []
  type: TYPE_NORMAL
- en: Using one-sided functions to your advantage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many people realize the usefulness of one-sided functions in ML, such as `low_count`
    and `high_mean`, to allow for the detection of anomalies only on the high side
    or on the low side. This is useful when you only care about a drop in revenue
    or a spike in response time.
  prefs: []
  type: TYPE_NORMAL
- en: However, when you care about deviations in both directions, you are often inclined
    to use just the regular function (such as `count` or `mean`). However, on some
    datasets, it is more optimal to use both the high and low versions of the function
    as two separate detectors. Why is this the case and under what conditions, you
    might ask?
  prefs: []
  type: TYPE_NORMAL
- en: 'The condition where this makes sense is when the dynamic range of the possible
    deviations is asymmetrical. In other words, the magnitude of potential spikes
    in the data is far, far bigger than the magnitude of the potential drops, possibly
    because the count or sum of something cannot be less than zero. Let''s look at
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.7 – An analysis using the two-sided "sum" function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.7 – An analysis using the two-sided "sum" function
  prefs: []
  type: TYPE_NORMAL
- en: Here, the two-sided `sum` function properly identifies the large spike with
    a critical anomaly on the left, but the lack of expected double bumps in the middle
    is identified with only warning anomalies. Again, this is because, with a double-sided
    function, the normalization process ranks all anomalies together. The magnitude
    (and therefore the unlikeliness) of the spike is far bigger than the lack of data
    around 18:00, so the anomaly scores are assigned relatively.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if the dataset was analyzed with two separate detectors, using an
    advanced job, that is, `low_sum(num_trx)` and `high_sum(num_trx)`, then the results
    would look very different. Here''s the result of the high side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.8 – An analysis using the one-sided "high_sum" function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.8 – An analysis using the one-sided "high_sum" function
  prefs: []
  type: TYPE_NORMAL
- en: 'And here''s the result of the low side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.9 – An analysis using the one-sided "low_sum" function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.9 – An analysis using the one-sided "low_sum" function
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the anomalies in the middle are now scored much higher (in this
    case, with a maximum score of 47 yellow).
  prefs: []
  type: TYPE_NORMAL
- en: So now, when the two one-sided detectors are run together in the same job, you've
    optimized the dynamic range of each detector (since they have their own normalization
    table)!
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring time periods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, people ask how they can get ML to ignore the fact that a certain event
    has occurred. Perhaps it was an expected maintenance window, or perhaps something
    was broken within the data ingest pipeline and data was lost for a few moments.
    There are a few ways that you can get ML to ignore time periods, and for distinction,
    we''ll separate them into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: A known, upcoming window of time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An unexpected window of time that is discovered only after the fact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate things, we''ll use a single-metric count job (from *Figure A.1*)
    on the `farequote` dataset that has an anomaly on the date of February 9th:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.10 – An analysis on the farequote dataset with an anomaly we''d
    like to ignore'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.10 – An analysis on the farequote dataset with an anomaly we'd like
    to ignore
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's explore the ways we can ignore the anomaly on February 9th using
    different situations.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring an upcoming (known) window of time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Two methods can be used to ignore an upcoming window of time, as shown in the
    following subsections. One involves creating a special calendar event, and the
    other manipulates the time that the data feed runs for.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a calendar event
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can easily create an event by clicking on **Settings** and then **Create**
    under the **Calendar** section. Here, I''ve created a calendar entry for February
    9th:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.11 – Creating a calendar event to ignore a specific time period'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.11 – Creating a calendar event to ignore a specific time period
  prefs: []
  type: TYPE_NORMAL
- en: 'If a new job was created (and in this case, belonged to the **farequote_jobs**
    group so that it obeyed this calendar), then if the job were to be run over the
    data, the entire day of February 9th would be completely ignored:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.12 – A time period being ignored via a calendar event'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.12 – A time period being ignored via a calendar event
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the entire day was masked, including the time of the anomalous
    spike.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping and starting the data feed to ignore the desired timeframe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By simply stopping and restarting the data feed of the anomaly detection job
    at the appropriate times, you can create a gap in the analysis. Here, the data
    feed was stopped at midnight on February 9th and restarted at midnight on February
    10th:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.13 – A time period being ignored via manipulation of the data feed'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.13 – A time period being ignored via manipulation of the data feed
  prefs: []
  type: TYPE_NORMAL
- en: It was like February 9th never happened! Now, let's discuss what you can do
    to ignore a window of time after the fact.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring an unexpected window of time, after the fact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To go *back in time* and *forget* that a window of time happened, we can use
    two methods. The first involves the simple cloning and re-running of historical
    data, and the second involves the use of model snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: Cloning the job and re-running the historical data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to the previous section as we saw, resulting in *Figure A.13*, we could
    create a new cloned job and just have the data feed avoid the window of time you
    wish to ignore. Stop it at the beginning of the window and resume it at the end
    of the window. This method works just fine if rebuilding the model from existing
    (still available) historical data isn't that burdensome. However, if you have
    really mature models that encapsulate data behaviors gleaned from data that you
    no longer have access to (because it aged out and was dropped from your cluster),
    then you will instead need to use the model snapshot technique discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Reverting a job to a prior model snapshot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When cloning and retraining the job on existing historical data isn't desired
    or practical, you can effectively remove a window of time by using the fact that
    a model snapshot is taken periodically by the running job. By default, snapshots
    are captured approximately every 3 to 4 hours. You can change this interval (`background_persist_interval`)
    when you create or update a job.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Retention of these snapshots is controlled by a few other parameters (such as
    `daily_model_snapshot_retention_after_days` and `model_snapshot_retention_days`).
    Consult the Anomaly Detection API documentation at [https://www.elastic.co/guide/en/machine-learning/current/ml-api-quickref.html](https://www.elastic.co/guide/en/machine-learning/current/ml-api-quickref.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic procedure for reverting an anomaly detection job to a prior snapshot
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Stop the job's data feed if it is running.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the most recent model snapshot that was taken just before the window of
    time you wish to erase by using the `get` `snapshots` API call as documented at
    [https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Revert the job to that snapshot by clicking the revert icon (![](img/B17040_14_15.png))
    or, if using the API, use the `_revert` command. In the Kibana UI, you will see
    options on how to delete the data and replay an analysis of the historical data
    after the snapshot time, including the ability to mask out the problematic time
    period using a **Calendar** event:![Figure A.15 – Reverting to a prior model snapshot
    with the ability to mask a period of time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_14_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure A.15 – Reverting to a prior model snapshot with the ability to mask a
    period of time
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Continue running the data feed in real time following the ignored time period,
    if desired.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With all of these helpful options, you can easily determine the right approach
    to ignoring a time interval and keeping your anomaly detection jobs from being
    polluted by problematic operational issues or undesired events.
  prefs: []
  type: TYPE_NORMAL
- en: Using custom rules and filters to your advantage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the anomaly detection jobs are incredibly useful, they are also agnostic
    to the domain and to the relevance of the raw data. In other words, the unsupervised
    machine learning algorithms do not know that a tenfold increase in CPU utilization
    (from 1% to 10%, for example) may not be that interesting to the proper operation
    of an application even though it may be statistically anomalous/unlikely in the
    scenario. Likewise, the anomaly detection jobs treat every entity analyzed equally,
    but the user might want to disavow results for a certain IP address or user ID,
    since the user knows that anomalies found for these entities are not desired or
    useful. The usage of custom rules and filters allows the user to inject domain
    knowledge into the anomaly detection job configuration, thereby having a fair
    amount of control as to what gets deemed or marked anomalous – or even if entities
    get considered part of the modeling process in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Creating custom rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To define a custom rule, you can either accomplish it at job creation time
    (but only if using the Create job API) or after the job has revealed some anomalies
    by using the **Configure rules** menu option from the **actions** menu in the
    Anomaly Explorer UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.16 – The Configure rules menu item in the Anomaly Explorer UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.16 – The Configure rules menu item in the Anomaly Explorer UI
  prefs: []
  type: TYPE_NORMAL
- en: 'When defining a rule, it is mostly self-explanatory. Here, we may decide that
    despite our response time anomaly of 282.025 ms (shown in *Figure A.16*), this
    is not that interesting and that we wish to ignore anomalies if the response time
    is still below 1 second (1,000 ms). We can define the rule as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.17 – The Create rule UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.17 – The Create rule UI
  prefs: []
  type: TYPE_NORMAL
- en: There are additional options to exclude the value from the modeling and also
    to limit the scope of the rule to a certain filter list to have the rule only
    apply to particular entities (for example, only servers that are in a certain
    location). Filter lists can be defined under **Settings** and then **Filter lists**
    in the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the rule definition applies to future analysis (from the point of
    rule definition, forward in time) and does not apply to past anomalies. To have
    the rule apply to past anomalies, you would have to clone the existing job (once
    the rule was defined) and then re-run the analysis on the historical raw data.
  prefs: []
  type: TYPE_NORMAL
- en: So, with rules and filters, the user has a lot of control as to what ultimately
    gets reported (and alerted) upon as anomalous. This allows a pretty major paradigm
    shift over the traditional approach of a bottom-up alert creation philosophy that
    has existed in IT operations for decades. An alternative approach is described
    in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Benefiting from custom rules for a "top-down" alerting philosophy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we asked, "what percentage of the data that you collect is being paid attention
    to?", often, a realistic answer is likely <10%, and maybe even <1%. The reason
    why this is the case is that the traditional approach to making data proactive
    is to start from scratch and then build up thresholds or rules-based alerts over
    time. This can be a daunting and/or tedious task that requires upfront knowledge
    (or at least a guess) as to what the expected behavior of each time series should
    be. Then, once the alerts have been configured, there can be an extended tuning
    process that balances alert sensitivity with annoying false positives. Additionally,
    there could also be metrics whose unusual behaviors could never be caught with
    a static threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Combine this challenge with scale; if I have 10 metrics per server and 100 servers,
    there are 1,000 individual metrics. Creating individual alerts for each of these
    is impractical.
  prefs: []
  type: TYPE_NORMAL
- en: However, a single anomaly detection job could be created against this data in
    less than 1 minute. Elastic ML's self-learning on historical data, which also
    takes very little time, will minimize false positives by adapting to the natural
    characteristics of each time series independently. However, if anomaly detection
    reveals things we don't care to know about, we can simply exclude them with custom
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: This top-down approach (coverage for everything and then begin to exclude what
    you don't want) is faster and provides broader proactive coverage of the data
    than the bottom-up approach (creating threshold alerts from scratch).
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection job throughput considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Elastic ML is awesome and is no doubt very fast and scalable, but there will
    still be a practical upper bound of events/second processed to any anomaly detection
    job, depending on a couple of different factors:'
  prefs: []
  type: TYPE_NORMAL
- en: The speed at which data can be delivered to the algorithms (that is, query performance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The speed at which the algorithms can chew through the data, given the desired
    analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the latter, much of the performance is based upon the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The function(s) chosen for the analysis, that is, `count` is faster than `lat_long`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `bucket_span` value chosen (longer bucket spans are faster than smaller
    bucket spans because more buckets analyzed per unit of time compound the per-bucket
    processing overhead, which is writing results and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, if you have a defined analysis set up and can''t change it for other
    reasons, then there''s not that much you can do unless you get creative and split
    the data up into multiple jobs. This is because the ML jobs (at least for now)
    are currently tied to a single CPU for the analysis bit (running the C++ process
    called autodetect). So, splitting the data into a few separate ML jobs to at least
    take advantage of multiple CPUs might be an option. But, before that, let''s focus
    on the former, the query''s performance, as there are a variety of possibilities
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid doing a cross-cluster search to limit data transmission across the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tweak data feed parameters to optimize performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Elasticsearch query aggregations to distribute the task of distilling the
    data to a smaller set of ML algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first one is sort of obvious. You're only going to improve performance if
    you move the analysis closer to the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: The second one may take some experimentation. There are parameters, such as
    `scroll_size`, which control the size of each scroll. The default is 1,000, and
    for decent-sized clusters, this could be safely increased to 10,000\. Run some
    tests at different scroll sizes and see how it affects query and cluster performance.
  prefs: []
  type: TYPE_NORMAL
- en: The last one should make the biggest impact on performance, in my opinion, but
    obviously, it is a little tricky and error-prone to get the ES aggregation correct
    for it to work properly with ML, but it's not so bad. Refer to the documentation
    at [https://www.elastic.co/guide/en/machine-learning/current/ml-configuring-aggregation.html](https://www.elastic.co/guide/en/machine-learning/current/ml-configuring-aggregation.html)
    for more information. The downside of using aggregations with ML, in general,
    is that you lose access to the other fields in the data that might be good as
    influencers.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, these are a few things to consider when optimizing the ML job's
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding the over-engineering of a use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I once worked with a user where we discussed different use cases for anomaly
    detection. In particular, this customer was building a hosted security operations
    center as part of their **managed security service provider** (**MSSP**) business,
    so they were keen to think about use cases in which ML could help.
  prefs: []
  type: TYPE_NORMAL
- en: A high-level theme to their use cases was to look at a user's behavior and find
    unexpected behavior. One example that was discussed was login activity from unusual/rare
    locations such as *Bob just logged in from Ukraine, but he doesn't normally log
    in from there*.
  prefs: []
  type: TYPE_NORMAL
- en: In the process of thinking the implementation through, there was talk of them
    having multiple clients, each of which had multiple users. Therefore, they were
    thinking of ways to split/partition the data so that they could execute `rare
    by country` for each and every user of every client.
  prefs: []
  type: TYPE_NORMAL
- en: I asked them to take a step back and said, "Is it worthy of an anomaly if anyone
    logs in from Ukraine, not just Bob?" to which the answer was "Yes."
  prefs: []
  type: TYPE_NORMAL
- en: So, in this case, there is no point in splitting the analysis out per user;
    perhaps just keep the partitioning at the client level and simply lump all of
    the user's locations from each client into a single pool of observed countries.
    This is actually a better scenario; there's more overall data, and as we know,
    the `rare` function works best when there is lots of routine data to contrast
    a novel observation against.
  prefs: []
  type: TYPE_NORMAL
- en: Using anomaly detection on runtime fields
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some cases, it might be necessary to analyze the value of a field that doesn't
    exist in the index mappings but can be calculated dynamically from other field
    values. This capability to dynamically define field values has existed for quite
    some time in Elasticsearch as **script fields**, but starting in v7.11, script
    fields are replaced by an updated concept known as **runtime fields**. In short,
    runtime fields are treated like first-class citizens in the Elasticsearch mapping
    (if defined there) and will eventually allow the user to promote a runtime field
    into an indexed field.
  prefs: []
  type: TYPE_NORMAL
- en: Users can define runtime fields in the mapping or only in the search request.
    It is good to note that at the time of writing, there is no support for definitions
    of runtime fields in the data feed of an anomaly detection job. However, if the
    runtime fields are defined in the mappings, then the anomaly detection job can
    leverage them seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information on runtime fields, please consult the Elastic documentation
    at [https://www.elastic.co/guide/en/elasticsearch/reference/current/runtime.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/runtime.html).
  prefs: []
  type: TYPE_NORMAL
- en: While the full details of runtime fields are beyond the scope of this book,
    it is important to know that anomaly detection jobs can leverage these dynamic
    fields just as if they were normal fields. Let's look at an interesting, albeit
    contrived, example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that we return to the `farequote` example shown in *Figure A.1*, where,
    for the sake of this argument, we declare that February 9th is a special day of
    some type for `airline:AAL` – perhaps the rough equivalent of Black Friday or
    Cyber Monday, or even just a day where we know things will be slightly off from
    normal by a known amount. We will contrive a scenario in which we know that AAL
    will experience predictably higher response times that could be 20% slower than
    normal (meaning measurements of `responsetime` should be 20% higher, in milliseconds).
    We don''t want to make February 9th a calendar event for Elastic ML to avoid completely,
    and we don''t want to stop looking at the data for AAL. Rather, we just want to
    temper the response time measures down by 20% so as not to upset our normal modeling
    and/or alerting. We can accomplish this via runtime fields:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is to define a new runtime field in the mapping for the
    index, called `responsetime_adjusted`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This field (for now) will be exactly the same as the `responsetime` field for
    all airlines, simply accomplished by multiplying the field value by the constant
    of `1.0`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will configure a job to use a `high_mean` detector on this new `responsetime_adjusted`
    field, where we will also split the analysis on the `airline` field:![Figure A.18
    – Configuring a job to analyze a runtime field
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_14_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure A.18 – Configuring a job to analyze a runtime field
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will run the data feed up until midnight on February 9th but stop the analysis
    there. To then temper the response time for AAL''s data by 20% (but leave other
    airlines'' data alone), we will execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we will continue the job's data feed to analyze the special day (February
    9th, but stop at midnight at the beginning of February 10th).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once February 9th's data has been analyzed, we will return the response time
    for AAL's data back to normal by re-invoking the command in *Step 1*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll allow the job to continue analyzing the rest of the data, as normal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The end result is that we were able to successfully suppress the values of
    AAL''s response times by 20% (as evidenced by the depressed values between the
    two annotations), but we were still able to pick up a significant anomaly despite
    our special treatment for AAL:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.19 – Results of the job that analyzed a dynamically changed runtime
    field'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_14_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.19 – Results of the job that analyzed a dynamically changed runtime
    field
  prefs: []
  type: TYPE_NORMAL
- en: This technique could be useful in bringing any number of dynamic modifications
    to the data on the fly for enhanced analysis or to support the analysis in aspects
    of the data that may not be available in the default field mappings of the index.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elastic ML is a powerful, flexible, yet easy-to-use feature that gives the power
    of data science to non-data scientists so that they can gain insight into massive
    amounts of data. Throughout this entire book, there are many different ways in
    which users can take advantage of technology to solve real-world challenges in
    IT. We hope that you will take the knowledge that you have gained in this book
    and implement some great use cases of your own. Don't worry about solving all
    possible problems on day 1 – start small, get some tangible wins, and grow your
    usage as you gain more confidence. Success will breed success!
  prefs: []
  type: TYPE_NORMAL
