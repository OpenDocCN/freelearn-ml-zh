- en: Principal Components Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: '"Some people skate to the puck. I skate to where the puck is going to be."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “有些人滑向冰球，我滑向冰球将要到达的地方。”
- en: '- Wayne Gretzky'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 沃伊恩·格雷茨基'
- en: This chapter is the second one where we will focus on unsupervised learning
    techniques. In the previous chapter, we covered cluster analysis, which provides
    us with the groupings of similar observations. In this chapter, we will see how
    to reduce the dimensionality and improve the understanding of our data by grouping
    the correlated variables with **Principal Components Analysis** (**PCA**). Then,
    we will use the principal components in supervised learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是第二章，我们将重点关注无监督学习技术。在前一章中，我们介绍了聚类分析，它为我们提供了相似观察值的分组。在本章中，我们将看到如何通过将相关变量分组为主成分分析（**PCA**）来降低数据的维度并提高对数据的理解。然后，我们将使用主成分进行监督学习。
- en: In many datasets, particularly in the social sciences, you will see many variables
    highly correlated with each other. They may additionally suffer from high dimensionality
    or, as it is better known, the **curse of dimensionality**. This is a problem
    because the number of samples needed to estimate a function grows exponentially
    with the number of input features. In such datasets, there may be the case that
    some variables are redundant as they end up measuring the same constructs, for
    example, income and poverty or depression and anxiety. The goal then is to use
    PCA in order to create a smaller set of variables that capture most of the information
    from the original set of variables, thus simplifying the dataset and often leading
    to hidden insights. These new variables (principal components) are highly uncorrelated
    with each other. In addition to supervised learning, it is also very common to
    use these components to perform data visualization.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多数据集中，尤其是在社会科学领域，你会看到许多变量之间高度相关。它们还可能遭受高维度的困扰，或者如人们所熟知的，**维度诅咒**。这是一个问题，因为估计函数所需的样本数量会随着输入特征数量的指数增长。在这样的数据集中，可能存在一些变量是冗余的，因为它们最终测量的是相同的结构，例如，收入和贫困或抑郁和焦虑。因此，目标是使用PCA来创建一个较小的变量集，该集能够从原始变量集中捕获大部分信息，从而简化数据集，并经常导致隐藏的洞察。这些新变量（主成分）彼此之间高度不相关。除了监督学习之外，使用这些成分进行数据可视化也非常常见。
- en: 'From over a decade of either doing or supporting analytics using PCA, it has
    been my experience that it is widely used but poorly understood, especially among
    people who don''t do the analysis but consume the results. It is intuitive to
    understand that you are creating a new variable from the other correlated variables.
    However, the technique itself is shrouded in potentially misunderstood terminology
    and mathematical concepts that often bewilder the layperson. The intention here
    is to provide a good foundation on what it is and how to use it by covering the
    following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用PCA进行或支持分析超过十年的经验中，我发现它被广泛使用，但理解得却很差，尤其是在那些不进行分析但消费结果的人中。理解你正在从其他相关变量中创建一个新变量是直观的。然而，这项技术本身却笼罩在可能被误解的术语和数学概念中，这些概念往往会使外行人感到困惑。本意在于通过涵盖以下内容，提供一个关于其是什么以及如何使用的良好基础：
- en: Preparing a datset for PCA
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备PCA数据集
- en: Conducting PCA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行PCA
- en: Selecting our principal components
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择我们的主成分
- en: Building a predictive model using principal components
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主成分构建预测模型
- en: Making out of sample predictions using the predictive model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预测模型进行样本外预测
- en: An overview of the principal components
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分概述
- en: PCA is the process of finding the principal components. What exactly are these?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是寻找主成分的过程。这些究竟是什么？
- en: We can consider that a component is a normalized linear combination of the features
    (James, 2012). The first principal component in a dataset is the linear combination
    that captures the maximum variance in the data. A second component is created
    by selecting another linear combination that maximizes the variance with the constraint
    that its direction is perpendicular to the first component. The subsequent components
    (equal to the number of variables) would follow this same rule.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将一个成分视为特征的正则化线性组合（詹姆斯，2012）。数据集中的第一个主成分是捕获数据中最大方差线性组合。第二个成分是通过选择另一个最大化方差且其方向与第一个成分垂直的线性组合来创建的。后续的成分（等于变量的数量）将遵循相同的规则。
- en: A couple of things here. This definition describes the **linear combination**,
    which is one of the key assumptions in PCA. If you ever try and apply PCA to a
    dataset of variables having a low correlation, you will likely end up with a meaningless
    analysis. Another key assumption is that the mean and variance for a variable
    are sufficient statistics. What this tells us is that the data should fit a normal
    distribution so that the covariance matrix fully describes our dataset, that is,
    **multivariate normality**. PCA is fairly robust to non-normally distributed data
    and is even used in conjunction with binary variables, so the results are still
    interpretable.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个要点。这个定义描述了**线性组合**，这是PCA中的一个关键假设。如果您尝试将PCA应用于变量相关性较低的数据集，您很可能会得到一个没有意义的分析。另一个关键假设是变量的均值和方差是足够的统计量。这告诉我们，数据应该符合正态分布，这样协方差矩阵就能完全描述我们的数据集，即**多元正态性**。PCA对非正态分布的数据相当稳健，甚至可以与二元变量一起使用，因此结果仍然可解释。
- en: 'Now, what is this direction described here and how is the linear combination
    determined? The best way to grasp this subject is with a visualization. Let''s
    take a small dataset with two variables and plot it. PCA is sensitive to scale,
    so the data has been scaled with a mean of zero and standard deviation of one.
    You can see in the following figure that this data happens to form the shape of
    an oval with the diamonds representing each observation:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这里描述的这个方向是什么，线性组合是如何确定的？掌握这个主题的最好方式是通过可视化。让我们用一个包含两个变量的小型数据集进行绘图。PCA对尺度敏感，因此数据已经被缩放到均值为零和标准差为一。您可以在下面的图中看到，这些数据恰好形成一个椭圆形，其中的钻石代表每个观测值：
- en: '![](img/image_09_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_09_001.jpg)'
- en: 'Looking at the plot, the data has the most variance along the *x* axis, so
    we can draw a dashed horizontal line to represent our **first principal component**
    as shown in the following image. This component is the linear combination of our
    two variables or *PC1 = α[11]X[1] + α[12]X[2]*, where the coefficient weights
    are the variable loadings on the principal component. They form the basis of the
    direction along which the data varies the most. This equation is constrained by
    *1* in order to prevent the selection of arbitrarily high values. Another way
    to look at this is that the dashed line minimizes the distance between itself
    and the data points. This distance is shown for a couple of points as arrows,
    as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图表，数据在*x*轴上具有最大的方差，因此我们可以画一条虚线来表示我们的**第一个主成分**，如下面的图像所示。这个成分是两个变量的线性组合，或*PC1
    = α[11]X[1] + α[12]X[2]*，其中系数权重是变量在主成分上的载荷。它们构成了数据变化最大的方向的基础。这个方程受*1*的限制，以防止选择任意高的值。另一种看法是，虚线最小化了它自身与数据点之间的距离。这个距离在几个点上用箭头表示，如下所示：
- en: '![](img/image_09_002.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_09_002.jpg)'
- en: 'The **second principal component** is then calculated in the same way, but
    it is uncorrelated with the first, that is, its direction is at a right angle
    or orthogonal to the first principal component. The following plot shows the second
    principal component added as a dotted line:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，以相同的方式计算**第二个主成分**，但它与第一个主成分不相关，也就是说，它的方向与第一个主成分成直角或正交。以下图显示了添加为虚线的第二个主成分：
- en: '![](img/image_09_003.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_09_003.jpg)'
- en: 'With the principal component loadings calculated for each variable, the algorithm
    will then provide us with the principal component scores. The scores are calculated
    for each principal component for each observation. For **PC1** and the first observation,
    this would equate to the formula: *Z[11] = α[11] * (X[11] - average of X[1]) +
    α[12] * (X[12] - average of X[2])*. For **PC2** and the first observation, the
    equation would be *Z[12] = α[21] * (X[11] - average of X[2]) + α[22] * (X[12]
    - average of X[2])*. These principal component scores are now the new feature
    space to be used in whatever analysis you will undertake.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个变量的主成分载荷计算完成后，算法将为我们提供主成分得分。这些得分是针对每个观测值和每个主成分计算的。对于**PC1**和第一个观测值，这相当于以下公式：*Z[11]
    = α[11] * (X[11] - X[1]的平均值) + α[12] * (X[12] - X[2]的平均值)*。对于**PC2**和第一个观测值，方程将是*Z[12]
    = α[21] * (X[11] - X[2]的平均值) + α[22] * (X[12] - X[2]的平均值)*。这些主成分得分现在是新特征空间，用于您将要进行的任何分析。
- en: Recall that the algorithm will create as many principal components as there
    are variables, accounting for 100 percent of the possible variance. So, how do
    we narrow down the components to achieve the original objective in the first place?
    There are some heuristics that one can use, and in the upcoming modeling process,
    we will look at the specifics; but a common method to select a principal component
    is if its **eigenvalue** is greater than one. While the algebra behind the estimation
    of eigenvalues and **eigenvectors** is outside the scope of this book, it is important
    to discuss what they are and how they are used in PCA.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，该算法将创建与变量数量一样多的主成分，解释了100%的可能方差。那么，我们如何缩小这些成分的范围，以实现最初的目标呢？有一些启发式方法可以使用，在即将到来的建模过程中，我们将探讨具体细节；但选择主成分的一个常见方法是如果其**特征值**大于一。虽然特征值和**特征向量**估计背后的代数超出了本书的范围，但讨论它们是什么以及如何在主成分分析（PCA）中使用它们是很重要的。
- en: The optimized linear weights are determined using linear algebra in order to
    create what is referred to as an eigenvector. They are optimal because no other
    possible combination of weights could explain variation better than they do. The
    eigenvalue for a principal component then is the total amount of variation that
    it explains in the entire dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 优化的线性权重是通过线性代数确定的，以便创建所谓的特征向量。它们是最优的，因为没有其他可能的权重组合能比它们更好地解释变化。因此，主成分的特征值就是它在整个数据集中解释的总变化量。
- en: Recall that the equation for the first principal component is *PC1 = α[11]X[1]
    + α[12]X[2]*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，第一个主成分的方程是 *PC1 = α[11]X[1] + α[12]X[2]*。
- en: As the first principal component accounts for the largest amount of variation,
    it will have the largest eigenvalue. The second component will have the second
    highest eigenvalue and so forth. So, an eigenvalue greater than one indicates
    that the principal component accounts for more variance than any of the original
    variables does by itself. If you standardize the sum of all the eigenvalues to
    one, you will have the percentage of the total variance that each component explains.
    This will also aid you in determining a proper cut-off point.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第一个主成分解释了最大的变化量，它将具有最大的特征值。第二个成分将具有第二高的特征值，依此类推。因此，一个大于一的特征值表明主成分解释的方差比任何原始变量单独解释的方差都要多。如果你将所有特征值的总和标准化为1，你将得到每个成分解释的总方差的百分比。这将也有助于你确定一个合适的截止点。
- en: The eigenvalue criterion is certainly not a hard-and-fast rule and must be balanced
    with your knowledge of the data and business problem at hand. Once you have selected
    the number of principal components, you can rotate them in order to simplify their
    interpretation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值标准并不是一个铁的规则，必须与你对数据和业务问题的了解相平衡。一旦你选择了主成分的数量，你就可以旋转它们，以便简化它们的解释。
- en: Rotation
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 旋转
- en: Should you rotate or not? As stated previously, rotation helps in the interpretation
    of the principal components by modifying the loadings of each variable. The overall
    variation explained by the rotated number of components will not change, but the
    contributions to the total variance explained by each component will change. What
    you will find by rotation is that the loading values will either move farther
    or closer to zero, theoretically aiding in identifying those variables that are
    important to each principal component. This is an attempt to associate a variable
    to only one principal component. Remember that this is unsupervised learning,
    so you are trying to understand your data, not test some hypothesis. In short,
    rotation aids you in this endeavor.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 是否应该旋转？如前所述，旋转通过修改每个变量的负载来帮助解释主成分。旋转后的成分所解释的总方差不会改变，但每个成分对总方差贡献的变化会改变。通过旋转，你会发现负载值要么远离零，要么靠近零，从理论上讲，这有助于识别对每个主成分重要的变量。这是尝试将一个变量与一个主成分关联起来的尝试。记住，这是无监督学习，所以你试图理解你的数据，而不是测试某个假设。简而言之，旋转有助于你在这个努力中。
- en: The most common form of principal component rotation is known as **varimax**.
    There are other forms such as **quartimax** and **equimax**, but we will focus
    on varimax rotation. In my experience, I've never seen the other methods provide
    better solutions. Trial and error on your part may be the best way to decide the
    issue.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的主成分旋转形式被称为**方差最大化（varimax）**。还有其他形式，如**四分最大化（quartimax）**和**等最大化（equimax）**，但我们将专注于方差最大化旋转。根据我的经验，我从未见过其他方法提供更好的解决方案。你自己的试错可能是决定这个问题的最佳方式。
- en: With varimax, we are maximizing the sum of the variances of the squared loadings.
    The varimax procedure rotates the axis of the feature space and their coordinates
    without changing the locations of the data points.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方差最大化（varimax），我们最大化了平方负载量的总和。方差最大化过程旋转了特征空间的轴及其坐标，而不改变数据点的位置。
- en: 'Perhaps, the best way to demonstrate this is via another simple illustration.
    Let''s assume that we have a dataset of variables **A** through **G** and we have
    two principal components. Plotting this data, we will end up with the following
    illustration:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 也许，最好的演示方式是通过另一个简单的说明。假设我们有一个变量 **A** 到 **G** 的数据集，并且有两个主成分。绘制这些数据，我们将得到以下示意图：
- en: '![](img/image_09_004.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_09_004.jpg)'
- en: 'For the sake of argument, let''s say that variable A''s loadings are -0.4 on
    **PC1** and 0.1 on **PC2.** Now, let''s say that variable D''s loadings are 0.4
    on PC1 and -0.3 on **PC2**. For point E, the loadings are -0.05 and -0.7, respectively.
    Note that the loadings will follow the direction of the principal component. After
    running a varimax procedure, the rotated components will look as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了辩论的目的，让我们假设变量 A 的负载量在 **PC1** 上为 -0.4，在 **PC2** 上为 0.1。现在，让我们假设变量 D 的负载量在
    PC1 上为 0.4，在 **PC2** 上为 -0.3。对于点 E，负载量分别为 -0.05 和 -0.7。请注意，负载量将遵循主成分的方向。在运行方差最大化过程后，旋转的成分将如下所示：
- en: '![](img/image_09_005.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_09_005.jpg)'
- en: 'The following are the new loadings on **PC1** and **PC2** after rotation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是旋转后的 **PC1** 和 **PC2** 上的新负载量：
- en: 'Variable **A**: -0.5 and 0.02'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量 **A**：-0.5 和 0.02
- en: 'Variable **D**: 0.5 and -0.3'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量 **D**：0.5 和 -0.3
- en: 'Variable **E**: 0.15 and -0.75'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量 **E**：0.15 和 -0.75
- en: The loadings have changed but the data points have not. With this simple illustration,
    we can't say that we have simplified the interpretation, but this should help
    you understand what is happening during the rotation of the principal components.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 负载量已改变，但数据点没有。通过这个简单的说明，我们可以说我们没有简化解释，但这应该有助于你理解主成分旋转过程中发生的事情。
- en: Business understanding
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业理解
- en: For this example, we will delve into the world of sports; in particular, the
    **National Hockey League** (**NHL**). Much work has been done on baseball (think
    of the book and movie, *Moneyball*) and football; both are American and games
    that people around the world play with their feet. For my money, there is no better
    spectator sport than hockey. Perhaps that is an artifact of growing up on the
    frozen prairie of North Dakota. Nonetheless, we can consider this analysis as
    our effort to start a MoneyPuck movement.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将深入体育界；特别是，**国家曲棍球联盟**（**NHL**）。在棒球（想想那本书和电影《点球成金》）和足球方面已经做了很多工作；两者都是美国人玩的用脚踢的球类运动。对我来说，没有比曲棍球更好的观赏性运动了。也许这只是一个在北达科他州的冰冻草原上长大的产物。无论如何，我们可以将这次分析视为我们开始
    MoneyPuck 运动的努力。
- en: In this analysis, we will look at the statistics for 30 NHL teams in a data
    set I've compiled from [www.nhl.com](http://www.nhl.com) and [www.puckalytics.com](http://www.puckalytics.com).
    The goal is to build a model that predicts the total points for a team from an
    input feature space developed using PCA in order to provide us with some insight
    on what it takes to be a top professional team. We will learn a model from the
    2015-16 season, which saw the Pittsburgh Penguins crowned as champions, and then
    test its performance on the current season's results as of February 15, 2017\.
    The files are `nhlTrain.csv` and `nhlTest.csv` on [https://github.com/datameister66/data/](https://github.com/datameister66/data/).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次分析中，我们将查看我从 [www.nhl.com](http://www.nhl.com) 和 [www.puckalytics.com](http://www.puckalytics.com)
    整理的数据集中 30 支 NHL 球队的统计数据。目标是构建一个模型，从使用 PCA 开发的输入特征空间预测球队的积分总和，以便为我们提供一些关于成为顶级职业球队所需条件的见解。我们将从
    2015-16 赛季学习模型，该赛季匹兹堡企鹅队夺冠，然后测试其在截至 2017 年 2 月 15 日的本赛季结果上的性能。文件是 `nhlTrain.csv`
    和 `nhlTest.csv`，位于 [https://github.com/datameister66/data/](https://github.com/datameister66/data/)。
- en: 'NHL standings are based on a points system, so our outcome will be team points
    per game. It is important to understand how the NHL awards points to the teams.
    Unlike football or baseball where only wins and losses count, professional hockey
    uses the following point system for each game:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: NHL 排名基于积分系统，因此我们的结果将是每场比赛的球队积分。了解 NHL 如何向球队颁发积分很重要。与足球或棒球不同，只有胜负计算，职业曲棍球为每场比赛使用以下积分系统：
- en: The winner gets two points whether that is in regulation, overtime, or as a
    result of the post-overtime shootout
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胜者无论是在常规时间、加时赛还是通过加时赛后的点球大战中获胜，都将获得两分
- en: A regulation loser receives no points
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位正规比赛的输家不会得分
- en: An overtime or shootout loser receives one point; the so-called **loser point**
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加时赛或点球大战的输家获得一分；所谓的**输家得分**
- en: The NHL started this point system in 2005 and it is not without controversy,
    but it hasn't detracted from the game's elegant and graceful violence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 纳什维尔曲棍球联盟在2005年开始实施这个得分系统，它并非没有争议，但它并没有损害比赛的优雅和优雅的暴力。
- en: Data understanding and preparation
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据理解和准备
- en: 'To begin with, we will load the necessary packages in order to download the
    data and conduct the analysis. Please ensure that you have these packages installed
    prior to loading:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载必要的包以便下载数据并进行分析。请在加载之前确保已安装这些包：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s also assume you''ve put the two `.csv` files into your working directory,
    so read the training data using the `read.csv()` function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经将两个`.csv`文件放入了你的工作目录，因此使用`read.csv()`函数读取训练数据：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Examine the data using the structure function, `str()`. For brevity, I''ve
    included only the first few lines of the output of the command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结构函数`str()`检查数据。为了简洁，我只包括了命令输出的前几行：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The next thing that we will need to do is look at the variable names.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来需要做的是查看变量名。
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s go over what they mean:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看它们代表什么：
- en: '`Team`: This is the team''s city'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Team`: 这是球队的所在城市'
- en: '`ppg`: The average points per game per the point calculation discussed earlier'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ppg`: 根据之前讨论的点数计算方法，每场比赛的平均得分'
- en: '`Goals_For`: The average goals the team scores per game'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Goals_For`: 每场比赛球队的平均进球数'
- en: '`Goals_Against`: The goals allowed per game'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Goals_Against`: 每场比赛允许的进球数'
- en: '`Shots_For`: Shots on goal per game'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Shots_For`: 每场比赛的射门次数'
- en: '`Shots_Against`: Opponent shots on goal per game'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Shots_Against`: 每场比赛对手的射门次数'
- en: '`PP_perc`: Percent of power play opportunities the team scores a goal'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PP_perc`: 球队得分的机会百分比'
- en: '`PK_perc`: Percent of time the team does not allow a goal when their opponent
    is on the power play'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PK_perc`: 当对手在加时赛中时，球队不允许进球的时间百分比'
- en: '`CF60_pp`: The team''s Corsi Score per 60 minutes of power play time; Corsi
    Score is the sum of shots for (Shots_For), shot attempts that miss the net and
    shots blocked by the opponent'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CF60_pp`: 在加时赛中，球队每60分钟的平均Corsi得分；Corsi得分是射门次数（Shots_For）、射门未命中和被对方挡住的射门次数之和'
- en: '`CA60_sh`: The opponents Corsi Score per 60 minutes of opponent power play
    time i.e. the team is shorthanded'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CA60_sh`: 在对手加时赛中，对手每60分钟的Corsi得分，即球队处于少人状态'
- en: '`OZFOperc_pp`: The percentage of face offs that took place in the offensive
    zone while the team was on the power play'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OZFOperc_pp`: 当球队在加时赛中时，在进攻区发生的争球次数的百分比'
- en: '`Give`: The average number per game that the team gives away the puck'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Give`: 每场比赛球队失去球权的平均次数'
- en: '`Take`: The average number per game that the team gains control of the puck'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Take`: 每场比赛球队获得球权的平均次数'
- en: '`hits`: The average number of the team''s bodychecks per game'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hits`: 每场比赛球队的平均身体冲撞次数'
- en: '`blks`: The average number per game of the team''s blocking an opponent''s
    shot on goal'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`blks`: 每场比赛球队挡住对方射门次数的平均值'
- en: 'We''ll need to have the data standardized with mean 0 and standard deviation
    of 1\. Once we do that we can create and plot the correlations of the input features
    using the `cor.plot()` function available in the psych package:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将数据标准化，使其均值为0，标准差为1。一旦完成，我们就可以使用psych包中的`cor.plot()`函数创建和绘制输入特征的关联图：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following is the output of the preceding command:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面命令的结果：
- en: '![](img/image_plotPCA_01.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_plotPCA_01.png)'
- en: A couple of things are of interest. Notice that `Shots_For` is correlated with
    `Goals_For` and conversely, `Shots_Against` with `Goals_Against`. There also is
    some negative correlation with `PP_perc` and `PK_perc` with `Goals_Against`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个有趣的事情。注意`Shots_For`与`Goals_For`相关，反之亦然，`Shots_Against`与`Goals_Against`相关。还有一些与`PP_perc`和`PK_perc`与`Goals_Against`的负相关。
- en: As such, this should be an adequate dataset to extract several principal components.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这应该是一个足够的数据集来提取几个主成分。
- en: Please note that these are features/variables that I've selected based on my
    interest. There are a bunch of different statistics you can gather on your own
    and see if you can improve the predictive power.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些是我根据我的兴趣选择的特征/变量。你可以收集很多不同的统计数据，看看你是否能提高预测能力。
- en: Modeling and evaluation
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模和评估
- en: 'For the modeling process, we will follow the following steps:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于建模过程，我们将遵循以下步骤：
- en: Extract the components and determine the number to retain.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取成分并确定要保留的数量。
- en: Rotate the retained components.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 旋转保留的成分。
- en: Interpret the rotated solution.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释旋转后的解。
- en: Create the factor scores.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建因子得分。
- en: Use the scores as input variables for regression analysis and evaluate the performance
    on the test data.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将得分作为回归分析的输入变量，并在测试数据上评估性能。
- en: There are many different ways and packages to conduct PCA in R, including what
    seems to be the most commonly used `prcomp()` and `princomp()` functions in base
    R. However, for my money, it seems that the `psych` package is the most flexible
    with the best options.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中进行PCA（主成分分析）有许多不同的方法和包，包括看起来在基础R中最常用的`prcomp()`和`princomp()`函数。然而，对我来说，`psych`包似乎是最灵活且选项最好的。
- en: Component extraction
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成分提取
- en: 'To extract the components with the `psych` package, you will use the `principal()`
    function. The syntax will include the data and whether or not we want to rotate
    the components at this time:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`psych`包提取成分，你将使用`principal()`函数。语法将包括数据和我们是否想要在此时刻旋转成分：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can examine the components by calling the `pca` object that we created.
    However, my primary intent is to determine what should be the number of components
    to retain. For that, a scree plot will suffice. A scree plot can aid you in assessing
    the components that explain the most variance in the data. It shows the `Component`
    number on the *x*-axis and their associated `Eigenvalues` on the *y*-axis:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过调用我们创建的`pca`对象来检查成分。然而，我的主要意图是确定应该保留多少个成分。为此，一个斜率图就足够了。斜率图可以帮助你评估解释数据中最大方差成分。它在*x*轴上显示`成分`编号，在*y*轴上显示它们相关的`特征值`：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following is the output of the preceding command:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在先前的命令输出：
- en: '![](img/image_pca_02.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![PCA图像](img/image_pca_02.png)'
- en: What you are looking for is a point in the scree plot where the rate of change
    decreases. This will be what is commonly called an elbow or bend in the plot.
    That elbow point in the plot captures the fact that additional variance explained
    by a component does not differ greatly from one component to the next. In other
    words, it is the break point where the plot flattens out. In this plot, five components
    look pretty compelling.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你要找的是斜率图中的一个点，其变化率降低。这通常被称为图中的“肘部”或“弯曲”。图中的这个肘部点捕捉到的事实是，一个成分额外解释的方差与下一个成分的方差差异不大。换句话说，它是图变平的断点。在这个图中，五个成分看起来相当有说服力。
- en: Another rule I've learned over the years is that you should capture about 70%
    of the total variance, which means that the cumulative variance explained by each
    of the selected components accounts for 70 percent of the variance explained by
    all the components.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些年我学到的另一条规则是，你应该捕捉到大约70%的总方差，这意味着每个选定的成分所解释的累积方差占所有成分解释的方差的70%。
- en: Orthogonal rotation and interpretation
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正交旋转和解释
- en: As we discussed previously, the point behind rotation is to maximize the loadings
    of the variables on a specific component, which helps in simplifying the interpretation
    by reducing/eliminating the correlation among these components. The method to
    conduct orthogonal rotation is known as `"varimax"`. There are other non-orthogonal
    rotation methods that allow correlation across factors/components. The choice
    of the rotation methodology that you will use in your profession should be based
    on the pertinent literature, which exceeds the scope of this chapter. Feel free
    to experiment with this dataset. I think that when in doubt, the starting point
    for any PCA should be orthogonal rotation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，旋转的目的是最大化变量在特定成分上的负荷，这有助于通过减少/消除这些成分之间的相关性来简化解释。进行正交旋转的方法被称为“`varimax`”。还有其他非正交旋转方法允许因素/成分之间的相关性。你将在职业中使用的旋转方法的选择应基于相关文献，这超出了本章的范围。请随意尝试这个数据集。我认为，在不确定的情况下，任何PCA的起点应该是正交旋转。
- en: 'For this process, we will simply turn back to the `principal()` function, slightly
    changing the syntax to account for 5 components and orthogonal rotation, as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个过程，我们将简单地回到`principal()`函数，稍微改变语法以考虑5个成分和正交旋转，如下所示：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There are two important things to digest here in the output. The first is the
    variable loadings for each of the five components that are labeled `RC1` through
    `RC5`. We see with component one that `Goals_Against` and `Shots_Against` have
    high positive loadings, while `PP_perc` and `PK_perc` have high negative loadings.
    The high loading for component two is `Goals_For`. Component five has high loadings
    with `Shots_For`, `ff`, and `OZFOperc_pp`. Component three seems to be only about
    the variables take while component four is about hits. Next, we will move on to
    the second part for examination: the table starting with the sum of square, `SS
    loadings`. Here, the numbers are the eigenvalues for each component. When they
    are normalized, you will end up with the `Proportion Explained` row, which as
    you may have guessed, stands for the proportion of the variance explained by each
    component. You can see that component one explains 28 percent of all the variance
    explained by the five rotated components. Remember above I mentioned the heuristic
    rule that your selected components should account for a minimum of about 70 of
    the total variation. Well, if you look at the `Cumulative Var` row, you see that
    these five rotated components account for 74% of the total and we can feel confident
    we have the right number to go forward with our modeling.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，有两个重要的事情需要消化。第一个是五个成分（标记为`RC1`至`RC5`）的变量载荷。我们看到成分一中的`Goals_Against`和`Shots_Against`具有高正载荷，而`PP_perc`和`PK_perc`具有高负载荷。成分二的较高载荷是`Goals_For`。成分五具有与`Shots_For`、`ff`和`OZFOperc_pp`的高载荷。成分三似乎只与变量take有关，而成分四与击球有关。接下来，我们将继续进行第二部分的检查：从平方和`SS
    loadings`开始的表格。在这里，数字是每个成分的特征值。当它们被归一化时，你将得到`Proportion Explained`行，正如你可能猜到的，这代表每个成分解释的方差比例。你可以看到，成分一解释了五个旋转成分解释的所有方差的28%。记住，我上面提到的一个启发式规则是，你选择的成分应该解释至少70%的总变异。如果你查看`Cumulative
    Var`行，你会发现这五个旋转成分解释了74%的总变异，我们可以有信心我们有了正确数量的模型来继续前进。
- en: Creating factor scores from the components
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从组成部分创建因子得分
- en: 'We will now need to capture the rotated component loadings as the factor scores
    for each individual team. These scores indicate how each observation (in our case,
    the NHL team) relates to a rotated component. Let''s do this and capture the scores
    in a data frame as we will need to use it for our regression analysis:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要捕获旋转成分的载荷作为每个团队的因子得分。这些得分表明每个观测值（在我们的案例中，是NHL球队）如何与旋转成分相关。让我们这样做，并将得分捕获在数据框中，因为我们将需要它来进行回归分析：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We now have the scores for each component for each team. These are simply the
    variables for each observation multiplied by the loadings on each component and
    then summed. We now can bring in the response (`ppg`) as a column in the data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了每个团队每个成分的得分。这些只是每个观测值（在我们的案例中，是NHL球队）的变量乘以每个成分的载荷，然后求和。现在我们可以将响应（`ppg`）作为一个列添加到数据中。
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this done, we will now move on to the predictive model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们现在将转向预测模型。
- en: Regression analysis
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归分析
- en: To do this part of the process, we will repeat the steps and code from [Chapter
    2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml), *Linear Regression - The Blocking
    and Tackling of Machine Learning*. If you haven't done so, please look at [Chapter
    2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml), *Linear Regression - The Blocking
    and Tackling of Machine Learning* for some insight on how to interpret the following
    output.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个过程的一部分，我们将重复[第2章](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml)中的步骤和代码，*线性回归
    - 机器学习的技巧和策略*。如果你还没有这样做，请查看[第2章](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml)中的*线性回归
    - 机器学习的技巧和策略*，以了解如何解释以下输出。
- en: 'We will use the following `lm()` function to create our linear model with all
    the factors as inputs and then summarize the results:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下`lm()`函数创建我们的线性模型，将所有因子作为输入，然后总结结果：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The good news is that our overall model is highly significant statistically,
    with `p-value` of `1.446e-06` and `Adjusted R-squared` is almost 70 percent. The
    bad news is that three components are not significant. We could simply choose
    to keep them in our model, but let''s see what happens if we exclude them, just
    keeping **RC1** and **RC2**:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我们的整体模型在统计学上高度显著，`p-value`为`1.446e-06`，而`Adjusted R-squared`接近70%。坏消息是，有三个组成部分并不显著。我们可以简单地选择将它们保留在我们的模型中，但让我们看看如果我们排除它们，只保留**RC1**和**RC2**会发生什么：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This model still achieves roughly the same `Adjusted R-squared` value (93.07
    percent) with statistically significant factor coefficients. I will spare you
    the details of running the diagnostic tests. Instead, let''s look at some plots
    in order to examine our analysis better. We can do a scatterplot of the predicted
    and actual values with the base R graphics, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型仍然实现了大致相同的调整后的R平方值（93.07%），并且具有统计上显著的因子系数。我将省略运行诊断测试的细节。相反，让我们看看一些图表，以便更好地检验我们的分析。我们可以使用基本的R图形进行预测值和实际值的散点图，如下所示：
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following is the output of the preceding command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面命令的结果：
- en: '![](img/image_pca_03.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_pca_03.png)'
- en: This confirms that our model does a good job of using two components to predict
    the team's success and also highlights the strong linear relationship between
    the principal components and team points per game. Let's kick it up a notch by
    doing a scatterplot using the `ggplot2` package and include the team names in
    it. The only problem is that it is a very powerful function with many options.
    There are numerous online resources to help you navigate the `ggplot()` maze,
    but this code should help you on your way. Let's first create our baseline plot
    and assign it to an object called `p` then add various plot functionality.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这证实了我们的模型在用两个成分预测团队成功方面做得很好，同时也突出了主成分和每场比赛团队得分之间的强线性关系。让我们通过使用`ggplot2`包进行散点图并包含团队名称来提高一个档次。唯一的问题是这是一个功能非常强大的函数，有很多选项。有大量的在线资源可以帮助你导航`ggplot()`迷宫，但这段代码应该能帮助你入门。让我们首先创建我们的基线图并将其分配给一个名为`p`的对象，然后添加各种绘图功能。
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following is the output of the preceding commands:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面命令的结果：
- en: '![](img/image_pca_04.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_pca_04.png)'
- en: 'The syntax to create `p` is very simple. We just specified the data frame and
    put in `aes()` what we want our `x` and `y` to be along with the variable that
    we want to use as labels. We then just add layers of neat stuff such as data points.
    Add whatever you want to the plot by including `+` in the syntax, as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`p`的语法非常简单。我们只是指定了数据框，并在`aes()`中放入我们想要的`x`和`y`以及我们想要用作标签的变量。然后我们只是添加了数据点等整洁的层。通过在语法中包含`+`，你可以将任何你想要的内容添加到图表中，如下所示：
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We specified how we wanted our `team` labels to appear. It takes quite a bit
    of trial and error to get the font size and position in order:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了`team`标签的显示方式。要调整字体大小和位置，需要大量的尝试和错误：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, specify the *x* and *y* axis limits, otherwise the plot will cut out any
    observations that fall outside them, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，指定*x*轴和*y*轴的极限，否则图表将裁剪掉任何超出这些极限的观测值，如下所示：
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we added a best fit line with no standard error shading:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加了一条没有标准误差阴影的最佳拟合线：
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: I guess one way to think about this plot is that the teams below the line underachieved,
    while those above it overachieved.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我想一种思考这个图表的方法是，位于线下方的团队表现不佳，而位于上方的团队表现超出预期。
- en: 'Another bit of analysis will be to plot the teams in relationship to their
    factor scores, what is referred to as a **biplot**. Once again, `ggplot()` facilitates
    this analysis. Using the preceding code as our guide, let''s update it and see
    what the result is:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项分析是将团队与他们的因子分数进行关系绘图，这被称为**双图**。再次，`ggplot()`简化了这项分析。以之前的代码为指南，让我们更新它并看看结果如何：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 前面命令的输出如下：
- en: '![](img/image_pca_05.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_pca_05.png)'
- en: As you can see, the *x* axis are the team scores on RC1 and the *y* axis are
    the scores on RC2\. Look at the Anaheim ducks with the lowest score on RC1 and
    an average score for RC2\. Now think about the impact of this. With the negative
    loadings on RC1 for the power play and penalty kill, along with the positive loading
    of `Goals_Against`, it would indicate that the team performed well defensively,
    and was effective shorthanded. By the way, Pittsburgh was the eventual winner
    of the Stanley Cup. Their scores are solid, but nothing noteworthy. Keep in mind
    that the team had a horrible start to the season and fired the coach they started
    the season with. It would be interesting to compare how they did on this analysis
    in the first half of the season versus the latter half.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，*x* 轴是 RC1 的团队得分，而 *y* 轴是 RC2 的得分。看看Anaheim ducks，RC1 的得分最低，而 RC2 的平均得分。现在考虑一下这种影响。由于
    RC1 的加时赛和罚球防守的负负荷以及 `Goals_Against` 的正负荷，这表明该队在防守方面表现良好，并且有效利用了少人作战。顺便说一句，匹兹堡队是最终的
    Stanley Cup 赢家。他们的得分很稳定，但没有什么值得注意的。记住，该队在赛季初表现糟糕，解雇了他们赛季初的教练。比较他们在赛季上半场和下半场在这项分析中的表现将很有趣。
- en: 'You can evaluate the model error as well, like we did previously. Let''s look
    at **Root Means Squared Error** (**RMSE**):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像我们之前做的那样评估模型误差。让我们看看**均方根误差**（**RMSE**）：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With that done, we need to see how it performs out of sample. We are going
    to load the test data, predict the team scores on the components, then make our
    predictions based on the linear model. The `predict` function from the psych package
    will automatically scale the test data:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这些之后，我们需要看看它在样本外的表现。我们将加载测试数据，预测组件上的团队得分，然后基于线性模型做出预测。来自 psych 包的 `predict`
    函数将自动缩放测试数据：
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'I think we should plot the results as we did above, showing team names. Let''s
    get this all in a data frame:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们应该像上面那样绘制结果，显示团队名称。让我们把这些都放入一个数据框中：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, utilize the power of `ggplot()`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，利用 `ggplot()` 的力量：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_pca_06.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_pca_06.png)'
- en: I abbreviated the team names in the test data to make it easier to understand.
    Our points per game leader is the Washington Capitals and the worst team is the
    Colorado Avalanche. In fact, when I pulled this data, Colorado had lost five straight
    games. They did break that losing streak as I watched them beat Carolina in overtime.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我在测试数据中简化了团队名称，以便更容易理解。我们的每场比赛得分领先者是华盛顿首都队，而最差的球队是科罗拉多雪崩队。事实上，当我提取这些数据时，科罗拉多已经连续输了五场比赛。他们最终在加时赛中击败了卡罗来纳队，打破了连败。
- en: Finally, let's check the RMSE.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们检查一下均方根误差（RMSE）。
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: That is not bad with an output of sample error of 0.1 versus in sample of 0.08\.
    I think we can declare this a valid model. However, there are still a ton of team
    statistics we could add here to improve predictive power and reduce error. I'll
    keep working on it, and I hope you do as well.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 样本误差为 0.1，而样本内误差为 0.08，这并不坏。我认为我们可以宣布这是一个有效的模型。然而，我们还可以添加大量的团队统计数据来提高预测能力和减少误差。我会继续努力，也希望你们也是如此。
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took a second stab at unsupervised learning techniques by
    exploring PCA, examining what it is, and applying it in a practical fashion. We
    explored how it can be used to reduce the dimensionality and improve the understanding
    of the dataset when confronted with numerous highly correlated variables. Then,
    we applied it to real data from the National Hockey League, using the resulting
    principal components in a regression analysis to predict total team points. Additionally,
    we explored ways to visualize the data and the principal components.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过探索主成分分析（PCA），检查其是什么，并以实际方式应用它，再次尝试无监督学习技术。我们探讨了它如何用于在面临众多高度相关的变量时降低数据集的维度并提高对其的理解。然后，我们将它应用于来自国家曲棍球联盟的真实数据，使用得到的特征成分在回归分析中预测总团队得分。此外，我们还探讨了可视化数据和特征成分的方法。
- en: As an unsupervised learning technique, it requires some judgment along with
    trial and error to arrive at an optimal solution that is acceptable to business
    partners. Nevertheless, it is a powerful tool to extract latent insights and to
    support supervised learning.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种无监督学习技术，它需要一些判断以及试错，以达到一个业务伙伴可以接受的优化解决方案。尽管如此，它是一个强大的工具，可以提取潜在见解并支持监督学习。
- en: We will next look at using unsupervised learning to develop market basket analyses
    and recommendation engines in which PCA can play an important role.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将探讨如何使用无监督学习来开发市场篮子分析和推荐引擎，其中主成分分析（PCA）可以发挥重要作用。
