- en: Principal Components Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Some people skate to the puck. I skate to where the puck is going to be."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Wayne Gretzky'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is the second one where we will focus on unsupervised learning
    techniques. In the previous chapter, we covered cluster analysis, which provides
    us with the groupings of similar observations. In this chapter, we will see how
    to reduce the dimensionality and improve the understanding of our data by grouping
    the correlated variables with **Principal Components Analysis** (**PCA**). Then,
    we will use the principal components in supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: In many datasets, particularly in the social sciences, you will see many variables
    highly correlated with each other. They may additionally suffer from high dimensionality
    or, as it is better known, the **curse of dimensionality**. This is a problem
    because the number of samples needed to estimate a function grows exponentially
    with the number of input features. In such datasets, there may be the case that
    some variables are redundant as they end up measuring the same constructs, for
    example, income and poverty or depression and anxiety. The goal then is to use
    PCA in order to create a smaller set of variables that capture most of the information
    from the original set of variables, thus simplifying the dataset and often leading
    to hidden insights. These new variables (principal components) are highly uncorrelated
    with each other. In addition to supervised learning, it is also very common to
    use these components to perform data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'From over a decade of either doing or supporting analytics using PCA, it has
    been my experience that it is widely used but poorly understood, especially among
    people who don''t do the analysis but consume the results. It is intuitive to
    understand that you are creating a new variable from the other correlated variables.
    However, the technique itself is shrouded in potentially misunderstood terminology
    and mathematical concepts that often bewilder the layperson. The intention here
    is to provide a good foundation on what it is and how to use it by covering the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a datset for PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting our principal components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a predictive model using principal components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making out of sample predictions using the predictive model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of the principal components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA is the process of finding the principal components. What exactly are these?
  prefs: []
  type: TYPE_NORMAL
- en: We can consider that a component is a normalized linear combination of the features
    (James, 2012). The first principal component in a dataset is the linear combination
    that captures the maximum variance in the data. A second component is created
    by selecting another linear combination that maximizes the variance with the constraint
    that its direction is perpendicular to the first component. The subsequent components
    (equal to the number of variables) would follow this same rule.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of things here. This definition describes the **linear combination**,
    which is one of the key assumptions in PCA. If you ever try and apply PCA to a
    dataset of variables having a low correlation, you will likely end up with a meaningless
    analysis. Another key assumption is that the mean and variance for a variable
    are sufficient statistics. What this tells us is that the data should fit a normal
    distribution so that the covariance matrix fully describes our dataset, that is,
    **multivariate normality**. PCA is fairly robust to non-normally distributed data
    and is even used in conjunction with binary variables, so the results are still
    interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what is this direction described here and how is the linear combination
    determined? The best way to grasp this subject is with a visualization. Let''s
    take a small dataset with two variables and plot it. PCA is sensitive to scale,
    so the data has been scaled with a mean of zero and standard deviation of one.
    You can see in the following figure that this data happens to form the shape of
    an oval with the diamonds representing each observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at the plot, the data has the most variance along the *x* axis, so
    we can draw a dashed horizontal line to represent our **first principal component**
    as shown in the following image. This component is the linear combination of our
    two variables or *PC1 = α[11]X[1] + α[12]X[2]*, where the coefficient weights
    are the variable loadings on the principal component. They form the basis of the
    direction along which the data varies the most. This equation is constrained by
    *1* in order to prevent the selection of arbitrarily high values. Another way
    to look at this is that the dashed line minimizes the distance between itself
    and the data points. This distance is shown for a couple of points as arrows,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **second principal component** is then calculated in the same way, but
    it is uncorrelated with the first, that is, its direction is at a right angle
    or orthogonal to the first principal component. The following plot shows the second
    principal component added as a dotted line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With the principal component loadings calculated for each variable, the algorithm
    will then provide us with the principal component scores. The scores are calculated
    for each principal component for each observation. For **PC1** and the first observation,
    this would equate to the formula: *Z[11] = α[11] * (X[11] - average of X[1]) +
    α[12] * (X[12] - average of X[2])*. For **PC2** and the first observation, the
    equation would be *Z[12] = α[21] * (X[11] - average of X[2]) + α[22] * (X[12]
    - average of X[2])*. These principal component scores are now the new feature
    space to be used in whatever analysis you will undertake.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the algorithm will create as many principal components as there
    are variables, accounting for 100 percent of the possible variance. So, how do
    we narrow down the components to achieve the original objective in the first place?
    There are some heuristics that one can use, and in the upcoming modeling process,
    we will look at the specifics; but a common method to select a principal component
    is if its **eigenvalue** is greater than one. While the algebra behind the estimation
    of eigenvalues and **eigenvectors** is outside the scope of this book, it is important
    to discuss what they are and how they are used in PCA.
  prefs: []
  type: TYPE_NORMAL
- en: The optimized linear weights are determined using linear algebra in order to
    create what is referred to as an eigenvector. They are optimal because no other
    possible combination of weights could explain variation better than they do. The
    eigenvalue for a principal component then is the total amount of variation that
    it explains in the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the equation for the first principal component is *PC1 = α[11]X[1]
    + α[12]X[2]*.
  prefs: []
  type: TYPE_NORMAL
- en: As the first principal component accounts for the largest amount of variation,
    it will have the largest eigenvalue. The second component will have the second
    highest eigenvalue and so forth. So, an eigenvalue greater than one indicates
    that the principal component accounts for more variance than any of the original
    variables does by itself. If you standardize the sum of all the eigenvalues to
    one, you will have the percentage of the total variance that each component explains.
    This will also aid you in determining a proper cut-off point.
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalue criterion is certainly not a hard-and-fast rule and must be balanced
    with your knowledge of the data and business problem at hand. Once you have selected
    the number of principal components, you can rotate them in order to simplify their
    interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Rotation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Should you rotate or not? As stated previously, rotation helps in the interpretation
    of the principal components by modifying the loadings of each variable. The overall
    variation explained by the rotated number of components will not change, but the
    contributions to the total variance explained by each component will change. What
    you will find by rotation is that the loading values will either move farther
    or closer to zero, theoretically aiding in identifying those variables that are
    important to each principal component. This is an attempt to associate a variable
    to only one principal component. Remember that this is unsupervised learning,
    so you are trying to understand your data, not test some hypothesis. In short,
    rotation aids you in this endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: The most common form of principal component rotation is known as **varimax**.
    There are other forms such as **quartimax** and **equimax**, but we will focus
    on varimax rotation. In my experience, I've never seen the other methods provide
    better solutions. Trial and error on your part may be the best way to decide the
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: With varimax, we are maximizing the sum of the variances of the squared loadings.
    The varimax procedure rotates the axis of the feature space and their coordinates
    without changing the locations of the data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps, the best way to demonstrate this is via another simple illustration.
    Let''s assume that we have a dataset of variables **A** through **G** and we have
    two principal components. Plotting this data, we will end up with the following
    illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the sake of argument, let''s say that variable A''s loadings are -0.4 on
    **PC1** and 0.1 on **PC2.** Now, let''s say that variable D''s loadings are 0.4
    on PC1 and -0.3 on **PC2**. For point E, the loadings are -0.05 and -0.7, respectively.
    Note that the loadings will follow the direction of the principal component. After
    running a varimax procedure, the rotated components will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the new loadings on **PC1** and **PC2** after rotation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Variable **A**: -0.5 and 0.02'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Variable **D**: 0.5 and -0.3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Variable **E**: 0.15 and -0.75'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loadings have changed but the data points have not. With this simple illustration,
    we can't say that we have simplified the interpretation, but this should help
    you understand what is happening during the rotation of the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this example, we will delve into the world of sports; in particular, the
    **National Hockey League** (**NHL**). Much work has been done on baseball (think
    of the book and movie, *Moneyball*) and football; both are American and games
    that people around the world play with their feet. For my money, there is no better
    spectator sport than hockey. Perhaps that is an artifact of growing up on the
    frozen prairie of North Dakota. Nonetheless, we can consider this analysis as
    our effort to start a MoneyPuck movement.
  prefs: []
  type: TYPE_NORMAL
- en: In this analysis, we will look at the statistics for 30 NHL teams in a data
    set I've compiled from [www.nhl.com](http://www.nhl.com) and [www.puckalytics.com](http://www.puckalytics.com).
    The goal is to build a model that predicts the total points for a team from an
    input feature space developed using PCA in order to provide us with some insight
    on what it takes to be a top professional team. We will learn a model from the
    2015-16 season, which saw the Pittsburgh Penguins crowned as champions, and then
    test its performance on the current season's results as of February 15, 2017\.
    The files are `nhlTrain.csv` and `nhlTest.csv` on [https://github.com/datameister66/data/](https://github.com/datameister66/data/).
  prefs: []
  type: TYPE_NORMAL
- en: 'NHL standings are based on a points system, so our outcome will be team points
    per game. It is important to understand how the NHL awards points to the teams.
    Unlike football or baseball where only wins and losses count, professional hockey
    uses the following point system for each game:'
  prefs: []
  type: TYPE_NORMAL
- en: The winner gets two points whether that is in regulation, overtime, or as a
    result of the post-overtime shootout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A regulation loser receives no points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overtime or shootout loser receives one point; the so-called **loser point**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NHL started this point system in 2005 and it is not without controversy,
    but it hasn't detracted from the game's elegant and graceful violence.
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin with, we will load the necessary packages in order to download the
    data and conduct the analysis. Please ensure that you have these packages installed
    prior to loading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also assume you''ve put the two `.csv` files into your working directory,
    so read the training data using the `read.csv()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Examine the data using the structure function, `str()`. For brevity, I''ve
    included only the first few lines of the output of the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The next thing that we will need to do is look at the variable names.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go over what they mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Team`: This is the team''s city'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ppg`: The average points per game per the point calculation discussed earlier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Goals_For`: The average goals the team scores per game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Goals_Against`: The goals allowed per game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Shots_For`: Shots on goal per game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Shots_Against`: Opponent shots on goal per game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PP_perc`: Percent of power play opportunities the team scores a goal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PK_perc`: Percent of time the team does not allow a goal when their opponent
    is on the power play'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CF60_pp`: The team''s Corsi Score per 60 minutes of power play time; Corsi
    Score is the sum of shots for (Shots_For), shot attempts that miss the net and
    shots blocked by the opponent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CA60_sh`: The opponents Corsi Score per 60 minutes of opponent power play
    time i.e. the team is shorthanded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OZFOperc_pp`: The percentage of face offs that took place in the offensive
    zone while the team was on the power play'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Give`: The average number per game that the team gives away the puck'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Take`: The average number per game that the team gains control of the puck'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hits`: The average number of the team''s bodychecks per game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`blks`: The average number per game of the team''s blocking an opponent''s
    shot on goal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll need to have the data standardized with mean 0 and standard deviation
    of 1\. Once we do that we can create and plot the correlations of the input features
    using the `cor.plot()` function available in the psych package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_plotPCA_01.png)'
  prefs: []
  type: TYPE_IMG
- en: A couple of things are of interest. Notice that `Shots_For` is correlated with
    `Goals_For` and conversely, `Shots_Against` with `Goals_Against`. There also is
    some negative correlation with `PP_perc` and `PK_perc` with `Goals_Against`.
  prefs: []
  type: TYPE_NORMAL
- en: As such, this should be an adequate dataset to extract several principal components.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that these are features/variables that I've selected based on my
    interest. There are a bunch of different statistics you can gather on your own
    and see if you can improve the predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the modeling process, we will follow the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the components and determine the number to retain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rotate the retained components.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpret the rotated solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the factor scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the scores as input variables for regression analysis and evaluate the performance
    on the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are many different ways and packages to conduct PCA in R, including what
    seems to be the most commonly used `prcomp()` and `princomp()` functions in base
    R. However, for my money, it seems that the `psych` package is the most flexible
    with the best options.
  prefs: []
  type: TYPE_NORMAL
- en: Component extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To extract the components with the `psych` package, you will use the `principal()`
    function. The syntax will include the data and whether or not we want to rotate
    the components at this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can examine the components by calling the `pca` object that we created.
    However, my primary intent is to determine what should be the number of components
    to retain. For that, a scree plot will suffice. A scree plot can aid you in assessing
    the components that explain the most variance in the data. It shows the `Component`
    number on the *x*-axis and their associated `Eigenvalues` on the *y*-axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_pca_02.png)'
  prefs: []
  type: TYPE_IMG
- en: What you are looking for is a point in the scree plot where the rate of change
    decreases. This will be what is commonly called an elbow or bend in the plot.
    That elbow point in the plot captures the fact that additional variance explained
    by a component does not differ greatly from one component to the next. In other
    words, it is the break point where the plot flattens out. In this plot, five components
    look pretty compelling.
  prefs: []
  type: TYPE_NORMAL
- en: Another rule I've learned over the years is that you should capture about 70%
    of the total variance, which means that the cumulative variance explained by each
    of the selected components accounts for 70 percent of the variance explained by
    all the components.
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonal rotation and interpretation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed previously, the point behind rotation is to maximize the loadings
    of the variables on a specific component, which helps in simplifying the interpretation
    by reducing/eliminating the correlation among these components. The method to
    conduct orthogonal rotation is known as `"varimax"`. There are other non-orthogonal
    rotation methods that allow correlation across factors/components. The choice
    of the rotation methodology that you will use in your profession should be based
    on the pertinent literature, which exceeds the scope of this chapter. Feel free
    to experiment with this dataset. I think that when in doubt, the starting point
    for any PCA should be orthogonal rotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this process, we will simply turn back to the `principal()` function, slightly
    changing the syntax to account for 5 components and orthogonal rotation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two important things to digest here in the output. The first is the
    variable loadings for each of the five components that are labeled `RC1` through
    `RC5`. We see with component one that `Goals_Against` and `Shots_Against` have
    high positive loadings, while `PP_perc` and `PK_perc` have high negative loadings.
    The high loading for component two is `Goals_For`. Component five has high loadings
    with `Shots_For`, `ff`, and `OZFOperc_pp`. Component three seems to be only about
    the variables take while component four is about hits. Next, we will move on to
    the second part for examination: the table starting with the sum of square, `SS
    loadings`. Here, the numbers are the eigenvalues for each component. When they
    are normalized, you will end up with the `Proportion Explained` row, which as
    you may have guessed, stands for the proportion of the variance explained by each
    component. You can see that component one explains 28 percent of all the variance
    explained by the five rotated components. Remember above I mentioned the heuristic
    rule that your selected components should account for a minimum of about 70 of
    the total variation. Well, if you look at the `Cumulative Var` row, you see that
    these five rotated components account for 74% of the total and we can feel confident
    we have the right number to go forward with our modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating factor scores from the components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now need to capture the rotated component loadings as the factor scores
    for each individual team. These scores indicate how each observation (in our case,
    the NHL team) relates to a rotated component. Let''s do this and capture the scores
    in a data frame as we will need to use it for our regression analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We now have the scores for each component for each team. These are simply the
    variables for each observation multiplied by the loadings on each component and
    then summed. We now can bring in the response (`ppg`) as a column in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With this done, we will now move on to the predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To do this part of the process, we will repeat the steps and code from [Chapter
    2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml), *Linear Regression - The Blocking
    and Tackling of Machine Learning*. If you haven't done so, please look at [Chapter
    2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml), *Linear Regression - The Blocking
    and Tackling of Machine Learning* for some insight on how to interpret the following
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following `lm()` function to create our linear model with all
    the factors as inputs and then summarize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The good news is that our overall model is highly significant statistically,
    with `p-value` of `1.446e-06` and `Adjusted R-squared` is almost 70 percent. The
    bad news is that three components are not significant. We could simply choose
    to keep them in our model, but let''s see what happens if we exclude them, just
    keeping **RC1** and **RC2**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This model still achieves roughly the same `Adjusted R-squared` value (93.07
    percent) with statistically significant factor coefficients. I will spare you
    the details of running the diagnostic tests. Instead, let''s look at some plots
    in order to examine our analysis better. We can do a scatterplot of the predicted
    and actual values with the base R graphics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_pca_03.png)'
  prefs: []
  type: TYPE_IMG
- en: This confirms that our model does a good job of using two components to predict
    the team's success and also highlights the strong linear relationship between
    the principal components and team points per game. Let's kick it up a notch by
    doing a scatterplot using the `ggplot2` package and include the team names in
    it. The only problem is that it is a very powerful function with many options.
    There are numerous online resources to help you navigate the `ggplot()` maze,
    but this code should help you on your way. Let's first create our baseline plot
    and assign it to an object called `p` then add various plot functionality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_pca_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The syntax to create `p` is very simple. We just specified the data frame and
    put in `aes()` what we want our `x` and `y` to be along with the variable that
    we want to use as labels. We then just add layers of neat stuff such as data points.
    Add whatever you want to the plot by including `+` in the syntax, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We specified how we wanted our `team` labels to appear. It takes quite a bit
    of trial and error to get the font size and position in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, specify the *x* and *y* axis limits, otherwise the plot will cut out any
    observations that fall outside them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we added a best fit line with no standard error shading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: I guess one way to think about this plot is that the teams below the line underachieved,
    while those above it overachieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another bit of analysis will be to plot the teams in relationship to their
    factor scores, what is referred to as a **biplot**. Once again, `ggplot()` facilitates
    this analysis. Using the preceding code as our guide, let''s update it and see
    what the result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_pca_05.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the *x* axis are the team scores on RC1 and the *y* axis are
    the scores on RC2\. Look at the Anaheim ducks with the lowest score on RC1 and
    an average score for RC2\. Now think about the impact of this. With the negative
    loadings on RC1 for the power play and penalty kill, along with the positive loading
    of `Goals_Against`, it would indicate that the team performed well defensively,
    and was effective shorthanded. By the way, Pittsburgh was the eventual winner
    of the Stanley Cup. Their scores are solid, but nothing noteworthy. Keep in mind
    that the team had a horrible start to the season and fired the coach they started
    the season with. It would be interesting to compare how they did on this analysis
    in the first half of the season versus the latter half.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can evaluate the model error as well, like we did previously. Let''s look
    at **Root Means Squared Error** (**RMSE**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With that done, we need to see how it performs out of sample. We are going
    to load the test data, predict the team scores on the components, then make our
    predictions based on the linear model. The `predict` function from the psych package
    will automatically scale the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'I think we should plot the results as we did above, showing team names. Let''s
    get this all in a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, utilize the power of `ggplot()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_pca_06.png)'
  prefs: []
  type: TYPE_IMG
- en: I abbreviated the team names in the test data to make it easier to understand.
    Our points per game leader is the Washington Capitals and the worst team is the
    Colorado Avalanche. In fact, when I pulled this data, Colorado had lost five straight
    games. They did break that losing streak as I watched them beat Carolina in overtime.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let's check the RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: That is not bad with an output of sample error of 0.1 versus in sample of 0.08\.
    I think we can declare this a valid model. However, there are still a ton of team
    statistics we could add here to improve predictive power and reduce error. I'll
    keep working on it, and I hope you do as well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a second stab at unsupervised learning techniques by
    exploring PCA, examining what it is, and applying it in a practical fashion. We
    explored how it can be used to reduce the dimensionality and improve the understanding
    of the dataset when confronted with numerous highly correlated variables. Then,
    we applied it to real data from the National Hockey League, using the resulting
    principal components in a regression analysis to predict total team points. Additionally,
    we explored ways to visualize the data and the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: As an unsupervised learning technique, it requires some judgment along with
    trial and error to arrive at an optimal solution that is acceptable to business
    partners. Nevertheless, it is a powerful tool to extract latent insights and to
    support supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: We will next look at using unsupervised learning to develop market basket analyses
    and recommendation engines in which PCA can play an important role.
  prefs: []
  type: TYPE_NORMAL
