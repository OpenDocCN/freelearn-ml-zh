<html><head></head><body>
		<div id="_idContainer131">
			<h1 id="_idParaDest-194"><em class="italic"><a id="_idTextAnchor201"/>Chapter 10</em>: Feature Transformation and Scaling</h1>
			<p>In the previous chapter, you have seen how to manage drift and drift detection in streaming and online machine learning models. Drift detection, although not the main concept in machine learning, is a very important accessory aspect of machine learning in production.</p>
			<p>Although many secondary topics are important in machine learning, some of the accessory topics are especially important with online models. Drift detection is particularly important, as the model's autonomy in relearning makes it slightly more black-box to the developer or data scientist. This has great advantages only as long as the retraining process is correctly managed by drift detection and comparable methods.</p>
			<p>In this chapter, you will see another secondary machine learning topic that has important implications for online machine learning and streaming. Feature transformation and scaling are practices that are relatively well defined in traditional, batch machine learning. They do not generally pose any theoretical difficulty.</p>
			<p>In online machine learning, scaling and feature transformation is not as straightforward. It is necessary to adapt the practice to the possibility that new data is not exactly comparable to the original data. This causes questions as to whether or not to refit feature transformations and scalers on every new piece of data arriving, but also on whether such practices will introduce bias into your already trained and continuously re-training models.</p>
			<p>The topics that are covered in this chapter are as follows:</p>
			<ul>
				<li>Challenges of data preparation with streaming data</li>
				<li>Scaling data for streaming</li>
				<li>Transforming features in a streaming context</li>
			</ul>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor202"/>Technical requirements</h1>
			<p>You can find all the code for this book on GitHub at the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python">https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python</a>. If you are not yet familiar with Git and GitHub, the easiest way to download the notebooks and code samples is the following:</p>
			<ol>
				<li>Go to the link of the repository.</li>
				<li>Go to the green <strong class="bold">Code</strong> button.</li>
				<li>Select <strong class="bold">Download ZIP</strong>.</li>
			</ol>
			<p>When you download the ZIP file, unzip it in your local environment, and you will be able to access the code through your preferred Python editor.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor203"/>Python environment</h2>
			<p>To follow<a id="_idIndexMarker549"/> along with this book, you can download the code in the repository and execute it using your preferred Python editor.</p>
			<p>If you are not yet familiar <a id="_idIndexMarker550"/>with Python environments, I would advise you to check out Anaconda (<a href="https://www.anaconda.com/products/individual">https://www.anaconda.com/products/individual</a>), which comes with Jupyter Notebook and JupyterLabs, which are both great for executing notebooks. It also comes with Spyder and VSCode for editing scripts and programs.</p>
			<p>If you have difficulty installing Python or the associated programs on your machine, you can check out<a id="_idIndexMarker551"/> Google Colab (<a href="https://colab.research.google.com/">https://colab.research.google.com/</a>) or <a id="_idIndexMarker552"/>Kaggle Notebooks (<a href="https://www.kaggle.com/code">https://www.kaggle.com/code</a>), which both allow you to run Python code in online notebooks for free, without any setup required.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code in the book will generally use Colab and Kaggle Notebooks with Python version <em class="italic">3.7.13</em>, and you can set up your own environment to mimic this. </p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor204"/>Challenges of data preparation with streaming data</h1>
			<p>Before deep-diving into specific algorithms and solutions, let's first have a general discussion of why data preparation may be different when working with data that arrives in a streaming fashion. Multiple reasons can be identified, such as the following:</p>
			<ul>
				<li>The first, obvious issue is data drift. As discussed in much detail in the previous chapter, trends <a id="_idIndexMarker553"/>and descriptive statistics of your data can slowly change over time due to data drift. If your feature engineering or data preparation processes are too dependent on your data following certain distributions, you may run into problems when data drift occurs. As many solutions for this have been proposed in the previous chapter, this topic will be left out of consideration in the current chapter.</li>
				<li>The second issue is that population parameters are unknown. When observing data in a streaming fashion, it is possible, and even likely, that your estimates of the population parameters are slowly going to improve over time. As seen in <a href="B18335_03_ePub.xhtml#_idTextAnchor051"><em class="italic">Chapter 3</em></a>, precision in your estimates of descriptive statistics will improve with the amount of data you have. When the descriptive statistic estimates are improving, the fact that they are changing over time does not make it easy to fix your formulas for data preparation, feature engineering, scaling, and the like:<ul><li>As the first example of this, consider the range. The range represents the minimum and maximum values of the data that you observe. This is used extensively in data scaling and also in other algorithms. Now, imagine that the range (minimum and maximum values) in a batch can be different from the global range (global minimum and global maximum) of the data. After all, when new data arrives, you may observe a value that is higher or lower than anything observed in your historical data, just by the process of random sampling. Observing an observation that is higher than your maximum may cause an issue in scaling if you do not treat it right.</li><li>Another example of this is when scaling with a normal distribution. The standard deviation and average in your batch may be different from the population standard deviation and population average. This may cause your scaler to behave differently after some time, which is a sort of data drift that is induced by your own scaling algorithm. Clearly, this must be avoided.</li></ul></li>
				<li>Many <a id="_idIndexMarker554"/>other cases of such problems exist, including observing new categories in a categorical value, which will lead to problems with your one-hot encoder or your models that use categorical variables. You can also imagine that occurring new types of values in your data such as NAs and InFs need to be managed well, rather than having them cause bugs. This is true in general, but when working with streaming, this tends to cause even more trouble than with regular data. </li>
			</ul>
			<p>In the next section, we will learn what scaling is and how to work with it.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor205"/>Scaling data for streaming</h1>
			<p>In the first <a id="_idIndexMarker555"/>part of this section, let's start by looking at some solutions for streaming scaling data. Before going into the solutions, let's do a quick recap of what scaling is and how it works.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor206"/>Introducing scaling</h2>
			<p>Numerical variables can be of any scale, meaning they can have very high average values or low average values, for example. Some machine learning algorithms are not at all impacted by the scale of a variable, whereas other machine learning algorithms can be strongly impacted.</p>
			<p>Scaling is <a id="_idIndexMarker556"/>the practice of taking a numerical variable and reducing its range, and potentially its standard deviation, to a pre-specified range. This will allow all machine learning algorithms to learn from the data without problems.</p>
			<h3>Scaling with MinMaxScaler</h3>
			<p>To achieve<a id="_idIndexMarker557"/> this goal, a<a id="_idIndexMarker558"/> commonly <a id="_idIndexMarker559"/>used method is the Min-Max scaler. The Min-Max scaler will take an input variable in any range and reduce all of the values to fall in between the range (<strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>), meaning that the minimum value of the scaled variable will be <strong class="source-inline">0</strong> and the maximum of the scaled variable will be <strong class="source-inline">1</strong>. Sometimes, an alternative is used in which the minimum is not <strong class="source-inline">0</strong>, but <strong class="source-inline">-1</strong>.</p>
			<p>The mathematical <a id="_idIndexMarker560"/>formula for Min-Max scaling is the following:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/Formula_10_001.jpg" alt=""/>
				</div>
			</div>
			<h3>Scaling with StandardScaler</h3>
			<p>Another <a id="_idIndexMarker561"/>very common<a id="_idIndexMarker562"/> approach to <a id="_idIndexMarker563"/>scaling is standardizing. Standardizing is a method strongly based on statistics, which allows you to take any variable <a id="_idIndexMarker564"/>and take it back to a standard normal distribution. The standard normal distribution has an average of <strong class="source-inline">0</strong> and a standard deviation of <strong class="source-inline">1</strong>. </p>
			<p>The mathematical formula for the StandardScaler is the following:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/Formula_10_002.jpg" alt=""/>
				</div>
			</div>
			<p>The values of the scaled variable will not be in any specific range; the new value of the scaled variable represents the number of standard deviations that the original value is away from the original mean. A very extreme value (imagine four or five standard deviations away from the mean) would have a value of four or five, which by the way can be both positive and negative.</p>
			<h3>Choosing your scaling method</h3>
			<p>The choice of <a id="_idIndexMarker565"/>scaling algorithm depends on the use case, and it is generally a good idea to do tuning of your machine learning pipeline in which different scaling methods are used with different algorithms. After all, the choice of scaling method has an impact on the performance of the training of the method.</p>
			<p>The Min-Max scaler is known to have difficulty with outliers. After all, a very extreme outlier would be set to the maximum value, that is, to <strong class="source-inline">1</strong>. Then, this may cause the other values to be reduced to a much smaller range.</p>
			<p>The StandardScaler deals with this in a better way, as the outliers would still be outliers and simply take on high values in the scaled variable. This can be a disadvantage at the same time, mainly when <a id="_idIndexMarker566"/>you are using machine learning algorithms that need the values to be between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor207"/>Adapting scaling to a streaming context</h2>
			<p>Let's now <a id="_idIndexMarker567"/>have a look at how we can adapt each of those approaches to the case of streaming data. We'll start with the Min-Max scaler.</p>
			<h3>Adapting the MinMaxScaler to streaming</h3>
			<p>The<a id="_idIndexMarker568"/> MinMaxScaler works perfectly on a fixed dataset. It guarantees that the values of the scaled data will be between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, just as required by some machine learning algorithms. However, in the case of streaming data, this is much less easy to manage.</p>
			<p>When new data arrives one by one (in a stream), it is impossible to decide on the minimum or maximum value. After all, you cannot expect one value to be both minimum and maximum. The same problem occurs when batching: there is no guarantee that the batch maximum is higher than the global maximum, and the same for the minimum.</p>
			<p>You could use the training data to decide on the minimum and the maximum, but then the problem is that your new data could be above the training maximum or below the training minimum. This would result in the scaled values being outside of the range (<strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>).</p>
			<p>A solution for this is to use a running minimum and a running maximum. This means that you continue updating the MinMaxScaler so that every time a lower minimum is observed, you update the minimum in the MinMaxScaler formula, and every time a higher maximum is observed, you update the maximum.</p>
			<p>The advantage of this method is that it guarantees that your scaled data will always be between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. A disadvantage is that the first values for training the MinMaxScaler will be scaled quite badly. This is easily solved by using some training data to initialize the MinMaxScaler. Outliers can also be a problem, as having one very extreme value will strongly affect the MinMaxScaler's formula, and scores will be very different after that. This could be solved by using an outlier detection method as described extensively in <a href="B18335_05_ePub.xhtml#_idTextAnchor097"><em class="italic">Chapter 5</em></a>.</p>
			<p>Let's now <a id="_idIndexMarker569"/>move on to a Python implementation of an adaptive MinMaxScaler: </p>
			<ol>
				<li value="1">For this, we will use the implementation of the MinMaxScaler in the Python library, <strong class="source-inline">River</strong>. We will use the following data for this example:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-1</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">data = np.random.randint(0, 100, size=1000)</p>
			<ol>
				<li value="2">The histogram of this data can be created using the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-2</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">plt.hist(data)</p>
			<p>The resulting histogram looks like the following:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B18335_10_1.jpg" alt="Figure 10.1 – Resulting histogram of Code Block 10-2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Resulting histogram of Code Block 10-2</p>
			<ol>
				<li value="3">Now, to scale this data, let's use the <strong class="source-inline">MinMaxScaler</strong> function from River. Looping<a id="_idIndexMarker570"/> through the data will simulate the data arriving in a streaming fashion, and the use of the <strong class="source-inline">learn_one</strong> method shows that the data is updated step by step:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-3</p>
			<p class="source-code">!pip install river</p>
			<p class="source-code">from river import preprocessing</p>
			<p class="source-code"># convert the data to required format</p>
			<p class="source-code">data_stream = [{'x':float(x)} for x in list(data)]</p>
			<p class="source-code"># initialize list for scaled values</p>
			<p class="source-code">data_scaled = []</p>
			<p class="source-code"># initialize scaler</p>
			<p class="source-code">my_scaler = preprocessing.MinMaxScaler()</p>
			<p class="source-code"># streaming</p>
			<p class="source-code">for observation in data_stream:</p>
			<p class="source-code">  # learn (update)</p>
			<p class="source-code">  my_scaler.learn_one(observation)</p>
			<p class="source-code">  # scale the observation</p>
			<p class="source-code">  scaled_obs = my_scaler.transform_one(observation)</p>
			<p class="source-code">  </p>
			<p class="source-code">  # store the scaled result</p>
			<p class="source-code">  data_scaled.append(scaled_obs['x'])</p>
			<ol>
				<li value="4">Now, it will <a id="_idIndexMarker571"/>be interesting to see the histogram of the scaled data. It can be created as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-4</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">plt.hist(data_scaled)</p>
			<p>The histogram is shown in the following:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B18335_10_2.jpg" alt="Figure 10.2 – Resulting histogram of Code Block 10-4&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Resulting histogram of Code Block 10-4</p>
			<p>This histogram <a id="_idIndexMarker572"/>clearly shows that we have been successful in scaling the data into the <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong> range. </p>
			<p>Now that you have seen the theory and implementation of the MinMaxScaler, let's now see the StandardScaler, a common alternative to this method.</p>
			<h3>Adapting the Standard Scaler to streaming</h3>
			<p>The<a id="_idIndexMarker573"/> problem that may occur in standard scaling when observing more extreme data in the future, is not exactly the same problem as the one that is seen in Min-Max scaling. Where the Min-Max scaler uses the minimum and the maximum to compute the scaling method, the standard scaler uses the mean and standard deviation.</p>
			<p>The reason why this is very different is that the minimum and maximum are relatively likely to be surpassed at one point in time. This would result in the scaled values being higher than <strong class="source-inline">1</strong> or lower than <strong class="source-inline">0</strong>, which may pose real problems for your machine learning algorithms.</p>
			<p>In the standard scaler, any extreme values occurring in the future will impact your estimate of the global mean and standard deviation, but they are much less likely to impact them very severely. After all, the mean and the standard deviation are much less sensitive to the observation of a small number of extreme values.</p>
			<p>Given this theoretical consideration, you may conclude that it isn't really necessary to update the standard scaler. However, it may be best to update it anyway, as this is a good way to keep your machine learning methods up to date. The added value of this will be less impacting than when using the Min-Max scaler, but it is a best practice to do it anyway.</p>
			<p>One<a id="_idIndexMarker574"/> solution that you can use is to use the AdaptiveStandardScaler in the <strong class="source-inline">Riverml</strong> package. It uses an exponentially-weighted running mean and variance to make sure that slight drifts of the normal distribution of your data are taken into account without having it weigh too strongly. Let's see a Python example of how to use the AdaptiveStandardScaler:</p>
			<ol>
				<li value="1">We will use the following data for this example:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-5</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">data = np.random.normal(12, 15, size=1000)</p>
			<ol>
				<li value="2">This data follows a normal distribution, as you can see from the histogram. You can create a histogram as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-6</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">plt.hist(data)</p>
			<p>The resulting histogram is shown here:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B18335_10_3.jpg" alt="Figure 10.3 – Resulting histogram of Code Block 10-6&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Resulting histogram of Code Block 10-6</p>
			<p>The<a id="_idIndexMarker575"/> data clearly follows a normal distribution, but it is not centered around <strong class="source-inline">0</strong> and it is not standardized to a standard deviation of <strong class="source-inline">1</strong>.</p>
			<ol>
				<li value="3">Now, to scale this data, let's use <strong class="source-inline">StandardScaler</strong> from River. Again, we will loop through the data to simulate streaming. Also, we again use the <strong class="source-inline">learn_one</strong> method to update the data step by step:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-7</p>
			<p class="source-code">from river import preprocessing</p>
			<p class="source-code"># convert the data to required format</p>
			<p class="source-code">data_stream = [{'x':float(x)} for x in list(data)]</p>
			<p class="source-code"># initialize list for scaled values</p>
			<p class="source-code">data_scaled = []</p>
			<p class="source-code"># initialize scaler</p>
			<p class="source-code">my_scaler = preprocessing.StandardScaler()</p>
			<p class="source-code"># streaming</p>
			<p class="source-code">for observation in data_stream:</p>
			<p class="source-code">  # learn (update)</p>
			<p class="source-code">  my_scaler.learn_one(observation)</p>
			<p class="source-code">  # scale the observation</p>
			<p class="source-code">  scaled_obs = my_scaler.transform_one(observation)</p>
			<p class="source-code">  </p>
			<p class="source-code">  # store the scaled result</p>
			<p class="source-code">  data_scaled.append(scaled_obs['x'])</p>
			<ol>
				<li value="4">To verify<a id="_idIndexMarker576"/> that it has worked correctly, let's redo the histogram using the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-8</p>
			<p class="source-code">plt.hist(data_scaled)</p>
			<p>The histogram is shown here:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B18335_10_4.jpg" alt="Figure 10.4 – Resulting histogram of Code Block 10-8&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Resulting histogram of Code Block 10-8</p>
			<p>As you can <a id="_idIndexMarker577"/>see, the data is clearly centered around <strong class="source-inline">0</strong>, and the new, scaled value indicates the number of standard deviations that each data point is away from the mean.</p>
			<p>In the next section, you will see how to adapt feature transformation in a streaming context.</p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor208"/>Transforming features in a streaming context</h1>
			<p>Scaling data is<a id="_idIndexMarker578"/> a way of pre-processing data for machine learning, but <a id="_idIndexMarker579"/>many other statistical methods can be used for data preparation. In this second part of this chapter let's deep dive into the <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>) method, a much-used method for preparing data at the beginning of any machine learning.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor209"/>Introducing PCA</h2>
			<p>PCA is a <a id="_idIndexMarker580"/>machine learning method that can be used for multiple applications. When working with highly multivariate data, PCA can be used in an interpretative way, where you use it to make sense of and analyze multivariate datasets. This is a use of PCA in data analysis.</p>
			<p>Another way to use PCA is to prepare data for machine learning. From a high-level point of view, PCA could be seen as an alternative to scaling that reduces the number of variables of your <a id="_idIndexMarker581"/>data to make it easier for the model to fit. This is the use of PCA that is most relevant for the current chapter, and this is how it will be used in the example.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor210"/>Mathematical definition of PCA</h2>
			<p>PCA works <a id="_idIndexMarker582"/>on multivariate data (or data with multiple columns). These columns generally have a business definition. The goal of PCA is to keep all information in the data but change the current variable definitions into variables with different interpretations.</p>
			<p>The new variables are <a id="_idIndexMarker583"/>called the <strong class="bold">principal components</strong>, and they are found in such a way that the first component contains the most possible variation, and the second component is the component that is orthogonal to the first one and explains the most variation possible while being orthogonal.</p>
			<p>A schematical overview<a id="_idIndexMarker584"/> is shown here:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B18335_10_5.jpg" alt="Figure 10.5 – Schematic overview of PCA&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Schematic overview of PCA</p>
			<p>This example clearly shows how the original data on the left is transformed into principal components on the right. The first principal component has much more value in terms of information than any of the original variables. When working with hundreds of variables, you can imagine that you will need to retain only a limited number of components (based on different criteria and your use case), which may make it easier for your machine learning algorithm to learn from the data.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor211"/>Regular PCA in Python</h2>
			<p>To have a good <a id="_idIndexMarker585"/>comparison between regular and incremental PCA, it is good to get everybody up to speed and do a quick <a id="_idIndexMarker586"/>example of a regular PCA first:</p>
			<ol>
				<li value="1">To do this, let's create some simulated sample data to work on the example. We can make a small example dataset as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-9</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">X1 = np.random.normal(5, 1, size=100)</p>
			<p class="source-code">X2 = np.random.normal(5, 0.5, size=100)</p>
			<p class="source-code">data = pd.DataFrame({'X1': X1, 'X2': X1 + X2})</p>
			<p class="source-code">data.head()</p>
			<p>The data looks like this:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B18335_10_6.jpg" alt="Figure 10.6 – The resulting data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – The resulting data</p>
			<ol>
				<li value="2">You can make a plot of this data as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-10</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">plt.scatter(data['X1'], data['X2'])</p>
			<p>The <a id="_idIndexMarker587"/>scatter plot shows a plot that is quite similar to the sketch in the earlier schematic drawing:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B18335_10_7.jpg" alt="Figure 10.7 – The resulting image of Code Block 10-10&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 10.7 – The resulting image of Code Block 10-10</p>
			<ol>
				<li value="3">Let's now use a regular PCA to identify the components and transform the data. The following block of code shows how to fit a PCA using <strong class="source-inline">scikit-learn</strong>:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-11</p>
			<p class="source-code">from sklearn.decomposition import PCA</p>
			<p class="source-code">my_pca = PCA()</p>
			<p class="source-code">transformed_data = my_pca.fit_transform(data)</p>
			<p class="source-code">transformed_data = pd.DataFrame(transformed_data, columns = ['PC1', 'PC2'])</p>
			<p class="source-code">transformed_data.head()</p>
			<p>The transformed data looks as follows:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B18335_10_8.jpg" alt="Figure 10.8 – The transformed data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – The transformed data</p>
			<ol>
				<li value="4">We <a id="_idIndexMarker588"/>can plot it just like we did with the previous data. This can be done using the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-12</p>
			<p class="source-code">plt.scatter(transformed_data['PC1'], transformed_data['PC2'])</p>
			<p class="source-code">plt.xlim(-4, 4)</p>
			<p class="source-code">plt.ylim(-4, 4)</p>
			<p class="source-code">plt.show()</p>
			<p>The plot looks as follows:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B18335_10_9.jpg" alt="Figure 10.9 – Plot of the transformed data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – Plot of the transformed data</p>
			<p>You can <a id="_idIndexMarker589"/>clearly see that this looks a lot like the resulting plot in the earlier theoretical introduction. This PCA has successfully identified the first principal component to be the component that explains the largest part of the data. The second component explains the largest part of the remaining data (after the first component).</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor212"/>Incremental PCA for streaming</h2>
			<p>PCA, in a <a id="_idIndexMarker590"/>streaming context, cannot be easily calculated on individual data points. After all, you can imagine that it is impossible to determine the standard deviation of a single data point, and therefore, there is no possible way to determine the best components.</p>
			<p>The proposed solution is to do this through batches and to compute your PCA in batches rather than all at once. The <strong class="source-inline">scikit-learn</strong> package has a functionality called <strong class="source-inline">IncrementalPCA</strong>, which allows you to fit PCA in batches. Let's use the following code for fitting <strong class="source-inline">IncrementalPCA</strong> on the same data as before and compare the results. The code to fit and transform using <strong class="source-inline">IncrementalPCA</strong> is shown in the following:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-13</p>
			<pre class="source-code">from sklearn.decomposition import IncrementalPCA</pre>
			<pre class="source-code">my_incremental_pca = IncrementalPCA(batch_size = 10)</pre>
			<pre class="source-code">transformed_data_2 = my_incremental_pca.fit_transform(data)</pre>
			<pre class="source-code">transformed_data_2 = pd.DataFrame(transformed_data_2, columns = ['PC1', 'PC2'])</pre>
			<pre class="source-code">transformed_data_2.head()</pre>
			<p>The<a id="_idIndexMarker591"/> transformed data using this second method looks as follows:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B18335_10_10.jpg" alt="Figure 10.10 – The transformed data using incremental PCA&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – The transformed data using incremental PCA</p>
			<p>Now, let's also make a plot of this data to see whether this batch-wise PCA was successful in fitting the real components, or whether it is far away from the original PCA:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 10-14</p>
			<pre class="source-code">plt.scatter(transformed_data_2['PC1'], transformed_data_2['PC2'])</pre>
			<pre class="source-code">plt.xlim(-4, 4)</pre>
			<pre class="source-code">plt.ylim(-4, 4)</pre>
			<pre class="source-code">plt.show()</pre>
			<p>The resulting scatter plot is shown in the following:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B18335_10_11.jpg" alt="Figure 10.11 – The scatter plot of the transformed data using incremental PCA&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – The scatter plot of the transformed data using incremental PCA</p>
			<p>This <a id="_idIndexMarker592"/>scatter plot shows that the PCA has been correctly fitted. Do not be confused by the fact that the incremental PCA has inversed the first component (the image is mirrored left to right compared to the preceding one). This is not wrong but just mirrored. This incremental PCA has captured the two components very well.</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor213"/>Summary</h1>
			<p>In this chapter, you have seen some common methods for data preparation being adapted to streaming and online data. For streaming data, it is important to have easily refitting or re-estimating models.</p>
			<p>In the first part of the chapter, you have seen two methods for scaling. The MinMaxScaler scales the data to the <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong> range and, therefore, needs to make sure that none of the new data points get outside of this range. The StandardScaler uses a statistical normalization process using the mean and standard deviation.</p>
			<p>The second part of the chapter demonstrated a regular PCA and a new, incremental version called <strong class="source-inline">IncrementalPCA</strong>. This incremental method allows you to fit PCA in batches, which can help you when fitting PCA on streaming data.</p>
			<p>With scaling and feature transformation in this chapter, and drift detection in the previous chapter, you have already seen a good part of the auxiliary tasks of machine learning on streaming. In the coming chapter, you will see the third and last secondary topic to machine learning and streaming, which is catastrophic forgetting: an impactful problem that can occur in online machine learning, causing the model to forget important learned trends. The chapter will explain how to detect and avoid it.</p>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor214"/>Further reading</h1>
			<ul>
				<li><em class="italic">MinMaxScaler in River</em>: <a href="https://riverml.xyz/latest/api/preprocessing/MinMaxScaler/">https://riverml.xyz/latest/api/preprocessing/MinMaxScaler/</a></li>
				<li><em class="italic">StandardScaler in River</em>: <a href="https://riverml.xyz/latest/api/preprocessing/StandardScaler/">https://riverml.xyz/latest/api/preprocessing/StandardScaler/</a></li>
				<li><em class="italic">PCA in scikit-learn</em>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a> </li>
				<li><em class="italic">Incremental PCA in scikit-learn</em>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html</a></li>
			</ul>
		</div>
	</body></html>