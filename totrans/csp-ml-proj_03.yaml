- en: Twitter Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to expand our knowledge of building classification
    models in C#. Along with the two packages, Accord.NET and Deedle, which we used
    in the previous chapter, we are going to start using the Stanford CoreNLP package
    to apply more advanced **natural language processing** (**NLP**) techniques, such
    as tokenization, **part of speech** (**POS**) tagging, and lemmatization. Using
    these packages, our goal for this chapter is to build a multi-class classification
    model that predicts the sentiments of tweets. We will be working with a raw Twitter
    dataset that contains not only words, but also emoticons, and will use it to train
    a **machine learning** (**ML**) model for sentiment prediction. We will be following
    the same steps that we follow when building ML models. We are going to start with
    the problem definition and then data preparation and analysis, feature engineering,
    and model development and validation. During our feature engineering step, we
    will expand our knowledge of NLP techniques and explore how we can apply tokenization,
    POS tagging, and lemmatization to build more advanced text features. In the model
    building step, we are going to explore a new classification algorithm, a random
    forest classifier, and compare its performance to the Naive Bayes classifier.
    Lastly, in our model validation step, we are going to expand our knowledge of
    confusion matrixes, precision, and recall, which we covered in the previous chapter,
    and discuss what the **Receiver Operating Characteristic** (**ROC**) curve and
    **area under the curve** (**AUC**) are and how these concepts can be used to evaluate
    our ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment with the Stanford CoreNLP package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem definition for the Twitter sentiment analysis project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation using Stanford CoreNLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis using lemmas as tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering using lemmatization and emoticons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes versus random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model validations using the ROC curve and AUC metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive into our Twitter sentiment analysis project, let''s set up our
    development environment with the Stanford CoreNLP package that we are going to
    use throughout this chapter. Multiple steps are required to get your environment
    ready with the Standford CoreNLP package, so it is a good idea to work through
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create a new Console App (.NET Framework) project in Visual
    Studio. Make sure you use a .NET Framework version higher than or equal to 4.6.1\.
    If you have an older version installed, go to [https://docs.microsoft.com/en-us/dotnet/framework/install/guide-for-developers](https://docs.microsoft.com/en-us/dotnet/framework/install/guide-for-developers) and
    follow the installation guide. Following is a screenshot of the project setup
    page (note that you can select your .NET Framework version in the top bar):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s install the Stanford CoreNLP package. You can type in the following
    command in your Package Manager Console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The version we are going to use in this chapter is `Stanford.NLP.CoreNLP` 3.9.1\.
    Over time, the versions might change and you might have to update your installations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We just have to do one more thing and our environment will be ready to start
    using the package. We need to install the CoreNLP models JAR, which contains various
    models for parsing, POS tagging, **Named Entity Recognition** (**NER**), and some
    other tools. Follow this link to download and unzip Stanford CoreNLP: [https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/).
    Once you have downloaded and unzipped it, you will see multiple files in there.
    The particular file of interest is `stanford-corenlp-<version-number>-models.jar`.
    We need to extract the contents from that jar file into a directory so that we
    can load all the model files within our C# project. You can use the following
    command to extract the contents from `stanford-corenlp-<version-number>-models.jar`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When you are done extracting all the model files from the models jar file, you
    are now ready to start using the Stanford CoreNLP package in your C# project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s check whether our installation was successful. The following code
    is a slight modification for this example ([https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html](https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html))
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If your installation was successful, you should see output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00041.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a closer look at this output. Tokens are character sequences that
    are grouped as individual semantic units. Often, tokens are *words* or *terms*.
    In each token output, we can see the original text, such as `We`, `''re`, and
    `going`. The `PartOfSpeech` tag refers to the category of each word, such as noun,
    verb, and adjective. For example, the `PartOfSpeech` tag of the first token in
    our example, `We`, is `PRP` and it stands for *personal pronoun*. The `PartOfSpeech`
    tag of the second token in our example, `''re`, is `VBP` and it stands for *v**erb,
    non-third-person singular present*. The complete list of POS tags can be found
    here ([http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))
    or in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A list of POS tags
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the `Lemma` tag in our tokenization example refers to the standard form
    of the given word. For example, the lemma of `am` and `are` is `be`. In our example,
    the word `going` in our third token has `go` as its lemma. We will discuss how
    we can use word lemmatization for feature engineering in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Problem definition for Twitter sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start our Twitter sentiment analysis project by clearly defining what
    models we will be building and what they are going to predict. You might have
    heard the term **sentiment analysis** in the past already. Sentiment analysis
    is essentially a process of computationally determining whether a given text expresses
    a positive, neutral, or negative emotion. Sentiment analysis for social media
    content can be used in various ways. For example, it can be used by marketers
    to identify how effective a marketing campaign was and how it affected consumers''
    opinions and attitudes towards a certain product or company. Sentiment analysis
    can also be used to predict stock market changes. Positive news and aggregate
    positive emotions towards a certain company often move its stock price in a positive
    direction, and sentiment analysis in the news and social media for a given company
    can be used to predict how stock prices will move in the near future. To experiment
    with how we can build a sentiment analysis model, we are going to use a precompiled
    and labeled airline sentiment Twitter dataset that originally came from CrowdFlower''s
    Data for Everyone library ([https://www.figure-eight.com/data-for-everyone/](https://www.figure-eight.com/data-for-everyone/)).
    Then, we are going to apply some NLP techniques, especially word tokenization,
    POS tagging, and lemmatization, to build meaningful text and emoticon features
    from raw tweet data. Since we want to predict three different emotions (positive,
    neutral, and negative) for each tweet, we are going to build a multi-class classification
    model and experiment with different learning algorithms—Naive Bayes and random
    forest. Once we build the sentiment analysis models, we are going to evaluate
    the performance mainly via these three metrics: precision, recall, and AUC.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s summarize our problem definition for the Twitter sentiment analysis
    project:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the problem? We need a Twitter sentiment analysis model to computationally
    identify the emotions in tweets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is it a problem? Identifying and measuring the emotions of users or consumers
    about a certain topic, such as a product, company, advertisement, and so forth,
    are often an essential tool to measure the impact and success of certain tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some of the approaches to solving this problem? We are going to use
    the Stanford CoreNLP package to apply various NLP techniques, such as tokenization,
    POS tagging, and lemmatization, to build meaningful features from a raw Twitter
    dataset. With these features, we are going to experiment with different learning
    algorithms to build a sentiment analysis model. We will use precision, recall,
    and AUC measures to evaluate the performance of the models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the success criteria? We want high precision rates, without sacrificing
    too much for recall rates, as correctly classifying a tweet into one of three
    emotion buckets (positive, neutral, and negative) is more important than a higher
    retrieval rate. Also, we want a high AUC number, which we will discuss in more
    detail in later sections of this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation using Stanford CoreNLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know what our goals are in this chapter, it is time to dive into
    the data. Similar to the last chapter, we are going to use precompiled and pre-labeled
    Twitter sentiment data. We are going to use a dataset from CrowdFlower''s Data
    for Everyone library ([https://www.figure-eight.com/data-for-everyone/](https://www.figure-eight.com/data-for-everyone/)) and
    you can download the data from this link: [https://www.kaggle.com/crowdflower/twitter-airline-sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment).
    The data we have here is about 15,000 tweets about US airlines. This Twitter data
    was scraped from February of 2015 and was then labeled into three buckets—positive,
    negative, and neutral. The link provides you with two types of data: a CSV file
    and an SQLite database. We are going to work with the CSV file for this project.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have downloaded this data, we need to get it prepared for our future
    analysis and model building. The two columns of interest in the dataset are `airline_sentiment`
    and `text`. The `airline_sentiment` column contains information about the sentiment—whether
    a tweet has positive, negative, or neutral sentiments—and the `text` column contains
    the raw Twitter text. To make this raw data readily available for our future data
    analysis and model building steps, we need to do the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clean up unnecessary text**: It''s hard to justify some parts of the text
    as providing many insights and much information for our models to learn from,
    such as URLs, user IDs, and raw numbers. So, the first step to prepare our raw
    data is to clean up unnecessary text that does not contain much information. In
    this example, we removed the URLs, Twitter user IDs, numbers, and hash signs in
    hashtags. We used `Regex` to replace such texts with empty strings. The following
    code illustrates the `Regex` expressions we used to filter out those texts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from this code, there are two ways to replace a string that
    matches a `Regex` pattern. You can instantiate a `Regex` object and then replace
    matching strings with the other string, as shown in the first two cases. You can
    also directly call the static `Regex.Replace` method for the same purpose, as
    shown in the last two cases. The static method is going to create a `Regex` object
    each time you call the `Regex.Replace` method, so if you are using the same pattern
    in multiple places, it will be better to go with the first approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Group and encode similar emoticons together**: Emoticons, such as smiley
    faces and sad faces, are frequently used in tweets and provide useful insights
    about the emotion of each tweet. Intuitively, one user will use smiley face emoticons
    to tweet about positive events, while another will use sad face emoticons to tweet
    about negative events. However, different smiley faces show similar positive emotions
    and can be grouped together. For example, a smiley face with a parenthesis, `:)`,
    will have the same meaning as another smiley face with a capital letter `D`, `:D`.
    So, we want to group these similar emoticons together and encode them as one group
    rather than having them in separate groups. We will use the R code that Romain
    Paulus and Jeffrey Pennington shared ([https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb](https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb)),
    translate it into C#, and then apply it to our raw Twitter dataset. The following
    is how we translated the emoticon `Regex` codes, written in R, into C#, so that
    we can group and encode similar emoticons together:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Group and encode additional helpful expressions together**: Lastly, there
    are some more expressions that can help our models detect the emotions of tweets.
    Repeated punctuation, such as `!!!` and `???`, and elongated words, such as `wayyyy`
    and `soooo`, can provide some extra information about the sentiments of tweets.
    We will group and encode them separately so that our models can learn from such
    expressions. The following code shows how we encoded such expressions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the code, for repeated punctuation we appended a string with a suffix,
    `_repeat`. For example, `!!!` will become `!_repeat` and `???` will become `?_repeat`.
    For elongated words, we appended a string with a suffix, `_emphasized`. For example,
    `wayyyy` will become `way_emphasized` and `soooo` will become `so_emphasized`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code that takes the raw dataset, processes individual Twitter text
    as discussed previously, and exports the processed Twitter text into another data
    file can be found in this repository: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataProcessor.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataProcessor.cs).
    Let''s briefly walk through the code. It first reads the raw `Tweets.csv` dataset
    into a Deedle data frame (lines 76–82). Then, it calls a method named `FormatTweets` with
    a column series that contains all the raw Twitter text. The `FormatTweets` method
    code in lines 56–65 looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This `FormatTweets`method iterates through each element in the series, which
    is the raw tweets, and calls the `CleanTweet` method. Within the `CleanTweet`
    method, each raw tweet is run against all the `Regex` patterns that we defined
    previously and is then processed as discussed earlier. The `CleanTweet` method
    in lines 11–54 looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once all the raw Twitter tweets are cleaned and processed, the result gets
    added to the original Deedle data frame as a separate column with `tweet` as its
    column name. The following code (line 89) shows how you can add an array of strings
    to a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have come this far, the only additional step we need to do is export
    the processed data. Using Deedle data frame''s `SaveCsv` method, you can easily
    export a data frame into a CSV file. The following code shows how we exported
    the processed data into a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have clean Twitter text, let''s tokenize and create a matrix representation
    of tweets. Similar to what we did in [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam
    Email Filtering*, we are going to break a string into words. However, we are going
    to use the Stanford CoreNLP package that we installed in the previous section
    of this chapter and utilize the sample code that we wrote in the previous section.
    The code to tokenize tweets and build a matrix representation of them is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the code, the main difference between this code and the
    sample code in the previous section is that this code iterates over each tweet
    and stores the tokens into a Deedle's data frame. As in [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam
    Email Filtering,* we are using one-hot encoding to assign each term's value (0
    versus 1) within the matrix. One thing to note here is how we have an option to
    create the matrix with lemmas or words. Words are the original untouched terms
    that are broken down from each tweet. For example, a string, `I am a data scientist`,
    will be broken down into `I`, `am`, `a`, `data`, and `scientist`, if you use words
    as tokens. Lemmas are standard forms of words in each token. For example, the
    same string, `I am a data scientist`, will be broken down into `I`, `be`, `a`,
    `data`, and `scientist`, if you use lemmas as tokens. Note that `be` is a lemma
    for `am`. We will discuss what lemmas are and what lemmatization is in the *Feature
    engineering using lemmatization and emoticons* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code to tokenize and create a matrix representation of tweets can
    be found here: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterTokenizer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterTokenizer.cs).
    There are a few things to note in this code. First, let''s look at how it counts
    how many samples we have for each sentiment. The following code snippet (lines
    122–127) shows how we computed the number of samples per sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, we first get the sentiment column, `airline_sentiment`,
    and group it by the values, where the values can be `neutral`, `negative`, or
    `positive`. Then, it counts the number of occurrences and returns the count.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second thing to note in the TwitterTokenizer code is how we encoded sentiments
    with integer values. The following is what you see in lines 149–154 of the full
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, note how we are calling the `CreateWordVec` method twice—once without
    lemmatization (lines 135-144) and once with lemmatization (lines 147-156). If
    we create a term matrix with one-hot encodings without lemmatization, we are essentially
    taking all the words as individual tokens in our term matrix. As you can imagine,
    this will create a much larger and more sparse matrix than one with lemmatization.
    We left both codes there for you to explore both options. You can try building
    ML models with a matrix with words as columns and compare them against those with
    lemmas as columns. In this chapter, we are going to use the matrix with lemmas
    instead of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run this code, it will output a bar chart that shows the sentiment
    distribution in the sample set. As you can see in the following chart, we have
    about 3,000 neutral tweets, 2,000 positive tweets, and 9,000 negative tweets in
    our sample set. The chart looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Data analysis using lemmas as tokens
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is now time to look at the actual data and seek any patterns or differences
    in the distributions of term frequencies along with the different sentiments of
    tweets. We are going to take the output from the previous step and get the distributions
    of the top seven most frequently occurring tokens for each sentiment. In this
    example, we use a term matrix with lemmas. Feel free to run the same analysis
    for a term matrix with words. The code to analyze the top N most frequently used
    tokens in each sentiment of tweets can be found here: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataAnalyzer.cs).
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one thing to note in this code. Unlike in the previous chapter, we
    need to compute term frequencies for three sentiment classes—neutral, negative,
    and positive. The following is the code snippet from the full code (lines 54-73):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the code, we call the `ColumnWiseSum` method for each sentiment
    class, and the code for this method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, it iterates through each column or term and sums
    all the values within that column. Since we used one-hot encodings, a simple column-wise
    sum will give us the number of occurrences for each term in our Twitter dataset.
    Once we have computed all the column-wise summations, we return them as a Deedle
    series object. With these results, we rank-order the terms by their frequencies
    and store this information into three separate files, `neutral-frequencies.csv`,
    `negative-frequencies.csv`, and `positive-frequencies.csv`. We are going to use
    the term frequency output in later sections for feature engineering and model
    building.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the code, it will generate the following charts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the charts, there are some obvious differences in distributions
    among different sentiments. Words such as **thanks** and **great** were two of
    the top seven frequently occurring terms in positive tweets, while words like
    **delay** and **cancelled** were two of the top seven frequently occurring terms
    in negative tweets. Intuitively, these make sense. You typically use words **thanks**
    and **great** when you express positive feelings towards someone or something.
    On the other hand, **delay** and **cancelled** are related to negative events
    in the context of flights or airlines. Maybe some of the users' flights were delayed
    or cancelled and they tweeted about their frustrations. Another interesting thing
    to note is how the term `emo_smiley` was ranked seventh of the most frequently
    occurring terms in positive tweets. If you remember, in the previous step we grouped
    and encoded all smiley face emoticons (such as `:)`, `:D`, and so on) as `emo_smiley`.
    This tells us that emoticons may play an important role for our models to learn
    how to classify the sentiment of each tweet. Now that we have a rough idea of
    what our data looks like and what kinds of terminology appear for each sentiment,
    let's talk about the feature engineering techniques we will employ in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering using lemmatization and emoticons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We briefly talked about lemmas in the previous section. Let's take a deeper
    look at what lemmas are and what lemmatization is. Depending on how and where
    a word is being used in a sentence, the word is going to be in different forms.
    For example, the word `like` can take the form of `likes` or `liked` depending
    on what came before. If we simply tokenize sentences into words, then our program
    is going to see the words `like`, `likes`, and `liked` as three different tokens.
    However, that might not be something we want. Those three words share the same
    meaning and when we are building models, it would be useful to group those words
    as one token in our feature set. This is what lemmatization does. A lemma is the
    base form of a word and lemmatization is transforming each word into a lemma based
    on the part of the sentence each word was used in. In the preceding example, `like`
    is the lemma for `likes` and `liked`, and systematically transforming `likes`
    and `liked` into `like` is a lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is an example of a lemmatization using Stanford CoreNLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, you can see that both `likes` and `like` were lemmatized into `like`.
    This is because both of those words were used as verbs in a sentence and the lemma
    for the verbal form is `like`. Let''s look at another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the first `likes` and the second `likes` have different lemmas. The first
    one has `like` as its lemma, while the second one has `likes` as its lemma. This
    is because the first one is used as a verb, while the second one as a noun. As
    you can see from these examples, depending on the parts of the sentence, the lemmas
    for the same words can be different. Using lemmatization for your text dataset
    can greatly reduce the sparsity and dimensions of your feature space and can help
    your models learn better without being exposed to too much noise.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to lemmatization, we also grouped similar emoticons into the same group.
    This is based on the assumption that similar emoticons have similar meanings.
    For example, `:)` and `:D` have almost the same meanings, if not exactly the same.
    In another case, depending on the users, the positions of the colon and parenthesis
    can differ. Some users might type `:)`, but some others might type `(:`. However,
    the only different between these two is the positioning of the colon and parenthesis
    and the meanings are the same. In all of these cases, we want our models to learn
    the same emotion and not create any noise. Grouping similar emoticons into the
    same group, as we did in the previous step, helps reduce unnecessary noise for
    our models and help them learn the most from these emoticons.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes versus random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is finally time to train our ML models to predict the sentiments of tweets.
    In this section, we are going to experiment with Naive Bayes and random forest
    classifiers. There are two things that we are going to do differently from the
    previous chapter. First, we are going to split our sample set into a train set
    and a validation set, instead of running k-fold cross-validation. This is also
    a frequently used technique, where the models learn only from a subset of the
    sample set and then they are tested and validated with the rest, which they did
    not observe at training time. This way, we can test how the models will perform
    in the unforeseen dataset and simulate how they are going to behave in a real-world
    case. We are going to use the `SplitSetValidation` class in the Accord.NET package
    to split our sample set into train and validation sets with pre-defined proportions
    for each set and fit a learning algorithm to the train set.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, our target variable is no longer binary (0 or 1), unlike in the previous
    [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470)*, Spam Email
    Filtering*. Instead, it can take any values from 0, 1, or 2, where 0 stands for
    neutral sentiment tweets, 1 for positive sentiment tweets, and 2 for negative sentiment
    tweets. So, we are now dealing with a multi-class classification problem, rather
    than a binary classification problem. We will have to approach things differently
    when evaluating our models. We will have to modify our accuracy, precision, and
    recall calculation codes from the previous chapter to compute those numbers for
    each of the three target sentiment classes in this project. Also, we will have
    to use a one-versus-rest approach when we look at certain metrics, such as a ROC
    curve and AUC, which we will be discussing in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first look at how to instantiate our learning algorithms with the `SplitSetValidation`
    class in the Accord.NET Framework. The following is how you can instantiate a `SplitSetValidation`object
    with the Naive Bayes classifier algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to how we instantiated a `SplitSetValidation` object with the Naive
    Bayes classifier, you can instantiate another one with the random forest classifier
    as in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We replaced the previous code with random forest as a model and `RandomForestLearning` as
    a learning algorithm. If you look closely, there are some hyperparameters that
    we can tune for `RandomForestLearning`. The first one is `NumberOfTrees`. This
    hyperparameter lets you choose the number of decision trees that go into your
    random forest. In general, having more trees in a random forest results in better
    performance, as you are essentially building more decision trees in the forest.
    However, the performance lift comes at the cost of training and prediction time.
    It will take more time to train and make predictions as you increase the number
    of trees in your random forest. The other two parameters to note here are `CoverageRatio`and
    `SampleRatio`. `CoverageRatio`sets the proportion of the feature set to be used
    in each tree, while `SampleRatio`sets the proportion of the train set to be used
    in each tree. Having a higher `CoverageRatio` and`SampleRatio` increases the performance
    of individual trees in the forest, but it also increases the correlation among
    the trees. Lower correlation among the trees helps reduce the generalization error;
    thus, finding a good balance between the prediction powers of individual trees
    and correlation among the trees will be essential in building a good random forest
    model. Tuning and experimenting with various combinations of these hyperparameters
    can help you avoid overfitting issues, as well as improving your model performance
    when training a random forest model. We recommend you build a number of random
    forest classifiers with various combinations of those hyperparameters and experiment
    with their effects on your model performances.
  prefs: []
  type: TYPE_NORMAL
- en: The full code that we used to train Naive Bayes and random forest classification
    models and output validation results can be found here: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterSentimentModeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterSentimentModeling.cs).
    Let's take a closer look at this code. In lines 36-41, it first reads in the token
    matrix file, `tweet-lemma.csv`, which we built in the data preparation step. Then
    in lines 43-51, we read in the term frequency files, `positive-frequencies.csv`
    and `negative-frequencies.csv`, which we built in the data analysis step. Similar
    to what we did in the previous chapter, we do feature selection based on the number
    of term occurrences in line 64\. In this example, we experimented with 5, 10,
    50, 100, and 150 as the thresholds for the minimum number of term occurrences
    in our sample tweets. From line 65, we iterate through those thresholds and start
    training and evaluating Naive Bayes and random forest classifiers. Each time a
    model is trained on a train set, it is then run against the validation set that
    was not observed during the training time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is part of the full code (lines 113-135) that runs the trained Naive
    Bayes model on the train and validation sets to measure in-sample and out-of-sample
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the part of the full code (lines 167-189) that runs the trained
    random forest model on the train and validation sets to measure in-sample and
    out-of-sample performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a closer look at these. For brevity, we will only take a look at
    the random forest model case, as it will be the same for the Naive Bayes classifier.
    In line 168, we first get the trained model from the learned results. Then, we
    get the indexes of in-sample (train set) and out-of-sample (test/validation set)
    sets from the `SplitSetValidation` object in lines 170-171, so that we can iterate
    through each row or record and make predictions. We iterate this process twice—once
    for the in-sample training set in lines 175-181 and again for the out-of-sample
    validation set in lines 183-189.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the prediction results on the train and test sets, we run those
    results through some validation methods (lines 138-141 for the Naive Bayes classifier
    and lines 192-196 for the random forest classifier). There are two methods that
    we wrote specifically for the model validation for this project—`PrintConfusionMatrix`
    and `DrawROCCurve`. `PrintConfusionMatrix`is an updated version of what we had
    in [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam Email
    Filtering*, where it now prints a 3 x 3 confusion matrix, instead of a 2 x 2 confusion
    matrix. On the other hand, the `DrawROCCurve`method brings in some new concepts
    and new model validation methods for this project. Let's discuss those new evaluation
    metrics, which we are using for this project, in greater detail in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Model validations – ROC curve and AUC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned before, we are using different model validation metrics in this
    chapter: the ROC curve and AUC. The ROC curve is a plot of a true positive rate
    against a false positive rate at various thresholds. Each point in the curve represents
    the true positive and false positive rate pair corresponding at a certain probability
    threshold. It is commonly used to select the best and the most optimal models
    among different model candidates.'
  prefs: []
  type: TYPE_NORMAL
- en: The area under the ROC curve (AUC) measures how well the model can distinguish
    the two classes. In the case of a binary classification, AUC measures how well
    a model distinguishes the positive outcomes from the negative outcomes. Since
    we are dealing with a multi-class classification problem in this project, we are
    using a one-versus-rest approach to build the ROC curve and compute the AUC. For
    example, one ROC curve can take positive tweets as positive outcomes and neutral
    and negative tweets as negative outcomes, while another ROC curve can take neutral
    tweets as positive outcomes and positive and negative tweets as negative outcomes.
    As shown in the following charts, we drew three ROC charts for each model we built—one
    for Neutral verses Rest (Positive and Negative), one for Positive versus Rest
    (Neutral and Negative), and one for Negative versus Rest (Neutral and Positive).
    The higher the AUC number is, the better the model is as it suggests that the
    model can distinguish positive classes from negative classes with much better
    chance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following charts show ROC curves for Naive Bayes classifiers with the minimum
    number of term occurrences at **10**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following charts show ROC curves for Naive Bayes classifiers with the minimum
    number of term occurrences at **50**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following charts show ROC curves for Naive Bayes classifiers with the minimum
    number of term occurrences at **150**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the charts, we can also detect overfitting issues from ROC
    charts by looking at the gaps between the curves from training and testing results.
    The larger the gap is, the more the model is overfitting. If you look at the first
    case, where we only filter out those terms that appear in tweets fewer than ten
    times, the gap between the two curves is large. As we increase the threshold,
    we can see that the gap decreases. When we are choosing the final model, we want
    the train ROC curve and the test/validation ROC curve to be as small as possible.
    As this resolution comes at the expense of the model performance, we need to find
    the right cutoff line for this trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at a sample of how one of our random forest classifiers did.
    The following is a sample result from fitting a random forest classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Ensemble methods, such as random forest, generally work well for classification
    problems and accuracy can be improved by ensembling with more trees. However,
    they come with some limitations, one of which is shown in the previous sample
    results for the random forest classifier. As is true for all decision tree-based
    models, the random forest model tends to overfit, especially when it tries to
    learn from many categorical variables. As you can see from the ROC curves for
    the random forest classifier, the gap between the train and test ROC curves is
    large, especially when compared to those for the Naive Bayes classifier. The Naive
    Bayes classifier with a minimum number of term occurrences threshold at 150 has
    almost no gap between the train and test ROC curves, whereas a random forest classifier
    at the same threshold shows a large gap between the two ROC curves. When dealing
    with such a dataset, where there are lots of categorical variables, we need to
    be careful about which model to choose and pay special attention to tuning the
    hyperparameters, such as `NumberOfTrees`, `CoverageRatio`, and `SampleRatio`*,*
    for a random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built and trained more advanced classification models for
    Twitter sentiment analysis. We applied what we have learned in the previous chapter
    to a multi-class classification problem with more complex text data. We first
    started off by setting up our environment with the Stanford CoreNLP package that
    we used for tokenization, POS tagging, and lemmatization in the data preparation
    and analysis steps. Then, we transformed the raw Twitter dataset into a one-hot
    encoded matrix by tokenizing and lemmatizing the tweets. During this data preparation
    step, we also discussed how we could use Regex to group similar emoticons together
    and remove unnecessary text, such as URLs, Twitter IDs, and raw numbers, from
    tweets. We further analyzed the distribution of frequently used terms and emoticons
    in our data analysis step and we saw how lemmatization and grouping similar emoticons
    together help in reducing unnecessary noise in the dataset. With data and insights
    from previous steps, we experimented with building multi-class classification
    models using Naive Bayes and random forest classifiers. As we built these models,
    we covered a frequently used model validation technique, where we split a sample
    set into two subsets, the train set and validation set, and used the train set
    to fit a model and the validation set to evaluate the model performance. We also
    covered new model validation metrics, the ROC curve and AUC, which we can use
    to select the best and most optimal model among model candidates.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to switch gears and start building regression
    models where the target variables are continuous variables. We will use a foreign
    exchange rate dataset to build time series features and explore some other ML
    models for regression problems. We will also discuss how evaluating the performance
    of regression models is different from that of classification models.
  prefs: []
  type: TYPE_NORMAL
