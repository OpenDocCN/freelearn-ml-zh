<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Categorizing Data</h1></div></div></div><p>In this chapter, we will explore <strong>classification</strong><a id="id210" class="indexterm"/>, which is yet another interesting problem in supervised learning. We will examine a handful of techniques for classifying data and also study how we can leverage these techniques in Clojure.</p><p>Classification can be defined as the problem of identifying the category or class of the observed data based on some empirical training data. The training data will have the values of the observable features or independent variables. It will also have a known category for these observed values. Classification is similar to regression in the sense that we predict a value based on another set of values. However, for classification, we are interested in the category of the observed values rather than predicting a value based on the given set of values. For example, if we train a linear regression model from a set of output values ranging from <em>0</em> to <em>5</em>, the trained classifier could predict the output value as <em>10</em> or <em>-1</em> for a set of input values. In classification, however, the predicted value of the output variable always belongs to a discrete set of values.</p><p>The independent variables of a classification model are also termed as the <em>explanatory variables</em> of<a id="id211" class="indexterm"/> the model, and the dependent variable is also called the <em>outcome</em>, <em>category</em>, or <em>class</em> of the observed values. The outcome of a classification model is always a discrete value, that is, a value from a predetermined set of values. This is one of the primary differences between classification and regression, as we predict a variable that can have a continuous range of values in regression modeling. Note that the terms "category" and "class" are used interchangeably in the context of classification.</p><p>Algorithms that implement classification techniques are called classifiers. A <strong>classifier</strong><a id="id212" class="indexterm"/> can be formally defined as a function that maps a set of values to a category or class. Classification is still an active area of research in computer science, and there are several prominent classifier algorithms that are used in software today. There are several practical applications of classification, such as data mining, machine vision, speech and handwriting recognition, biological classification, and geostatistics.</p><div><div><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Understanding the binary and multiclass classification</h1></div></div></div><p>We will first study some theoretical aspects about data classification. As with other supervised machine learning techniques, the goal is to estimate a model or classifier from the sample data and use it to predict a given set of outcomes. Classification can be thought of as a way of determining a<a id="id213" class="indexterm"/> function that maps the features of the sample data to a specific class. The predicted class is selected from a given set of predetermined classes. Thus, similar to regression, the problem of classifying the observed values for the given independent variables is analogous to determining a best-fit function for the given training data.</p><p>In some cases, we might be interested in only a single class, that is, whether the observed values belong to a specific class. This form of classification is termed as <strong>binary classification</strong><a id="id214" class="indexterm"/>, and the output variable of the model can have either the value <em>0</em> or <em>1</em>. Thus, we can say that <img src="img/4351OS_03_01.jpg" alt="Understanding the binary and multiclass classification"/>, where <em>y</em> is the outcome or dependent variable of the classification model. The outcome is said to be negative when <img src="img/4351OS_03_03.jpg" alt="Understanding the binary and multiclass classification"/>, and conversely, the outcome is termed positive when <img src="img/4351OS_03_04.jpg" alt="Understanding the binary and multiclass classification"/>.</p><p>In this perspective, when some observed values for the independent variables of the model are provided, we must be able to determine the probability of a positive outcome. Thus, the estimated model of the given sample data has the probability <img src="img/4351OS_03_05.jpg" alt="Understanding the binary and multiclass classification"/> and can be expressed as follows:</p><div><img src="img/4351OS_03_06.jpg" alt="Understanding the binary and multiclass classification"/></div><p>In the preceding equation, the parameters <img src="img/4351OS_03_07.jpg" alt="Understanding the binary and multiclass classification"/> represent the independent variables of the estimated classification model <img src="img/4351OS_03_08.jpg" alt="Understanding the binary and multiclass classification"/>, and the term <img src="img/4351OS_03_09.jpg" alt="Understanding the binary and multiclass classification"/> represents the estimated parameter vector of this model.</p><p>An example<a id="id215" class="indexterm"/> of this kind of classification is deciding whether a new e-mail is spam or not, depending on the sender or the content within the e-mail. Another simple example of binary classification is determining the possibility of rainfall on a particular day depending on the observed humidity and the minimum and maximum temperatures on that day. The training data for this example might appear similar to the data in the following table:</p><div><img src="img/4351OS_03_10.jpg" alt="Understanding the binary and multiclass classification"/></div><p>A mathematical function that we can use to model binary classification is the <strong>sigmoid</strong><a id="id216" class="indexterm"/> or <strong>logistic</strong> function. If the<a id="id217" class="indexterm"/> outcome for the feature <em>X</em> has an estimated parameter vector <img src="img/4351OS_03_12.jpg" alt="Understanding the binary and multiclass classification"/>, we can define the estimated probability of the positive outcome <em>Y</em> (as a sigmoid function) as follows:</p><div><img src="img/4351OS_03_13.jpg" alt="Understanding the binary and multiclass classification"/></div><p>To visualize the preceding equation, we can simplify it by substituting <em>Z</em> as <img src="img/4351OS_03_14.jpg" alt="Understanding the binary and multiclass classification"/> as follows:</p><div><img src="img/4351OS_03_15.jpg" alt="Understanding the binary and multiclass classification"/></div><p>We could model the data using several other functions as well. However, the sample data for a binary classifier can be easily transformed such that it can be modeled using a sigmoid function. This modeling of a classification problem using the logistic function is called <strong>logistic regression</strong><a id="id218" class="indexterm"/>. The simplified sigmoid function defined in the preceding equation produces the following plot:</p><div><img src="img/4351OS_03_16.jpg" alt="Understanding the binary and multiclass classification"/></div><p>Note that if the term <em>Z</em> has a negative value, the plot appears reversed and is a mirror of the previous plot. We can visualize how<a id="id219" class="indexterm"/> the sigmoid function varies with respect to the term <em>Z</em>, through the following plot:</p><div><img src="img/4351OS_03_18.jpg" alt="Understanding the binary and multiclass classification"/></div><p>In the previous graph, the sigmoid function is shown for different values of the term <img src="img/4351OS_03_20.jpg" alt="Understanding the binary and multiclass classification"/>; it ranges from <em>-5</em> to <em>5</em>. Note that for two dimensions, the term <img src="img/4351OS_03_20.jpg" alt="Understanding the binary and multiclass classification"/> is a linear function of the independent variable <em>x</em>. Interestingly, for <img src="img/4351OS_03_21.jpg" alt="Understanding the binary and multiclass classification"/> and <img src="img/4351OS_03_22.jpg" alt="Understanding the binary and multiclass classification"/>, the sigmoid function looks more or less like a straight line. This function reduces to a straight line when <img src="img/4351OS_03_23.jpg" alt="Understanding the binary and multiclass classification"/> and can be represented by a constant <em>y</em> value (the equation <img src="img/4351OS_03_24.jpg" alt="Understanding the binary and multiclass classification"/> in this case).</p><p>We observe that the<a id="id220" class="indexterm"/> estimated outcome <em>Y</em> is always between <em>0</em> and <em>1</em>, as it represents the probability of a positive outcome for the given observed values. Also, this range of the outcome <em>Y</em> is not affected by the sign of the term <img src="img/4351OS_03_25.jpg" alt="Understanding the binary and multiclass classification"/>. Thus, in retrospect, the sigmoid function is an effective representation of binary classification.</p><p>To use the logistic function to estimate a classification model from the training data, we can define the cost function of a logistic regression model as follows:</p><div><img src="img/4351OS_03_26.jpg" alt="Understanding the binary and multiclass classification"/></div><p>The preceding equation essentially sums up the differences between the actual and predicted values of the output variables in our model, just like linear regression. However, as we are dealing with probability values between <em>0</em> and <em>1</em>, we use the preceding log function to effectively measure the differences between the actual and predicted output values. Note that the term <em>N</em> denotes the number of samples in the training data. We can apply the gradient descent to this cost function to determine the local minimum or rather the predicted class of a set of observed values. This equation can be regularized to produce the following cost function:</p><div><img src="img/4351OS_03_27.jpg" alt="Understanding the binary and multiclass classification"/></div><p>Note that in this equation, the second summation term is added as a regularization term, like we discussed in <a class="link" href="ch02.html" title="Chapter 2. Understanding Linear Regression">Chapter 2</a>, <em>Understanding Linear Regression</em>. This term basically prevents the <em>underfitting</em> and <em>overfitting</em> of the estimated model over the sample data. Note that the term <img src="img/4351OS_03_28.jpg" alt="Understanding the binary and multiclass classification"/> is the regularization parameter and has to be appropriately selected <a id="id221" class="indexterm"/>depending on how accurate we want the model to be.</p><p>
<strong>Multiclass classification</strong><a id="id222" class="indexterm"/>, which is the other form of classification, predicts the outcome of the classification as a value from a specific set of predetermined values. Thus, the outcome is selected from <em>k</em> discrete values, that is, <img src="img/4351OS_03_29.jpg" alt="Understanding the binary and multiclass classification"/>. This model produces <em>k</em> probabilities for each possible class of the observed values. This brings us to the following formal definition of multiclass classification:</p><div><img src="img/4351OS_03_30.jpg" alt="Understanding the binary and multiclass classification"/></div><p>Thus, in multiclass classification, we predict <em>k</em> distinct values, in which each value indicates the probability of the input values belonging to a particular class. Interestingly, binary classification can be reasoned as a specialization of multiclass classification in which there are only two possible classes, that is, <img src="img/4351OS_03_31.jpg" alt="Understanding the binary and multiclass classification"/> and <img src="img/4351OS_03_01.jpg" alt="Understanding the binary and multiclass classification"/>.</p><p>As a special case of multiclass classification, we can say that the class with the maximum probability is the outcome or simply, the predicted class of the given set of observed values. This specialization of multiclass classification is called<a id="id223" class="indexterm"/> <strong>one-vs-all</strong> classification<a id="id224" class="indexterm"/>. Here, a single class with the maximum (or minimum) probability of occurrence is determined from a given set of observed values instead of finding the probabilities of the occurrences of all the possible classes in our model. Thus, if we intend to predict a single class from a specific set of classes, we can define the outcome <em>C</em> as follows:</p><div><img src="img/4351OS_03_32.jpg" alt="Understanding the binary and multiclass classification"/></div><p>For example, let's assume that we want to determine the classification model for a fish packing plant. In this scenario, the fish are separated into two distinct classes. Let's say that we can categorize the fish either as a sea bass or salmon. We can create some training data for our model by selecting a sufficiently large sample of fish and analyzing their distributions over some selected features. Let's say that we've identified two features to categorize the data, namely, the length of the fish and the lightness of its skin. </p><p>The distribution of the first feature, that is, the length of the fish, can be visualized as follows:</p><div><img src="img/4351OS_03_33.jpg" alt="Understanding the binary and multiclass classification"/></div><p>Similarly, the distribution <a id="id225" class="indexterm"/>of the lightness of the skin of the fish from the sample data can be visualized through the following plot:</p><div><img src="img/4351OS_03_35.jpg" alt="Understanding the binary and multiclass classification"/></div><p>From the preceding graphs, we can say that only specifying the length of the fish is not enough information to determine its type. Thus, this feature has a smaller coefficient in the classification model. On the contrary, since the lightness of the skin of the fish plays a larger role in determining the type of the fish, this feature will have a larger coefficient in the parameter vector of the estimated classification model.</p><p>Once we have modeled a given classification problem, we can partition the training data into two (or more) sets. The surface in the vector space that partitions these two sets is called the <strong>decision boundary</strong><a id="id226" class="indexterm"/> of the formulated classification model. All the points on one side of the decision boundary are part of one class, while the points on the other side of the decision boundary are part<a id="id227" class="indexterm"/> of the other class. An obvious corollary is that depending on the number of distinct classes, a given classification model can have several such decision boundaries.</p><p>We can now combine these two features to train our model, and this produces an estimated decision boundary between the two categories of fish. This boundary can be visualized over a scatter plot of the training data as follows:</p><div><img src="img/4351OS_03_37.jpg" alt="Understanding the binary and multiclass classification"/></div><p>In the preceding plot, we approximate the classification model by using a straight line, and hence, we effectively model the classification as a linear function. We can alternatively model our data as a polynomial function, as it would produce a more accurate classification model. Such a model produces a decision boundary that can be visualized as follows:</p><div><img src="img/4351OS_03_39.jpg" alt="Understanding the binary and multiclass classification"/></div><p>The decision boundary <a id="id228" class="indexterm"/>partitions the sample data into two dimensions as shown in the preceding graphs. The decision boundary will become more complex to visualize when the sample data has a higher <a id="id229" class="indexterm"/>number of features or dimensions. For example, for three features, the decision boundary will be a three-dimensional surface, as shown in the following plot. Note that the sample data points are not shown for the sake of clarity. Also, two of the plotted features are assumed to vary within the range <img src="img/4351OS_03_41.jpg" alt="Understanding the binary and multiclass classification"/>, and the third feature is assumed to vary within the range <img src="img/4351OS_03_42.jpg" alt="Understanding the binary and multiclass classification"/>.</p><div><img src="img/4351OS_03_43.jpg" alt="Understanding the binary and multiclass classification"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Understanding the Bayesian classification</h1></div></div></div><p>We will now explore the<a id="id230" class="indexterm"/> Bayesian techniques that are used to classify data. A <strong>Bayes</strong> classifier<a id="id231" class="indexterm"/> is essentially a probabilistic classifier that is built using the Bayes' theorem of conditional probability. A model based on the Bayes classification assumes that the sample data has strongly independent features. By <em>independent</em>, we mean that every feature of the model can vary independent of the other features in the model. In other words, the features of the model are mutually exclusive. Thus, a Bayes classifier assumes that the presence or absence of a particular feature is completely independent of the presence or absence of the other features of the classification model.</p><p>The term <img src="img/4351OS_03_45.jpg" alt="Understanding the Bayesian classification"/> is used to represent the probability of occurrence of the condition or the feature <em>A</em>. Its value is always a fractional value within the range of <em>0</em> and <em>1</em>, both inclusive. It can also be represented as a percentage valve. For example, the probability <em>0.5</em> is also written as <em>50%</em> or <em>50 percent</em>. Let's assume that we want to find the probability of occurrence of a feature <em>A</em> or <img src="img/4351OS_03_46.jpg" alt="Understanding the Bayesian classification"/>, from a given number of samples. Thus, a higher value of <img src="img/4351OS_03_46.jpg" alt="Understanding the Bayesian classification"/> indicates a higher chance of occurrence of the feature <em>A</em>. We can formally represent the probability <img src="img/4351OS_03_46.jpg" alt="Understanding the Bayesian classification"/> as follows:</p><div><img src="img/4351OS_03_47.jpg" alt="Understanding the Bayesian classification"/></div><p>If <em>A</em> and <em>B</em> are two conditions or features in our classification model, then we use the term <img src="img/4351OS_03_48.jpg" alt="Understanding the Bayesian classification"/> to represent the occurrence of <em>A</em> when <em>B</em> is known to have occurred. This value is called the <strong>conditional probability</strong><a id="id232" class="indexterm"/> of <em>A</em> given <em>B</em>, and the term <img src="img/4351OS_03_48.jpg" alt="Understanding the Bayesian classification"/> is also read as the probability of <em>A</em> given <em>B</em>. In the term <img src="img/4351OS_03_48.jpg" alt="Understanding the Bayesian classification"/>, <em>B</em> is also called the evidence of <em>A</em>. In conditional probability, the two events, <em>A</em> and <em>B</em>, may or may not be independent of each other. However, if <em>A</em> and <em>B</em> are indeed independent conditions,<a id="id233" class="indexterm"/> then the probability <img src="img/4351OS_03_48.jpg" alt="Understanding the Bayesian classification"/> is equal to the product of the probabilities of the separate occurrences of <em>A</em> and <em>B</em>. We can express this axiom as follows:</p><div><img src="img/4351OS_03_49.jpg" alt="Understanding the Bayesian classification"/></div><p>Bayes' theorem describes a relation between the conditional probabilities, <img src="img/4351OS_03_48.jpg" alt="Understanding the Bayesian classification"/> and <img src="img/4351OS_03_50.jpg" alt="Understanding the Bayesian classification"/>, and the probabilities, <img src="img/4351OS_03_46.jpg" alt="Understanding the Bayesian classification"/> and <img src="img/4351OS_03_51.jpg" alt="Understanding the Bayesian classification"/>. It is formally expressed using the following equality:</p><div><img src="img/4351OS_03_52.jpg" alt="Understanding the Bayesian classification"/></div><p>Of course, the probabilities, <img src="img/4351OS_03_46.jpg" alt="Understanding the Bayesian classification"/> and <img src="img/4351OS_03_51.jpg" alt="Understanding the Bayesian classification"/>, must both be greater than <em>0</em> for the preceding relation to be true.</p><p>Let's revisit the classification example of the fish packaging plant that we described earlier. The problem is that we need to determine whether a fish is a sea bass or salmon depending on its physical features. We will now implement a solution to this problem using a Bayes classifier. Then, we will use Bayes' theorem to model our data.</p><p>Let's assume that each category of fish has three independent and distinct features, namely, the lightness of its skin <a id="id234" class="indexterm"/>and its length and width. Hence, our training data will look like that in the following table:</p><div><img src="img/4351OS_03_53.jpg" alt="Understanding the Bayesian classification"/></div><p>For simplicity in implementation, let's use the Clojure symbols to represent these features. We need to first generate the following data:</p><div><pre class="programlisting">(defn make-sea-bass []
  ;; sea bass are mostly long and light in color
  #{:sea-bass
    (if (&lt; (rand) 0.2) :fat :thin)
    (if (&lt; (rand) 0.7) :long :short)
    (if (&lt; (rand) 0.8) :light :dark)})

(defn make-salmon []
  ;; salmon are mostly fat and dark
  #{:salmon
    (if (&lt; (rand) 0.8) :fat :thin)
    (if (&lt; (rand) 0.5) :long :short)
    (if (&lt; (rand) 0.3) :light :dark)})

(defn make-sample-fish []
  (if (&lt; (rand) 0.3) (make-sea-bass) (make-salmon)))

(def fish-training-data
  (for [i (range 10000)] (make-sample-fish)))</pre></div><p>Here, we define two functions, <code class="literal">make-sea-bass</code> and <code class="literal">make-salmon</code>, to create a set of symbols to represent the two categories of fish. We conveniently use the<code class="literal">:salmon</code> and <code class="literal">:sea-bass</code> keywords to represent these two categories. Similarly, we can also use Clojure keywords to enumerate the features of a fish. In this example, the lightness of skin is either <code class="literal">:light</code> or <code class="literal">:dark</code>, the length is either <code class="literal">:long</code> or <code class="literal">:short</code>, and the width is either <code class="literal">:fat</code> or <code class="literal">:thin</code>. Also, we define the <code class="literal">make-sample-fish</code> function to randomly create a fish that is represented by the set of features defined earlier.</p><p>Note that we define these<a id="id235" class="indexterm"/> two categories of fish such that the sea bass are mostly long and light in skin color, and the salmon are mostly fat and dark. Also, we generate more salmon than sea bass in the <code class="literal">make-sample-fish</code> function<a id="id236" class="indexterm"/>. We add this partiality in our data only to provide more illustrative results, and the reader is encouraged to experiment with a more realistic distribution of data. The <em>Iris</em> dataset, which is available in the Incanter library that we introduced in <a class="link" href="ch02.html" title="Chapter 2. Understanding Linear Regression">Chapter 2</a>, <em>Understanding Linear Regression</em>, is an example of a real-world dataset that can be used to study classification.</p><p>Now, we will implement the following function to calculate the probability of a particular condition:</p><div><pre class="programlisting">(defn probability
  "Calculates the probability of a specific category
   given some attributes, depending on the training data."
  [attribute &amp; {:keys
                [category prior-positive prior-negative data]
                :or {category nil
                     data fish-training-data}}]
  (let [by-category (if category
                      (filter category data)
                      data)
        positive (count (filter attribute by-category))
        negative (- (count by-category) positive)
        total (+ positive negative)]
    (/ positive negative)))</pre></div><p>We essentially implement the basic definition of probability by the number of occurrences.</p><p>The <code class="literal">probability</code> function defined in the preceding code requires a single argument to represent the attribute or condition whose probability of occurrence we want to calculate. Also, the function accepts several optional arguments, such as the data to be used to calculate this value, which defaults to the <code class="literal">fish-training-data</code> sequence that we had defined earlier, and a category, which can be reasoned simply as another condition. The arguments, <code class="literal">category</code> and <code class="literal">attribute</code>, are in fact analogous to the conditions <em>A</em> and <em>B</em> in the <img src="img/4351OS_03_48.jpg" alt="Understanding the Bayesian classification"/> probability. The <code class="literal">probability</code> function determines the total positive occurrences of the condition by filtering the training data using the <code class="literal">filter</code> function. It then determines the number of negative occurrences by calculating the difference between the positive and total number of values represented by <code class="literal">(count by-category)</code>, in the sample data. The function finally returns the ratio of the positive occurrences of the condition to the total number of occurrences in the given data. </p><p>Let's use the <code class="literal">probability</code> function to tell us a bit about our training data as follows:</p><div><pre class="programlisting">user&gt; (probability :dark :category :salmon)
1204/1733
user&gt; (probability :dark :category :sea-bass)
621/3068
user&gt; (probability :light :category :salmon)
529/1733
user&gt; (probability :light :category :sea-bass)
2447/3068</pre></div><p>As shown in the preceding code,<a id="id237" class="indexterm"/> the probability that a salmon is dark in appearance is high, or specifically, <code class="literal">1204/1733</code>. The probabilities of a sea bass being dark and a salmon being light are also low when compared to the probabilities of a sea bass being light and a salmon being dark.</p><p>Let's assume that our observed values for the features of a fish are that it is dark-skinned, long, and fat. Given these conditions, we need to classify the fish as either a sea bass or a salmon. In terms of probability, we need to determine the probability that a fish is a salmon or a sea bass given that the fish is dark, long, and fat. Formally, this probability is represented by the terms <img src="img/4351OS_03_55.jpg" alt="Understanding the Bayesian classification"/> and <img src="img/4351OS_03_56.jpg" alt="Understanding the Bayesian classification"/> for either category of fish. If we calculate these two probabilities, we can select the category with the highest of these two probabilities to determine the category of the fish.</p><p>Using Bayes' theorem, we define the terms, <img src="img/4351OS_03_55.jpg" alt="Understanding the Bayesian classification"/> and <img src="img/4351OS_03_56.jpg" alt="Understanding the Bayesian classification"/>, as follows:</p><div><img src="img/4351OS_03_57.jpg" alt="Understanding the Bayesian classification"/></div><div><img src="img/4351OS_03_58.jpg" alt="Understanding the Bayesian classification"/></div><p>The terms, <img src="img/4351OS_03_55.jpg" alt="Understanding the Bayesian classification"/> and <img src="img/4351OS_03_59.jpg" alt="Understanding the Bayesian classification"/>, might seem a bit confusing, but the difference between these two terms is the order of occurrence of the specified conditions. The term, <img src="img/4351OS_03_60.jpg" alt="Understanding the Bayesian classification"/>, represents the probability that a fish that is dark, long, and fat is a salmon, while the term, <img src="img/4351OS_03_61.jpg" alt="Understanding the Bayesian classification"/>, represents the probability that a salmon is a dark, long, and fat fish.</p><p>The <img src="img/4351OS_03_62.jpg" alt="Understanding the Bayesian classification"/> probability can be calculated from the given training data as follows. As the three features of the fish are assumed to be mutually independent, the term, <img src="img/4351OS_03_62.jpg" alt="Understanding the Bayesian classification"/>, is simply the<a id="id238" class="indexterm"/> product of the probabilities of the occurrences of each individual feature. By mutually independent, we mean that the variance or distribution of these features does not depend on any of the other features of the classification model.</p><p>The term, <img src="img/4351OS_03_62.jpg" alt="Understanding the Bayesian classification"/>, is also called the <strong>evidence</strong> of the given category, which is the category "salmon" in this case. We can express the <img src="img/4351OS_03_62.jpg" alt="Understanding the Bayesian classification"/> probability as the product of the probabilities of the independent features of the model; this is shown as follows:</p><div><img src="img/4351OS_03_63.jpg" alt="Understanding the Bayesian classification"/></div><p>Interestingly, the terms, <img src="img/4351OS_03_64.jpg" alt="Understanding the Bayesian classification"/>, <img src="img/4351OS_03_65.jpg" alt="Understanding the Bayesian classification"/>, and <img src="img/4351OS_03_66.jpg" alt="Understanding the Bayesian classification"/>, can be easily calculated from the training data and the <code class="literal">probability</code> function<a id="id239" class="indexterm"/>, which we had implemented earlier. Similarly, we can find the probability that a fish is a salmon or <img src="img/4351OS_03_67.jpg" alt="Understanding the Bayesian classification"/>. Thus, the only term that's not accounted for in the definition of <img src="img/4351OS_03_60.jpg" alt="Understanding the Bayesian classification"/> is the term, <img src="img/4351OS_03_68.jpg" alt="Understanding the Bayesian classification"/>. We can actually avoid calculating this term altogether using a simple trick in probability.</p><p>Given that a fish is dark, long, and fat, it can either be a salmon or a sea bass. The two probabilities of occurrence of either category of fish are both complementary, that is, they both account for all the possible conditions that could occur in our model. In other words, these two probabilities both add up to a probability of <em>1</em>. Thus, we can formally express the term, <img src="img/4351OS_03_68.jpg" alt="Understanding the Bayesian classification"/>, as follows:</p><div><img src="img/4351OS_03_70.jpg" alt="Understanding the Bayesian classification"/></div><p>Both terms on the right-hand side of the preceding equality can be determined from the training data, which is similar to the terms, <img src="img/4351OS_03_67.jpg" alt="Understanding the Bayesian classification"/>, <img src="img/4351OS_03_71.jpg" alt="Understanding the Bayesian classification"/>, and so on. Hence, we can calculate the <img src="img/4351OS_03_60.jpg" alt="Understanding the Bayesian classification"/> probability directly from our training data. We express this probability through the following equality:</p><div><img src="img/4351OS_03_72.jpg" alt="Understanding the Bayesian classification"/></div><p>Now, let's implement the<a id="id240" class="indexterm"/> preceding equality using the training data and the <code class="literal">probability</code> function<a id="id241" class="indexterm"/>, which we defined earlier. Firstly, the evidence of a fish being a salmon, given that it's dark, long, and fat in appearance, can be expressed as follows:</p><div><pre class="programlisting">(defn evidence-of-salmon [&amp; attrs]
  (let [attr-probs (map #(probability % :category :salmon) attrs)
        class-and-attr-prob (conj attr-probs
                                  (probability :salmon))]
    (float (apply * class-and-attr-prob))))</pre></div><p>To be explicit, we implement a function to calculate the probability of the term, <img src="img/4351OS_03_73.jpg" alt="Understanding the Bayesian classification"/>, from the given training data. The equality of the terms, <img src="img/4351OS_03_62.jpg" alt="Understanding the Bayesian classification"/>, <img src="img/4351OS_03_64.jpg" alt="Understanding the Bayesian classification"/>, <img src="img/4351OS_03_65.jpg" alt="Understanding the Bayesian classification"/>, and <img src="img/4351OS_03_66.jpg" alt="Understanding the Bayesian classification"/> will be used as a base for this implementation.</p><p>In the preceding code, we determine the terms, <img src="img/4351OS_03_67.jpg" alt="Understanding the Bayesian classification"/> and <img src="img/4351OS_03_74.jpg" alt="Understanding the Bayesian classification"/>, for all the attributes or conditions of <em>i</em> by using the <code class="literal">probability</code> function. Then, we multiply all these terms using a composition of the <code class="literal">apply</code> and <code class="literal">*</code> functions. Since all the calculated probabilities are ratios returned by the <code class="literal">probability</code> function, we cast the final ratio to a floating-point value using the <code class="literal">float</code> function . We can try out this function in the REPL as follows:</p><div><pre class="programlisting">user&gt; (evidence-of-salmon :dark)
0.4816
user&gt; (evidence-of-salmon :dark :long)
0.2396884
user&gt; (evidence-of-salmon)
0.6932</pre></div><p>As the REPL output indicates, 48.16 percent of all the fish in the training data are salmon with dark skin. Similarly, 23.96 percent of all the fish are dark and long salmon, and 69.32 percent of all the fish are salmon. The value returned by the <code class="literal">(evidence-of-salmon :dark :long)</code> call can be expressed as <img src="img/4351OS_03_75.jpg" alt="Understanding the Bayesian classification"/>, and similarly, <img src="img/4351OS_03_76.jpg" alt="Understanding the Bayesian classification"/> is returned by <code class="literal">(evidence-of-salmon)</code>.</p><p>Similarly, we can define the <code class="literal">evidence-of-sea-bass</code> function<a id="id242" class="indexterm"/> that determines the evidence of occurrence of<a id="id243" class="indexterm"/> a sea bass given some observed features of the fish. As we are dealing with only two categories, <img src="img/4351OS_03_77.jpg" alt="Understanding the Bayesian classification"/>, we can easily verify this equality in the REPL. Interestingly, a small error is observed, but this error is not related to the training data. This small error is, in fact, a floating-point rounding error, which arises due to the limitations of floating-point numbers. In practice, we can avoid this using the decimal or <code class="literal">BigDecimal</code> (from <code class="literal">java.lang</code>) data types, instead of floating-point numbers. We can verify this using the <code class="literal">evidence-of-sea-bass</code> and <code class="literal">evidence-of-salmon</code> functions in the REPL as follows:</p><div><pre class="programlisting">user&gt; (+ (evidence-of-sea-bass) (evidence-of-salmon))
1.0000000298023224</pre></div><p>We can generalize <a id="id244" class="indexterm"/>the <code class="literal">evidence-of-salmon</code> and <code class="literal">evidence-of-sea-bass</code> functions such that we are able to determine the probability of any category with some observed features; this is shown in the following code:</p><div><pre class="programlisting">(defn evidence-of-category-with-attrs
  [category &amp; attrs]
  (let [attr-probs (map #(probability % :category category) attrs)
        class-and-attr-prob (conj attr-probs
                                  (probability category))]
    (float (apply * class-and-attr-prob))))</pre></div><p>The function defined in the preceding code returns values that agree with those returned by the following <code class="literal">evidence-of-salmon</code> and <code class="literal">evidence-of-sea-bass</code> functions:</p><div><pre class="programlisting">user&gt; (evidence-of-salmon :dark :fat)
0.38502988
user&gt; (evidence-of-category-with-attrs :salmon :dark :fat)
0.38502988</pre></div><p>Using the <code class="literal">evidence-of-salmon</code> <a id="id245" class="indexterm"/>and <code class="literal">evidence-of-sea-bass</code> functions<a id="id246" class="indexterm"/>, we can calculate the probability in terms of <code class="literal">probability-dark-long-fat-is-salmon</code> as follows:</p><div><pre class="programlisting">(def probability-dark-long-fat-is-salmon
  (let [attrs [:dark :long :fat]
        sea-bass? (apply evidence-of-sea-bass attrs)
        salmon? (apply evidence-of-salmon attrs)]
    (/ salmon?
       (+ sea-bass? salmon?))))</pre></div><p>We can inspect the <code class="literal">probability-dark-long-fat-is-salmon</code> value in the REPL as follows:</p><div><pre class="programlisting">user&gt; probability-dark-long-fat-is-salmon
0.957091799207812</pre></div><p>The <code class="literal">probability-dark-long-fat-is-salmon</code> value indicates that a fish that is dark, long, and fat and has a 95.7 percent probability of being a salmon.</p><p>Using the preceding definition of the <code class="literal">probability-dark-long-fat-is-salmon</code> function as a template, we can generalize the calculations that it performs. Let's first define a simple data structure<a id="id247" class="indexterm"/> that can be passed around. In the spirit of idiomatic Clojure, we conveniently use a map for this purpose. Using a map, we can represent a category in our model along with the evidence and probability of its occurrence. Also, given the evidences for several categories, we can calculate the total probability of occurrence of a particular category as shown in the following code:</p><div><pre class="programlisting">(defn make-category-probability-pair
  [category attrs]
  (let [evidence-of-category (apply
  evidence-of-category-with-attrs
                              category attrs)]
    {:category category
     :evidence evidence-of-category}))

(defn calculate-probability-of-category
  [sum-of-evidences pair]
  (let [probability-of-category (/ (:evidence pair)
                                   sum-of-evidences)]
    (assoc pair :probability probability-of-category)))</pre></div><p>The <code class="literal">make-category-probability-pair</code> function uses the <code class="literal">evidence-category-with-attrs</code> function<a id="id248" class="indexterm"/> we defined in the preceding code to calculate the evidence of a category and its conditions or attributes. Then, it returns this value, as a map, along with the category itself. Also, we define the <code class="literal">calculate-probability-of-category</code> function, which calculates the total probability of a category and its conditions using the <code class="literal">sum-of-evidences</code> parameter<a id="id249" class="indexterm"/> and a value returned by the <code class="literal">make-category-probability-pair</code> function.</p><p>We can compose the preceding two functions to determine the total probability of all the categories given some observed values and then select the category with the highest probability, as follows:</p><div><pre class="programlisting">(defn classify-by-attrs
  "Performs Bayesian classification of the attributes,
   given some categories.
   Returns a map containing the predicted category and
   the category's
   probability of occurrence."
  [categories &amp; attrs]
  (let [pairs (map #(make-category-probability-pair % attrs)
                   categories)
        sum-of-evidences (reduce + (map :evidence pairs))
        probabilities (map #(calculate-probability-of-category
                              sum-of-evidences %)
                           pairs)
        sorted-probabilities (sort-by :probability probabilities)
        predicted-category (last sorted-probabilities)]
    predicted-category))</pre></div><p>The <code class="literal">classify-by-attrs</code> function defined in the preceding code maps all the possible categories over the <code class="literal">make-category-probability-pair</code> function<a id="id250" class="indexterm"/>, given some conditions or observed values for the features of our model. As we are dealing with a sequence of pairs returned by <code class="literal">make-category-probability-pair</code>, we can use a simple composition of the <code class="literal">reduce</code>, <code class="literal">map</code>, and <code class="literal">+</code> functions to calculate the sum of all the evidence in this sequence. <a id="id251" class="indexterm"/>We then map the <code class="literal">calculate-probability-of-category</code> function<a id="id252" class="indexterm"/> over the sequence of category-evidence pairs and select the category-evidence pair with the highest probability. We do this by sorting the sequence through ascending probabilities and selecting the last element in the sorted sequence.</p><p>Now, we can use the <code class="literal">classify-by-attrs</code> function<a id="id253" class="indexterm"/> to determine the probability that an observed fish, which is dark, long, and fat, is a salmon. It is also represented by the <code class="literal">probability-dark-long-fat-is-salmon</code> value, which we defined earlier. Both expressions produce the same probability of 95.7 percent of a fish being a salmon, given that it's dark, long, and fat in appearance. We will implement the <code class="literal">classify-by-attrs</code> function as shown in the following code:</p><div><pre class="programlisting">user&gt; (classify-by-attrs [:salmon :sea-bass] :dark :long :fat)
{:probability 0.957091799207812, :category :salmon, :evidence 0.1949689}
user&gt; probability-dark-long-fat-is-salmon
0.957091799207812</pre></div><p>The <code class="literal">classify-by-attrs</code> function also returns the predicted category (that is, <code class="literal">:salmon</code>) of the given observed conditions <code class="literal">:dark</code>, :<code class="literal">long</code>, and <code class="literal">:fat</code>. We can use this function to tell us more about the training data as follows:</p><div><pre class="programlisting">user&gt; (classify-by-attrs [:salmon :sea-bass] :dark)
{:probability 0.8857825967670728, :category :salmon, :evidence 0.4816}
user&gt; (classify-by-attrs [:salmon :sea-bass] :light)
{:probability 0.5362699908806723, :category :sea-bass, :evidence 0.2447}
user&gt; (classify-by-attrs [:salmon :sea-bass] :thin)
{:probability 0.6369809383442954, :category :sea-bass, :evidence 0.2439}</pre></div><p>As shown in the preceding code, a fish that is dark in appearance is mostly a salmon, and the one that is light in appearance is mostly a sea bass. Also, a fish that's thin is most likely a sea bass. The following values do, in fact, agree with the training data that we defined earlier:</p><div><pre class="programlisting">user&gt; (classify-by-attrs [:salmon] :dark)
{:probability 1.0, :category :salmon, :evidence 0.4816}
user&gt; (classify-by-attrs [:salmon])
{:probability 1.0, :category :salmon, :evidence 0.6932}</pre></div><p>Note that calling the <code class="literal">classify-by-attrs</code> function<a id="id254" class="indexterm"/> with only <code class="literal">[:salmon]</code> as a parameter returns the probability that any<a id="id255" class="indexterm"/> given fish is a salmon. An obvious corollary is that given a single category, the <code class="literal">classify-by-attrs</code> function always predicts the supplied category with complete certainty, that is, a probability of <em>1.0</em>. However, the evidence returned by this function vary depending on the observed features passed to it as well as the sample data that we used to train our model.</p><p>In a nutshell, the preceding implementation describes a Bayes classifier that can be trained using some sample data. It also classifies some observed values for the features of our model.</p><p>We can describe a generic Bayes classifier by building upon the definition of the <img src="img/4351OS_03_60.jpg" alt="Understanding the Bayesian classification"/> probability from our previous example. To quickly recap, the term <img src="img/4351OS_03_60.jpg" alt="Understanding the Bayesian classification"/> can be formally expressed as follows:</p><div><img src="img/4351OS_03_72.jpg" alt="Understanding the Bayesian classification"/></div><p>In the preceding equality, we deal with a single class, namely salmon, and three mutually independent features, namely the length, width, and lightness of the skin of a fish. We can generalize this equality for <em>N</em> features as follows:</p><div><img src="img/4351OS_03_78.jpg" alt="Understanding the Bayesian classification"/></div><p>Here, the term <em>Z</em> is the evidence of the classification model, which we described in the preceding equation. We can <a id="id256" class="indexterm"/>use the sum and product notation to describe the preceding equality more concisely as follows:</p><div><img src="img/4351OS_03_79.jpg" alt="Understanding the Bayesian classification"/></div><p>The preceding equality describes the probability of occurrence of a single class, <em>C</em>. If we are given a number of classes to choose from, we must select the class with the highest probability of occurrence. This brings us to the basic definition of a Bayes classifier, which is formally expressed as follows:</p><div><img src="img/4351OS_03_80.jpg" alt="Understanding the Bayesian classification"/></div><p>In the preceding equation, the function <img src="img/4351OS_03_81.jpg" alt="Understanding the Bayesian classification"/> describes a Bayes classifier that selects the class with the highest probability of occurrence. Note that the terms <img src="img/4351OS_03_82.jpg" alt="Understanding the Bayesian classification"/> represent the various features of our classification model, whereas the terms <img src="img/4351OS_03_83.jpg" alt="Understanding the Bayesian classification"/> represent the set of observed values of these features. Also, the variable <em>c</em>, on the right-hand side of the equation, can have values from within a set of all the distinct classes in the classification model.</p><p>We can further simplify the preceding equation of a Bayes classifier via the <strong>Maximum a Posteriori</strong> (<strong>MAP</strong>) estimation,<a id="id257" class="indexterm"/> which can be thought of as a regularization of the features in Bayesian statistics. A simplified Bayes classifier can be formally expressed as follows:</p><div><img src="img/4351OS_03_84.jpg" alt="Understanding the Bayesian classification"/></div><p>This definition essentially means that the <em>classify</em> function determines the class with the maximum probability of occurrence for the given features. Thus, the preceding equation describes a Bayes classifier that<a id="id258" class="indexterm"/> can be trained using some sample data and then be used to predict the class of a given set of observed values. We will now focus on using an existing implementation of a Bayes classifier to model a given classification problem.</p><p>The <code class="literal">clj-ml</code> library<a id="id259" class="indexterm"/> (<a class="ulink" href="https://github.com/joshuaeckroth/clj-ml">https://github.com/joshuaeckroth/clj-ml</a>) contains several implemented algorithms that we can choose from to model a given classification problem. This library is actually just a Clojure wrapper for the popular <strong>Weka</strong> library<a id="id260" class="indexterm"/> (<a class="ulink" href="http://www.cs.waikato.ac.nz/ml/weka/">http://www.cs.waikato.ac.nz/ml/weka/</a>), which is a Java library that contains implementations for several machine learning algorithms. It also has several methods for evaluating and validating a generated classification model. However, we will concentrate on the <code class="literal">clj-ml</code> library's implementations of classifiers within the context of this chapter.</p><div><div><h3 class="title"><a id="note17"/>Note</h3><p>The <code class="literal">clj-ml</code> library can be added to a Leiningen project by adding the following dependency to the <code class="literal">project.clj</code> file:</p><div><pre class="programlisting">[cc.artifice/clj-ml "0.4.0"]</pre></div><p>For the upcoming example, the namespace declaration should look similar to the following declaration:</p><div><pre class="programlisting">(ns my-namespace
  (:use [clj-ml classifiers data]))</pre></div></div></div><p>We'll now introduce the <code class="literal">clj-ml</code> library using an implementation of a Bayes classifier to model our previous problem involving the fish packaging plant. First, let's refine our training data to use numeric values rather than the keywords, which we described earlier, for the various features of our model. Of course, we will still maintain the partiality in our training data such that the salmon are mostly fat and dark-skinned, while the sea bass are mostly long and light-skinned. The following code implements this:</p><div><pre class="programlisting">(defn rand-in-range
  "Generates a random integer within the given range"
  [min max]
  (let [len      (- max min)
        rand-len (rand-int len)]
    (+ min rand-len)))

;; sea bass are mostly long and light in color
(defn make-sea-bass []
  (vector :sea-bass
          (rand-in-range 6 10)          ; length
          (rand-in-range 0 5)           ; width
          (rand-in-range 4 10)))        ; lightness of skin

;; salmon are mostly fat and dark
(defn make-salmon []
  (vector :salmon
          (rand-in-range 0 7)           ; length
          (rand-in-range 4 10)          ; width
          (rand-in-range 0 6)))         ; lightness of skin</pre></div><p>Here, we define the <code class="literal">rand-in-range</code> function<a id="id261" class="indexterm"/>, which simply generates random integers within a given range of values. We then redefine the <code class="literal">make-sea-bass</code> and <code class="literal">make-salmon</code> functions to use the <code class="literal">rand-in-range</code> function to generate values within the range of <code class="literal">0</code> and <code class="literal">10</code> for the three features <a id="id262" class="indexterm"/>of a fish, namely its length, width, and the darkness of its skin. A fish with a lighter skin color is indicated by a higher value for this feature. Note that we reuse the definitions of the <code class="literal">make-sample-fish</code> function<a id="id263" class="indexterm"/> and <code class="literal">fish-dataset</code> variable to generate our training data. Also, a fish is represented by a vector rather than a set, as described in the earlier definitions of the <code class="literal">make-sea-bass</code> and <code class="literal">make-salmon</code> functions.</p><p>We can create a classifier from the <code class="literal">clj-ml</code> library using the <a id="id264" class="indexterm"/>
<code class="literal">make-classifier</code> function, which can be found in the <code class="literal">clj-ml.classifiers</code> namespace. We can specify the type of classifier to be used by passing two keywords as arguments to the functions. As we intend to use a Bayes classifier, we supply the keywords, <code class="literal">:bayes</code> and <code class="literal">:naive</code>, to the <code class="literal">make-classifier</code> function. In a nutshell, we can use the following declaration to create a Bayes classifier. Note that the keyword, <code class="literal">:naive</code>, used in the following code signifies a naïve Bayes classifier that assumes that the features in our model are independent:</p><div><pre class="programlisting">(def bayes-classifier (make-classifier :bayes :naive))</pre></div><p>The <code class="literal">clj-ml</code> library's classifier implementations use datasets that are defined or generated using functions from the <code class="literal">clj-ml.data</code> namespace. We can convert the <code class="literal">fish-dataset</code> sequence, which is a sequence of vectors, into such a dataset using the <code class="literal">make-dataset</code> function. This function requires an arbitrary string name for the dataset, a template for each item in the collection, and a collection of items to add to the dataset. The template supplied to the <code class="literal">make-dataset</code> function<a id="id265" class="indexterm"/> is easily represented by a map, which is shown as follows:</p><div><pre class="programlisting">(def fish-template
  [{:category [:salmon :sea-bass]}
   :length :width :lightness])

(def fish-dataset
  (make-dataset "fish" fish-template fish-training-data))</pre></div><p>The <code class="literal">fish-template</code> map defined in the preceding code simply says that a fish, as represented by a vector, comprises<a id="id266" class="indexterm"/> the fish's category, length, width, and lightness of its skin, in that specific order. Note that the category of the fish is described using either <code class="literal">:salmon</code> or <code class="literal">:sea-bass</code>. We can now use <code class="literal">fish-dataset</code> to train the classifier represented by the<a id="id267" class="indexterm"/> <code class="literal">bayes-classifier</code> variable.</p><p>Although the <code class="literal">fish-template</code> map defines all the attributes of a fish, it's still lacking one important detail. It doesn't specify which of these attributes represent the class or category of the fish. In order to specify a particular attribute in the vector to represent the category of an entire set of observed values, we use the <code class="literal">dataset-set-class</code> function. This function takes a single argument that specifies the index of an attribute and is used to represent the category of the set of observed values in the vector. Note that this function does actually mutate or modify the dataset it's supplied with. We can then train our classifier using the <code class="literal">classifier-train</code> function<a id="id268" class="indexterm"/>, which takes a classifier and a dataset as parameters; this is shown in the following code:</p><div><pre class="programlisting">(defn train-bayes-classifier []
  (dataset-set-class fish-dataset 0)
  (classifier-train bayes-classifier fish-dataset))</pre></div><p>The preceding <code class="literal">train-bayes-classifier</code> function<a id="id269" class="indexterm"/> simply calls the <code class="literal">dataset-set-class</code> and <code class="literal">classifier-train</code> functions to train our classifier. When we call the <code class="literal">train-bayes-classifier</code> function, the classifier is trained with the following supplied data and then printed to the REPL output:</p><div><pre class="programlisting">user&gt; (train-bayes-classifier)
#&lt;NaiveBayes Naive Bayes Classifier

                     Class
Attribute        salmon  sea-bass
                  (0.7)    (0.3)
=================================
length
  mean            2.9791   7.5007
  std. dev.       1.9897   1.1264
  weight sum        7032     2968
  precision            1        1

width
  mean            6.4822   1.9747
  std. dev.        1.706    1.405
  weight sum        7032     2968
  precision            1        1

lightness
  mean            2.5146   6.4643
  std. dev.       1.7047   1.7204
  weight sum        7032     2968
  precision            1        1

&gt;</pre></div><p>This output gives us some basic information about the training data, such as the mean and standard deviation of the various features that we model. We can now use this trained classifier to predict the category of a set of observed values for the features of our model.</p><p>Let's first define the observed values that we intend to classify. To do so, we use the following <code class="literal">make-instance</code> function, which requires a dataset and a vector of observed values that agree with the data template of the supplied dataset:</p><div><pre class="programlisting">(def sample-fish
  (make-instance fish-dataset [:salmon 5.0 6.0 3.0]))</pre></div><p>Here, we simply defined a<a id="id270" class="indexterm"/> sample fish using the <code class="literal">make-instance</code> function<a id="id271" class="indexterm"/>. We can now predict the class of the fish represented by <code class="literal">sample-fish</code> as follows:</p><div><pre class="programlisting">user&gt; (classifier-classify bayes-classifier sample-fish)
:salmon</pre></div><p>As shown in the preceding code, the fish is classified as a <code class="literal">salmon</code>. Note that although we provide the class of the fish as <code class="literal">:salmon</code> while defining <code class="literal">sample-fish</code>, it's only for conformance with the data template defined by <code class="literal">fish-dataset</code>. In fact, we could specify the class of <code class="literal">sample-fish</code> as <code class="literal">:sea-bass</code> or a third value, say <code class="literal">:unknown</code>, to represent an undefined value, and the classifier would still classify <code class="literal">sample-fish</code> as a <code class="literal">salmon</code>.</p><p>When dealing with the continuous values for the various features of the given classification model, we can specify a Bayes classifier to use discretization of continuous features. By this, we mean that all the values for the various features of the model will be converted to discrete values by the probability density estimation. We can specify this option to the <code class="literal">make-classifier</code> function<a id="id272" class="indexterm"/> by simply passing an extra argument, <code class="literal">{:supervised-discretization true}</code>, to the function. This map actually describes all the possible options that can be provided to the specified classifier.</p><p>In conclusion, the <code class="literal">clj-ml</code> library provides<a id="id273" class="indexterm"/> a fully operational Bayes classifier that we can use to classify arbitrary data. Although we generated the training data ourselves in the previous example, this data can be fetched from the Web or a database as well.</p></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Using the k-nearest neighbors algorithm</h1></div></div></div><p>A simple technique that can be used to classify a set of observed values is the <strong>k-nearest neighbors</strong> (abbreviated as <strong>k-NN</strong>) algorithm. This algorithm is a form of <strong>lazy learning</strong> in which all the computation is deferred until classification. Also, in the classification phase, the k-NN algorithm approximates the class of the observed values using only a few values from the training data, and the reading of other values is deferred until they are actually needed.</p><p>While we now explore the k-NN algorithm<a id="id274" class="indexterm"/> in the context of classification, it can be applied to regression as well by simply selecting the predicted value as the average of the nearest values of the dependent variable for a set of observed feature values. Interestingly, this technique of modeling regression is, in fact, a generalization of <strong>linear interpolation</strong><a id="id275" class="indexterm"/> (for more information, refer to <em>An introduction to kernel and nearest-neighbor nonparametric regression</em>).</p><p>The k-NN algorithm reads<a id="id276" class="indexterm"/> some training data and analyzes this data lazily, that is, only when needed. Apart from the training data, the algorithm requires a set of observed values and a constant <em>k</em> as parameters to classify the set of observed values. To classify these observed values, the algorithm predicts the class that is the most frequent among the <em>k</em> training samples nearest to the set of observed values. By nearest, we mean a point with the least Euclidean distance from the point that is represented by a set of observed values in the Euclidean space of the training data.</p><p>An obvious corollary is that when <img src="img/4351OS_03_85.jpg" alt="Using the k-nearest neighbors algorithm"/>, the predicted class is the class of the single neighbor nearest to the set of observed values. This special case of the k-NN algorithm<a id="id277" class="indexterm"/> is called the <a id="id278" class="indexterm"/>
<strong>nearest neighbor</strong> algorithm.</p><p>We can create a classifier that uses the k-NN algorithm using the <code class="literal">clj-ml</code> library's <code class="literal">make-classifier</code> function. Such a classifier is specified using the keywords <code class="literal">:lazy</code> and <code class="literal">:ibk</code> as arguments to the <code class="literal">make-classifier</code> function. We will now use such a classifier to model our previous example of a fish packaging plant, as follows:</p><div><pre class="programlisting">(def K1-classifier (make-classifier :lazy :ibk))

(defn train-K1-classifier []
  (dataset-set-class fish-dataset 0)
  (classifier-train K1-classifier fish-dataset))</pre></div><p>The preceding code defines a k-NN classifier as <code class="literal">K1-classifier</code> and a <code class="literal">train-K1-classifier</code> function<a id="id279" class="indexterm"/> to train the classifier with the training data using <code class="literal">fish-dataset</code>, which we defined in the preceding code.</p><p>Note that the <code class="literal">make-classifier</code> function defaults the constant <em>k</em> or rather the number of neighbors to <em>1</em>, which implies a single nearest neighbor. We can optionally specify the constant <em>k</em> as a key-value pair with the<code class="literal">:num-neighbors</code> key to the <code class="literal">make-classifier</code> function as shown in the following code:</p><div><pre class="programlisting">(def K10-classifier (make-classifier
                     :lazy :ibk {:num-neighbors 10}))</pre></div><p>We can now call the <code class="literal">train-K1-classifier</code> function to train the classifier as follows:</p><div><pre class="programlisting">user&gt; (train-K1-classifier)
#&lt;IBk IB1 instance-based classifier
using 1 nearest neighbour(s) for classification
&gt;</pre></div><p>We can now use the <code class="literal">classifier-classify</code> function<a id="id280" class="indexterm"/> to classify the fish represented by <code class="literal">sample-fish</code>, which we had defined earlier, using the classifier represented by the <code class="literal">K1-classifier</code> variable:</p><div><pre class="programlisting">user&gt; (classifier-classify K1-classifier sample-fish)
:salmon</pre></div><p>As shown in the preceding code, the k-NN classifier predicts the fish class as salmon, thus agreeing with our earlier predictions that used a Bayes classifier. In conclusion, the <code class="literal">clj-ml</code> library provides a concise implementation of a classifier that uses the k-NN algorithm to predict the class of a set of observed values.</p><p>The k-NN classifier provided by the <code class="literal">clj-ml</code> library performs the normalization of the features of the classification model by default using the mean and standard deviation of the values of these features.<a id="id281" class="indexterm"/> We can specify an option to the <code class="literal">make-classifier</code> function to skip this normalization phase by passing a map entry with the<code class="literal">:no-normalization</code> key in the map of options passed to the <code class="literal">make-classifier</code> function.</p></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Using decision trees</h1></div></div></div><p>We can also use decision trees<a id="id282" class="indexterm"/> to model a given classification problem. A decision tree is, in fact, constructed from the given training data, and we can use this decision tree to predict the class of a given set of observed values. The process of constructing a decision tree is loosely based on the concepts of information entropy and information gain from information theory (for more information, refer to <em>Induction of Decision Trees</em>). It is also often termed as <strong>decision tree learning</strong><a id="id283" class="indexterm"/>. Unfortunately, a detailed study of information theory is beyond the scope of this book. However, in this section, we will explore some concepts in information theory that will be used in the context of machine learning.</p><p>A decision tree is a tree or graph that describes a model of decisions and their possible consequences. An internal node in a decision tree represents a decision, or rather a condition of a particular feature in the context of classification. It has two possible outcomes that are represented by the left and right subtrees of the node. Of course, a node in the decision tree could also have more than two subtrees. Each leaf node in a decision tree represents a particular class, or a consequence, in our classification model.</p><p>For example, our previous classification problem involving the fish packaging plant could have the following decision tree:</p><div><img src="img/4351OS_03_86.jpg" alt="Using decision trees"/></div><p>The previously illustrated decision tree uses two conditions to classify a fish as either a salmon or a sea bass. The internal nodes represent the two conditions based on the features of our classification model. Note that the decision tree uses only two of the three features of our classification model. We can thus say the tree is <em>pruned</em>. We shall briefly explore this technique as well in this section.</p><p>To classify a set of observed <a id="id284" class="indexterm"/>values using a decision tree, we traverse the tree starting from the root node until we reach a leaf node that represents the predicted class of the set of observed values. This technique of predicting the class of a set of observed values from a decision tree is always the same, irrespective of how the decision tree was constructed. For the decision tree described earlier, we can classify a fish by first comparing its length followed by the lightness of its skin. The second comparison is only needed if the length of the fish is greater than <strong>6</strong> as specified by the internal node with the expression <strong>Length &lt; 6</strong> in the decision tree. If the length of the fish is indeed greater than <strong>6</strong>, we use the lightness of the skin of the fish to decide whether it's a salmon or a sea bass.</p><p>There are actually several algorithms that are used to construct a decision tree from some training data. Generally, the tree is constructed by splitting the set of sample values in the training data into smaller subsets based on an attribute value test. The process is repeated on each subset until splitting a given subset of sample values no longer adds internal nodes to the decision tree. As we mentioned earlier, it's possible for an internal node in a decision tree to have more than two subtrees.</p><p>We will now explore the <strong>C4.5</strong> algorithm to construct a decision tree (for more information, refer to <em>C4.5: Programs for Machine Learning</em>). This algorithm uses the concept of information entropy to decide the feature and the corresponding value on which the set of sample values must be partitioned. <strong>Information entropy</strong><a id="id285" class="indexterm"/> is defined as the measure of uncertainty in a given feature or random variable (for more information, refer to "A Mathematical Theory of Communication").</p><p>For a given feature or <a id="id286" class="indexterm"/>attribute <em>f</em>, which has values within the range of <em>1</em> to <em>m</em>, we can define the information entropy of the <img src="img/4351OS_03_88.jpg" alt="Using decision trees"/> feature as follows:</p><div><img src="img/4351OS_03_89.jpg" alt="Using decision trees"/></div><p>In the preceding equation, the term <img src="img/4351OS_03_90.jpg" alt="Using decision trees"/> represents the number of occurrences of the feature <em>f</em> with respect to the value <em>i</em>. Based on this definition of the information entropy of a feature, we define the normalized information gain <img src="img/4351OS_03_91.jpg" alt="Using decision trees"/>. In the following equality, the term <em>T</em> refers to the set of sample values or training data supplied to the algorithm:</p><div><img src="img/4351OS_03_92.jpg" alt="Using decision trees"/></div><p>In terms of information entropy, the preceding definition of the information gain of a given attribute is the change in information entropy of the total set of values when the attribute <em>f</em> is removed from the given set of features in the model.</p><p>The algorithm selects a feature <em>A</em> from the given set of features in the training data such that the feature <em>A</em> has the maximum possible information gain in the set of features. We can represent this with the help of the following equality:</p><div><img src="img/4351OS_03_93.jpg" alt="Using decision trees"/></div><p>In the preceding equation, <img src="img/4351OS_03_94.jpg" alt="Using decision trees"/> represents the set of all the possible values that a feature <em>a</em> is known to have. The <img src="img/4351OS_03_95.jpg" alt="Using decision trees"/> set represents the observed values in which the feature <em>a</em> has the value <em>v</em> and the term <img src="img/4351OS_03_96.jpg" alt="Using decision trees"/> represents the information entropy of this set of values.</p><p>Using the preceding equation to select a feature with the maximum information gain from the training data, we can describe the C4.5 algorithm through the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">For each feature <em>a</em>, find the normalized information gain from partitioning the sample data on the feature <em>a</em>.</li><li class="listitem">Select the feature <em>A</em> with the maximum normalized information gain.</li><li class="listitem">Create an internal decision node based on the selected feature <em>A</em>. Both the subtrees created from this step are either leaf nodes or a new set of sample values to be partitioned further.</li><li class="listitem">Repeat this process on each partitioned set of sample values produced from the previous step. We repeat the preceding steps until all the features in a subset of sample values have the same information entropy.</li></ol></div><p>Once a decision tree has been created,<a id="id287" class="indexterm"/> we can optionally perform <strong>pruning</strong> on the tree. Pruning is simply the process of removing any extraneous decision nodes from the tree. This can be thought of as a form for the regularization of decision trees through which we prevent underfitting or overfitting of the estimated decision tree model.</p><p>
<strong>J48</strong> is an open source implementation of the C4.5 algorithm in Java, and the <code class="literal">clj-ml</code> library contains a working J48 decision tree classifier. We can create a decision tree classifier using the <code class="literal">make-classifier</code> function, and we supply the keywords <code class="literal">:decision-tree</code> and <code class="literal">:c45</code> as parameters to this function to create a J48 classifier as shown in the following code:</p><div><pre class="programlisting">(def DT-classifier (make-classifier :decision-tree :c45))

(defn train-DT-classifier []
  (dataset-set-class fish-dataset 0)
  (classifier-train DT-classifier fish-dataset))</pre></div><p>The <code class="literal">train-DT-classifier</code> function<a id="id288" class="indexterm"/> defined in the preceding code simply trains the classifier represented by <code class="literal">DT-classifier</code> with the training data from our previous example of the fish packaging plant. The <code class="literal">classifier-train</code> function also prints the following trained classifier:</p><div><pre class="programlisting">user&gt; (train-DT-classifier)
#&lt;J48 J48 pruned tree
------------------
width &lt;= 3: sea-bass (2320.0)
width &gt; 3
|   length &lt;= 6
|   |   lightness &lt;= 5: salmon (7147.0/51.0)
|   |   lightness &gt; 5: sea-bass (95.0)
|   length &gt; 6: sea-bass (438.0)

Number of Leaves  : 4

Size of the tree : 7
&gt;</pre></div><p>The preceding output gives a good idea of what the decision tree of the trained classifier looks like as well as the size and number of leaf nodes in the decision tree. Apparently, the decision tree has three distinct internal nodes. The root node of the tree is based on the width of a fish, the subsequent node is based on the length of a fish, and the last decision node is based on the lightness of the skin of a fish.</p><p>We can now use the decision tree classifier to predict the class of a fish, and we use the following <code class="literal">classifier-classify</code> function<a id="id289" class="indexterm"/> to perform this classification:</p><div><pre class="programlisting">user&gt; (classifier-classify DT-classifier sample-fish)
:salmon</pre></div><p>As shown in the preceding code, the classifier predicts the class of the fish represented by <code class="literal">sample-fish</code> as a <code class="literal">:salmon</code> keyword just like the other classifiers used in the earlier examples.</p><p>The J48 decision tree classifier<a id="id290" class="indexterm"/> implementation provided by the <code class="literal">clj-ml</code> library performs pruning as a final step while training the classifier. We can generate an unpruned tree by specifying the <code class="literal">:unpruned</code> key in the map of options passed to the <code class="literal">make-classifier</code> function<a id="id291" class="indexterm"/> as shown in the following code:</p><div><pre class="programlisting">(def UDT-classifier (make-classifier
                     :decision-tree :c45 {:unpruned true}))</pre></div><p>The previously defined classifier will not perform pruning on the decision tree generated from training the classifier with the given training data. We can inspect what an unpruned tree looks like by defining and calling the <code class="literal">train-UDT-classifier</code> function<a id="id292" class="indexterm"/>, which simply trains the classifier using the <code class="literal">classifier-train</code> function<a id="id293" class="indexterm"/> with the <code class="literal">fish-dataset</code> training data. This function can be defined as being analogous to the <code class="literal">train-UDT-classifier</code> function and produces the following output when it is called:</p><div><pre class="programlisting">user&gt; (train-UDT-classifier)
#&lt;J48 J48 unpruned tree
------------------
width &lt;= 3: sea-bass (2320.0)
width &gt; 3
|   length &lt;= 6
|   |   lightness &lt;= 5
|   |   |   length &lt;= 5: salmon (6073.0)
|   |   |   length &gt; 5
|   |   |   |   width &lt;= 4
|   |   |   |   |   lightness &lt;= 3: salmon (121.0)
|   |   |   |   |   lightness &gt; 3
|   |   |   |   |   |   lightness &lt;= 4: salmon (52.0/25.0)
|   |   |   |   |   |   lightness &gt; 4: sea-bass (50.0/24.0)
|   |   |   |   width &gt; 4: salmon (851.0)
|   |   lightness &gt; 5: sea-bass (95.0)
|   length &gt; 6: sea-bass (438.0)

Number of Leaves  : 8

Size of the tree : 15</pre></div><p>As shown in the preceding code, the unpruned decision tree has a lot more internal decision nodes as compared to the decision tree that is generated after pruning it. We can now use the following <code class="literal">classifier-classify</code> function<a id="id294" class="indexterm"/> to predict the class of a fish using the trained classifier:</p><div><pre class="programlisting">user&gt; (classifier-classify UDT-classifier sample-fish)
:salmon</pre></div><p>Interestingly, the unpruned tree also predicts the class of the fish represented by <code class="literal">sample-fish</code> as <code class="literal">:salmon</code>, thus agreeing<a id="id295" class="indexterm"/> with the class predicted by the pruned decision tree, which we had described earlier. In summary, the <code class="literal">clj-ml</code> library provides us with a working implementation of a decision tree classifier based on the C4.5 algorithm.</p><p>The <code class="literal">make-classifier</code> function supports several interesting options for the J48 decision tree classifier. We've already explored the <code class="literal">:unpruned</code> option, which indicated that the decision tree is not pruned. We can specify the<code class="literal">:reduced-error-pruning</code> option to the <code class="literal">make-classifier</code> function to force the usage of reduced error pruning (for more information, refer to "Pessimistic decision tree pruning based on tree size"), which is a form of pruning based on reducing the overall error of the model. Another interesting option that we can specify to the <code class="literal">make-classifier</code> function is the maximum number of internal nodes or folds that can be removed by pruning the decision tree. We can specify this option using the <code class="literal">:pruning-number-of-folds</code> option, and by default, the <code class="literal">make-classifier</code> function imposes no such limit while pruning the decision tree. Also, we can specify that each internal decision node in the <a id="id296" class="indexterm"/>decision tree has only two subtrees by specifying the <code class="literal">:only-binary-splits</code> option to the <code class="literal">make-classifier</code> function.</p></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Summary</h1></div></div></div><p>In this chapter, we explored classification and the various algorithms that can be used to model a given classification problem. Although classification techniques are very useful, they do not perform too well when the sample data has a large number of dimensions. Also, the features may vary in a nonlinear manner, as we will describe in <a class="link" href="ch04.html" title="Chapter 4. Building Neural Networks">Chapter 4</a>, <em>Building Neural Networks</em>. We will also explore more about these aspects and the alternate methods of supervised learning in the following chapters. The following are few of the points that we looked at in this chapter:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We described two broad types of classifications, namely, binary and multiclass classification. We also briefly studied the logistic function and how it can be used to model classification problems through logistic regression.</li><li class="listitem" style="list-style-type: disc">We studied and implemented a Bayes classifier, which used a probabilistic model used for modeling classification. We also described how we could use the <code class="literal">clj-ml</code> library's Bayes classifier implementation to model a given classification problem.</li><li class="listitem" style="list-style-type: disc">We also explored the simple k-nearest neighbor algorithm and how we can leverage it using the <code class="literal">clj-ml</code> library.</li><li class="listitem" style="list-style-type: disc">We studied decision trees and the C4.5 algorithm. The <code class="literal">clj-ml</code> library provides us with a configurable implementation of a classifier based on the C4.5 algorithm, and we described how this implementation could be used as well.</li></ul></div><p>We will explore artificial neural networks in the following chapter. Interestingly, we can use artificial neural networks to model regression and classification problems, and we will study these aspects of neural networks as well.</p></div></body></html>