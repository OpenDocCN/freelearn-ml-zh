- en: 'Chapter 5: Understanding Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章：理解机器学习
- en: Over the last few years, you have likely heard some of the many popular buzz
    words such as **Artificial Intelligence** (**AI**), **Machine Learning** (**ML**),
    and **Deep Learning** (**DL**) that have rippled through most major industries.
    Although many of these phrases tend to be used interchangeably in company-wide
    all-staff and leadership meetings, each of these phrases does in fact refer to
    a distinct concept. So, let's take a look closer look at what these phrases actually
    refer to.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，你很可能已经听到了许多流行词汇，如**人工智能**（**AI**）、**机器学习**（**ML**）和**深度学习**（**DL**），这些词汇在大多数主要行业中引起了波澜。尽管这些短语在公司全体员工和领导层的会议中往往被互换使用，但每个短语实际上都指代一个独特概念。因此，让我们更仔细地看看这些短语实际上指的是什么。
- en: AI generally refers to the overarching domain of human-like intelligence demonstrated
    by software and machines. We can think of AI as the space that encompasses many
    of the topics we will discuss within the scope of this book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能通常指的是由软件和机器展现出的类似人类智能的总体领域。我们可以将人工智能视为一个空间，它包含了本书范围内我们将讨论的许多主题。
- en: Within the AI domain, there exists a sub-domain that we refer to as **machine
    learning**. ML can be defined as *the study of algorithms in conjunction with
    data to develop predictive models*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能领域，存在一个我们称之为**机器学习**的子领域。机器学习可以被定义为*研究算法与数据相结合以开发预测模型*。
- en: Within the ML domain, there exists yet another sub-domain we refer to as **deep
    learning**. We will define **DL** as *the application of ML specifically through
    the use of artificial neural networks*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，还存在另一个我们称之为**深度学习**的子领域。我们将**DL**定义为*通过使用人工神经网络特别应用机器学习*。
- en: Now that we have gained a better sense of the differences between these terms,
    let's define the concept of ML in a little more detail. There are several different
    definitions of ML that you will encounter depending on who you ask. Physicists
    tend to link the definition to applications in *performance optimization*, whereas
    mathematicians have a tendency to link the definition to *statistical probabilities*
    and, finally, computer scientists tend to link the definition to *algorithms*
    and *code*. To a certain extent, all three are technically correct. For the purposes
    of this book, we will define ML as a field of research concerning the development
    of mathematically optimized models using computer code, which *learn* or *generalize*
    from historical data to unlock useful insights and make predictions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经更好地理解了这些术语之间的差异，让我们更详细地定义机器学习的概念。根据你询问的人不同，你将遇到几种不同的机器学习定义。物理学家倾向于将定义与*性能优化*的应用联系起来，而数学家倾向于将定义与*统计概率*联系起来，最后，计算机科学家倾向于将定义与*算法*和*代码*联系起来。在某种程度上，三者都是技术正确的。为了本书的目的，我们将机器学习定义为研究使用计算机代码开发数学优化模型，该模型能够从历史数据中*学习*或*泛化*，以解锁有用的见解并做出预测。
- en: Although this definition may seem straightforward, most experienced interview
    candidates still tend to struggle when defining this concept. Make note of the
    exact phrasing we used here, as it may prove to be useful in a future setting.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个定义可能看起来很简单，但大多数有经验的面试候选人仍然倾向于在定义这个概念时感到困难。请注意我们在这里使用的确切措辞，因为它可能在未来的某个场合证明是有用的。
- en: Over the course of this chapter, we will visit various aspects of ML, and we
    will review some of the most common steps a developer must take when it comes
    to developing a **predictive model**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的进程中，我们将探讨机器学习的各个方面，并回顾开发**预测模型**时开发者必须采取的一些最常见步骤。
- en: 'In this chapter, we will review the following main topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾以下主要主题：
- en: Understanding ML
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习
- en: Overfitting and underfitting
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合与欠拟合
- en: Developing an ML model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发机器学习模型
- en: With all this in mind, let's get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些，让我们开始吧！
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will apply our understanding of `pandas` and `numpy`. In
    addition, we will also use some ML libraries such as `sklearn` and `tensorflow`.
    Recall that the process of installing a new library can be done via the command
    line:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将应用我们对`pandas`和`numpy`的理解。此外，我们还将使用一些机器学习库，如`sklearn`和`tensorflow`。回想一下，安装新库的过程可以通过命令行完成：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's begin!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始！
- en: Understanding ML
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习
- en: 'In the introduction, we broadly defined the concept of ML as it pertains to
    this book. With that definition in mind, let''s now take a look at some examples
    to elaborate on our definition. In its broadest sense, ML can be divided into
    four areas: **classification**, **regression**, **clustering**, and **dimensionality
    reduction**. These four categories are often referred to as the field of **data
    science**. Data science is a very broad term used to refer to various applications
    relating to data, as well as the field of AI and its subsets. We can visualize
    the relationships between these fields in *Figure 5.1*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍中，我们广泛地定义了与本书相关的机器学习的概念。有了这个定义，现在让我们看看一些例子来阐述我们的定义。在最广泛的意义上，机器学习可以分为四个领域：**分类**、**回归**、**聚类**和**降维**。这四个类别通常被称为**数据科学**领域。数据科学是一个非常广泛的概念，用于指代与数据相关的各种应用，以及人工智能及其子集。我们可以在*图
    5.1*中可视化这些领域之间的关系：
- en: '![Figure 5.1 – The domain of AI as it relates to other fields ](img/B17761_05_001.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 人工智能与其他领域相关的领域](img/B17761_05_001.jpg)'
- en: Figure 5.1 – The domain of AI as it relates to other fields
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 人工智能与其他领域相关的领域
- en: With these concepts in mind, let's discuss these four ML methods in more detail.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在心中牢记这些概念，让我们更详细地讨论这四种机器学习方法。
- en: '`X`) and their subsequent output values (generally referred to as `ŷ`) are
    used to train a classifier. This classifier can then be used to make predictions
    on new and unseen data. We can represent this visually in *Figure 5.2*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`X`)及其后续的输出值（通常称为`ŷ`）用于训练一个*分类器*。这个分类器然后可以用于对新数据和未见数据做出预测。我们可以在*图 5.2*中直观地表示这一点：'
- en: '![Figure 5.2 – An example of a classification model ](img/B17761_05_002.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 分类模型的示例](img/B17761_05_002.jpg)'
- en: Figure 5.2 – An example of a classification model
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 分类模型的示例
- en: '**Clustering** is similar to classification in the sense that the outcome of
    the model is a label (or category), but the difference here is that a clustering
    model is not trained on a list of predefined classes but is based on the similarities
    between objects. The clustering model then groups the data points together in
    *clusters*. The total number of clusters formed is not always known ahead of time,
    and this depends heavily on the parameters the model is trained on. In the following
    example, three clusters were formed using the original dataset:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**在模型结果为标签（或类别）的意义上与分类相似，但这里的区别在于聚类模型不是基于预定义的类别列表进行训练，而是基于对象之间的相似性。聚类模型然后将数据点分组到*簇*中。形成的簇的总数不一定事先就知道，这很大程度上取决于模型训练的参数。在以下示例中，使用原始数据集形成了三个簇：'
- en: '![Figure 5.3 – An example of a clustering model ](img/B17761_05_003.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 聚类模型的示例](img/B17761_05_003.jpg)'
- en: Figure 5.3 – An example of a clustering model
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 聚类模型的示例
- en: 'On the other hand, when it comes to `X`) and their subsequent output values
    (`ŷ`) are used to train a *regressor*. This regressor can then be used to make
    predictions on new and unseen data:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当涉及到`X`)及其后续的输出值（通常称为`ŷ`）用于训练一个*回归器*时。这个回归器然后可以用于对新数据和未见数据做出预测：
- en: '![Figure 5.4 – An example of a regression model ](img/B17761_05_004.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 回归模型的示例](img/B17761_05_004.jpg)'
- en: Figure 5.4 – An example of a regression model
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 回归模型的示例
- en: 'Finally, when it comes to **dimensionality reduction**, ML can be applied not
    for the purposes of predicting a value, but in the sense of transforming data
    from a *high-dimensional* representation to a *low-dimensional* representation.
    Take, for example, the vast toxicity dataset we worked with in the previous chapters.
    We could apply a method such as **Principal Component Analysis** (**PCA**) to
    reduce the 10+ columns of features down to only two or three columns by *combining
    the importance* of these features together. We will examine this in greater detail
    in [*Chapter 7*](B17761_07_Final_JM_ePub.xhtml#_idTextAnchor101), *Understanding
    Supervised Machine Learning*. We can see a visual representation of this in *Figure
    5.5*:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当涉及到**降维**时，机器学习不仅可以用于预测值的目的，而且在将数据从*高维*表示转换为*低维*表示的意义上得到应用。以我们在前几章中使用的庞大的毒性数据集为例。我们可以应用如**主成分分析**（PCA）这样的方法，通过*结合这些特征的重要性*，将10多个特征列减少到只有两到三列。我们将在[*第7章*](B17761_07_Final_JM_ePub.xhtml#_idTextAnchor101)“理解监督机器学习”中更详细地探讨这一点。我们可以在*图
    5.5*中看到这一过程的视觉表示：
- en: '![Figure 5.5 – An example of a dimensionality reduction model ](img/B17761_05_005.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 降维模型的一个示例](img/B17761_05_005.jpg)'
- en: Figure 5.5 – An example of a dimensionality reduction model
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 降维模型的一个示例
- en: The ML field is vast, complex, and extends well beyond the four basic examples
    we just touched on. However, the most common applications of ML models tend to
    focus on *predicting a category*, *predicting a value*, or *uncovering hidden
    insights* within data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习领域非常广泛、复杂，远远超出了我们刚才提到的四个基本示例。然而，ML 模型的最常见应用往往集中在 *预测一个类别*、*预测一个值* 或 *在数据中揭示隐藏的见解*
    上。
- en: 'As scientists, we always want to organize our thoughts as best we can, and
    as it happens, the concepts we just discussed can be placed into two main categories:
    `X`) and the output (`ŷ`). We call this a *supervised* method because the model
    was taught (*supervised*) which output label corresponds to which input value.
    On the other hand, UML encompasses ML models in which only the input (`X`) is
    known. Looking back to the four methods we discussed, we can divide them across
    both learning methods in the sense that **classification** and **regression**
    fall under SML, whereas **clustering** and **dimensionality reduction** fall under
    UML:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作为科学家，我们总是希望尽可能好地组织我们的思想，而且正如所发生的那样，我们刚才讨论的概念可以分为两大类：`X`) 和输出 (`ŷ`)。我们称这种方法为
    *监督* 方法，因为模型是经过（监督）训练的，其输出标签对应于哪个输入值。另一方面，UML 包括仅知道输入 (`X`) 的 ML 模型。回顾我们刚才讨论的四种方法，我们可以根据这两种学习方法来划分它们，即
    **分类** 和 **回归** 属于 SML，而 **聚类** 和 **降维** 属于 UML：
- en: '![Figure 5.6 – A representation of supervised and unsupervised machine learning
    ](img/B17761_05_006.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 监督学习和无监督学习的表示](img/B17761_05_006.jpg)'
- en: Figure 5.6 – A representation of supervised and unsupervised machine learning
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 监督学习和无监督学习的表示
- en: Over the course of the following chapters, we will explore many popular ML models
    and algorithms that fall under these four general categories. As you follow along,
    I encourage you to develop a mind map of your own, and further branch out each
    of the four categories to all the different models you will learn. For example,
    we will explore a *Naïve Bayes* model in the *Saving a model for deployment* section
    of this chapter, which could be added to the **classification** branch of *Figure
    5.6*. Perhaps you could branch out each of the models with some notes regarding
    the model itself. A map or visual aid may prove to be useful when preparing for
    a technical interview.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨许多属于这四个一般类别的流行 ML 模型和算法。随着你的学习，我鼓励你开发自己的思维导图，并将每个四个类别进一步分支到你将要学习的所有不同模型中。例如，我们将在本章的
    *保存模型以部署* 部分探讨一个 *朴素贝叶斯* 模型，这可以添加到 *图 5.6* 的 **分类** 分支中。也许你可以为每个模型添加一些关于模型本身的注释。在准备技术面试时，地图或视觉辅助工具可能很有用。
- en: Throughout each of the models we develop, we will follow a particular set of
    steps to acquire our data, preprocess it, build a model, evaluate its performance,
    and finally, if the model is sufficient, deploy it to our end users or data engineers.
    Before we begin developing our models, let's discuss the common dangers known
    as *overfitting* and *underfitting*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开发的每个模型中，我们将遵循一系列特定的步骤来获取我们的数据，对其进行预处理，构建模型，评估其性能，最后，如果模型足够好，将其部署给我们的最终用户或数据工程师。在我们开始开发模型之前，让我们讨论一下被称为
    *过拟合* 和 *欠拟合* 的常见危险。
- en: Overfitting and underfitting
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: Within the context of SML, we will prepare our models by *fitting* them with
    historical data. The process of fitting a model generally outputs a measure of
    how well the model generalizes to data that is similar to the data on which the
    model was trained. Using this output, usually in the form of **precision**, **accuracy**,
    and **recall**, we can determine whether the method we implemented or the parameters
    we changed had a positive impact on our model. If we revisit the definition of
    ML models that from earlier in this chapter, we specifically refer to them as
    models that *learn* or *generalize* from historical data. Models that are able
    to learn from historical data are referred to as *well-fitted* models, in the
    sense that they are able to perform accurately on new and unseen data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在SML的背景下，我们将通过用历史数据**拟合**我们的模型来准备我们的模型。拟合模型的过程通常输出一个度量，表示模型在多大程度上能够泛化到与训练模型的数据相似的数据。使用这个输出，通常以**精确度**、**准确度**和**召回率**的形式，我们可以确定我们实施的方法或更改的参数是否对我们的模型产生了积极的影响。如果我们回顾本章前面关于ML模型的定义，我们特别将它们称为从历史数据中**学习**或**泛化**的模型。能够从历史数据中学习的模型被称为**拟合良好**的模型，从某种意义上说，它们能够在新的、未见过的数据上准确执行。
- en: There are instances in which models are underfitted. *Underfitted* models generally
    perform poorly on datasets, which means they have not learned to generalize well.
    These cases are generally the result of an inappropriate model being selected
    for a given dataset or the inadequate setting of a parameter/hyperparameter for
    that model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些情况是模型欠拟合。**欠拟合**的模型在数据集上通常表现不佳，这意味着它们没有学会很好地泛化。这些情况通常是由于为给定的数据集选择了不适当的模型，或者对该模型的参数/超参数设置不足。
- en: Important note
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '**Parameters and hyperparameters**: Note that while parameters and hyperparameters
    are terms that are often used interchangeably, there is a difference between the
    two. *Hyperparameters* are parameters that are not learned by a model''s estimator
    and must be manually tuned.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数和超参数**：请注意，虽然参数和超参数是经常可以互换使用的术语，但两者之间是有区别的。**超参数**是模型估计器没有学习到的参数，必须手动调整。'
- en: 'There are also instances in which models are overfitted. *Overfitted* models
    are models that *know* the dataset a little too well, and this means that they
    are no longer *learning* but *memorizing*. Overfitting generally occurs when a
    model begins to learn from the noise within a dataset and is no longer able to
    generalize well on new data. The differences between well-fitted, overfitted,
    and underfitted models can be seen in *Figure 5.7*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些情况是模型过拟合。**过拟合**的模型是那些对数据集**了解**得有点太多的模型，这意味着它们不再是**学习**而是在**记忆**。过拟合通常发生在模型开始从数据集中的噪声中学习，并且不再能够很好地泛化到新数据时。拟合良好、过拟合和欠拟合模型之间的区别可以在**图5.7**中看到：
- en: '![Figure 5.7 – A representation of overfitting and underfitting data ](img/B17761_05_007.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 过拟合和欠拟合数据的表示](img/B17761_05_007.jpg)'
- en: Figure 5.7 – A representation of overfitting and underfitting data
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 过拟合和欠拟合数据的表示
- en: 'The objective of every data scientist is to develop a balanced model with optimal
    performance when it comes to your metrics of interest. One of the best ways to
    ensure that you are developing a balanced model that is not underfitting or overfitting
    is by splitting your dataset ahead of time and ensuring that the model is only
    ever trained on a subset of the data. We can split datasets into two categories:
    *training data* and *testing data* (also often referred to as *validation data*).
    We can use the training dataset to train the model, and we can use the testing
    dataset to test (or validate) the model. One of the most common classes to use
    for this purpose is the `train_test_split()` class from `sklearn`. If you think
    of your dataset with `X` being your input variables and `ŷ` being your output,
    you can split the dataset using the following code snippet. First, we import the
    data. Then, we isolate the features we are interested in and output their respective
    variables. Then, we implement the `train_test_split()` function to split the data
    accordingly:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据科学家的目标都是开发一个在您感兴趣的指标上具有最佳性能的平衡模型。确保您正在开发一个既不过度拟合也不会欠拟合的平衡模型的最佳方法之一是在事先分割您的数据集，并确保模型只训练在数据的一个子集上。我们可以将数据集分为两类：*训练数据*和*测试数据*（也常被称为*验证数据*）。我们可以使用训练数据集来训练模型，并使用测试数据集来测试（或验证）模型。用于此目的的最常见的类是来自`sklearn`的`train_test_split()`类。如果您将数据集视为`X`是您的输入变量，`ŷ`是您的输出，您可以使用以下代码片段来分割数据集。首先，我们导入数据。然后，我们隔离我们感兴趣的特性并输出它们各自的变量。然后，我们实现`train_test_split()`函数来相应地分割数据：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can visualize the split dataset in *Figure 5.8*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图5.8*中可视化分割后的数据集：
- en: '![Figure 5.8 – A visual representation of data that has been split for training
    and testing ](img/B17761_05_008.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – 用于训练和测试的数据的视觉表示](img/B17761_05_008.jpg)'
- en: Figure 5.8 – A visual representation of data that has been split for training
    and testing
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 用于训练和测试的数据的视觉表示
- en: With the data split in this fashion, we can now use `X_train` and `y_train`
    for the purposes of training our model, and `X_test` and `y_test` for the purposes
    of testing (or validating) our model. The default splitting ratio is 75% training
    data to 25% testing data; however, we can pass the `test_size` parameter to change
    this to any other ratio. We generally want to train on as much data as possible
    but still keep a meaningful amount of unseen data in reserve, and so *75/25* is
    a commonly accepted ratio in the industry. With this concept in mind, let's move
    on to developing a full ML model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式分割数据后，我们现在可以使用`X_train`和`y_train`来训练我们的模型，以及使用`X_test`和`y_test`来测试（或验证）我们的模型。默认分割比例是75%训练数据到25%测试数据；然而，我们可以传递`test_size`参数来改变这个比例。我们通常希望尽可能多地训练数据，但仍然保留有意义的未见过数据量，因此*75/25*在行业中是一个普遍接受的比率。有了这个概念，让我们继续开发一个完整的机器学习模型。
- en: Developing an ML model
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发机器学习模型
- en: 'There are numerous ML models that we interact with on a daily basis as end
    users, and we likely do not even realize it. Think back to all the activities
    you did today: scrolling through social media, checking your email, or perhaps
    you visited a store or a supermarket. In each of these settings, you likely interacted
    with an already deployed ML model. On social media, the posts that are presented
    on your feed are likely the output of a supervised **recommendation** model. The
    emails you opened were likely filtered for spam emails using a **classification**
    model. And, finally, the number of goods available within the grocery store was
    likely the output of a **regression** model, allowing them to predict today''s
    demand. In each of these models, a great deal of time and effort was dedicated
    to ensuring they function and operate correctly. In these situations, while the
    development of the model is important, the most important thing is how the data
    is prepared ahead of time. As scientists, we always have a tendency to organize
    our thoughts and processes as best we can, so let''s organize a workflow for the
    process of developing ML models:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 作为终端用户，我们每天都会与许多机器学习模型进行交互，而我们可能甚至没有意识到这一点。回想一下你今天所做的一切活动：浏览社交媒体、查看电子邮件，或者你可能访问了一家商店或超市。在这些环境中，你很可能已经与一个已经部署的机器学习模型进行了交互。在社交媒体上，你信息流中显示的帖子很可能是监督**推荐**模型的输出。你打开的电子邮件很可能是使用**分类**模型过滤掉的垃圾邮件。最后，杂货店中商品的种类数量很可能是**回归**模型的输出，允许它们预测今天的需求。在这些模型中，大量的时间和精力都投入到了确保它们能够正确运行。在这些情况下，虽然模型的开发很重要，但最重要的是如何提前准备数据。作为科学家，我们总是倾向于尽可能好地组织我们的思想和过程，因此让我们为开发机器学习模型的过程组织一个工作流程：
- en: '**Data acquisition**: Collecting data via SQL queries, local imports, or API
    requests'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据获取**：通过SQL查询、本地导入或API请求收集数据'
- en: '**EDA and preprocessing**: Understanding and cleaning up the dataset'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**EDA和预处理**：理解和清理数据集'
- en: '**Model development and validation**: Training a model and verifying the results'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型开发和验证**：训练模型并验证结果'
- en: '**Deployment**: Making your model available to end users'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署**：使你的模型可供最终用户使用'
- en: With these steps in mind, let's go ahead and develop our first model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些步骤，让我们继续开发我们的第一个模型。
- en: We begin by importing our data. We will use a new dataset that we have not worked
    with yet known as the `Breast Cancer Wisconsin` dataset. This is a *multivariate*
    dataset, published in 1995, containing several hundred instances of breast cancer
    masses. These masses are described in the form of measurements that we will use
    as *features* (`X`). The dataset also includes information regarding the malignancy
    of each of the instances, which we will use for our output *label* (`ŷ`). Given
    that we have both the input data and the output data, this calls for the use of
    a *classification* model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入我们的数据。我们将使用一个我们尚未使用过的新数据集，称为“威斯康星乳腺癌”数据集。这是一个*多元*数据集，于1995年发布，包含数百个乳腺癌肿瘤的实例。这些肿瘤以测量值的形式描述，我们将将其用作*特征*（`X`）。数据集还包括有关每个实例的恶性信息，我们将使用它作为我们的输出*标签*（`ŷ`）。鉴于我们既有输入数据又有输出数据，这就需要使用一个*分类*模型。
- en: Data acquisition
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据获取
- en: 'Let''s import our data and check its overall shape:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入我们的数据并检查其整体形状：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We notice that there are 569 rows (which we generally call *observations*) and
    32 columns (which we generally call *features*) of data. We generally want our
    dataset to have many more *observations* than *features*. There is no golden rule
    about the ideal ratio between the two, but you generally want to have at least
    10x more observations than features. So, with 32 columns, you would want to have
    at least 320 observations – which we do in this case!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到有569行数据（我们通常称之为*观测值*）和32列数据（我们通常称之为*特征*）。我们通常希望我们的数据集拥有比特征多得多的*观测值*。关于两者之间理想比例并没有金科玉律，但通常你希望观测值至少比特征多10倍。所以，对于32列，你希望至少有320个观测值——在这个案例中我们确实做到了！
- en: 'Exploratory data analysis and preprocessing:'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索性数据分析与预处理：
- en: '**Exploratory Data Analysis** (**EDA**) is arguably one of the most important
    and time-consuming steps in any given ML project. This step generally consists
    of many smaller steps whose objectives are as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索性数据分析**（**EDA**）可以说是任何机器学习项目中最重要的且耗时最长的步骤之一。这一步骤通常包括许多更小的步骤，其目标如下：'
- en: Understand the data and its features.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据和其特征。
- en: Address any inconsistencies or missing values.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决任何不一致或缺失值。
- en: Check for any correlations between the features.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查特征之间的任何相关性。
- en: Please note that the order in which we carry out these steps may be different
    depending on your dataset. With all this in mind, let's get started!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们执行这些步骤的顺序可能因数据集而异。考虑到所有这些，让我们开始吧！
- en: Examining the dataset
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查数据集
- en: 'One of the first steps after importing your dataset is to quickly check the
    quality of the data. Recall that we can use square brackets (`[]`) to specify
    the columns of interest, and we can use the `head()` or `tail()` functions to
    see the first or last five rows of data:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入数据集后的第一步之一是快速检查数据的质量。回想一下，我们可以使用方括号（`[]`）来指定感兴趣的列，并且我们可以使用`head()`或`tail()`函数来查看数据的前五行或后五行：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can see the results of this code in *Figure 5.9*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图 5.9*中看到此代码的结果：
- en: '![Figure 5.9 – A sample of the Breast Cancer Wisconsin dataset ](img/B17761_05_09.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 威斯康星乳腺癌数据集的样本](img/B17761_05_09.jpg)'
- en: Figure 5.9 – A sample of the Breast Cancer Wisconsin dataset
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 威斯康星乳腺癌数据集的样本
- en: We can quickly get a sense of the fact that the data is very well organized,
    and from a first glance, it does not appear to have any problematic values, such
    as unusual characters or missing values. Looking over these select columns, we
    notice that there is a unique identifier in the beginning consisting of *integers*,
    followed by the diagnosis (**M** = **malignant** and **B** = **benign**) consisting
    of *strings*. The rest of the columns are all features, and they all appear to
    be of the *float* (decimals) data type. I encourage you to expand the scope of
    the preceding table and explore all the other features within this dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以快速了解数据组织得非常好，并且从第一眼看上去，似乎没有任何问题值，例如不寻常的字符或缺失值。查看这些选择的列，我们注意到在开头有一个唯一的标识符，由*整数*组成，后面跟着诊断（**M**
    = **malignant** 和 **B** = **benign**）由*字符串*组成。其余的列都是特征，它们看起来都是*浮点数*（小数）数据类型。我鼓励你扩展前面表格的范围，并探索这个数据集中所有的其他特征。
- en: 'In addition to exploring the values, we can also explore some of the summary
    statistics provided by the `describe()` function in the `pandas` library. Using
    this function, we get a sense of the total count, as well as some descriptive
    statistics such as the mean, maximum, and minimum values:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 除了探索值之外，我们还可以探索`pandas`库中`describe()`函数提供的某些汇总统计。使用此函数，我们可以了解总数，以及一些描述性统计，如平均值、最大值和最小值：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of this function can be seen in the following screenshot:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数的输出可以在以下屏幕截图中看到：
- en: '![Figure 5.10 – A table of some summary statistics for a DataFrame ](img/B17761_05_10.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – DataFrame 的某些汇总统计表](img/B17761_05_10.jpg)'
- en: Figure 5.10 – A table of some summary statistics for a DataFrame
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – DataFrame 的某些汇总统计表
- en: Looking over the code, we notice that we had requested the statistics for seven
    columns, however, only five appeared in the table. We can see that the `id` values
    (which are *primary keys* or *unique identifiers*) were summarized here. These
    values are meaningless, as the mean, maximum, and minimum of a set of primary
    keys tell us nothing. We can ignore this column for now. We also asked for the
    `diagnosis` column; however, the `diagnosis` column does not use a numerical value.
    Instead, it contains *strings*. Finally, we see that the `concave points_worst`
    feature was also not included in this table, indicating that the data type is
    not numerical for whatever reason. We will take a closer look at this in the next
    section when we clean the data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 检查代码，我们注意到我们请求了七个列的统计信息，然而，表中只出现了五个。我们可以看到`id`值（它们是*主键*或*唯一标识符*）在这里进行了汇总。这些值没有意义，因为一组主键的平均值、最大值和最小值告诉我们什么都没有。我们可以暂时忽略这个列。我们还请求了`diagnosis`列；然而，`diagnosis`列不使用数值。相反，它包含*字符串*。最后，我们看到`concave
    points_worst`特征也没有包含在这个表中，这表明数据类型由于某种原因不是数值。我们将在下一节清理数据时更仔细地查看这一点。
- en: Cleaning up values
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清理值
- en: Getting the values within your dataset cleaned up is one of the most important
    steps when handling an ML project. A famous saying among data scientists when
    describing models is *garbage in, garbage out*. If you want to have a strong predictive
    model, then ensuring the data that supports it is of good quality is an important
    first step.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理机器学习项目时，清理数据集中的值是其中最重要的步骤之一。数据科学家在描述模型时经常说“垃圾输入，垃圾输出”。如果你想拥有一个强大的预测模型，那么确保支持它的数据质量良好是一个重要的第一步。
- en: 'To begin, let''s take a closer look at the data types, given that there may
    be some inconsistencies here. We can get a sense of the data types for each of
    the 32 columns using the following code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们更仔细地查看数据类型，因为这里可能存在一些不一致性。我们可以使用以下代码来获取每个32个列的数据类型感：
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can see the output of this code in *Figure 5.11*, where the column names
    are shown with their respective data types:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图5.11*中看到这段代码的输出，其中列出了列名及其相应的数据类型：
- en: '![Figure 5.11 – A list of all of the columns in a dataset with their respective
    data types ](img/B17761_05_011.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11 – 一个数据集中所有列及其相应数据类型的列表](img/B17761_05_011.jpg)'
- en: Figure 5.11 – A list of all of the columns in a dataset with their respective
    data types
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 – 一个数据集中所有列及其相应数据类型的列表
- en: 'Looking over the listed data types, we see that the `id` column is listed as
    an integer and the `diagnosis` column is listed as an object, which seems consistent
    with the fact that it appeared to be a single-letter string in *Figure 5.9*. Looking
    over the features, they are all listed as floats, completely consistent with what
    we previously saw, with the exception of one feature: `concave points_worst`.
    This feature is listed as an object, indicating that it might be a string. We
    noted earlier that this column consisted of float values, and so the column itself
    should be of the float type. Let''s take a look at this inconsistency sooner rather
    than later. We can make an attempt to *cast* the column to be of the float type
    instead of using the `astype()` function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 查看列出的数据类型，我们看到`id`列被列为整数，而`diagnosis`列被列为对象，这似乎与它在*图5.9*中看起来像单个字母字符串的事实一致。查看特征，它们都被列为浮点数，这与我们之前看到的一致，除了一个特征：`concave
    points_worst`。这个特征被列为对象，表明它可能是一个字符串。我们之前提到，这个列由浮点值组成，因此这个列本身应该是浮点类型。让我们早点看看这个不一致性。我们可以尝试将列*转换*为浮点类型，而不是使用`astype()`函数：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'However, you will find that this code will error out, indicating that there
    is a row in which the `\\n` characters are present and it is unable to convert
    the string to a float. This is known as a *newline character* and it is one of
    the most common items or *impurities* you will deal with when handling datasets.
    Let''s move on and identify the lines in which this character is present and decide
    how to deal with it. We can use the `contains()` function to find all instances
    of a particular string:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你会发现这段代码会出错，表明存在一行包含`\\n`字符，并且它无法将字符串转换为浮点数。这被称为*换行符*，它是你在处理数据集时遇到的最常见项目或*杂质*之一。让我们继续前进，确定包含此字符的行，并决定如何处理它。我们可以使用`contains()`函数来查找特定字符串的所有实例：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of this function shows that only the row with the `146` index contains
    this character. Let''s take a closer look at the specific cell from the `146`
    row:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的输出显示，只有索引为`146`的行包含这个字符。让我们更仔细地查看`146`行的特定单元格：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We see that the cell contains the `0.1865\\n\\n` string. It appears as though
    the character is printed twice and only in this row. We could easily open the
    CSV file and correct this value manually, given that it only occurred a single
    time. However, what if this string appeared 10 times, or 100 times? Luckily, we
    can use a `replace()` function to *replace* them. We can specifically chain this
    function on `df` instead of the single column to ensure that the function parses
    the full DataFrame:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到单元格包含`0.1865\\n\\n`字符串。看起来字符被打印了两次，而且只在这一行。如果我们打开CSV文件并手动纠正这个值，那很容易，因为这种情况只发生了一次。然而，如果这个字符串出现了10次或100次呢？幸运的是，我们可以使用一个`replace()`函数来*替换*它们。我们可以将这个函数特别链接到`df`上，而不是单个列，以确保函数解析整个DataFrame：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'A regex is a powerful tool that you will often rely on for various text matching
    and cleaning tasks. You can remove spaces, numbers, characters, or unusual combinations
    of characters using regex functions. We can double-check the regex function''s
    success by once again examining that specific cell''s value:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式是一个强大的工具，你将经常依赖它来完成各种文本匹配和清理任务。你可以使用正则表达式函数来删除空格、数字、字符或字符的异常组合。我们可以再次检查特定单元格的值来双重验证正则表达式函数的成功：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The value is now only `0.1865`, indicating that the function was, in fact, successful.
    We can now cast the column's type to a float using the `astype()` function and
    then also confirm the correct datatypes are listed using `df.dtypes`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的值仅为`0.1865`，这表明函数实际上成功了。我们现在可以使用`astype()`函数将列的类型转换为浮点，然后使用`df.dtypes`来确认是否列出了正确的数据类型。
- en: 'So far, we were able to address issues in which invalid characters made their
    way into the dataset. However, what about items that are missing? We can run a
    quick check on our dataset to determine if any values are missing using the `isna()`
    function:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经能够解决无效字符进入数据集的问题。然而，对于缺失的项怎么办？我们可以使用 `isna()` 函数快速检查我们的数据集，以确定是否有任何缺失值：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The value returned shows that there are seven rows of data in which a value
    is missing. Recall that in [*Chapter 4*](B17761_04_Final_JM_ePub.xhtml#_idTextAnchor066),
    *Visualizing Data with Python*, we looked over a few methods to address missing
    values. Given that we have a sufficiently large dataset, it would be appropriate
    to simply eliminate these few rows using the `dropna()` function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的值显示，有七行数据中存在缺失值。回想一下，在 [*第 4 章*](B17761_04_Final_JM_ePub.xhtml#_idTextAnchor066)，*使用
    Python 可视化数据* 中，我们讨论了几种处理缺失值的方法。鉴于我们有一个足够大的数据集，使用 `dropna()` 函数简单地删除这些几行是合适的：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can check the dataset's shape before and after implementing the function
    to ensure the proper number of rows was in fact dropped.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现函数前后，我们可以检查数据集的形状，以确保确实删除了正确的行数。
- en: Taking some time to clean up your dataset ahead of time is always recommended,
    as it will help prevent problems and unusual errors down the line. It's always
    important to check the *data types* and *missing values*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集清理之前花些时间是推荐的，因为它将有助于防止后续出现问题和异常错误。始终重要的是要检查 *数据类型* 和 *缺失值*。
- en: Understanding the meaning of the data
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解数据的意义
- en: 'Let''s now take a closer look at some of the data within this dataset, beginning
    with the output values in the second column. We know these values correspond to
    the labels as being *M* for *malignant* and *B* for *benign*. We can use the `value_counts()`
    function to determine the sum of each category:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更仔细地查看数据集内的某些数据，从第二列的输出值开始。我们知道这些值对应于标签，*M* 代表 *恶性*，*B* 代表 *良性*。我们可以使用
    `value_counts()` 函数来确定每个类别的总和：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The results show that there are 354 instances of benign masses and 208 instances
    of malignant masses. We can visualize this ratio using the `seaborn` library:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，良性肿块有 354 个实例，恶性肿块有 208 个实例。我们可以使用 `seaborn` 库来可视化这个比例：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output of this code can be seen as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可以看到此代码输出的内容：
- en: '![Figure 5.12 – A bar plot showing the number of instances for each class ](img/B17761_05_012.png.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – 展示每个类别的实例数量的条形图](img/B17761_05_012.png.jpg)'
- en: Figure 5.12 – A bar plot showing the number of instances for each class
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 展示每个类别的实例数量的条形图
- en: 'In most ML models, we try to ensure that the output column is *well balanced*,
    in the sense that the categories are *roughly equal*. Training a model on an imbalanced
    dataset with, for example, 95 rows of malignant observations and 5 rows of benign
    observations would lead to an imbalanced model with poor performance. In addition
    to visualizing the diagnosis or output column, we can also visualize the features
    to get a sense of any trends or correlations using the `pairplot()` function we
    reviewed in [*Chapter 4*](B17761_04_Final_JM_ePub.xhtml#_idTextAnchor066), *Visualizing
    Data with Python*. We can implement this with a handful of features:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数机器学习模型中，我们试图确保输出列是 *平衡良好* 的，即类别大致相等。在包含例如 95 行恶性观察和 5 行良性观察的不平衡数据集上训练模型会导致性能不佳的不平衡模型。除了可视化诊断或输出列之外，我们还可以使用我们回顾过的
    `pairplot()` 函数来可视化特征，以了解任何趋势或相关性。我们可以使用一些特征来实现这一点：
- en: '[PRE15]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following graph shows the output of this:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了这一结果：
- en: '![Figure 5.13 – A pair plot of selected features ](img/B17761_05_013.png.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13 – 选择特征的配对图](img/B17761_05_013.png.jpg)'
- en: Figure 5.13 – A pair plot of selected features
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – 选择特征的配对图
- en: Looking over these last few plots, we notice a distinguishable separation between
    the two clusters of data. The clusters appear to exhibit some of the characteristics
    of a **normal distribution**, in the sense that most points are localized closer
    to the center, with fewer points further away. Given this nature, one of the first
    models we may try within this dataset is a **Naïve Bayes classifier**, which tends
    to work well for this type of data. However, we will discuss this model in greater
    detail later on in this chapter.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这些最后的几个图表，我们注意到数据集的两个簇之间存在明显的分离。簇似乎表现出**正态分布**的一些特征，即大多数点都集中在中心附近，而远离中心的点较少。鉴于这种性质，我们可能首先尝试在这个数据集中使用的模型是**朴素贝叶斯分类器**，这种分类器通常适用于此类数据。然而，我们将在本章的后面部分更详细地讨论这个模型。
- en: In each of these plots, we see some degree of overlap between the two classes,
    indicating that two columns alone are not enough to maintain a good degree of
    separation. So, we could ensure that our ML models utilize more columns or we
    could try to eliminate any potential outliers that may contribute to this overlap
    – or we could do both!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些图表中的每一个，我们都看到两个类别之间存在某种程度的重叠，这表明仅凭两列数据不足以保持良好的分离度。因此，我们可以确保我们的机器学习模型利用更多的列，或者我们可以尝试消除可能造成这种重叠的任何潜在异常值——或者我们可以两者都做！
- en: 'First, we can utilize some descriptive *statistics*. Specifically, we can use
    the `dfm`. We can then define the first quartile (`Q1`) and third quartile (`Q3`)
    using the `radius_mean` feature:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以利用一些描述性*统计量*。具体来说，我们可以使用`dfm`。然后，我们可以使用`radius_mean`特征定义第一四分位数（`Q1`）和第三四分位数（`Q3`）：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can then print the outputs of these variables to determine the IQR in conjunction
    with the `mean()` and `median()` functions to get a sense of the distribution
    of the data. We can visualize these metrics alongside the upper and lower ranges
    using the `boxplot()` function provided in `seaborn`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以打印这些变量的输出，结合`mean()`和`median()`函数来确定IQR，以了解数据的分布。我们可以使用`seaborn`中提供的`boxplot()`函数将这些指标与上下限范围一起可视化：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This gives us *Figure 5.14*:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了*图5.14*：
- en: '![Figure 5.14 – A box-whisker plot of the radius_mean feature ](img/B17761_05_014.png.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图5.14 – 半径均值特征的箱线图](img/B17761_05_014.png.jpg)'
- en: Figure 5.14 – A box-whisker plot of the radius_mean feature
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 – 半径均值特征的箱线图
- en: 'Using the upper and lower ranges, we can filter the DataFrame to exclude any
    data that falls outside of this scope using the `query()` class within the `pandas`
    library:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上下限范围，我们可以使用`pandas`库中的`query()`类过滤DataFrame，排除任何超出此范围的数据：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: With the code executed, we have successfully removed several outliers from our
    dataset. If we go ahead and replot the data using one of the preceding scatter
    plots, we will see that while some of the overlap was indeed reduced, there is
    still considerable overlap between the two classes, indicating that any future
    models we develop will need to take advantage of multiple columns to ensure adequate
    separation as we begin developing a robust classifier. Before we can start training
    any classifiers, we will first need to address any potential *correlations* within
    the features.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，我们已经成功从数据集中移除了几个异常值。如果我们继续使用前面的散点图之一重新绘制数据，我们会看到虽然重叠确实有所减少，但两个类别之间仍然存在相当大的重叠，这表明我们开发的任何未来模型都需要利用多个列以确保在开发鲁棒分类器时能够实现足够的分离。在我们开始训练任何分类器之前，我们首先需要解决特征中可能存在的任何*相关性*。
- en: Finding correlations
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找相关性
- en: 'With the outliers filtered out, we are now ready to start taking a closer look
    at any correlations between the features in our dataset. Given that this dataset
    consists of 30 features, we can take advantage of the `corr()` class we implemented
    in [*Chapter 4*](B17761_04_Final_JM_ePub.xhtml#_idTextAnchor066), *Visualizing
    Data with Python*. We can create a `corr()` function and the `heatmap()` function
    from `seaborn`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 经过过滤掉异常值后，我们现在可以开始仔细观察数据集中特征之间的任何相关性。鉴于这个数据集包含30个特征，我们可以利用我们在[*第4章*](B17761_04_Final_JM_ePub.xhtml#_idTextAnchor066)，“使用Python可视化数据”中实现的`corr()`类。我们可以从`seaborn`创建一个`corr()`函数和`heatmap()`函数：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output of this code can be seen in *Figure 5.15*, showing a heat map of
    the various features in which the most correlated features are shown in a lighter
    color and the least correlated features are shown in a darker color:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的输出可以在*图5.15*中看到，显示了一个各种特征的散点图，其中相关性最高的特征以较浅的颜色显示，相关性最低的特征以较深的颜色显示：
- en: '![Figure 5.15 – A heat map showing the correlation of features ](img/B17761_05_015.png.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – 展示特征相关性的热图](img/B17761_05_015.png.jpg)'
- en: Figure 5.15 – A heat map showing the correlation of features
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 展示特征相关性的热图
- en: As we look at this heat map, we see that there is a great deal of correlation
    between multiple features within this dataset. Take, for instance, the very strong
    correlation between the `radius_worst` feature and the `perimeter_mean`, and `area_mean`
    features. When there are strong correlations between independent variables or
    features within a dataset, this is known as `corr()` function and create a matrix
    of these values. We can then select the upper triangle (half of the heat map)
    and then identify the features whose correlations are greater than `0.90:`
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看这个热图时，我们看到这个数据集中多个特征之间存在大量的相关性。例如，`radius_worst` 特征与 `perimeter_mean` 和
    `area_mean` 特征之间存在着非常强的相关性。当数据集中的独立变量或特征之间存在强相关性时，这被称为 `corr()` 函数，并创建这些值的矩阵。然后我们可以选择上三角（热图的一半）并识别相关性大于
    `0.90` 的特征：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `to_drop` variable now represents a list of columns that should be dropped
    to ensure that any correlations above the threshold we set are effectively removed.
    Notice that we used **list comprehension** (a concept that we talked about in
    [*Chapter 2*](B17761_02_Final_JM_ePub.xhtml#_idTextAnchor023), *Introducing Python
    and the Command Line*) to iterate through these values quickly and effectively.
    We can then go ahead and drop the columns from our dataset:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_drop` 变量现在代表一个应该删除的列的列表，以确保我们设置的阈值以上的任何相关性都能被有效移除。注意，我们使用了 **列表推导**（这是我们曾在
    [*第 2 章*](B17761_02_Final_JM_ePub.xhtml#_idTextAnchor023)，*介绍 Python 和命令行*）来快速有效地遍历这些值。然后我们可以继续从我们的数据集中删除这些列：'
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can once again plot the heat map to ensure that any potential collinearity
    is addressed:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次绘制热图以确保任何潜在的多重共线性问题得到解决：
- en: '![Figure 5.16 – A heat map showing the correlation of features without multicollinearity
    ](img/B17761_05_016.png.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – 展示特征相关性的热图，无多重共线性](img/B17761_05_016.png.jpg)'
- en: Figure 5.16 – A heat map showing the correlation of features without multicollinearity
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – 展示特征相关性的热图，无多重共线性
- en: Notice that the groups of highly correlated features are no longer present.
    With the correlations now addressed, we have not only ensured that any potential
    models we created will not suffer from any performance problems relating to *multicollinearity*,
    but we also inadvertently reduced the size of the dataset from 30 columns of features
    down to only 19, making it a little easier to handle and visualize! With the dataset
    now completely preprocessed, we are now ready to start training and preparing
    some ML models.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，高度相关的特征组不再存在。现在我们已经解决了相关性问题，不仅确保了我们创建的任何潜在模型都不会因任何与 *多重共线性* 相关的性能问题而受到影响，而且无意中还将数据集的特征列从
    30 列减少到只有 19 列，这使得处理和可视化变得稍微容易一些！现在数据集已经完全预处理完毕，我们现在可以开始训练并准备一些机器学习模型。
- en: Developing and validating models
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发和验证模型
- en: Now that the data is ready to go, we can explore a few models. Recall that our
    objective here is to develop a *classification* model. Therefore, our first step
    will be to separate our `X` and *ŷ* values.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经准备好，我们可以探索一些模型。回想一下，我们在这里的目标是开发一个 *分类* 模型。因此，我们的第一步将是分离我们的 `X` 和 *ŷ* 值。
- en: 'We will create a variable, `X`, representing all of the features within the
    dataset (excluding the `id` and `diagnosis` columns, as these are not features).
    We will then create a variable, `y`, representing the output column:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个变量 `X`，代表数据集中所有的特征（排除 `id` 和 `diagnosis` 列，因为这些不是特征）。然后我们将创建一个变量 `y`，代表输出列：
- en: '[PRE22]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Within most of the datasets we will work with, we will generally see a large
    difference in the *magnitude* of values, in the sense that one column could be
    on the order of 1,000, and another column could be on the order of 0.1\. This
    means that features with far greater values will be perceived by the model to
    make far greater contributions to a prediction – which is not true. For example,
    think of a project in which we are trying to predict the lipophilicity of a molecule
    using 30 different features, with one of those being the molecular weight – a
    feature with a significantly large value but not that large a contribution.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们将要处理的大多数数据集中，我们通常会看到值的大小有很大的差异，也就是说，一列可能是1,000的数量级，而另一列可能是0.1的数量级。这意味着具有远大值的特征会被模型认为对预测有更大的贡献——这并不正确。例如，考虑一个项目，我们试图使用30个不同的特征来预测分子的亲脂性，其中一个特征是分子量——这个特征具有显著大的值，但贡献并不大。
- en: 'To address this challenge, values within a dataset must be `StandardScaler()`
    function from the `sklearn` library:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了应对这个挑战，数据集中的值必须使用`sklearn`库中的`StandardScaler()`函数进行标准化：
- en: '[PRE23]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'With the features now normalized, our next step is to split the data up into
    our *training* and *testing* sets. Recall that the purpose of the training set
    is to train the model, and the testing set is to test the model. This is done
    to avoid any *overfitting* in the development process:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在特征已经标准化，我们的下一步是将数据分成我们的*训练集*和*测试集*。回想一下，训练集的目的是训练模型，测试集的目的是测试模型。这样做是为了避免在开发过程中的任何*过拟合*：
- en: '[PRE24]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'With the data now split up into four variables, we are now ready to train a
    few models, beginning with the **Gaussian Naïve Bayes classifier**. This model
    is a supervised algorithm based on the application of Bayes'' theorem. The model
    is called *naïve* because it makes the assumption that the *features* of each
    *observation* are independent of one another, which is rarely true. However, this
    model tends to show strong performance anyway. The main idea behind the Gaussian
    Naïve Bayes classifier can be examined from a *probability* perspective. To explain
    what we mean by this, consider the following equation:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在数据已经分成四个变量，我们现在可以开始训练几个模型，首先是**高斯朴素贝叶斯分类器**。这个模型是一个基于贝叶斯定理应用的监督算法。这个模型被称为*朴素*，因为它假设每个*观测值*的特征是相互独立的，这很少是真实的。然而，这个模型仍然表现出强大的性能。高斯朴素贝叶斯分类器背后的主要思想可以从*概率*的角度来考察。为了解释我们的意思，考虑以下方程：
- en: '![](img/Formula_B17761_05_001.jpg)'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/Formula_B17761_05_001.jpg)'
- en: This states that the probability of the label (given some data) is equal to
    the probability of the data (given a label – Gaussian, given the normal distribution)
    multiplied by the probability of the label (prior probability), all divided by
    the probability of the data (predictor prior probability). Given the simplicity
    of such a model, Naïve Bayes classifiers can be extremely fast to use in relation
    to more complex models.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表示标签的概率（给定一些数据）等于数据（给定标签——高斯，给定正态分布）的概率乘以标签的概率（先验概率），所有这些除以数据的概率（预测先验概率）。鉴于这样一个模型的简单性，朴素贝叶斯分类器在相对于更复杂的模型时可以非常快地使用。
- en: 'Let''s take a look at its implementation. We will begin by importing our libraries
    of interest:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看它的实现。我们首先导入我们感兴趣的库：
- en: '[PRE25]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we can create an instance of the actual model in the form of a variable
    we call `gnb_clf`:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建一个实际模型的实例，我们将这个变量称为`gnb_clf`：
- en: '[PRE26]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can then fit or train the model using the training dataset we split off
    earlier:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用之前分离开的训练数据集来拟合或训练模型：
- en: '[PRE27]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we can use the trained model to make predictions on the testing data
    and compare the results with the known values. We can use a simple accuracy score
    to test the model:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用训练好的模型对测试数据进行预测，并将结果与已知值进行比较。我们可以使用一个简单的准确率分数来测试模型：
- en: '[PRE28]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With that, we have successfully developed a model performing with roughly 95%
    accuracy – not a bad start for our first model!
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有了这个，我们已经成功开发了一个大约95%准确率的模型——对我们第一个模型来说，这真是个不错的开始！
- en: 'While accuracy is always a fantastic metric, it is not the only metric we can
    use to assess the performance of a model. We can also use `classification_report()`
    function provided by `sklearn`:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然准确率始终是一个出色的指标，但它不是我们用来评估模型性能的唯一指标。我们还可以使用`sklearn`提供的`classification_report()`函数：
- en: '[PRE29]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Looking at the following output, we can see our two classes of interest (B
    and M) listed with their respective metrics: `precision`, `recall`, and `f1-score`:'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过查看以下输出，我们可以看到我们感兴趣的两种类别（B和M）及其相应的指标：`precision`、`recall`和`f1-score`：
- en: '![Figure 5.17 – The classification report of the Naïve Bayes classifier ](img/B17761_05_017.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图5.17 – Naïve Bayes分类器的分类报告](img/B17761_05_017.jpg)'
- en: Figure 5.17 – The classification report of the Naïve Bayes classifier
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17 – Naïve Bayes分类器的分类报告
- en: We will discuss these metrics in much more detail in [*Chapter 7*](B17761_07_Final_JM_ePub.xhtml#_idTextAnchor101),
    *Understanding Supervised Machine Learning*. For now, we can see that all of these
    metrics are quite high, indicating that the model performed reasonably well.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第7章*](B17761_07_Final_JM_ePub.xhtml#_idTextAnchor101)“理解监督机器学习”中更详细地讨论这些指标。现在，我们可以看到所有这些指标都很高，这表明模型表现相当不错。
- en: Saving a model for deployment
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存模型以部署
- en: 'When an ML model has been trained and is operating at a reasonable level of
    accuracy, we may wish to make this model available for others to use. However,
    we would not directly deliver the data or the code to data engineers to deploy
    the model into production. Instead, we would want to deliver a single trained
    model that they can take and deploy without having to worry about any moving pieces.
    Luckily for us, there is a great library known as `pickle` that can help us *gather*
    the model into a single entity, allowing us to *save* the model. Recall that we
    explored the `pickle` library in [*Chapter 2*](B17761_02_Final_JM_ePub.xhtml#_idTextAnchor023),
    *Getting Started with Python and the Command Line*. We *pickle* a model, such
    as the model we named `gnb_clf`, by using the `dump()` function:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个机器学习模型经过训练并在合理的准确度水平上运行时，我们可能希望使这个模型可供他人使用。然而，我们不会直接将数据或代码交付给数据工程师以部署模型到生产环境中。相反，我们希望交付一个单一的已训练模型，他们可以将其部署而无需担心任何移动部件。幸运的是，有一个名为`pickle`的出色库可以帮助我们将模型收集成一个单一实体，从而允许我们保存模型。回想一下，我们在[*第2章*](B17761_02_Final_JM_ePub.xhtml#_idTextAnchor023)“Python和命令行入门”中探讨了`pickle`库。我们通过使用`dump()`函数来*pickle*一个模型，例如我们命名为`gnb_clf`的模型：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To prove that the model did in fact save correctly, we can load it using the
    `load()` function, and once again, we can calculate the accuracy score:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明模型确实正确保存，我们可以使用`load()`函数加载它，然后再次计算准确度得分：
- en: '[PRE31]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Notice that the output of this scoring calculation results in the same value
    (95%) as we saw earlier, indicating that the model did, in fact, save correctly!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个评分计算的结果与之前看到的相同（95%），这表明模型确实正确地保存了！
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took an ambitious step toward understanding some of the
    most important and useful concepts in ML. We looked over the various terms used
    to describe the field as it relates to the domain of AI, examined the main areas
    of ML and the governing categories of *supervised* and *unsupervised* learning,
    and then proceeded to explore the full process of developing an ML model for a
    given dataset.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们迈出了雄心勃勃的一步，去理解机器学习（ML）中一些最重要和有用的概念。我们回顾了用于描述该领域与人工智能（AI）领域相关的各种术语，检查了机器学习的主要领域以及*监督学习*和*无监督学习*的统治类别，然后继续探讨为给定数据集开发机器学习模型的全过程。
- en: While developing our model, we explored many useful steps. We explored and preprocessed
    the data to remove inconsistencies and missing values. We also examined the data
    in great detail, and we subsequently addressed issues relating to *multicollinearity*.
    Next, we developed a *Gaussian Naïve Bayes* classification model, which operated
    with a robust 95% rate of accuracy – on our first try too! Finally, we looked
    at one of the most common ways data scientists hand over their fully trained models
    to data engineers to move ML models into production.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发我们的模型时，我们探索了许多有用的步骤。我们探索并预处理了数据，以消除不一致性和缺失值。我们还对数据进行详细了解，并随后解决了与*多重共线性*相关的问题。接下来，我们开发了一个*高斯Naïve
    Bayes*分类模型，它以稳健的95%准确率运行——而且是在我们的第一次尝试中！最后，我们查看了一种数据科学家将完全训练好的模型交给数据工程师以将机器学习模型投入生产的最常见方式。
- en: Although we took the time within this chapter to understand ML within the scope
    of a supervised classifier, in the following chapter, we will gain a much better
    understanding of the nuances and differences as we train several unsupervised
    models.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在这章中花时间理解了监督分类器范围内的机器学习，但在下一章中，我们将通过训练几个无监督模型，获得对细微差别和差异的更深入理解。
