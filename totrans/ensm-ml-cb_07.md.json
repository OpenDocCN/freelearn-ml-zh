["```py\n# import required packages\nimport os\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\n\n# Set working directory as per your need\nos.chdir(\".../.../Chapter 8\")\nos.getcwd()\n```", "```py\ndf_breastcancer = pd.read_csv(\"breastcancer.csv\")\n```", "```py\ndf_breastcancer.head(5)\n```", "```py\n# import LabelEncoder from sklearn.preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nlb = LabelEncoder()\ndf_breastcancer['diagnosis'] =lb.fit_transform(df_breastcancer['diagnosis']) \ndf_breastcancer.head(5)\n```", "```py\ndf_breastcancer.isnull().sum()\n```", "```py\ndf_breastcancer.shape\n```", "```py\n# Create feature & response variables\n# Drop the response var and id column as it'll not make any sense to the analysis\nX = df_breastcancer.iloc[:,2:31]\n\n# Target\nY = df_breastcancer.iloc[:,0]\n\n# Create train & test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=0, stratify= Y)\n```", "```py\ndtree = DecisionTreeClassifier(max_depth=3, random_state=0)\ndtree.fit(X_train, Y_train)\n```", "```py\n# Mean accuracy\nprint('The mean accuracy is: ',(dtree.score(X_test,Y_test))*100,'%')\n\n#AUC score\ny_pred_dtree = dtree.predict_proba(X_test)\nfpr_dtree, tpr_dtree, thresholds = roc_curve(Y_test, y_pred_dtree[:,1])\nauc_dtree = auc(fpr_dtree, tpr_dtree)\nprint ('AUC Value: ', auc_dtree)\n```", "```py\nAdaBoost = AdaBoostClassifier(n_estimators=100, base_estimator=dtree, learning_rate=0.1, random_state=0)\nAdaBoost.fit(X_train, Y_train)\n```", "```py\n# Mean accuracy\nprint('The mean accuracy is: ',(AdaBoost.score(X_test,Y_test))*100,'%')\n\n#AUC score\ny_pred_adaboost = AdaBoost.predict_proba(X_test)\nfpr_ab, tpr_ab, thresholds = roc_curve(Y_test, y_pred_adaboost[:,1])\nauc_adaboost = auc(fpr_ab, tpr_ab)\nprint ('AUC Value: ', auc_adaboost)\n```", "```py\n# Tuning the hyperparams\nAdaBoost_with_tuning = AdaBoostClassifier(n_estimators=100, base_estimator=dtree, learning_rate=0.4, random_state=0)\nAdaBoost_with_tuning.fit(X_train, Y_train)\n```", "```py\n# Mean accuracy\nprint('The mean accuracy is: ',(AdaBoost_with_tuning.score(X_test,Y_test))*100,'%')\n\n#AUC score\ny_pred_adaboost_tune = AdaBoost.predict_proba(X_test)\nfpr_ab_tune, tpr_ab_tune, thresholds = roc_curve(Y_test, y_pred_adaboost_tune[:,1])\nauc_adaboost_tune = auc(fpr_ab_tune, tpr_ab_tune)\nprint ('AUC Value: ', auc_adaboost_tune)\n```", "```py\nfrom sklearn.svm import SVC\n\nAdaboost_with_svc_rbf = AdaBoostClassifier(n_estimators=100, base_estimator=SVC(probability=True, kernel='rbf'), learning_rate=1, random_state=0)\nAdaboost_with_svc_rbf.fit(X_train, Y_train)\n```", "```py\n# Mean accuracy\nprint('The mean accuracy is: ',(Adaboost_with_svc_rbf.score(X_test,Y_test))*100,'%') \n\n#AUC score\ny_pred_svc_rbf = Adaboost_with_svc_rbf.predict_proba(X_test)\nfpr_svc_rbf, tpr_svc_rbf, thresholds = roc_curve(Y_test, y_pred_svc_rbf[:,1])\nauc_svc_rbf = auc(fpr_svc_rbf, tpr_svc_rbf)\nprint ('AUC Value: ', auc_svc_rbf)\n```", "```py\nAdaboost_with_svc_linear =AdaBoostClassifier(n_estimators=100, base_estimator=SVC(probability=True, kernel='linear'), learning_rate=1, random_state=0)\nAdaboost_with_svc_linear.fit(X_train, Y_train)\n```", "```py\nimport matplotlib.pyplot as plt\n% matplotlib inline\nplt.figure(figsize=(8,8))\n\nplt.plot(fpr_dtree, tpr_dtree,label=\"Model1: Dtree, auc=\"+str(auc_dtree))\nplt.plot(fpr_ab, tpr_ab,label=\"Model2: Adaboost, auc=\"+str(auc_adaboost))\nplt.plot(fpr_ab_tune,tpr_ab_tune,label=\"Model3: Adaboost with Tuning, auc=\"+str(auc_adaboost_tune))\nplt.plot(fpr_svc_rbf, tpr_svc_rbf, label=\"Model4: Adaboost with SVC (RBF Kernel), auc=\"+str(auc_svc_rbf))\nplt.plot(fpr_svc_lin, tpr_svc_lin, label=\"Model5: Adaboost with SVC (Linear Kernel), auc=\"+str(auc_svc_linear))\n\nplt.legend(loc=5)\nplt.show()\n```", "```py\nimport matplotlib.pyplot as plt\n% matplotlib inline\nplt.figure(figsize=(8,8))\n\nlabel = ['Decison Tree', 'Adaboost', 'Adaboost with Tuning', 'Adaboost with SVC (RBF)', 'Adaboost with SVC (Linear)']\n\nvalues = [dtree.score(X_test,Y_test),\n        AdaBoost.score(X_test,Y_test),\n        AdaBoost_with_tuning.score(X_test,Y_test),\n        Adaboost_with_svc_rbf.score(X_test,Y_test),\n        Adaboost_with_svc_linear.score(X_test,Y_test)]\n\ndef plot_bar_accuracy():\n    # this is for plotting purpose\n    index = np.arange(len(label))\n    plt.bar(index, values)\n    plt.xlabel('Algorithms', fontsize=10)\n    plt.ylabel('Accuracy', fontsize=10)\n    plt.xticks(index, label, fontsize=10, rotation=90)\n    plt.title('Model Accuracies')\n    plt.show()\n\nplot_bar_accuracy()\n```", "```py\n#grid search using svm\nAdaboost_with_svc = AdaBoostClassifier(n_estimators=100, base_estimator=SVC(probability=True, kernel='linear'), learning_rate=1, algorithm= 'SAMME')\n\nAda_Grid = {'n_estimators': [10,30,40,100],\n           'learning_rate': [0.1, 0.2, 0.3]}\n\nestimator = Adaboost_with_svc\nAdaboost_with_grid_search = GridSearchCV(estimator,Ada_Grid).fit(X_train, Y_train)\nprint(Adaboost_with_grid_search.best_params_)\nprint(Adaboost_with_grid_search.best_score_)\n```", "```py\nimport os\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport matplotlib.pyplot as plt\nimport itertools\n```", "```py\n# Read the Dataset\ndf_breastcancer = pd.read_csv(\"breastcancer.csv\")\n\nfrom sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ndf_breastcancer['diagnosis'] = lb.fit_transform(df_breastcancer['diagnosis']) \ndf_breastcancer.head(5)\n```", "```py\n# create feature & response variables\n# drop the response var and id column as it'll not make any sense to the analysis\nX = df_breastcancer.iloc[:,2:31]\n\n# Target variable\nY = df_breastcancer.iloc[:,0]\n\n# Create train & test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=0, stratify= Y)\n```", "```py\nGBM_model = GradientBoostingClassifier() \nGBM_model.fit(X_train, Y_train)\n```", "```py\nY_pred_gbm = GBM_model.predict(X_test)\n```", "```py\nprint(classification_report(Y_test, Y_pred_gbm))\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\n```", "```py\nparameters = {\n    \"n_estimators\":[100,150,200],\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n    \"min_samples_split\":np.linspace(0.1, 0.5, 4),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5, 4),\n    \"max_depth\":[3, 5, 8],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\", \"mae\"],\n    \"subsample\":[0.3, 0.6, 1.0]\n    }\n```", "```py\ngrid = GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs=-1) \ngrid.fit(X_train, Y_train)\n```", "```py\ngrid.best_estimator_\n```", "```py\ngrid_predictions = grid.predict(X_test)\n```", "```py\nprint(classification_report(Y_test, grid_predictions))\n```", "```py\ncnf_matrix = confusion_matrix(Y_test, grid_predictions)\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\n\n```", "```py\nprint(\"Accuracy score = {:0.2f}\".format(accuracy_score(Y_test, grid_predictions)))\nprint(\"Area under ROC curve = {:0.2f}\".format(roc_auc_score(Y_test, grid_predictions)))\n```", "```py\n!pip install xgboost\n```", "```py\n# Import required libraries\nimport os\nimport pandas as pd\nimport numpy as np\n\nfrom numpy import sort\n\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_tree\nfrom xgboost import plot_importance\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport itertools\n```", "```py\nos.chdir(\"/.../Chapter 7\")\nos.getcwd()\n\ndf_glassdata = pd.read_csv('glassdata.csv')\ndf_glassdata.shape\n```", "```py\ndf_glassdata.head()\n```", "```py\n# split data into X and Y\nX = df_glassdata.iloc[:,1:10]\nY = df_glassdata.iloc[:,10]\n\nprint(X.shape)\nprint(Y.shape)\n```", "```py\ndf_glassdata.isnull().sum()\n```", "```py\n# Create train & test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=0)\n```", "```py\nxg_model = XGBClassifier()\nxg_model.fit(X_train, Y_train)\n```", "```py\nplot_tree(xg_model, num_trees=0, rankdir='LR')\nfig = pyplot.gcf()\nfig.set_size_inches(30, 30)\n```", "```py\nplot_tree(xg_model, num_trees=5, rankdir='LR')\nfig = pyplot.gcf()\nfig.set_size_inches(30, 30)\n```", "```py\ntest_predictions = xg_model.predict(X_test)\ntest_accuracy = accuracy_score(Y_test, test_predictions)\n\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\n```", "```py\nconfusion_matrix(Y_test, predictions)\n```", "```py\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n```", "```py\nY.unique()\n```", "```py\n# Set names to each level of our target variable\ntarget_names = [ '1', '2', '3', '5', '6', '7']\n\n# Pass Actual & Predicted values to confusion_matrix()\ncm = confusion_matrix(Y_test, predictions)\n\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names)\nplt.show()\n```", "```py\nprint(xg_model.feature_importances_)\n```", "```py\nplot_importance(xg_model)\n```", "```py\n# The threshold value to use for feature selection. \nfeature_importance = sort(xg_model.feature_importances_)\n\n# select features using threshold\nfor each_threshold in feature_importance:\n    selection = SelectFromModel(xg_model, threshold=each_threshold, prefit=True)\n\n    # Reduce X_train only to the selected feature\n    selected_feature_X_train = selection.transform(X_train)\n\n    # Train the model\n    selection_model = XGBClassifier()\n    selection_model.fit(selected_feature_X_train, Y_train)\n\n    # Reduce X_test only to the selected feature\n    selected_feature_X_test = selection.transform(X_test)\n\n    # Predict using the test value of the selected feature\n    predictions = selection_model.predict(selected_feature_X_test)\n\n    accuracy = accuracy_score(Y_test, predictions)\n    print(\"Threshold=%.5f, Number of Features=%d, Model Accuracy: %.2f%%\" % (each_threshold, selected_feature_X_train.shape[1],accuracy*100))\n```", "```py\nkfold = KFold(n_splits=40, random_state=0)\nxg_model_with_kfold = XGBClassifier()\n\ncv_results = cross_val_score(xg_model_with_kfold, X_train, Y_train, cv=kfold, verbose=True)\nprint(\"Mean Accuracy: %.2f%% Standard Deviation %.2f%%\" % (cv_results.mean()*100, cv_results.std()*100))\n```", "```py\nStratfold = StratifiedKFold(n_splits=40, random_state=0)\nxg_model_with_stratfold = XGBClassifier()\n\nsf_results = cross_val_score(xg_model_with_stratfold, X_train, Y_train, cv=Stratfold, verbose=True)\nprint(\"Mean Accuracy: %.2f%% Standard Deviation %.2f%%\" % (sf_results.mean()*100, sf_results.std()*100))\n```"]