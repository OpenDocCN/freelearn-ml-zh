- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transforming Numerical Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The statistical methods that are used in data analysis make certain assumptions
    about the data. For example, in the general linear model, it is assumed that the
    values of the dependent variable (the target) are independent, that there is a
    linear relationship between the target and the independent (predictor) variables,
    and that the residuals – that is, the difference between the predictions and the
    real values of the target – are normally distributed and centered at `0`. When
    these assumptions are not met, the resulting probabilistic statements might not
    be accurate. To correct for failure in the assumptions and thus improve the performance
    of the models, we can transform variables before the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: When we transform a variable, we replace its original values with a function
    of that variable. Transforming variables with mathematical functions helps reduce
    variable skewness, improves the value spread, and sometimes unmasks linear and
    additive relationships between predictors and the target. Commonly used mathematical
    transformations include the logarithm, reciprocal, power, and square and cube
    root transformations, as well as the Box-Cox and Yeo-Johnson transformations.
    This set of transformations is commonly referred to as **variance stabilizing
    transformations**. Variance stabilizing transformations intend to bring the distribution
    of the variable to a more symmetric – that is, Gaussian – shape. In this chapter,
    we will discuss when to use each transformation and then implement them using
    NumPy, scikit-learn, and Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Transforming variables with the logarithm function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming variables with the reciprocal function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the square root to transform variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using power transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing Box-Cox transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing Yeo-Johnson transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming variables with the logarithm function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The logarithm function is a powerful transformation for dealing with positive
    data with a right-skewed distribution (observations accumulate at lower values
    of the variable). A common example is the `income` variable, with a heavy accumulation
    of values toward lower salaries. The logarithm transformation has a strong effect
    on the shape of the variable distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will perform logarithmic transformation using NumPy, scikit-learn,
    and Feature-engine. We will also create a diagnostic plot function to evaluate
    the effect of the transformation on the variable distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate the variable distribution and understand whether a transformation
    improves value spread and stabilizes the variance, we can visually inspect the
    data with histograms and **Quantile-Quantile** (**Q-Q**) plots. A Q-Q plot helps
    us determine whether two variables show a similar distribution. In a Q-Q plot,
    we plot the quantiles of one variable against the quantiles of the second variable.
    If we plot the quantiles of the variable of interest against the expected quantiles
    of the normal distribution, then we can determine whether our variable is also
    normally distributed. If the variable is normally distributed, the points in the
    Q-Q plot will fall along a 45-degree diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A quantile is the value below which there is a certain fraction of data points
    in the distribution. Thus, the 20th quantile is the point in the distribution
    at which 20% of the observations fall below and 80% above that value.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the libraries and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python libraries and dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s explore the distributions of all the variables in the dataset by plotting
    histograms with pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the `MedInc` variable shows a mild
    right-skewed distribution, variables such as `AveRooms` and `Population` are heavily
    right-skewed, and the `HouseAge` variable shows an even spread of values across
    its range:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: figure 3
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Histograms with the distribution of the numerical variables](img/B22396_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Histograms with the distribution of the numerical variables
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the effect of the transformation on the variable distribution,
    we’ll create a function that takes a DataFrame and a variable name as inputs and
    plots a histogram next to a Q-Q plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot the distribution of the `MedInc` variable with the function from
    *step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output shows that `MedInc` has a right-skewed distribution:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.2 – A histogram and Q-Q plot of the MedInc variable](img/B22396_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – A histogram and Q-Q plot of the MedInc variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s transform the data with the logarithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s make a copy of the original DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We’ve created a copy so that we can modify the values in the copy and not in
    the original DataFrame, which we need for the rest of this recipe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If we execute `X_tf = X` instead of using pandas’ `copy()`function, `X_tf` will
    not be a copy of the DataFrame; instead, it will be another view of the same data.
    Therefore, changes made in `X_tf` will be reflected in `X` as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make a list with the variables that we want to transform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s apply the logarithmic transformation with NumPy to the variables from
    *step 7* and capture the transformed variables in the new DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the logarithm transformation can only be applied to strictly positive
    variables. If the variables have zero or negative values, sometimes, it is useful
    to add a constant to make those values positive. We could add a constant value
    of `1` using `X_tf[vars] = np.log(X[vars] +` `1)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check the distribution of `MedInc` after the transformation with the
    diagnostic function from *step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the logarithmic transformation returned
    a more evenly distributed variable that better approximates the theoretical normal
    distribution in the Q-Q plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 3.3 – A histogram and Q-Q plot of the MedInc variable after the logarithm\
    \ transformation\uFEFF](img/B22396_03_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – A histogram and Q-Q plot of the MedInc variable after the logarithm
    transformation
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and plot the other transformed variables to familiarize yourself with
    the effect of the logarithm transformation on distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s apply the logarithmic transformation with `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import `FunctionTransformer()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Before we proceed, we need to take a copy of the original dataset, as we did
    in *step 6*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We’ll set up the transformer to apply the logarithm and to be able to revert
    the transformed variable to its original representation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If we set up `FunctionTransformer()` with the default parameter, `validate=False`,
    we don’t need to fit the transformer before transforming the data. If we set `validate`
    to `True`, the transformer will check the data input to the `fit` method. The
    latter is useful when fitting the transformer with a DataFrame so that it learns
    and stores the variable names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s transform the positive variables from *step 7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn transformers return NumPy arrays and transform the entire DataFrame
    by default. In this case, we assigned the results of the array directly to our
    existing DataFrame. We can change the returned format through the `set_output`
    method and we can restrict the variables to transform with `ColumnTransformer()`.
  prefs: []
  type: TYPE_NORMAL
- en: Check the results of the transformation with the diagnostic function from *step
    4*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now revert the transformation to the original variable representation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you check the distribution by executing `diagnostic_plots(X_tf, "MedInc")`,
    you should see a plot that is identical to that returned by *step 5*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To add a constant value to the variables, in case they are not strictly positive,
    use `transformer = FunctionTransformer(lambda x: np.log(x +` `1))`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s apply the logarithm transformation with Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the `LogTransformer()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We’ll set up the transformer to transform the variables from *step 7* and then
    fit the transformer to the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the `variables` argument is left as `None`, `LogTransformer()` applies the
    logarithm to all the numerical variables found during `fit()`. Alternatively,
    we can indicate which variables to modify, as we did in *step 15*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s transform the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_tf` is a copy of the `X` DataFrame, where the variables from *step 7* are
    transformed with the logarithm.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can also revert the transformed variables to their original representation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you check the distribution of the variables after *step 17*, they should
    be identical to those of the original data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Feature-engine has a dedicated transformer that adds constant values to the
    variables before the applying the logarithm transformation. Check the *There’s
    more…* section later in this recipe for more details.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we applied the logarithm transformation to a subset of positive
    variables using NumPy, scikit-learn, and Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: To compare the effect of the transformation on the variable distribution, we
    created a diagnostic function to plot a histogram next to a Q-Q plot. To create
    the Q-Q plot, we used `scipy.stats.probplot()`, which plotted the quantiles of
    the variable of interest in the *y* axis versus the quantiles of a theoretical
    normal distribution, which we indicated by setting the `dist` parameter to `norm`
    in the *x* axis. We used `matplotlib` to display the plot by setting the `plot`
    parameter to `plt`.
  prefs: []
  type: TYPE_NORMAL
- en: With `plt.figure()` and `figsize`, we adjusted the size of the figure and, with
    `plt.subplot()`, we organized the two plots in `one` row with `two` columns –
    that is, one plot next to the other. The numbers within `plt.subplot()` indicated
    the number of rows, the number of columns, and the place of the plot in the figure,
    respectively. We placed the histogram in position 1 and the Q-Q plot in position
    2 – that is, left and right, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: To test the function, we plotted a histogram and a Q-Q plot for the `MedInc`
    variable before the transformation and observed that `MedInc` was not normally
    distributed. Most observations were at the left of the histogram and the values
    deviated from the 45-degree line in the Q-Q plot at both ends of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Next, using `np.log()`, we applied the logarithm to a slice of the DataFrame
    with four positive variables. To evaluate the effect of the transformation, we
    plotted a histogram and Q-Q plot of the transformed `MedInc` variable. We observed
    that, after the logarithm transformation, the values were more centered in the
    histogram and that, in the Q-Q plot, they only deviated from the 45-degree line
    toward the ends of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we used the `FunctionTransformer()` from scikit-learn, which applies any
    user-defined function to a dataset. We passed `np.log()` as an argument to apply
    the logarithm transformation and NumPy’s `exp()` for the inverse transformation
    to `FunctionTransfomer()`. With the `transform()` method, we transformed a slice
    of the DataFrame with the positive variables by using the logarithm. With `inverse_transform()`,
    we reverted the variable values to their original representation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we used Feature-engine’s `LogTransformer()` and specified the variables
    to transform in a list using the `variables` argument. With `fit()`, the transformer
    checked that the variables were numerical and positive, and with `transform()`,
    it applied `np.log()` under the hood to transform the selected variables. With
    `inverse_transform()`, we reverted the transformed variables to their original
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature-engine has a dedicated transformer for adding a constant value to variables
    that are not strictly positive, before applying the logarithm: `LogCpTransformer()`.
    `LogCpTransformer()` can:'
  prefs: []
  type: TYPE_NORMAL
- en: Add the same constant to all variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically identify and add the minimum value required to make the variables
    positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add different values defined by the user to different variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find a code implementation of `LogCpTransformer()` in this book’s GitHub
    repository: [https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch03-variable-transformation/Recipe-1-logarithmic-transformation.ipynb](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch03-variable-transformation/Recipe-1-logarithmic-transformation.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: Transforming variables with the reciprocal function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reciprocal function is defined as 1/x. It is often useful when we have ratios
    – that is, values resulting from the division of two variables. Examples of this
    are **population density** – that is, people per area – and, as we will see in
    this recipe, **house occupancy** – that is, the number of occupants per house.
  prefs: []
  type: TYPE_NORMAL
- en: The reciprocal transformation is not defined for the `0` value, and although
    it is defined for negative values, it is mainly useful for transforming positive
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will implement the reciprocal transformation using `NumPy`,
    `scikit-learn`, and `Feature-engine`, and compare its effect on variable distribution
    using histograms and a Q-Q plot.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the libraries and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python libraries and data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To evaluate variable distributions, we’ll create a function that takes a DataFrame
    and a variable name as inputs and plots a histogram next to a Q-Q plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s plot the distribution of the `AveOccup` variable, which specifies
    the average occupancy of the house:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `AveOccup` variable shows a very strong right-skewed distribution, as shown
    in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.4 – A histogram and Q-Q plot of the AveOccup variable](img/B22396_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – A histogram and Q-Q plot of the AveOccup variable
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `AveOccup` variable refers to the average number of household members –
    that is, the ratio between the number of people and the number of houses in a
    certain area. This is a promising variable for a reciprocal transformation. You
    can find more details about the variables and the dataset by executing `data =
    fetch_california_housing()` followed by `print(data.DESCR)`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s apply the reciprocal transformation with NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s make a copy of the original DataFrame so that we can modify the
    values in the copy and not in the original one, which we will need for the rest
    of this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Remember that executing `X_tf = X` instead of using pandas’ `copy()` creates
    an additional view of the same data. Therefore, changes that are made in `X_tf`
    will be reflected in `X` as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply the reciprocal transformation to the `AveOccup` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s check the distribution of the `AveOccup` variable after the transformation
    with the diagnostic function we created in *step 3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: After the transformation, `AveOccup` is now the ratio of the number of houses
    and the number of people in a certain area – in other words, houses per citizen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see a dramatic change in the distribution of the `AveOccup` variable
    after the reciprocal transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.5 – A histogram and Q-Q plot of the AveOccup variable after the\
    \ reciproc\uFEFFal transfo\uFEFFrmation](img/B22396_03_05.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – A histogram and Q-Q plot of the AveOccup variable after the reciprocal
    transformation
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s apply the reciprocal transformation with `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the `FunctionTransformer()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the transformer by passing `np.reciprocal` as an argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By default, `FunctionTransformer()` does not need to be fit before transforming
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s make a copy of the original dataset and transform the variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can check the effect of the transformation using the function from *step
    3*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The inverse transformation of the reciprocal function is also the reciprocal
    function. Hence, if you re-apply `transform()` to the transformed data, you will
    revert it to its original representation. A better practice would be to set the
    `inverse_transform` parameter of the `FunctionTransformer()` to `np.reciprocal`
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s apply the reciprocal transformation with `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the `ReciprocalTransformer()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the transformer to modify the `AveOccup` variable and then fit
    it to the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the `variables` argument is set to `None`, the transformer applies the reciprocal
    function to *all the numerical variables* in the dataset. If some of the variables
    contain a `0` value, the transformer will raise an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s transform the selected variable in our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ReciprocalTransformer()` will return a new pandas DataFrame containing the
    original variables, where the variable indicated in *step 12* is transformed with
    the reciprocal function.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we applied the reciprocal transformation using NumPy, scikit-learn,
    and Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the variable distribution, we used the function to plot a histogram
    next to a Q-Q plot that we described in the *How it works…* section of the *Transforming
    variables with the logarithm function* recipe earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We plotted the histogram and Q-Q plot of the `AveOccup` variable, which showed
    a heavy right-skewed distribution; most of its values were at the left of the
    histogram and they deviated from the 45-degree line toward the right end of the
    distribution in the Q-Q plot.
  prefs: []
  type: TYPE_NORMAL
- en: To carry out the reciprocal transformation, we applied `np.reciprocal()` to
    the variable. After the transformation, `AveOccup`’s values were more evenly distributed
    across the value range and followed the theoretical quantiles of the normal distribution
    in the Q-Q plot more closely.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we used `np.reciprocal()` with scikit-learn’s `FunctionTransformer()`.
    The `transform()` method applied `np.reciprocal()` to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To restrict the effect of `FunctionTransformer()` to a group of variables, use
    the `ColumnTransformer()`. To change the output to a pandas DataFrame, set the
    transform output to pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we used Feature-engine’s `ReciprocalTransformer()` to specifically
    modify one variable. With `fit()`, the transformer checked that the variable was
    numerical. With `transform()`, the transformer applied `np.reciprocal()` under
    the hood to transform the variable.
  prefs: []
  type: TYPE_NORMAL
- en: Feature-engine’s `ReciprocalTransformer()` provides functionality to revert
    the variable to its original representation out of the box via the `inverse_transform()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Using the transformers from scikit-learn or Feature-engine, instead of NumPy’s
    `reciprocal()` function, allows us to apply the reciprocal function as an additional
    step of a feature engineering pipeline within the `Pipeline` object from scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between `FunctionTransformer()` and `ReciprocalTransformer()`
    is that the first one can apply any user-specified transformation, whereas the
    latter only applies the reciprocal function. scikit-learn returns NumPy arrays
    by default and transforms all variables in the dataset. Feature-engine’s transformer,
    on the other hand, returns pandas DataFrames and can modify subsets of variables
    within the data without using additional classes.
  prefs: []
  type: TYPE_NORMAL
- en: Using the square root to transform variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The square root transformation, **√x**, as well as its variations, the Anscombe
    transformation, **√(x+3/8)**, and the Freeman-Tukey transformation, **√x + √(x+1)**,
    are variance stabilizing transformations that transform a variable with a Poisson
    distribution into one with an approximately standard Gaussian distribution. The
    square root transformation is a form of power transformation where the exponent
    is **1/2** and is only defined for positive values.
  prefs: []
  type: TYPE_NORMAL
- en: The Poisson distribution is a probability distribution that indicates the number
    of times an event is likely to occur. In other words, it is a count distribution.
    It is right-skewed and its variance equals its mean. Examples of variables that
    could follow a Poisson distribution are the number of financial items of a customer,
    such as the number of current accounts or credit cards, the number of passengers
    in a vehicle, and the number of occupants in a household.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will implement square root transformations using NumPy, scikit-learn,
    and Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll start by creating a dataset with two variables whose values are drawn
    from a Poisson distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a DataFrame with two variables drawn from a Poisson distribution
    with mean values of `2` and `3`, respectively, and `10000` observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a function that takes a DataFrame and a variable name as inputs
    and plots a bar graph with the number of observations per value next to a Q-Q
    plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a bar plot and a Q-Q plot for one of the variables in the data
    using the function from *step 3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we can see the Poisson distribution in the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 3.6 – The bar and Q-Q pl\uFEFFots of the counts1 variable](img/B22396_03_06.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – The bar and Q-Q plots of the counts1 variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s make a copy of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s apply the square root transformation to both variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s round the values to two decimals for a nicer visualization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot the distribution of `counts1` after the transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We see a more **stabilized** variance, as the dots in the Q-Q plot follow the
    45-degree diagonal more closely:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 3.7 – The bar and Q-Q plots of the counts1 variable after th\uFEFF\
    e square root transformation](img/B22396_03_07.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – The bar and Q-Q plots of the counts1 variable after the square
    root transformation
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s apply the square root transformation with `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import `FunctionTransformer()` and set up it to perform a square root
    transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If we wanted to round the values as we did in *step 7*, we can set up the transformer
    using `transformer = FunctionTransformer(func=lambda x:` `np.round(np.sqrt(x),
    2))`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make a copy of the data and transform the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Go ahead and check the result of the transformation as we did in *step 8*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To apply the square root with Feature-engine instead, we use the `PowerTransformer()`
    with an exponent of 0.5:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we fit the transformer to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The transformer automatically identifies the numerical variables, which we can
    explore by executing `root_t.variables_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s transform the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`PowerTransformer()` returns a pandas DataFrame with the transformed variables.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we applied the square root transformation using NumPy, scikit-learn,
    and Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: We used NumPy’s `sqrt()` function either directly or within scikit-learn’s `FunctionTrasnformer()`
    to determine the variables’ square root. Alternatively, we used Feature-engine’s
    `PowerTransformer()`, setting the exponent to 0.5, that of the square root function.
    NumPy modified the variables directly. The transformers of scikit-learn and Feature-engine
    modified the variables when calling the `transform()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Using power transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Power functions are mathematical transformations that follow the ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><msup><mi>X</mi><mrow><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow></mrow></math>](img/15.png)
    format, where lambda can take any value. The square and cube root transformations
    are special cases of power transformations where lambda is 1/2 or 1/3, respectively.
    The challenge resides in finding the value for the lambda parameter. The Box-Cox
    transformation, which is a generalization of the power transformations, finds
    the optimal lambda value via maximum likelihood. We will discuss the Box-Cox transformation
    in the following recipe. In practice, we will try different lambda values and
    visually inspect the variable distribution to determine which one offers the best
    transformation. In general, if the data is right-skewed – that is, if observations
    accumulate toward lower values – we use a lambda value that is smaller than 1,
    while if the data is left-skewed – that is, there are more observations around
    higher values – then we use a lambda value that is greater than 1.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will carry out power transformations using NumPy, scikit-learn,
    and Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the libraries and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python libraries and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To evaluate variable distributions, we’ll create a function that takes a DataFrame
    and a variable name as inputs and plots a histogram next to a Q-Q plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot the distribution of the `Population` variable with the previous
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the plots returned by the previous command, we can see that `Population`
    is heavily skewed to the right:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 3.8 – A histogram and \uFEFFQ-Q plot of the Population variable](img/B22396_03_08.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – A histogram and Q-Q plot of the Population variable
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s apply a power transformation to the `MedInc` and `Population` variables.
    As both are skewed to the right, an exponent smaller than *1* might return a better
    spread of the variable values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s capture the variables to transform in a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a copy of the DataFrame and then apply a power transformation to
    the variables from *step 5,* where the exponent is `0.3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With `np.power()`, we can apply any power transformation by changing the value
    of the exponent in the second position of the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the change in the distribution of `Population`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As shown in the plots returned by the previous command, `Population` is now
    more evenly distributed across the value range and follows the quantiles of the
    normal distribution more closely:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 3.9 – A histogram and Q-Q plot of the Populatio\uFEFFn variable after\
    \ the transformation](img/B22396_03_09.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – A histogram and Q-Q plot of the Population variable after the transformation
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s apply a power transformation with scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up `FunctionTransformer()` with a power transformation with an exponent
    of `0.3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a copy of the DataFrame and transform the variables from *step 5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That’s it – we can now examine the variable distribution. Finally, let’s perform
    an exponential transformation with Feature-engine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s set up `PowerTransformer()` with an exponent of `0.3` to transform the
    variables from *step 5*. Then, we’ll fit it to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If we don’t define the variables to transform, `PowerTransformer()` will select
    and transform all of the numerical variables in the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s transform those two variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The transformer returns a DataFrame containing the original variables, where
    the two variables specified in *step 5* are transformed with the power function.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we applied power transformations using NumPy, scikit-learn,
    and Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: To apply power functions with NumPy, we applied the `power()` method to the
    slice of the dataset containing the variables to transform. To apply this transformation
    with scikit-learn, we set up the `FunctionTransformer()`with `np.power()` within
    a `lambda` function, using `0.3` as the exponent. To apply power functions with
    Feature-engine, we set up the `PowerTransformer()` with a list of the variables
    to transform and an exponent of `0.3`.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn and Feature-engine transformers applied the transformation when
    we called the `transform()` method. scikit-learn’s `FunctionTransformer()` modifies
    the entire dataset and returns NumPy arrays by default. To return pandas DataFrames,
    we need to set the transform output to pandas, and to apply the transformation
    to specific variables, we can use `ColumnTransformer()`. Feature-engine’s `PowerTransformer()`,
    on the other hand, can apply the transformation to a subset of variables out of
    the box, returning pandas DataFrames by default.
  prefs: []
  type: TYPE_NORMAL
- en: Performing Box-Cox transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Box-Cox transformation is a generalization of the power family of transformations
    and is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>λ</mi><mo>)</mo></mrow></msup><mo>=</mo><mfrac><mrow><mo>(</mo><msup><mi>y</mi><mi>λ</mi></msup><mo>−</mo><mn>1</mn><mo>)</mo></mrow><mi>λ</mi></mfrac><mi>i</mi><mi>f</mi><mi>λ</mi><mo>≠</mo><mn>0</mn></mrow></mrow></math>](img/16.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>λ</mi><mo>)</mo></mrow></msup><mo>=</mo><mi>log</mi><mfenced
    open="(" close=")"><mi>y</mi></mfenced><mi>i</mi><mi>f</mi><mi>λ</mi><mo>=</mo><mn>0</mn></mrow></mrow></math>](img/17.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* is the variable and *λ* is the transformation parameter. It includes
    important special cases of transformations, such as untransformed *(λ = 1)*, the
    logarithm *(λ = 0)*, the reciprocal *(λ = - 1)*, the square root (when *λ* *=
    0.5*, it applies a scaled and shifted version of the square root function), and
    the cube root.
  prefs: []
  type: TYPE_NORMAL
- en: The Box-Cox transformation evaluates several values of *λ* using the maximum
    likelihood and selects the *λ* parameter that returns the best transformation.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will perform the Box-Cox transformation using scikit-learn
    and Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The Box-Cox transformation can only be used on positive variables. If your variables
    have negative values, try the Yeo-Johnson transformation, which is described in
    the next recipe, *Performing Yeo-Johnson transformation*. Alternatively, you can
    shift the variable distribution by adding a constant before the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the necessary libraries and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python libraries and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s drop the `Latitude` and `Longitude` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s inspect the variable distributions with histograms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the `MedInc` variable shows a mild
    right-skewed distribution, variables such as `AveRooms` and `Population` are heavily
    right-skewed, and the `HouseAge` variable shows an even spread of values across
    its range:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Histograms of the numerical variables](img/B22396_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Histograms of the numerical variables
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s capture the variable names in a list since we will use these in the following
    step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a function that will plot Q-Q plots for all the variables in the
    data in two rows with three plots each:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s display the Q-Q plots using the preceding function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By looking at the following plots, we can corroborate that the variables are
    not normally distributed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Q-Q plots of the numerical variables](img/B22396_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Q-Q plots of the numerical variables
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s carry out the Box-Cox transformation using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up `PowerTransformer()` to apply the Box-Cox transformation and fit
    it to the data so that it finds the optimal *λ* parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To avoid data leakage, the *λ* parameter should be learned from the train set
    and then used to transform the train and test sets. Thus, remember to split your
    data into train and test sets before fitting `PowerTransformer()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s transform the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn’s `PowerTransformer()` stores the learned lambdas in its `lambdas_`
    attribute, which you can display by executing `transformer.lambdas_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect the distributions of the transformed data with histograms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the variables’ values are more evenly
    spread across their ranges:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Histograms of the variables after the transformation](img/B22396_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Histograms of the variables after the transformation
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s return Q-Q plots of the transformed variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that, after the transformation, the variables
    follow the theoretical normal distribution more closely:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 3.13 – Q-Q plots o\uFEFFf the variables after the transformation](img/B22396_03_13.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Q-Q plots of the variables after the transformation
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s implement the Box-Cox transformation with Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up `BoxCoxTransformer()` to transform all the variables in the dataset
    and then fit it to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s go ahead and transform the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The transformation returns a pandas DataFrame containing the modified variables.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`PowerTransformer()` from scikit-learn will transform the entire dataset. On
    the other hand, `BoxCoxTransformer()` from Feature-engine can modify a subset
    of the variables, if we pass their names in a list to the `variables` parameter
    when setting up the transformer. If the `variables` parameter is set to `None`,
    the transformer will transform all numerical variables seen during `fit()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimal lambdas for the Box-Cox transformation are stored in the `lambda_dict_`
    attribute. Let’s inspect them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the previous command is the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now you know how to implement the Box-Cox transformation with two different
    Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: scikit-learn’s `PowerTransformer()` can apply both Box-Cox and Yeo-Johnson transformations,
    so we specified the transformation when setting up the transformer by passing
    the b`ox-cox` string. Next, we fit the transformer to the data so that the transformer
    learned the optimal lambdas for each variable. The learned lambdas were stored
    in the `lambdas_` attribute. Finally, we used the `transform()` method to transform
    the variables.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Remember that to return DataFrames instead of arrays, you need to specify the
    transform output through the `set_output()` method. You can apply the transformation
    to a subset of values by using the `ColumnTransformer()`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we applied the Box-Cox transformation using Feature-engine. We initialized
    `BoxCoxTransformer()`, leaving the parameter `variables` set the `None`. Due to
    this, the transformer automatically found the numerical variables in the data
    during `fit()`. We fit the transformer to the data so that it learned the optimal
    lambdas per variable, which were stored in `lambda_dict_`, and transformed the
    variables using the `transform()` method. Feature-engine’s `BoxCoxTransformer()`
    can take the entire DataFrame as input and it yet modify only the selected variables.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can apply the Box-Cox transformation with the SciPy library. For a code
    implementation, visit this book’s GitHub repository: [https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch03-variable-transformation/Recipe-5-Box-Cox-transformation.ipynb](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch03-variable-transformation/Recipe-5-Box-Cox-transformation.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: Performing Yeo-Johnson transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Yeo-Johnson transformation is an extension of the Box-Cox transformation
    that is no longer constrained to positive values. In other words, the Yeo-Johnson
    transformation can be used on variables with zero and negative values, as well
    as positive values. These transformations are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac></mml:math>](img/18.png);
    if λ ≠ 0 and X >= 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ln(X + 1 ); if λ = 0 and X >= 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>−</mo><mstyle
    scriptlevel="+1"><mfrac><mrow><msup><mrow><mo>(</mo><mo>−</mo><mi>X</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow><mrow><mn>2</mn><mo>−</mo><mi>λ</mi></mrow></msup><mo>−</mo><mn>1</mn></mrow><mrow><mn>2</mn><mo>−</mo><mi>λ</mi></mrow></mfrac></mstyle></mrow></mrow></math>](img/19.png);
    if λ ≠ 2 and X < 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -ln(-X + 1); if λ = 2 and X < 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the variable has only positive values, then the Yeo-Johnson transformation
    is like the Box-Cox transformation of the variable plus one. If the variable has
    only negative values, then the Yeo-Johnson transformation is like the Box-Cox
    transformation of the negative of the variable plus one, at the power of *2- λ*.
    If the variable has a mix of positive and negative values, the Yeo-Johnson transformation
    applies different powers to the positive and negative values.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will perform the Yeo-Johnson transformation using scikit-learn
    and Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the necessary libraries and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python libraries and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset into a pandas DataFrame and then
    drop the `Latitude` and `Longitude` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can evaluate the variable distribution with histograms and Q-Q plots, as
    we did in *steps 4* to *7* of the *Performing Box-Cox* *transformations* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s apply the Yeo-Johnson transformation with scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up `PowerTransformer()` with the `yeo-johnson` transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s fit the transformer to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The *λ* parameter should be learned from the train set and then used to transform
    the train and test sets. Thus, remember to separate your data into train and test
    sets before fitting `PowerTransformer()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s transform the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`PowerTransformer()` stores the learned parameters in its `lambda_` attribute,
    which you can return by executing `transformer.lambdas_`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect the distributions of the transformed data with histograms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the variables’ values are more evenly
    spread across their ranges:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Histograms of the variables after the yeo-Johnson transformation](img/B22396_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Histograms of the variables after the yeo-Johnson transformation
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s implement the Yeo-Johnson transformation with Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up `YeoJohnsonTransformer()` to transform all numerical variables
    and then fit it to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the `variables` argument is left set to `None`, the transformer selects and
    transforms all the numerical variables in the dataset. Alternatively, we can pass
    the names of the variables to modify in a list.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to `PowerTransformer()` from scikit-learn, Feature-engine’s transformer
    can take the entire DataFrame as an argument of the `fit()` and `transform()`
    methods, and yet it will only modify the selected variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s transform the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`YeoJohnsonTransformer()` stores the best parameters per variable in its `lambda_dict_`
    attribute, which we can display as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous command returns the following dictionary:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now you know how to implement the Yeo-Johnson transformation with two different
    open source libraries.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we applied the Yeo-Johnson transformation using `scikit-learn`
    and `Feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn`’s `PowerTransformer()` can apply both Box-Cox and Yeo-Johnson
    transformations, so we specified the transformation with the `yeo-johnson` string.
    The `standardize` argument allowed us to determine whether we wanted to standardize
    (scale) the transformed values. Next, we fit the transformer to the DataFrame
    so that it learned the optimal lambdas for each variable. `PowerTransformer()`
    stored the learned lambdas in its `lambdas_` attribute. Finally, we used the `transform()`
    method to return the transformed variables. We set the transform output to `pandas`
    to return DataFrames after the transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: After that, we applied the Yeo-Johnson transformation using Feature-engine.
    We set up `YeoJohnsonTransformer()` so that it transforms all numerical variables
    seen during `fit()`. We fitted the transformer to the data so that it learned
    the optimal lambdas per variable, which were stored in `lambda_dict_`, and finally
    transformed the variables using the `transform()` method. Feature-engine’s `YeoJohnnsonTransformer()`
    can take the entire DataFrame as input, yet it will only transform the selected
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can apply the Yeo-Johnson transformation with the SciPy library. For a code
    implementation, visit this book’s GitHub repository: [https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Second-Edition/blob/main/ch03-variable-transformation/Recipe-6-Yeo-Johnson-transformation.ipynb](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Second-Edition/blob/main/ch03-variable-transformation/Recipe-6-Yeo-Johnson-transformation.ipynb)'
  prefs: []
  type: TYPE_NORMAL
