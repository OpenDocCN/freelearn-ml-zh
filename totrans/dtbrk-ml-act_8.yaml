- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Monitoring, Evaluating, and More
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控、评估和更多
- en: “Focus on how the end-user customers perceive the impact of your innovation
    – rather than on how you, the innovators, perceive it.” — Thomas A. Edison
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “关注最终用户客户如何看待你的创新的影响——而不是你，作为创新者，如何看待它。” —— 托马斯·A·爱迪生
- en: Congratulations, you’ve made it to the final chapter! We’ve come a long way,
    yet there is still more to explore in Databricks. As we wrap up, we will take
    another look at Lakehouse Monitoring. We’ll focus on monitoring model inference
    data. After all the work you’ve put in to build a robust model and push it into
    production, it’s essential to share the learnings, predictions, and other outcomes
    with a broad audience. Sharing results with dashboards is very common. We will
    cover how to create visualizations for dashboards in both the new Lakeview dashboards
    and the standard Databricks SQL dashboards. Deployed models can be shared via
    a web application. Therefore, we will not only introduce Hugging Face Spaces but
    also deploy the RAG chatbot using the Gradio app in *Applying our learning*. Lastly,
    we’ll demonstrate how analysts can invoke LLMs via SQL AI Functions! By the end
    of this chapter, you will be ready to monitor inference data, create visualizations,
    deploy an ML web app, and use the groundbreaking DBRX open source LLM with SQL.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你已经到达了最后一章！我们已经走了很长的路，但在Databricks中仍有更多内容可以探索。在我们结束之前，我们将再次审视Lakehouse Monitoring。我们将专注于监控模型推理数据。毕竟，你在构建一个健壮的模型并将其推入生产后投入了大量的工作，与广泛的受众分享学习成果、预测和其他结果至关重要。通过仪表板共享结果非常常见。我们将介绍如何在新的Lakeview仪表板和标准的Databricks
    SQL仪表板中创建仪表板可视化。部署的模型可以通过Web应用程序共享。因此，我们不仅将介绍Hugging Face Spaces，还将通过Gradio应用程序在*应用我们的学习*中部署RAG聊天机器人。最后，我们将演示分析师如何通过SQL
    AI函数调用LLMs！到本章结束时，你将准备好监控推理数据、创建可视化、部署ML Web应用程序，并使用突破性的DBRX开源LLM与SQL一起使用。
- en: 'Here is the roadmap for this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的路线图如下：
- en: Monitoring your models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控您的模型
- en: Building gold layer visualizations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建金层可视化
- en: Connecting your applications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接您的应用程序
- en: Incorporating LLMs for analysts
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为分析师整合LLMs
- en: Applying our learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: Monitoring your models
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控您的模型
- en: 'The ML lifecycle does not end at deployment. Once a model is in production,
    we want to monitor the input data and output results of the model. In [*Chapter
    4*](B16865_04.xhtml#_idTextAnchor180), we explored two key features of Databricks
    Lakehouse Monitoring integrated with Unity Catalog: Snapshot and TimeSeries profiles.
    Snapshot profiles are designed to provide an overview of a dataset at a specific
    point in time, capturing its current state. This is particularly useful for identifying
    immediate data quality issues or changes. On the other hand, TimeSeries profiles
    focus on how data evolves over time, making them ideal for tracking trends, patterns,
    and gradual changes in data distributions.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习生命周期并不在部署后结束。一旦模型投入生产，我们希望监控模型的输入数据和输出结果。在[*第4章*](B16865_04.xhtml#_idTextAnchor180)中，我们探讨了与Unity
    Catalog集成的Databricks Lakehouse Monitoring的两个关键特性：快照和时序配置文件。快照配置文件旨在提供在特定时间点的数据集概览，捕捉其当前状态。这对于识别即时数据质量问题或变化特别有用。另一方面，时序配置文件专注于数据随时间的变化，因此它们非常适合跟踪趋势、模式和数据分布的渐进性变化。
- en: Expanding on these capabilities, Databricks also provides an Inference profile,
    tailored for monitoring machine learning models in production. This advanced profile
    builds upon the concept of TimeSeries profiles, adding critical functionalities
    for comprehensive model performance evaluation. It includes model quality metrics,
    essential for tracking the accuracy and reliability of predictions over time.
    It also records predictions and, optionally, ground truth labels, directly comparing
    expected and actual outcomes. This functionality is key for identifying model
    drift, where shifts in input data or the relationship between inputs and outputs
    occur.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些功能的基础上，Databricks还提供了一种推理配置文件，专门用于监控生产中的机器学习模型。这个高级配置文件建立在时序配置文件的概念之上，增加了全面模型性能评估的关键功能。它包括模型质量指标，这对于跟踪预测的准确性和可靠性随时间的变化至关重要。它还记录预测，以及可选的地面真实标签，直接比较预期和实际结果。这一功能对于识别模型漂移至关重要，其中输入数据的变化或输入与输出之间的关系发生变化。
- en: Inference Tables in Databricks further bolster this monitoring capability. They
    contain essential elements such as model predictions, input features, timestamps,
    and potentially ground truth labels. Building a monitor on top of InferenceTables
    with the corresponding `InferenceLog` allows us to monitor model performance and
    data drift continuously.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 中的推理表进一步增强了这种监控能力。它们包含模型预测、输入特征、时间戳以及可能的地面实标签等基本元素。在 InferenceTables
    上构建具有相应 `InferenceLog` 的监控器，使我们能够持续监控模型性能和数据漂移。
- en: In the event of drift detection, immediate actions should be taken – data pipeline
    verification or model retraining and evaluation are recommended. These steps ensure
    the model adapts to new data patterns, maintaining accuracy and effectiveness.
    Continuous monitoring against your baseline and cross-model versions is a strategy
    to adopt when trying to ensure a stable process across various deployed solutions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在检测到漂移事件时，应立即采取行动 – 建议进行数据管道验证或模型重新训练和评估。这些步骤确保模型适应新的数据模式，保持准确性和有效性。持续监控基准和跨模型版本是尝试确保跨各种部署解决方案的稳定过程的一种策略。
- en: '*Figure 8**.1* is a code sample for creating an Inference monitor with model
    quality metrics using the `InferenceLog` profile type. This illustrates a practical
    application of this monitoring setup. We specify the `schedule` argument to make
    sure that this monitor is refreshed hourly.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8**.1* 是使用 `InferenceLog` 配置文件类型创建具有模型质量指标的推理监控器的代码示例。这展示了这种监控设置的实用应用。我们指定
    `schedule` 参数以确保这个监控器每小时刷新一次。'
- en: '![Figure 8.1 – Creating an Inference profile monitor that refreshes hourly'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.1 – 创建每小时刷新一次的推理配置文件监控器'
- en: '](img/B16865_08_1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_1.jpg)'
- en: Figure 8.1 – Creating an Inference profile monitor that refreshes hourly
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 创建每小时刷新一次的推理配置文件监控器
- en: Model monitoring is an effective way to ensure your models are working for you
    as expected. We hope this gets you thinking about how you use monitoring in your
    MLOPs process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控是确保你的模型按预期为你工作的一种有效方式。我们希望这能让你思考你在 MLOPs 流程中使用监控的方式。
- en: Next, we’ll learn about ways to create dashboards.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解创建仪表板的方法。
- en: Building gold layer visualizations
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建金层可视化
- en: The gold layer in your lakehouse is the consumption-ready layer. In this layer,
    final transformations and aggregations crystallize the insights within your data
    so it is ready for reporting and dashboarding. Being able to share your data with
    an audience is critical, and there are several options for doing so in the DI
    Platform. In fact, both Lakeview and Databricks SQL dashboards allow you to transform
    and aggregate your data within visualizations. Let’s walk through how to do that.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的数据湖中的金层是消费就绪层。在这个层中，最终转换和聚合使数据中的见解结晶，以便为报告和仪表板准备就绪。能够与受众分享你的数据至关重要，DI 平台提供了几种这样做的方式。事实上，Lakeview
    和 Databricks SQL 仪表板都允许你在可视化中转换和聚合你的数据。让我们看看如何做到这一点。
- en: Leveraging Lakeview dashboards
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用 Lakeview 仪表板
- en: Lakeview dashboards in Databricks are a powerful tool for creating data visualizations
    and sharing insights hidden in data. Visualizations can be made in the English
    language, making dashboard creation available to more users. To create a Lakeview
    dashboard, first click `ml_in_action.favorita_forecasting.train_set` table. This
    creates a dataset by selecting all records from the provided table. Notice how
    we do not *have to* write any SQL or create aggregates in order to visualize data
    aggregations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 中的 Lakeview 仪表板是创建数据可视化和共享数据中隐藏的见解的有力工具。可视化可以使用英语进行，这使得仪表板创建对更多用户可用。要创建
    Lakeview 仪表板，首先点击 `ml_in_action.favorita_forecasting.train_set` 表。这通过选择提供的表中的所有记录来创建一个数据集。注意我们不需要*必须*编写任何
    SQL 或创建聚合来可视化数据聚合。
- en: '![Figure 8.2 – The Data tab for adding data to your Lakeview dashboard'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2 – 添加数据到 Lakeview 仪表板的“数据”选项卡'
- en: '](img/B16865_08_2.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_2.jpg)'
- en: Figure 8.2 – The Data tab for adding data to your Lakeview dashboard
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 添加数据到 Lakeview 仪表板的“数据”选项卡
- en: Once you have a dataset, return to the **Canvas** tab. Select the **Add a visualization**
    button found on the blue bar toward the bottom of your browser window. This gives
    you a widget to place on your dashboard. Once placed, your widget will look similar
    to *Figure 8**.3*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了数据集，返回到 **画布** 选项卡。选择位于浏览器窗口底部蓝色栏上的 **添加可视化** 按钮。这为你提供了一个可以放置在仪表板上的小部件。放置后，你的小部件将类似于
    *图 8**.3*。
- en: '![Figure 8.3 – A new Lakeview widget'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3 – 新的 Lakeview 小部件'
- en: '](img/B16865_08_3.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_3.jpg)'
- en: Figure 8.3 – A new Lakeview widget
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 一个新的Lakeview小部件
- en: In the new widget, you can manually create a visualization using the options
    on the right-side menu. Alternatively, Databricks Assistant can help you rapidly
    build a chart using just English. You can write your own question, or explore
    the suggested queries. We selected the suggested question *What is the trend of
    onpromotion over date?* to automatically generate a chart, and *Figure 8**.4*
    is the result.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的小部件中，您可以使用右侧菜单上的选项手动创建一个可视化。或者，Databricks 助手可以帮助您仅用英语快速构建图表。您可以写下自己的问题，或者探索建议的查询。我们选择了建议的问题“*按日期查看促销趋势是什么？*”来自动生成图表，以及“*图8**.4*”的结果。
- en: '![Figure 8.4 – English text generated Lakeview widget'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.4 – English text generated Lakeview widget](img/B16865_08_4.jpg)'
- en: '](img/B16865_08_4.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_4.jpg](img/B16865_08_4.jpg)'
- en: Figure 8.4 – English text generated Lakeview widget
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – English text generated Lakeview widget
- en: When you are ready to share your dashboard, you can publish it! The engine powering
    Lakeview dashboards is optimized for performance, driving faster interactive charts
    for your data. It’s also powerful enough to handle streaming data. Furthermore,
    Lakeview dashboards are unified with the DI Platform through Unity Catalog, providing
    data lineage. They are designed for easy sharing across workspaces, meaning users
    in other workspaces can access your curated dashboard.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当您准备好分享您的仪表板时，您可以发布它！Lakeview仪表板背后的引擎针对性能进行了优化，可以提供更快的交互式图表。它也足够强大，可以处理流数据。此外，Lakeview仪表板通过Unity
    Catalog与DI平台统一，提供数据血缘。它们旨在轻松跨工作区共享，这意味着其他工作区中的用户可以访问您精心制作的仪表板。
- en: Visualizing big data with Databricks SQL dashboards
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Databricks SQL仪表板可视化大数据
- en: Lakeview dashboards are the future of Databricks. However, you can also build
    dashboards with `select` statement, you can produce the data and use it for multiple
    visualizations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Lakeview仪表板是Databricks的未来。然而，您也可以使用`select`语句构建仪表板，您可以生成数据并用于多个可视化。
- en: Note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To recreate *Figure 8**.5* in your own workspace, you will want to uncheck the
    **LIMIT 1000** box. There is still a limit for the visualizations of 64,000 rows.
    The best way to get around this is by filtering or aggregating.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您自己的工作区中重新创建“*图8**.5*”，您需要取消选中**LIMIT 1000**复选框。可视化仍有64,000行的限制。绕过这个限制的最好方法是过滤或聚合。
- en: '*Figure 8**.5* is an example visualization we created from a simple SQL query
    against the *Favorita Store* *Sales* data.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8**.5*是我们从一个简单的SQL查询（针对*Favorita Store* *Sales*数据）创建的示例可视化。'
- en: '![Figure 8.5 – After executing the select statement in the DBSQL editor, we
    create the visualizations without writing any code'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.5 – After executing the select statement in the DBSQL editor, we
    create the visualizations without writing any code](img/B16865_08_5.jpg)'
- en: '](img/B16865_08_5.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_5.jpg](img/B16865_08_5.jpg)'
- en: Figure 8.5 – After executing the select statement in the DBSQL editor, we create
    the visualizations without writing any code
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 在DBSQL编辑器中执行选择语句后，我们创建可视化而无需编写任何代码
- en: Suppose your dataset has categorical variables that you want to use to filter
    and compare features, as with the *Favorita* sales data. You can add filters within
    the DBSQL editor without revising the query. To add a filter, click the **+**
    and choose either **filter** or **parameter**. Both options provide widgets for
    filtering, as shown in *Figure 8**.6*. You can use the widgets with any visualizations
    or dashboards associated with the query.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的数据集包含您想要用于过滤和比较特征的分类变量，例如与*Favorita*销售数据一样。您可以在DBSQL编辑器中添加过滤器而无需修改查询。要添加过滤器，请点击**+**并选择**filter**或**parameter**。这两个选项都提供了用于过滤的小部件，如图*图8**.6*所示。您可以使用这些小部件与任何与查询关联的可视化或仪表板。
- en: '![Figure 8.6 – (L) The configuration for the state and family filter; (R) the
    result of adding two filters to the Favorita sales query'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.6 – (L) The configuration for the state and family filter; (R) the
    result of adding two filters to the Favorita sales query](img/B16865_08_6.jpg)'
- en: '](img/B16865_08_6.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_6.jpg](img/B16865_08_6.jpg)'
- en: Figure 8.6 – (L) The configuration for the state and family filter; (R) the
    result of adding two filters to the Favorita sales query
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – (L) 状态和家庭过滤器配置； (R) 向Favorita销售查询添加两个过滤器后的结果
- en: The dashboard functionality shown in *Figure 8**.7* is built into Databricks
    SQL as a way to present the charts and other visualizations created from one or
    many queries.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图8**.7*所示的仪表板功能内置在Databricks SQL中，作为展示从单个或多个查询创建的图表和其他可视化的方式。
- en: '![Figure 8.7 – A dashboard with charts created from the Favorita sales data
    query'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.7 – A dashboard with charts created from the Favorita sales data
    query](img/B16865_08_7.jpg)'
- en: '](img/B16865_08_7.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_7.jpg](img/B16865_08_7.jpg)'
- en: Figure 8.7 – A dashboard with charts created from the Favorita sales data query
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 从Favorita销售数据查询创建的图表仪表板
- en: The built-in visualization capabilities of DBSQL are a quick way to explore
    data without connecting to an external dashboarding or data visualization tool.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: DBSQL 内置的可视化功能是快速探索数据的一种方式，无需连接到外部仪表板或数据可视化工具。
- en: Next, we’ll look at an example of using Python **User-Defined Functions** (**UDFs**)
    for reusable Python code within DBSQL.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过一个示例来了解如何在 DBSQL 中使用 Python **用户定义函数**（**UDFs**）进行可重用 Python 代码。
- en: Python UDFs
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python UDFs
- en: 'Python UDFs are a way to create reusable snippets of code in Python, and these
    can be used in DBSQL. In this example, we’ll create a UDF for sales analysts to
    redact information in a customer record. Line five indicates the language syntax
    for the function is Python between `$$` signs:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Python UDFs 是在 Python 中创建可重用代码片段的方法，这些代码片段可以在 DBSQL 中使用。在这个例子中，我们将为销售分析师创建一个用于在客户记录中编辑信息的
    UDF。第五行指示函数的语言语法是在 `$$` 符号之间使用 Python：
- en: '![Figure 8.8 – Creating a Python UDF in DBSQL'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.8 – 在 DBSQL 中创建 Python UDF'
- en: '](img/B16865_08_8.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_8.jpg)'
- en: Figure 8.8 – Creating a Python UDF in DBSQL
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 – 在 DBSQL 中创建 Python UDF
- en: UDFs are defined and managed as part of Unity Catalog. Once a UDF is defined,
    you can give teams the ability to execute the UDF using `GRANT EXECUTE`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: UDFs（用户定义函数）作为 Unity Catalog 的一部分进行定义和管理。一旦定义了 UDF，您可以使用 `GRANT EXECUTE` 授予团队执行
    UDF 的能力。
- en: '![Figure 8.9 – Granting permissions for the sales-analysts groups to execute
    a UDF'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.9 – 授予销售分析师组执行 UDF 的权限'
- en: '](img/B16865_08_8.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_8.jpg)'
- en: Figure 8.9 – Granting permissions for the sales-analysts groups to execute a
    UDF
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 授予销售分析师组执行 UDF 的权限
- en: In this SQL query, we are applying the `redact` UDF to the `contact_info` field.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 SQL 查询中，我们将 `redact` UDF 应用到 `contact_info` 字段。
- en: '![Figure 8.10 – Using the Python UDF in a SQL query'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.10 – 在 SQL 查询中使用 Python UDF'
- en: '](img/B16865_08_8_(b).jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_8_(b).jpg)'
- en: Figure 8.10 – Using the Python UDF in a SQL query
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 在 SQL 查询中使用 Python UDF
- en: Now that we have the basics for visualizing data and applying Python UDFs in
    SQL, let’s cover a couple of tips and tricks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了可视化数据和在 SQL 中应用 Python UDFs 的基础知识，接下来让我们介绍一些小贴士和技巧。
- en: Tips and tricks
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士和技巧
- en: 'This section covers our tips and tricks related to DBSQL. Some tips apply to
    DBSQL and Lakeview, but not all:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了与 DBSQL 相关的小贴士和技巧。一些技巧适用于 DBSQL 和 Lakeview，但并非全部：
- en: '**Use managed compute (a.k.a. serverless compute) when possible**: Query performance
    using Databricks’ SQL warehouses is record-setting, as mentioned in [*Chapter
    1*](B16865_01.xhtml#_idTextAnchor016). The new managed compute for DBSQL puts
    the new first query performance at roughly 10 seconds. This means that idle time
    has drastically been reduced, which translates into cost savings.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尽可能使用托管计算（也称为无服务器计算）**：如 *第 1 章* [(*Chapter 1*](B16865_01.xhtml#_idTextAnchor016))
    中提到的，使用 Databricks 的 SQL 仓库进行查询的性能创下了记录。DBSQL 的新托管计算将首次查询性能缩短到大约 10 秒。这意味着空闲时间大大减少，这转化为成本节约。'
- en: '**Use a subquery as a parameter filter**: In your query visualizations and
    dashboards, you can prepopulate drop-down filter boxes. You can do this by creating
    and saving a query in the SQL editor. For example, you could create a query that
    returns a distinct list of customer names. In *Figure 8**.11*, we select a query
    called **Customer Name Lookup Qry** as a subquery to filter the query visualization
    by customer name. Therefore, we can filter on **Customer** using a drop-down list.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用子查询作为参数过滤器**：在您的查询可视化和仪表板中，您可以预先填充下拉筛选框。您可以通过在 SQL 编辑器中创建和保存查询来实现这一点。例如，您可以创建一个返回客户名称唯一列表的查询。在
    *图 8*.11 中，我们选择了一个名为 **Customer Name Lookup Qry** 的查询作为子查询，以按客户名称筛选查询可视化。因此，我们可以使用下拉列表来筛选
    **客户**。'
- en: '![Figure 8.11 – Using a subquery as a parameter for a query'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.11 – 将子查询作为查询的参数使用'
- en: '](img/B16865_08_9.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_9.jpg)'
- en: Figure 8.11 – Using a subquery as a parameter for a query
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 将子查询作为查询的参数使用
- en: '**Schedule report delivery**: If you have users who want to receive an up-to-date
    dashboard regularly, you can schedule the refresh and have it sent to subscribers.
    For DBSQL dashboards, remember to turn off **Enabled** when you are developing
    so that users don’t get too many updates.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安排报告交付**：如果您有希望定期收到最新仪表板的用户，您可以安排刷新并将其发送给订阅者。对于 DBSQL 仪表板，请记住在开发时关闭 **启用**，以免用户收到过多的更新。'
- en: '![Figure 8.12 – Scheduling a dashboard report with subscribers (T) DBSQL (B)
    Lakeview'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.12 – 使用订阅者（T）DBSQL（B）Lakeview 计划仪表板报告'
- en: '](img/B16865_08_10.jpg)![Figure 8.12 – Scheduling a dashboard report with subscribers
    (T) DBSQL (B) Lakeview'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_10.jpg)![图 8.12 – 使用订阅者（T）DBSQL（B）Lakeview 计划仪表板报告'
- en: '](img/B16865_08_11.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_11.jpg]'
- en: Figure 8.12 – Scheduling a dashboard report with subscribers (T) DBSQL (B) Lakeview
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – 使用订阅者（T）DBSQL（B）Lakeview调度仪表板报告
- en: '**Speed up development with Databricks Assistant**: As we covered in [*Chapter
    4*](B16865_04.xhtml#_idTextAnchor180), Databricks Assistant is an AI-based interface
    that can help generate, transform, fix, and explain code. The Assistant is context-aware,
    meaning it uses Unity Catalog to look at the metadata of your tables and columns,
    personalized in your environment. In *Figure 8**.13*, we ask the Assistant to
    help write a query with syntax for grouping. It sees the metadata of the **Favorita****Stores**
    table and provides code specific to that table and the column of interest.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用Databricks Assistant加速开发**：正如我们在[*第4章*](B16865_04.xhtml#_idTextAnchor180)中所述，Databricks
    Assistant是一个基于AI的界面，可以帮助生成、转换、修复和解释代码。助手是上下文感知的，这意味着它使用Unity Catalog来查看您环境中表和列的元数据，并为您个性化。在*图8*.13中，我们要求助手帮助编写一个使用分组语法的查询。它看到了**Favorita****Stores**表的元数据，并为该表和感兴趣的列提供了特定的代码。'
- en: '![Figure 8.13 – Using Databricks Assistant for help writing a query'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.13 – 使用Databricks Assistant帮助编写查询'
- en: '](img/B16865_08_12.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_12.jpg]'
- en: Figure 8.13 – Using Databricks Assistant for help writing a query
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 使用Databricks Assistant帮助编写查询
- en: '**Be informed**: Keep an eye out for important data changes with alerts. Use
    SQL to calibrate the alert and schedule the condition evaluation at specific intervals
    through the UI shown in *Figure 8**.14*. You can use HTML to create a formatted
    alert email.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保持警觉**：通过警报关注重要数据变化。使用SQL调整警报，并通过*图8*.14中显示的UI在特定间隔内安排条件评估。您可以使用HTML创建格式化的警报电子邮件。'
- en: '![Figure 8.14 – Scheduling alerts to trigger when certain conditions are met'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.14 – 当满足特定条件时触发调度警报'
- en: '](img/B16865_08_13.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_13.jpg]'
- en: Figure 8.14 – Scheduling alerts to trigger when certain conditions are met
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – 当满足特定条件时触发调度警报
- en: '**Use tags to track usage**: When creating a new SQL warehouse, use tags to
    code your warehouse endpoint with the correct project. Tagging is a great way
    to understand usage by project or team. System tables contain the information
    for tracking usage.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用标签跟踪使用情况**：在创建新的SQL仓库时，使用标签对您的仓库端点进行编码，以正确标记项目。标记是了解按项目或团队使用情况的好方法。系统表包含跟踪使用情况的信息。'
- en: '![Figure 8.15 – Using tags to connect an endpoint to a project'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.15 – 使用标签将端点连接到项目'
- en: '](img/B16865_08_14.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_14.jpg]'
- en: Figure 8.15 – Using tags to connect an endpoint to a project
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – 使用标签将端点连接到项目
- en: Next, you’ll learn how to connect your models to applications.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将学习如何将您的模型连接到应用程序。
- en: Connecting your applications
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接您的应用程序
- en: You can deploy your model anywhere using Databricks Model Serving, which is
    how you deployed your RAG chatbot model in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325).
    In this section, we will introduce how to host ML demo apps in **Hugging Face**
    (**HF**). Having an easy way to host ML apps allows you to build your ML portfolio,
    showcase your projects at conferences or with stakeholders, and work collaboratively
    with others in the ML ecosystem. With HF Spaces, you have multiple options for
    which Python library you use to create a web app. Two common ones are Streamlit
    and Gradio.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Databricks Model Serving在任何地方部署您的模型，这是您在[*第7章*](B16865_07.xhtml#_idTextAnchor325)中部署您的RAG聊天机器人模型的方式。在本节中，我们将介绍如何在**Hugging
    Face**（**HF**）中托管ML演示应用程序。拥有一种简单的方式来托管ML应用程序，让您能够构建您的ML投资组合，在会议或与利益相关者展示您的项目，并与ML生态系统中的其他人协作工作。使用HF
    Spaces，您有多种选择来决定您使用哪个Python库来创建Web应用程序。其中两个常见的选择是Streamlit和Gradio。
- en: We prefer Gradio. It is an open source Python package that allows you to quickly
    build a demo or web application for your machine learning model, API, or any arbitrary
    Python function. You can then share a link to your demo or web application in
    just a few seconds using Gradio’s built-in sharing features. No JavaScript, CSS,
    or web hosting experience is needed – we love it!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更喜欢Gradio。它是一个开源的Python包，允许您快速为您的机器学习模型、API或任何任意的Python函数构建演示或Web应用程序。然后，您只需使用Gradio内置的共享功能，在几秒钟内就可以分享您的演示或Web应用程序的链接。无需JavaScript、CSS或Web托管经验
    – 我们非常喜欢它！
- en: We will walk you through deploying a chatbot to an HF Space in the *Applying
    our learning* section’s RAG project work.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在*应用我们的学习*部分的RAG项目工作中，我们将向您展示如何将聊天机器人部署到HF Space。
- en: Incorporating LLMs for analysts with SQL AI Functions
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将LLMs与SQL AI函数结合为分析师
- en: There are many use cases where you can integrate an LLM, such as DBRX or OpenAI,
    for insights. With the Databricks Data Intelligence Platform, it’s also possible
    for analysts who are most comfortable in SQL to take advantage of advances in
    machine learning and artificial intelligence.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多用例可以集成LLM，如DBRX或OpenAI，以获得见解。使用Databricks数据智能平台，对于最舒适使用SQL的分析师来说，利用机器学习和人工智能的进步也是可能的。
- en: Within Databricks, you can use **AI Functions**, which are built-in SQL functions
    to access LLMs directly. AI Functions are available for use in the DBSQL interface,
    SQL warehouse JDBC connection, or via the Spark SQL API. In *Figure 8**.16*, we
    are leveraging the Databricks SQL editor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在Databricks中，你可以使用**AI函数**，这些是内置的SQL函数，可以直接访问LLMs。AI函数可用于DBSQL界面、SQL仓库JDBC连接或通过Spark
    SQL API。在*图8*.16中，我们正在利用Databricks SQL编辑器。
- en: Foundational Models API
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型API
- en: The storage and processing of data for Databricks-hosted foundation models occur
    entirely within the Databricks Platform. Importantly, this data is not shared
    with any third-party model providers. This is not necessarily true when using
    the External Models API, which connects you to services such as OpenAI that have
    their own data privacy policies. Keep this in mind when you are concerned about
    data privacy. You may be able to pay for a tier of service that restricts the
    use of your data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks托管的基础模型的数据存储和处理完全在Databricks平台内部进行。重要的是，这些数据不会与任何第三方模型提供商共享。当使用连接到具有自己数据隐私政策的服务的External
    Models API时，这并不一定成立。当你关注数据隐私时，请记住这一点。你可能能够支付一个限制你数据使用的服务层级的费用。
- en: 'Let’s do some simple emotion classification. Since the three datasets we’ve
    been working with don’t include any natural language, we’ll create a small dataset
    ourselves first. You can also download a dataset (such as the Emotions dataset
    from Kaggle) or use any other natural language source available to you:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一些简单的情绪分类。由于我们一直在使用的三个数据集都不包含任何自然语言，我们首先创建一个小数据集。你也可以下载一个数据集（如Kaggle的Emotions数据集）或使用你可用的任何其他自然语言来源：
- en: First, let’s explore the built-in `AI_QUERY` DBSQL function. This command will
    send our prompt to the remote model configured and retrieve the result. We’re
    using Databricks’ DBRX model, but you can use a variety of other open source and
    proprietary models as well. Open up the Databricks SQL editor and type in the
    code as shown in *Figure 8**.16*. Let’s write a query to give us a sample sentence
    that we can classify.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们探索内置的`AI_QUERY` DBSQL函数。此命令将我们的提示发送到远程配置的模型并检索结果。我们使用Databricks的DBRX模型，但你也可以使用各种其他开源和专有模型。打开Databricks
    SQL编辑器，并输入如图8.16所示的代码。让我们编写一个查询，以获取我们可以分类的样本句子。
- en: '![Figure 8.16 – Crafting a prompt using the AI_QUERY function'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.16 – 使用AI_QUERY函数构建提示'
- en: '](img/B16865_08_15.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_15.jpg](img/B16865_08_15.jpg)'
- en: Figure 8.16 – Crafting a prompt using the AI_QUERY function
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 – 使用AI_QUERY函数构建提示
- en: If you don’t have a dataset ready and don’t want to download one, you can build
    a function to generate a dataset for you, as shown here. We’re expanding on the
    prompt from *Step 1* to get several sentences back in JSON format.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你没有准备好数据集，也不想下载，你可以构建一个为你生成数据集的函数，如图所示。我们正在扩展*步骤1*的提示，以获取几个JSON格式的句子。
- en: '![Figure 8.17 – Creating a function to generate fake data'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.17 – 创建生成虚假数据的函数'
- en: '](img/B16865_08_16.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_16.jpg](img/B16865_08_16.jpg)'
- en: Figure 8.17 – Creating a function to generate fake data
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 – 创建生成虚假数据的函数
- en: Now use the `GENERATE_EMOTIONS_DATA` function to build a small dataset. After
    a quick review of the data, it looks like we have a good sample of emotions.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用`GENERATE_EMOTIONS_DATA`函数构建一个小数据集。快速查看数据后，看起来我们有一个很好的情绪样本。
- en: '![Figure 8.18 – Generating fake emotion data'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.18 – 生成虚假情绪数据'
- en: '](img/B16865_08_17.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_17.jpg](img/B16865_08_17.jpg)'
- en: Figure 8.18 – Generating fake emotion data
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 – 生成虚假情绪数据
- en: Now, we’re going to write a function called `CLASSIFY_EMOTION`. We’re using
    the AI Function `AI_QUERY` again, but this function will use a new prompt asking
    the model to classify a given sentence as one of six emotions.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将编写一个名为`CLASSIFY_EMOTION`的函数。我们再次使用AI函数`AI_QUERY`，但这个函数将使用一个新的提示，要求模型将给定的句子分类为六种情绪之一。
- en: '![Figure 8.19 – Creating a function to classify sentences by emotion'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.19 – 创建按情绪分类句子的函数'
- en: '](img/B16865_08_18.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_18.jpg](img/B16865_08_18.jpg)'
- en: Figure 8.19 – Creating a function to classify sentences by emotion
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 – 创建按情绪分类句子的函数
- en: Let’s call our function to evaluate an example sentence and take a look at the
    results.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们调用我们的函数来评估一个示例句子，并查看结果。
- en: '![Figure 8.20 – Calling the CLASSIFY_EMOTION function'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.20 – 调用 CLASSIFY_EMOTION 函数'
- en: '](img/B16865_08_19.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_19.jpg]'
- en: Figure 8.20 – Calling the CLASSIFY_EMOTION function
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.20 – 调用 CLASSIFY_EMOTION 函数
- en: Finally, to classify all records in a table, we call the `CLASSIFY_EMOTION`
    function on the records in our table and view the results.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，为了对表中的所有记录进行分类，我们在表中的记录上调用`CLASSIFY_EMOTION`函数并查看结果。
- en: '![Figure 8.21 – Calling the CLASSIFY_EMOTION function on a table'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.21 – 在表上调用 CLASSIFY_EMOTION 函数'
- en: '](img/B16865_08_20.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_20.jpg]'
- en: Figure 8.21 – Calling the CLASSIFY_EMOTION function on a table
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.21 – 在表上调用 CLASSIFY_EMOTION 函数
- en: SQL AI Functions are a great way to put the power of LLMs in the hands of SQL
    users. Solutions like SQL AI Functions still require some technical knowledge.
    Databricks is researching ways to allow business users direct access to data,
    with less upfront development required to get your team moving even faster. Keep
    an eye out for exciting new product features that remove the programming experience
    barrier to unlock the value of your data!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: SQL AI 函数是将 LLM 的力量交到 SQL 用户手中的绝佳方式。像 SQL AI 函数这样的解决方案仍然需要一些技术知识。Databricks
    正在研究允许业务用户直接访问数据的方法，这样就不需要太多的前期开发，以便让您的团队更快地运转。请密切关注令人兴奋的新产品功能，这些功能将消除编程经验障碍，释放您数据的价值！
- en: Applying our learning
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: Let’s use what we have learned to build a SQL chatbot using the Favorita project’s
    table metadata, monitor the streaming transaction project’s model, and deploy
    the chatbot that we have assembled and evaluated.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运用我们所学的知识，使用 Favorita 项目的表元数据构建一个 SQL 聊天机器人，监控流式事务项目的模型，并部署我们已组装和评估的聊天机器人。
- en: Technical requirements
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The technical requirements needed to complete the hands-on examples in this
    chapter are as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章动手实践所需的技术要求如下：
- en: The SQLbot will require OpenAI credentials.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQLbot 将需要 OpenAI 凭证。
- en: We will use the Databricks Secrets API to store our OpenAI credentials.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用 Databricks Secrets API 来存储我们的 OpenAI 凭证。
- en: You will need a **personal access token** (**PAT**) to deploy your web app to
    HF. See *Further reading* for detailed instructions.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要**个人访问令牌**（**PAT**）才能将您的 Web 应用部署到 HF。请参阅*进一步阅读*以获取详细说明。
- en: 'Project: Favorita store sales'
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目：Favorita 店铺销售
- en: Let’s build a simple **SQLbot** using OpenAI’s GPT to ask questions about our
    Favorita Sales tables. Please note that while this section continues to use the
    *Favorita Store Sales* data, it is not a continuation of the earlier project work.
    In this example, you’ll create instructions on how the bot can ask for a list
    of tables, get information from those tables, and sample data from the tables.
    The SQLbot will be able to build a SQL query and then interpret the results. To
    run the notebooks in this example, you will need an account with OpenAI on the
    OpenAI developer site and request a key for the OpenAI API.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 OpenAI 的 GPT 构建一个简单的 **SQLbot**，以询问有关我们的 Favorita 销售表的问题。请注意，尽管本节继续使用
    *Favorita Store Sales* 数据，但它并不是早期项目工作的延续。在这个例子中，您将创建有关机器人如何请求表列表、从这些表中获取信息以及从表中采样数据的说明。SQLbot
    将能够构建 SQL 查询并解释结果。要运行本例中的笔记本，您需要在 OpenAI 开发者网站上拥有一个账户，并请求 OpenAI API 的密钥。
- en: 'To follow along in your own workspace, please open the following notebook:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要在自己的工作区中跟随，请打开以下笔记本：
- en: '`CH8-01-SQL Chatbot`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`CH8-01-SQL Chatbot`'
- en: Keeping secret API keys in your Databricks notebook is far from best practice.
    You can lock down your notebook access and add a configuration notebook to your
    `.gitignore` file. However, your ability to remove people’s access may not be
    in your control, depending on your role. Generally, the permissions of admins
    include the ability to see all code. The OpenAI API key ties back to your account
    and your credit card. Note that running the notebook once cost us $0.08.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 笔记本中保留秘密 API 密钥绝不是最佳实践。您可以锁定笔记本访问并添加配置笔记本到您的 `.gitignore` 文件中。然而，您移除人们访问的能力可能不在您的控制之下，这取决于您的角色。通常，管理员权限包括查看所有代码的能力。OpenAI
    API 密钥与您的账户和信用卡相关联。请注意，运行笔记本一次花费了我们 $0.08。
- en: We added our API key to Databricks secrets. The Secrets API requires the Databricks
    CLI. We set up our CLI through Homebrew. If you haven’t already, we suggest getting
    Secrets set up for your workspace. This may require admin assistance. Start by
    installing or updating the Databricks CLI. You know the CLI is installed correctly
    when you get version v0.2 or higher. We are working with `Databricks` `CLI v0.208.0`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的 API 密钥添加到 Databricks 密钥中。Secrets API 需要使用 Databricks CLI。我们通过 Homebrew
    设置了我们的 CLI。如果您还没有设置，我们建议为您的 workspace 设置 Secrets。这可能需要管理员协助。首先，安装或更新 Databricks
    CLI。当您获得版本 v0.2 或更高版本时，您就知道 CLI 已正确安装。我们正在使用 `Databricks` `CLI v0.208.0`。
- en: 'We followed these steps to set up our API key as a secret:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循以下步骤设置我们的 API 密钥作为密钥：
- en: 'Create a scope:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个作用域：
- en: '[PRE0]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a secret within the scope:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在作用域内创建一个密钥：
- en: '[PRE1]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Paste your API key into the prompt.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的 API 密钥粘贴到提示中。
- en: Once your secret is successfully saved, we can access it via `dbutils.secrets`
    in our notebooks.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的密钥成功保存，我们就可以通过笔记本中的 `dbutils.secrets` 访问它。
- en: We are all set up to use OpenAI via the API now. We do not have to worry about
    accidentally committing our API or a coworker running the code, not knowing it
    costs you money.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经设置好通过 API 使用 OpenAI。我们不必担心意外提交我们的 API 或同事运行代码，而不知道这会花费您金钱。
- en: 'Next, let’s focus on creating our SQLbot notebook, step by step, beginning
    with the setup:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们一步一步地专注于创建我们的 SQLbot 笔记本，从设置开始：
- en: 'First, we install three libraries: `openai`, `langchain_experimental`, and
    `sqlalchemy-databricks`.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们安装了三个库：`openai`、`langchain_experimental` 和 `sqlalchemy-databricks`。
- en: To create a connection to OpenAI, pass the secret we set up previously and open
    a `ChatOpenAI` connection.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建与 OpenAI 的连接，请传递之前设置的密钥并打开一个 `ChatOpenAI` 连接。
- en: In *Figure 8**.22*, we create two different models. The first is the default
    model and the second uses GPT 3.5 Turbo.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **图 8**.22 中，我们创建了两个不同的模型。第一个是默认模型，第二个使用 GPT 3.5 Turbo。
- en: '![Figure 8.22 – OpenAI API connection'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.22 – OpenAI API 连接'
- en: '](img/B16865_08_21.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_21.jpg)'
- en: Figure 8.22 – OpenAI API connection
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.22 – OpenAI API 连接
- en: The setup file does not set your schema variable. Define your schema; we chose
    `favorita_forecasting`. We have been using `database_name` rather than a schema.
    However, we specify the database we want to ask SQL questions against, which is
    different.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置文件没有设置您的模式变量。定义您的模式；我们选择了 `favorita_forecasting`。我们一直使用 `database_name` 而不是模式。然而，我们指定了我们要对其提出
    SQL 问题的数据库，这是不同的。
- en: '![Figure 8.23 – (L) Collecting the table schema and system information schema;
    (R) dropping unnecessary and repetitive columns'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.23 – (L) 收集表模式和系统信息模式； (R) 删除不必要的重复列'
- en: '](img/B16865_08_22.jpg)![Figure 8.23 – (L) Collecting the table schema and
    system information schema; (R) dropping unnecessary and repetitive columns'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_22.jpg)![图 8.23 – (L) 收集表模式和系统信息模式； (R) 删除不必要的重复列'
- en: '](img/B16865_08_23.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_23.jpg)'
- en: Figure 8.23 – (L) Collecting the table schema and system information schema;
    (R) dropping unnecessary and repetitive columns
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.23 – (L) 收集表模式和系统信息模式； (R) 删除不必要的重复列
- en: Next, we create two helper functions. The first function organizes the schema
    information provided, `table_schemas`, creating a table definition. The second
    collects two rows of data as examples.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建了两个辅助函数。第一个函数组织提供的模式信息 `table_schemas`，创建一个表定义。第二个函数收集两行数据作为示例。
- en: '![Figure 8.24 – Helper functions for organizing table information'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.24 – 组织表格信息的辅助函数'
- en: '](img/B16865_08_24.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_24.jpg)'
- en: Figure 8.24 – Helper functions for organizing table information
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.24 – 组织表格信息的辅助函数
- en: Iterate through the table and column data, leveraging our helper functions to
    format the SQL database input.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历表和列数据，利用我们的辅助函数格式化 SQL 数据库输入。
- en: '![Figure 8.25 – Iterating through the tables and leveraging the helper functions'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.25 – 遍历表并利用辅助函数'
- en: '](img/B16865_08_25.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_25.jpg)'
- en: Figure 8.25 – Iterating through the tables and leveraging the helper functions
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.25 – 遍历表并利用辅助函数
- en: We now have all of our data ready to be able to create a SQL database for OpenAI
    to talk to. You will need to edit `endpoint_http_path` to match the path of an
    active SQL warehouse in your workspace. The database is passed to both the default
    OpenAI model and the GPT 3.5 model.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们所有的数据都已准备好，可以创建一个 SQL 数据库，以便 OpenAI 进行通信。您需要编辑 `endpoint_http_path` 以匹配您
    workspace 中活动 SQL 仓库的路径。数据库被传递到默认的 OpenAI 模型和 GPT 3.5 模型。
- en: '![Figure 8.26 – Create a database for OpenAI to query containing only the information
    we provided'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.26 – 为OpenAI创建一个只包含我们提供的信息的查询数据库'
- en: '](img/B16865_08_26.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_26.jpg]'
- en: Figure 8.26 – Create a database for OpenAI to query containing only the information
    we provided
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.26 – 为OpenAI创建一个只包含我们提供的信息的查询数据库
- en: 'With the setup complete, we can now interact with our SQL chatbot models! Let’s
    start with a basic question: *Which store sold* *the most?*'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成后，我们现在可以与我们的SQL聊天机器人模型交互了！让我们从一个基本问题开始：*哪个商店销售* *最多？*
- en: In *Figure 8**.27*, we run both models on the question and get back two different
    answers.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图8**.27中，我们对问题运行了两个模型并得到了两个不同的答案。
- en: '![Figure 8.27 – The SQL chatbot model’s responses to our question “Which store
    sold the most?”. (T) db_chain.run(question) (B) chat_chain.run(question)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.27 – SQL聊天机器人模型对我们问题“哪个商店销售最多？”的响应。（T）db_chain.run(question) （B）chat_chain.run(question）'
- en: '](img/B16865_08_27.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_27.jpg]'
- en: Figure 8.27 – The SQL chatbot model’s responses to our question “Which store
    sold the most?”. (T) db_chain.run(question) (B) chat_chain.run(question)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.27 – SQL聊天机器人模型对我们问题“哪个商店销售最多？”的响应。（T）db_chain.run(question) （B）chat_chain.run(question）
- en: As new versions of OpenAI’s GPT model are released, the results and behavior
    of your SQLbot may change. As new models and approaches become available, it is
    good practice to test them and see how the changes impact your work and the results
    of your chatbot. Leveraging MLflow with your SQLbot experiment will help you track
    and compare the different features and configurations throughout your production
    process.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 随着OpenAI的GPT模型新版本的发布，你的SQLbot的结果和行为可能会发生变化。随着新模型和方法的可用，测试它们并观察这些变化如何影响你的工作和聊天机器人的结果是一个好的实践。在SQLbot实验中利用MLflow将帮助你追踪和比较生产过程中的不同特性和配置。
- en: Project -streaming transactions
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 -流式传输事务
- en: You are ready to wrap up this project. The production workflow notebooks are
    the `CH7-08-Production Generating Records`, `CH7-09-Production Auto Loader`, and
    `CH7-10-Production Feature Engineering` components in the workflow job created
    in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325). You will run that same job
    once the new workflow is in place. To follow along in your own workspace, please
    open the following notebook:`CH8-05-Production Monitoring`
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经准备好完成这个项目了。生产工作流程笔记本是工作流程作业中创建的`CH7-08-Production Generating Records`、`CH7-09-Production
    Auto Loader`和`CH7-10-Production Feature Engineering`组件。一旦新工作流程到位，你将运行相同的作业。要在自己的工作区中跟随，请打开以下笔记本：`CH8-05-Production
    Monitoring`
- en: In the `CH8-05-Production Monitoring` notebook, you create two monitors – one
    for the `prod_transactions` table and one for the `packaged_transaction_model_predictions`
    table. See *Figure 8**.28* for the latter.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在`CH8-05-Production Monitoring`笔记本中，你创建了两个监控器 – 一个用于`prod_transactions`表，另一个用于`packaged_transaction_model_predictions`表。参见*图8**.28*了解后者。
- en: '![Figure 8.28 – The inference table monitor'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.28 – 推理表监控'
- en: '](img/B16865_08_28.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_28.jpg]'
- en: Figure 8.28 – The inference table monitor
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.28 – 推理表监控
- en: 'Congratulations! The streaming project is complete. We encourage you to add
    improvements and commit them back to the repository. Here are a few possible examples:
    add more validation metrics to the validation notebook, incorporate the inference
    performance results into the decision to retrain, and make adjustments to the
    configuration of data generation to simulate drift.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！流式传输项目已完成。我们鼓励你添加改进并将其提交回仓库。以下是一些可能的示例：向验证笔记本添加更多验证指标，将推理性能结果纳入重新训练的决定，并对数据生成的配置进行调整以模拟漂移。
- en: 'Project: retrieval-augmented generation chatbot'
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目：检索增强生成聊天机器人
- en: 'To follow along in your own workspace, please open the following notebooks
    and resources:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要在自己的工作区中跟随，请打开以下笔记本和资源：
- en: CH8-`app.py`
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CH8-`app.py`
- en: '`CH8-01-Deploy Your Endpoint` `with SDK`'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH8-01-Deploy Your Endpoint` `with SDK`'
- en: The **Hugging Face** **Spaces** page
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face** **Spaces** 页面'
- en: First, we need to ensure our chatbot is deployed using Model Serving, as shown
    in *Figure 8**.30*. Here, we are using the fastest way by going through the UI
    of the Model Serving page. To follow along and serve, we are selecting the registered
    model that we registered in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325). Select
    the latest version – in our case, it’s version 4\. For this demo project, we expect
    minimal concurrency so the endpoint will have only four possible concurrent runs
    and will scale to 0 when no traffic is available. We are enabling inference tables
    under the same catalog to track and potentially further monitor our payload. We
    are not going to demonstrate in this chapter how to set a monitor or data quality
    pipeline for the RAG project, as it was demonstrated for the streaming project.
    We encourage you to apply it on your own!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确保我们的聊天机器人使用模型服务进行部署，如图 *图 8**.30* 所示。在这里，我们通过模型服务页面的 UI 使用最快的方式。为了跟随并服务，我们选择了我们在
    [*第 7 章*](B16865_07.xhtml#_idTextAnchor325) 中注册的已注册模型。选择最新版本 – 在我们的案例中，它是版本 4。对于这个演示项目，我们预计并发量最小，因此端点将只有四个可能的并发运行，并在没有流量时扩展到
    0。我们正在同一目录下启用推理表以跟踪和可能进一步监控我们的有效载荷。我们不会在本章中演示如何为 RAG 项目设置监控器或数据质量管道，因为它已经在流项目中被演示过。我们鼓励您在自己的项目中应用它！
- en: '![Figure 8.29 – Example of the Model Serving deployment via UI'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.29 – 通过 UI 进行模型服务部署的示例'
- en: '](img/B16865_08_29.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_29.jpg)'
- en: Figure 8.29 – Example of the Model Serving deployment via UI
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.29 – 通过 UI 进行模型服务部署的示例
- en: 'In order for your application to be able to connect to the resources attached
    to it, such as Vector Search, the endpoint requires you to provide additional
    configurations to the endpoint such as your PAT and host under **Advanced configuration**:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使您的应用程序能够连接到附加的资源，例如向量搜索，端点需要您在 **高级配置** 中为端点提供额外的配置，例如您的 PAT 和主机：
- en: '![Figure 8.30 – Advanced configuration requirements'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.30 – 高级配置要求'
- en: '](img/B16865_08_30.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_30.jpg)'
- en: Figure 8.30 – Advanced configuration requirements
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.30 – 高级配置要求
- en: Note
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You could also use the Databricks SDK service to deploy your endpoints. If you
    are interested in seeing how to use SDK deployment, please use the notebook attached
    under `CH8 - 01 -Deploy Your Endpoint` `with SDK`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 Databricks SDK 服务来部署您的端点。如果您想了解如何使用 SDK 部署，请使用附在 `CH8 - 01 -Deploy Your
    Endpoint` 下的笔记本 `with SDK`。
- en: Jump over to the Hugging Face Spaces website. Instructions on how to deploy
    your first HF Space are explained very well on the main page of HF Spaces, so
    we will not duplicate them here. We would like to highlight that we are using
    a free deployment option of Spaces with 2 CPUs and 16 GB of memory.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 跳转到 Hugging Face Spaces 网站。如何在 HF Spaces 中部署您的第一个 HF Space 的说明在 HF Spaces 的主页上有很好的解释，所以我们在这里不会重复它们。我们想强调的是，我们正在使用
    Spaces 的免费部署选项，带有 2 个 CPU 和 16 GB 的内存。
- en: 'When you deploy your Space, it will look like this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当您部署您的 Space 时，它将看起来像这样：
- en: '![Figure 8.31 – Empty Hugging Face Space'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.31 – 空的 Hugging Face Space'
- en: '](img/B16865_08_31.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_31.jpg)'
- en: Figure 8.31 – Empty Hugging Face Space
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.31 – 空的 Hugging Face Space
- en: 'We would like to highlight a few things that are important in order to connect
    to your chatbot, which is served in real time with Databricks Model Serving. To
    connect the chatbot to your HF Space, you must set `API_TOKEN` and an `API_ENDPOINT`.
    Here’s how to set these values:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想强调一些重要的事情，以便连接到使用 Databricks 模型服务实时提供的聊天机器人。要将聊天机器人连接到您的 HF Space，您必须设置 `API_TOKEN`
    和 `API_ENDPOINT`。以下是设置这些值的方法：
- en: Go to **Settings** in the HF Space you created.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往您创建的 HF Space 的 **设置**。
- en: Scroll down to **Variables** **and secrets**.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到 **变量** **和秘密**。
- en: Set your API_ENDPOINT as the URL from the REST API provided on the Databricks
    Model Serving page.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的 API_ENDPOINT 设置为 Databricks 模型服务页面上提供的 REST API 的 URL。
- en: Set your API_TOKEN using a personal access token generated by Databricks. This
    is required to connect to the endpoint.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Databricks 生成的个人访问令牌设置您的 API_TOKEN。这是连接到端点所必需的。
- en: '![Figure 8.32 – Example of Variables and secrets on HF Spaces'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.32 – HF Spaces 上的变量和秘密示例'
- en: '](img/B16865_08_32.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_32.jpg)'
- en: Figure 8.32 – Example of Variables and secrets on HF Spaces
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.32 – HF Spaces 上的变量和秘密示例
- en: Once this is set, you are ready to bring your Gradio web app script into your
    HF Space.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦设置完成，您就可以将您的 Gradio 网页应用脚本带入您的 HF Space。
- en: '![Figure 8.33 – Example of Variables and secrets on HF Spaces'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.33 – HF Spaces 上的变量和秘密示例'
- en: '](img/B16865_08_33.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_33.jpg)'
- en: Figure 8.33 – Example of Variables and secrets on HF Spaces
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.33 – HF 空间中变量和秘密的示例
- en: When your endpoint is ready, jump back to your HF Space.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您的端点准备好后，返回您的 HF 空间。
- en: Go to the **Files** tab under the pre-created Space and click **+****Add File**.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预先创建的空间下方的 **文件** 选项卡中点击 **+** 添加文件**。
- en: Now add `CH8-app.py` that was given to you for [*Chapter 8*](B16865_08.xhtml#_idTextAnchor384)
    – you can create your own web application. Feel free to experiment with the design
    according to your business needs.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在添加您得到的 `CH8-app.py` 文件 – 您可以创建自己的网络应用。根据您的业务需求，自由地尝试设计。
- en: Let’s talk a bit about the `respond` function in the `CH8-app.py` file – see
    *Figure 8**.34*, which is passed to our app’s UI chatbot. The `respond` function,
    in this case, is the caller of your deployed endpoints, where we do not just send
    and receive responses but also can shape the format of the input or output. In
    our case, the endpoint expects to receive a request in the format of a JSON with
    field inputs with the questions within a list, while the output is a JSON with
    field predictions.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要谈谈 `CH8-app.py` 文件中的 `respond` 函数 – 见 *图 8.34*，它被传递到我们应用的 UI 聊天机器人。在这个例子中，`respond`
    函数是您部署的端点的调用者，我们不仅发送和接收响应，还可以塑造输入或输出的格式。在我们的案例中，端点期望接收一个格式为 JSON 的请求，其中包含字段输入的列表中的问题，而输出是一个包含字段预测的
    JSON。
- en: '![Figure 8.34 – The respond function written in the Gradio app'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.34 – 在 Gradio 应用中编写的响应函数'
- en: '](img/B16865_08_34.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_34.jpg)'
- en: Figure 8.34 – The respond function written in the Gradio app
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.34 – 在 Gradio 应用中编写的响应函数
- en: To create a chatbot, as was mentioned in the introduction section, we are using
    a simple example from Gradio where we add options such as the title of our application,
    a description, and example questions. *Figure 8**.35* shows the full code.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 正如引言部分所述，要创建聊天机器人，我们使用了一个简单的 Gradio 示例，其中添加了诸如应用程序标题、描述和示例问题等选项。*图 8.35* 展示了完整的代码。
- en: '![Figure 8.35 – The Gadio app.py interface for your LLM'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.35 – 您 LLM 的 Gadio app.py 界面'
- en: '](img/B16865_08_35.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_35.jpg)'
- en: Figure 8.35 – The Gadio app.py interface for your LLM
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.35 – 您 LLM 的 Gadio app.py 界面
- en: The chatbot is now available in a more user-friendly interface, as shown in
    *Figure 8**.36*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人现在拥有一个更用户友好的界面，如图 *图 8.36* 所示。
- en: '![Figure 8.36 – The interface for your chatbot application'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.36 – 您聊天机器人应用的界面'
- en: '](img/B16865_08_36.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_36.jpg)'
- en: Figure 8.36 – The interface for your chatbot application
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.36 – 您聊天机器人应用的界面
- en: Let’s ask a few questions to make sure our RAG chatbot is providing correct
    results.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提出几个问题以确保我们的 RAG 聊天机器人提供正确的结果。
- en: '![Figure 8.37 – Examples of chatbot answers from our RAG application'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.37 – 我们 RAG 应用中聊天机器人回答的示例'
- en: '](img/B16865_08_37.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_37.jpg)'
- en: Figure 8.37 – Examples of chatbot answers from our RAG application
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.37 – 我们 RAG 应用中聊天机器人回答的示例
- en: If the responses look good, your application is ready to be used!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果响应看起来不错，您的应用程序就准备好使用了！
- en: Summary
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Sharing insights from your models is an important way to get value from your
    machine learning practice. Using dashboards as a medium for sharing your information
    is an accessible way to communicate insights to business users and teams outside
    of the data science team. In this chapter, we discussed how to build the gold
    layer, tips on presenting your data using DBSQL dashboards, taking advantage of
    innovations such as OpenAI models in DBSQL, and how you can share data and AI
    artifacts through Databricks Marketplace to get the most value from your enterprise
    data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从您的机器学习实践中获取价值的重要方式之一是分享您模型中的洞察。使用仪表板作为分享您信息的中介是一种便于与业务用户和数据科学团队之外的小组沟通洞察的方法。在本章中，我们讨论了如何构建黄金层，使用
    DBSQL 仪表板展示数据的技巧，利用 DBSQL 中的创新，如 OpenAI 模型，以及您如何通过 Databricks 市场共享数据和 AI 艺术品，以从企业数据中获得最大价值。
- en: We hope you have had a chance to get hands-on with building your lakehouse.
    From exploration, cleaning, building pipelines, and building models to finding
    insights hidden in your data, all the way to sharing insights – it’s all doable
    on the Databricks Platform. We encourage you to take the notebooks and experiment!
    The authors would love to hear feedback, and whether this has been helpful on
    your journey with the Databricks Platform.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望您有机会亲自动手构建您的数据湖。从探索、清洗、构建管道、构建模型到发现数据中的隐藏洞察，再到分享洞察 – 所有这些都可以在 Databricks
    平台上完成。我们鼓励您尝试使用笔记本进行实验！作者们很乐意听到您的反馈，以及这在本旅途中对您使用 Databricks 平台是否有帮助。
- en: Questions
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let’s test ourselves on what we’ve learned by going through the following questions:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下问题来测试一下我们所学的知识：
- en: What are some differences between the gold layer and the silver layer?
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 金层和银层之间有哪些区别？
- en: What is one way you could set an alert to identify that a table has an invalid
    value?
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过什么方式设置一个警报来识别一个表有无效值？
- en: Why would you choose to use an external dashboarding tool?
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你为什么会选择使用外部仪表盘工具？
- en: If you use a language model through an API such as OpenAI, what are some considerations
    about the data you send via the API?
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你通过API（如OpenAI）使用语言模型，你发送API的数据有哪些考虑因素？
- en: What are some reasons a company would share data in Databricks Marketplace?
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一家公司为什么会在Databricks Marketplace上共享数据？
- en: Answers
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: 'After putting thought into the questions, compare your answers to ours:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考了这些问题之后，比较你的答案和我们的答案：
- en: The gold layer is more refined and aggregated than the silver layer. The silver
    layer powers data science and machine learning, and the gold layer powers analytics
    and dashboarding.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 金层比银层更精细和聚合。银层为数据科学和机器学习提供动力，而金层为分析和仪表盘提供动力。
- en: You can monitor the values of a field and send an email alert when the value
    is invalid.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以监控字段的值，并在值无效时发送电子邮件警报。
- en: Sometimes companies use multiple dashboarding tools. You might need to provide
    data in a dashboard a team is accustomed to using.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时公司会使用多个仪表盘工具。你可能需要提供团队习惯使用的仪表盘中的数据。
- en: If I were a language model through an API, I would be cautious about sending
    sensitive data, including PII, customer information, or proprietary information.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我是一个通过API的语言模型，我会对发送敏感数据保持谨慎，包括PII、客户信息或专有信息。
- en: A company might share data in Databricks Marketplace in order to monetize the
    data or make it available externally for people to use easily and securely.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一家公司可能会在Databricks Marketplace上共享数据，以便货币化数据或使其对外部人员易于使用且安全地使用。
- en: Further reading
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'In this chapter, we pointed out specific technologies, technical features,
    and options. Please look at these resources to get deeper into the areas that
    interest you most:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们指出了具体的技术、技术特性和选项。请查看这些资源，以深入了解你最感兴趣的领域：
- en: '*Databricks SQL Statement Execution* *API*: [https://www.databricks.com/blog/2023/03/07/databricks-sql-statement-execution-api-announcing-public-preview.html](https://www.databricks.com/blog/2023/03/07/databricks-sql-statement-execution-api-announcing-public-preview.html)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks SQL语句执行* *API*: [https://www.databricks.com/blog/2023/03/07/databricks-sql-statement-execution-api-announcing-public-preview.html](https://www.databricks.com/blog/2023/03/07/databricks-sql-statement-execution-api-announcing-public-preview.html)'
- en: '*Power to the SQL People: Introducing Python UDFs in Databricks* *SQL*: [https://www.databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html](https://www.databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*赋予SQL人群力量：介绍Databricks SQL中的Python UDFs* *：[https://www.databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html](https://www.databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html)'
- en: '*Actioning Customer Reviews at Scale with Databricks SQL AI* *Functions*: [https://www.databricks.com/blog/actioning-customer-reviews-scale-databricks-sql-ai-functions](https://www.databricks.com/blog/actioning-customer-reviews-scale-databricks-sql-ai-functions)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Databricks SQL AI* *函数*在规模上执行客户评价*：[https://www.databricks.com/blog/actioning-customer-reviews-scale-databricks-sql-ai-functions](https://www.databricks.com/blog/actioning-customer-reviews-scale-databricks-sql-ai-functions)'
- en: '*Databricks sets the official data warehousing performance* *record*: https://dbricks.co/benchmark'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks创下了官方数据仓库性能* *记录*：https://dbricks.co/benchmark'
- en: '*Databricks Lakehouse and Data* *Mesh*: [https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html](https://www.databricks.com/blog/databricks-lakehouse-and-data-mesh-part-1)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks Lakehouse和数据* *Mesh*：[https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html](https://www.databricks.com/blog/databricks-lakehouse-and-data-mesh-part-1)'
- en: '*Hugging* *Face*: [https://huggingface.co/spaces](https://huggingface.co/spaces)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hugging* *Face*: [https://huggingface.co/spaces](https://huggingface.co/spaces)'
- en: '*Gradio*: [https://www.gradio.app/](https://www.gradio.app/)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gradio*: [https://www.gradio.app/](https://www.gradio.app/)'
- en: '*Hugging Face* *Spaces*: [https://huggingface.co/docs/hub/en/spaces-overview](https://huggingface.co/docs/hub/en/spaces-overview)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hugging Face* *Spaces*: [https://huggingface.co/docs/hub/en/spaces-overview](https://huggingface.co/docs/hub/en/spaces-overview)'
- en: '*Databricks Lakehouse monitoring* *documentation*: [https://api-docs.databricks.com/python/lakehouse-monitoring/latest/databricks.lakehouse_monitoring.html#module-databricks.lakehouse_monitoring](https://api-docs.databricks.com/python/lakehouse-monitoring/latest/databricks.lakehouse_monitoring.html#module-databricks.lakehouse_monitoring
    )'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks Lakehouse监控* 文档：[https://api-docs.databricks.com/python/lakehouse-monitoring/latest/databricks.lakehouse_monitoring.html#module-databricks.lakehouse_monitoring](https://api-docs.databricks.com/python/lakehouse-monitoring/latest/databricks.lakehouse_monitoring.html#module-databricks.lakehouse_monitoring)'
- en: Databricks personal access token authentication [https://docs.databricks.com/en/dev-tools/auth/pat.html](https://docs.databricks.com/en/dev-tools/auth/pat.html)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks个人访问令牌认证 [https://docs.databricks.com/en/dev-tools/auth/pat.html](https://docs.databricks.com/en/dev-tools/auth/pat.html)
