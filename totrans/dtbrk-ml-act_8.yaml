- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Monitoring, Evaluating, and More
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控、评估和更多
- en: “Focus on how the end-user customers perceive the impact of your innovation
    – rather than on how you, the innovators, perceive it.” — Thomas A. Edison
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “关注最终用户客户如何看待你的创新的影响——而不是你，作为创新者，如何看待它。” —— 托马斯·A·爱迪生
- en: Congratulations, you’ve made it to the final chapter! We’ve come a long way,
    yet there is still more to explore in Databricks. As we wrap up, we will take
    another look at Lakehouse Monitoring. We’ll focus on monitoring model inference
    data. After all the work you’ve put in to build a robust model and push it into
    production, it’s essential to share the learnings, predictions, and other outcomes
    with a broad audience. Sharing results with dashboards is very common. We will
    cover how to create visualizations for dashboards in both the new Lakeview dashboards
    and the standard Databricks SQL dashboards. Deployed models can be shared via
    a web application. Therefore, we will not only introduce Hugging Face Spaces but
    also deploy the RAG chatbot using the Gradio app in *Applying our learning*. Lastly,
    we’ll demonstrate how analysts can invoke LLMs via SQL AI Functions! By the end
    of this chapter, you will be ready to monitor inference data, create visualizations,
    deploy an ML web app, and use the groundbreaking DBRX open source LLM with SQL.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你已经到达了最后一章！我们已经走了很长的路，但在Databricks中仍有更多内容可以探索。在我们结束之前，我们将再次审视Lakehouse Monitoring。我们将专注于监控模型推理数据。毕竟，你在构建一个健壮的模型并将其推入生产后投入了大量的工作，与广泛的受众分享学习成果、预测和其他结果至关重要。通过仪表板共享结果非常常见。我们将介绍如何在新的Lakeview仪表板和标准的Databricks
    SQL仪表板中创建仪表板可视化。部署的模型可以通过Web应用程序共享。因此，我们不仅将介绍Hugging Face Spaces，还将通过Gradio应用程序在*应用我们的学习*中部署RAG聊天机器人。最后，我们将演示分析师如何通过SQL
    AI函数调用LLMs！到本章结束时，你将准备好监控推理数据、创建可视化、部署ML Web应用程序，并使用突破性的DBRX开源LLM与SQL一起使用。
- en: 'Here is the roadmap for this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的路线图如下：
- en: Monitoring your models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控您的模型
- en: Building gold layer visualizations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建金层可视化
- en: Connecting your applications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接您的应用程序
- en: Incorporating LLMs for analysts
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为分析师整合LLMs
- en: Applying our learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: Monitoring your models
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控您的模型
- en: 'The ML lifecycle does not end at deployment. Once a model is in production,
    we want to monitor the input data and output results of the model. In [*Chapter
    4*](B16865_04.xhtml#_idTextAnchor180), we explored two key features of Databricks
    Lakehouse Monitoring integrated with Unity Catalog: Snapshot and TimeSeries profiles.
    Snapshot profiles are designed to provide an overview of a dataset at a specific
    point in time, capturing its current state. This is particularly useful for identifying
    immediate data quality issues or changes. On the other hand, TimeSeries profiles
    focus on how data evolves over time, making them ideal for tracking trends, patterns,
    and gradual changes in data distributions.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习生命周期并不在部署后结束。一旦模型投入生产，我们希望监控模型的输入数据和输出结果。在[*第4章*](B16865_04.xhtml#_idTextAnchor180)中，我们探讨了与Unity
    Catalog集成的Databricks Lakehouse Monitoring的两个关键特性：快照和时序配置文件。快照配置文件旨在提供在特定时间点的数据集概览，捕捉其当前状态。这对于识别即时数据质量问题或变化特别有用。另一方面，时序配置文件专注于数据随时间的变化，因此它们非常适合跟踪趋势、模式和数据分布的渐进性变化。
- en: Expanding on these capabilities, Databricks also provides an Inference profile,
    tailored for monitoring machine learning models in production. This advanced profile
    builds upon the concept of TimeSeries profiles, adding critical functionalities
    for comprehensive model performance evaluation. It includes model quality metrics,
    essential for tracking the accuracy and reliability of predictions over time.
    It also records predictions and, optionally, ground truth labels, directly comparing
    expected and actual outcomes. This functionality is key for identifying model
    drift, where shifts in input data or the relationship between inputs and outputs
    occur.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些功能的基础上，Databricks还提供了一种推理配置文件，专门用于监控生产中的机器学习模型。这个高级配置文件建立在时序配置文件的概念之上，增加了全面模型性能评估的关键功能。它包括模型质量指标，这对于跟踪预测的准确性和可靠性随时间的变化至关重要。它还记录预测，以及可选的地面真实标签，直接比较预期和实际结果。这一功能对于识别模型漂移至关重要，其中输入数据的变化或输入与输出之间的关系发生变化。
- en: Inference Tables in Databricks further bolster this monitoring capability. They
    contain essential elements such as model predictions, input features, timestamps,
    and potentially ground truth labels. Building a monitor on top of InferenceTables
    with the corresponding `InferenceLog` allows us to monitor model performance and
    data drift continuously.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 中的推理表进一步增强了这种监控能力。它们包含模型预测、输入特征、时间戳以及可能的地面实标签等基本元素。在 InferenceTables
    上构建具有相应 `InferenceLog` 的监控器，使我们能够持续监控模型性能和数据漂移。
- en: In the event of drift detection, immediate actions should be taken – data pipeline
    verification or model retraining and evaluation are recommended. These steps ensure
    the model adapts to new data patterns, maintaining accuracy and effectiveness.
    Continuous monitoring against your baseline and cross-model versions is a strategy
    to adopt when trying to ensure a stable process across various deployed solutions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在检测到漂移事件时，应立即采取行动 – 建议进行数据管道验证或模型重新训练和评估。这些步骤确保模型适应新的数据模式，保持准确性和有效性。持续监控基准和跨模型版本是尝试确保跨各种部署解决方案的稳定过程的一种策略。
- en: '*Figure 8**.1* is a code sample for creating an Inference monitor with model
    quality metrics using the `InferenceLog` profile type. This illustrates a practical
    application of this monitoring setup. We specify the `schedule` argument to make
    sure that this monitor is refreshed hourly.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8**.1* 是使用 `InferenceLog` 配置文件类型创建具有模型质量指标的推理监控器的代码示例。这展示了这种监控设置的实用应用。我们指定
    `schedule` 参数以确保这个监控器每小时刷新一次。'
- en: '![Figure 8.1 – Creating an Inference profile monitor that refreshes hourly'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.1 – 创建每小时刷新一次的推理配置文件监控器'
- en: '](img/B16865_08_1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_1.jpg)'
- en: Figure 8.1 – Creating an Inference profile monitor that refreshes hourly
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 创建每小时刷新一次的推理配置文件监控器
- en: Model monitoring is an effective way to ensure your models are working for you
    as expected. We hope this gets you thinking about how you use monitoring in your
    MLOPs process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控是确保你的模型按预期为你工作的一种有效方式。我们希望这能让你思考你在 MLOPs 流程中使用监控的方式。
- en: Next, we’ll learn about ways to create dashboards.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解创建仪表板的方法。
- en: Building gold layer visualizations
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建金层可视化
- en: The gold layer in your lakehouse is the consumption-ready layer. In this layer,
    final transformations and aggregations crystallize the insights within your data
    so it is ready for reporting and dashboarding. Being able to share your data with
    an audience is critical, and there are several options for doing so in the DI
    Platform. In fact, both Lakeview and Databricks SQL dashboards allow you to transform
    and aggregate your data within visualizations. Let’s walk through how to do that.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的数据湖中的金层是消费就绪层。在这个层中，最终转换和聚合使数据中的见解结晶，以便为报告和仪表板准备就绪。能够与受众分享你的数据至关重要，DI 平台提供了几种这样做的方式。事实上，Lakeview
    和 Databricks SQL 仪表板都允许你在可视化中转换和聚合你的数据。让我们看看如何做到这一点。
- en: Leveraging Lakeview dashboards
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用 Lakeview 仪表板
- en: Lakeview dashboards in Databricks are a powerful tool for creating data visualizations
    and sharing insights hidden in data. Visualizations can be made in the English
    language, making dashboard creation available to more users. To create a Lakeview
    dashboard, first click `ml_in_action.favorita_forecasting.train_set` table. This
    creates a dataset by selecting all records from the provided table. Notice how
    we do not *have to* write any SQL or create aggregates in order to visualize data
    aggregations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 中的 Lakeview 仪表板是创建数据可视化和共享数据中隐藏的见解的有力工具。可视化可以使用英语进行，这使得仪表板创建对更多用户可用。要创建
    Lakeview 仪表板，首先点击 `ml_in_action.favorita_forecasting.train_set` 表。这通过选择提供的表中的所有记录来创建一个数据集。注意我们不需要*必须*编写任何
    SQL 或创建聚合来可视化数据聚合。
- en: '![Figure 8.2 – The Data tab for adding data to your Lakeview dashboard'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2 – 添加数据到 Lakeview 仪表板的“数据”选项卡'
- en: '](img/B16865_08_2.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_2.jpg)'
- en: Figure 8.2 – The Data tab for adding data to your Lakeview dashboard
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 添加数据到 Lakeview 仪表板的“数据”选项卡
- en: Once you have a dataset, return to the **Canvas** tab. Select the **Add a visualization**
    button found on the blue bar toward the bottom of your browser window. This gives
    you a widget to place on your dashboard. Once placed, your widget will look similar
    to *Figure 8**.3*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了数据集，返回到 **画布** 选项卡。选择位于浏览器窗口底部蓝色栏上的 **添加可视化** 按钮。这为你提供了一个可以放置在仪表板上的小部件。放置后，你的小部件将类似于
    *图 8**.3*。
- en: '![Figure 8.3 – A new Lakeview widget'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3 – 新的 Lakeview 小部件'
- en: '](img/B16865_08_3.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_08_3.jpg)'
- en: Figure 8.3 – A new Lakeview widget
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: In the new widget, you can manually create a visualization using the options
    on the right-side menu. Alternatively, Databricks Assistant can help you rapidly
    build a chart using just English. You can write your own question, or explore
    the suggested queries. We selected the suggested question *What is the trend of
    onpromotion over date?* to automatically generate a chart, and *Figure 8**.4*
    is the result.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – English text generated Lakeview widget'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_4.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – English text generated Lakeview widget
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: When you are ready to share your dashboard, you can publish it! The engine powering
    Lakeview dashboards is optimized for performance, driving faster interactive charts
    for your data. It’s also powerful enough to handle streaming data. Furthermore,
    Lakeview dashboards are unified with the DI Platform through Unity Catalog, providing
    data lineage. They are designed for easy sharing across workspaces, meaning users
    in other workspaces can access your curated dashboard.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing big data with Databricks SQL dashboards
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lakeview dashboards are the future of Databricks. However, you can also build
    dashboards with `select` statement, you can produce the data and use it for multiple
    visualizations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: To recreate *Figure 8**.5* in your own workspace, you will want to uncheck the
    **LIMIT 1000** box. There is still a limit for the visualizations of 64,000 rows.
    The best way to get around this is by filtering or aggregating.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.5* is an example visualization we created from a simple SQL query
    against the *Favorita Store* *Sales* data.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – After executing the select statement in the DBSQL editor, we
    create the visualizations without writing any code'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_5.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – After executing the select statement in the DBSQL editor, we create
    the visualizations without writing any code
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Suppose your dataset has categorical variables that you want to use to filter
    and compare features, as with the *Favorita* sales data. You can add filters within
    the DBSQL editor without revising the query. To add a filter, click the **+**
    and choose either **filter** or **parameter**. Both options provide widgets for
    filtering, as shown in *Figure 8**.6*. You can use the widgets with any visualizations
    or dashboards associated with the query.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – (L) The configuration for the state and family filter; (R) the
    result of adding two filters to the Favorita sales query'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_6.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – (L) The configuration for the state and family filter; (R) the
    result of adding two filters to the Favorita sales query
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The dashboard functionality shown in *Figure 8**.7* is built into Databricks
    SQL as a way to present the charts and other visualizations created from one or
    many queries.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – A dashboard with charts created from the Favorita sales data
    query'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_7.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – A dashboard with charts created from the Favorita sales data query
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The built-in visualization capabilities of DBSQL are a quick way to explore
    data without connecting to an external dashboarding or data visualization tool.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at an example of using Python **User-Defined Functions** (**UDFs**)
    for reusable Python code within DBSQL.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Python UDFs
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python UDFs are a way to create reusable snippets of code in Python, and these
    can be used in DBSQL. In this example, we’ll create a UDF for sales analysts to
    redact information in a customer record. Line five indicates the language syntax
    for the function is Python between `$$` signs:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Creating a Python UDF in DBSQL'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_8.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – Creating a Python UDF in DBSQL
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: UDFs are defined and managed as part of Unity Catalog. Once a UDF is defined,
    you can give teams the ability to execute the UDF using `GRANT EXECUTE`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Granting permissions for the sales-analysts groups to execute
    a UDF'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_8.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 – Granting permissions for the sales-analysts groups to execute a
    UDF
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: In this SQL query, we are applying the `redact` UDF to the `contact_info` field.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Using the Python UDF in a SQL query'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_8_(b).jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.10 – Using the Python UDF in a SQL query
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the basics for visualizing data and applying Python UDFs in
    SQL, let’s cover a couple of tips and tricks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section covers our tips and tricks related to DBSQL. Some tips apply to
    DBSQL and Lakeview, but not all:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '**Use managed compute (a.k.a. serverless compute) when possible**: Query performance
    using Databricks’ SQL warehouses is record-setting, as mentioned in [*Chapter
    1*](B16865_01.xhtml#_idTextAnchor016). The new managed compute for DBSQL puts
    the new first query performance at roughly 10 seconds. This means that idle time
    has drastically been reduced, which translates into cost savings.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use a subquery as a parameter filter**: In your query visualizations and
    dashboards, you can prepopulate drop-down filter boxes. You can do this by creating
    and saving a query in the SQL editor. For example, you could create a query that
    returns a distinct list of customer names. In *Figure 8**.11*, we select a query
    called **Customer Name Lookup Qry** as a subquery to filter the query visualization
    by customer name. Therefore, we can filter on **Customer** using a drop-down list.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Using a subquery as a parameter for a query'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_9.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.11 – Using a subquery as a parameter for a query
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**Schedule report delivery**: If you have users who want to receive an up-to-date
    dashboard regularly, you can schedule the refresh and have it sent to subscribers.
    For DBSQL dashboards, remember to turn off **Enabled** when you are developing
    so that users don’t get too many updates.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Scheduling a dashboard report with subscribers (T) DBSQL (B)
    Lakeview'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_10.jpg)![Figure 8.12 – Scheduling a dashboard report with subscribers
    (T) DBSQL (B) Lakeview'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_11.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_11.jpg]'
- en: Figure 8.12 – Scheduling a dashboard report with subscribers (T) DBSQL (B) Lakeview
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – 使用订阅者（T）DBSQL（B）Lakeview调度仪表板报告
- en: '**Speed up development with Databricks Assistant**: As we covered in [*Chapter
    4*](B16865_04.xhtml#_idTextAnchor180), Databricks Assistant is an AI-based interface
    that can help generate, transform, fix, and explain code. The Assistant is context-aware,
    meaning it uses Unity Catalog to look at the metadata of your tables and columns,
    personalized in your environment. In *Figure 8**.13*, we ask the Assistant to
    help write a query with syntax for grouping. It sees the metadata of the **Favorita****Stores**
    table and provides code specific to that table and the column of interest.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用Databricks Assistant加速开发**：正如我们在[*第4章*](B16865_04.xhtml#_idTextAnchor180)中所述，Databricks
    Assistant是一个基于AI的界面，可以帮助生成、转换、修复和解释代码。助手是上下文感知的，这意味着它使用Unity Catalog来查看您环境中表和列的元数据，并为您个性化。在*图8*.13中，我们要求助手帮助编写一个使用分组语法的查询。它看到了**Favorita****Stores**表的元数据，并为该表和感兴趣的列提供了特定的代码。'
- en: '![Figure 8.13 – Using Databricks Assistant for help writing a query'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.13 – 使用Databricks Assistant帮助编写查询'
- en: '](img/B16865_08_12.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_12.jpg]'
- en: Figure 8.13 – Using Databricks Assistant for help writing a query
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 使用Databricks Assistant帮助编写查询
- en: '**Be informed**: Keep an eye out for important data changes with alerts. Use
    SQL to calibrate the alert and schedule the condition evaluation at specific intervals
    through the UI shown in *Figure 8**.14*. You can use HTML to create a formatted
    alert email.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保持警觉**：通过警报关注重要数据变化。使用SQL调整警报，并通过*图8*.14中显示的UI在特定间隔内安排条件评估。您可以使用HTML创建格式化的警报电子邮件。'
- en: '![Figure 8.14 – Scheduling alerts to trigger when certain conditions are met'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.14 – 当满足特定条件时触发调度警报'
- en: '](img/B16865_08_13.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_13.jpg]'
- en: Figure 8.14 – Scheduling alerts to trigger when certain conditions are met
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – 当满足特定条件时触发调度警报
- en: '**Use tags to track usage**: When creating a new SQL warehouse, use tags to
    code your warehouse endpoint with the correct project. Tagging is a great way
    to understand usage by project or team. System tables contain the information
    for tracking usage.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用标签跟踪使用情况**：在创建新的SQL仓库时，使用标签对您的仓库端点进行编码，以正确标记项目。标记是了解按项目或团队使用情况的好方法。系统表包含跟踪使用情况的信息。'
- en: '![Figure 8.15 – Using tags to connect an endpoint to a project'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.15 – 使用标签将端点连接到项目'
- en: '](img/B16865_08_14.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_14.jpg]'
- en: Figure 8.15 – Using tags to connect an endpoint to a project
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – 使用标签将端点连接到项目
- en: Next, you’ll learn how to connect your models to applications.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将学习如何将您的模型连接到应用程序。
- en: Connecting your applications
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接您的应用程序
- en: You can deploy your model anywhere using Databricks Model Serving, which is
    how you deployed your RAG chatbot model in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325).
    In this section, we will introduce how to host ML demo apps in **Hugging Face**
    (**HF**). Having an easy way to host ML apps allows you to build your ML portfolio,
    showcase your projects at conferences or with stakeholders, and work collaboratively
    with others in the ML ecosystem. With HF Spaces, you have multiple options for
    which Python library you use to create a web app. Two common ones are Streamlit
    and Gradio.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Databricks Model Serving在任何地方部署您的模型，这是您在[*第7章*](B16865_07.xhtml#_idTextAnchor325)中部署您的RAG聊天机器人模型的方式。在本节中，我们将介绍如何在**Hugging
    Face**（**HF**）中托管ML演示应用程序。拥有一种简单的方式来托管ML应用程序，让您能够构建您的ML投资组合，在会议或与利益相关者展示您的项目，并与ML生态系统中的其他人协作工作。使用HF
    Spaces，您有多种选择来决定您使用哪个Python库来创建Web应用程序。其中两个常见的选择是Streamlit和Gradio。
- en: We prefer Gradio. It is an open source Python package that allows you to quickly
    build a demo or web application for your machine learning model, API, or any arbitrary
    Python function. You can then share a link to your demo or web application in
    just a few seconds using Gradio’s built-in sharing features. No JavaScript, CSS,
    or web hosting experience is needed – we love it!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更喜欢Gradio。它是一个开源的Python包，允许您快速为您的机器学习模型、API或任何任意的Python函数构建演示或Web应用程序。然后，您只需使用Gradio内置的共享功能，在几秒钟内就可以分享您的演示或Web应用程序的链接。无需JavaScript、CSS或Web托管经验
    – 我们非常喜欢它！
- en: We will walk you through deploying a chatbot to an HF Space in the *Applying
    our learning* section’s RAG project work.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在*应用我们的学习*部分的RAG项目工作中，我们将向您展示如何将聊天机器人部署到HF Space。
- en: Incorporating LLMs for analysts with SQL AI Functions
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将LLMs与SQL AI函数结合为分析师
- en: There are many use cases where you can integrate an LLM, such as DBRX or OpenAI,
    for insights. With the Databricks Data Intelligence Platform, it’s also possible
    for analysts who are most comfortable in SQL to take advantage of advances in
    machine learning and artificial intelligence.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多用例可以集成LLM，如DBRX或OpenAI，以获得见解。使用Databricks数据智能平台，对于最舒适使用SQL的分析师来说，利用机器学习和人工智能的进步也是可能的。
- en: Within Databricks, you can use **AI Functions**, which are built-in SQL functions
    to access LLMs directly. AI Functions are available for use in the DBSQL interface,
    SQL warehouse JDBC connection, or via the Spark SQL API. In *Figure 8**.16*, we
    are leveraging the Databricks SQL editor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在Databricks中，你可以使用**AI函数**，这些是内置的SQL函数，可以直接访问LLMs。AI函数可用于DBSQL界面、SQL仓库JDBC连接或通过Spark
    SQL API。在*图8*.16中，我们正在利用Databricks SQL编辑器。
- en: Foundational Models API
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型API
- en: The storage and processing of data for Databricks-hosted foundation models occur
    entirely within the Databricks Platform. Importantly, this data is not shared
    with any third-party model providers. This is not necessarily true when using
    the External Models API, which connects you to services such as OpenAI that have
    their own data privacy policies. Keep this in mind when you are concerned about
    data privacy. You may be able to pay for a tier of service that restricts the
    use of your data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks托管的基础模型的数据存储和处理完全在Databricks平台内部进行。重要的是，这些数据不会与任何第三方模型提供商共享。当使用连接到具有自己数据隐私政策的服务的External
    Models API时，这并不一定成立。当你关注数据隐私时，请记住这一点。你可能能够支付一个限制你数据使用的服务层级的费用。
- en: 'Let’s do some simple emotion classification. Since the three datasets we’ve
    been working with don’t include any natural language, we’ll create a small dataset
    ourselves first. You can also download a dataset (such as the Emotions dataset
    from Kaggle) or use any other natural language source available to you:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一些简单的情绪分类。由于我们一直在使用的三个数据集都不包含任何自然语言，我们首先创建一个小数据集。你也可以下载一个数据集（如Kaggle的Emotions数据集）或使用你可用的任何其他自然语言来源：
- en: First, let’s explore the built-in `AI_QUERY` DBSQL function. This command will
    send our prompt to the remote model configured and retrieve the result. We’re
    using Databricks’ DBRX model, but you can use a variety of other open source and
    proprietary models as well. Open up the Databricks SQL editor and type in the
    code as shown in *Figure 8**.16*. Let’s write a query to give us a sample sentence
    that we can classify.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们探索内置的`AI_QUERY` DBSQL函数。此命令将我们的提示发送到远程配置的模型并检索结果。我们使用Databricks的DBRX模型，但你也可以使用各种其他开源和专有模型。打开Databricks
    SQL编辑器，并输入如图8.16所示的代码。让我们编写一个查询，以获取我们可以分类的样本句子。
- en: '![Figure 8.16 – Crafting a prompt using the AI_QUERY function'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.16 – 使用AI_QUERY函数构建提示'
- en: '](img/B16865_08_15.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_15.jpg](img/B16865_08_15.jpg)'
- en: Figure 8.16 – Crafting a prompt using the AI_QUERY function
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 – 使用AI_QUERY函数构建提示
- en: If you don’t have a dataset ready and don’t want to download one, you can build
    a function to generate a dataset for you, as shown here. We’re expanding on the
    prompt from *Step 1* to get several sentences back in JSON format.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你没有准备好数据集，也不想下载，你可以构建一个为你生成数据集的函数，如图所示。我们正在扩展*步骤1*的提示，以获取几个JSON格式的句子。
- en: '![Figure 8.17 – Creating a function to generate fake data'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.17 – 创建生成虚假数据的函数'
- en: '](img/B16865_08_16.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_16.jpg](img/B16865_08_16.jpg)'
- en: Figure 8.17 – Creating a function to generate fake data
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 – 创建生成虚假数据的函数
- en: Now use the `GENERATE_EMOTIONS_DATA` function to build a small dataset. After
    a quick review of the data, it looks like we have a good sample of emotions.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用`GENERATE_EMOTIONS_DATA`函数构建一个小数据集。快速查看数据后，看起来我们有一个很好的情绪样本。
- en: '![Figure 8.18 – Generating fake emotion data'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.18 – 生成虚假情绪数据'
- en: '](img/B16865_08_17.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_17.jpg](img/B16865_08_17.jpg)'
- en: Figure 8.18 – Generating fake emotion data
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 – 生成虚假情绪数据
- en: Now, we’re going to write a function called `CLASSIFY_EMOTION`. We’re using
    the AI Function `AI_QUERY` again, but this function will use a new prompt asking
    the model to classify a given sentence as one of six emotions.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将编写一个名为`CLASSIFY_EMOTION`的函数。我们再次使用AI函数`AI_QUERY`，但这个函数将使用一个新的提示，要求模型将给定的句子分类为六种情绪之一。
- en: '![Figure 8.19 – Creating a function to classify sentences by emotion'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.19 – 创建按情绪分类句子的函数'
- en: '](img/B16865_08_18.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16865_08_18.jpg](img/B16865_08_18.jpg)'
- en: Figure 8.19 – Creating a function to classify sentences by emotion
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 – 创建按情绪分类句子的函数
- en: Let’s call our function to evaluate an example sentence and take a look at the
    results.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们调用我们的函数来评估一个示例句子，并查看结果。
- en: '![Figure 8.20 – Calling the CLASSIFY_EMOTION function'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.20 – 调用 CLASSIFY_EMOTION 函数'
- en: '](img/B16865_08_19.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_19.jpg]'
- en: Figure 8.20 – Calling the CLASSIFY_EMOTION function
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.20 – 调用 CLASSIFY_EMOTION 函数
- en: Finally, to classify all records in a table, we call the `CLASSIFY_EMOTION`
    function on the records in our table and view the results.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，为了对表中的所有记录进行分类，我们在表中的记录上调用`CLASSIFY_EMOTION`函数并查看结果。
- en: '![Figure 8.21 – Calling the CLASSIFY_EMOTION function on a table'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.21 – 在表上调用 CLASSIFY_EMOTION 函数'
- en: '](img/B16865_08_20.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16865_08_20.jpg]'
- en: Figure 8.21 – Calling the CLASSIFY_EMOTION function on a table
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.21 – 在表上调用 CLASSIFY_EMOTION 函数
- en: SQL AI Functions are a great way to put the power of LLMs in the hands of SQL
    users. Solutions like SQL AI Functions still require some technical knowledge.
    Databricks is researching ways to allow business users direct access to data,
    with less upfront development required to get your team moving even faster. Keep
    an eye out for exciting new product features that remove the programming experience
    barrier to unlock the value of your data!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: SQL AI 函数是将 LLM 的力量交到 SQL 用户手中的绝佳方式。像 SQL AI 函数这样的解决方案仍然需要一些技术知识。Databricks
    正在研究允许业务用户直接访问数据的方法，这样就不需要太多的前期开发，以便让您的团队更快地运转。请密切关注令人兴奋的新产品功能，这些功能将消除编程经验障碍，释放您数据的价值！
- en: Applying our learning
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: Let’s use what we have learned to build a SQL chatbot using the Favorita project’s
    table metadata, monitor the streaming transaction project’s model, and deploy
    the chatbot that we have assembled and evaluated.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运用我们所学的知识，使用 Favorita 项目的表元数据构建一个 SQL 聊天机器人，监控流式事务项目的模型，并部署我们已组装和评估的聊天机器人。
- en: Technical requirements
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The technical requirements needed to complete the hands-on examples in this
    chapter are as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章动手实践所需的技术要求如下：
- en: The SQLbot will require OpenAI credentials.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQLbot 将需要 OpenAI 凭证。
- en: We will use the Databricks Secrets API to store our OpenAI credentials.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用 Databricks Secrets API 来存储我们的 OpenAI 凭证。
- en: You will need a **personal access token** (**PAT**) to deploy your web app to
    HF. See *Further reading* for detailed instructions.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要**个人访问令牌**（**PAT**）才能将您的 Web 应用部署到 HF。请参阅*进一步阅读*以获取详细说明。
- en: 'Project: Favorita store sales'
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目：Favorita 店铺销售
- en: Let’s build a simple **SQLbot** using OpenAI’s GPT to ask questions about our
    Favorita Sales tables. Please note that while this section continues to use the
    *Favorita Store Sales* data, it is not a continuation of the earlier project work.
    In this example, you’ll create instructions on how the bot can ask for a list
    of tables, get information from those tables, and sample data from the tables.
    The SQLbot will be able to build a SQL query and then interpret the results. To
    run the notebooks in this example, you will need an account with OpenAI on the
    OpenAI developer site and request a key for the OpenAI API.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 OpenAI 的 GPT 构建一个简单的 **SQLbot**，以询问有关我们的 Favorita 销售表的问题。请注意，尽管本节继续使用
    *Favorita Store Sales* 数据，但它并不是早期项目工作的延续。在这个例子中，您将创建有关机器人如何请求表列表、从这些表中获取信息以及从表中采样数据的说明。SQLbot
    将能够构建 SQL 查询并解释结果。要运行本例中的笔记本，您需要在 OpenAI 开发者网站上拥有一个账户，并请求 OpenAI API 的密钥。
- en: 'To follow along in your own workspace, please open the following notebook:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要在自己的工作区中跟随，请打开以下笔记本：
- en: '`CH8-01-SQL Chatbot`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`CH8-01-SQL Chatbot`'
- en: Keeping secret API keys in your Databricks notebook is far from best practice.
    You can lock down your notebook access and add a configuration notebook to your
    `.gitignore` file. However, your ability to remove people’s access may not be
    in your control, depending on your role. Generally, the permissions of admins
    include the ability to see all code. The OpenAI API key ties back to your account
    and your credit card. Note that running the notebook once cost us $0.08.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 笔记本中保留秘密 API 密钥绝不是最佳实践。您可以锁定笔记本访问并添加配置笔记本到您的 `.gitignore` 文件中。然而，您移除人们访问的能力可能不在您的控制之下，这取决于您的角色。通常，管理员权限包括查看所有代码的能力。OpenAI
    API 密钥与您的账户和信用卡相关联。请注意，运行笔记本一次花费了我们 $0.08。
- en: We added our API key to Databricks secrets. The Secrets API requires the Databricks
    CLI. We set up our CLI through Homebrew. If you haven’t already, we suggest getting
    Secrets set up for your workspace. This may require admin assistance. Start by
    installing or updating the Databricks CLI. You know the CLI is installed correctly
    when you get version v0.2 or higher. We are working with `Databricks` `CLI v0.208.0`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'We followed these steps to set up our API key as a secret:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a scope:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a secret within the scope:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Paste your API key into the prompt.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once your secret is successfully saved, we can access it via `dbutils.secrets`
    in our notebooks.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: We are all set up to use OpenAI via the API now. We do not have to worry about
    accidentally committing our API or a coworker running the code, not knowing it
    costs you money.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s focus on creating our SQLbot notebook, step by step, beginning
    with the setup:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we install three libraries: `openai`, `langchain_experimental`, and
    `sqlalchemy-databricks`.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To create a connection to OpenAI, pass the secret we set up previously and open
    a `ChatOpenAI` connection.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Figure 8**.22*, we create two different models. The first is the default
    model and the second uses GPT 3.5 Turbo.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.22 – OpenAI API connection'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_21.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.22 – OpenAI API connection
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The setup file does not set your schema variable. Define your schema; we chose
    `favorita_forecasting`. We have been using `database_name` rather than a schema.
    However, we specify the database we want to ask SQL questions against, which is
    different.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.23 – (L) Collecting the table schema and system information schema;
    (R) dropping unnecessary and repetitive columns'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_22.jpg)![Figure 8.23 – (L) Collecting the table schema and
    system information schema; (R) dropping unnecessary and repetitive columns'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_23.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.23 – (L) Collecting the table schema and system information schema;
    (R) dropping unnecessary and repetitive columns
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create two helper functions. The first function organizes the schema
    information provided, `table_schemas`, creating a table definition. The second
    collects two rows of data as examples.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.24 – Helper functions for organizing table information'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_24.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.24 – Helper functions for organizing table information
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Iterate through the table and column data, leveraging our helper functions to
    format the SQL database input.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.25 – Iterating through the tables and leveraging the helper functions'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_25.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.25 – Iterating through the tables and leveraging the helper functions
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: We now have all of our data ready to be able to create a SQL database for OpenAI
    to talk to. You will need to edit `endpoint_http_path` to match the path of an
    active SQL warehouse in your workspace. The database is passed to both the default
    OpenAI model and the GPT 3.5 model.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.26 – Create a database for OpenAI to query containing only the information
    we provided'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_26.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.26 – Create a database for OpenAI to query containing only the information
    we provided
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'With the setup complete, we can now interact with our SQL chatbot models! Let’s
    start with a basic question: *Which store sold* *the most?*'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 8**.27*, we run both models on the question and get back two different
    answers.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.27 – The SQL chatbot model’s responses to our question “Which store
    sold the most?”. (T) db_chain.run(question) (B) chat_chain.run(question)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_27.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.27 – The SQL chatbot model’s responses to our question “Which store
    sold the most?”. (T) db_chain.run(question) (B) chat_chain.run(question)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: As new versions of OpenAI’s GPT model are released, the results and behavior
    of your SQLbot may change. As new models and approaches become available, it is
    good practice to test them and see how the changes impact your work and the results
    of your chatbot. Leveraging MLflow with your SQLbot experiment will help you track
    and compare the different features and configurations throughout your production
    process.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Project -streaming transactions
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are ready to wrap up this project. The production workflow notebooks are
    the `CH7-08-Production Generating Records`, `CH7-09-Production Auto Loader`, and
    `CH7-10-Production Feature Engineering` components in the workflow job created
    in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325). You will run that same job
    once the new workflow is in place. To follow along in your own workspace, please
    open the following notebook:`CH8-05-Production Monitoring`
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: In the `CH8-05-Production Monitoring` notebook, you create two monitors – one
    for the `prod_transactions` table and one for the `packaged_transaction_model_predictions`
    table. See *Figure 8**.28* for the latter.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.28 – The inference table monitor'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_28.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.28 – The inference table monitor
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Congratulations! The streaming project is complete. We encourage you to add
    improvements and commit them back to the repository. Here are a few possible examples:
    add more validation metrics to the validation notebook, incorporate the inference
    performance results into the decision to retrain, and make adjustments to the
    configuration of data generation to simulate drift.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Project: retrieval-augmented generation chatbot'
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To follow along in your own workspace, please open the following notebooks
    and resources:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: CH8-`app.py`
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH8-01-Deploy Your Endpoint` `with SDK`'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Hugging Face** **Spaces** page
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we need to ensure our chatbot is deployed using Model Serving, as shown
    in *Figure 8**.30*. Here, we are using the fastest way by going through the UI
    of the Model Serving page. To follow along and serve, we are selecting the registered
    model that we registered in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325). Select
    the latest version – in our case, it’s version 4\. For this demo project, we expect
    minimal concurrency so the endpoint will have only four possible concurrent runs
    and will scale to 0 when no traffic is available. We are enabling inference tables
    under the same catalog to track and potentially further monitor our payload. We
    are not going to demonstrate in this chapter how to set a monitor or data quality
    pipeline for the RAG project, as it was demonstrated for the streaming project.
    We encourage you to apply it on your own!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.29 – Example of the Model Serving deployment via UI'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_29.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.29 – Example of the Model Serving deployment via UI
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for your application to be able to connect to the resources attached
    to it, such as Vector Search, the endpoint requires you to provide additional
    configurations to the endpoint such as your PAT and host under **Advanced configuration**:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.30 – Advanced configuration requirements'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_30.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.30 – Advanced configuration requirements
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: You could also use the Databricks SDK service to deploy your endpoints. If you
    are interested in seeing how to use SDK deployment, please use the notebook attached
    under `CH8 - 01 -Deploy Your Endpoint` `with SDK`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Jump over to the Hugging Face Spaces website. Instructions on how to deploy
    your first HF Space are explained very well on the main page of HF Spaces, so
    we will not duplicate them here. We would like to highlight that we are using
    a free deployment option of Spaces with 2 CPUs and 16 GB of memory.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'When you deploy your Space, it will look like this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.31 – Empty Hugging Face Space'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_31.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.31 – Empty Hugging Face Space
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'We would like to highlight a few things that are important in order to connect
    to your chatbot, which is served in real time with Databricks Model Serving. To
    connect the chatbot to your HF Space, you must set `API_TOKEN` and an `API_ENDPOINT`.
    Here’s how to set these values:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Go to **Settings** in the HF Space you created.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down to **Variables** **and secrets**.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set your API_ENDPOINT as the URL from the REST API provided on the Databricks
    Model Serving page.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set your API_TOKEN using a personal access token generated by Databricks. This
    is required to connect to the endpoint.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.32 – Example of Variables and secrets on HF Spaces'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_32.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.32 – Example of Variables and secrets on HF Spaces
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Once this is set, you are ready to bring your Gradio web app script into your
    HF Space.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.33 – Example of Variables and secrets on HF Spaces'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_33.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.33 – Example of Variables and secrets on HF Spaces
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: When your endpoint is ready, jump back to your HF Space.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **Files** tab under the pre-created Space and click **+****Add File**.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now add `CH8-app.py` that was given to you for [*Chapter 8*](B16865_08.xhtml#_idTextAnchor384)
    – you can create your own web application. Feel free to experiment with the design
    according to your business needs.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s talk a bit about the `respond` function in the `CH8-app.py` file – see
    *Figure 8**.34*, which is passed to our app’s UI chatbot. The `respond` function,
    in this case, is the caller of your deployed endpoints, where we do not just send
    and receive responses but also can shape the format of the input or output. In
    our case, the endpoint expects to receive a request in the format of a JSON with
    field inputs with the questions within a list, while the output is a JSON with
    field predictions.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.34 – The respond function written in the Gradio app'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_34.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.34 – The respond function written in the Gradio app
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: To create a chatbot, as was mentioned in the introduction section, we are using
    a simple example from Gradio where we add options such as the title of our application,
    a description, and example questions. *Figure 8**.35* shows the full code.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.35 – The Gadio app.py interface for your LLM'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_35.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.35 – The Gadio app.py interface for your LLM
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The chatbot is now available in a more user-friendly interface, as shown in
    *Figure 8**.36*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.36 – The interface for your chatbot application'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_36.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.36 – The interface for your chatbot application
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Let’s ask a few questions to make sure our RAG chatbot is providing correct
    results.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.37 – Examples of chatbot answers from our RAG application'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_37.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.37 – Examples of chatbot answers from our RAG application
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: If the responses look good, your application is ready to be used!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sharing insights from your models is an important way to get value from your
    machine learning practice. Using dashboards as a medium for sharing your information
    is an accessible way to communicate insights to business users and teams outside
    of the data science team. In this chapter, we discussed how to build the gold
    layer, tips on presenting your data using DBSQL dashboards, taking advantage of
    innovations such as OpenAI models in DBSQL, and how you can share data and AI
    artifacts through Databricks Marketplace to get the most value from your enterprise
    data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: We hope you have had a chance to get hands-on with building your lakehouse.
    From exploration, cleaning, building pipelines, and building models to finding
    insights hidden in your data, all the way to sharing insights – it’s all doable
    on the Databricks Platform. We encourage you to take the notebooks and experiment!
    The authors would love to hear feedback, and whether this has been helpful on
    your journey with the Databricks Platform.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s test ourselves on what we’ve learned by going through the following questions:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: What are some differences between the gold layer and the silver layer?
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is one way you could set an alert to identify that a table has an invalid
    value?
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would you choose to use an external dashboarding tool?
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you use a language model through an API such as OpenAI, what are some considerations
    about the data you send via the API?
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some reasons a company would share data in Databricks Marketplace?
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After putting thought into the questions, compare your answers to ours:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The gold layer is more refined and aggregated than the silver layer. The silver
    layer powers data science and machine learning, and the gold layer powers analytics
    and dashboarding.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can monitor the values of a field and send an email alert when the value
    is invalid.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sometimes companies use multiple dashboarding tools. You might need to provide
    data in a dashboard a team is accustomed to using.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If I were a language model through an API, I would be cautious about sending
    sensitive data, including PII, customer information, or proprietary information.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A company might share data in Databricks Marketplace in order to monetize the
    data or make it available externally for people to use easily and securely.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we pointed out specific technologies, technical features,
    and options. Please look at these resources to get deeper into the areas that
    interest you most:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '*Databricks SQL Statement Execution* *API*: [https://www.databricks.com/blog/2023/03/07/databricks-sql-statement-execution-api-announcing-public-preview.html](https://www.databricks.com/blog/2023/03/07/databricks-sql-statement-execution-api-announcing-public-preview.html)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Power to the SQL People: Introducing Python UDFs in Databricks* *SQL*: [https://www.databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html](https://www.databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Actioning Customer Reviews at Scale with Databricks SQL AI* *Functions*: [https://www.databricks.com/blog/actioning-customer-reviews-scale-databricks-sql-ai-functions](https://www.databricks.com/blog/actioning-customer-reviews-scale-databricks-sql-ai-functions)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks sets the official data warehousing performance* *record*: https://dbricks.co/benchmark'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Lakehouse and Data* *Mesh*: [https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html](https://www.databricks.com/blog/databricks-lakehouse-and-data-mesh-part-1)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hugging* *Face*: [https://huggingface.co/spaces](https://huggingface.co/spaces)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gradio*: [https://www.gradio.app/](https://www.gradio.app/)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hugging Face* *Spaces*: [https://huggingface.co/docs/hub/en/spaces-overview](https://huggingface.co/docs/hub/en/spaces-overview)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Lakehouse monitoring* *documentation*: [https://api-docs.databricks.com/python/lakehouse-monitoring/latest/databricks.lakehouse_monitoring.html#module-databricks.lakehouse_monitoring](https://api-docs.databricks.com/python/lakehouse-monitoring/latest/databricks.lakehouse_monitoring.html#module-databricks.lakehouse_monitoring
    )'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks personal access token authentication [https://docs.databricks.com/en/dev-tools/auth/pat.html](https://docs.databricks.com/en/dev-tools/auth/pat.html)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
