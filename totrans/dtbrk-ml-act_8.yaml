- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring, Evaluating, and More
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Focus on how the end-user customers perceive the impact of your innovation
    – rather than on how you, the innovators, perceive it.” — Thomas A. Edison
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, you’ve made it to the final chapter! We’ve come a long way,
    yet there is still more to explore in Databricks. As we wrap up, we will take
    another look at Lakehouse Monitoring. We’ll focus on monitoring model inference
    data. After all the work you’ve put in to build a robust model and push it into
    production, it’s essential to share the learnings, predictions, and other outcomes
    with a broad audience. Sharing results with dashboards is very common. We will
    cover how to create visualizations for dashboards in both the new Lakeview dashboards
    and the standard Databricks SQL dashboards. Deployed models can be shared via
    a web application. Therefore, we will not only introduce Hugging Face Spaces but
    also deploy the RAG chatbot using the Gradio app in *Applying our learning*. Lastly,
    we’ll demonstrate how analysts can invoke LLMs via SQL AI Functions! By the end
    of this chapter, you will be ready to monitor inference data, create visualizations,
    deploy an ML web app, and use the groundbreaking DBRX open source LLM with SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the roadmap for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring your models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building gold layer visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting your applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating LLMs for analysts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring your models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ML lifecycle does not end at deployment. Once a model is in production,
    we want to monitor the input data and output results of the model. In [*Chapter
    4*](B16865_04.xhtml#_idTextAnchor180), we explored two key features of Databricks
    Lakehouse Monitoring integrated with Unity Catalog: Snapshot and TimeSeries profiles.
    Snapshot profiles are designed to provide an overview of a dataset at a specific
    point in time, capturing its current state. This is particularly useful for identifying
    immediate data quality issues or changes. On the other hand, TimeSeries profiles
    focus on how data evolves over time, making them ideal for tracking trends, patterns,
    and gradual changes in data distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: Expanding on these capabilities, Databricks also provides an Inference profile,
    tailored for monitoring machine learning models in production. This advanced profile
    builds upon the concept of TimeSeries profiles, adding critical functionalities
    for comprehensive model performance evaluation. It includes model quality metrics,
    essential for tracking the accuracy and reliability of predictions over time.
    It also records predictions and, optionally, ground truth labels, directly comparing
    expected and actual outcomes. This functionality is key for identifying model
    drift, where shifts in input data or the relationship between inputs and outputs
    occur.
  prefs: []
  type: TYPE_NORMAL
- en: Inference Tables in Databricks further bolster this monitoring capability. They
    contain essential elements such as model predictions, input features, timestamps,
    and potentially ground truth labels. Building a monitor on top of InferenceTables
    with the corresponding `InferenceLog` allows us to monitor model performance and
    data drift continuously.
  prefs: []
  type: TYPE_NORMAL
- en: In the event of drift detection, immediate actions should be taken – data pipeline
    verification or model retraining and evaluation are recommended. These steps ensure
    the model adapts to new data patterns, maintaining accuracy and effectiveness.
    Continuous monitoring against your baseline and cross-model versions is a strategy
    to adopt when trying to ensure a stable process across various deployed solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.1* is a code sample for creating an Inference monitor with model
    quality metrics using the `InferenceLog` profile type. This illustrates a practical
    application of this monitoring setup. We specify the `schedule` argument to make
    sure that this monitor is refreshed hourly.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Creating an Inference profile monitor that refreshes hourly'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Creating an Inference profile monitor that refreshes hourly
  prefs: []
  type: TYPE_NORMAL
- en: Model monitoring is an effective way to ensure your models are working for you
    as expected. We hope this gets you thinking about how you use monitoring in your
    MLOPs process.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll learn about ways to create dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: Building gold layer visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The gold layer in your lakehouse is the consumption-ready layer. In this layer,
    final transformations and aggregations crystallize the insights within your data
    so it is ready for reporting and dashboarding. Being able to share your data with
    an audience is critical, and there are several options for doing so in the DI
    Platform. In fact, both Lakeview and Databricks SQL dashboards allow you to transform
    and aggregate your data within visualizations. Let’s walk through how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging Lakeview dashboards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lakeview dashboards in Databricks are a powerful tool for creating data visualizations
    and sharing insights hidden in data. Visualizations can be made in the English
    language, making dashboard creation available to more users. To create a Lakeview
    dashboard, first click `ml_in_action.favorita_forecasting.train_set` table. This
    creates a dataset by selecting all records from the provided table. Notice how
    we do not *have to* write any SQL or create aggregates in order to visualize data
    aggregations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – The Data tab for adding data to your Lakeview dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – The Data tab for adding data to your Lakeview dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a dataset, return to the **Canvas** tab. Select the **Add a visualization**
    button found on the blue bar toward the bottom of your browser window. This gives
    you a widget to place on your dashboard. Once placed, your widget will look similar
    to *Figure 8**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – A new Lakeview widget'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – A new Lakeview widget
  prefs: []
  type: TYPE_NORMAL
- en: In the new widget, you can manually create a visualization using the options
    on the right-side menu. Alternatively, Databricks Assistant can help you rapidly
    build a chart using just English. You can write your own question, or explore
    the suggested queries. We selected the suggested question *What is the trend of
    onpromotion over date?* to automatically generate a chart, and *Figure 8**.4*
    is the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – English text generated Lakeview widget'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – English text generated Lakeview widget
  prefs: []
  type: TYPE_NORMAL
- en: When you are ready to share your dashboard, you can publish it! The engine powering
    Lakeview dashboards is optimized for performance, driving faster interactive charts
    for your data. It’s also powerful enough to handle streaming data. Furthermore,
    Lakeview dashboards are unified with the DI Platform through Unity Catalog, providing
    data lineage. They are designed for easy sharing across workspaces, meaning users
    in other workspaces can access your curated dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing big data with Databricks SQL dashboards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lakeview dashboards are the future of Databricks. However, you can also build
    dashboards with `select` statement, you can produce the data and use it for multiple
    visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To recreate *Figure 8**.5* in your own workspace, you will want to uncheck the
    **LIMIT 1000** box. There is still a limit for the visualizations of 64,000 rows.
    The best way to get around this is by filtering or aggregating.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.5* is an example visualization we created from a simple SQL query
    against the *Favorita Store* *Sales* data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – After executing the select statement in the DBSQL editor, we
    create the visualizations without writing any code'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – After executing the select statement in the DBSQL editor, we create
    the visualizations without writing any code
  prefs: []
  type: TYPE_NORMAL
- en: Suppose your dataset has categorical variables that you want to use to filter
    and compare features, as with the *Favorita* sales data. You can add filters within
    the DBSQL editor without revising the query. To add a filter, click the **+**
    and choose either **filter** or **parameter**. Both options provide widgets for
    filtering, as shown in *Figure 8**.6*. You can use the widgets with any visualizations
    or dashboards associated with the query.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – (L) The configuration for the state and family filter; (R) the
    result of adding two filters to the Favorita sales query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – (L) The configuration for the state and family filter; (R) the
    result of adding two filters to the Favorita sales query
  prefs: []
  type: TYPE_NORMAL
- en: The dashboard functionality shown in *Figure 8**.7* is built into Databricks
    SQL as a way to present the charts and other visualizations created from one or
    many queries.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – A dashboard with charts created from the Favorita sales data
    query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – A dashboard with charts created from the Favorita sales data query
  prefs: []
  type: TYPE_NORMAL
- en: The built-in visualization capabilities of DBSQL are a quick way to explore
    data without connecting to an external dashboarding or data visualization tool.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at an example of using Python **User-Defined Functions** (**UDFs**)
    for reusable Python code within DBSQL.
  prefs: []
  type: TYPE_NORMAL
- en: Python UDFs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python UDFs are a way to create reusable snippets of code in Python, and these
    can be used in DBSQL. In this example, we’ll create a UDF for sales analysts to
    redact information in a customer record. Line five indicates the language syntax
    for the function is Python between `$$` signs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Creating a Python UDF in DBSQL'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – Creating a Python UDF in DBSQL
  prefs: []
  type: TYPE_NORMAL
- en: UDFs are defined and managed as part of Unity Catalog. Once a UDF is defined,
    you can give teams the ability to execute the UDF using `GRANT EXECUTE`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Granting permissions for the sales-analysts groups to execute
    a UDF'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 – Granting permissions for the sales-analysts groups to execute a
    UDF
  prefs: []
  type: TYPE_NORMAL
- en: In this SQL query, we are applying the `redact` UDF to the `contact_info` field.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Using the Python UDF in a SQL query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_8_(b).jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.10 – Using the Python UDF in a SQL query
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the basics for visualizing data and applying Python UDFs in
    SQL, let’s cover a couple of tips and tricks.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section covers our tips and tricks related to DBSQL. Some tips apply to
    DBSQL and Lakeview, but not all:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use managed compute (a.k.a. serverless compute) when possible**: Query performance
    using Databricks’ SQL warehouses is record-setting, as mentioned in [*Chapter
    1*](B16865_01.xhtml#_idTextAnchor016). The new managed compute for DBSQL puts
    the new first query performance at roughly 10 seconds. This means that idle time
    has drastically been reduced, which translates into cost savings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use a subquery as a parameter filter**: In your query visualizations and
    dashboards, you can prepopulate drop-down filter boxes. You can do this by creating
    and saving a query in the SQL editor. For example, you could create a query that
    returns a distinct list of customer names. In *Figure 8**.11*, we select a query
    called **Customer Name Lookup Qry** as a subquery to filter the query visualization
    by customer name. Therefore, we can filter on **Customer** using a drop-down list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Using a subquery as a parameter for a query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.11 – Using a subquery as a parameter for a query
  prefs: []
  type: TYPE_NORMAL
- en: '**Schedule report delivery**: If you have users who want to receive an up-to-date
    dashboard regularly, you can schedule the refresh and have it sent to subscribers.
    For DBSQL dashboards, remember to turn off **Enabled** when you are developing
    so that users don’t get too many updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Scheduling a dashboard report with subscribers (T) DBSQL (B)
    Lakeview'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_10.jpg)![Figure 8.12 – Scheduling a dashboard report with subscribers
    (T) DBSQL (B) Lakeview'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.12 – Scheduling a dashboard report with subscribers (T) DBSQL (B) Lakeview
  prefs: []
  type: TYPE_NORMAL
- en: '**Speed up development with Databricks Assistant**: As we covered in [*Chapter
    4*](B16865_04.xhtml#_idTextAnchor180), Databricks Assistant is an AI-based interface
    that can help generate, transform, fix, and explain code. The Assistant is context-aware,
    meaning it uses Unity Catalog to look at the metadata of your tables and columns,
    personalized in your environment. In *Figure 8**.13*, we ask the Assistant to
    help write a query with syntax for grouping. It sees the metadata of the **Favorita****Stores**
    table and provides code specific to that table and the column of interest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Using Databricks Assistant for help writing a query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.13 – Using Databricks Assistant for help writing a query
  prefs: []
  type: TYPE_NORMAL
- en: '**Be informed**: Keep an eye out for important data changes with alerts. Use
    SQL to calibrate the alert and schedule the condition evaluation at specific intervals
    through the UI shown in *Figure 8**.14*. You can use HTML to create a formatted
    alert email.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Scheduling alerts to trigger when certain conditions are met'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.14 – Scheduling alerts to trigger when certain conditions are met
  prefs: []
  type: TYPE_NORMAL
- en: '**Use tags to track usage**: When creating a new SQL warehouse, use tags to
    code your warehouse endpoint with the correct project. Tagging is a great way
    to understand usage by project or team. System tables contain the information
    for tracking usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Using tags to connect an endpoint to a project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.15 – Using tags to connect an endpoint to a project
  prefs: []
  type: TYPE_NORMAL
- en: Next, you’ll learn how to connect your models to applications.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting your applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can deploy your model anywhere using Databricks Model Serving, which is
    how you deployed your RAG chatbot model in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325).
    In this section, we will introduce how to host ML demo apps in **Hugging Face**
    (**HF**). Having an easy way to host ML apps allows you to build your ML portfolio,
    showcase your projects at conferences or with stakeholders, and work collaboratively
    with others in the ML ecosystem. With HF Spaces, you have multiple options for
    which Python library you use to create a web app. Two common ones are Streamlit
    and Gradio.
  prefs: []
  type: TYPE_NORMAL
- en: We prefer Gradio. It is an open source Python package that allows you to quickly
    build a demo or web application for your machine learning model, API, or any arbitrary
    Python function. You can then share a link to your demo or web application in
    just a few seconds using Gradio’s built-in sharing features. No JavaScript, CSS,
    or web hosting experience is needed – we love it!
  prefs: []
  type: TYPE_NORMAL
- en: We will walk you through deploying a chatbot to an HF Space in the *Applying
    our learning* section’s RAG project work.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating LLMs for analysts with SQL AI Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many use cases where you can integrate an LLM, such as DBRX or OpenAI,
    for insights. With the Databricks Data Intelligence Platform, it’s also possible
    for analysts who are most comfortable in SQL to take advantage of advances in
    machine learning and artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Within Databricks, you can use **AI Functions**, which are built-in SQL functions
    to access LLMs directly. AI Functions are available for use in the DBSQL interface,
    SQL warehouse JDBC connection, or via the Spark SQL API. In *Figure 8**.16*, we
    are leveraging the Databricks SQL editor.
  prefs: []
  type: TYPE_NORMAL
- en: Foundational Models API
  prefs: []
  type: TYPE_NORMAL
- en: The storage and processing of data for Databricks-hosted foundation models occur
    entirely within the Databricks Platform. Importantly, this data is not shared
    with any third-party model providers. This is not necessarily true when using
    the External Models API, which connects you to services such as OpenAI that have
    their own data privacy policies. Keep this in mind when you are concerned about
    data privacy. You may be able to pay for a tier of service that restricts the
    use of your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do some simple emotion classification. Since the three datasets we’ve
    been working with don’t include any natural language, we’ll create a small dataset
    ourselves first. You can also download a dataset (such as the Emotions dataset
    from Kaggle) or use any other natural language source available to you:'
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s explore the built-in `AI_QUERY` DBSQL function. This command will
    send our prompt to the remote model configured and retrieve the result. We’re
    using Databricks’ DBRX model, but you can use a variety of other open source and
    proprietary models as well. Open up the Databricks SQL editor and type in the
    code as shown in *Figure 8**.16*. Let’s write a query to give us a sample sentence
    that we can classify.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Crafting a prompt using the AI_QUERY function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.16 – Crafting a prompt using the AI_QUERY function
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have a dataset ready and don’t want to download one, you can build
    a function to generate a dataset for you, as shown here. We’re expanding on the
    prompt from *Step 1* to get several sentences back in JSON format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Creating a function to generate fake data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.17 – Creating a function to generate fake data
  prefs: []
  type: TYPE_NORMAL
- en: Now use the `GENERATE_EMOTIONS_DATA` function to build a small dataset. After
    a quick review of the data, it looks like we have a good sample of emotions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.18 – Generating fake emotion data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.18 – Generating fake emotion data
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’re going to write a function called `CLASSIFY_EMOTION`. We’re using
    the AI Function `AI_QUERY` again, but this function will use a new prompt asking
    the model to classify a given sentence as one of six emotions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Creating a function to classify sentences by emotion'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.19 – Creating a function to classify sentences by emotion
  prefs: []
  type: TYPE_NORMAL
- en: Let’s call our function to evaluate an example sentence and take a look at the
    results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.20 – Calling the CLASSIFY_EMOTION function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.20 – Calling the CLASSIFY_EMOTION function
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to classify all records in a table, we call the `CLASSIFY_EMOTION`
    function on the records in our table and view the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.21 – Calling the CLASSIFY_EMOTION function on a table'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.21 – Calling the CLASSIFY_EMOTION function on a table
  prefs: []
  type: TYPE_NORMAL
- en: SQL AI Functions are a great way to put the power of LLMs in the hands of SQL
    users. Solutions like SQL AI Functions still require some technical knowledge.
    Databricks is researching ways to allow business users direct access to data,
    with less upfront development required to get your team moving even faster. Keep
    an eye out for exciting new product features that remove the programming experience
    barrier to unlock the value of your data!
  prefs: []
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s use what we have learned to build a SQL chatbot using the Favorita project’s
    table metadata, monitor the streaming transaction project’s model, and deploy
    the chatbot that we have assembled and evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The technical requirements needed to complete the hands-on examples in this
    chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The SQLbot will require OpenAI credentials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use the Databricks Secrets API to store our OpenAI credentials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need a **personal access token** (**PAT**) to deploy your web app to
    HF. See *Further reading* for detailed instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Project: Favorita store sales'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s build a simple **SQLbot** using OpenAI’s GPT to ask questions about our
    Favorita Sales tables. Please note that while this section continues to use the
    *Favorita Store Sales* data, it is not a continuation of the earlier project work.
    In this example, you’ll create instructions on how the bot can ask for a list
    of tables, get information from those tables, and sample data from the tables.
    The SQLbot will be able to build a SQL query and then interpret the results. To
    run the notebooks in this example, you will need an account with OpenAI on the
    OpenAI developer site and request a key for the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow along in your own workspace, please open the following notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH8-01-SQL Chatbot`'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping secret API keys in your Databricks notebook is far from best practice.
    You can lock down your notebook access and add a configuration notebook to your
    `.gitignore` file. However, your ability to remove people’s access may not be
    in your control, depending on your role. Generally, the permissions of admins
    include the ability to see all code. The OpenAI API key ties back to your account
    and your credit card. Note that running the notebook once cost us $0.08.
  prefs: []
  type: TYPE_NORMAL
- en: We added our API key to Databricks secrets. The Secrets API requires the Databricks
    CLI. We set up our CLI through Homebrew. If you haven’t already, we suggest getting
    Secrets set up for your workspace. This may require admin assistance. Start by
    installing or updating the Databricks CLI. You know the CLI is installed correctly
    when you get version v0.2 or higher. We are working with `Databricks` `CLI v0.208.0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We followed these steps to set up our API key as a secret:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a scope:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a secret within the scope:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Paste your API key into the prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once your secret is successfully saved, we can access it via `dbutils.secrets`
    in our notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: We are all set up to use OpenAI via the API now. We do not have to worry about
    accidentally committing our API or a coworker running the code, not knowing it
    costs you money.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s focus on creating our SQLbot notebook, step by step, beginning
    with the setup:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we install three libraries: `openai`, `langchain_experimental`, and
    `sqlalchemy-databricks`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To create a connection to OpenAI, pass the secret we set up previously and open
    a `ChatOpenAI` connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Figure 8**.22*, we create two different models. The first is the default
    model and the second uses GPT 3.5 Turbo.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.22 – OpenAI API connection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.22 – OpenAI API connection
  prefs: []
  type: TYPE_NORMAL
- en: The setup file does not set your schema variable. Define your schema; we chose
    `favorita_forecasting`. We have been using `database_name` rather than a schema.
    However, we specify the database we want to ask SQL questions against, which is
    different.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.23 – (L) Collecting the table schema and system information schema;
    (R) dropping unnecessary and repetitive columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_22.jpg)![Figure 8.23 – (L) Collecting the table schema and
    system information schema; (R) dropping unnecessary and repetitive columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.23 – (L) Collecting the table schema and system information schema;
    (R) dropping unnecessary and repetitive columns
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create two helper functions. The first function organizes the schema
    information provided, `table_schemas`, creating a table definition. The second
    collects two rows of data as examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.24 – Helper functions for organizing table information'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.24 – Helper functions for organizing table information
  prefs: []
  type: TYPE_NORMAL
- en: Iterate through the table and column data, leveraging our helper functions to
    format the SQL database input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.25 – Iterating through the tables and leveraging the helper functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.25 – Iterating through the tables and leveraging the helper functions
  prefs: []
  type: TYPE_NORMAL
- en: We now have all of our data ready to be able to create a SQL database for OpenAI
    to talk to. You will need to edit `endpoint_http_path` to match the path of an
    active SQL warehouse in your workspace. The database is passed to both the default
    OpenAI model and the GPT 3.5 model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.26 – Create a database for OpenAI to query containing only the information
    we provided'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.26 – Create a database for OpenAI to query containing only the information
    we provided
  prefs: []
  type: TYPE_NORMAL
- en: 'With the setup complete, we can now interact with our SQL chatbot models! Let’s
    start with a basic question: *Which store sold* *the most?*'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 8**.27*, we run both models on the question and get back two different
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.27 – The SQL chatbot model’s responses to our question “Which store
    sold the most?”. (T) db_chain.run(question) (B) chat_chain.run(question)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.27 – The SQL chatbot model’s responses to our question “Which store
    sold the most?”. (T) db_chain.run(question) (B) chat_chain.run(question)
  prefs: []
  type: TYPE_NORMAL
- en: As new versions of OpenAI’s GPT model are released, the results and behavior
    of your SQLbot may change. As new models and approaches become available, it is
    good practice to test them and see how the changes impact your work and the results
    of your chatbot. Leveraging MLflow with your SQLbot experiment will help you track
    and compare the different features and configurations throughout your production
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Project -streaming transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are ready to wrap up this project. The production workflow notebooks are
    the `CH7-08-Production Generating Records`, `CH7-09-Production Auto Loader`, and
    `CH7-10-Production Feature Engineering` components in the workflow job created
    in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325). You will run that same job
    once the new workflow is in place. To follow along in your own workspace, please
    open the following notebook:`CH8-05-Production Monitoring`
  prefs: []
  type: TYPE_NORMAL
- en: In the `CH8-05-Production Monitoring` notebook, you create two monitors – one
    for the `prod_transactions` table and one for the `packaged_transaction_model_predictions`
    table. See *Figure 8**.28* for the latter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.28 – The inference table monitor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.28 – The inference table monitor
  prefs: []
  type: TYPE_NORMAL
- en: 'Congratulations! The streaming project is complete. We encourage you to add
    improvements and commit them back to the repository. Here are a few possible examples:
    add more validation metrics to the validation notebook, incorporate the inference
    performance results into the decision to retrain, and make adjustments to the
    configuration of data generation to simulate drift.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Project: retrieval-augmented generation chatbot'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To follow along in your own workspace, please open the following notebooks
    and resources:'
  prefs: []
  type: TYPE_NORMAL
- en: CH8-`app.py`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH8-01-Deploy Your Endpoint` `with SDK`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Hugging Face** **Spaces** page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we need to ensure our chatbot is deployed using Model Serving, as shown
    in *Figure 8**.30*. Here, we are using the fastest way by going through the UI
    of the Model Serving page. To follow along and serve, we are selecting the registered
    model that we registered in [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325). Select
    the latest version – in our case, it’s version 4\. For this demo project, we expect
    minimal concurrency so the endpoint will have only four possible concurrent runs
    and will scale to 0 when no traffic is available. We are enabling inference tables
    under the same catalog to track and potentially further monitor our payload. We
    are not going to demonstrate in this chapter how to set a monitor or data quality
    pipeline for the RAG project, as it was demonstrated for the streaming project.
    We encourage you to apply it on your own!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.29 – Example of the Model Serving deployment via UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.29 – Example of the Model Serving deployment via UI
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for your application to be able to connect to the resources attached
    to it, such as Vector Search, the endpoint requires you to provide additional
    configurations to the endpoint such as your PAT and host under **Advanced configuration**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.30 – Advanced configuration requirements'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.30 – Advanced configuration requirements
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You could also use the Databricks SDK service to deploy your endpoints. If you
    are interested in seeing how to use SDK deployment, please use the notebook attached
    under `CH8 - 01 -Deploy Your Endpoint` `with SDK`.
  prefs: []
  type: TYPE_NORMAL
- en: Jump over to the Hugging Face Spaces website. Instructions on how to deploy
    your first HF Space are explained very well on the main page of HF Spaces, so
    we will not duplicate them here. We would like to highlight that we are using
    a free deployment option of Spaces with 2 CPUs and 16 GB of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you deploy your Space, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.31 – Empty Hugging Face Space'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_31.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.31 – Empty Hugging Face Space
  prefs: []
  type: TYPE_NORMAL
- en: 'We would like to highlight a few things that are important in order to connect
    to your chatbot, which is served in real time with Databricks Model Serving. To
    connect the chatbot to your HF Space, you must set `API_TOKEN` and an `API_ENDPOINT`.
    Here’s how to set these values:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to **Settings** in the HF Space you created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down to **Variables** **and secrets**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set your API_ENDPOINT as the URL from the REST API provided on the Databricks
    Model Serving page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set your API_TOKEN using a personal access token generated by Databricks. This
    is required to connect to the endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.32 – Example of Variables and secrets on HF Spaces'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_32.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.32 – Example of Variables and secrets on HF Spaces
  prefs: []
  type: TYPE_NORMAL
- en: Once this is set, you are ready to bring your Gradio web app script into your
    HF Space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.33 – Example of Variables and secrets on HF Spaces'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.33 – Example of Variables and secrets on HF Spaces
  prefs: []
  type: TYPE_NORMAL
- en: When your endpoint is ready, jump back to your HF Space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **Files** tab under the pre-created Space and click **+****Add File**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now add `CH8-app.py` that was given to you for [*Chapter 8*](B16865_08.xhtml#_idTextAnchor384)
    – you can create your own web application. Feel free to experiment with the design
    according to your business needs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s talk a bit about the `respond` function in the `CH8-app.py` file – see
    *Figure 8**.34*, which is passed to our app’s UI chatbot. The `respond` function,
    in this case, is the caller of your deployed endpoints, where we do not just send
    and receive responses but also can shape the format of the input or output. In
    our case, the endpoint expects to receive a request in the format of a JSON with
    field inputs with the questions within a list, while the output is a JSON with
    field predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.34 – The respond function written in the Gradio app'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.34 – The respond function written in the Gradio app
  prefs: []
  type: TYPE_NORMAL
- en: To create a chatbot, as was mentioned in the introduction section, we are using
    a simple example from Gradio where we add options such as the title of our application,
    a description, and example questions. *Figure 8**.35* shows the full code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.35 – The Gadio app.py interface for your LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_35.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.35 – The Gadio app.py interface for your LLM
  prefs: []
  type: TYPE_NORMAL
- en: The chatbot is now available in a more user-friendly interface, as shown in
    *Figure 8**.36*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.36 – The interface for your chatbot application'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_36.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.36 – The interface for your chatbot application
  prefs: []
  type: TYPE_NORMAL
- en: Let’s ask a few questions to make sure our RAG chatbot is providing correct
    results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.37 – Examples of chatbot answers from our RAG application'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_08_37.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.37 – Examples of chatbot answers from our RAG application
  prefs: []
  type: TYPE_NORMAL
- en: If the responses look good, your application is ready to be used!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sharing insights from your models is an important way to get value from your
    machine learning practice. Using dashboards as a medium for sharing your information
    is an accessible way to communicate insights to business users and teams outside
    of the data science team. In this chapter, we discussed how to build the gold
    layer, tips on presenting your data using DBSQL dashboards, taking advantage of
    innovations such as OpenAI models in DBSQL, and how you can share data and AI
    artifacts through Databricks Marketplace to get the most value from your enterprise
    data.
  prefs: []
  type: TYPE_NORMAL
- en: We hope you have had a chance to get hands-on with building your lakehouse.
    From exploration, cleaning, building pipelines, and building models to finding
    insights hidden in your data, all the way to sharing insights – it’s all doable
    on the Databricks Platform. We encourage you to take the notebooks and experiment!
    The authors would love to hear feedback, and whether this has been helpful on
    your journey with the Databricks Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s test ourselves on what we’ve learned by going through the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What are some differences between the gold layer and the silver layer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is one way you could set an alert to identify that a table has an invalid
    value?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would you choose to use an external dashboarding tool?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you use a language model through an API such as OpenAI, what are some considerations
    about the data you send via the API?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some reasons a company would share data in Databricks Marketplace?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After putting thought into the questions, compare your answers to ours:'
  prefs: []
  type: TYPE_NORMAL
- en: The gold layer is more refined and aggregated than the silver layer. The silver
    layer powers data science and machine learning, and the gold layer powers analytics
    and dashboarding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can monitor the values of a field and send an email alert when the value
    is invalid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sometimes companies use multiple dashboarding tools. You might need to provide
    data in a dashboard a team is accustomed to using.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If I were a language model through an API, I would be cautious about sending
    sensitive data, including PII, customer information, or proprietary information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A company might share data in Databricks Marketplace in order to monetize the
    data or make it available externally for people to use easily and securely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we pointed out specific technologies, technical features,
    and options. Please look at these resources to get deeper into the areas that
    interest you most:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Databricks SQL Statement Execution* *API*: [https://www.databricks.com/blog/2023/03/07/databricks-sql-statement-execution-api-announcing-public-preview.html](https://www.databricks.com/blog/2023/03/07/databricks-sql-statement-execution-api-announcing-public-preview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Power to the SQL People: Introducing Python UDFs in Databricks* *SQL*: [https://www.databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html](https://www.databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Actioning Customer Reviews at Scale with Databricks SQL AI* *Functions*: [https://www.databricks.com/blog/actioning-customer-reviews-scale-databricks-sql-ai-functions](https://www.databricks.com/blog/actioning-customer-reviews-scale-databricks-sql-ai-functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks sets the official data warehousing performance* *record*: https://dbricks.co/benchmark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Lakehouse and Data* *Mesh*: [https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html](https://www.databricks.com/blog/databricks-lakehouse-and-data-mesh-part-1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hugging* *Face*: [https://huggingface.co/spaces](https://huggingface.co/spaces)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gradio*: [https://www.gradio.app/](https://www.gradio.app/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hugging Face* *Spaces*: [https://huggingface.co/docs/hub/en/spaces-overview](https://huggingface.co/docs/hub/en/spaces-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Lakehouse monitoring* *documentation*: [https://api-docs.databricks.com/python/lakehouse-monitoring/latest/databricks.lakehouse_monitoring.html#module-databricks.lakehouse_monitoring](https://api-docs.databricks.com/python/lakehouse-monitoring/latest/databricks.lakehouse_monitoring.html#module-databricks.lakehouse_monitoring
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks personal access token authentication [https://docs.databricks.com/en/dev-tools/auth/pat.html](https://docs.databricks.com/en/dev-tools/auth/pat.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
