<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer149" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-197"><a id="_idTextAnchor259" class="calibre6 pcalibre pcalibre1"/>10</h1>
<h1 id="_idParaDest-198" class="calibre5"><a id="_idTextAnchor260" class="calibre6 pcalibre pcalibre1"/>Deploying, Monitoring, and Scaling in Production</h1>
<p class="calibre3">Some people may read this book from beginning to end to gain an overall understanding of as many concepts as possible in the realm of AI/ML on Google Cloud, while others may use it as a reference, whereby they pick it up and read certain chapters on specific topics whenever they need to work with those topics as part of a project or client engagement. If you’ve been reading this book from the beginning, then you have come a long way, and we have journeyed together through the majority of the <strong class="bold">ML model development life cycle</strong> (<strong class="bold">MDLC</strong>). While <a id="_idIndexMarker1106" class="calibre6 pcalibre pcalibre1"/>model training is what often gets the most attention in the press – and that is where a lot of the magic happens – you know by now that training is just one piece of the overall <span>life cycle.</span></p>
<p class="calibre3">When we’ve trained and tested our models, and we believe they’re ready to be exposed to our clients, we need to find a way to host them so that they can be used accordingly. In this chapter, we will dive into that part of the process in more detail, including some of the challenges that exist when it comes to hosting and managing models and monitoring them on an ongoing basis to ensure that they stay relevant and perform optimally in perpetuity. We’ll begin by discussing how we can host <span>our models.</span></p>
<p class="calibre3">This chapter covers the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">How do I make my models available to <span>my applications?</span></li>
<li class="calibre8">Fundamental concepts for <span>serving models</span></li>
<li class="calibre8"><span>A/B testing</span></li>
<li class="calibre8">Common challenges of serving models <span>in production</span></li>
<li class="calibre8">Monitoring models <span>in production</span></li>
<li class="calibre8">Optimizing for AI/ML at <span>the edge</span></li>
</ul>
<h1 id="_idParaDest-199" class="calibre5"><a id="_idTextAnchor261" class="calibre6 pcalibre pcalibre1"/>How do I make my models available to my applications?</h1>
<p class="calibre3">We introduced <a id="_idIndexMarker1107" class="calibre6 pcalibre pcalibre1"/>this concept in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, and we talked about the various things you would need to do to host a model on your own, such as setting up all of the required infrastructure, including load balancers, routers, switches, cables, servers, and storage, among other things, and then managing all of that infrastructure on an ongoing basis. This would require a lot of your time <span>and resources.</span></p>
<p class="calibre3">Luckily, all of that stuff was in the old days, and you no longer need to do any of that. This is because Google Cloud provides the Vertex AI prediction service, which enables you to host models in production within minutes, using infrastructure that is all managed for you <span>by Google.</span></p>
<p class="calibre3">For completeness, I will also mention that if you would like to host your models on Google Cloud without using Vertex, numerous other Google Cloud services can be used for that<a id="_idIndexMarker1108" class="calibre6 pcalibre pcalibre1"/> purpose, such as <strong class="bold">Google Compute Engine</strong> (<strong class="bold">GCE</strong>), <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), <strong class="bold">Google App Engine</strong> (<strong class="bold">GAE</strong>), Cloud Run, and <a id="_idIndexMarker1109" class="calibre6 pcalibre pcalibre1"/>Cloud <a id="_idIndexMarker1110" class="calibre6 pcalibre pcalibre1"/>Functions. We described all of these services, as well as some pointers on how to choose between them, in <a href="B18143_03.xhtml#_idTextAnchor059" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 3</em></span></a><span>.</span></p>
<p class="calibre3">Remember that choosing the right platform to host your ML models depends on your specific use case and requirements. Factors such as scalability, latency, costs, development effort, and operational management all play a role in choosing the best solution. You may also make certain decisions based on the framework you’re using to build your ML models. For example, you might want to use TensorFlow Serving if you’re building models in TensorFlow, or TorchServe if you’re building models <span>in PyTorch.</span></p>
<p class="calibre3">In most cases, my recommendation would be to start with a service that is dedicated and optimized for the task at hand, and in the case of building and hosting ML models, Vertex AI is that service. In this chapter, we will deploy our first model using Vertex AI, but before we dive into the hands-on activities, we’ll introduce some <span>important concepts.</span></p>
<h1 id="_idParaDest-200" class="calibre5"><a id="_idTextAnchor262" class="calibre6 pcalibre pcalibre1"/>Fundamental concepts for serving models</h1>
<p class="calibre3">In this section, we<a id="_idIndexMarker1111" class="calibre6 pcalibre pcalibre1"/> will introduce some important topics related to how we can host our models so that our clients can interact <span>with them.</span></p>
<h2 id="_idParaDest-201" class="calibre9"><a id="_idTextAnchor263" class="calibre6 pcalibre pcalibre1"/>Online and offline model serving</h2>
<p class="calibre3">In ML, there are <a id="_idIndexMarker1112" class="calibre6 pcalibre pcalibre1"/>generally two options we have for serving predictions from models: <strong class="bold">online</strong> serving (also<a id="_idIndexMarker1113" class="calibre6 pcalibre pcalibre1"/> known<a id="_idIndexMarker1114" class="calibre6 pcalibre pcalibre1"/> as <strong class="bold">real-time</strong> serving) and <strong class="bold">offline</strong> serving (also known<a id="_idIndexMarker1115" class="calibre6 pcalibre pcalibre1"/> as <strong class="bold">batch</strong> serving). The high-level use cases associated with each of these methods are <strong class="bold">online (or real-time) inference</strong> and <strong class="bold">offline (or batch) inference</strong>, respectively. Let’s take a few minutes to introduce<a id="_idIndexMarker1116" class="calibre6 pcalibre pcalibre1"/> these <a id="_idIndexMarker1117" class="calibre6 pcalibre pcalibre1"/>methods and understand their <span>use cases.</span></p>
<h3 class="calibre11">Online/real-time model serving</h3>
<p class="calibre3">As the name suggests, in the <a id="_idIndexMarker1118" class="calibre6 pcalibre pcalibre1"/>case <a id="_idIndexMarker1119" class="calibre6 pcalibre pcalibre1"/>of real-time model serving, the model needs to respond “in real time” to prediction requests, which usually means that a client (perhaps a customer, or some other system) needs to receive an inference response as quickly as possible, and may be waiting synchronously for a response from the model. An example of this would be a fraud detection system for credit card transactions. As you can imagine, credit card companies want to detect possible fraudulent transactions as quickly as possible – ideally during the transaction process, in which case they could prevent the transaction from processing completely, if possible. It would not be as useful for them to check for fraudulent transactions at some arbitrary <span>point later.</span></p>
<p class="calibre3">Considering that online inference requests usually require a response to be returned as quickly as possible, factors such as ensuring low latency, handling high request volumes, and providing a reliable, always-available service are some of the main challenges in <span>this area.</span></p>
<p class="calibre3">In this case, when we refer to low latency, we mean that the prediction response time is usually in the order of milliseconds or, at the very most, seconds. The actual response time requirements will depend on the business use case. For example, some users may accept needing to wait for a few seconds for their credit card transaction to be approved or rejected, but another example of real-time inference is the use of ML models in self-driving cars, and in that scenario, for example, a car must be able to respond to its environment in milliseconds if it needs to suddenly take some kind of action, such as avoiding an unexpected obstacle that comes into its path. <span><em class="italic">Figure 10</em></span><em class="italic">.1</em> shows an example of a batch prediction workflow that we will implement in the practical exercises in this chapter. We will explain each of the components that are depicted in detail. In <span><em class="italic">Figure 10</em></span><em class="italic">.1</em>, the solid lines represent steps we will explicitly perform in the practical exercises, whereas the dotted lines represent the steps that Vertex AI will perform<a id="_idIndexMarker1120" class="calibre6 pcalibre pcalibre1"/> automatically <a id="_idIndexMarker1121" class="calibre6 pcalibre pcalibre1"/>on <span>our behalf:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer143">
<img alt="Figure 10.1: Online prediction" src="image/B18143_10_1.jpg" class="calibre141"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.1: Online prediction</p>
<p class="calibre3">The steps outlined in <span><em class="italic">Figure 10</em></span><em class="italic">.1</em> are <span>as follows:</span></p>
<ol class="calibre7">
<li class="calibre8">Train the model in <span>our notebook.</span></li>
<li class="calibre8">Save the resulting model artifacts to Google <span>Cloud Storage.</span></li>
<li class="calibre8">Register the model details in the Vertex AI Model Registry (explained in more detail in <span>this chapter).</span></li>
<li class="calibre8">Create an endpoint to host and serve <span>our model.</span></li>
<li class="calibre8">The Vertex AI Online Prediction service (explained in more detail in this chapter) fetches our model’s details from the Vertex AI <span>Model Registry.</span></li>
<li class="calibre8">The Vertex AI Online Prediction service fetches our saved model from Google <span>Cloud Storage.</span></li>
<li class="calibre8">We send a prediction request to our model and receive a response. In this case, we are sending the request from our Vertex AI Workbench notebook, but it’s important to note that when our model is hosted on an endpoint, prediction requests could be sent from any client application that can access <span>that endpoint.</span></li>
<li class="calibre8">The Vertex AI Online Prediction service saves the prediction inputs, outputs, and other details to Google Cloud BigQuery. This is an optional feature that we can enable so that we can perform analytical queries on the inputs, outputs, and other details related to our <span>model’s predictions.</span></li>
</ol>
<p class="calibre3">It should also be noted that in the case of online model serving, predictions are generally made on<a id="_idIndexMarker1122" class="calibre6 pcalibre pcalibre1"/> demand, and usually<a id="_idIndexMarker1123" class="calibre6 pcalibre pcalibre1"/> for a single instance or a small batch of instances. In this case, your model and its serving infrastructure need to be able to quickly react to sudden – and possibly unexpected – changes in inference <span>traffic volume.</span></p>
<h3 class="calibre11">Offline/batch model serving</h3>
<p class="calibre3">Given our<a id="_idIndexMarker1124" class="calibre6 pcalibre pcalibre1"/> description of online model <a id="_idIndexMarker1125" class="calibre6 pcalibre pcalibre1"/>serving, it may have become obvious that offline serving means that no client is waiting for an immediate response in real time. In fact, rather than our model receiving on-demand inference requests from individual clients, we can feed many input observations into our model in large batches, which it can process over longer periods; perhaps hours or even days, depending on the business case. The predictions produced by our models can then be stored and used later, rather than being acted on immediately. Examples of batch inference use cases include predicting the next day’s stock prices or sending out targeted emails to users based on their predicted preferences. <span><em class="italic">Figure 10</em></span><em class="italic">.2</em> shows an example of a batch prediction workflow that we will implement in the practical exercises in this chapter. In <span><em class="italic">Figure 10</em></span><em class="italic">.2</em>, the solid lines represent steps we will explicitly perform in the practical exercises, whereas the dotted lines represent the steps that Vertex AI will perform automatically on <span>our behalf:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer144">
<img alt="Figure 10.2: Batch prediction" src="image/B18143_10_2.jpg" class="calibre142"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.2: Batch prediction</p>
<p class="calibre3">The steps <a id="_idIndexMarker1126" class="calibre6 pcalibre pcalibre1"/>outlined in <span><em class="italic">Figure 10</em></span><em class="italic">.2</em> are <span>as </span><span><a id="_idIndexMarker1127" class="calibre6 pcalibre pcalibre1"/></span><span>follows:</span></p>
<ol class="calibre7">
<li class="calibre8">Train the model in <span>our notebook.</span></li>
<li class="calibre8">Save the resulting model artifacts to Google <span>Cloud Storage.</span></li>
<li class="calibre8">Register the model details in the Vertex AI <span>Model Registry.</span></li>
<li class="calibre8">Save the test data in Google Cloud Storage. This will be used as the input data in our batch prediction <span>job later.</span></li>
<li class="calibre8">Create a batch <span>prediction job.</span></li>
<li class="calibre8">The Vertex AI Batch Prediction service (explained in more detail in this chapter) fetches our model’s details from the Vertex AI <span>Model Registry.</span></li>
<li class="calibre8">The Vertex AI Batch Prediction service fetches our saved model from Google <span>Cloud Storage.</span></li>
<li class="calibre8">The Vertex AI Batch Prediction service fetches our input data from Google <span>Cloud Storage.</span></li>
<li class="calibre8">The Vertex AI Batch Prediction runs a batch prediction job, using our model and <span>input data.</span></li>
<li class="calibre8">The Vertex AI Batch Prediction service saves the prediction outputs to Google <span>Cloud Storage.</span></li>
</ol>
<p class="calibre3">Rather than being optimized for low latency, batch prediction systems are generally optimized to handle a large number of instances at once (that is, high throughput), and therefore usually fall into the category of large-scale distributed computing use cases, which can benefit<a id="_idIndexMarker1128" class="calibre6 pcalibre pcalibre1"/> from parallelized <a id="_idIndexMarker1129" class="calibre6 pcalibre pcalibre1"/>execution, and can be scheduled to automatically execute periodically (for example, once a day), or can be triggered by an event (for example, when a new batch of data <span>is available).</span></p>
<p class="calibre3">It’s important to note that the decision between online and offline serving is not always strictly binary, and you may find that a combination of online and offline serving best meets your needs. For example, you may use offline serving to generate large-scale reports, while also using online serving to make real-time predictions in user-facing applications. In either case, both online and offline serving require the model to be deployed in a serving infrastructure. This infrastructure is responsible for loading the model, receiving prediction requests, making predictions using the model, and returning the predictions. In Google Cloud, there are various tools and platforms available to assist with this, such as TensorFlow Serving, and the Vertex AI prediction service, which we will discuss in more detail in subsequent sections in this chapter. First, however, let’s introduce another <a id="_idIndexMarker1130" class="calibre6 pcalibre pcalibre1"/>tool that is <a id="_idIndexMarker1131" class="calibre6 pcalibre pcalibre1"/>important for managing our models as part of our end-to-end model development <span>life cycle.</span></p>
<h2 id="_idParaDest-202" class="calibre9"><a id="_idTextAnchor264" class="calibre6 pcalibre pcalibre1"/>Vertex AI Model Registry</h2>
<p class="calibre3">Earlier in this book, we <a id="_idIndexMarker1132" class="calibre6 pcalibre pcalibre1"/>made an analogy <a id="_idIndexMarker1133" class="calibre6 pcalibre pcalibre1"/>between the traditional <strong class="bold">software development life cycle</strong> (<strong class="bold">SDLC</strong>) and<a id="_idIndexMarker1134" class="calibre6 pcalibre pcalibre1"/> the MDLC. To dive into this analogy in more depth, let’s consider some important tools in the <span>SDLC process:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Shared repositories</strong>: As a part<a id="_idIndexMarker1135" class="calibre6 pcalibre pcalibre1"/> of the SDLC process, we usually want to store our code and related artifacts in registries or repositories that can be accessed by multiple contributors who need to collaborate on a specific development project. Such repositories often include metadata that helps describe certain aspects of the code assets so that people can easily understand how those assets were developed, as well as how they <span>are used.</span></li>
<li class="calibre8"><strong class="bold">Version control</strong>: As contributors make changes to code assets in a given project, we want to ensure that we are tracking such changes and contributions and that all contributors can easily access that information. This also enables us to roll back to previous versions if we notice issues in a newly deployed version of <span>our software.</span></li>
</ul>
<p class="calibre3">Experience has taught us that similar tools are required when we want to efficiently deploy and manage ML models, especially when doing so at a large scale (remember that some companies may have thousands of ML models, owned by hundreds of <span>different teams).</span></p>
<p class="calibre3">Also, bear in mind that data science projects are often highly experimental, in which the data science teams may try out lots of different algorithms, datasets, and hyperparameter values, training lots of different models to see which options produce the best results. This is especially true in the early stages of a data science project, but this also often holds true on an ongoing basis, whereby the data scientists constantly strive to improve the models even after they have been deployed to a production environment. They do this to keep abreast of emerging trends in the industry and produce <span>better results.</span></p>
<p class="calibre3">Google Cloud’s Vertex AI platform includes a Model Registry service that allows us to manage our ML models in a centralized place, making it much easier for us to track how models are developed, even when multiple teams are contributing to the development of those models. Let’s take a look at some of the Model Registry’s <span>important features:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Model versioning</strong>: The <a id="_idIndexMarker1136" class="calibre6 pcalibre pcalibre1"/>Model Registry allows us to create multiple versions of a model, where each version can correspond to a different set of training parameters or a different set of training data. This helps us keep track of different experiments <span>or deployments.</span></li>
<li class="calibre8"><strong class="bold">Model metadata</strong>: For each model, we can record metadata such as the model’s description, the input and output schemas, the labels (useful for categorization), and the metrics (useful for comparing models). For each version, we can record additional metadata such as the description, the runtime version (corresponding to the version of the Vertex AI platform services), the Python version, the machine type used for serving, and the <span>serving settings.</span></li>
<li class="calibre8"><strong class="bold">Model artifacts</strong>: These are the artifacts that are used to produce the model. They can be stored in Google Cloud Storage and linked to the model in <span>the registry.</span></li>
<li class="calibre8"><strong class="bold">Access control</strong>: We can control who can view, edit, and deploy models in the registry through Google <a id="_idIndexMarker1137" class="calibre6 pcalibre pcalibre1"/>Cloud’s <strong class="bold">Identity and Access Management</strong> (<span><strong class="bold">IAM</strong></span><span>) system.</span></li>
</ul>
<p class="calibre3">It’s also important to understand that the Model Registry is well-integrated with other Vertex AI components. For example, we can use the Vertex AI training service to train a model and then automatically upload the trained model to the registry, after which we can deploy the<a id="_idIndexMarker1138" class="calibre6 pcalibre pcalibre1"/> model to the Google Cloud <a id="_idIndexMarker1139" class="calibre6 pcalibre pcalibre1"/>Vertex AI prediction service, which we’ll describe next. We can compare model versions against each other and easily change which versions are deployed to production. We can also automate all of those steps using Vertex AI Pipelines, something we’ll explore in the <span>next chapter.</span></p>
<h2 id="_idParaDest-203" class="calibre9"><a id="_idTextAnchor265" class="calibre6 pcalibre pcalibre1"/>Vertex AI prediction service</h2>
<p class="calibre3">The Google Cloud <a id="_idIndexMarker1140" class="calibre6 pcalibre pcalibre1"/>Vertex AI prediction<a id="_idIndexMarker1141" class="calibre6 pcalibre pcalibre1"/> service is an offering within the Vertex AI ecosystem that makes it easy for us to host ML models and serve them to our clients, thus supporting both batch and online model-serving use cases. It’s a managed service, so when we use it to host our models, we don’t need to worry about managing the servers and infrastructure required to do so; the service will automatically scale the required infrastructure and computing resources up and down based on the amount of traffic being sent to <span>our models.</span></p>
<p class="calibre3">As we mentioned in the previous section, it integrates with Vertex AI Model Registry so that we can easily control which versions of our models are deployed to production, and it also integrates with many other Google Cloud services, such as Vertex AI Pipelines, which allows us to automate the development and deployment of our models, and Google Cloud Operations Suite, which provides integrated logging and <span>monitoring functionality.</span></p>
<p class="calibre3">We will perform hands-on activities in which we will use the Vertex AI Model Registry and the Vertex AI prediction service to store and serve models in Google Cloud shortly, but first, let’s cover one more concept that’s important in the context of model deployment and management: <span><strong class="bold">A/B testing</strong></span><span>.</span></p>
<h1 id="_idParaDest-204" class="calibre5"><a id="_idTextAnchor266" class="calibre6 pcalibre pcalibre1"/>A/B testing</h1>
<p class="calibre3">A/B testing is the<a id="_idIndexMarker1142" class="calibre6 pcalibre pcalibre1"/> practice of testing one model (model A) against another model (model B) to see which one performs better. While the term could technically apply to testing and comparing any models, the usual scenario is to test a new version of a model to improve model performance concerning the <span>business objective.</span></p>
<p class="calibre3">Vertex AI allows us to deploy more than one model to a single endpoint, as well as control the amount of traffic that is served by each model by using the <strong class="source-inline">traffic_split</strong> variable, as shown in <span><em class="italic">Figure 10</em></span><span><em class="italic">.3</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer145">
<img alt="Figure 10.3: A/B configuration using Vertex AI’s traffic_split" src="image/B18143_10_3.jpg" class="calibre143"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.3: A/B configuration using Vertex AI’s traffic_split</p>
<p class="calibre3">As you will see in the practical exercises in this chapter, if we don’t set any value for the <strong class="source-inline">traffic_split</strong> variable, the default behavior is to keep all traffic directed to the original model that was already deployed to our endpoint. This is a safety mechanism that prevents unexpected behavior in terms of how our models serve traffic from our clients. The <strong class="source-inline">traffic_split</strong> configuration gives us very granular control over how much traffic we want to send to each deployed model or model version. For example, we could set the <strong class="source-inline">traffic_split</strong> configuration so that it suddenly starts sending all traffic to our new model by allocating 100% of the traffic to that model, which would effectively perform an<a id="_idIndexMarker1143" class="calibre6 pcalibre pcalibre1"/> in-place replacement of our model. However, we may want to test our new model version with a small subset of our production traffic before completely replacing our prior model version, which equates to the idea of <strong class="bold">canary testing</strong> in <a id="_idIndexMarker1144" class="calibre6 pcalibre pcalibre1"/><span>software development.</span></p>
<p class="calibre3">When we determine that our new model is behaving as intended, we can gradually (or suddenly, depending on the business requirements) change the <strong class="source-inline">traffic_split</strong> variable to send more (or all) traffic to the <span>new model.</span></p>
<p class="calibre3">Now that we’ve covered many of the important concepts related to hosting and serving models in production, let’s see how this works in the real world by deploying a model using <span>Vertex AI.</span></p>
<p class="calibre3">We’ve prepared a Vertex AI Workbench notebook that will walk you through all of the steps required to do this. Again, we can use the same Vertex AI Workbench managed notebook instance that we created in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a> for this purpose. Please open JupyterLab on that notebook instance. In the directory explorer on the left-hand side of the screen, navigate to the <strong class="source-inline">Chapter-10</strong> directory and open the <strong class="source-inline">deployment-prediction.ipynb</strong> notebook. You can choose TensorFlow 2 (Local) as the kernel. Again, you can run each cell in the notebook by selecting the cell and pressing <em class="italic">Shift</em> + <em class="italic">Enter</em> on your keyboard. In addition to the relevant code, the notebook contains markdown text that describes what the code is doing. I recommend only executing the model training and deployment sections, and A/B testing, and then reading through some more of the topics in this chapter before proceeding to the other activities in <span>the notebook.</span></p>
<p class="calibre3">We must also enable a feature named <strong class="source-inline">prediction-request-response-logging</strong> in the notebook, which will log our models’ responses for the prediction requests that are received. We can save those responses in a Google Cloud BigQuery table, which enables us to perform analysis on the prediction responses from each of our models and see how they <span>are performing.</span></p>
<p class="calibre3">Once you have completed the model training and deployment sections of the notebook (or if you’d like to just keep reading for now), you can move on to the next section, where we will discuss<a id="_idIndexMarker1145" class="calibre6 pcalibre pcalibre1"/> the kinds of challenges that companies usually run into when deploying, serving, and managing models <span>in production.</span></p>
<h1 id="_idParaDest-205" class="calibre5"><a id="_idTextAnchor267" class="calibre6 pcalibre pcalibre1"/>Common challenges of serving models in production</h1>
<p class="calibre3">Deploying and hosting<a id="_idIndexMarker1146" class="calibre6 pcalibre pcalibre1"/> ML models in production often comes with numerous challenges. If you’re developing and serving just one model, you may encounter some of these challenges, but if you are developing tens, hundreds, or thousands of models, then you will likely run into the majority of these challenges <span>and concerns.</span></p>
<h2 id="_idParaDest-206" class="calibre9"><a id="_idTextAnchor268" class="calibre6 pcalibre pcalibre1"/>Deployment infrastructure</h2>
<p class="calibre3">Choosing the right<a id="_idIndexMarker1147" class="calibre6 pcalibre pcalibre1"/> infrastructure to host ML models, setting it up, and managing it can be complex, particularly in hybrid or multi-cloud environments. Again, Google Cloud Vertex AI takes care of all of this for us automatically, but without such cloud offerings, many companies find this to be perhaps one of the most challenging aspects of any data <span>science project.</span></p>
<h2 id="_idParaDest-207" class="calibre9"><a id="_idTextAnchor269" class="calibre6 pcalibre pcalibre1"/>Model availability and scaling in production</h2>
<p class="calibre3">This is an extension of <a id="_idIndexMarker1148" class="calibre6 pcalibre pcalibre1"/>deployment infrastructure management. As demand increases, our model needs to serve more predictions. The ability to scale services up and down based on demand is crucial and can be difficult to manage manually. The Vertex AI Autoscaling feature enables us to do this easily. All we have to do is specify the minimum and maximum number of machines that we want to run for <span>each model.</span></p>
<p class="calibre3">For example, if we know that we always need at least three nodes running to handle our regularly expected traffic, but we sometimes get increased levels of traffic up to double the normal amount, we could specify that we want Vertex AI to always run at least three machines, and to automatically scale up to a maximum of six machines <span>when needed.</span></p>
<p class="calibre3">We can specify our autoscaling preferences by configuring the <strong class="source-inline">minReplicaCount</strong> and <strong class="source-inline">maxReplicaCount</strong> variables, which are elements of the machine specification for our deployed model. We’ve provided steps on how to do this in the Jupyter Notebook that is associated with this chapter. Note that we can specify these details per model, not just per endpoint. This enables us to scale each of our models independently, which gives us flexibility, depending on <span>our needs.</span></p>
<p class="calibre3">Vertex AI also gives us the flexibility to either deploy multiple models to the same endpoint or to create separate, dedicated endpoints for each model. If we have completely different types of models for different use cases, then we would typically deploy those models to separate endpoints. At this point, you might be wondering why we would ever want to deploy multiple models to a single endpoint. The most common reason for doing this is when we want to implement A/B testing. Again, we’ve covered the steps for implementing A/B testing use cases in the Jupyter Notebook that accompanies <span>this chapter.</span></p>
<h2 id="_idParaDest-208" class="calibre9"><a id="_idTextAnchor270" class="calibre6 pcalibre pcalibre1"/>Data quality</h2>
<p class="calibre3">Often, the data <a id="_idIndexMarker1149" class="calibre6 pcalibre pcalibre1"/>that’s used in a production environment may contain errors or may not be as clean as the data used for model training, which could lead to inaccurate predictions. As a result, we may need to implement data processing steps in production that prepare and clean up the data “on-the-fly” at prediction time. In general, any data transformation techniques that we may have applied when preparing the data for training also need to be applied at <span>inference time.</span></p>
<h2 id="_idParaDest-209" class="calibre9"><a id="_idTextAnchor271" class="calibre6 pcalibre pcalibre1"/>Model/data/concept drift</h2>
<p class="calibre3">In addition to preparing <a id="_idIndexMarker1150" class="calibre6 pcalibre pcalibre1"/>and cleaning up our data, if you think back to some of the data exploration activities we performed in previous chapters of this book, you may remember that one important aspect of our data is the statistical distribution of the variables in our dataset. Examples include the mean value of each variable in the dataset, the observed range between the minimum and maximum values of each variable, or the kinds of values of each variable, such as discreet or continuous. Collectively, we can refer to these characteristics as the “shape” of our data. <span><em class="italic">Figure 10</em></span><em class="italic">.4</em> shows some examples of different types of <span>data distributions:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer146">
<img alt="Figure 10.4: Data distributions  (source: https://commons.wikimedia.org/wiki/File:Cauchy_pdf.svg)" src="image/B18143_10_4.jpg" class="calibre144"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.4: Data distributions (source: https://commons.wikimedia.org/wiki/File:Cauchy_pdf.svg)</p>
<p class="calibre3">In an ideal world, the shape of the data that is used to train our model should be the same as the shape of the data that the model is expected to encounter in production. For this reason, we usually want to use real-world data (that is, data that was previously observed in production) to train our models. However, over time, the shape of the data that’s observed in production might begin to deviate from the shape of the data that was used to train the model. This can happen suddenly, such as when a global pandemic drastically changes the purchasing behavior of consumers within a short period (for example, everybody is suddenly purchasing toilet paper and hand sanitizer, rather than fashion and cosmetic products), or seasonally, or it could occur more gradually due to natural evolutionary changes in a given business domain <span>or market.</span></p>
<p class="calibre3">In any case, such a deviation is generally referred to as “drift,” or more specifically <strong class="bold">model drift</strong>, <strong class="bold">data drift</strong>, or <strong class="bold">concept drift</strong>. Model drift refers to the overall degradation <a id="_idIndexMarker1151" class="calibre6 pcalibre pcalibre1"/>of<a id="_idIndexMarker1152" class="calibre6 pcalibre pcalibre1"/> the <a id="_idIndexMarker1153" class="calibre6 pcalibre pcalibre1"/>model’s objective performance over time, and it can be caused by factors such as data drift or concept drift. Data drift is when the statistical distribution of the data in production differs from the statistical distribution of the data that was used to train our model. On the other hand, concept drift is when the relationships change between the input features and the target variable that a model is trying to predict. This means that even if the input data stays the same, the underlying relationships between the variables may change over time, which can affect the accuracy of our model. The example of consumer behavior changing during a pandemic is an instance of <span>concept drift.</span></p>
<p class="calibre3">Regardless of the type of drift, it can lead to degrading model performance. As such, we need to constantly evaluate the performance of our models to ensure they consistently meet our business <a id="_idIndexMarker1154" class="calibre6 pcalibre pcalibre1"/>requirements. If we find that the performance of our models is degrading, we need to assess whether it is due to drift, and if we detect that it is, we need to take corrective measures accordingly. We will discuss such measures in more <span>detail later.</span></p>
<h2 id="_idParaDest-210" class="calibre9"><a id="_idTextAnchor272" class="calibre6 pcalibre pcalibre1"/>Security and privacy</h2>
<p class="calibre3">Ensuring that <a id="_idIndexMarker1155" class="calibre6 pcalibre pcalibre1"/>the data used by our model complies with security and privacy regulations can be complex, particularly in industries dealing with sensitive data. Depending on the industry and use case, this can be considered one of the most important aspects of model development <span>and management.</span></p>
<h2 id="_idParaDest-211" class="calibre9"><a id="_idTextAnchor273" class="calibre6 pcalibre pcalibre1"/>Model interpretability</h2>
<p class="calibre3">Often, models are treated<a id="_idIndexMarker1156" class="calibre6 pcalibre pcalibre1"/> as “black box” implementations, in which case we don’t have much visibility into how the model is operating internally, making it difficult to understand how they’re making predictions. This can cause issues, especially in regulated industries where explanations are required. We will explore this topic in a lot more detail later in <span>this book.</span></p>
<h2 id="_idParaDest-212" class="calibre9"><a id="_idTextAnchor274" class="calibre6 pcalibre pcalibre1"/>Tracking ML model metadata</h2>
<p class="calibre3">As we mentioned <a id="_idIndexMarker1157" class="calibre6 pcalibre pcalibre1"/>earlier, ML model development usually involves some experimentation, in which data scientists evaluate different versions of their datasets, as well as different algorithms, hyperparameter values, and other components of the development process. Also, the development process often involves collaboration among multiple team members or multiple separate teams. Considering that some companies develop and deploy hundreds or even thousands of ML models, it’s important to track all of these experiments and <span>development iterations.</span></p>
<p class="calibre3">For example, if we have a model deployed in production, and we observe that the model is making inaccurate or inappropriate predictions, then we need to understand why that model is behaving as it is. To do that, we need to know all of the steps that were performed to create that specific model version, as well as all of the inputs and outputs associated with the development of that model, such as the version of the input dataset that was used to train the model, as well as the algorithm and hyperparameter values used during the training process. We refer to this information as the metadata of the model. Without this, it would be very difficult to understand why our model behaves the way <span>it does.</span></p>
<p class="calibre3">This concept is somewhat linked to the topic of model interpretability but also includes additional aspects of ML <span>model development.</span></p>
<h2 id="_idParaDest-213" class="calibre9"><a id="_idTextAnchor275" class="calibre6 pcalibre pcalibre1"/>Integration with existing systems</h2>
<p class="calibre3">Most companies <a id="_idIndexMarker1158" class="calibre6 pcalibre pcalibre1"/>have various software systems that were developed or procured over many years, and integrating the ML model into these systems can <span>be complex.</span></p>
<h2 id="_idParaDest-214" class="calibre9"><a id="_idTextAnchor276" class="calibre6 pcalibre pcalibre1"/>Monitoring</h2>
<p class="calibre3">Setting up robust<a id="_idIndexMarker1159" class="calibre6 pcalibre pcalibre1"/> monitoring for the performance of ML models in production can be a challenge if you manage the infrastructure yourself. We need to constantly monitor the model’s predictive performance to ensure it is still valid and hasn’t degraded <span>over time.</span></p>
<p class="calibre3">This is such an important aspect of model management that we will dedicate the next section of this chapter specifically to <span>this topic.</span></p>
<h1 id="_idParaDest-215" class="calibre5"><a id="_idTextAnchor277" class="calibre6 pcalibre pcalibre1"/>Monitoring models in production</h1>
<p class="calibre3">Our work isn’t over<a id="_idIndexMarker1160" class="calibre6 pcalibre pcalibre1"/> once we’ve developed and deployed a model – we need to track the model’s performance and its overall health over time and make adjustments if we observe that the model performance deteriorates. In this section, we’ll discuss some of the characteristics of our models that we typically need <span>to monitor.</span></p>
<h2 id="_idParaDest-216" class="calibre9"><a id="_idTextAnchor278" class="calibre6 pcalibre pcalibre1"/>Objective model performance</h2>
<p class="calibre3">Not surprisingly, the <a id="_idIndexMarker1161" class="calibre6 pcalibre pcalibre1"/>most prominent aspect of our model that we need to monitor is how it performs concerning the objective it was created to achieve. We discussed objective metrics in<a id="_idIndexMarker1162" class="calibre6 pcalibre pcalibre1"/> previous chapters of this book, such as <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>), Accuracy, F1 score, and AUC-ROC, among others. An example of AUC-ROC is depicted in <span><em class="italic">Figure 10</em></span><em class="italic">.5</em> <span>for reference:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer147">
<img alt="Figure 10.5: AUC-ROC" src="image/B18143_10_5.jpg" class="calibre145"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.5: AUC-ROC</p>
<p class="calibre3">Objective metrics tell us how our model is performing in terms of the main purpose our model is intended to serve, such as predicting housing prices or identifying cats in photographs. If we notice a sustained degradation in these metrics beyond a certain threshold that we deem acceptable to the needs of the business, then we usually need to take <span>corrective action.</span></p>
<p class="calibre3">A common cause of degradation of model performance is data drift, which we discussed in the previous section. If we find that data drift has occurred, a corrective action is often to retrain our <a id="_idIndexMarker1163" class="calibre6 pcalibre pcalibre1"/>model with updated data that matches the shape or distribution of the data that is currently being observed by the model in production. This may require gathering fresh data from our production systems or other <span>appropriate sources.</span></p>
<h2 id="_idParaDest-217" class="calibre9"><a id="_idTextAnchor279" class="calibre6 pcalibre pcalibre1"/>Monitoring specifically for data drift</h2>
<p class="calibre3">Monitoring for <a id="_idIndexMarker1164" class="calibre6 pcalibre pcalibre1"/>data drift involves comparing the statistical properties of the incoming data with those of the training data. In this case, we analyze each feature or variable in the data, and we compare the distribution of the incoming data with the distribution of the training data. In addition to comparing simple descriptive statistics such as the mean, median, mode, and standard deviation, there are also some specific statistical tests we can evaluate, such as the Kullback-Leibler divergence, the Kolmogorov-Smirnov test, or the chi-squared test. We will discuss these mechanisms in more detail later in <span>this book.</span></p>
<p class="calibre3">Vertex AI Model Monitoring provides automated tests that can detect different varieties of drift. Specifically, it can <a id="_idIndexMarker1165" class="calibre6 pcalibre pcalibre1"/>check for <strong class="bold">feature skew and drift</strong> and <strong class="bold">attribution skew and drift</strong>, both of <a id="_idIndexMarker1166" class="calibre6 pcalibre pcalibre1"/>which we’ll describe next. It also provides model explanation functionality, which we’ll explore in great detail later in <span>this book.</span></p>
<p class="calibre3">Skew is also<a id="_idIndexMarker1167" class="calibre6 pcalibre pcalibre1"/> called <strong class="bold">training-serving skew</strong>, and it refers to a scenario in which the distribution of feature data in production differs from the distribution of feature data that was used to train the model. To detect this type of skew, we generally need to have access to the original training data since Vertex AI Model Monitoring will compare the distribution of the training data against what is seen in the inference requests that are sent to our model <span>in production.</span></p>
<p class="calibre3"><strong class="bold">Prediction drift</strong> refers<a id="_idIndexMarker1168" class="calibre6 pcalibre pcalibre1"/> to the scenario in which the feature data in production changes over time. In the absence of access to the original training data, we can still turn on drift detection to check the input data for changes <span>over time.</span></p>
<p class="calibre3">It’s important to note that some amount of drift and skew are likely tolerable, but we generally need to<a id="_idIndexMarker1169" class="calibre6 pcalibre pcalibre1"/> determine and configure thresholds beyond which we need to take corrective actions. These thresholds depend on the needs of the business for our particular <span>use case.</span></p>
<h2 id="_idParaDest-218" class="calibre9"><a id="_idTextAnchor280" class="calibre6 pcalibre pcalibre1"/>Anomalous model behavior</h2>
<p class="calibre3">Anomalies could be<a id="_idIndexMarker1170" class="calibre6 pcalibre pcalibre1"/> spikes in prediction requests, unusual values of input features, or unusual prediction response values. For example, if a model that is built for fraud detection starts flagging an unusually high number of transactions as fraudulent, it may be an anomaly and warrant further investigation. Anomalies may also occur due to data drift, or due to temporary changes in <span>the environment.</span></p>
<h2 id="_idParaDest-219" class="calibre9"><a id="_idTextAnchor281" class="calibre6 pcalibre pcalibre1"/>Resource utilization</h2>
<p class="calibre3">This includes<a id="_idIndexMarker1171" class="calibre6 pcalibre pcalibre1"/> monitoring aspects such as CPU usage, memory usage, network I/O, and others to ensure that the model’s serving infrastructure is operating within its capacity. These metrics are important indicators for determining when to scale resources up or down based on factors such as the amount of traffic currently being sent to <span>our models.</span></p>
<h2 id="_idParaDest-220" class="calibre9"><a id="_idTextAnchor282" class="calibre6 pcalibre pcalibre1"/>Model bias and fairness</h2>
<p class="calibre3">We need to ensure <a id="_idIndexMarker1172" class="calibre6 pcalibre pcalibre1"/>that our models are making fair predictions on an ongoing basis. This can involve tracking fairness metrics and checking for bias in our models’ predictions. We will explore this topic in much more detail in the <span>next chapter.</span></p>
<p class="calibre3">These are some of the main aspects of our models that we need to monitor. The exact items that need to be prioritized depend on our <span>business case.</span></p>
<p class="calibre3">Now that we’ve talked about many of the most common topics in the space of model monitoring, let’s discuss what to do if we detect that our model’s performance <span>is degrading.</span></p>
<h2 id="_idParaDest-221" class="calibre9"><a id="_idTextAnchor283" class="calibre6 pcalibre pcalibre1"/>Addressing model performance degradation</h2>
<p class="calibre3">If we find that the <a id="_idIndexMarker1173" class="calibre6 pcalibre pcalibre1"/>values of the metrics we’re monitoring for our models are showing a decline in performance, there are some actions we can take to correct the situation, and possibly prevent such a scenario from occurring in the future. This section discusses some relevant <span>corrective actions.</span></p>
<p class="calibre3">Perhaps the most effective way to ensure that our models stay up to date and are performing optimally is to implement a robust MLOps pipeline. This includes <strong class="bold">continuous integration and continuous deployment</strong> (<strong class="bold">CI/CD</strong>) for ML models, and a major<a id="_idIndexMarker1174" class="calibre6 pcalibre pcalibre1"/> component of this is to regularly train our models on new data that is seen in production – this is referred to <a id="_idIndexMarker1175" class="calibre6 pcalibre pcalibre1"/>as <strong class="bold">continuous training</strong>. For this purpose, we need to implement a mechanism for capturing data in production, and then automatically update <span>our models.</span></p>
<p class="calibre3">We could either update our models periodically (for example, every night or every month), or we could trigger retraining to occur based on outputs from a Vertex AI Model Monitoring job. For example, if a Model Monitoring job detects that drift has occurred beyond an acceptable threshold, an automated notification could be sent, and it could automatically kick off a pipeline to train and deploy a new version of <span>our model.</span></p>
<p class="calibre3">The next chapter is dedicated entirely to the topic of MLOps, so we will dive into these concepts in much more <span>detail there.</span></p>
<p class="calibre3">In the meantime, if you’d like to switch from theory to practical learning, now would be a good time to execute the <em class="italic">Model Monitoring</em> section of the Vertex AI Workbench notebook that<a id="_idIndexMarker1176" class="calibre6 pcalibre pcalibre1"/> accompanies this chapter. Otherwise, let’s continue with the rest of the topics in this chapter, which include optimizing for AI/ML use cases at <span>the edge.</span></p>
<h1 id="_idParaDest-222" class="calibre5"><a id="_idTextAnchor284" class="calibre6 pcalibre pcalibre1"/>Optimizing for AI/ML at the edge</h1>
<p class="calibre3">Serving ML models at the <a id="_idIndexMarker1177" class="calibre6 pcalibre pcalibre1"/>edge refers to running your models directly on user devices such as smartphones or IoT devices. The term “edge” is based on traditional network architecture terminology, in which the core of the network is in the network owner’s data centers, and the edge of the network is where user devices connect to the network. Running models and other types of systems at the edge can provide benefits such as lower latency, increased privacy, and reduced server costs. However, edge devices usually have limited computing power, so we may need to make some changes to our models for them to run efficiently on those devices. There are several things we can do to optimize our models to run at the edge, all of which we will discuss in <span>this section.</span></p>
<h2 id="_idParaDest-223" class="calibre9"><a id="_idTextAnchor285" class="calibre6 pcalibre pcalibre1"/>Model optimization</h2>
<p class="calibre3">Let’s start by <a id="_idIndexMarker1178" class="calibre6 pcalibre pcalibre1"/>discussing what kinds of measures we can take to optimize our models so that they can be used at <span>the edge.</span></p>
<h3 class="calibre11">Model selection</h3>
<p class="calibre3">First, we should try to <a id="_idIndexMarker1179" class="calibre6 pcalibre pcalibre1"/>choose lightweight models that can still provide good performance on our objectives. For example, decision trees and linear models often require less memory and computational power than deep learning models. Of course, our model selection process also depends on our business needs. Sometimes, we will need larger models to achieve the required objective. As such, this recommendation for optimizing AI/ML workloads at the edge is simply a first-step guideline. We will <a id="_idIndexMarker1180" class="calibre6 pcalibre pcalibre1"/>discuss more advanced <span>strategies next.</span></p>
<h3 class="calibre11">Model pruning</h3>
<p class="calibre3">Pruning is a technique for <a id="_idIndexMarker1181" class="calibre6 pcalibre pcalibre1"/>reducing the size of a model by removing parameters (for example, “weight pruning”) or whole neurons (for example, “neuron pruning”) that contribute the least to the model’s performance. An example of neuron pruning is depicted in <span><em class="italic">Figure 10</em></span><em class="italic">.6</em>, in which a neuron has been removed from each hidden layer (as represented by the red X covering each of the <span>removed neurons):</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer148">
<img alt="Figure 10.6: Neuron pruning" src="image/B18143_10_6.jpg" class="calibre146"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.6: Neuron pruning</p>
<p class="calibre3">The resulting pruned model requires less memory and computational resources. If we remove too many weights or neurons, then it could affect the accuracy of our model, so the idea, of course, is to find a balance that reduces the computational resources required, with<a id="_idIndexMarker1182" class="calibre6 pcalibre pcalibre1"/> minimal impact <span>on accuracy.</span></p>
<h3 class="calibre11">Model quantization</h3>
<p class="calibre3">Quantization is a<a id="_idIndexMarker1183" class="calibre6 pcalibre pcalibre1"/> way of reducing the numerical precision of the model’s weights. For example, weights might be stored as 32-bit floating-point numbers during training, but they can often be quantized as 8-bit integers for inference without a significant reduction in performance. This reduces the memory requirements and computational cost of the model. This is particularly useful in the context<a id="_idIndexMarker1184" class="calibre6 pcalibre pcalibre1"/> of <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>), which can have hundreds of billions of weights. We will cover this in detail in the <em class="italic">Generative AI</em> section of <span>this book.</span></p>
<h3 class="calibre11">Knowledge distillation</h3>
<p class="calibre3">This technique involves <a id="_idIndexMarker1185" class="calibre6 pcalibre pcalibre1"/>training a smaller model, sometimes referred to as a “student” model, to mimic the behavior of a larger, “teacher” model, or ensemble of models. The smaller model is trained to produce outputs as similar as possible to those of the larger model so that it can perform a somewhat similar task, with perhaps a tolerable reduction in model accuracy. Again, we need to find a balance in the trade-off between model size reduction and reduction in accuracy. Distillation is also particularly useful in the context <span>of LLMs.</span></p>
<h3 class="calibre11">Making use of efficient model architectures</h3>
<p class="calibre3">Some model architectures<a id="_idIndexMarker1186" class="calibre6 pcalibre pcalibre1"/> are designed to be efficient on edge devices. For example, MobileNet and EfficientNet are efficient variants<a id="_idIndexMarker1187" class="calibre6 pcalibre pcalibre1"/> of <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) that are suitable for <span>mobile devices.</span></p>
<p class="calibre3">Now that we’ve discussed some changes that we can make to our models to optimize them for edge use cases, let’s take a look at what other kinds of mechanisms we can use for <span>this purpose.</span></p>
<h2 id="_idParaDest-224" class="calibre9"><a id="_idTextAnchor286" class="calibre6 pcalibre pcalibre1"/>Optimization beyond model techniques</h2>
<p class="calibre3">All of the previous optimization<a id="_idIndexMarker1188" class="calibre6 pcalibre pcalibre1"/> techniques involved making changes to our models to make them run more efficiently on edge devices. There are also additional measures we can take, such as converting trained models into other formats that are optimized for edge devices or optimizing the hardware that runs<a id="_idIndexMarker1189" class="calibre6 pcalibre pcalibre1"/> our models at the edge. Let’s discuss these mechanisms in a bit <span>more detail.</span></p>
<h3 class="calibre11">Hardware-specific optimization</h3>
<p class="calibre3">Depending on the <a id="_idIndexMarker1190" class="calibre6 pcalibre pcalibre1"/>specific hardware on the edge device (for example, CPU, GPU, <strong class="bold">tensor processing unit</strong> (<strong class="bold">TPU</strong>), and so on), different optimization strategies can be used. For example, some libraries provide tools for optimizing computation graphs based on <span>specific hardware.</span></p>
<h3 class="calibre11">Specialized libraries</h3>
<p class="calibre3">Libraries such <a id="_idIndexMarker1191" class="calibre6 pcalibre pcalibre1"/>as TensorFlow Lite <a id="_idIndexMarker1192" class="calibre6 pcalibre pcalibre1"/>and the <strong class="bold">Open Neural Network Exchange</strong> (<strong class="bold">ONNX</strong>) Runtime can convert models into a format optimized for edge devices, further reducing the memory footprint and increasing the speed of models. We’ll discuss these libraries in more detail in <span>this section.</span></p>
<h4 class="calibre20">TensorFlow Lite</h4>
<p class="calibre3">TensorFlow Lite is a <a id="_idIndexMarker1193" class="calibre6 pcalibre pcalibre1"/>set of tools provided by TensorFlow to help us run models on mobile, embedded, and IoT devices. It does this by converting our TensorFlow models into a more efficient format for use on such edge devices. It also includes tools for optimizing the model’s size and performance, as well as for implementing hardware acceleration. We’ve used TensorFlow Lite to convert our model; this can be found in the Jupyter Notebook that accompanies <span>this chapter.</span></p>
<h4 class="calibre20">Edge TPU Compiler</h4>
<p class="calibre3">Google’s Edge TPU<a id="_idIndexMarker1194" class="calibre6 pcalibre pcalibre1"/> Compiler is a tool for compiling models to run on Google’s Edge TPUs, which are designed for running TensorFlow Lite models on <span>edge devices.</span></p>
<h4 class="calibre20">ONNX Runtime</h4>
<p class="calibre3">ONNX is an open<a id="_idIndexMarker1195" class="calibre6 pcalibre pcalibre1"/> format for representing ML models that enables models to be transferred between various ML frameworks. It also provides a cross-platform inference engine called the ONNX Runtime, which includes support for various kinds of hardware accelerators and is designed to provide fast inference and lower the resource requirements of <span>ML models.</span></p>
<h4 class="calibre20">TensorRT</h4>
<p class="calibre3">TensorRT is a deep<a id="_idIndexMarker1196" class="calibre6 pcalibre pcalibre1"/> learning model optimizer and runtime library developed by NVIDIA for deploying neural network models on GPUs, particularly on NVIDIA’s embedded platform <span>called Jetson.</span></p>
<h4 class="calibre20">TVM</h4>
<p class="calibre3">Apache TVM is an <a id="_idIndexMarker1197" class="calibre6 pcalibre pcalibre1"/>open source ML compiler stack that aims to enable efficient deployment of ML models on a variety of hardware platforms. TVM supports model inputs from various deep learning frameworks, including TensorFlow, Keras, PyTorch, ONNX, <span>and others.</span></p>
<p class="calibre3">These are just some of the tools that exist for optimizing ML models so that they can run at the edge. With the constant proliferation of new types of technological devices, edge optimization is an active area of research, and new tools and mechanisms continue to <span>be developed.</span></p>
<p class="calibre3">In our hands-on activities in the Jupyter Notebook that accompanies this chapter, we used TensorFlow Lite to optimize our model, after which we stored the optimized model in Google Cloud Storage. From there, we can easily deploy our model to any device that supports the TensorFlow Lite interpreter. A list of supported platforms is provided in the TensorFlow Lite documentation, which also contains a lot of additional useful information on how<a id="_idIndexMarker1198" class="calibre6 pcalibre pcalibre1"/> TensorFlow Lite works in greater <span>detail: </span><a href="https://www.tensorflow.org/lite/guide/inference#supported_platforms" class="calibre6 pcalibre pcalibre1"><span>https://www.tensorflow.org/lite/guide/inference#supported_platforms</span></a><span>.</span></p>
<p class="calibre3">At this point, we’ve covered a lot of different topics related to model deployment. Let’s take some time to recap what <span>we’ve learned.</span></p>
<h1 id="_idParaDest-225" class="calibre5"><a id="_idTextAnchor287" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we discussed various Google Cloud services for hosting ML models, such as Vertex AI, Cloud Functions, GKE, and Cloud Run. We differentiated between online and offline model serving, whereby online serving is used for real-time predictions, and offline serving is used for batch predictions. Then, we explored common challenges in deploying ML models, such as data/model drift, scaling, monitoring, performance, and keeping our models up to date. We also introduced specific components of Vertex AI that make it easier for us to deploy and manage models, such as the Vertex AI Model Registry, the Vertex AI prediction service, and Vertex AI <span>Model Monitoring.</span></p>
<p class="calibre3">Specifically, we dived quite deep into monitoring models in production, focusing on data drift and model drift. We discussed mechanisms to combat these drifts, such as automated <span>continuous training.</span></p>
<p class="calibre3">Next, we explained A/B testing for comparing two versions of a model, and we discussed optimizing ML models for edge deployment using methods such as model pruning and quantization, as well as libraries and tools for optimizing our models, such as <span>TensorFlow Lite.</span></p>
<p class="calibre3">At this point, we have covered all the major steps in the MDLC. Our next topic of focus will be how to automate the entire life cycle using MLOps. Join us in the next chapter, where you’ll continue your journey of becoming an expert AI/ML solutions architect, in which you have already come a very long way and are making <span>great progress!</span></p>
</div>
</div></body></html>