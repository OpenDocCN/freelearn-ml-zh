<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer629">
    <h1 class="chapterNumber">14</h1>
    <h1 class="chapterTitle" id="_idParaDest-315">Building Better Learners</h1>
    <p class="normal">When a sports team falls short of meeting its goal—whether it is to obtain an Olympic gold medal, a league championship, or a world record time—it must search for possible improvements. Imagine that you’re the team’s coach. How would you spend your practice sessions? Perhaps you’d direct the athletes to train harder or train differently in order to maximize every bit of their potential. You might also focus on teamwork to use each athlete’s strengths and weaknesses more smartly.</p>
    <p class="normal">Now imagine that you’re training a championship machine learning algorithm. Perhaps you hope to compete in machine learning competitions or maybe you simply need to outperform business competitors. Where do you begin? Despite the different context, the strategies for improving a sports team’s performance are like those used for improving the performance of statistical learners. As the coach, it is your job to find the combination of training techniques and teamwork skills that allow the machine learning project to meet your performance goals.</p>
    <p class="normal">This chapter builds on the material covered throughout this book to introduce techniques that improve the predictive ability of learning algorithms. You will learn:</p>
    <ul>
      <li class="bulletList">Techniques for automating model performance tuning by systematically searching for the optimal set of training conditions</li>
      <li class="bulletList">Methods for combining models into groups that use teamwork to tackle tough learning tasks</li>
      <li class="bulletList">How to use and differentiate among popular variants of decision trees that have become popular due to their impressive performance</li>
    </ul>
    <p class="normal">None of these methods will be successful for every problem. Yet looking at the winning entries to machine learning competitions, you’ll likely find at least one of them has been employed. To be competitive, you too will need to add these skills to your repertoire.</p>
    <h1 class="heading-1" id="_idParaDest-316">Tuning stock models for better performance</h1>
    <p class="normal">Some <a id="_idIndexMarker1567"/>machine learning tasks are well suited to be solved by the stock models presented in prior chapters. For these tasks, it may not be necessary to spend much time iterating and refining the model, because it may perform well enough without additional effort. On the other hand, many real-world tasks are inherently more difficult. For these tasks, the underlying concepts to be learned tend to be extremely complex, requiring an understanding of many subtle relationships, or the problem may be affected by substantial amounts of random variability, which makes it difficult to find the signal within the noise.</p>
    <p class="normal">Developing models that perform extremely well on these types of challenging problems is every bit an art as it is a science. Sometimes a bit of intuition is helpful when trying to identify areas where performance can be improved. In other cases, finding improvements will require a brute-force, trial-and-error approach. Of course, this is one of the strengths of using machines that never tire and never become bored; searching for numerous potential improvements can be made easier by automated programs. As we will see, however, human effort and computing time are not always fungible, and creating a finely-tuned learning algorithm can come with its own costs.</p>
    <p class="normal">We attempted a difficult machine learning problem<em class="italic"> in </em><em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>, as we attempted to predict bank loans that were likely to enter default. Although we were able to achieve a respectable classification accuracy of 82 percent, upon more careful examination in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, we realized that the accuracy statistic was a bit misleading. The kappa statistic—a better measure of performance for unbalanced outcomes­—was only about 0.294 as measured via 10-fold <strong class="keyWord">cross-validation</strong> (<strong class="keyWord">CV</strong>), which suggested that the model was performing somewhat poorly, despite the high accuracy. In this section, we’ll revisit the credit scoring model to see whether we can improve the results.</p>
    <div class="packt_tip">
      <p class="normal">To follow along with the examples, download the <code class="inlineCode">credit.csv</code> file from the Packt Publishing website and save it to your R working directory. Load the file into R using the following command: <code class="inlineCode">credit &lt;- read.csv("credit.csv")</code>.</p>
    </div>
    <p class="normal">You may <a id="_idIndexMarker1568"/>recall that we first used a stock C5.0 decision tree to build the classifier for the credit data and later attempted to improve the classifier’s performance by adjusting the <code class="inlineCode">trials</code> option to increase the number of boosting iterations. </p>
    <p class="normal">By changing the number of iterations from the default value of 1 up to the value of 10, we were able to increase the model’s accuracy. As defined in <em class="chapterRef">Chapter 11</em>, <em class="italic">Being Successful with Machine Learning</em>, these model options, known as hyperparameters, are not learned automatically from the data but are instead set before training. The process of testing various hyperparameter settings to achieve a better model fit is thus<a id="_idIndexMarker1569"/> called <strong class="keyWord">hyperparameter tuning</strong>, and strategies for tuning range from simple ad hoc trial and error to more rigorous and systematic iteration.</p>
    <p class="normal">Hyperparameter tuning is not limited to decision trees. For instance, we tuned k-NN models when we searched for the best value of <em class="italic">k</em>. We also tuned neural networks and support vector machines as we adjusted the number of nodes and the number of hidden layers, or chose different kernel functions. Most machine learning algorithms allow the adjustment of at least one hyperparameter, and the most sophisticated models offer many ways to tweak the model fit. Although this allows the model to be tailored closely to the learning task, the complexity of the many options can be daunting. A more systematic approach is warranted.</p>
    <h2 class="heading-2" id="_idParaDest-317">Determining the scope of hyperparameter tuning</h2>
    <p class="normal">When<a id="_idIndexMarker1570"/> performing hyperparameter <a id="_idIndexMarker1571"/>tuning, it is important to put bounds on the scope to prevent the search from proceeding endlessly. The computer provides the muscle, but it is up to the human to dictate where to look and for how long. Even as computing power is growing and cloud computing costs are shrinking, the search can easily get out of hand when sifting through nearly endless combinations of values. A narrow or shallow tuning scope may last long enough to grab a cup of coffee, while a wide or deep scope may give you time to get a good night of sleep—or more!</p>
    <p class="normal">Time and money are often fungible, as you may be able to buy time in the form of additional computing resources or by enlisting additional team members to build models faster or in parallel. Even so, taking this for granted can lead to ruin in the form of budget overruns or missed deadlines because it is easy for the scope to balloon quickly when work proceeds down countless tangents and dead ends without a plan. To avoid such pitfalls, it is wise to strategize about the breadth and depth of the tuning process beforehand.</p>
    <p class="normal">You<a id="_idIndexMarker1572"/> might start by thinking about tuning as a<a id="_idIndexMarker1573"/> process much like playing the classic board game <em class="italic">Battleship</em>. In this game, your opponent has placed a fleet of battleships on a two-dimensional grid, which is hidden out of your view. Your goal is to destroy the opponent’s fleet by guessing the coordinates of all their ships before they do the same to yours. Because the ships are known sizes and shapes, a smart player will begin by broadly probing the search grid in a checkerboard pattern but quickly focus on a specific target once it has been hit. </p>
    <p class="normal">This is a better strategy than guessing coordinates at random or iterating across each coordinate systematically, both of which are inefficient in comparison.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_14_01.png"/></figure>
    <p class="packt_figref">Figure 14.1: The hunt for the optimal machine learning hyperparameters can be much like playing the classic Battleship board game</p>
    <p class="normal">Similarly, there are methods for tuning that are more efficient than systematic iteration over endless values and combinations of values. With experience, you will develop an intuition for how to proceed, but for the first few attempts, it may be useful to think intentionally about the process. The following general strategy, listed as a series of steps, can be <a id="_idIndexMarker1574"/>adapted to your machine learning project, computing and staffing resources, and work style:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1"><strong class="keyWord">Replicate the real-world evaluation criteria</strong>: To find the single best set of model hyperparameters, it is important that the models are evaluated using the same criteria as will be used in deployment. This may mean choosing an evaluation metric that mirrors the final, real-world metric, or it may involve writing a function that simulates the deployment environment.</li>
      <li class="numberedList"><strong class="keyWord">Consider the resource usage for one iteration</strong>: As tuning will be iterating many times on the same algorithm, you should have an estimate of the time and computing resources needed for a single iteration. If it takes one hour to train a single model, it will take 100 hours or more for 100 iterations. If the computer memory is already at its limit, it is likely that you will exceed the limit during tuning. If this is a problem, you will need to invest in additional computing power, run the experiment in parallel, or reduce the size of the dataset via random sampling.</li>
      <li class="numberedList"><strong class="keyWord">Begin with a shallow search to probe for patterns</strong>: The initial tuning process should be interactive and shallow. It is intended to develop your own understanding of what options and values are important. When probing a single hyperparameter, keep increasing or decreasing its setting in reasonable increments until the performance stops improving (or starts decreasing). Depending on the option, this may be increments of one, multiples of five or ten, or incrementally small fractions, such as 0.1, 0.01, 0.001, and so on. When tuning two or more hyperparameters, it may help to focus on one at a time and keep the other values static. This is a more efficient approach than testing all possible combinations of settings, but may ultimately miss important combinations that would have been discovered if all combinations were tested.</li>
      <li class="numberedList"><strong class="keyWord">Narrow in on the optimal set of hyperparameter values</strong>: Once you have a sense of a range of values suspected to contain the optimal settings, you can reduce the increments between the tested values and test a narrower range with greater precision or test a greater number of combinations of values. The previous step should have already resulted in a reasonable set of hyperparameters, so this step should only improve and never detract from the model’s performance; it can be stopped at any time.</li>
      <li class="numberedList"><strong class="keyWord">Determine a reasonable stopping point</strong>: Deciding when to stop the tuning process is easier said than done—the thrill of the hunt and the possibility of a slightly better model can lead to a stubborn desire to keep going! Sometimes, the stopping point is a project deadline when time is running out. In other cases, the work can only stop once the desired performance level has been reached. In any case, because the only way to guarantee finding the optimal hyperparameter values is to test an infinite number of possibilities, rather than working toward burnout, you will need to define the point at which performance is “good enough” to stop the process.</li>
    </ol>
    <p class="normal"><em class="italic">Figure 14.2</em> illustrates <a id="_idIndexMarker1575"/>the process of homing in on hyperparameter values for single-parameter tuning. Five potential values (1, 2, 3, 4, and 5) denoted by solid circles were evaluated in the initial pass, and the accuracy was highest when the hyperparameter was set to 3. To check whether an even better hyperparameter setting might exist, eight additional <a id="_idIndexMarker1576"/>values (from 2.2 to 3.8 in increments of 0.2, denoted by vertical tick marks) were tested within the range between 2 and 4, which led to the discovery of a higher accuracy when the hyperparameter was set to 3.2. If time allows, one could test even more values in a narrower range around this value to possibly find an even better setting.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_14_02.png"/></figure>
    <p class="packt_figref">Figure 14.2: Strategies for parameter tuning home in on the optimal value by searching broadly and then narrowly</p>
    <p class="normal">Tuning two or more hyperparameters is more complicated, because the optimal value of one parameter may depend on the value of the others. Constructing a visualization like the one depicted in <em class="italic">Figure 14.3</em> may help in understanding how to find the best combinations of parameters; within hot spots where certain combinations of values result in better model performance, one might test more values in narrower and narrower ranges:</p>
    <figure class="mediaobject"><img alt="Chart, diagram  Description automatically generated" src="../Images/B17290_14_03.png"/></figure>
    <p class="packt_figref">Figure 14.3: Tuning strategies become more challenging as more hyperparameters are added, as the model’s best performance depends on combinations of values</p>
    <p class="normal">This <em class="italic">Battleship</em>-style <strong class="keyWord">grid search</strong>, in <a id="_idIndexMarker1577"/>which hyperparameters and combinations of hyperparameters<a id="_idIndexMarker1578"/> are tested systematically, is not the only approach to tuning, although it may be the most widely used. A more<a id="_idIndexMarker1579"/> intelligent approach called <strong class="keyWord">Bayesian optimization</strong> treats<a id="_idIndexMarker1580"/> the tuning process as a learning problem that can be solved using modeling. This approach is included in some automated machine learning software but is outside the scope of this book. Instead, for the remainder of this section, we will focus on applying the idea of grid search to our real-world dataset.</p>
    <h2 class="heading-2" id="_idParaDest-318">Example – using caret for automated tuning</h2>
    <p class="normal">Thankfully, we <a id="_idIndexMarker1581"/>can use R to conduct the iterative search <a id="_idIndexMarker1582"/>through many possible hyperparameter values and combinations of values to find the best set. This approach is a relatively easy yet sometimes computationally expensive brute-force method of optimizing a learning algorithm’s performance.</p>
    <p class="normal">The <code class="inlineCode">caret</code> package, which was used previously in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, provides tools to assist with this form of automated tuning. The core tuning functionality is provided by a <code class="inlineCode">train()</code> function that serves as a standardized interface for over 200 different machine learning models for both classification and numeric prediction tasks. Using this function, it is possible to automate the search for optimal models using a choice of evaluation methods and metrics.</p>
    <p class="normal">Automated parameter tuning with <code class="inlineCode">caret</code> will require you to consider three questions:</p>
    <ul>
      <li class="bulletList">What type of machine learning algorithm (and specific R implementation of this algorithm) should be trained on the data?</li>
      <li class="bulletList">Which hyperparameters can be adjusted for this algorithm, and how extensively should they be tuned to find the optimal settings?</li>
      <li class="bulletList">What criterion should be used to evaluate the candidate models to identify the best overall set of tuning values?</li>
    </ul>
    <p class="normal">Answering <a id="_idIndexMarker1583"/>the first question involves finding a <a id="_idIndexMarker1584"/>match between the machine learning task and one of the many models available to the <code class="inlineCode">caret</code> package. This requires a general understanding of the types of machine learning models, which you may already have if you’ve been working through this book chronologically. It can also help to work through a process of elimination. Nearly half of the models can be eliminated depending on whether the task is classification or numeric prediction; others can be excluded based on the format of the training data or the need to avoid black box models, and so on. In any case, there’s also no reason you can’t create several highly tuned models and compare them across the set.</p>
    <p class="normal">Addressing the second question is a matter largely dictated by the choice of model since each algorithm utilizes its own set of hyperparameters. The available tuning options for the predictive models covered in this book are listed in the following table. Keep in mind that although some models have additional options not shown, only those listed in the table are supported by <code class="inlineCode">caret</code> for automatic tuning.</p>
    <table class="table-container" id="table001-9">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Model</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Learning Task</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Method Name</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Hyperparameters</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">k-Nearest Neighbors</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Classification</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">knn</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">k</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Naive Bayes</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Classification</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">nb</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">fL, usekernel</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Decision Trees</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Classification</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">C5.0</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">model, trials, winnow</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">OneR Rule Learner</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Classification</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">OneR</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">None</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">RIPPER Rule Learner</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Classification</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">JRip</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">NumOpt</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Linear Regression</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Regression</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">lm</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">None</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Regression Trees</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Regression</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">rpart</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">cp</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Model Trees</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Regression</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">M5</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">pruned, smoothed, rules</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Neural Networks</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Dual Use</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">nnet</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">size, decay</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Support Vector Machines (Linear Kernel)</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Dual Use</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">svmLinear</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">C</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Support Vector Machines (Radial Basis Kernel)</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Dual Use</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">svmRadial</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">C, sigma</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Random Forests</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Dual Use</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">rf</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">mtry</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Gradient Boosting Machines (GBM)</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Dual Use</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">gbm</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">n.trees, interaction.depth, shrinkage, n.minobsinnode</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">XGBoost (XGB)</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">Dual Use</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">xgboost</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">eta, max_depth, colsample_bytree, subsample, nrounds, gamma, min_child_weight</code></p>
          </td>
        </tr>
      </tbody>
    </table>
    <div class="packt_tip">
      <p class="normal">For a complete list of the models and corresponding tuning options covered by <code class="inlineCode">caret</code>, refer to the table provided by package author Max Kuhn at <a href="http://topepo.github.io/caret/available-models.html"><span class="url">http://topepo.github.io/caret/available-models.html</span></a>.</p>
    </div>
    <p class="normal">If you<a id="_idIndexMarker1585"/> ever <a id="_idIndexMarker1586"/>forget the tuning parameters for a particular model, the <code class="inlineCode">modelLookup()</code> function can be used to find them. Simply supply the method name as illustrated for the C5.0 model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> modelLookup<span class="hljs-punctuation">(</span><span class="hljs-string">"C5.0"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  model parameter                 label forReg forClass probModel
1  C5.0    trials # Boosting Iterations  FALSE     TRUE      TRUE
2  C5.0     model            Model Type  FALSE     TRUE      TRUE
3  C5.0    winnow                Winnow  FALSE     TRUE      TRUE
</code></pre>
    <p class="normal">The goal of automatic tuning is to iterate over the set of candidate models comprising the search grid of potential parameter combinations. As it is impractical to search every conceivable combination, only a subset of possibilities is used to construct the grid. By default, <code class="inlineCode">caret</code> searches, at most, three values for each of the model’s <em class="italic">p</em> hyperparameters, which means that, at most, <em class="italic">3</em><sup class="superscript-italic" style="font-style: italic;">p</sup> candidate models will be tested. For example, by default, the automatic tuning of k-nearest neighbors will compare <em class="italic">3</em><sup class="superscript-italic" style="font-style: italic;">1</sup><em class="italic"> = 3</em> candidate models with <code class="inlineCode">k=5</code>, <code class="inlineCode">k=7</code>, and <code class="inlineCode">k=9</code>. Similarly, tuning a decision tree will result in a comparison of up to 27 different candidate models, comprising the grid of <em class="italic">3</em><sup class="superscript-italic" style="font-style: italic;">3</sup><em class="italic"> = 27</em> combinations of <code class="inlineCode">model</code>, <code class="inlineCode">trials</code>, and <code class="inlineCode">winnow</code> settings. In practice, however, only 12 models are tested. This is because <code class="inlineCode">model</code> and <code class="inlineCode">winnow</code> can only take two values (<code class="inlineCode">tree</code> versus <code class="inlineCode">rules</code> and <code class="inlineCode">TRUE</code> versus <code class="inlineCode">FALSE</code>, respectively), which makes the grid size <em class="italic">3*2*2 = 12</em>.</p>
    <div class="packt_tip">
      <p class="normal">Since the default search grid may not be ideal for your learning problem, <code class="inlineCode">caret</code> allows you to provide a custom search grid defined by a simple command, which we will cover later.</p>
    </div>
    <p class="normal">The<a id="_idIndexMarker1587"/> third and final step in automatic model tuning <a id="_idIndexMarker1588"/>involves identifying the best model among the candidates. This uses the methods discussed in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, including the choice of resampling strategy for creating training and test datasets, and the use of model performance statistics to measure the predictive accuracy. All the resampling strategies and many of the performance statistics we’ve learned are supported by <code class="inlineCode">caret</code>. These include statistics such as accuracy and kappa for classifiers and R-squared <a id="_idIndexMarker1589"/>or <strong class="keyWord">root-mean-square error</strong> (<strong class="keyWord">RMSE</strong>) for numeric models. Cost-sensitive measures like sensitivity, specificity, and AUC can also be used if desired.</p>
    <p class="normal">By default, <code class="inlineCode">caret </code>will select the candidate model with the best value of the desired performance measure. Because this practice sometimes results in the selection of models that achieve minor performance improvements via large increases in model complexity, alternative model selection functions are provided. These alternatives allow us to choose simpler models that are still reasonably close to the best model, which may be desirable in the case where a bit of predictive performance is worth sacrificing for an improvement in computational efficiency.</p>
    <p class="normal">Given the wide variety of options in the <code class="inlineCode">caret</code> tuning process, it is helpful that many of the function’s defaults are reasonable. For instance, without specifying the settings manually, <code class="inlineCode">caret</code> uses prediction accuracy or RMSE on a bootstrap sample to choose the best performer for classification and numeric prediction models, respectively. Similarly, it will automatically define a limited grid to search. These defaults allow us to start with a simple tuning process and learn to tweak the <code class="inlineCode">train()</code> function to design a wide variety of experiments of our choosing.</p>
    <h3 class="heading-3" id="_idParaDest-319">Creating a simple tuned model</h3>
    <p class="normal">To illustrate the <a id="_idIndexMarker1590"/>process of tuning a model, let’s begin by observing what happens when we attempt to tune the credit scoring model using the <code class="inlineCode">caret</code> package’s default settings. The simplest way to tune a learner requires only that you specify a model type via the <code class="inlineCode">method</code> parameter. Since we used C5.0 decision trees previously with the credit model, we’ll continue our work by optimizing this learner. The basic <code class="inlineCode">train()</code> command for tuning a C5.0 decision tree using the default settings is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>caret<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">300</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> m <span class="hljs-operator">&lt;-</span> train<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">,</span> method <span class="hljs-operator">=</span> <span class="hljs-string">"C5.0"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">First, the <code class="inlineCode">set.seed()</code> function is used to initialize R’s random number generator to a set starting position. You may recall that we used this function in several prior chapters. By setting the <code class="inlineCode">seed</code> parameter (in this case, to the arbitrary number 300), the random numbers will follow a predefined sequence. This allows simulations that use random sampling to be repeated with identical results—a very helpful feature if you are sharing code or attempting to replicate a prior result.</p>
    <p class="normal">Next, we define a tree as <code class="inlineCode">default ~ .</code> using the R formula interface. This models a loan default status (<code class="inlineCode">yes</code> or <code class="inlineCode">no</code>) using all the other features in the <code class="inlineCode">credit</code> dataset. The parameter <code class="inlineCode">method = "C5.0"</code> tells the function to use the C5.0 decision tree algorithm.</p>
    <p class="normal">After you’ve entered the preceding command, depending upon your computer’s capabilities, there may be a significant delay as the tuning process occurs. Even though this is a small dataset, a substantial amount of calculation must occur. R must repeatedly generate random bootstrap samples of data, build decision trees, compute performance statistics, and evaluate the result. Because there are 12 candidate models with varying hyperparameter values to be evaluated, and 25 bootstrap samples per candidate model to compute an average performance measure, there are <em class="italic">25*12 = 300</em> decision tree models being built using C5.0—and this doesn’t even count the additional decision trees being built when the boosting trials are set!</p>
    <p class="normal">A list <a id="_idIndexMarker1591"/>named <code class="inlineCode">m</code> stores the result of the <code class="inlineCode">train()</code> experiment, and the command <code class="inlineCode">str(m)</code> will display the associated results, but the contents can be substantial. Instead, simply type the name of the object for a condensed summary of the results. For instance, typing <code class="inlineCode">m</code> yields the following output (note that numbered labels have been added for clarity):</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_14_04.png"/></figure>
    <p class="packt_figref">Figure 14.4: The results of a caret experiment are separated into four components, as annotated in this figure</p>
    <p class="normal">The labels highlight four main components in the output:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1"><strong class="keyWord">A brief description of the input dataset</strong>: If you are familiar with your data and have applied the <code class="inlineCode">train()</code> function correctly, this information should not be surprising.</li>
      <li class="numberedList"><strong class="keyWord">A report of the preprocessing and resampling methods applied</strong>: Here we see that 25 bootstrap samples, each including 1,000 examples, were used to train the models.</li>
      <li class="numberedList"><strong class="keyWord">A list of the candidate models evaluated</strong>: In this section, we can confirm that 12 different models were tested, based on the combinations of three C5.0 hyperparameters: <code class="inlineCode">model</code>, <code class="inlineCode">trials</code>, and <code class="inlineCode">winnow</code>. The average accuracy and kappa statistics for each candidate model are also shown.</li>
      <li class="numberedList"><strong class="keyWord">The choice of best model</strong>: As the footnote describes, the model with the best accuracy (in other words, “largest”) was selected. This was the C5.0 model that used a decision tree with the settings <code class="inlineCode">winnow = FALSE</code> and <code class="inlineCode">trials = 20</code>.</li>
    </ol>
    <p class="normal">After <a id="_idIndexMarker1592"/>identifying the best model, the <code class="inlineCode">train()</code> function uses the tuned hyperparameters to build a model on the full input dataset, which is stored in <code class="inlineCode">m</code> as <code class="inlineCode">m$finalModel</code>. In most cases, you will not need to work directly with the <code class="inlineCode">finalModel</code> sub-object. Instead, simply use the <code class="inlineCode">predict()</code> function with the <code class="inlineCode">m</code> object as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> p <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>m<span class="hljs-punctuation">,</span> credit<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting vector of predictions works as expected, allowing us to create a confusion matrix that compares the predicted and actual values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>p<span class="hljs-punctuation">,</span> credit<span class="hljs-operator">$</span>default<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">p      no yes
  no  700   2
  yes   0 298
</code></pre>
    <p class="normal">Of the 1,000 examples used for training the final model, only two were misclassified, for an accuracy of 99.8 percent. However, it is very important to note that since the model was built on both the training and test data, this accuracy is optimistic and thus should not be viewed as indicative of performance on unseen data. The bootstrap accuracy estimate of 72.996 percent, which can be found in the last row of section three of the <code class="inlineCode">train()</code>output in <em class="italic">Figure 14.4</em>, is a far more realistic estimate of future accuracy.</p>
    <p class="normal">In addition to automatic hyperparameter tuning, using the <code class="inlineCode">caret</code> package’s <code class="inlineCode">train()</code> and <code class="inlineCode">predict()</code> functions also offers a pair of benefits beyond the functions found in the stock packages.</p>
    <p class="normal">First, any data preparation steps applied by the <code class="inlineCode">train()</code> function will be similarly applied to the data used for generating predictions. This includes transformations like centering and scaling, as well as the imputation of missing values. Allowing <code class="inlineCode">caret</code> to handle the data preparation will ensure that the steps that contributed to the best model’s performance will remain in place when the model is deployed.</p>
    <p class="normal">Second, the <code class="inlineCode">predict()</code> function provides a standardized interface for obtaining predicted class<a id="_idIndexMarker1593"/> values and predicted class probabilities, even for model types that ordinarily would require additional steps to obtain this information. For a classification model, the predicted classes are provided by default:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>predict<span class="hljs-punctuation">(</span>m<span class="hljs-punctuation">,</span> credit<span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] no  yes no  no  yes no 
Levels: no yes
</code></pre>
    <p class="normal">To obtain the estimated probabilities for each class, use the <code class="inlineCode">type = "prob"</code> parameter:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>predict<span class="hljs-punctuation">(</span>m<span class="hljs-punctuation">,</span> credit<span class="hljs-punctuation">,</span> type <span class="hljs-operator">=</span> <span class="hljs-string">"prob"</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">         no        yes
1 0.9606970 0.03930299
2 0.1388444 0.86115560
3 1.0000000 0.00000000
4 0.7720279 0.22797207
5 0.2948061 0.70519387
6 0.8583715 0.14162853
</code></pre>
    <p class="normal">Even in cases where the underlying model refers to the prediction probabilities using a different string (for example, <code class="inlineCode">"raw"</code> for a <code class="inlineCode">naiveBayes</code> model), the <code class="inlineCode">predict()</code> function will translate <code class="inlineCode">type = "prob"</code> to the appropriate parameter setting automatically.</p>
    <h3 class="heading-3" id="_idParaDest-320">Customizing the tuning process</h3>
    <p class="normal">The<a id="_idIndexMarker1594"/> decision tree we created previously demonstrates the <code class="inlineCode">caret</code> package’s ability to produce an optimized model with minimal intervention. The default settings allow optimized models to be created easily. However, it is also possible to change the default settings as desired, which may assist with unlocking the upper echelon of performance. Before the tuning process begins, it’s worth answering a series of questions that will help guide the setup of the <code class="inlineCode">caret</code> experiment:</p>
    <ul>
      <li class="bulletList">How long does it take for one iteration? In other words, how long does it take to train a single instance of the model being tuned?</li>
      <li class="bulletList">Given the time it takes to train a single instance, how long will it take to perform the model evaluation using the chosen resampling method? For example, 10-fold CV will require 10 times as much time as training a single model.</li>
      <li class="bulletList">How much time are you willing to spend on tuning? Based on this number, one can determine the total number of hyperparameter values that can be tested. For instance, if it takes one minute to evaluate a model using 10-fold CV, then 60 hyperparameter settings can be tested per hour.</li>
    </ul>
    <p class="normal">Using time as<a id="_idIndexMarker1595"/> the key limiting factor will help put bounds on the tuning process and prevent you from chasing better and better performance endlessly.</p>
    <p class="normal">Once you’ve decided how much time to spend on the trials, it is easy to customize the process to your liking. To illustrate this flexibility, let’s modify our work on the credit decision tree to mirror the process we used in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>. In that chapter, we estimated the kappa statistic using 10-fold CV. We’ll do the same here, using kappa to tune the boosting trials for the C5.0 decision tree algorithm and find the optimal setting for our data. Note that decision tree boosting was first covered in <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>, and will also be covered in greater detail later in this chapter.</p>
    <p class="normal">The <code class="inlineCode">trainControl()</code> function is used to create a set of configuration options known as a <strong class="keyWord">control object</strong>. This <a id="_idIndexMarker1596"/>object guides the <code class="inlineCode">train()</code> function and allows for the selection of model evaluation criteria such as the resampling strategy and the measure used for choosing the best model. Although this function can be used to modify nearly every aspect of a <code class="inlineCode">caret</code> tuning experiment, we’ll focus on two important parameters: <code class="inlineCode">method</code> and <code class="inlineCode">selectionFunction</code>.</p>
    <div class="packt_tip">
      <p class="normal">If you’re eager for more details about the control object, you can use the <code class="inlineCode">?trainControl</code> command for a list of all the parameters.</p>
    </div>
    <p class="normal">When using the <code class="inlineCode">trainControl()</code> function, the <code class="inlineCode">method</code> parameter sets the resampling method, such as holdout sampling or k-fold CV. The following table lists the possible <code class="inlineCode">method</code> values, as well as any additional parameters for adjusting the sample size and the number of iterations. Although the default options for these resampling methods follow popular conventions, you may choose to adjust these depending on the size of your dataset and the complexity of your model.</p>
    <table class="table-container" id="table002-6">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Resampling method</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Method name</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Additional options and default values</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Holdout sampling</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">LGOCV</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">p = 0.75</code> (training data proportion)</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">k-fold CV</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">cv</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">number = 10</code> (number of folds)</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Repeated k-fold CV</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">repeatedcv</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">number = 10</code> (number of folds)</p>
            <p class="normal"><code class="inlineCode">repeats = 10</code> (number of iterations)</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Bootstrap sampling</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">boot</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">number = 25</code> (resampling iterations)</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">0.632 bootstrap</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">boot632</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">number = 25</code> (resampling iterations)</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><em class="italic">Leave-one-out CV</em></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">LOOCV</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">None</em></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">The <code class="inlineCode">selectionFunction</code> parameter is used to specify the function that will choose the optimal <a id="_idIndexMarker1597"/>model among the candidates. Three such functions are included. The <code class="inlineCode">best</code> function simply chooses the candidate with the best value on the specified performance measure. This is used by default. The other two functions are used to choose the most parsimonious, or simplest, model that is within a certain threshold of the best model’s performance. The <code class="inlineCode">oneSE</code> function chooses the simplest candidate within one standard error of the best performance, and <code class="inlineCode">tolerance</code> uses the simplest candidate within a user-specified percentage.</p>
    <div class="packt_tip">
      <p class="normal">Some subjectivity is involved with the <code class="inlineCode">caret</code> package’s ranking of models by simplicity. For information on how models are ranked, see the help page for the selection functions by typing <code class="inlineCode">?best</code> at the R command prompt.</p>
    </div>
    <p class="normal">To create a control object named <code class="inlineCode">ctrl</code> that uses 10-fold CV and the <code class="inlineCode">oneSE</code> selection function, use the following command, noting that <code class="inlineCode">number = 10</code> is included only for clarity; since this is the default value for <code class="inlineCode">method = "cv"</code>, it could have been omitted:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> ctrl <span class="hljs-operator">&lt;-</span> trainControl<span class="hljs-punctuation">(</span>method <span class="hljs-operator">=</span> <span class="hljs-string">"cv"</span><span class="hljs-punctuation">,</span> number <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-punctuation">,</span>
                       selectionFunction <span class="hljs-operator">=</span> <span class="hljs-string">"oneSE"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">We’ll use the result of this function shortly.</p>
    <p class="normal">In the meantime, the next step in setting up our experiment is to create the search grid for hyperparameter tuning. The grid must include a column named for each hyperparameter in the desired model, regardless of whether it will be tuned. It must also include a row for each desired combination of values to test. Since we are using a C5.0 decision tree, this means we’ll need columns named <code class="inlineCode">model</code>, <code class="inlineCode">trials</code>, and <code class="inlineCode">winnow</code>, corresponding to the three options that can be tuned. For other machine learning models, refer to the table presented earlier in this chapter or use the <code class="inlineCode">modelLookup()</code> function to find the hyperparameters as described previously.</p>
    <p class="normal">Rather than <a id="_idIndexMarker1598"/>filling the grid data frame cell by cell—a tedious task if there are many possible combinations of values—we can use the <code class="inlineCode">expand.grid()</code> function, which creates data frames from the combinations of all values supplied. For example, suppose we would like to hold constant <code class="inlineCode">model = "tree"</code> and <code class="inlineCode">winnow = FALSE</code> while searching eight different values of <code class="inlineCode">trials</code>. </p>
    <p class="normal">This can be created as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> grid <span class="hljs-operator">&lt;-</span> expand.grid<span class="hljs-punctuation">(</span>model <span class="hljs-operator">=</span> <span class="hljs-string">"tree"</span><span class="hljs-punctuation">,</span>
                      trials <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span><span class="hljs-punctuation">,</span> <span class="hljs-number">15</span><span class="hljs-punctuation">,</span> <span class="hljs-number">20</span><span class="hljs-punctuation">,</span> <span class="hljs-number">25</span><span class="hljs-punctuation">,</span> <span class="hljs-number">30</span><span class="hljs-punctuation">,</span> <span class="hljs-number">35</span><span class="hljs-punctuation">),</span>
                      winnow <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting <code class="inlineCode">grid</code> data frame contains <em class="italic">1*8*1 = 8</em> rows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> grid
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  model trials winnow
1  tree      1  FALSE
2  tree      5  FALSE
3  tree     10  FALSE
4  tree     15  FALSE
5  tree     20  FALSE
6  tree     25  FALSE
7  tree     30  FALSE
8  tree     35  FALSE
</code></pre>
    <p class="normal">The <code class="inlineCode">train()</code> function will build a candidate model for evaluation using each <code class="inlineCode">grid</code> row’s combination of model parameters.</p>
    <p class="normal">Given the search grid and the control object created previously, we are ready to run a thoroughly customized <code class="inlineCode">train()</code> experiment. As before, we’ll set the random seed to the arbitrary number <code class="inlineCode">300</code> in order to ensure repeatable results. But this time, we’ll pass our control object and tuning grid while adding a parameter <code class="inlineCode">metric = "Kappa"</code>, indicating the statistic to be used by the model evaluation function—in this case, <code class="inlineCode">"oneSE"</code>. The full set of commands is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">300</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> m <span class="hljs-operator">&lt;-</span> train<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">,</span> method <span class="hljs-operator">=</span> <span class="hljs-string">"C5.0"</span><span class="hljs-punctuation">,</span>
             metric <span class="hljs-operator">=</span> <span class="hljs-string">"Kappa"</span><span class="hljs-punctuation">,</span>
             trControl <span class="hljs-operator">=</span> ctrl<span class="hljs-punctuation">,</span>
             tuneGrid <span class="hljs-operator">=</span> grid<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This results in<a id="_idIndexMarker1599"/> an object that we can view by typing its name:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> m
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">C5.0 
1000 samples
  16 predictor
   2 classes: 'no', 'yes' 
No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... 
Resampling results across tuning parameters:
  trials  Accuracy  Kappa    
   1      0.710     0.2859380
   5      0.726     0.3256082
  10      0.725     0.3054657
  15      0.726     0.3204938
  20      0.733     0.3292403
  25      0.732     0.3308708
  30      0.733     0.3298968
  35      0.738     0.3449912
Tuning parameter 'model' was held constant at a value of tree
Tuning parameter 'winnow' was held constant at a value of FALSE
Kappa was used to select the optimal model using the one SE rule.
The final values used for the model were trials = 5, model = tree
 and winnow = FALSE.
</code></pre>
    <p class="normal">Although the output is similar to the automatically tuned model, there are a few notable differences. Because 10-fold CV was used, the sample size to build each candidate model was reduced to 900 rather than the 1,000 used in the bootstrap. Furthermore, eight candidate models were tested rather than the 12 in the prior experiment. Lastly, because <code class="inlineCode">model</code> and <code class="inlineCode">winnow</code> were held constant, their values are no longer shown in the results; instead, they are listed as a footnote.</p>
    <p class="normal">The best <a id="_idIndexMarker1600"/>model here differs quite significantly from the prior experiment. Before, the best model used <code class="inlineCode">trials = 20</code>, whereas here, it used <code class="inlineCode">trials = 1</code>. This change is because we used the <code class="inlineCode">oneSE</code> function rather than the <code class="inlineCode">best</code> function to select the optimal model. Even though the model with<code class="inlineCode"> trials = 35</code> obtained the best kappa, the single-trial model offers reasonably close performance with a much simpler algorithm. </p>
    <div class="packt_tip">
      <p class="normal">Due to the large number of configuration parameters, <code class="inlineCode">caret</code> can seem overwhelming at first. Don’t let this deter you—there is no easier way to test the performance of models using 10-fold CV. Instead, think of the experiment as defined by two parts: a <code class="inlineCode">trainControl()</code> object that dictates the testing criteria, and a tuning grid that determines what model parameters to evaluate. Supply these to the <code class="inlineCode">train()</code> function and with a bit of computing time, your experiment will be complete!</p>
    </div>
    <p class="normal">Of course, tuning is just one possibility for building better learners. In the next section, you will discover that in addition to buffing up a single learner to make it stronger, it is also possible to combine several weaker models to form a more powerful team.</p>
    <h1 class="heading-1" id="_idParaDest-321">Improving model performance with ensembles</h1>
    <p class="normal">Just as the <a id="_idIndexMarker1601"/>best sports teams have players with complementary rather than overlapping skillsets, some of the best machine learning algorithms utilize teams of complementary models. Since a model brings a unique bias <a id="_idIndexMarker1602"/>to a learning task, it may readily learn one subset of examples but have trouble with another. Therefore, by intelligently using the talents of several diverse team members, it is possible to create a strong team of multiple weak learners.</p>
    <p class="normal">This technique of combining and managing the predictions of multiple models falls into a wider set of <strong class="keyWord">meta-learning methods</strong>, which are techniques that involve learning how to learn. This includes anything from simple algorithms that gradually improve performance by iterating over design decisions—for instance, the automated parameter tuning used earlier in this chapter—to highly complex algorithms that use concepts borrowed from evolutionary biology and genetics for self-modifying and adapting to learning tasks.</p>
    <p class="normal">Suppose<a id="_idIndexMarker1603"/> you were a contestant on a television trivia show that allowed you to choose a panel of five friends to assist you with<a id="_idIndexMarker1604"/> answering the final question for the million-dollar prize. Most people would try to stack the panel with a diverse set of subject matter experts. A panel containing professors of literature, science, history, and art, along with a current pop-culture expert, would be safely well-rounded. Given their breadth of knowledge, it would be unlikely that a question would stump the group.</p>
    <p class="normal">The meta-learning approach that utilizes a similar principle of creating a varied team of experts is known as <a id="_idIndexMarker1605"/>an <strong class="keyWord">ensemble</strong>. For the remainder of this chapter, we’ll focus on meta-learning only as it pertains to ensembling—the task of modeling a relationship between the predictions of several models and the desired outcome. The teamwork-based methods covered here are quite powerful and are used often to build more effective classifiers.</p>
    <h2 class="heading-2" id="_idParaDest-322">Understanding ensemble learning</h2>
    <p class="normal">All ensemble methods are based on the idea that by combining multiple weaker learners, a <a id="_idIndexMarker1606"/>stronger learner is created. Ensembles contain two or more machine learning models, which can be of the same type, such as several decision trees, or of different types, such as a decision tree and a neural network. Though there are myriad ways to construct an ensemble, they tend to fall into several general categories, which can be distinguished, in large part, by the answers to two questions:</p>
    <ul>
      <li class="bulletList">How are the ensemble’s models chosen and trained?</li>
      <li class="bulletList">How are the models’ predictions combined to make a single final prediction?</li>
    </ul>
    <p class="normal">When answering these questions, it can be helpful to imagine the ensemble in terms of the following process diagram, which encompasses nearly all ensembling approaches:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_14_05.png"/></figure>
    <p class="packt_figref">Figure 14.5: Ensembles combine multiple weaker models into a single stronger model</p>
    <p class="normal">In this design pattern, input<a id="_idIndexMarker1607"/> training data is used to build several models. The <strong class="keyWord">allocation function</strong> dictates<a id="_idIndexMarker1608"/> how much and what subsets of the training data each model receives. Do they each receive the full training dataset or merely a sample? Do they each receive every feature or a subset of features? The decisions made here will shape the training of the weaker learners that comprise the stronger ensemble.</p>
    <p class="normal">Just as you’d want a variety of experts to advise your appearance on a television trivia game show, ensembles depend on a <strong class="keyWord">diverse</strong> set of classifiers, which means that they have uncorrelated classifications but still perform better than random chance. In other words, each classifier must be making an independent prediction, but each must also be doing more than merely guessing.</p>
    <p class="normal">Diversity can be added to the ensemble by including a variety of machine learning techniques, such as an ensemble that groups a decision tree, a neural network, and a logistic regression model. </p>
    <p class="normal">Alternatively, the allocation function itself can also be a source of diversity by acting <a id="_idIndexMarker1609"/>as a <strong class="keyWord">data manipulator</strong> and artificially varying the input data to bias the resulting learners, even if they use the same learning algorithm. As we will see in practice later, the allocation and data manipulation processes may be automated or included as part of the ensembling algorithm itself, or they may be performed by hand as part of the data engineering and model-building process. Overall, modes of increasing the ensemble’s diversity generally fall into five categories:</p>
    <ul>
      <li class="bulletList">Using assorted base learning algorithms</li>
      <li class="bulletList">Manipulating the training sample by taking different samples at random, often by using bootstrapping</li>
      <li class="bulletList">Manipulating a single learning algorithm by using different hyperparameter settings</li>
      <li class="bulletList">Changing how the target feature is represented, such as representing an outcome as binary, categorical, or numeric</li>
      <li class="bulletList">Partitioning the training data into subgroups that represent different patterns to be learned; for instance, one might stratify the examples by key features, and let models in the ensemble become experts on different subsets of the training data</li>
    </ul>
    <p class="normal">For <a id="_idIndexMarker1610"/>instance, in an ensemble of decision trees, the allocation function might use bootstrap sampling to construct unique training datasets for each tree, or it may pass each one a different subset of features. On the other hand, if the ensemble already includes a diverse set of algorithms—such as a neural network, a decision tree, and a k-NN classifier—then the allocation function might pass the training data on to each algorithm relatively unchanged.</p>
    <p class="normal">After the ensemble’s models are trained, they can be used to generate predictions on future data, but this set of multiple predictions must be reconciled somehow to generate a single final prediction. The <strong class="keyWord">combination function</strong> is the step in the ensembling process<a id="_idIndexMarker1611"/> that takes each of these predictions and combines them into a single authoritative prediction for the set. Of course, because some of the models may disagree on the predicted value, the function must somehow blend or unify the information from the learners. The combination function is also known as a <strong class="keyWord">composer</strong> due<a id="_idIndexMarker1612"/> to its work synthesizing the final prediction.</p>
    <p class="normal">There are two main strategies for merging or composing final predictions. The simpler of the two approaches<a id="_idIndexMarker1613"/> involves <strong class="keyWord">weighting methods</strong>, which assign a score to each prediction that dictates how heavily it will factor into the final prediction. These range from a simple majority vote in which each classifier is weighted evenly, to more complex performance-based methods that grant more authority to some models than others if they have proven to be more reliable on past data. </p>
    <p class="normal">The second approach uses more<a id="_idIndexMarker1614"/> complex meta-learning methods, such as the model stacking technique, which will be covered in depth later in this chapter. These use the initial set of predictions from the weak learners to train a secondary machine learning algorithm to make the final prediction—a process that is analogous to a committee making recommendations to a leader that makes the final decision.</p>
    <p class="normal">Ensembling methods are used to gain better performance than what is possible using only a single learning algorithm—the primary goal of the ensemble is to turn a group of weaker learners into a stronger, unified team. Still, there are many additional benefits, some of which may be surprising. These suggest additional reasons why one might turn to an ensemble, even<a id="_idIndexMarker1615"/> outside of a machine learning competition environment:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">The use of independent ensembles allows work in parallel</strong>: Training independent classifiers separately means that work can be divided across multiple people. This allows more rapid iteration and may increase creativity. Each team member builds their best model, and the results can be easily combined into an ensemble at the end.</li>
      <li class="bulletList"><strong class="keyWord">Improved performance on massive or minuscule datasets</strong>: Many algorithms run into memory or complexity limits when an extremely large set of features or examples are used. An ensemble of independent models can be fed subsets of features or examples, which are more computationally efficient to train than a single full model, and importantly, can often be run in parallel using distributed computing methods. On the other side of the spectrum, ensembles also do well on the smallest datasets because resampling methods like bootstrapping are inherently part of the allocation function of many ensemble designs.</li>
      <li class="bulletList"><strong class="keyWord">The ability to synthesize data from distinct domains</strong>: Since there is no one-size-fits-all learning algorithm, and each learning algorithm has its own biases and heuristics, the ensemble’s ability to incorporate evidence from multiple types of learners is increasingly important for modeling the most challenging learning tasks relying on data drawn from diverse domains.</li>
      <li class="bulletList"><strong class="keyWord">A more nuanced understanding of difficult learning tasks</strong>: Real-world phenomena are often extremely complex, with many interacting intricacies. Methods like ensembles, which divide the task into smaller modeled portions, are more able to capture subtle patterns that a single model might miss. Some learners in the set can go narrower and deeper to learn a specific subset of the most challenging cases.</li>
    </ul>
    <p class="normal">None of these benefits would be very helpful if you weren’t able to easily apply ensemble methods in R, and there are many packages available to do just that. Let’s look at several of the most popular ensemble methods and how they can be used to improve the performance of the credit model we’ve been working on.</p>
    <h2 class="heading-2" id="_idParaDest-323">Popular ensemble-based algorithms</h2>
    <p class="normal">Thankfully, using<a id="_idIndexMarker1616"/> teams of machine learners to improve the predictive performance doesn’t mean you’ll need to train each ensemble member separately by hand, although this option does exist, as you will learn later in this chapter. Instead, there are ensemble-based algorithms that manipulate the allocation function to train a very large number of simpler models in a single step automatically. In this way, an ensemble that includes a hundred learners or more can be trained with no more human time and input than training a single learner. As easily as one might build a single decision tree model, it is possible to build an ensemble with hundreds of such trees and harness the power of teamwork. Although it would be tempting to assume this is a magic bullet, such power, of course, comes with downsides such as loss of interpretability and a less diverse set of base algorithms from which to choose. This will be apparent in the sections that follow, which cover the evolution of two decades’ worth of popular ensembling algorithms—all of which, not coincidentally, are based on decision trees.</p>
    <h3 class="heading-3" id="_idParaDest-324">Bagging</h3>
    <p class="normal">One of the<a id="_idIndexMarker1617"/> first ensemble methods to gain widespread acceptance used a technique <a id="_idIndexMarker1618"/>called <strong class="keyWord">bootstrap aggregating</strong> or <strong class="keyWord">bagging</strong> for<a id="_idIndexMarker1619"/> short. As described by Leo Breiman in the mid-1990s, bagging begins by generating several new training datasets using bootstrap sampling on the original training data. These datasets are then used to generate a set of models using a single learning algorithm. The models’ predictions are combined using voting for classification and averaging for numeric prediction.</p>
    <div class="note">
      <p class="normal">For additional information on bagging, refer to <em class="italic">Bagging predictors. Breiman L., Machine Learning, 1996, Vol. 24, pp. 123-140</em>.</p>
    </div>
    <p class="normal">Although bagging is a relatively simple ensemble, it can perform quite well if it is used with<a id="_idIndexMarker1620"/> relatively <strong class="keyWord">unstable</strong> learners, that is, those generating models that tend to change substantially when the input data changes only slightly. Unstable models are essential for ensuring the ensemble’s diversity despite only minor variations across the bootstrap training datasets. </p>
    <p class="normal">For this reason, bagging is most often used with decision trees, which have the tendency to vary dramatically given minor changes in input data.</p>
    <p class="normal">The <code class="inlineCode">ipred</code> package offers a <a id="_idIndexMarker1621"/>classic implementation of bagged decision trees. To train the model, the <code class="inlineCode">bagging()</code> function works similarly to many of the models used previously. The <code class="inlineCode">nbagg</code> parameter is used to control the number of decision trees voting in the ensemble, with a default value of <code class="inlineCode">25</code>. Depending on the difficulty of the learning task and the amount of training data, increasing this number may improve the model’s performance, up to a limit. The downside is that this creates additional computational expense, and a large number of trees may take some time to train.</p>
    <p class="normal">After installing the <code class="inlineCode">ipred</code> package, we can create the ensemble as follows. We’ll stick to the default value of <code class="inlineCode">25</code> decision trees: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>ipred<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"credit.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">123</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> mybag <span class="hljs-operator">&lt;-</span> bagging<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">,</span> nbagg <span class="hljs-operator">=</span> <span class="hljs-number">25</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting <code class="inlineCode">mybag</code> model works as expected in concert with the <code class="inlineCode">predict()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_pred <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>mybag<span class="hljs-punctuation">,</span> credit<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>credit_pred<span class="hljs-punctuation">,</span> credit<span class="hljs-operator">$</span>default<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">           
credit_pred  no yes
        no  699   4
        yes   1 296
</code></pre>
    <p class="normal">Given the preceding results, the model seems to have fit the data extremely well—<em class="italic">too well</em>, probably, as the results are based only on the training data and thus may reflect overfitting rather than true performance on future unseen data. To obtain a better estimate of future performance, we can use the bagged decision tree method in the <code class="inlineCode">caret</code> package to obtain a 10-fold CV estimate of accuracy and kappa. Note that the method name for the <code class="inlineCode">ipred</code> bagging function is <code class="inlineCode">treebag</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>caret<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"credit.csv"</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">300</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> ctrl <span class="hljs-operator">&lt;-</span> trainControl<span class="hljs-punctuation">(</span>method <span class="hljs-operator">=</span> <span class="hljs-string">"cv"</span><span class="hljs-punctuation">,</span> number <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> train<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">,</span> method <span class="hljs-operator">=</span> <span class="hljs-string">"treebag"</span><span class="hljs-punctuation">,</span>
        trControl <span class="hljs-operator">=</span> ctrl<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Bagged CART 
1000 samples
  16 predictor
   2 classes: 'no', 'yes' 
No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... 
Resampling results:
  Accuracy  Kappa    
  0.732     0.3319334
</code></pre>
    <p class="normal">The<a id="_idIndexMarker1622"/> kappa statistic of 0.33 for this model suggests that the bagged tree model performs roughly as well as the C5.0 decision tree we tuned earlier in this chapter, which had a kappa statistic ranging from 0.32 to 0.34, depending on the tuning parameters. Keep this performance in mind as you read the next section, and consider the differences between the simple bagging technique and the more complex methods that build upon it.</p>
    <h3 class="heading-3" id="_idParaDest-325">Boosting</h3>
    <p class="normal">Another <a id="_idIndexMarker1623"/>common ensemble-based method is called <strong class="keyWord">boosting</strong> because<a id="_idIndexMarker1624"/> it improves or “boosts” the performance of weak learners to attain the performance of stronger learners. This method is based largely on the work of Robert Schapire and Yoav Freund, who have published extensively on the topic since the 1990s.</p>
    <div class="note">
      <p class="normal">For additional information on boosting, refer to <em class="italic">Boosting: Foundations and Algorithms, Schapire, RE, Freund, Y, Cambridge, MA: The MIT Press, 2012</em>.</p>
    </div>
    <p class="normal">Like bagging, boosting uses ensembles of models trained on resampled data and a vote to determine the final prediction. There are two key distinctions. First, the resampled datasets in boosting are constructed specifically to generate complementary learners. This means that the work cannot occur in parallel, as the ensemble’s models are no longer independent from one another. Second, rather than giving each learner an equal vote, boosting gives each learner a vote that is weighted based on its past performance. Models that perform better have greater influence over the ensemble’s final prediction.</p>
    <p class="normal">Boosting will result <a id="_idIndexMarker1625"/>in performance that is often somewhat better and certainly no worse than the best model in the ensemble. Since the models in the ensemble are purposely built to be complementary, it is possible to increase ensemble performance to an arbitrary threshold simply by adding additional classifiers to the group, assuming that each additional classifier performs better than random chance. Given the obvious utility of this <a id="_idIndexMarker1626"/>finding, boosting is thought to be one of the most significant discoveries in machine learning.</p>
    <div class="packt_tip">
      <p class="normal">Although boosting can create a model that meets an arbitrarily low error rate, this may not always be reasonable in practice. One reason for this is that the performance gains are incrementally smaller as additional learners are gained, making some thresholds practically infeasible. Additionally, the pursuit of pure accuracy may result in the model being overfitted to the training data and not generalizable to unseen data.</p>
    </div>
    <p class="normal">A boosting algorithm<a id="_idIndexMarker1627"/> called <strong class="keyWord">AdaBoost</strong>, short for <strong class="keyWord">adaptive boosting</strong>, was proposed by Freund and Schapire in 1997. The algorithm is based on the idea of generating weak learners that iteratively learn a larger portion of the difficult-to-classify examples in the training data by paying more attention (that is, giving more weight) to often misclassified examples.</p>
    <p class="normal">Beginning from an unweighted dataset, the first classifier attempts to model the outcome. Examples that the classifier predicted correctly will be less likely to appear in the training dataset for the following classifier, and conversely, the difficult-to-classify examples will appear more frequently. As additional rounds of weak learners are added, they are trained on data with successively more difficult examples. The process continues until the desired overall error rate is reached or performance no longer improves. At that point, each classifier’s vote is weighted according to its accuracy on the training data on which it was built.</p>
    <p class="normal">Though boosting principles can be applied to nearly any type of model, the principles are most often used with decision trees. We already applied the boosting technique earlier in this chapter, as well as in <em class="chapterRef">Chapter 5</em>,<em class="italic"> Divide and Conquer – Classification Using Decision Trees and Rules</em>, as a method to improve the performance of a C5.0 decision tree. With C5.0, boosting can be enabled by simply setting a <code class="inlineCode">trials</code> parameter to an integer value greater than one.</p>
    <p class="normal">The <strong class="keyWord">AdaBoost.M1</strong> algorithm <a id="_idIndexMarker1628"/>provides a standalone implementation of AdaBoost for classification with trees. The algorithm can be found in the <code class="inlineCode">adabag</code> package.</p>
    <div class="note">
      <p class="normal">For more information about the <code class="inlineCode">adabag</code> package, refer to <em class="italic">adabag: An R Package for Classification with Boosting and Bagging, Alfaro, E, Gamez, M, Garcia, N, Journal of Statistical Software, 2013, Vol. 54, pp. 1-35</em>.</p>
    </div>
    <p class="normal">Let’s <a id="_idIndexMarker1629"/>create an <code class="inlineCode">AdaBoost.M1</code> classifier for the credit data. The general syntax for this algorithm is similar to other modeling techniques:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>adabag<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"credit.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">300</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> m_adaboost <span class="hljs-operator">&lt;-</span> boosting<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">As usual, the <code class="inlineCode">predict()</code> function is applied to the resulting object to make predictions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> p_adaboost <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>m_adaboost<span class="hljs-punctuation">,</span> credit<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Departing from convention, rather than returning a vector of predictions, this returns an object with information about the model. The predictions are stored in a sub-object called <code class="inlineCode">class</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>p_adaboost<span class="hljs-operator">$</span><span class="hljs-built_in">class</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "no"  "yes" "no"  "no"  "yes" "no"
</code></pre>
    <p class="normal">A confusion matrix can be found in the <code class="inlineCode">confusion</code> sub-object:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> p_adaboost<span class="hljs-operator">$</span>confusion
               Observed Class
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Predicted Class  no yes
            no  700   0
            yes   0 300
</code></pre>
    <p class="normal">Before you get your hopes up about the perfect accuracy, note that the preceding confusion matrix is based on the model’s performance on the training data. Since boosting allows the error rate to be reduced to an arbitrarily low level, the learner simply continued until it made no more errors. This likely resulted in overfitting on the training dataset.</p>
    <p class="normal">For a more <a id="_idIndexMarker1630"/>accurate assessment of performance on unseen data, we need to use another evaluation method. The <code class="inlineCode">adabag</code> package provides a simple function to use 10-fold CV:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">300</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> adaboost_cv <span class="hljs-operator">&lt;-</span> boosting.cv<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Depending on your computer’s capabilities, this may take some time to run, during which it will log each iteration to the screen—on a recent MacBook Pro computer, it took about a minute. After it completes, we can view a more reasonable confusion matrix:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> adaboost_cv<span class="hljs-operator">$</span>confusion
               Observed Class
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Predicted Class  no yes
            no  598 160
            yes 102 140
</code></pre>
    <p class="normal">We can find the kappa statistic using the <code class="inlineCode">vcd</code> package, as demonstrated in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>vcd<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> Kappa<span class="hljs-punctuation">(</span>adaboost_cv<span class="hljs-operator">$</span>confusion<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">            value     ASE     z  Pr(&gt;|z|)
Unweighted 0.3397 0.03255 10.44 1.676e-25
Weighted   0.3397 0.03255 10.44 1.676e-25
</code></pre>
    <p class="normal">With a kappa of <code class="inlineCode">0.3397</code>, the boosted model is slightly outperforming the bagged decision trees, which had a kappa of around <code class="inlineCode">0.3319</code>. Let’s see how boosting compares to another ensemble method.</p>
    <div class="packt_tip">
      <p class="normal">Note that the prior results were obtained using R version 4.2.3 on a Windows PC and verified on Linux. At the time this was written, slightly different results are obtained using R 4.2.3 for Apple silicon on a recent MacBook Pro. Also note that the AdaBoost.M1 algorithm can be tuned with <code class="inlineCode">caret</code> by specifying <code class="inlineCode">method = "AdaBoost.M1"</code>.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-326">Random forests</h3>
    <p class="normal">Yet another <a id="_idIndexMarker1631"/>tree-based ensemble-based <a id="_idIndexMarker1632"/>method, called <strong class="keyWord">random forests</strong>, builds upon the principles of bagging but adds additional diversity to the decision trees by only allowing the algorithm to choose from a randomly selected subset of features each time it attempts to split. Beginning at the root node, the random forest algorithm might only be allowed to choose from a small number of features selected at random from the full set of predictors; at each subsequent split, a different random subset is provided. As is the case for bagging, once the ensemble of trees (the forest) is generated, the algorithm performs a simple vote to make the final prediction.</p>
    <div class="note">
      <p class="normal">For more detail on how random forests are constructed, refer to <em class="italic">Random Forests, Breiman L, Machine Learning, 2001, Vol. 45, pp. 5-32</em>. Note that the phrase “random forests” is trademarked by Breiman and Cutler but is used colloquially to refer to any type of decision tree ensemble. A pedant would use the more general<a id="_idIndexMarker1633"/> term <strong class="keyWord">decision tree forests</strong> except when referring to their specific implementation.</p>
    </div>
    <p class="normal">The fact that each tree is built on different and randomly selected sets of features helps ensure that each tree in the ensemble is unique. It is even possible that two trees in the forest may have been built from completely different sets of features. Random feature selection limits the decision tree’s greedy heuristic from picking the same low-hanging fruit each time the tree is grown, which may help the algorithm discover subtle patterns that the standard tree-growing method may miss. On the other hand, the potential for overfitting is limited given that each tree has just one vote of many in the forest.</p>
    <p class="normal">Given these strengths, it is no surprise that the random forest algorithm quickly grew to become one of the most popular learning algorithms—only recently has its hype been surpassed by a newer ensemble method, which you will learn about shortly. Random forests combine versatility and power into a single machine learning approach and are not especially prone to overfitting or underfitting. Because the tree-growing algorithm uses only a small, random portion of the full feature set, random forests can handle extremely large datasets, where the so-called curse of dimensionality might cause other models to fail. At the same time, its predictive performance on most learning tasks is as good as, if not better than, all but the most sophisticated methods. The following table summarizes the strengths and weaknesses of random forest models:</p>
    <table class="table-container" id="table003-4">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">An all-purpose model that performs well on most problems, including both classification and numeric prediction</li>
              <li class="bulletList">Can handle noisy or missing data as well as categorical or continuous features</li>
              <li class="bulletList">Selects only the most important features</li>
              <li class="bulletList">Can be used on data with an extremely large number of features or examples</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Unlike a decision tree, the model is not easily interpretable</li>
              <li class="bulletList">May struggle with categorical features with very large numbers of levels</li>
              <li class="bulletList">Cannot be extensively tuned if greater performance is desired</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Their <a id="_idIndexMarker1634"/>strong performance combined with the ease of use makes random forests a terrific place to begin most real-world machine learning projects. The <a id="_idIndexMarker1635"/>algorithm also provides a solid benchmark for other comparisons with highly tuned models, as well as the other, more complex approaches you will learn about later.</p>
    <p class="normal">For a hands-on demonstration of random forests, we’ll apply the technique to the credit-scoring data we’ve been using in this chapter. Although there are several packages with random forest implementations in R, the aptly named <code class="inlineCode">randomForest</code> package is perhaps the simplest, while the <code class="inlineCode">ranger</code> package offers much better performance on large datasets. Both are supported by the <code class="inlineCode">caret</code> package for experimentation and automated parameter tuning. The syntax for training a model with <code class="inlineCode">randomForest</code> is as follows:</p>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B17290_14_06.png"/></figure>
    <p class="packt_figref">Figure 14.6: Random forest syntax</p>
    <p class="normal">By <a id="_idIndexMarker1636"/>default, the <code class="inlineCode">randomForest()</code> function creates <a id="_idIndexMarker1637"/>an ensemble of 500 decision trees that each consider <code class="inlineCode">sqrt(p)</code> random features at each split, where <code class="inlineCode">p</code> is the number of features in the training dataset and <code class="inlineCode">sqrt()</code> refers to R’s square root function. For example, since the credit data has 16 features, each of the 500 decision trees would be allowed to consider only <em class="italic">sqrt</em>(<em class="italic">16</em>) =<em class="italic"> 4</em> predictors each time the algorithm attempts to split.</p>
    <p class="normal">Whether or not these default <code class="inlineCode">ntree</code> and <code class="inlineCode">mtry</code> parameters are appropriate depends on the nature of the learning task and training data. Generally, more complex learning problems and larger datasets (both more features as well as more examples) warrant a larger number of trees, though this needs to be balanced with the computational expense of training more trees. Once the <code class="inlineCode">ntree</code> parameter is set to a sufficiently large value, the <code class="inlineCode">mtry</code> parameter can be tuned to determine the best setting; however, the default tends to work <a id="_idIndexMarker1638"/>well in practice. Assuming the number of trees is large enough, the number <a id="_idIndexMarker1639"/>of randomly selected features can be surprisingly low before performance is degraded—but trying a few values is still a good practice. Ideally, the number of trees should be set large enough such that each feature has a chance of appearing in several models.</p>
    <p class="normal">Let’s see how the default <code class="inlineCode">randomForest()</code> parameters work with the credit data. We’ll train the model just as we have done with other learners. As usual, the <code class="inlineCode">set.seed()</code> function ensures that the result can be replicated:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>randomForest<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">300</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> rf <span class="hljs-operator">&lt;-</span> randomForest<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">For a summary of model performance, we can simply type the resulting object’s name:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> rf
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Call:
 randomForest(formula = default ~ ., data = credit) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 4
        OOB estimate of  error rate: 23.3%
Confusion matrix:
     no yes class.error
no  638  62  0.08857143
yes 171 129  0.57000000
</code></pre>
    <p class="normal">The output shows that the random forest included 500 trees and tried four variables at each split, as expected. At first glance, you might be alarmed at the seemingly poor performance according to the confusion matrix—the error rate of 23.3 percent is far worse than the resubstitution error of any of the other ensemble methods so far. However, this <a id="_idIndexMarker1640"/>confusion matrix does not show a resubstitution error. Instead, it reflects the <strong class="keyWord">out-of-bag error rate</strong> (listed in the output as <code class="inlineCode">OOB estimate of error rate</code>), which, unlike a resubstitution error, is an unbiased estimate of the test set error. This means that it should be a fair estimate of future performance.</p>
    <p class="normal">The out-of-bag <a id="_idIndexMarker1641"/>estimate is computed using a clever technique during the construction of the random forest. Essentially, any example not <a id="_idIndexMarker1642"/>selected for a single tree’s bootstrap sample can be used to test the model’s performance on unseen data. At the end of the forest construction, for each of the 1,000 examples in the dataset, any trees that did not use the example in training are allowed to make a prediction. These predictions are tallied, and a vote is taken to determine the single final prediction for the example. The total error rate of such predictions across all 1,000 examples becomes the out-of-bag error rate. Because each prediction uses only a subset of the forest, it is not equivalent to a true validation or test set estimation, but it is a reasonable substitute.</p>
    <div class="note">
      <p class="normal">In <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, it was stated that any given example has a 63.2 percent chance of being included in a bootstrap sample. This implies that an average of 36.8 percent of the 500 trees in the random forest voted for each of the 1,000 examples in the out-of-bag estimate.</p>
    </div>
    <p class="normal">To calculate the kappa statistic on the out-of-bag predictions, we can use the function in the <code class="inlineCode">vcd</code> package as follows. The code applies the <code class="inlineCode">Kappa()</code> function to the first two rows and columns of the <code class="inlineCode">confusion</code> object, which stores the confusion matrix of the out-of-bag predictions for the <code class="inlineCode">rf</code> random forest model object:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>vcd<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> Kappa<span class="hljs-punctuation">(</span>rf<span class="hljs-operator">$</span>confusion<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">2</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">           value     ASE     z  Pr(&gt;|z|)
Unweighted 0.381 0.03215 11.85 2.197e-32
Weighted   0.381 0.03215 11.85 2.197e-32
</code></pre>
    <p class="normal">With a kappa statistic of <code class="inlineCode">0.381</code>, the random forest is our best-performing model yet. Its performance was better than the bagged decision tree ensemble, which had a kappa of about <code class="inlineCode">0.332</code>, as well as the AdaBoost.M1 model, which had a kappa of about <code class="inlineCode">0.340</code>.</p>
    <p class="normal">The <code class="inlineCode">ranger</code> package, as mentioned previously, is a substantially faster implementation of the random forest algorithm. For a dataset as small as the credit dataset, optimizing for computational efficiency may be less important than ease of use, and by default, <code class="inlineCode">ranger</code> sacrifices some conveniences in order to increase speed and reduce the memory footprint. Consequently, although the <code class="inlineCode">ranger</code> function is nearly identical to <code class="inlineCode">randomForest()</code> in syntax, in practice, you may find that it breaks existing code or takes a bit of digging through the help pages.</p>
    <p class="normal">To recreate <a id="_idIndexMarker1643"/>the previous model using <code class="inlineCode">ranger</code>, we simply <a id="_idIndexMarker1644"/>change the function name:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>ranger<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">300</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> m_ranger <span class="hljs-operator">&lt;-</span> ranger<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting model has quite a similar out-of-bag prediction error:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> m_ranger
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Ranger result
Call:
 ranger(default ~ ., data = credit) 
Type:                             Classification 
Number of trees:                  500 
Sample size:                      1000 
Number of independent variables:  16 
Mtry:                             4 
Target node size:                 1 
Variable importance mode:         none 
Splitrule:                        gini 
OOB prediction error:             23.10 %
</code></pre>
    <p class="normal">We can compute kappa much as before while noting the slight difference in how the model’s confusion matrix sub-object was named:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> Kappa<span class="hljs-punctuation">(</span>m_ranger<span class="hljs-operator">$</span>confusion.matrix<span class="hljs-punctuation">)</span>
           value    ASE     z  Pr<span class="hljs-punctuation">(</span><span class="hljs-operator">&gt;|</span>z<span class="hljs-operator">|</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Unweighted 0.381 0.0321 11.87 1.676e-32
Weighted   0.381 0.0321 11.87 1.676e-32
</code></pre>
    <p class="normal">The kappa value is <code class="inlineCode">0.381</code>, which is the same as the result from the earlier random forest model. Note that this is coincidental, as the two algorithms are not guaranteed to produce identical results.</p>
    <div class="packt_tip">
      <p class="normal">As with AdaBoost, the prior results were obtained using R version 4.2.3 on a Windows PC and verified on Linux. At the time this was written, slightly different results are obtained using R 4.2.3 for Apple silicon on a recent MacBook Pro.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-327">Gradient boosting</h3>
    <p class="normal"><strong class="keyWord">Gradient boosting</strong><strong class="keyWord"><a id="_idIndexMarker1645"/></strong> is an evolution of the <a id="_idIndexMarker1646"/>boosting algorithm based on the finding that it is possible to treat the boosting process as an optimization problem to be solved using the gradient descent technique. We first encountered gradient descent in <em class="chapterRef">Chapter 7</em>, <em class="italic">Black-Box Methods – Neural Networks and Support Vector Machines</em>, where it was introduced as a solution to optimize the weights in a neural network. You may recall that a cost function—essentially, the prediction error—relates the input values to the target. Then, by systematically analyzing how changes to the weights affect the cost, it is possible to find the set of weights that minimizes the cost. Gradient boosting treats the process of boosting in much the same way, with the weak learners in the ensemble being treated as the parameters to optimize. Models using this technique are termed <strong class="keyWord">gradient boosting machines</strong> or <strong class="keyWord">generalized boosting models</strong>—both of which can be abbreviated as <strong class="keyWord">GBMs</strong>.</p>
    <div class="note">
      <p class="normal">For more on GBMs, see <em class="italic">Greedy Function Approximation: A Gradient Boosting Machine, Friedman JH, 2001, Annals of Statistics 29(5):1189-1232</em>.</p>
    </div>
    <p class="normal">The following table summarizes the strengths and weaknesses of GBMs. In short, gradient boosting is extremely powerful and can produce some of the most accurate models but may require tuning to find the balance between over- and underfitting.</p>
    <table class="table-container" id="table004-2">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">An all-purpose classifier that can perform extremely well on both classification and numeric prediction</li>
              <li class="bulletList">Can achieve even better performance than random forests</li>
              <li class="bulletList">Performs well on large datasets</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">May require tuning to match the performance of the random forest algorithm and more extensive tuning to exceed its performance</li>
              <li class="bulletList">Because there are several hyperparameters to tune, finding the best combination requires many iterations and more computing power</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">We’ll use the <code class="inlineCode">gbm()</code> function in the <code class="inlineCode">gbm</code> package for creating GBMs for both <a id="_idIndexMarker1647"/>classification and numeric prediction. You’ll need to install and load this package to your R session if you haven’t already. As the following box shows, the syntax is like the<a id="_idIndexMarker1648"/> machine learning functions used <a id="_idIndexMarker1649"/>previously, but it has several new parameters that may need to be adjusted. These parameters control the complexity of the model and the balance between over- and underfitting. Without tuning, the GBM may not perform as well as simpler methods, but it generally can surpass the performance of most other methods once parameter values have been optimized.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_14_07.png"/></figure>
    <p class="packt_figref">Figure 14.7: Gradient boosting machine (GBM) syntax</p>
    <p class="normal">We can train a simple GBM to predict loan defaults on the <code class="inlineCode">credit</code> dataset as follows. For simplicity, we set <code class="inlineCode">stringsAsFactors = TRUE</code> to avoid recoding the predictors, but<a id="_idIndexMarker1650"/> then the target <code class="inlineCode">default</code> feature must be converted back to a binary outcome, as the <code class="inlineCode">gbm()</code> function requires this for binary classification. We’ll create a random <a id="_idIndexMarker1651"/>sample for training and testing, then apply the <code class="inlineCode">gbm()</code> function to the training data, leaving the parameters set to their defaults:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"credit.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit<span class="hljs-operator">$</span>default <span class="hljs-operator">&lt;-</span> ifelse<span class="hljs-punctuation">(</span>credit<span class="hljs-operator">$</span>default <span class="hljs-operator">==</span> <span class="hljs-string">"yes"</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">123</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> train_sample <span class="hljs-operator">&lt;-</span> sample<span class="hljs-punctuation">(</span><span class="hljs-number">1000</span><span class="hljs-punctuation">,</span> <span class="hljs-number">900</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit_train <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span>train_sample<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
<span class="hljs-operator">&gt;</span> credit_test  <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span><span class="hljs-operator">-</span>train_sample<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
<span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>gbm<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">300</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> m_gbm <span class="hljs-operator">&lt;-</span> gbm<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit_train<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Typing the name of the model provides some basic information about the GBM process:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> m_gbm
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">gbm(formula = default ~ ., data = credit_train)
A gradient boosted model with 59 bernoulli loss function.
100 iterations were performed.
There were 16 predictors of which 14 had non-zero influence.
</code></pre>
    <p class="normal">More importantly, we can evaluate the model on the test set. Note that we need to convert the predictions to binary, as they are given as probabilities. If the probability of loan default is greater than 50 percent, we will predict default, otherwise, we predict non-default. The table shows the agreement between the predicted and actual values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> p_gbm <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>m_gbm<span class="hljs-punctuation">,</span> credit_test<span class="hljs-punctuation">,</span> type <span class="hljs-operator">=</span> <span class="hljs-string">"response"</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> p_gbm_c <span class="hljs-operator">&lt;-</span> ifelse<span class="hljs-punctuation">(</span>p_gbm <span class="hljs-operator">&gt;</span> <span class="hljs-number">0.50</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>credit_test<span class="hljs-operator">$</span>default<span class="hljs-punctuation">,</span> p_gbm_c<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   p_gbm_c
1
  0 60  5
  1 21 14
</code></pre>
    <p class="normal">To measure the performance, we’ll apply the <code class="inlineCode">Kappa()</code> function to this table:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>vcd<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> Kappa<span class="hljs-punctuation">(</span>table<span class="hljs-punctuation">(</span>credit_test<span class="hljs-operator">$</span>default<span class="hljs-punctuation">,</span> p_gbm_c<span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">            value     ASE    z  Pr(&gt;|z|)
Unweighted 0.3612 0.09529 3.79 0.0001504
Weighted   0.3612 0.09529 3.79 0.0001504
</code></pre>
    <p class="normal">The resulting kappa <a id="_idIndexMarker1652"/>value of about <code class="inlineCode">0.361</code> is better than what was obtained with the boosted decision tree, but worse than the random forest model. Perhaps with a bit of tuning, we can get this higher.</p>
    <p class="normal">We’ll use the <code class="inlineCode">caret</code> package to tune the GBM model and obtain a more robust performance <a id="_idIndexMarker1653"/>measure. Recall that tuning needs a search grid, which we can define for GBM as follows. This will test three values for three of the <code class="inlineCode">gbm()</code> function parameters and one value for the remaining parameter, which results in <em class="italic">3 * 3 * 3 * 1 = 27</em> models to evaluate:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> grid_gbm <span class="hljs-operator">&lt;-</span> expand.grid<span class="hljs-punctuation">(</span>
    n.trees <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">100</span><span class="hljs-punctuation">,</span> <span class="hljs-number">150</span><span class="hljs-punctuation">,</span> <span class="hljs-number">200</span><span class="hljs-punctuation">),</span>
    interaction.depth <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">3</span><span class="hljs-punctuation">),</span>
    shrinkage <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">0.01</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0.1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0.3</span><span class="hljs-punctuation">),</span>
    n.minobsinnode <span class="hljs-operator">=</span> <span class="hljs-number">10</span>
  <span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Next, we set the <code class="inlineCode">trainControl</code> object to select the best model from a 10-fold CV experiment:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>caret<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> ctrl <span class="hljs-operator">&lt;-</span> trainControl<span class="hljs-punctuation">(</span>method <span class="hljs-operator">=</span> <span class="hljs-string">"cv"</span><span class="hljs-punctuation">,</span> number <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-punctuation">,</span>
                       selectionFunction <span class="hljs-operator">=</span> <span class="hljs-string">"best"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Lastly, we read in the <code class="inlineCode">credit</code> dataset and supply the required objects to the <code class="inlineCode">caret()</code> function while specifying the <code class="inlineCode">gbm</code> method and the <code class="inlineCode">Kappa</code> performance metric. Depending on the capabilities of your computer, this may take a few minutes to run:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"credit.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">300</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> m_gbm_c <span class="hljs-operator">&lt;-</span> train<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">,</span> method <span class="hljs-operator">=</span> <span class="hljs-string">"gbm"</span><span class="hljs-punctuation">,</span>
                   trControl <span class="hljs-operator">=</span> ctrl<span class="hljs-punctuation">,</span> tuneGrid <span class="hljs-operator">=</span> grid_gbm<span class="hljs-punctuation">,</span>
                   metric <span class="hljs-operator">=</span> <span class="hljs-string">"Kappa"</span><span class="hljs-punctuation">,</span>
                   verbose <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Typing the name of the object shows the results of the experiment. Note that some lines of output have <a id="_idIndexMarker1654"/>been omitted for brevity, but the full output contains 27 rows—one for each <a id="_idIndexMarker1655"/>model evaluated:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> m_gbm_c
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Stochastic Gradient Boosting 
1000 samples
  16 predictor
   2 classes: 'no', 'yes' 
No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... 
Resampling results across tuning parameters:
  shrinkage  interaction.depth  n.trees  Accuracy  Kappa      
  0.10       1                  100      0.737     0.269966697
  0.10       1                  150      0.738     0.295886773
  0.10       1                  200      0.742     0.320157816
  0.10       2                  100      0.747     0.327928587
  0.10       2                  150      0.750     0.347848347
  0.10       2                  200      0.759     0.380641164
  0.10       3                  100      0.747     0.342691964
  0.10       3                  150      0.748     0.356836684
  0.10       3                  200      0.764     0.394578005
Tuning parameter 'n.minobsinnode' was held constant at a value of 10
Kappa was used to select the optimal model using the largest value.
The final values used for the model were n.trees = 200,
interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.
</code></pre>
    <p class="normal">From the output, we can see that the best GBM model had a kappa of <code class="inlineCode">0.394</code>, which exceeds the random forest trained previously. With additional tuning, it may be possible to bring the kappa up even higher. Or, as you will see in the next section, a more intensive form of boosting can be employed in the pursuit of even better performance.</p>
    <h3 class="heading-3" id="_idParaDest-328">Extreme gradient boosting with XGBoost</h3>
    <p class="normal">A <a id="_idIndexMarker1656"/>cutting-edge implementation of the<a id="_idIndexMarker1657"/> gradient boosting technique can be found in the <strong class="keyWord">XGBoost </strong>algorithm (<code class="inlineCode">https://xgboost.ai</code>), which <a id="_idIndexMarker1658"/>takes boosting to the “extreme” by<a id="_idIndexMarker1659"/> improving the algorithm’s efficiency and performance. In the time since the algorithm was introduced in 2014, XGBoost has been found on top of the leaderboards of many machine learning competitions. In fact, according to the algorithm’s authors, among 29 winning<a id="_idIndexMarker1660"/> solutions on Kaggle in 2015, a total of 17 used the XGBoost algorithm. Likewise, in the 2015 KDD Cup (described in <em class="chapterRef">Chapter 11</em>, <em class="italic">Being Successful with Machine Learning</em>), all of the top 10 winners used XGBoost. Today, the algorithm is still the champion for traditional machine learning problems involving classification and numeric prediction, whereas its closest <a id="_idIndexMarker1661"/>challenger, deep neural networks, tends to win only on unstructured data, such as image, audio, and text processing.</p>
    <div class="note">
      <p class="normal">For more information on XGBoost, see <em class="italic">XGBoost: A Scalable Tree Boosting System, Chen T and Guestrin C, 2016</em>. <a href="https://arxiv.org/abs/1603.02754"><span class="url">https://arxiv.org/abs/1603.02754</span></a>.</p>
    </div>
    <p class="normal">The great power of the XGBoost algorithm comes with the downside that the algorithm is not quite as easy to use and requires substantially more tuning than other methods examined so far. On the other hand, its performance ceiling tends to be higher than any other approach. The strengths and weaknesses of XGBoost are found in the following table:</p>
    <table class="table-container" id="table005">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">An all-purpose classifier that can perform extremely well on both classification and numeric prediction</li>
              <li class="bulletList">Perhaps undisputedly, the current champion of performance on traditional learning problems; wins virtually every machine learning competition on structured data</li>
              <li class="bulletList">Highly scalable, performs well on large datasets, and can be run in parallel on distributed computing platforms</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">More challenging to use than other functions, as it relies on external frameworks that do not use native R data structures</li>
              <li class="bulletList">Requires extensive tuning of a large set of hyperparameters that can be difficult to understand without a strong math background</li>
              <li class="bulletList">Because there are many tuning parameters, finding the best combination requires many iterations and more computing power</li>
              <li class="bulletList">Results in a “black box” model that is nearly impossible to interpret without explainability tools</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">To apply the algorithm, we’ll use the <code class="inlineCode">xgboost()</code> function in the <code class="inlineCode">xgboost</code> package, which provides an R interface to the XGBoost framework. Entire books could be written about this framework, as it includes features for many types of machine learning tasks, and is highly extensible<a id="_idIndexMarker1662"/> and adaptable to many high-performance computing environments. For more information about the XGBoost framework, see the excellent documentation on the web at <a href="https://xgboost.readthedocs.io"><span class="url">https://xgboost.readthedocs.io</span></a>. Our work will focus on a narrow slice of its functionality, as shown in the following syntax box, which is much denser than those<a id="_idIndexMarker1663"/> for other algorithms due to a large increase in complexity and hyperparameters that may be tuned:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_14_08.png"/></figure>
    <p class="packt_figref">Figure 14.8: XGBoost (XGB) syntax</p>
    <p class="normal">One of the challenges with using XGBoost in R is its need to use data in matrix format rather<a id="_idIndexMarker1664"/> than R’s preferred formats of tibbles or data frames. Because XGBoost is designed for extremely large <a id="_idIndexMarker1665"/>datasets, it can also use sparse matrices, such as those discussed in previous chapters. You may recall that a sparse matrix only stores non-zero values, which makes it more memory-efficient than traditional matrices when many feature values are zeros.</p>
    <p class="normal">Data in matrix form is often sparse because factors are typically one-hot or dummy coded during the transition between the data frame and matrix. These encodings create additional columns for additional levels of the factor, and all columns are set to zero except the one “hot” value that indicates the level for the given example. In the case of dummy coding, one feature level is left out of the transformation, so it results in one fewer column than one-hot; the missing level can be indicated by the presence of zeros in all of the <em class="italic">n</em> - <em class="italic">1</em> columns.</p>
    <div class="packt_tip">
      <p class="normal">One-hot and dummy coding generally produce the same results, with the exception that statistics-based models like regression require dummy coding and will present errors or warning messages if one-hot is used instead.</p>
    </div>
    <p class="normal">Let’s begin by reading the <code class="inlineCode">credit.csv</code> file and creating a sparse matrix of data from the <code class="inlineCode">credit</code> data frame. The <code class="inlineCode">Matrix</code> package provides a function to perform this task, which uses the R formula interface to determine the columns to include in the matrix. Here, the formula <code class="inlineCode">~ . -default</code> tells the function to use all features except <code class="inlineCode">default</code>, which we don’t want in the matrix, as this is our target feature for prediction:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"credit.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>Matrix<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit_matrix <span class="hljs-operator">&lt;-</span> sparse.model.matrix<span class="hljs-punctuation">(</span><span class="hljs-operator">~</span> . <span class="hljs-operator">-</span>default<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">To confirm our work, let’s check the dimensions of the matrix:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-built_in">dim</span><span class="hljs-punctuation">(</span>credit_matrix<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 1000   36
</code></pre>
    <p class="normal">We still have 1,000 rows, but the columns have increased from 16 features in the original data frame to 36 in the sparse matrix. This is due to the dummy coding that was applied automatically when converting to matrix form. We can see this if we examine the first five rows and 15 columns of the sparse matrix using the <code class="inlineCode">print()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> print<span class="hljs-punctuation">(</span>credit_matrix<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">5</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">15</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">5 x 15 sparse Matrix of class "dgCMatrix"
   [[ suppressing 15 column names '(Intercept)', 'checking_balance&gt; 200 DM', 'checking_balance1 - 200 DM' ... ]]
                                   
1 1 . . .  6 . . . . . . . 1 . 1169
2 1 . 1 . 48 1 . . . . . . 1 . 5951
3 1 . . 1 12 . . . . . . 1 . . 2096
4 1 . . . 42 1 . . . . . . 1 . 7882
5 1 . . . 24 . . 1 . 1 . . . . 4870
</code></pre>
    <p class="normal">The<a id="_idIndexMarker1666"/> matrix is depicted with the dot (<code class="inlineCode">.</code>) character indicating cells with zero values. The first column (<code class="inlineCode">1</code>, <code class="inlineCode">2</code>, <code class="inlineCode">3</code>, <code class="inlineCode">4</code>, <code class="inlineCode">5</code>) is <a id="_idIndexMarker1667"/>the row number and the second column (<code class="inlineCode">1</code>, <code class="inlineCode">1</code>, <code class="inlineCode">1</code>, <code class="inlineCode">1</code>, <code class="inlineCode">1</code>) is a column for the intercept term, which was added automatically by the R formula interface. Two columns have numbers (<code class="inlineCode">6</code>, <code class="inlineCode">48</code>, …) and (<code class="inlineCode">1169</code>, <code class="inlineCode">5951</code>, …) that correspond to the numeric values of the <code class="inlineCode">months_loan_duration</code> and <code class="inlineCode">amount</code> features, respectively. All other columns are dummy-coded versions of factor variables. For instance, the third, fourth, and fifth columns reflect the <code class="inlineCode">checking_balance</code> feature, with a <code class="inlineCode">1</code> in the third column indicating a value of <code class="inlineCode">'&gt; 200 DM'</code>, a <code class="inlineCode">1</code> in the fourth column indicating <code class="inlineCode">'1 – 200 DM'</code>, and a 1 in the fifth column indicating the <code class="inlineCode">'unknown'</code> feature value. Rows showing the sequence <code class="inlineCode">. . .</code> in columns 3, 4, and 5 fall into the reference category, which was the <code class="inlineCode">'&lt; 0 DM'</code> feature level.</p>
    <p class="normal">Since we are not building a regression model, the intercept column full of <code class="inlineCode">1</code> values is useless for this analysis and can be removed from the matrix:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_matrix <span class="hljs-operator">&lt;-</span> credit_matrix<span class="hljs-punctuation">[,</span> <span class="hljs-operator">-</span><span class="hljs-number">1</span><span class="hljs-punctuation">]</span>
</code></pre>
    <p class="normal">Next, we’ll split the matrix at random into training and test sets using a 90-10 split as we’ve done before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">12345</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> train_ids <span class="hljs-operator">&lt;-</span> sample<span class="hljs-punctuation">(</span><span class="hljs-number">1000</span><span class="hljs-punctuation">,</span> <span class="hljs-number">900</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit_train <span class="hljs-operator">&lt;-</span> credit_matrix<span class="hljs-punctuation">[</span>train_ids<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
<span class="hljs-operator">&gt;</span> credit_test <span class="hljs-operator">&lt;-</span> credit_matrix<span class="hljs-punctuation">[</span><span class="hljs-operator">-</span>train_ids<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
</code></pre>
    <p class="normal">To confirm the work was done correctly, we’ll check the dimensions of these matrices:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-built_in">dim</span><span class="hljs-punctuation">(</span>credit_train<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 900  35
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-built_in">dim</span><span class="hljs-punctuation">(</span>credit_test<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 100  35
</code></pre>
    <p class="normal">As <a id="_idIndexMarker1668"/>expected, the<a id="_idIndexMarker1669"/> training set has 900 rows and 35 columns, and the test set has 100 rows and a matching set of columns.</p>
    <p class="normal">Lastly, we’ll create training and test vectors of labels for <code class="inlineCode">default</code>, the target to be predicted. These are transformed from factors to binary <code class="inlineCode">1</code> or <code class="inlineCode">0</code> values using an <code class="inlineCode">ifelse()</code> function so that they can be used to train and evaluate the XGBoost model, respectively:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_train_labels <span class="hljs-operator">&lt;-</span>
    ifelse<span class="hljs-punctuation">(</span>credit<span class="hljs-punctuation">[</span>train_ids<span class="hljs-punctuation">,</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"default"</span><span class="hljs-punctuation">)]</span> <span class="hljs-operator">==</span> <span class="hljs-string">"yes"</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit_test_labels <span class="hljs-operator">&lt;-</span>
    ifelse<span class="hljs-punctuation">(</span>credit<span class="hljs-punctuation">[</span><span class="hljs-operator">-</span>train_ids<span class="hljs-punctuation">,</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"default"</span><span class="hljs-punctuation">)]</span> <span class="hljs-operator">==</span> <span class="hljs-string">"yes"</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">We’re now ready to start building the model. After installing the <code class="inlineCode">xgboost</code> package, we’ll load the library and start to define the hyperparameters for training. Without knowing where else to begin, we’ll set the values to their defaults:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>xgboost<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> params.xgb <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">list</span><span class="hljs-punctuation">(</span>objective   <span class="hljs-operator">=</span> <span class="hljs-string">"binary:logistic"</span><span class="hljs-punctuation">,</span>
                     max_depth   <span class="hljs-operator">=</span> <span class="hljs-number">6</span><span class="hljs-punctuation">,</span>
                     eta         <span class="hljs-operator">=</span> <span class="hljs-number">0.3</span><span class="hljs-punctuation">,</span>
                     <span class="hljs-built_in">gamma</span>       <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>
                     colsample_bytree <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span>
                     min_child_weight <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span>
                     subsample <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Next, after setting the random seed, we’ll train the model, supplying our parameters object as well as the matrix of training data and the target labels. The <code class="inlineCode">nrounds</code> parameter determines the number of boosting iterations. Without a better guess, we’ll set this to <code class="inlineCode">100</code>, which is a common starting point due to empirical evidence suggesting that results tend to improve very little beyond this value. Lastly, the <code class="inlineCode">verbose</code> and <code class="inlineCode">print_every_n</code> options are used to turn on diagnostic output and display the progress after every 10 boosting iterations:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">555</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> xgb_credit <span class="hljs-operator">&lt;-</span> xgboost<span class="hljs-punctuation">(</span>params  <span class="hljs-operator">=</span> params.xgb<span class="hljs-punctuation">,</span>
                        data    <span class="hljs-operator">=</span> credit_train<span class="hljs-punctuation">,</span>
                        label   <span class="hljs-operator">=</span> credit_train_labels<span class="hljs-punctuation">,</span> 
                        nrounds <span class="hljs-operator">=</span> <span class="hljs-number">100</span><span class="hljs-punctuation">,</span>
                        verbose <span class="hljs-operator">=</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span>
                        print_every_n <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1670"/>output should appear as the <a id="_idIndexMarker1671"/>training is completed, showing that all 100 iterations occurred and the training error (labeled <code class="inlineCode">train-logloss</code>) continued to decline with additional rounds of boosting:</p>
    <pre class="programlisting con"><code class="hljs-con">[1] train-logloss:0.586271 
[11]	train-logloss:0.317767 
[21]	train-logloss:0.223844 
[31]	train-logloss:0.179252 
[41]	train-logloss:0.135629 
[51]	train-logloss:0.108353 
[61]	train-logloss:0.090580 
[71]	train-logloss:0.077314 
[81]	train-logloss:0.065995 
[91]	train-logloss:0.057018 
[100] train-logloss:0.050837 
</code></pre>
    <p class="normal">Knowing whether additional iterations would help the model performance or result in overfitting is something we can determine via tuning later. Before doing so, let’s look at the performance of this trained model on the test set, which we held out earlier. First, the <code class="inlineCode">predict()</code> function obtains the predicted probability of loan default for each row of test data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> prob_default <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>xgb_credit<span class="hljs-punctuation">,</span> credit_test<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Then, we use <code class="inlineCode">ifelse()</code> to predict a default (value <code class="inlineCode">1</code>) if the probability of a default is at least 0.50, or non-default (value <code class="inlineCode">0</code>) otherwise:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> pred_default <span class="hljs-operator">&lt;-</span> ifelse<span class="hljs-punctuation">(</span>prob_default <span class="hljs-operator">&gt;</span> <span class="hljs-number">0.50</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Comparing the predicted to actual values, we find an accuracy of <em class="italic">(62 + 14) / 100 = 76</em> percent:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>pred_default<span class="hljs-punctuation">,</span> credit_test_labels<span class="hljs-punctuation">)</span>
            credit_test_labels
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">pred_default  0  1
           0 62 13
           1 11 14
</code></pre>
    <p class="normal">On the other hand, the kappa statistic suggests there is still room to improve:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>vcd<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> Kappa<span class="hljs-punctuation">(</span>table<span class="hljs-punctuation">(</span>pred_default<span class="hljs-punctuation">,</span> credit_test_labels<span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">            value    ASE     z  Pr(&gt;|z|)
Unweighted 0.3766 0.1041 3.618 0.0002967
Weighted   0.3766 0.1041 3.618 0.0002967
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1672"/>value of <code class="inlineCode">0.3766</code> is a bit <a id="_idIndexMarker1673"/>lower than the <code class="inlineCode">0.394</code> we obtained with the GBM model, so perhaps a bit of hyperparameter tuning can help. For this, we’ll use <code class="inlineCode">caret</code>, starting with a tuning grid comprising a variety of options for each of the hyperparameters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> grid_xgb <span class="hljs-operator">&lt;-</span> expand.grid<span class="hljs-punctuation">(</span>
    eta <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">0.3</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0.4</span><span class="hljs-punctuation">),</span>
    max_depth <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">3</span><span class="hljs-punctuation">),</span>
    colsample_bytree <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">0.6</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0.8</span><span class="hljs-punctuation">),</span>
    subsample <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">0.50</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0.75</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1.00</span><span class="hljs-punctuation">),</span>
    nrounds <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">50</span><span class="hljs-punctuation">,</span> <span class="hljs-number">100</span><span class="hljs-punctuation">,</span> <span class="hljs-number">150</span><span class="hljs-punctuation">),</span>
    <span class="hljs-built_in">gamma</span> <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">),</span>
    min_child_weight <span class="hljs-operator">=</span> <span class="hljs-number">1</span>
  <span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting grid contains <em class="italic">2 * 3 * 2 * 3 * 3 * 2 * 1 = 216</em> different combinations of <code class="inlineCode">xgboost</code> hyperparameter values. We’ll evaluate each of these potential models in <code class="inlineCode">caret</code> using 10-fold CV, as we’ve done for other models. Note that the <code class="inlineCode">verbosity</code> parameter is set to zero so that the <code class="inlineCode">xgboost()</code> function output is suppressed for the many iterations:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>caret<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span>  ctrl <span class="hljs-operator">&lt;-</span> trainControl<span class="hljs-punctuation">(</span>method <span class="hljs-operator">=</span> <span class="hljs-string">"cv"</span><span class="hljs-punctuation">,</span> number <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-punctuation">,</span>
                       selectionFunction <span class="hljs-operator">=</span> <span class="hljs-string">"best"</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"credit.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">300</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> m_xgb <span class="hljs-operator">&lt;-</span> train<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit<span class="hljs-punctuation">,</span> method <span class="hljs-operator">=</span> <span class="hljs-string">"xgbTree"</span><span class="hljs-punctuation">,</span>
                      trControl <span class="hljs-operator">=</span> ctrl<span class="hljs-punctuation">,</span> tuneGrid <span class="hljs-operator">=</span> grid_xgb<span class="hljs-punctuation">,</span>
                      metric <span class="hljs-operator">=</span> <span class="hljs-string">"Kappa"</span><span class="hljs-punctuation">,</span> verbosity <span class="hljs-operator">=</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Depending on the capabilities of your computer, the experiment may take a few minutes to complete, but once it finishes, typing <code class="inlineCode">m_xgb</code> will provide the results of all 216 models<a id="_idIndexMarker1674"/> tested. We can also<a id="_idIndexMarker1675"/> obtain the best model directly as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> m_xgb<span class="hljs-operator">$</span>bestTune
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    nrounds max_depth eta gamma colsample_bytree
        50         3  0.4     1              0.6
    min_child_weight subsample
1
</code></pre>
    <p class="normal">The kappa value for this model can be found using the <code class="inlineCode">max()</code> function to find the highest value as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-built_in">max</span><span class="hljs-punctuation">(</span>m_xgb<span class="hljs-operator">$</span>results<span class="hljs-punctuation">[</span><span class="hljs-string">"Kappa"</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.4062946
</code></pre>
    <p class="normal">The kappa value of <code class="inlineCode">0.406</code> is our best-performing model so far, exceeding the <code class="inlineCode">0.394</code> of the GBM model and the <code class="inlineCode">0.381</code> of the random forest. The fact that XGBoost required so little effort to train—with a bit of fine-tuning—yet still surpassed other powerful techniques provides examples of why it always seems to win machine learning competitions. Yet, with even more tuning, it may be possible to go higher still! Leaving that as an exercise to you, the reader, we’ll now turn our attention to the question of why all of these popular ensembles seem to focus exclusively on decision tree-based methods.</p>
    <h3 class="heading-3" id="_idParaDest-329">Why are tree-based ensembles so popular?</h3>
    <p class="normal">After <a id="_idIndexMarker1676"/>reading the prior sections, you <a id="_idIndexMarker1677"/>would not be the first person to wonder why ensembling algorithms seem to always be built upon decision trees. Although trees are not required for building an ensemble, there are several reasons why they are especially well-suited for this process. You may have noted some of them already:</p>
    <ul>
      <li class="bulletList">Ensembles work best with diversity, and because decision trees are not robust to small changes in the data, random sampling the same training data can easily create a diverse set of tree-based models</li>
      <li class="bulletList">Because of the greedy “divide-and-conquer” based algorithm, decision trees are computationally efficient and perform relatively well despite this fact</li>
      <li class="bulletList">Decision trees can be grown purposely large or small to overfit and underfit as needed</li>
      <li class="bulletList">Decision trees can automatically ignore irrelevant features, which reduces the negative impact of the “curse of dimensionality”</li>
      <li class="bulletList">Decision trees can be used for numeric prediction as well as classification</li>
    </ul>
    <p class="normal">Based <a id="_idIndexMarker1678"/>on these characteristics, it is<a id="_idIndexMarker1679"/> not difficult to see why we’ve ended up with a wealth of tree-based ensembling approaches such as bagging, boosting, and random forests. The distinctions among them are subtle but important. </p>
    <p class="normal">The following table may help contrast the tree-based ensembling algorithms covered in this chapter:</p>
    <table class="table-container" id="table006">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Ensembling Algorithm</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Allocation Function</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Combination Function</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Other Notes</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Bagging</p>
          </td>
          <td class="table-cell">
            <p class="normal">Provides each learner with a bootstrap sample of the training data </p>
          </td>
          <td class="table-cell">
            <p class="normal">The learners are combined using a vote for classification or a weighted average for numeric prediction</p>
          </td>
          <td class="table-cell">
            <p class="normal">Uses an independent ensemble — the learners can be run in parallel </p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Boosting</p>
          </td>
          <td class="table-cell">
            <p class="normal">The first learner is given a random sample; subsequent samples are weighted to have more difficult-to-predict cases</p>
          </td>
          <td class="table-cell">
            <p class="normal">The learners’ predictions are combined as above but weighted according to their performance on training data</p>
          </td>
          <td class="table-cell">
            <p class="normal">Uses a dependent ensemble — each tree in the sequence receives data that earlier trees found challenging</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Random Forest</p>
          </td>
          <td class="table-cell">
            <p class="normal">Like bagging, each tree receives a bootstrap sample of training data; however, features are also randomly selected for each tree split</p>
          </td>
          <td class="table-cell">
            <p class="normal">Similar to bagging</p>
          </td>
          <td class="table-cell">
            <p class="normal">Similar to bagging, but the added diversity via random feature selection allows additional benefits for larger ensembles</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Gradient Boosting Machine (GBM)</p>
          </td>
          <td class="table-cell">
            <p class="normal">Conceptually similar to boosting </p>
          </td>
          <td class="table-cell">
            <p class="normal">Similar to boosting, but there are many more learners and they comprise a complex mathematical function </p>
          </td>
          <td class="table-cell">
            <p class="normal">Uses gradient descent to make a more efficient boosting algorithm; the trees are generally not very deep (decision tree “stumps”) but there are many more of them; requires more tuning</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">eXtreme Gradient Boosting (XGB)</p>
          </td>
          <td class="table-cell">
            <p class="normal">Similar to GBM</p>
          </td>
          <td class="table-cell">
            <p class="normal">Similar to GBM</p>
          </td>
          <td class="table-cell">
            <p class="normal">Similar to GBM but more extreme; uses optimized data structures, parallel processing, and heuristics to create a very performant boosting algorithm; tuning is essential</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">To be <a id="_idIndexMarker1680"/>able to distinguish among these <a id="_idIndexMarker1681"/>approaches reveals a deep understanding of several aspects of ensembling. Additionally, the most recent techniques, such as random forests and gradient boosting, are among the best-performing learning algorithms and are being used as off-the-shelf solutions to solve some of the most challenging business problems. This may help explain why companies hiring data scientists and machine learning engineers often ask candidates to describe or compare these algorithms as part of the interview process. Thus, even though tree-based ensembling algorithms are not the only approach to machine learning, it is important to be aware of their potential uses. However, as the next section describes, trees aren’t the only approach to building a diverse ensemble.</p>
    <h1 class="heading-1" id="_idParaDest-330">Stacking models for meta-learning</h1>
    <p class="normal">Rather than<a id="_idIndexMarker1682"/> using a <a id="_idIndexMarker1683"/>canned ensembling method like bagging, boosting, or random forests, there are situations in which a tailored approach to ensembling is warranted. Although these tree-based ensembling techniques combine hundreds or even thousands of learners into a single, stronger learner, the process is not much different than training a traditional machine learning algorithm, and suffers some of the same limitations, albeit to a lesser degree. Being based on decision trees that have been weakly trained and minimally tuned may, in <a id="_idIndexMarker1684"/>some cases, put a ceiling on the ensemble’s performance relative to one composed of a more diverse set of learning algorithms that have been extensively tuned with the benefit of human intelligence. Furthermore, although it is possible to parallelize tree-based ensembles like random forests and XGB, this only parallelizes the computer’s effort—not the human effort of model building.</p>
    <p class="normal">Indeed, it is possible to<a id="_idIndexMarker1685"/> increase an ensemble’s diversity by not only adding additional learning algorithms but by distributing the work of model building to additional human teams working in parallel. In fact, many of the world’s competition-winning models were built by taking other teams’ best models and ensembling them together. </p>
    <p class="normal">This type of ensemble is conceptually quite simple, offering performance boosts that would be otherwise unobtainable, but can become complex in practice. Getting the implementation details correct is crucial to avoid disastrous levels of overfitting. Done correctly, the ensemble will perform at least as well as the strongest model in the ensemble, and often substantially better.</p>
    <p class="normal">Examining <strong class="keyWord">receiver operating characteristic</strong> (<strong class="keyWord">ROC</strong>) curves, as introduced in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, provides a simple method to determine whether two or more models would benefit from ensembling. If two models have intersecting ROC curves, their <strong class="keyWord">convex hull</strong>—the outermost boundary that would be obtained by stretching an imaginary rubber band around the curves—represents a hypothetical model that can be obtained by interpolating, or combining, the predictions from these models. As depicted in <em class="italic">Figure 14.9</em>, two ROC curves with identical <strong class="keyWord">area under the curve </strong>(<strong class="keyWord">AUC</strong>) values of <em class="italic">0.70</em> might create a new model with an AUC of <em class="italic">0.72</em> when paired in an ensemble:</p>
    <figure class="mediaobject"><img alt="Diagram, radar chart  Description automatically generated" src="../Images/B17290_14_09.png"/></figure>
    <p class="packt_figref">Figure 14.9: When two or more ROC curves intersect, their convex hull represents a potentially better classifier that can be generated by combining their predictions in an ensemble</p>
    <p class="normal">Because this <a id="_idIndexMarker1686"/>form of ensembling is performed largely by hand, a human <a id="_idIndexMarker1687"/>needs to provide the allocation and combination functions for the models in the ensemble. In their simplest form, these can be implemented quite pragmatically. For example, suppose that the same training dataset has been given to three different teams. This is the allocation function. These teams can use this dataset however they see fit to build the best possible model using evaluation criteria of their choosing. </p>
    <p class="normal">Next, each team is given the test set, and their models are used to make predictions, which must be combined into a single, final prediction. The combination function can take multiple different forms: the groups could vote, the predictions could be averaged, or the predictions could be weighted according to how well each group performed in the past. Even the simple approach of choosing one group at random is a viable strategy, assuming each group performs better than all others at least once in a while. Of course, even more intelligent approaches are possible, as you will soon learn.</p>
    <h2 class="heading-2" id="_idParaDest-331">Understanding model stacking and blending</h2>
    <p class="normal">Some of <a id="_idIndexMarker1688"/>the most sophisticated custom ensembles apply machine learning to learn a combination function for the final prediction. Essentially, it is trying to learn which models can and cannot be trusted. This arbiter learner may realize that one model in the ensemble is a poor performer and shouldn’t be trusted or that another deserves more weight in the ensemble. The arbiter function may also learn more complex patterns. For example, suppose that when models <em class="italic">M1</em> and <em class="italic">M2</em> agree on the outcome, the prediction is almost always accurate, but otherwise <em class="italic">M3</em> is generally more accurate than either of the two. In this case, an additional arbiter model could learn to ignore the vote of <em class="italic">M1</em> and <em class="italic">M2</em> except when they agree. This process of using the predictions <a id="_idIndexMarker1689"/>of several <a id="_idIndexMarker1690"/>models to train a final model is called <strong class="keyWord">stacking</strong>.</p>
    <figure class="mediaobject"><img alt="A picture containing text, clipart  Description automatically generated" src="../Images/B17290_14_10.png"/></figure>
    <p class="packt_figref">Figure 14.10: Stacking is a sophisticated ensemble that uses an arbiter learning algorithm to combine the predictions of a set of learners and make a final prediction</p>
    <p class="normal">More broadly, stacking falls within a methodology <a id="_idIndexMarker1691"/>known as <strong class="keyWord">stacked generalization</strong>. As formally defined, the stack is constructed using first-level models that have been trained via CV, and a second-level model or <strong class="keyWord">meta-model</strong> that is <a id="_idIndexMarker1692"/>trained using the predictions for the out-of-fold samples—the examples the model does not see during training but is tested on during the CV process. </p>
    <p class="normal">For example, suppose three first-level models are included in the stack and each one is trained using 10-fold CV. If the training dataset includes 1,000 rows, each of the three first-stage models is trained on 900 rows and tested on 100 rows ten times. The 100-row test sets, when combined, comprise the entire training dataset.</p>
    <p class="normal">As all three models have made a prediction for every row of the training data, a new table can be constructed with four columns and 1,000 rows: the first three columns represent the predictions for the three models and column four represents the true value of the target. Note that because the predictions made for each of these 100 rows were made on the other 900 rows, all 1,000 rows are predictions on unseen data. This allows the second-stage meta-model, which is often a regression or logistic regression model, to learn which first-stage models perform better by training using the predicted values as the predictors of the true value. This process of finding the optimal combination of learners is sometimes<a id="_idIndexMarker1693"/> called <strong class="keyWord">super learning</strong>, and the resulting model may be <a id="_idIndexMarker1694"/>called a <strong class="keyWord">super learner</strong>. This process is often performed by machine learning software or packages, which train numerous learning algorithms in parallel and stack them together automatically.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated with medium confidence" src="../Images/B17290_14_11.png"/></figure>
    <p class="packt_figref">Figure 14.11: In a stacked ensemble, the second-stage meta-model or “super learner” learns from the predictions of the first-stage models on out-of-fold samples</p>
    <p class="normal">For a more hands-on approach, a special case of stacked generalization<a id="_idIndexMarker1695"/> called <strong class="keyWord">blending</strong> or <strong class="keyWord">holdout stacking</strong> provides<a id="_idIndexMarker1696"/> a simplified way to implement stacking by replacing CV with a holdout sample. This allows the work to be distributed across teams more easily by merely dividing the training data into a training set for the first-level models and using a holdout set for the second-level meta-learner. It may also be less prone to overfitting the CV “information leak” described in <em class="chapterRef">Chapter 11</em>, <em class="italic">Being Successful with Machine Learning</em>. Thus, even though it is a simple approach, it can be quite effective; blending is often what competition-winning teams do when they take other models and ensemble them together for better results.</p>
    <div class="packt_tip">
      <p class="normal">The terminology around stacking, blending, and super learning is somewhat fuzzy, and many use the terms interchangeably.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-332">Practical methods for blending and stacking in R</h2>
    <p class="normal">To perform<a id="_idIndexMarker1697"/> blending in R requires a careful roadmap, as getting the details <a id="_idIndexMarker1698"/>wrong can lead to extreme overfitting and models that perform no better than random guessing. The following figure illustrates the process. Begin by imagining that you are tasked with predicting loan defaults and have access to one million rows <a id="_idIndexMarker1699"/>of past data. Immediately, you should partition the dataset into training and test sets; of course, the test set <a id="_idIndexMarker1700"/>should be kept in a vault for evaluating the ensemble later. Assume the training set is 750,000 rows and the test set is 250,000 rows. The training set must then be divided yet again to create datasets for training the level one models and the level two meta-learner. The exact proportions are somewhat arbitrary, but it is customary to use a smaller set for the second-stage model—sometimes as low as ten percent. As <em class="italic">Figure 14.12</em> depicts, we might use 500,000 rows for level one and 250,000 rows for level two: </p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_14_12.png"/></figure>
    <p class="packt_figref">Figure 14.12: The full training dataset must be divided into distinct subsets for training the level one and level two models</p>
    <p class="normal">The 500,000-row level one training dataset is used to train the first-level models exactly as we have done many times throughout this book. The <em class="italic">M1</em>, <em class="italic">M2</em>, and <em class="italic">M3</em> models may use any learning algorithm, and the work of building these models can even be distributed across different teams working independently. </p>
    <p class="normal">There is no need for the models or teams to use the same set of features from the training data or the same form of feature engineering, assuming each team’s feature engineering pipeline can be replicated or automated in the future when the ensemble is to be deployed. The important thing is that <em class="italic">M1</em>, <em class="italic">M2</em>, and <em class="italic">M3</em> should be able to take a dataset with identical features and produce a prediction for each row.</p>
    <p class="normal">The 250,000-row level two training dataset is then fed into the <em class="italic">M1</em>, <em class="italic">M2</em>, and <em class="italic">M3</em> models after being processed <a id="_idIndexMarker1701"/>through their associated feature engineering pipelines, and three vectors of 250,000 predictions are obtained. These vectors are labeled <em class="italic">p1</em>, <em class="italic">p2</em>, and <em class="italic">p3</em> in the diagram. When <a id="_idIndexMarker1702"/>combined with the 250,000 true values of the target (labeled <em class="italic">c</em> in the diagram) obtained from the level two training dataset, a four-column data frame is produced, as depicted in <em class="italic">Figure 14.13</em>: </p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_14_13.png"/></figure>
    <p class="packt_figref">Figure 14.13: The dataset used to train the meta-model is composed of the predictions from the first-level models and the actual target value from the level two training data</p>
    <p class="normal">This type of data frame is used to create a meta-model, typically using regression or logistic regression, which predicts the actual target value (<em class="italic">c</em> in <em class="italic">Figure 14.12</em>) using the predictions of <em class="italic">M1</em>, <em class="italic">M2</em>, and <em class="italic">M3</em> (<em class="italic">p1</em>, <em class="italic">p2</em>, and <em class="italic">p3</em> in <em class="italic">Figure 14.12</em>) as predictors. In an R formula, this might be specified in a form like <code class="inlineCode">c ~ p1 + p2 + p3</code>, which results in a model that weighs the input from three different predictions to make its own final prediction.</p>
    <p class="normal">To estimate the future performance of this final meta-model, we must use the 250,000-row test set, which, as illustrated in <em class="italic">Figure 14.12 </em>previously, was held out during the training process. As shown in <em class="italic">Figure 14.14</em>, the test dataset is then fed to the <em class="italic">M1</em>, <em class="italic">M2</em>, and <em class="italic">M3</em> models and their associated feature engineering pipelines, and much like in the previous step, three vectors of 250,000 predictions are obtained. However, rather than <em class="italic">p1</em>, <em class="italic">p2</em>, and <em class="italic">p3</em> being used to train a meta-model, they are now used as predictors for the existing meta-model to obtain a final prediction (labeled <em class="italic">p4</em>) for each of the 250,000 test cases. This vector can be compared to the 250,000 true values of the target in the test set to perform the performance evaluation and obtain an unbiased estimate of the ensemble’s future performance.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_14_14.png"/></figure>
    <p class="packt_figref">Figure 14.14: To obtain an unbiased estimate of the ensemble’s future performance, the test set is used to generate predictions for the level one models, which are then used to obtain the meta-model’s final predictions</p>
    <p class="normal">The above <a id="_idIndexMarker1703"/>methodology is flexible to create other interesting <a id="_idIndexMarker1704"/>types of ensembles. <em class="italic">Figure 14.15</em> illustrates a blended ensemble that combines models trained on completely different subsets of features. Specifically, it envisions a learning task in which Twitter profile data is used to make a prediction about the user—perhaps their gender or whether they would be interested in purchasing a particular product:</p>
    <figure class="mediaobject"><img alt="Diagram, timeline  Description automatically generated" src="../Images/B17290_14_15.png"/></figure>
    <p class="packt_figref">Figure 14.15: The stack’s first-level models can be trained on different features in the training set, while the second-level model is trained on their predictions</p>
    <p class="normal">The first model receives the profile’s picture and trains a deep learning neural network with the image data to predict the outcome. Model two receives a set of tweets for the user and uses a text-based model like Naive Bayes to predict the outcome. Lastly, model three is a more conventional model using a traditional data frame of demographic data like location, total number of tweets, last login date, and so on. </p>
    <p class="normal">All three models are combined, and the meta-model can learn whether the image, text, or profile data is most helpful<a id="_idIndexMarker1705"/> for predicting the gender or purchasing behavior. Alternatively, because the meta-model is a logistic regression model like <em class="italic">M3</em>, it would be possible to supply the profile data directly to the second-stage model and skip the construction of <em class="italic">M3</em> altogether.</p>
    <p class="normal">Aside from <a id="_idIndexMarker1706"/>constructing blended ensembles by hand as described here, there is a growing set of R packages to assist with this process. The <code class="inlineCode">caretEnsemble</code> package can assist with ensemble models trained with the <code class="inlineCode">caret</code> package and ensure that the stack’s sampling is handled correctly for stacking or blending. The <code class="inlineCode">SuperLearner</code> package provides an easy way to create a super learner; it can apply dozens of base algorithms to the same dataset and stack them together automatically. As an off-the-shelf algorithm, this may be useful for building a powerful ensemble with the least amount of effort.</p>
    <h1 class="heading-1" id="_idParaDest-333">Summary</h1>
    <p class="normal">After reading this chapter, you should now know the approaches that are used to win data mining and machine learning competitions. Automated tuning methods can assist with squeezing every bit of performance out of a single model. On the other hand, tremendous gains are possible by creating groups of machine learning models called ensembles, which work together to achieve greater performance than single models can by working alone. A variety of tree-based algorithms, including random forests and gradient boosting, provide the benefits of ensembles but can be trained as easily as a single model. On the other hand, learners can be stacked or blended into ensembles by hand, which allows the approach to be carefully tailored to a learning problem.</p>
    <p class="normal">With a variety of options for improving the performance of a model, where should someone begin? There is no single best approach, but practitioners tend to fall into one of three camps. First, some begin with one of the more sophisticated ensembles such as random forests or XGBoost, and spend most of their time tuning and feature engineering to achieve the highest possible performance for this model. A second group might try a variety of approaches, then collect the models into a single stacked or blended ensemble to create a more powerful learner. The third approach might be described as “throw everything at the computer and see what sticks.” This attempts to feed the learning algorithm as much data as possible and as quickly as possible, and is sometimes combined with automated feature engineering or dimensionality reduction techniques like those described in the previous chapters. With practice, you may be drawn to some of these ideas more than others, so feel free to use whichever works best for you.</p>
    <p class="normal">Although this chapter was designed to help you prepare competition-ready models, note that your fellow competitors have access to the same techniques. You won’t be able to get away with stagnancy; therefore, continue to add proprietary methods to your bag of tricks. Perhaps you can bring unique subject-matter expertise to the table, or perhaps your strengths include an eye for detail in data preparation. In any case, practice makes perfect, so take advantage of competitions to test, evaluate, and improve your machine learning skillset. In the next chapter—the last in this book—we’ll look at ways to apply cutting-edge “big data” techniques to some highly specialized and difficult data tasks using R.</p>
    <h1 class="heading-1" id="_idParaDest-334">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>