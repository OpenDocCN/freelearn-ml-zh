<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer018">
<h1 class="chapter-number" id="_idParaDest-31"><a id="_idTextAnchor034"/>2</h1>
<h1 id="_idParaDest-32"><a id="_idTextAnchor035"/>Annotating Real Data</h1>
<p>The fuel of the <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) engine is data. Data is available in almost every part of our technology-driven world. ML models usually need to be trained or evaluated on annotated data, not just data! Thus, data by itself is not very useful for ML but annotated data is what ML <span class="No-Break">models need.</span></p>
<p>In this chapter, we will learn why ML models need annotated data. We will see why the annotation process is expensive, error-prone, and biased. At the same time, you will be introduced to the annotation process for a number of ML tasks, such as <strong class="bold">image classification</strong>, <strong class="bold">semantic segmentation</strong>, and <strong class="bold">instance segmentation</strong>. We will highlight the main annotation problems. At the same time, we will understand why ideal ground truth generation is impossible or extremely difficult for tasks such as <strong class="bold">optical flow estimation</strong> and <span class="No-Break"><strong class="bold">depth estimation</strong></span><span class="No-Break">.</span></p>
<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>The need to annotate real data <span class="No-Break">for ML</span></li>
<li>Issues with the <span class="No-Break">annotation process</span></li>
<li>Optical flow and depth estimation: ground truth <span class="No-Break">and annotation</span></li>
</ul>
<h1 id="_idParaDest-33"><a id="_idTextAnchor036"/>Annotating data for ML</h1>
<p>In this section, you learn why ML <a id="_idIndexMarker029"/>models need annotated data and not simply data! Furthermore, you will be introduced to a diverse set of <span class="No-Break">annotation tools.</span></p>
<h2 id="_idParaDest-34"><a id="_idTextAnchor037"/>Learning from data</h2>
<p>As humans, we learn<a id="_idIndexMarker030"/> differently from ML models. We just require <em class="italic">implicit</em> data annotation. However, ML models need <em class="italic">explicit</em> annotation of the data. For example, let’s say you want to train an ML model to classify cat and dog images; you cannot simply feed this model with many images of cats and dogs expecting the model to learn to differentiate between these two classes. Instead, you need to describe what each image is and then you can train your “cat-dog” classifier (see <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 2.1 – Training data for the cat-dog classifier" height="364" src="image/B18494_02_001.jpg" width="750"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Training data for the cat-dog classifier</p>
<p>It should be noted that the amazing<a id="_idIndexMarker031"/> capabilities of ML models are closely related to and highly affected by the quality and quantity of the training data and ground truth. Generally, we need humans to annotate data for two main reasons: training and testing ML models. Next, we will be looking at these in <span class="No-Break">more detail.</span></p>
<h2 id="_idParaDest-35"><a id="_idTextAnchor038"/>Training your ML model</h2>
<p>We have four main steps<a id="_idIndexMarker032"/> for training an ML model. We will look at each of them next (see <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="Figure 2.2 – Training process of a typical ML model" height="548" src="image/B18494_02_002.jpg" width="1315"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Training process of a typical ML model</p>
<h3>Initialization</h3>
<p>At the beginning<a id="_idIndexMarker033"/> of the training process, the model’s parameters<a id="_idIndexMarker034"/> should be initialized. Usually, the parameters (weights and biases) of the <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) model are set to random small numbers because this is what the <strong class="bold">stochastic optimization process</strong> expects at the beginning of the optimization process. The stochastic optimization process<a id="_idIndexMarker035"/> is a method of finding the best solution for a mathematical problem where randomness and uncertainty are involved to enhance the <span class="No-Break">search procedure.</span></p>
<h3>Prediction</h3>
<p>The model utilizes<a id="_idIndexMarker036"/> its past knowledge about the task and predicts the output<a id="_idIndexMarker037"/> given the input data. We can imagine that the model builds its own understanding of the problem by fitting a <strong class="bold">hyperplane</strong> (a decision boundary) to the training data in the training process, and then it projects any given input on this hyperplane to give us the model’s prediction for this specific input. Please remember that, at this step, we feed the model with <em class="italic">data only</em>, without any ground truth. If we go back to our “cat-dog” classifier, the model will be fed with cat and dog images and asked to predict the class or the label of <span class="No-Break">these images.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout"><em class="italic">Training</em>: The ML model develops its own comprehension of the problem by adjusting its parameters and evaluating its performance until it reaches a satisfactory understanding of the <span class="No-Break">training data.</span></p>
<p class="callout"><em class="italic">Testing</em>: After the training, the ML model is evaluated on new data to assess its performance by using various metrics, such as F1 score, precision, <span class="No-Break">and accuracy.</span></p>
<h3>Error calculation</h3>
<p>Given what<a id="_idIndexMarker038"/> the model has predicted, we now need to assess the <em class="italic">correctness</em> of this prediction. This is exactly where we need the ground truth or the annotations. At this step, we have two inputs: the model’s prediction and the ground truth. Going back to our “cat-dog” classifier, the model may wrongly predict the class of a cat image as “dog.” Now, since we have the true class of the training cat image, we can tell the model that its understanding of the cat-dog problem was wrong this time (for this training sample). Furthermore, we can calculate how close the model was by using the loss function, which depends on the type of the problem. Please note that we essentially<a id="_idIndexMarker039"/> have two types of problems in ML: <strong class="bold">classification</strong> and <strong class="bold">regression</strong> problems. In classification problems, the ML model learns to categorize training data. For example, the “cat-dog” problem<a id="_idIndexMarker040"/> is a classification problem, and the error could be 0 or 1 since you have two categories: cat or dog. On the other hand, in regression problems, the ML model learns to leverage input data to predict a continuous value. For example, assume you are training a model to predict a house’s price based on some information about the house: location, number of rooms, age, and other information. Assume that the model predicted the house’s price to be £100,000 but the actual price (from the ground truth) is £105,000. Then, the error in this case <span class="No-Break">is £5,000.</span></p>
<p>The error is the essence of the learning process in ML; it provides guidance for training the model – for example, how much it needs to update its parameters and <span class="No-Break">which parameters.</span></p>
<h3>Backpropagation</h3>
<p>This is where the learning<a id="_idIndexMarker041"/> happens. Given the calculated error, we need to update the model’s parameters or weights based on the input and error. In other words, the model needs to “debug” the cause of this error in the prediction. If the error is small, the model will slightly update its weights or understanding of the problem. On the other hand, if the error is huge, the model will need to make major changes to the weights, thus, the understanding of the problem. Going back again to the “cat-dog” classifier, at the beginning of the training process, most predictions will be wrong, thus the model will be updating its weights drastically. In contrast, when the model is close to convergence (the best possible understanding of the training data), ideally, it starts<a id="_idIndexMarker042"/> to get most of its predictions right, thus making just slight updates on <span class="No-Break">the weights.</span></p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor039"/>Testing your ML model</h2>
<p>To assess the performance<a id="_idIndexMarker043"/> of your ML model, you need annotated data too. Thus, the annotations are not just needed for training but also for testing. Usually, qualitative results are good for having an overall understanding of how the model is performing in general or in some individual interesting scenarios. However, quantitative results are the most important way to understand the ML model’s robustness, accuracy, <span class="No-Break">and precision.</span></p>
<p>Using the ground truth, we can examine our trained model’s performance on a large number of examples. Thus, there is no need to look at predictions individually as the overall average, standard deviation, and other statistics will be a good description for that. In the next section, we will delve into common issues with the annotation of <span class="No-Break">real data.</span></p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor040"/>Issues with the annotation process</h1>
<p>As we have seen so far, annotations<a id="_idIndexMarker044"/> are critical to both training and testing. Thus, any mislabeling, biased annotations, or insufficient annotated data will drastically impact the learning and evaluation process of your ML model. As you can expect, the annotation process is time-consuming, expensive, and error-prone, and this is what we will see in <span class="No-Break">this section.</span></p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor041"/>The annotation process is expensive</h2>
<p>To train state-of-the-art<a id="_idIndexMarker045"/> computer vision or <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) models, you need large-scale training<a id="_idIndexMarker046"/> data. For example, <em class="italic">BERT</em> (<a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>) was trained on BooksCorpos (800 million words) and Wikipedia (2,500 million words). Similarly, <em class="italic">ViT</em> (<a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>) was trained on ImageNet (14 million images) and JFT (303 million images). Annotating such huge datasets is extremely difficult and challenging. Furthermore, it is time-consuming and expensive. It should be noted that the time required to annotate a dataset depends on three main elements: the task or problem, dataset size, and granularity level. Next, we will be looking at each of these in <span class="No-Break">more detail.</span></p>
<h3>Task</h3>
<p>For example, annotating a dataset<a id="_idIndexMarker047"/> for a binary classification problem is easier and requires less time compared to annotating a dataset for semantic segmentation. Thus, the nature of the task also imposes clear difficulty on the annotation process. Even for the same task, let’s say semantic segmentation, annotating a single image under standard weather conditions and normal illumination takes approximately 90 minutes for the <em class="italic">Cityscapes</em> dataset (Marius Cordts, et al. <em class="italic">The cityscapes dataset for semantic urban scene understanding</em>. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213–3223, 2016). However, doing similar annotation for images under adverse conditions such as snow, rain, and fog or at low illumination such as nighttime takes up to 3 hours for the <em class="italic">ACDC</em> dataset (Christos Sakaridis, et al. <em class="italic">ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding</em>. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages <span class="No-Break">10765–10775, 2021.).</span></p>
<h3>Dataset size</h3>
<p>As expected, the larger the dataset, the harder it is to annotate. The complexity comes from managing such a huge dataset and ensuring the same annotation and data collection protocol is being followed by a large group of annotators. These annotators may have different languages, backgrounds, experiences, and skills. Indeed, guiding such a huge, diverse team, probably in different geographical locations, is <span class="No-Break">not simple.</span></p>
<h3>Granularity level</h3>
<p>The more detail you want your ground truth<a id="_idIndexMarker048"/> to capture, the more work for the annotators to perform. Let’s take <strong class="bold">visual object tracking</strong> as an example. Annotating images for single-object tracking is easier than multi-object tracking. We find the same thing for semantic segmentation, too. Annotating a semantic segmentation dataset with 3 classes is easier than 10 classes. Furthermore, the type of class also creates difficulty for the annotator. In other words, small objects may be harder to differentiate from the background and thus harder<a id="_idIndexMarker049"/> <span class="No-Break">to annotate.</span></p>
<p>Next, we look at the main reasons behind noisy ground truth issues commonly seen in <span class="No-Break">real datasets.</span></p>
<h2 id="_idParaDest-39"><a id="_idTextAnchor042"/>The annotation process is error-prone</h2>
<p>In this section, we shed<a id="_idIndexMarker050"/> light on the key reasons behind issues in manually annotated <span class="No-Break">real data.</span></p>
<h3>Human factor</h3>
<p>The most important element in the annotation process is humans. However, we are limited by our perceptions of the world. Humans struggle to perceive with the naked eye the visual content in scenarios such as low illumination, cluttered scenes, or when objects are far from the camera, transparent, and so on. At the same time, miscommunication and misunderstanding of annotation protocol is another major issue. For example, assume you asked a team of annotators to annotate images for a visual object-tracking training dataset. You aim only to consider the <em class="italic">person object</em> for this task. Some annotators will annotate humans without objects while other annotators may consider other objects carried by humans as part of the object of interest (see <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.3</em>). Furthermore, some annotators may consider only the unoccluded part of the human. This will cause a major inconsistency in the training data and the model will struggle to learn the task and will <span class="No-Break">never converge.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 2.3 – Samples of annotation errors due to unclear annotation protocol" height="150" src="image/B18494_02_003.jpg" width="624"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Samples of annotation errors due to unclear annotation protocol</p>
<h3>Recording tools</h3>
<p>If the recoding camera is shaky, the captured images will be blurred and thus the annotators will fail to accurately identify the actual pixels of the object from the background. Furthermore, the intrinsic and extrinsic parameters of the camera drastically change how the 3D scene will be projected into a 2D image. The focal length of the lens, shutter speed, lens distortion, and others all introduce certain errors in the annotation process. In other words, the objects annotated by annotators may not exactly correspond to the same object in the raw image or even in the <span class="No-Break">3D world.</span></p>
<h3>Scene attributes</h3>
<p>Attributes such<a id="_idIndexMarker051"/> as weather conditions and time of the day all play an important role in the annotation process. As we have mentioned earlier, clear weather in the daytime may help the annotators to clearly identify objects as compared to adverse conditions at nighttime. In parallel to this, crowded and cluttered scenes are much more difficult to annotate and more subject to errors (see <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="Figure 2.4 – Scene attribute: crowded scenes are more subject to annotation errors" height="744" src="image/B18494_02_004.jpg" width="1142"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Scene attribute: crowded scenes are more subject to annotation errors</p>
<h3>Annotation tools</h3>
<p>To enhance the annotation process, there are various annotation tools, such as Labelbox, Scale AI, Dataloop, HiveData, and LabelMe. Some of the annotation tools integrate AI components to optimize the annotation process by assisting the human annotator, such as Labelbox. While these AI-assisted methods are promising, they are not practical and reliable yet. In other words, the human annotator still needs to verify and correct the predictions. Additionally, some of these methods are slow and far from able to provide real-time assistance. In addition to this, if the problem is novel, the AI assistance will not work as expected because the AI model was not trained on <span class="No-Break">similar scenarios.</span></p>
<p>Given the task, dataset size, and team specifications, a suitable annotation tool should be selected. The annotation tool should be the same for all annotators to ensure consistency among the annotators<a id="_idIndexMarker052"/> and the created <span class="No-Break">ground truth.</span></p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor043"/>The annotation process is biased</h2>
<p>To understand<a id="_idIndexMarker053"/> the world and to reason about it efficiently, our brains build fast decisions and judgments based on our previous experience and systems of beliefs. For more details, see <em class="italic">Decision Making: Factors that Influence Decision Making, Heuristics Used, and Decision Outcomes</em> (<a href="http://www.inquiriesjournal.com/a?id=180">http://www.inquiriesjournal.com/a?id=180</a>). ML models learn to reason and perceive the world using the training data. We try to collect and annotate the data objectively. However, unintentionally, we reflect our biases on the data we collect and annotate. Consequently, ML models also become biased and unfair. We will discuss the three common factors of annotation process <span class="No-Break">bias next.</span></p>
<h3>Understanding the problem and task</h3>
<p>The annotator may not know the problem, may not understand the data, or understand why the data is collected and annotated. Thus, they may make wrong assumptions or misinterpret data. Furthermore, given the differences between the annotators, they may understand the <span class="No-Break">problem differently.</span></p>
<h3>Background, ideology, and culture</h3>
<p>This is a critical factor behind inconsistency in the annotation process. Let’s imagine that you asked<a id="_idIndexMarker054"/> a group of 10 annotators to annotate a dataset for <strong class="bold">action recognition</strong>. You have only two actions: confirmation or negation. Your annotation team members are from the UK, Bulgaria, and India. The Bulgarian annotators will understand and annotate head shaking as “Yes” and nodding as “No.” The other annotators will do the opposite. Thus, you will have wrong training data and your model will not learn this task. There are also other scenarios where the bias is not clear and cannot be easily identified, and this is the hardest issue under <span class="No-Break">this scope.</span></p>
<h3>Subjectivity and emotions</h3>
<p>For some problems such as <strong class="bold">text sentiment analysis</strong>, a well-known NLP technique used to understand textual data, a human annotator<a id="_idIndexMarker055"/> may be biased toward certain political parties, football teams, genders, and ethnicities. Thus, the annotations will be biased to the annotator’s point<a id="_idIndexMarker056"/> of view <span class="No-Break">as well.</span></p>
<p class="callout-heading">Common issues in the annotation process</p>
<p class="callout">Always avoid the following: using the wrong labeling tool, vague annotation protocol, miscommunication between annotators, adding new labels after starting the annotation process, and modifying the annotation protocol during <span class="No-Break">the process.</span></p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor044"/>Optical flow and depth estimation</h1>
<p>In this section, we will look at different ML tasks and the followed procedures to generate their corresponding <span class="No-Break">ground truth.</span></p>
<h2 id="_idParaDest-42"><a id="_idTextAnchor045"/>Ground truth generation for computer vision</h2>
<p><strong class="bold">Computer vision</strong> aims at enabling computers to see using<a id="_idIndexMarker057"/> digital images. It is not surprising<a id="_idIndexMarker058"/> to know that vision is one of the most complex functionalities performed by our brain. Thus, imitating vision is not simple, and it is rather complex for state-of-the-art computer <span class="No-Break">vision models.</span></p>
<p>Computer vision tasks include semantic segmentation, instance segmentation, optical flow estimation, depth estimation, normal map estimation, visual object tracking, and many more. Each task has its own unique way of generating the corresponding ground truth. Next, we will see samples of <span class="No-Break">these tasks.</span></p>
<h3>Image classification</h3>
<p>The training images<a id="_idIndexMarker059"/> for this task usually contain one object, which is the object of interest. The annotation for this task is simply looking at each image and selecting one class or more describing the object in <span class="No-Break">the image.</span></p>
<h3>Semantic and instance segmentation</h3>
<p>For semantic and instance segmentation, the annotator<a id="_idIndexMarker060"/> needs to assign a class label for each pixel in the image. In other words, the annotator is asked to partition the image into different segments where each segment demonstrates one class for semantic segmentation and one instance for instance segmentation. Please refer to <a href="https://github.com/mrgloom/awesome-semantic-segmentation">https://github.com/mrgloom/awesome-semantic-segmentation</a> for an exhaustive list of semantic segmentation methods, such as <em class="italic">U-Net</em>, <em class="italic">DeepLab</em>, and <em class="italic">D2Det</em>. For instance segmentation, please <span class="No-Break">check </span><a href="https://github.com/topics/instance-segmentation"><span class="No-Break">https://github.com/topics/instance-segmentation</span></a><span class="No-Break">.</span></p>
<h3>Object detection and tracking</h3>
<p>In object detection and tracking, the annotator<a id="_idIndexMarker061"/> draws a bounding box around each object in the image. Object tracking works using video<a id="_idIndexMarker062"/> to track an initially selected object throughout the video. On the other hand, <strong class="bold">object detection</strong> works on images to detect objects as required by the task. Please refer to <a href="https://github.com/topics/object-detection">https://github.com/topics/object-detection</a> for a list of well-known and state-of-the-art object detection methods and useful Python libraries, such as <em class="italic">YOLOv5</em>, <em class="italic">Mask R-CNN</em>, and <em class="italic">OpenMMLab</em>. For object tracking, please refer to <a href="https://github.com/topics/object-tracking">https://github.com/topics/object-tracking</a> for a list of models, such as <em class="italic">SiamMask</em>, <em class="italic">HQTrack</em>, <span class="No-Break">and </span><span class="No-Break"><em class="italic">CFNet</em></span><span class="No-Break">.</span></p>
<p>Now, we will examine two specific tasks in computer vision that are extremely hard to generate ground truth for using a standard approach. This is basically just an example of the limitation of <span class="No-Break">real data.</span></p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor046"/>Optical flow estimation</h2>
<p><strong class="bold">Optical flow</strong> is the relative apparent motion of objects<a id="_idIndexMarker063"/> from one frame to another. The motion could be because of objects or camera motion. Optical flow has many key applications in tasks, such as <strong class="bold">structure from motion</strong>, <strong class="bold">video compression</strong>, and <strong class="bold">video stabilization</strong>. Structure from motion is widely used in 3D construction, and navigation and manipulation tasks in robotics, augmented reality, and games. Video compression<a id="_idIndexMarker064"/> is essential for video streaming, storage, and transmission; video<a id="_idIndexMarker065"/> stabilization, on the other hand, is crucial for timelapse videos, and videos<a id="_idIndexMarker066"/> recorded by drones or head-mounted cameras. Thus, optical flow has enormous applications in practice. For a comprehensive list of optical flow methods such as <em class="italic">SKFlow</em>, <em class="italic">GMFlow</em>, and <em class="italic">RAFT</em>, please refer <span class="No-Break">to </span><a href="https://github.com/hzwer/Awesome-Optical-Flow"><span class="No-Break">https://github.com/hzwer/Awesome-Optical-Flow</span></a><span class="No-Break">.</span></p>
<p>Please note that it is extremely hard to generate ground truth for optical flow. Some approaches apply complex procedures to achieve this under many assumptions, such as an indoor environment<a id="_idIndexMarker067"/> and a limited number of objects <span class="No-Break">and motions.</span></p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor047"/>Depth estimation</h2>
<p><strong class="bold">Depth estimation</strong> is the task of measuring the distance<a id="_idIndexMarker068"/> of each pixel in the scene to the camera. It is essential for 3D vision and has many applications, such as 3D scene reconstruction, autonomous cars and navigation, medical imaging, and augmented reality. Usually, there are two major approaches for depth estimation: one uses monocular images and the other is based on stereo images utilizing epipolar geometry. Similar to optical flow, generating ground truth for depth estimation is extremely hard in the real world. Please refer to <a href="https://github.com/topics/depth-estimation">https://github.com/topics/depth-estimation</a> for a list of recent depth estimation methods, such as <em class="italic">AdaBins</em>, <em class="italic">SC-Depth</em>, <span class="No-Break">and </span><span class="No-Break"><em class="italic">Monodepth2</em></span><span class="No-Break">.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">For optical flow and depth estimation, most of the standard datasets used are synthetic datasets. For optical flow, we can recognize synthetic datasets such as <em class="italic">FlyingChairs</em>, <em class="italic">FlyingThings3D</em>, and <em class="italic">Kubric</em> (<a href="https://github.com/google-research/kubric">https://github.com/google-research/kubric</a>). For depth estimation, we can mention <em class="italic">Virtual KITTI</em>, <em class="italic">DENSE</em> (<a href="https://github.com/uzh-rpg/rpg_e2depth">https://github.com/uzh-rpg/rpg_e2depth</a>), and <span class="No-Break"><em class="italic">DrivingStereo</em></span><span class="No-Break"> (</span><a href="https://drivingstereo-dataset.github.io/"><span class="No-Break">https://drivingstereo-dataset.github.io/</span></a><span class="No-Break">).</span></p>
<h1 id="_idParaDest-45"><a id="_idTextAnchor048"/>Summary</h1>
<p>In this chapter, we learned why ML models need annotated real data. At the same time, we explored some of the common issues in the annotation process. Our exploration has led us to a deeper understanding of real data collection and annotation issues, such as being a time-consuming process and subject to annotator errors. Additionally, we covered the limitations of real data for tasks such as optical flow and depth estimation. In the next chapter, we will look specifically at the privacy issues with <span class="No-Break">real data.</span></p>
<p>In the following chapters of the book, we will continue our exciting journey to understand the limitations of real data and the promising solutions of <span class="No-Break">synthetic data.</span></p>
</div>
</div></body></html>