["```py\n    !pip install feast[aws]==0.19.3 pandas xgboost\n    ```", "```py\n    !git clone <repo_url>\n    ```", "```py\n    # change directory\n    %cd customer_segmentation\n    \"\"\"import feast and load feature store object with the path to the directory which contains feature_story.yaml.\"\"\"\n    from feast import FeatureStore\n    store = FeatureStore(repo_path=\".\")\n    for entity in store.list_entities():\n      print(f\"entity: {entity}\")\n    ```", "```py\n    import pandas as pd\n    ##Read the OnlineRetail.csv\n    retail_data = pd.read_csv('/content/OnlineRetail.csv',\n                              encoding= 'unicode_escape')\n    retail_data['InvoiceDate'] = pd.to_datetime(\n      retail_data['InvoiceDate'], errors = 'coerce')\n    ```", "```py\n## filter data for United Kingdom\nuk_data = retail_data.query(\"Country=='United Kingdom'\").reset_index(drop=True)\nt1 = pd.Timestamp(\"2011-06-01 00:00:00.054000\")\nt2 = pd.Timestamp(\"2011-03-01 00:00:00.054000\")\nuk_data_3m = uk_data[(uk_data.InvoiceDate < t1) & (uk_data.InvoiceDate >= t2)].reset_index(drop=True)\n```", "```py\n    from datetime import datetime\n    entity_df = pd.DataFrame(data = {\n        \"customerid\": [str(item) for item in uk_data_3m.CustomerID.unique().tolist()],\n        \"event_timestamp\": datetime.now()\n    })\n    entity_df.head()\n    ```", "```py\n    feature_view = store.get_feature_view(\"customer_rfm_features\")\n    print(feature_view.to_proto())\n    ```", "```py\n  name: \"customer_rfm_features\"\n  entities: \"customer\"\n  features {\n    name: \"recency\"\n    value_type: INT32\n  }\n  features {\n    name: \"frequency\"\n    value_type: INT32\n  }\n  features {\n    name: \"monetaryvalue\"\n    value_type: DOUBLE\n  }\n  …\n\nmeta {\n  created_timestamp {\n    seconds: 1647301293\n    nanos: 70471000\n  }\n  last_updated_timestamp {\n    seconds: 1647301293\n    nanos: 70471000\n  }\n}\n```", "```py\n    import os\n    from datetime import datetime\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<aws_key_id>\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<aws_secret>\"\n    os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n    job = store.get_historical_features(\n        entity_df=entity_df,\n        features=[\n                  \"customer_rfm_features:recency\", \n                  \"customer_rfm_features:monetaryvalue\", \n                  \"customer_rfm_features:r\", \n                  \"customer_rfm_features:m\",\n                  \"customer_rfm_features:rfmscore\",\n                  \"customer_rfm_features:segmenthighvalue\",\n                  \"customer_rfm_features:segmentlowvalue\"\n                  \"customer_rfm_features:segmentmidvalue\",\n                  \"customer_rfm_features:ltvcluster\"\n                  ]\n        )\n    feature_data = job.to_df()\n    feature_data = feature_data.dropna()\n    feature_data.head()\n    ```", "```py\n    from sklearn.metrics import classification_report,confusion_matrix\n    import xgboost as xgb\n    from sklearn.model_selection import KFold, cross_val_score, train_test_split\n    #Drop prediction column along with event time and customerId columns from X\n    X = feature_data.drop(['ltvcluster', 'customerid', \n                           'event_timestamp'], axis=1)\n    y = feature_data['ltvcluster']\n    X_train, X_test, y_train, y_test = \\ \n    train_test_split(X, y, test_size=0.1)\n    ```", "```py\n    xgb_classifier = xgb.XGBClassifier(max_depth=5, objective='multi:softprob')\n    #model training\n    xgb_model = xgb_classifier.fit(X_train, y_train)\n    #Model scoring\n    acc = xgb_model.score(X_test,y_test)\n    print(f\"Model accuracy: {acc}\")\n    ```", "```py\nModel accuracy: 0.8840579710144928\n```", "```py\n    #Run prediction on the test dataset\n    y_pred = xgb_model.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    ```", "```py\n    import os\n    from datetime import datetime\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<aws_key_id>\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<aws_secret>\"\n    os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n    job = store.get_historical_features(\n        entity_df=entity_df,\n        features=[\n                 \"customer_rfm_features:r\", \n                 \"customer_rfm_features:m\",\n                 \"customer_rfm_features:f\",\n                 \"customer_rfm_features:segmenthighvalue\",\n                 \"customer_rfm_features:segmentlowvalue\",\n                 \"customer_rfm_features:segmentmidvalue\",\n                 \"customer_rfm_features:ltvcluster\"\n                 ]\n        )\n    feature_data = job.to_df()\n    feature_data = feature_data.dropna()\n    feature_data.head()\n    ```", "```py\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nX = feature_data.drop(['ltvcluster', 'customerid',\n                       'event_timestamp'], axis=1)\ny = feature_data['ltvcluster']\nX_train, X_test, y_train, y_test = \\ \ntrain_test_split(X, y, test_size=0.1)\nmodel =  (random_state=0).fit(X_train, y_train)\nacc = model.score(X_test,y_test)\nprint(f\"Model accuracy: {acc}\")\n```", "```py\nModel accuracy: 0.8623188405797102\n```", "```py\n    y_pred = model.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    ```", "```py\n    #install job lib library for model packaging\n    !pip install joblib\n    ```", "```py\n    import joblib\n    joblib.dump(xgb_model, '/content/customer_segment-v0.0')\n    ```", "```py\n    loaded_model = joblib.load('/content/customer_segment-v0.0')\n    prediction = loaded_model.predict(X_test.head())\n    prediction.tolist()\n    ```", "```py\n[0.0, 0.0, 0.0, 2.0, 0.0]\n```", "```py\nimport boto3\ns3_client = boto3.client('s3')\ns3_client.upload_file(\n  '/content/customer_segment-v0.0', \n  \"feast-demo-mar-2022\", \n  \"model-repo/customer_segment-v0.0\")\n```", "```py\n    !pip install feast[aws]==0.19.3 pandas xgboost joblib\n    ```", "```py\nimport boto3\nimport os\n#aws Credentials\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"<aws_key_id>\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<aws_secret>\"\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n#Download model from s3\nmodel_name = \"customer_segment-v0.0\"\ns3 = boto3.client('s3')\ns3.download_file(\"feast-demo-mar-2022\", \n                 f\"model-repo/{model_name}\", \n                 model_name)\n```", "```py\ndef fetch_customers_from_raw_data():\n  ## todo: code to fetch customers from raw data\n  return [\"12747.0\", \"12841.0\", \"12849.0\", \n          \"12854.0\", \"12863.0\"]\ncustomer_to_be_scored=fetch_customers_from_raw_data()\n```", "```py\n    import pandas as pd\n    from datetime import datetime\n    entity_df = pd.DataFrame(data={\n        \"customerid\": customer_to_be_scored,\n        \"event_timestamp\": datetime.now()\n    })\n    entity_df.head()\n    ```", "```py\n    %cd customer_segmentation\n    from feast import FeatureStore\n    store = FeatureStore(repo_path=\".\")\n    job = store.get_historical_features(\n        entity_df=entity_df,\n        features=[\n                  \"customer_rfm_features:recency\", \n                  \"customer_rfm_features:monetaryvalue\", \n                  \"customer_rfm_features:r\", \n                  \"customer_rfm_features:m\",\n                  \"customer_rfm_features:rfmscore\",\n                  \"customer_rfm_features:segmenthighvalue\",\n                  \"customer_rfm_features:segmentlowvalue\",\n                  \"customer_rfm_features:segmentmidvalue\"\n              ]\n        )\n    pred_feature_data = job.to_df()\n    pred_feature_data = pred_feature_data.dropna()\n    pred_feature_data.head()\n    ```", "```py\n    import joblib\n    ## Drop unwanted columns\n    features = pred_feature_data.drop(\n        ['customerid', 'event_timestamp'], axis=1)\n    loaded_model = joblib.load('/content/customer_segment-v0.0')\n    prediction = loaded_model.predict(features)\n    ```", "```py\n    file_name = f\"customer_ltv_pred_results_{datetime.now()}.parquet\"\n    pred_feature_data[\"predicted_ltvcluster\"] = prediction.tolist()\n    s3_url = f's3://feast-demo-mar-2022/prediction_results/{file_name}'\n    pred_feature_data.to_parquet(s3_url)\n    ```", "```py\n    !pip install feast[aws]==0.19.3\n    ```", "```py\n    %cd customer_segmentation/\n    from datetime import datetime\n    import os\n    #aws Credentials\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<aws_key_id>\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<aws_secret>\"\n    os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n    # Command to sync offline features into online.\n    !feast materialize-incremental {datetime.now().isoformat()}\n    ```", "```py\n    import pandas as pd\n    from feast import FeatureStore\n    store = FeatureStore(repo_path=\".\")\n    feature_vector = store.get_online_features(\n        features=[\n            \"customer_rfm_features:recency\", \n            \"customer_rfm_features:monetaryvalue\", \n            \"customer_rfm_features:r\", \n            \"customer_rfm_features:m\",\n        ],\n        entity_rows=[\n            {\"customer\": \"12747.0\"},\n            {\"customer\": \"12841.0\"},\n    {\"customer\": \"abcdef\"},\n        ],\n    ).to_dict()\n    df = pd.DataFrame(feature_vector)\n    df.head()\n    ```", "```py\n    POST /invocations\n    {\n       \"customer_list\": [\"id1\", \"id2\", …]\n    }\n    Response: status 200\n    {\n    \"predictions\": [0, 1, …]\n    }\n    ```", "```py\n    from fastapi import FastAPI\n    app = FastAPI()\n    @app.get(\"/ping\")\n    def ping():\n        return {\"ping\": \"ok\"}\n    @app.post(\"/invocations\")\n    def inference(customers: dict):\n        return customers\n    ```", "```py\n    cd <project_folder>\n    uvicorn main:app --reload\n    ```", "```py\n    $ uvicorn main:app --reload\n    INFO:     Will watch for changes in these directories: ['<folder path>']\n    INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n    INFO:     Started reloader process [24664] using watchgod\n    WARNING:  The --reload flag should not be used in production on Windows.\n    INFO:     Started server process [908]\n    INFO:     Waiting for application startup.\n    INFO:     Application startup complete.\n    ```", "```py\n    import boto3\n    Import joblib\n    model_name = \"customer_segment-v0.0\"\n    s3 = boto3.client('s3')\n    ## download file from s3\n    s3.download_file(\n        \"feast-demo-mar-2022\",\n        f\"model-repo/{model_name}\",\n        model_name)\n    ## Load the model into memory.\n    loaded_model = joblib.load('customer_segment-v0.0')\n    ```", "```py\n    #initialize the feature store object.\n    store = FeatureStore(repo_path=os.path.join(os.getcwd(), \"customer_segmentation\"))\n    ```", "```py\n    @app.post(\"/invocations\")\n    def inference(customers: dict):\n        ##Step1: list of features required for scoring the model\n        required_features = [\n            \"customer_rfm_features:recency\",\n            \"customer_rfm_features:monetaryvalue\",\n            \"customer_rfm_features:r\",\n            \"customer_rfm_features:m\",\n            \"customer_rfm_features:rfmscore\",\n            \"customer_rfm_features:segmenthighvalue\",\n            \"customer_rfm_features:segmentlowvalue\",\n            \"customer_rfm_features:segmentmidvalue\"\n        ]\n        ##step 2: get entity rows from the input\n        entity_rows = [{\"customer\": cust_id} for cust_id in customers[\"customer_list\"]]\n        ##Step 3: query online store\n        feature_vector = store.get_online_features(\n            features=required_features,\n            entity_rows=entity_rows,\n        ).to_dict()\n        ##Step 4: convert features to dataframe and reorder the feature columns in the same order that model expects.\n        features_in_order = ['recency', 'monetaryvalue', \n                             'r', 'm', 'rfmscore', \n                             'segmenthighvalue', \n                             'segmentlowvalue', \n                             'segmentmidvalue']\n        df = pd.DataFrame(feature_vector)\n        features = df.drop(['customerid'], axis=1)\n        features = features.dropna()\n        features = features[features_in_order]\n        ##Step 5: run prediction and return the list\n        prediction = loaded_model.predict(features)\n        return {\"predictions\": prediction.tolist()}\n    ```", "```py\n    <aws_key_id> and <aws_secret> in the preceding code block with the user credentials created in *Chapter 4*, *Adding Feature Store to ML Models*.\n    ```", "```py\n    {\"customer_list\":[\"12747.0\", \"12841.0\"]}\n    ```"]