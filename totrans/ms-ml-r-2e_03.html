<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Logistic Regression and Discriminant Analysis</h1>
            </header>

            <article>
                
<div class="packt_quote">"The true logic of this world is the calculus of probabilities."<br/>
                                                               - James Clerk Maxwell, Scottish physicist</div>
<p>In the previous chapter, we took a look at using <strong>Ordinary Least Squares</strong> (<strong>OLS</strong>) to predict a quantitative outcome, or in other words, linear regression. It is now time to shift gears somewhat and examine how we can develop algorithms to predict qualitative outcomes. Such outcome variables could be binary (male versus female, purchase versus does not purchase, tumor is benign versus malignant) or multinomial categories (education level or eye color). Regardless of whether the outcome of interest is binary or multinomial, the task of the analyst is to predict the probability of an observation belonging to a particular category of the outcome variable. In other words, we develop an algorithm in order to classify the observations.</p>
<p>To begin exploring classification problems, we will discuss why applying the OLS linear regression is not the correct technique and how the algorithms introduced in this chapter can solve these issues. We will then look at a problem of predicting whether or not a biopsied tumor mass is classified as benign or malignant. The dataset is the well-known and widely available <strong>Wisconsin Breast Cancer Data</strong>. To tackle this problem, we will begin by building and interpreting logistic regression models. We will also begin examining methods so as to select features and the most appropriate model. Next, we will discuss both linear and quadratic discriminant analyses and compare and contrast these with logistic regression. Then, building predictive models on the breast cancer data will follow. Finally, we will wrap it up by looking at multivariate regression splines and ways to select the best overall algorithm in order to address the question at hand. These methods (creating <kbd>test</kbd>/<kbd>train</kbd> datasets and cross-validation) will set the stage for more advanced machine learning methods in subsequent chapters.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classification methods and linear regression</h1>
            </header>

            <article>
                
<p>So, why can't we just use the least square regression method that we learned in the previous chapter for a qualitative outcome? Well, as it turns out, you can, but at your own risk. Let's assume for a second that you have an outcome that you are trying to predict and it has three different classes: mild, moderate, and severe. You and your colleagues also assume that the difference between mild and moderate and moderate and severe is an equivalent measure and a linear relationship. You can create a dummy variable where 0 is equal to mild, 1 is equal to moderate, and 2 is equal to severe. If you have reason to believe this, then linear regression might be an acceptable solution. However, qualitative assessments such as the previous ones might lend themselves to a high level of measurement error that can bias the OLS. In most business problems, there is no scientifically acceptable way to convert a qualitative response to one that is quantitative. What if you have a response with two outcomes, say fail and pass? Again, using the dummy variable approach, we could code the fail outcome as <kbd>0</kbd> and the pass outcome as <kbd>1</kbd>. Using linear regression, we could build a model where the predicted value is the probability of an observation of pass or fail. However, the estimates of <kbd>Y</kbd> in the model will most likely exceed the probability constraints of <kbd>[0,1]</kbd> and thus be a bit difficult to interpret.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Logistic regression</h1>
            </header>

            <article>
                
<p>As previously discussed, our classification problem is best modeled with the probabilities that are bound by <kbd>0</kbd> and <kbd>1</kbd>. We can do this for all of our observations with a number of different functions, but here we will focus on the logistic function. The logistic function used in logistic regression is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="41" width="294" src="assets/image_03_01a.png"/></div>
<p>If you have ever placed a friendly wager on horse races or the World Cup, you may understand the concept better as odds. The logistic function can be turned to odds with the formulation of <em>Probability (Y) / 1 - Probability (Y)</em>. For instance, if the probability of Brazil winning the World Cup is 20 percent, then the odds are <em>0.2 / 1 - 0.2</em>, which is equal to <em>0.25</em>, translating to odds of one in four.</p>
<p>To translate the odds back to probability, take the odds and divide by one plus the odds. The World Cup example is thus <em>0.25 / 1 + 0.25</em>, which is equal to 20 percent. Additionally, let's consider the odds ratio. Assume that the odds of Germany winning the Cup are <em>0.18</em>. We can compare the odds of Brazil and Germany with the odds ratio. In this example, the odds ratio would be the odds of Brazil divided by the odds of Germany. We will end up with an odds ratio equal to <em>0.25/0.18</em>, which is equal to <em>1.39</em>. Here, we will say that Brazil is <em>1.39</em> times more likely than Germany to win the World Cup.</p>
<p>One way to look at the relationship of logistic regression with linear regression is to show logistic regression as the log odds or <em>log (P(Y)/1 - P(Y))</em> is equal to <em>Bo + B1x</em>. The coefficients are estimated using a maximum likelihood instead of the OLS. The intuition behind the maximum likelihood is that we are calculating the estimates for <em>Bo</em> and <em>B1,</em> which will create a predicted probability for an observation that is as close as possible to the actual observed outcome of <em>Y</em>, a so-called likelihood. The R language does what other software packages do for the maximum likelihood, which is to find the optimal combination of beta values that maximize the likelihood.</p>
<p>With these facts in mind, logistic regression is a very powerful technique to predict the problems involving classification and is often the starting point for model creation in such problems. Therefore, in this chapter, we will attack the upcoming business problem with logistic regression first and foremost.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<p>Dr. William H. Wolberg from the University of Wisconsin commissioned the Wisconsin Breast Cancer Data in 1990. His goal behind collecting the data was to identify whether a tumor biopsy was malignant or benign. His team collected the samples using <strong>Fine Needle Aspiration</strong> (<strong>FNA</strong>). If a physician identifies the tumor through examination or imaging an area of abnormal tissue, then the next step is to collect a biopsy. FNA is a relatively safe method of collecting the tissue, and complications are rare. Pathologists examine the biopsy and attempt to determine the diagnosis (malignant or benign). As you can imagine, this is not a trivial conclusion. Benign breast tumors are not dangerous as there is no risk of the abnormal growth spreading to other body parts. If a benign tumor is large enough, surgery might be needed to remove it. On the other hand, a malignant tumor requires medical intervention. The level of treatment depends on a number of factors, but it's most likely that surgery will be required, which can be followed by radiation and/or chemotherapy.</p>
<p>Therefore, the implications of a misdiagnosis can be extensive. A false positive for malignancy can lead to costly and unnecessary treatment, subjecting the patient to a tremendous emotional and physical burden. On the other hand, a false negative can deny a patient the treatment that they need, causing the cancer cells to spread and leading to premature death. Early treatment intervention in breast cancer patients can greatly improve their survival.</p>
<p>Our task then is to develop the best possible diagnostic machine learning algorithm in order to assist the patient's medical team in determining whether the tumor is malignant or not.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding and preparation</h1>
            </header>

            <article>
                
<p>This dataset consists of tissue samples from 699 patients. It is in a data frame with 11 variables, as follows:</p>
<ul>
<li><kbd>ID</kbd>: Sample code number</li>
<li><kbd>V1</kbd>: Thickness</li>
<li><kbd>V2</kbd>: Uniformity of the cell size</li>
<li><kbd>V3</kbd>: Uniformity of the cell shape</li>
<li><kbd>V4</kbd>: Marginal adhesion</li>
<li><kbd>V5</kbd>: Single epithelial cell size</li>
<li><kbd>V6</kbd>: Bare nucleus (16 observations are missing)</li>
<li><kbd>V7</kbd>: Bland chromatin</li>
<li><kbd>V8</kbd>: Normal nucleolus</li>
<li><kbd>V9</kbd>: Mitosis</li>
<li><kbd>class</kbd>: Whether the tumor diagnosis is benign or malignant; this will be the outcome that we are trying to predict</li>
</ul>
<p>The medical team has scored and coded each of the nine features on a scale of <kbd>1</kbd> to <kbd>10</kbd>.</p>
<p>The data frame is available in the R <kbd>MASS</kbd> package under the <kbd>biopsy</kbd> name. To prepare this data, we will load the data frame, confirm the structure, rename the variables to something meaningful, and delete the missing observations. At this point, we can begin to explore the data visually. Here is the code that will get us started when we first load the library and then the dataset; using the <kbd>str()</kbd> function, we will examine the underlying structure of the data:</p>
<pre>
    <strong>&gt; library(MASS)</strong><br/>    <strong>&gt; data(biopsy)</strong><br/>    <strong>&gt; str(biopsy)</strong><br/>    <strong>'data.frame':   699 obs. of  11 variables:</strong><br/>    <strong> $ ID   : chr  "1000025" "1002945" "1015425" <br/>       "1016277" ...</strong><br/>    <strong> $ V1   : int  5 5 3 6 4 8 1 2 2 4 ...</strong><br/>    <strong> $ V2   : int  1 4 1 8 1 10 1 1 1 2 ...</strong><br/>    <strong> $ V3   : int  1 4 1 8 1 10 1 2 1 1 ...</strong><br/>    <strong> $ V4   : int  1 5 1 1 3 8 1 1 1 1 ...</strong><br/>    <strong> $ V5   : int  2 7 2 3 2 7 2 2 2 2 ...</strong><br/>    <strong> $ V6   : int  1 10 2 4 1 10 10 1 1 1 ...</strong><br/>    <strong> $ V7   : int  3 3 3 3 3 9 3 3 1 2 ...</strong><br/>    <strong> $ V8   : int  1 2 1 7 1 7 1 1 1 1 ...</strong><br/>    <strong> $ V9   : int  1 1 1 1 1 1 1 1 5 1 ...</strong><br/>    <strong> $ class: Factor w/ 2 levels "benign","malignant": 1 1 1 1 1 2 1 1 <br/>     1 1 ...</strong>
</pre>
<p>An examination of the data structure shows that our features are integers and the outcome is a factor. No transformation of the data to a different structure is needed. </p>
<p>We can now get rid of the <kbd>ID</kbd> column, as follows:</p>
<pre>
<strong>    &gt; biopsy$ID = NULL</strong>
</pre>
<p>Next, we will rename the variables and confirm that the code has worked as intended:</p>
<pre>
<strong>     &gt; names(biopsy) &lt;- c("thick", "u.size", "u.shape", <br/>        "adhsn", "s.size", "nucl",    "chrom", "n.nuc", <br/>            "mit", "class")</strong><br/><strong>     &gt; names(biopsy)</strong><br/><strong>     [1] "thick"   "u.size"  "u.shape" "adhsn"   <br/>        "s.size"  "nucl"<br/></strong><strong>        "chrom"   "n.nuc"</strong><br/><strong>     [9] "mit"     "class"</strong>
</pre>
<p>Now, we will delete the missing observations. As there are only 16 observations with the missing data, it is safe to get rid of them as they account for only 2 percent of all the observations. A thorough discussion of how to handle the missing data is outside the scope of this chapter and has been included in the <a href="5d62fd21-3280-4845-a21e-4700d91225d1.xhtml" target="_blank">Appendix A</a>, <em>R Fundamentals</em>, where I cover data manipulation. In deleting these observations, a new working data frame is created. One line of code does this trick with the <kbd>na.omit</kbd> function, which deletes all the missing observations:</p>
<pre>
    <strong>&gt; biopsy.v2 &lt;- na.omit(biopsy)</strong>
</pre>
<p>Depending on the package in R that you are using to analyze the data, the outcome needs to be numeric, which is <kbd>0</kbd> or <kbd>1</kbd>. In order to accommodate that requirement, create the variable <kbd>y</kbd>, where benign is zero and malignant 1, using the <kbd>ifelse()</kbd> function as shown here:</p>
<pre>
<strong>   &gt; y &lt;- ifelse(biopsy$class == "malignant", 1, 0)</strong>
</pre>
<p>There are a number of ways in which we can understand the data visually in a classification problem, and I think a lot of it comes down to personal preference. One of the things that I like to do in these situations is examine the <strong>boxplots</strong> of the features that are split by the classification outcome. This is an excellent way to begin understanding which features may be important to the algorithm. Boxplots are a simple way to understand the distribution of the data at a glance. In my experience, it also provides you with an effective way to build the presentation story that you will deliver to your customers. There are a number of ways to do this quickly, and the <kbd>lattice</kbd> and <kbd>ggplot2</kbd> packages are quite good at this task. I will use <kbd>ggplot2</kbd> in this case with the additional package, <kbd>reshape2</kbd>. After loading the packages, you will need to create a data frame using the <kbd>melt()</kbd> function. The reason to do this is that melting the features will allow the creation of a matrix of boxplots, allowing us to easily conduct the following visual inspection:</p>
<pre>
    <strong>&gt; library(reshape2)</strong><br/>    <strong>&gt; library(ggplot2)</strong>
</pre>
<p>The following code melts the data by their values into one overall feature and groups them by class:</p>
<pre>
    <strong>&gt; biop.m &lt;- melt(biopsy.v2, id.var = "class")</strong>
</pre>
<p>Through the magic of <kbd>ggplot2</kbd>, we can create a 3x3 boxplot matrix, as follows:</p>
<pre>
    <strong>&gt; ggplot(data = biop.m, aes(x = class, y = value)) <br/>    + geom_boxplot() + facet_wrap(~ variable, ncol = 3)</strong>
</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_03_01.png"/></div>
<p>How do we interpret a boxplot? First of all, in the preceding screenshot, the thick white boxes constitute the upper and lower quartiles of the data; in other words, half of all the observations fall in the thick white box area. The dark line cutting across the box is the median value. The lines extending from the boxes are also quartiles, terminating at the maximum and minimum values, outliers notwithstanding. The black dots constitute the outliers.</p>
<p>By inspecting the plots and applying some judgment, it is difficult to determine which features will be important in our classification algorithm. However, I think it is safe to assume that the nuclei feature will be important, given the separation of the median values and corresponding distributions. Conversely, there appears to be little separation of the mitosis feature by class, and it will likely be an irrelevant feature. We shall see!</p>
<p>With all of our features quantitative, we can also do a correlation analysis as we did with linear regression. Collinearity with logistic regression can bias our estimates just as we discussed with linear regression. Let's load the <kbd>corrplot</kbd> package and examine the correlations as we did in the previous chapter, this time using a different type of correlation matrix, which has both shaded ovals and the correlation coefficients in the same plot, as follows:</p>
<pre>
    <strong>&gt; library(corrplot)</strong><br/>    <strong>&gt; bc &lt;- cor(biopsy.v2[, 1:9]) #create an object of <br/>       the features</strong><br/>    <strong>&gt; corrplot.mixed(bc)</strong>
</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="233" width="435" class="image-border" src="assets/image_03_02.png"/></div>
<p>The correlation coefficients are indicating that we may have a problem with collinearity, in particular, the features of uniform shape and uniform size that are present. As part of the logistic regression modeling process, it will be necessary to incorporate the VIF analysis as we did with linear regression. The final task in the data preparation will be the creation of our <kbd>train</kbd> and <kbd>test</kbd> datasets. The purpose of creating two different datasets from the original one is to improve our ability so as to accurately predict the previously unused or unseen data.</p>
<p>In essence, in machine learning, we should not be so concerned with how well we can predict the current observations and should be more focused on how well we can predict the observations that were not used in order to create the algorithm. So, we can create and select the best algorithm using the training data that maximizes our predictions on the <kbd>test</kbd> set. The models that we will build in this chapter will be evaluated by this criterion.</p>
<p>There are a number of ways to proportionally split our data into <kbd>train</kbd> and <kbd>test</kbd> sets: 50/50, 60/40, 70/30, 80/20, and so forth. The data split that you select should be based on your experience and judgment. For this exercise, I will use a 70/30 split, as follows:</p>
<pre>
    <strong>&gt; set.seed(123) #random number generator</strong><br/>    <strong>&gt; ind &lt;- sample(2, nrow(biopsy.v2), replace = TRUE, <br/>       prob = c(0.7, 0.3))</strong><br/>    <strong>&gt; train &lt;- biopsy.v2[ind==1, ] #the training data <br/>       set</strong><br/>    <strong>&gt; test &lt;- biopsy.v2[ind==2, ] #the test data set</strong><br/>    <strong>&gt; str(test) #confirm it worked</strong><br/>    <strong>'data.frame':   209 obs. of  10 variables:</strong><br/>    <strong> $ thick  : int  5 6 4 2 1 7 6 7 1 3 ...</strong><br/>    <strong> $ u.size : int  4 8 1 1 1 4 1 3 1 2 ...</strong><br/>    <strong> $ u.shape: int  4 8 1 2 1 6 1 2 1 1 ...</strong><br/>    <strong> $ adhsn  : int  5 1 3 1 1 4 1 10 1 1 ...</strong><br/>    <strong> $ s.size : int  7 3 2 2 1 6 2 5 2 1 ...</strong><br/>    <strong> $ nucl   : int  10 4 1 1 1 1 1 10 1 1 ...</strong><br/>    <strong> $ chrom  : int  3 3 3 3 3 4 3 5 3 2 ...</strong><br/>    <strong> $ n.nuc  : int  2 7 1 1 1 3 1 4 1 1 ...</strong><br/>    <strong> $ mit    : int  1 1 1 1 1 1 1 4 1 1 ...</strong><br/>    <strong> $ class  : Factor w/ 2 levels benign","malignant": <br/>       1 1 1 1 1 2 1 <br/>       2 1 1 ...</strong>
</pre>
<p>To ensure that we have a well-balanced outcome variable between the two datasets, we will perform the following check:</p>
<pre>
    <strong>&gt; table(train$class)</strong><br/>    <strong>   benign malignant</strong><br/>    <strong>      302       172</strong><br/>    <strong>&gt; table(test$class)</strong><br/>    <strong>   benign malignant</strong><br/>    <strong>      142        67</strong>
</pre>
<p>This is an acceptable ratio of our outcomes in the two datasets; with this, we can begin the modeling and evaluation.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>For this part of the process, we will start with a logistic regression model of all the input variables and then narrow down the features with the best subsets. After this, we will try our hand at <strong>discriminant analysis</strong> and <strong>Multivariate Adaptive Regression Splines</strong> (<strong>MARS</strong>).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The logistic regression model</h1>
            </header>

            <article>
                
<p>We've already discussed the theory behind logistic regression, so we can begin fitting our models. An R installation comes with the <kbd>glm()</kbd> function fitting the generalized linear models, which are a class of models that includes logistic regression. The code syntax is similar to the <kbd>lm()</kbd> function that we used in the previous chapter. One big difference is that we must use the <kbd>family = binomial</kbd> argument in the function, which tells R to run a logistic regression method instead of the other versions of the generalized linear models. We will start by creating a model that includes all of the features on the <kbd>train</kbd> set and see how it performs on the <kbd>test</kbd> set, as follows:</p>
<pre>
    <strong>&gt; full.fit &lt;- glm(class ~ ., family = binomial, <br/>      data = train)</strong><br/>    <strong>&gt; summary(full.fit)</strong><br/>    <strong>Call:</strong><br/>    <strong>glm(formula = class ~ ., family = binomial, data = <br/>      train)</strong> <br/>    <strong>Deviance Residuals:</strong><br/>    <strong>    Min       1Q   Median       3Q      Max  </strong><br/>    <strong>-3.3397  -0.1387  -0.0716   0.0321   2.3559  </strong><br/>    <strong>Coefficients:</strong><br/>    <strong>            Estimate Std. Error z value Pr(&gt;|z|)    </strong><br/>    <strong>(Intercept)  -9.4293     1.2273  -7.683 1.55e-14 <br/>      ***</strong><br/>    <strong>thick         0.5252     0.1601   3.280 0.001039 **</strong><br/>    <strong>u.size       -0.1045     0.2446  -0.427 0.669165    </strong><br/>    <strong>u.shape       0.2798     0.2526   1.108 0.268044    </strong><br/>    <strong>adhsn         0.3086     0.1738   1.776 0.075722 .  </strong><br/>    <strong>s.size        0.2866     0.2074   1.382 0.167021    </strong><br/>    <strong>nucl          0.4057     0.1213   3.344 0.000826 <br/>      ***</strong><br/>    <strong>chrom         0.2737     0.2174   1.259 0.208006    </strong><br/>    <strong>n.nuc         0.2244     0.1373   1.635 0.102126    </strong><br/>    <strong>mit           0.4296     0.3393   1.266 0.205402    </strong><br/>    <strong>---</strong><br/>    <strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 <br/>      '.' 0.1 ' ' 1</strong><br/>    <strong>(Dispersion parameter for binomial family taken to <br/>       be 1)</strong><br/>    <strong>    Null deviance: 620.989  on 473  degrees of <br/>       freedom</strong><br/>    <strong>Residual deviance:  78.373  on 464  degrees of <br/>       freedom</strong><br/>    <strong>AIC: 98.373</strong><br/>    <strong>Number of Fisher Scoring iterations: 8</strong>
</pre>
<p>The <kbd>summary()</kbd> function allows us to inspect the coefficients and their p-values. We can see that only two features have p-values less than <kbd>0.05</kbd> (thickness and nuclei). An examination of the 95 percent confidence intervals can be called on with the <kbd>confint()</kbd> function, as follows:</p>
<pre>
    <strong>&gt; confint(full.fit)</strong><br/>    <strong>                   2.5 %     97.5 %</strong><br/>    <strong>(Intercept) -12.23786660 -7.3421509</strong><br/>    <strong>thick         0.23250518  0.8712407</strong><br/>    <strong>u.size       -0.56108960  0.4212527</strong><br/>    <strong>u.shape      -0.24551513  0.7725505</strong><br/>    <strong>adhsn        -0.02257952  0.6760586</strong><br/>    <strong>s.size       -0.11769714  0.7024139</strong><br/>    <strong>nucl          0.17687420  0.6582354</strong><br/>    <strong>chrom        -0.13992177  0.7232904</strong><br/>    <strong>n.nuc        -0.03813490  0.5110293</strong><br/>    <strong>mit          -0.14099177  1.0142786</strong>
</pre>
<p>Note that the two significant features have confidence intervals that do not cross zero. You cannot translate the coefficients in logistic regression as the change in <em>Y</em> is based on a one-unit change in <em>X</em>. This is where the odds ratio can be quite helpful. The beta coefficients from the log function can be converted to odds ratios with an exponent (beta).</p>
<p>In order to produce the odds ratios in R, we will use the following <kbd>exp(coef())</kbd> syntax:</p>
<pre>
    <strong>&gt; exp(coef(full.fit))</strong><br/>    <strong> (Intercept)        thick       u.size      u.shape        <br/>     adhsn</strong><br/>    <strong>8.033466e-05 1.690879e+00 9.007478e-01 1.322844e+00 <br/>     1.361533e+00</strong><br/>   <strong>s.size         nucl        chrom        n.nuc  mit</strong><br/>   <strong>1.331940e+00 1.500309e+00 1.314783e+00 1.251551e+00 <br/>    1.536709e+00</strong>
</pre>
<p>The interpretation of an odds ratio is the change in the outcome odds resulting from a unit change in the feature. If the value is greater than 1, it indicates that, as the feature increases, the odds of the outcome increase. Conversely, a value less than 1 would mean that, as the feature increases, the odds of the outcome decrease. In this example, all the features except <kbd>u.size</kbd> will increase the log odds.</p>
<p>One of the issues pointed out during data exploration was the potential issue of multi-collinearity. It is possible to produce the VIF statistics that we did in linear regression with a logistic model in the following way:</p>
<pre>
    <strong>&gt; library(car)</strong><br/>    <strong>&gt; vif(full.fit)</strong><br/>    <strong>   thick  u.size  u.shape  adhsn   s.size   nucl   <br/>      chrom   n.nuc</strong><br/>    <strong> 1.2352 3.2488  2.8303   1.3021  1.6356   1.3729 <br/>       1.5234  1.3431</strong><br/>    <strong>   mit</strong><br/>    <strong>1.059707</strong>
</pre>
<p>None of the values are greater than the VIF rule of thumb statistic of five, so collinearity does not seem to be a problem. Feature selection will be the next task; but, for now, let's produce some code to look at how well this model does on both the <kbd>train</kbd> and <kbd>test</kbd> sets.</p>
<p>You will first have to create a vector of the predicted probabilities, as follows:</p>
<pre>
    <strong>&gt; train.probs &lt;- predict(full.fit, type = <br/>       "response")</strong><br/>    <strong>&gt; train.probs[1:5] #inspect the first 5 predicted <br/>        probabilities</strong><br/>    <strong>[1] 0.02052820 0.01087838 0.99992668 0.08987453 <br/>        0.01379266</strong>
</pre>
<p>Next, we need to evaluate how well the model performed in training and then evaluate how it fits on the test set. A quick way to do this is to produce a confusion matrix. In later chapters, we will examine the version provided by the <kbd>caret</kbd> package. There is also a version provided in the <kbd>InformationValue</kbd> package. This is where we will need the outcome as 0's and 1's. The default value by which the function selects either benign or malignant is 0.50, which is to say that any probability at or above 0.50 is classified as malignant:</p>
<pre>
<strong>    &gt; trainY &lt;- y[ind==1]</strong><br/><strong>    &gt; testY &lt;- y[ind==2]</strong><br/><strong>    &gt; confusionMatrix(trainY, train.probs)</strong><br/><strong>        0    1</strong><br/><strong>    0 294    7</strong><br/><strong>    1   8  165<br/></strong>
</pre>
<p><span>The rows signify the predictions, and the columns signify the actual values. The diagonal elements are the correct classifications. The top right value,</span> <kbd>7</kbd><span>, is the number of false negatives, and the bottom left value,</span> <kbd>8</kbd><span>, is the number of false positives. We can also take a look at the error rate,</span> <span>as follows:</span></p>
<pre>
<strong>    &gt; misClassError(trainY, train.probs)</strong><br/><strong>    [1] 0.0316<br/></strong>
</pre>
<p>It seems we have done a fairly good job with only a <em>3.16%</em> error rate on the training set. As we previously discussed, we must be able to accurately predict unseen data, in other words, our <kbd>test</kbd> set.</p>
<p>The method to create a confusion matrix for the <kbd>test</kbd> set is similar to how we did it for the training data:</p>
<pre>
    <strong>&gt; test.probs &lt;- predict(full.fit, newdata = test, <br/>       type = "response")<br/>    &gt; misClassError(testY, test.probs)<br/>    [1] 0.0239<br/>    &gt; confusionMatrix(testY, test.probs)<br/>        0    1<br/>    0 139    2<br/>    1   3   65</strong>
</pre>
<p>It looks like we have done pretty well in creating a model with all the features. The roughly 98 percent prediction rate is quite impressive. However, we must still check whether there is room for improvement. Imagine that you or your loved one is a patient who has been diagnosed incorrectly. As previously mentioned, the implications can be quite dramatic. With this in mind, is there perhaps a better way to create a classification algorithm? Let's have a look!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Logistic regression with cross-validation</h1>
            </header>

            <article>
                
<p>The purpose of cross-validation is to improve our prediction of the <kbd>test</kbd> set and minimize the chance of overfitting. With the <strong>K-fold cross-validation</strong>, the dataset is split into K equal-sized parts. The algorithm learns by alternatively holding out one of the <strong>K-sets</strong>; it fits a model to the other <strong>K-1 parts</strong>, and obtains predictions for the left-out K-set. The results are then averaged so as to minimize the errors, and appropriate features are selected. You can also perform the <strong>Leave-One-Out-Cross-Validation</strong> (<strong>LOOCV</strong>) method, where K is equal to 1. Simulations have shown that the LOOCV method can have averaged estimates that have a high variance. As a result, most machine learning experts will recommend that the number of K-folds should be 5 or 10.</p>
<p>An R package that will automatically do <kbd>CV</kbd> for logistic regression is the <kbd>bestglm</kbd> package. This package is dependent on the <kbd>leaps</kbd> package that we used for linear regression. The syntax and formatting of the data require some care, so let's walk through this in detail:</p>
<pre>
    <strong>&gt; library(bestglm)</strong><br/>    <strong>Loading required package: leaps</strong>
</pre>
<p>After loading the package, we will need our outcome coded to <kbd>0</kbd> or <kbd>1</kbd>. If left as a factor, it will not work. <span>The other requirement to utilize the package is that your outcome, or</span> <kbd>y</kbd><span>, is the last column and all the extraneous columns have been removed. A new data frame will do the trick for us, via the following code:</span></p>
<pre>
<strong>    &gt; X &lt;- train[, 1:9]</strong><br/><strong>    &gt; Xy &lt;- data.frame(cbind(X, trainY))<br/></strong>
</pre>
<p><span> </span><span>Here is the code to run in order to use the</span> <kbd>CV</kbd> <span>technique with our data:</span></p>
<pre>
    <strong>&gt; bestglm(Xy = biopsy.cv, IC="CV", <br/>      CVArgs=list(Method="HTF", K=10, <br/>      REP=1), family=binomial)</strong>
</pre>
<p>The syntax, <kbd>Xy = Xy</kbd>, points to our properly formatted data frame. <kbd>IC = "CV"</kbd> tells the package that the information criterion to use is cross-validation. <kbd>CVArgs</kbd> are the <kbd>CV</kbd> arguments that we want to use. The <kbd>HTF</kbd> method is K-fold, which is followed by the number of folds, <kbd>K = 10</kbd>, and we are asking it to do only one iteration of the random folds with <kbd>REP = 1</kbd>. Just as with <kbd>glm()</kbd>, we will need to use <kbd>family = binomial</kbd>. On a side note, you can use <kbd>bestglm</kbd> for linear regression as well by specifying <kbd>family = gaussian</kbd>. So, after running the analysis, we will end up with the following output, giving us three features for <kbd>Best Model</kbd>, such as <kbd>thick</kbd>, <kbd>u.size</kbd>, and <kbd>nucl</kbd>. The statement on <kbd>Morgan-Tatar search</kbd> simply means that a simple exhaustive search was done for all the possible subsets, as follows:</p>
<pre>
    <strong>Morgan-Tatar search since family is non-gaussian.</strong><br/>    <strong>CV(K = 10, REP = 1)</strong><br/>    <strong>BICq equivalent for q in (7.16797006619085e-05, <br/>    0.273173435514231)</strong><br/>    <strong>Best Model:</strong><br/>    <strong>Estimate Std. Error   z value     Pr(&gt;|z|)</strong><br/>    <strong>(Intercept) -7.8147191 0.90996494 -8.587934 <br/>     8.854687e-18</strong><br/>    <strong>thick        0.6188466 0.14713075  4.206100 <br/>     2.598159e-05</strong><br/>    <strong>u.size       0.6582015 0.15295415  4.303260 <br/>     1.683031e-05</strong><br/>    <strong>nucl         0.5725902 0.09922549  5.770596 <br/>     7.899178e-09</strong>
</pre>
<p>We can put these features in <kbd>glm()</kbd> and then see how well the model did on the <kbd>test</kbd> set. A <kbd>predict()</kbd> function will not work with <kbd>bestglm</kbd>, so this is a required step:</p>
<pre>
    <strong>&gt; reduce.fit &lt;- glm(class ~ thick + u.size + nucl, <br/>       family = binomial, data = train)</strong>
</pre>
<p>As before, the following code allows us to compare the predicted labels with the actual ones on the test set:</p>
<pre>
    <strong>&gt; test.cv.probs &lt;- predict(reduce.fit, newdata = <br/>       test, type = "response")<br/>    &gt; misClassError(testY, test.cv.probs)<br/>    [1] 0.0383<br/>    &gt; confusionMatrix(testY, test.cv.probs)<br/>        0    1<br/>    0 139    5<br/>    1   3   62</strong>
</pre>
<p>The reduced feature model is slightly less accurate than the full feature model, but all is not lost. We can utilize the <kbd>bestglm</kbd> package again, this time using the best subsets with the information criterion set to <kbd>BIC</kbd>:</p>
<pre>
    <strong>&gt; bestglm(Xy = Xy, IC = "BIC", family = binomial)</strong><br/>    <strong>Morgan-Tatar search since family is non-gaussian.</strong><br/>    <strong>BIC</strong><br/>    <strong>BICq equivalent for q in (0.273173435514231, <br/>      0.577036596263757)</strong><br/>    <strong>Best Model:</strong><br/>    <strong> Estimate Std. Error   z value     Pr(&gt;|z|)</strong><br/>    <strong>(Intercept) -8.6169613 1.03155250 -8.353391 <br/>      6.633065e-17</strong><br/>    <strong>thick        0.7113613 0.14751510  4.822295 <br/>      1.419160e-06</strong><br/>    <strong>adhsn        0.4537948 0.15034294  3.018398 <br/>      2.541153e-03</strong><br/>    <strong>nucl         0.5579922 0.09848156  5.665956 <br/>      1.462068e-08</strong><br/>    <strong>n.nuc        0.4290854 0.11845720  3.622282 <br/>      2.920152e-04</strong>
</pre>
<p>These four features provide the minimum <kbd>BIC</kbd> score for all possible subsets. Let's try this and see how it predicts the <kbd>test</kbd> set, as follows:</p>
<pre>
    <strong>&gt; bic.fit &lt;- glm(class ~ thick + adhsn + nucl + <br/>       n.nuc, family = binomial, data = train)</strong><br/>    <strong>&gt; test.bic.probs &lt;- predict(bic.fit, newdata = <br/>       test, type = "response")<br/>    &gt; misClassError(testY, test.bic.probs)<br/>    [1] 0.0239<br/>    &gt; confusionMatrix(testY, test.bic.probs)<br/>        0    1<br/>    0 138    1<br/>    1   4   66</strong>
</pre>
<p>Here we have five errors, just like the full model. The obvious question then is: which one is better? In any normal situation, the rule of thumb is to default to the simplest or the easiest-to-interpret model, given equality of generalization. We could run a completely new analysis with a new randomization and different ratios of the <kbd>train</kbd> and <kbd>test</kbd> sets among others. However, let's assume for a moment that we've exhausted the limits of what logistic regression can do for us. We will come back to the full model and the model that we developed on a <kbd>BIC</kbd> minimum at the end and discuss the methods of model selection. Now, let's move on to our discriminant analysis methods, which we will also include as possibilities in the final recommendation.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Discriminant analysis overview</h1>
            </header>

            <article>
                
<p><strong>Discriminant Analysis</strong> (<strong>DA</strong>), also known as <strong>Fisher Discriminant Analysis</strong> (<strong>FDA</strong>), is another popular classification technique. It can be an effective alternative to logistic regression when the classes are well-separated. If you have a classification problem where the outcome classes are well-separated, logistic regression can have unstable estimates, which is to say that the confidence intervals are wide and the estimates themselves likely vary from one sample to another (James, 2013). DA does not suffer from this problem and, as a result, may outperform and be more generalized than logistic regression. Conversely, if there are complex relationships between the features and outcome variables, it may perform poorly on a classification task. For our breast cancer example, logistic regression performed well on the testing and training sets, and the classes were not well-separated. For the purpose of comparison with logistic regression, we will explore DA, both <strong>Linear Discriminant Analysis</strong> (<strong>LDA</strong>) and <strong>Quadratic Discriminant Analysis</strong> (<strong>QDA</strong>).</p>
<p>DA utilizes <strong>Baye's theorem</strong> in order to determine the probability of the class membership for each observation. If you have two classes, for example, benign and malignant, then DA will calculate an observation's probability for both the classes and select the highest probability as the proper class.</p>
<p>Bayes' theorem states that the probability of <em>Y</em> occurring--given that <em>X</em> has occurred--is equal to the probability of both <em>Y</em> and <em>X</em> occurring, divided by the probability of <em>X</em> occurring, which can be written as follows:</p>
<div class="CDPAlignCenter CDPAlign">  <img height="58" width="229" src="assets/image_03_aa.png"/></div>
<p>The numerator in this expression is the likelihood that an observation is from that class level and has these feature values. The denominator is the likelihood of an observation that has these feature values across all the levels. Again, the classification rule says that if you have the joint distribution of <em>X</em> and <em>Y</em> and if <em>X</em> is given, the optimal decision about which class to assign an observation to is by choosing the class with the larger probability (the posterior probability).</p>
<p>The process of attaining posterior probabilities goes through the following steps:</p>
<ol>
<li>Collect data with a known class membership.</li>
<li>Calculate the prior probabilities; this represents the proportion of the sample that belongs to each class.</li>
<li>Calculate the mean for each feature by their class.</li>
<li>Calculate the variance--covariance matrix for each feature; if it is an LDA, then this would be a pooled matrix of all the classes, giving us a linear classifier, and if it is a QDA, then a variance--covariance created for each class.</li>
<li>Estimate the normal distribution (Gaussian densities) for each class.</li>
<li>Compute the <kbd>discriminant</kbd> function that is the rule for the classification of a new object.</li>
<li>Assign an observation to a class based on the <kbd>discriminant</kbd> function.</li>
</ol>
<p>This will provide an expanded notation on the determination of the posterior probabilities, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/image_03_bb.png"/></div>
<p>Even though LDA is elegantly simple, it is limited by the assumption that the observations of each class are said to have a multivariate normal distribution, and there is a common covariance across the classes. QDA still assumes that observations come from a normal distribution, but it also assumes that each class has its own covariance.</p>
<p>Why does this matter? When you relax the common covariance assumption, you now allow quadratic terms into the discriminant score calculations, which was not possible with LDA. The mathematics behind this can be a bit intimidating and are outside the scope of this book. The important part to remember is that QDA is a more flexible technique than logistic regression, but we must keep in mind our <strong>bias-variance</strong> trade-off. With a more flexible technique, you are likely to have a lower bias but potentially a higher variance. Like a lot of flexible techniques, a robust set of training data is needed to mitigate a high classifier variance.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Discriminant analysis application</h1>
            </header>

            <article>
                
<p>LDA is performed in the <kbd>MASS</kbd> package, which we have already loaded so that we can access the biopsy data. The syntax is very similar to the <kbd>lm()</kbd> and <kbd>glm()</kbd> functions. </p>
<p>We can now begin fitting our LDA model, which is as follows:</p>
<pre>
    <strong>&gt; lda.fit &lt;- lda(class ~ ., data = train)</strong><br/>    <strong>&gt; lda.fit</strong><br/>    <strong>Call:</strong><br/>    <strong>lda(class ~ ., data = train)</strong><br/>    <strong>Prior probabilities of groups:</strong><br/>    <strong>   benign malignant</strong><br/>    <strong>0.6371308 0.3628692</strong><br/>    <strong>Group means:</strong><br/><strong>    thick  u.size u.shape   adhsn  s.size    nucl   <br/>      chrom</strong><br/>    <strong>benign    2.9205 1.30463 1.41390 1.32450 2.11589 <br/>      1.39735 2.08278</strong><br/>    <strong>malignant 7.1918 6.69767 6.68604 5.66860 5.50000 <br/>      7.67441 5.95930</strong><br/>    <strong>            n.nuc     mit</strong><br/>    <strong>benign    1.22516 1.09271</strong><br/>    <strong>malignant 5.90697 2.63953</strong><br/>    <strong>Coefficients of linear discriminants:</strong><br/>    <strong>                LD1</strong><br/>    <strong>thick    0.19557291</strong><br/>    <strong>u.size   0.10555201</strong><br/>    <strong>u.shape  0.06327200</strong><br/>    <strong>adhsn    0.04752757</strong><br/>    <strong>s.size   0.10678521</strong><br/>    <strong>nucl     0.26196145</strong><br/>    <strong>chrom    0.08102965</strong><br/>    <strong>n.nuc    0.11691054</strong><br/>    <strong>mit     -0.01665454</strong>
</pre>
<p>This output shows us that <kbd>Prior probabilities of groups</kbd> are approximately 64 percent for benign and 36 percent for malignancy. Next is <kbd>Group means</kbd>. This is the average of each feature by their class. <kbd>Coefficients of linear discriminants</kbd> are the standardized linear combination of the features that are used to determine an observation's discriminant score. The higher the score, the more likely that the classification is <kbd>malignant</kbd>.</p>
<p>The <kbd>plot()</kbd> function in LDA will provide us with a histogram and/or the densities of the discriminant scores, as follows:</p>
<pre>
    <strong>&gt; plot(lda.fit, type = "both")</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="295" width="486" class="image-border" src="assets/image_03_03.png"/></div>
<p>We can see that there is some overlap in the groups, indicating that there will be some incorrectly classified observations.</p>
<p>The <kbd>predict()</kbd> function available with LDA provides a list of three elements: class, posterior, and x. The class element is the prediction of <span class="packt_screen">benign</span> or <span class="packt_screen">malignant</span>, the posterior is the probability score of x being in each class, and x is the linear discriminant score. Let's just extract the probability of an observation being malignant:</p>
<pre>
    <strong>&gt; train.lda.probs &lt;- predict(lda.fit)$posterior[, <br/>      2]</strong><br/>    <strong>&gt; misClassError(trainY, train.lda.probs)</strong><br/><strong>    [1] 0.0401</strong><br/><strong>    &gt; confusionMatrix(trainY, train.lda.probs)</strong><br/><strong>        0    1</strong><br/><strong>    0 296   13</strong><br/><strong>    1   6  159<br/></strong>
</pre>
<p>Well, unfortunately, it appears that our LDA model has performed much worse than the logistic regression models. The primary question is to see how this will perform on the <kbd>test</kbd> data:</p>
<pre>
    <strong>&gt; test.lda.probs &lt;- predict(lda.fit, newdata = <br/>       test)$posterior[, 2]<br/>    &gt; misClassError(testY, test.lda.probs)<br/>    [1] 0.0383<br/>    &gt; confusionMatrix(testY, test.lda.probs)<br/>        0    1<br/>    0 140    6<br/>    1   2   61</strong>
</pre>
<p>That's actually not as bad as I thought, given the lesser performance on the training data. From a correctly classified perspective, it still did not perform as well as logistic regression (96 percent versus almost 98 percent with logistic regression).</p>
<p>We will now move on to fit a QDA model. In R, QDA is also part of the <kbd>MASS</kbd> package and the function is <kbd>qda()</kbd>. Building the model is rather straightforward again, and we will store it in an object called <kbd>qda.fit</kbd>, as follows:</p>
<pre>
    <strong>&gt; qda.fit = qda(class ~ ., data = train)</strong> <br/>    <strong>&gt; qda.fit</strong><br/>    <strong>Call:</strong><br/>    <strong>qda(class ~ ., data = train)</strong><br/>    <strong>Prior probabilities of groups:</strong><br/>    <strong>   benign malignant</strong><br/>    <strong>0.6371308 0.3628692</strong><br/>    <strong>Group means:</strong><br/><strong>    Thick u.size u.shape  adhsn s.size   nucl  chrom  <br/>     n.nuc</strong><br/>    <strong>benign    2.9205 1.3046  1.4139 1.3245 2.1158 <br/>      1.3973 2.0827 1.2251</strong><br/>    <strong>malignant 7.1918 6.6976  6.6860 5.6686 5.5000 <br/>      7.6744 5.9593 5.9069</strong><br/>    <strong>               mit</strong><br/>    <strong>benign    1.092715</strong><br/>    <strong>malignant 2.639535</strong>
</pre>
<p>As with LDA, the output has <kbd>Group means</kbd> but does not have the coefficients because it is a quadratic function as discussed previously.</p>
<p>The predictions for the <kbd>train</kbd> and <kbd>test</kbd> data follow the same flow of code as with LDA:</p>
<pre>
<strong>    &gt; train.qda.probs &lt;- predict(qda.fit)$posterior[,          <br/>      2]</strong><br/><strong>    &gt; misClassError(trainY, train.qda.probs)</strong><br/><strong>    [1] 0.0422</strong><br/><strong>    &gt; confusionMatrix(trainY, train.qda.probs)</strong><br/><strong>        0    1</strong><br/><strong>    0 287    5</strong><br/><strong>    1  15  167</strong><br/><strong>    &gt; test.qda.probs &lt;- predict(qda.fit, newdata = <br/>      test)$posterior[, 2]</strong><br/><strong>    &gt; misClassError(testY, test.qda.probs)</strong><br/><strong>    [1] 0.0526</strong><br/><strong>    &gt; confusionMatrix(testY, test.qda.probs)</strong><br/><strong>        0    1</strong><br/><strong>    0 132    1</strong><br/><strong>    1  10   66<br/></strong>
</pre>
<p>We can quickly tell that QDA has performed the worst on the training data with the confusion matrix, and it has classified the <kbd>test</kbd> set poorly with 11 incorrect predictions. In particular, it has a high rate of false positives.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Multivariate Adaptive Regression Splines (MARS)</h1>
            </header>

            <article>
                
<p>How would you like a modeling technique that provides all of the following?</p>
<ul>
<li>Offers the flexibility to build linear and nonlinear models for both regression and classification</li>
<li>Can support variable interaction terms</li>
<li>Is simple to understand and explain</li>
<li>Requires little data preprocessing</li>
<li>Handles all types of data: numeric, factors, and so on</li>
<li>Performs well on unseen data, that is, it does well in bias-variance trade-off </li>
</ul>
<p>If that all sounds appealing, then I cannot recommend the use of MARS models enough. The method was brought to my attention several months ago, and I have found it to perform extremely well. In fact, in a recent case of mine, it outperformed both a random forest and boosted trees on test/validation data. It has quickly become my baseline model and all others are competitors. The other benefit I've seen is that it has negated much of the feature engineering I was doing. Much of that was using <strong>Weight-of-Evidence</strong> (<strong>WOE</strong>) and <strong>Information Values</strong> (<strong>IV</strong>) to capture nonlinearity and recode the variables.  This WOE/IV technique was something I was planning to write about at length in this second edition. However, I've done a number of tests, and MARS does an excellent job of doing what that technique does (that is, capture non-linearity), so I will not discuss WOE/IV at all.</p>
<p>To understand MARS is quite simple. First, just start with a linear or generalized linear model like we discussed previously. Then, to capture any nonlinear relationship, a <kbd>hinge</kbd> function is added. These <kbd>hinges</kbd> are simply points in the input feature that equate to a coefficient change. For example, say we have <em>Y = 12.5 (our intercept) + 1.5(variable 1) + 3.3(variable 2); where variables 1 and 2 are on a scale of 1 to 10</em>. Now, let's see how a <kbd>hinge</kbd> function for variable <em>2</em> could come into play:</p>
<p><em>Y = 11 (new intercept) + 1.5(variable 1) + 4.26734(max(0, variable 2 - 5.5)</em></p>
<p>Thus, we read the <kbd>hinge</kbd> function as: we take the maximum of either <em>0</em> or variable <em>2 minus 5.50</em>. So, whenever variable <em>2</em> has a value greater than <em>5.5</em>, that value will be multiplied times the coefficient; otherwise, it will be zero. The method will accommodate multiple hinges for each variable and also interaction terms.</p>
<p>The other interesting thing about MARS is automatic variable selection. This can be done with cross-validation, but the default is to build through a forward pass, much like forward step wise regression, then a backward pass to prune the model, which after the forward pass is likely to overfit the data. This backward pass prunes input features and removes hinges based on <strong>Generalized Cross Validation</strong> (<strong>GCV</strong>):</p>
<p><em>GCV = RSS / (N * (1 - Effective Number of Parameters / N)<sup>2</sup>)</em></p>
<p><em>Effective Number of Parameters = Number of Input Features + Penalty * (Number of Input Features - 1) / 2</em></p>
<p>In the <kbd>earth</kbd> package, <em>Penalty = 2</em> for additive model and <em>3</em> for multiplicative model, that is one with interaction terms.</p>
<p>In R, there are quite a few parameters you can tune. I will demonstrate in the example an effective and simple way to implement the methodology. If you so desire, you can learn more about its flexibility in the excellent online <em>Notes on the earth package</em>, by <em>Stephen Milborrow</em>, available at this link:</p>
<p><a href="http://www.milbo.org/doc/earth-notes.pdf">http://www.milbo.org/doc/earth-notes.pdf</a></p>
<p>With that introduction out of the way, let's get started. You can use the <kbd>MDA</kbd> package, but I learned on <kbd>earth</kbd>, so that is what I will present. The code is similar to the earlier examples, where we used <kbd>glm()</kbd>. However, it is important to specify how you want the model pruned and that it is a binomial response variable. Here, I specify a model selection of a five-fold cross validation (<kbd>pmethod = "cv"</kbd> and <kbd>nfold = 5</kbd>), repeated 3 times (<kbd>ncross = 3</kbd>), as an additive model only with no interactions (<kbd>degree = 1</kbd>) and only one hinge per input feature (<kbd>minspan = -1</kbd>). In the data I've been working with, both interaction terms and multiple hinges have led to overfitting.  Of course, your results may vary. The code is as follows:</p>
<pre>
<strong>    &gt; library(earth)</strong><br/><strong>    &gt; set.seed(1)</strong><br/><strong>    &gt; earth.fit &lt;- earth(class ~ ., data = train,</strong><br/><strong>                        pmethod = "cv",</strong><br/><strong>                        nfold = 5,</strong><br/><strong>                        ncross = 3,</strong><br/><strong>                        degree = 1,</strong><br/><strong>                        minspan = -1,</strong><br/><strong>                        glm=list(family=binomial)</strong><br/><strong>                        )<br/></strong>
</pre>
<p>We now examine the model summary. At first, it can seem a little confusing. Of course, we have the model formula and the logistic coefficients, including the <kbd>hinge</kbd> functions, followed by some commentary and numbers related to generalized R-squared, and so on.  What happens is that a MARS model is built on the data first as a standard linear regression where the response variable is internally coded to 0's and 1's.  After feature/variable pruning with final model creation, then it is translated to a GLM.  So, just ignore the R-squared values:</p>
<pre>
<strong>    &gt; summary(earth.fit)</strong><br/><strong>    Call: earth(formula=class~., data=train, <br/>      pmethod="cv",</strong><br/><strong>    glm=list(family=binomial), degree=1, ncross=3, <br/>      nfold=5, minspan=-1)</strong><br/><strong>    GLM coefficients</strong><br/><strong>                 malignant</strong><br/><strong>    (Intercept) -6.5746417</strong><br/><strong>    u.size 0.1502747</strong><br/><strong>    adhsn 0.3058496</strong><br/><strong>    s.size 0.3188098</strong><br/><strong>    nucl 0.4426061</strong><br/><strong>    n.nuc 0.2307595</strong><br/><strong>    h(thick-3) 0.7019053</strong><br/><strong>    h(3-chrom) -0.6927319</strong><br/><strong>    Earth selected 8 of 10 terms, and 7 of 9 predictors <br/>      using pmethod="cv"</strong><br/><strong>    Termination condition: RSq changed by less than <br/>      0.001 at 10 terms</strong><br/><strong>    Importance: nucl, u.size, thick, n.nuc, chrom, <br/>      s.size, adhsn, <br/>      u.shape-unused,  ...</strong><br/><strong>    Number of terms at each degree of interaction: 1 7 <br/>      (additive model)</strong><br/><strong>    Earth GRSq 0.8354593 RSq 0.8450554 mean.oof.RSq <br/>      0.8331308 (sd 0.0295)</strong><br/><strong>    GLM null.deviance 620.9885 (473 dof) deviance <br/>      81.90976 (466 dof) <br/>      iters 8</strong><br/><strong>    pmethod="backward" would have selected the same <br/>      model:</strong><br/><strong>    8 terms 7 preds, GRSq 0.8354593 RSq 0.8450554 <br/>      mean.oof.RSq <br/>      0.8331308</strong>
</pre>
<p>The model gives us eight terms, including the Intercept and seven predictors. Two of the predictors have <kbd>hinge</kbd> functions--thickness and chromatin. If the thickness is greater than 3, the coefficient of 0.7019 is multiplied by that value; otherwise, it is 0. For chromatin, if less than 3 then the coefficient is multiplied by the values; otherwise, it is 0.</p>
<p>Plots are available. The first one using the <kbd>plotmo()</kbd> function produces plots showing the model's response when varying that predictor and holding the others constant. You can clearly see the hinge function at work for thickness:</p>
<pre>
<strong>    &gt; plotmo(earth.fit)</strong>
</pre>
<p><span>The following is the output of the preceding command:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_03_04.png"/></div>
<p>With <kbd>plotd()</kbd>, you can see density plots of the predicted probabilities by class label:</p>
<pre>
<strong>    &gt; plotd(earth.fit)</strong>
</pre>
<p><span>The following is the output of the preceding command:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="254" width="416" class="image-border" src="assets/image_03_05.png"/></div>
<p>One can look at relative variable importance. Here we see the variable name, <kbd>nsubsets</kbd>, which is the number of model subsets that include the variable after the pruning pass, and the <kbd>gcv</kbd> and <kbd>rss</kbd> columns show the decrease in the respective value that the variable contributes (<kbd>gcv</kbd> and <kbd>rss</kbd> are scaled 0 to 100): </p>
<pre>
<strong>    &gt; evimp(earth.fit)</strong><br/><strong>           nsubsets   gcv   rss</strong><br/><strong>    nucl          7 100.0 100.0</strong><br/><strong>    u.size        6  44.2  44.8</strong><br/><strong>    thick         5  23.8  25.1</strong><br/><strong>    n.nuc         4  15.1  16.8</strong><br/><strong>    chrom         3   8.3  10.7</strong><br/><strong>    s.size        2   6.0   8.1</strong><br/><strong>    adhsn         1   2.3   4.6</strong>
</pre>
<p>Let's see how well it did on the test dataset:</p>
<pre>
<strong>    &gt; test.earth.probs &lt;- predict(earth.fit, newdata = <br/>      test, type = "response")</strong><br/><strong>    &gt; misClassError(testY, test.earth.probs)</strong><br/><strong>    [1] 0.0287</strong><br/><strong>    &gt; confusionMatrix(testY, test.earth.probs)</strong><br/><strong>        0    1</strong><br/><strong>    0 138    2</strong><br/><strong>    1   4   65</strong>
</pre>
<p>This is very comparable to our logistic regression models. We can now compare the models and see what our best choice would be.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Model selection</h1>
            </header>

            <article>
                
<p>What are we to conclude from all this work? We have the confusion matrices and error rates from our models to guide us, but we can get a little more sophisticated when it comes to selecting the classification models. An effective tool for a classification model comparison is the <strong>Receiver Operating Characteristic</strong> (<strong>ROC</strong>) chart. Very simply, ROC is a technique for visualizing, organizing, and selecting classifiers based on their performance (Fawcett, 2006). On the ROC chart, the y-axis is the <strong>True Positive Rate</strong> (<strong>TPR</strong>) and the x-axis is the <strong>False Positive Rate</strong> (<strong>FPR</strong>). The following are the calculations, which are quite simple:</p>
<p><em>TPR = Positives correctly classified / total positives</em></p>
<p><em>FPR = Negatives incorrectly classified / total negatives</em></p>
<p>Plotting the ROC results will generate a curve, and thus you are able to produce the <strong>Area Under the Curve</strong> (<strong>AUC</strong>). The AUC provides you with an effective indicator of performance, and it can be shown that the AUC is equal to the probability that the observer will correctly identify the positive case when presented with a randomly chosen pair of cases in which one case is positive and one case is negative (Hanley JA &amp; McNeil BJ, 1982). In our case, we will just switch the observer with our algorithms and evaluate accordingly.</p>
<p>To create an ROC chart in R, you can use the <kbd>ROCR</kbd> package. I think this is a great package and allows you to build a chart in just three lines of code. The package also has an excellent companion website (with examples and a presentation) that can be found at the following link: </p>
<p><a href="http://rocr.bioinf.mpi-sb.mpg.de/."><span class="URLPACKT">http://rocr.bioinf.mpi-sb.mpg.de/</span>.</a></p>
<p>What I want to show are three different plots on our ROC chart: the full model, the reduced model using BIC to select the features, the MARS model, and a bad model. This so-called bad model will include just one predictive feature and will provide an effective contrast to our other models. Therefore, let's load the <kbd>ROCR</kbd> package, build this poorly performing model, and call it <kbd>bad.fit</kbd> for simplicity, using the <kbd>thick</kbd> feature:</p>
<pre>
    <strong>&gt; library(ROCR)</strong><br/>    <strong>&gt; bad.fit &lt;- glm(class ~ thick, family = binomial, <br/>      data = test)</strong><br/>    <strong>&gt; test.bad.probs = predict(bad.fit, type = <br/>      "response") #save <br/>      probabilities</strong>
</pre>
<p>It is now possible to build the ROC chart with three lines of code per model using the <kbd>test</kbd> dataset. We will first create an object that saves the predicted probabilities with the actual classification. Next, we will use this object to create another object with the calculated TPR and FPR. Then, we will build the chart with the <kbd>plot()</kbd> function. Let's get started with the model using all of the features or, as I call it, the full model. This was the initial one that we built back in the <em>Logistic regression model</em> section of this chapter:</p>
<pre>
    <strong>&gt; pred.full &lt;- prediction(test.probs, test$class)</strong>
</pre>
<p>The following is the performance object with the TPR and FPR:</p>
<pre>
    <strong>&gt; perf.full &lt;- performance(pred.full, "tpr", "fpr")</strong>
</pre>
<p>The following <kbd>plot</kbd> command with the title of <kbd>ROC</kbd> and <kbd>col=1</kbd> will color the line black:</p>
<pre>
    <strong>&gt; plot(perf.full, main = "ROC", col = 1)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="197" width="296" class="image-border" src="assets/image_03_09.png"/></div>
<p>As stated previously, the curve represents TPR on the y-axis and FPR on the x-axis. If you have the perfect classifier with no false positives, then the line will run vertically at <em>0.0</em> on the x-axis. If a model is no better than chance, then the line will run diagonally from the lower left corner to the upper right one. As a reminder, the full model missed out on five labels: three false positives and two false negatives. We can now add the other models for comparison using a similar code, starting with the model built using BIC (refer to the <em>Logistic regression with cross-validation</em> section of this chapter), as follows:</p>
<pre>
    <strong>&gt; pred.bic &lt;- prediction(test.bic.probs, <br/>      test$class)</strong><br/>    <strong>&gt; perf.bic &lt;- performance(pred.bic, "tpr", "fpr")</strong><br/>    <strong>&gt; plot(perf.bic, col = 2, add = TRUE)</strong>
</pre>
<p>The <kbd>add=TRUE</kbd> parameter in the <kbd>plot</kbd> command added the line to the existing chart. Finally, we will add the poorly performing model, the MARS model, and include a <kbd>legend</kbd> chart, as follows:</p>
<pre>
    <strong>&gt; pred.bad &lt;- prediction(test.bad.probs, <br/>      test$class)</strong><br/>    <strong>&gt; perf.bad &lt;- performance(pred.bad, "tpr", "fpr")</strong><br/>    <strong>&gt; plot(perf.bad, col = 3, add = TRUE)<br/>    &gt; pred.earth &lt;- prediction(test.earth.probs, <br/>      test$class)<br/>    &gt; perf.earth &lt;- performance(pred.earth, "tpr", <br/>      "fpr")<br/>    &gt; plot(perf.earth, col = 4, add = TRUE)<br/>    &gt; legend(0.6, 0.6, c("FULL", "BIC", "BAD", <br/>      "EARTH"), 1:4)</strong>
</pre>
<p><span>The following is the output of the preceding code snippet:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="212" width="413" class="image-border" src="assets/image_03_06.png"/></div>
<p>We can see that the <kbd>FULL</kbd> model, <kbd>BIC</kbd> model and the <kbd>MARS</kbd> model are nearly superimposed. It is also quite clear that the <kbd>BAD</kbd> model performed as poorly as was expected.<br/>
The final thing that we can do here is compute the AUC. This is again done in the <kbd>ROCR</kbd> package with the creation of a <kbd>performance</kbd> object, except that you have to substitute <kbd>auc</kbd> for <kbd>tpr</kbd> and <kbd>fpr</kbd>. The code and output are as follows:</p>
<pre>
<strong>    &gt; performance(pred.full, "auc")@y.values</strong><br/><strong>    [[1]]</strong><br/><strong>    [1] 0.9972672</strong><br/><strong>    &gt; performance(pred.bic, "auc")@y.values</strong><br/><strong>    [[1]]</strong><br/><strong>    [1] 0.9944293</strong><br/><strong>    &gt; performance(pred.bad, "auc")@y.values</strong><br/><strong>    [[1]]</strong><br/><strong>    [1] 0.8962056</strong><br/><strong>    &gt; performance(pred.earth, "auc")@y.values</strong><br/><strong>    [[1]]</strong><br/><strong>    [1] 0.9952701<br/></strong>
</pre>
<p>The highest AUC is for the full model at <kbd>0.997</kbd>. We also see <kbd>99.4</kbd> percent for the BIC model, <kbd>89.6</kbd> percent for the bad model and <kbd>99.5</kbd> for MARS. So, to all intents and purposes, with the exception of the bad model we have no difference in predictive powers between them. What are we to do? A simple solution would be to re-randomize the <kbd>train</kbd> and <kbd>test</kbd> sets and try this analysis again, perhaps using a 60/40 split and a different randomization seed. But if we end up with a similar result, then what? I think a statistical purist would recommend selecting the most parsimonious model, while others may be more inclined to include all the variables. It comes down to trade-offs, that is, model accuracy versus interpretability, simplicity, and scalability. In this instance, it seems safe to default to the simpler model, which has the same accuracy. It goes without saying that we won't always get this level of predictability with just GLMs or discriminant analysis. We will tackle these problems in upcoming chapters with more complex techniques and hopefully improve our predictive ability. The beauty of machine learning is that there are several ways to skin the proverbial cat.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we looked at using probabilistic linear models to predict a qualitative response with three methods: logistic regression, discriminant analysis, and MARS. Additionally, we began the process of using ROC charts in order to explore model selection visually and statistically. We also briefly discussed the model selection and trade-offs that you need to consider. In future chapters, we will revisit the breast cancer dataset to see how more complex techniques perform.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>