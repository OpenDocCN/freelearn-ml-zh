- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with Outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An outlier is a data point that diverges notably from other values within a
    variable. Outliers may stem from the inherent variability of the feature itself,
    manifesting as extreme values that occur infrequently within the distribution
    (typically found in the tails). They can be the result of experimental errors
    or inaccuracies in data collection processes, or they can signal important events.
    For instance, an unusually high expense in a card transaction may indicate fraudulent
    activity, warranting flagging and potentially blocking the card to safeguard customers.
    Similarly, unusually distinct tumor morphologies can suggest malignancy, prompting
    further examination.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers can exert a disproportionately large impact on a statistical analysis.
    For example, a small number of outliers can reverse the statistical significance
    of a test in either direction (think A/B testing) or directly influence the estimation
    of the parameters of the statistical model (think coefficients). Some machine
    learning models are well known for being susceptible to outliers, such as linear
    regression. Other models are known for being robust to outliers, such as decision-tree-based
    models. AdaBoost is said to be sensitive to outliers in the target variable, and
    in principle, distance-based models, such as PCA and KNN, could also be affected
    by the presence of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: There isn’t a strict mathematical definition for what qualifies as an outlier,
    and there is also no consensus on how to handle outliers in statistical or machine
    learning models. If outliers stem from flawed data collection, discarding them
    seems like a safe option. However, in many datasets, pinpointing the exact nature
    of outliers is challenging. Ultimately, detecting and handling outliers remains
    a subjective exercise, reliant on domain knowledge and an understanding of their
    potential impact on models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will begin by discussing methods to identify potential outliers,
    or more precisely, observations that significantly deviate from the rest. Then,
    we’ll proceed under the assumption that these observations are not relevant for
    the analysis, and show how to either remove them or reduce their impact on models
    through truncation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing outliers with boxplots and the inter-quartile proximity rule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding outliers using the mean and standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the median absolute deviation to find outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bringing outliers back within acceptable limits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying winsorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use the Python `numpy`, `pandas`, `matplotlib`, `seaborn`,
    and `feature-engine` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing outliers with boxplots and the inter-quartile proximity rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A common way to visualize outliers is by using boxplots. Boxplots provide a
    standardized display of the variable’s distribution based on quartiles. The box
    contains the observations within the first and third quartiles, known as the **Inter-Quartile
    Range**(**IQR**). The first quartile is the value below which 25% of the observations
    lie (equivalent to the 25th percentile), while the third quartile is the value
    below which 75% of the observations lie (equivalent to the 75th percentile). The
    IQR is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>I</mml:mi><mml:mi>Q</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo> </mml:mo><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo> </mml:mo><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:math>](img/21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Boxplots also display whiskers, which are lines that protrude from each end
    of the box toward the minimum and maximum values and up to a limit. These limits
    are given by the minimum or maximum value of the distribution or, in the presence
    of extreme values, by the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>u</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>r</mi><mi>l</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>t</mi><mo>=</mo><mn>3</mn><mi>r</mi><mi>d</mi><mi>q</mi><mi>u</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>l</mi><mi>e</mi><mo>+</mo><mi>I</mi><mi>Q</mi><mi>R</mi><mo>×</mo><mn>1.5</mn></mrow></mrow></math>](img/22.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>l</mi><mi>o</mi><mi>w</mi><mi>e</mi><mi>r</mi><mi>l</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>t</mi><mo>=</mo><mn>1</mn><mi>s</mi><mi>t</mi><mi>q</mi><mi>u</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>l</mi><mi>e</mi><mo>−</mo><mi>I</mi><mi>Q</mi><mi>R</mi><mo>×</mo><mn>1.5</mn></mrow></mrow></math>](img/23.png)'
  prefs: []
  type: TYPE_IMG
- en: According to the **IQR proximity rule**, we can consider a value an outlier
    if it falls beyond the whisker limits determined by the previous equations. In
    boxplots, outliers are indicated as dots.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the variable has a normal distribution, about 99% of the observations will
    be located within the interval delimited by the whiskers. Hence, we can treat
    values beyond the whiskers as outliers. Boxplots are, however, non-parametric,
    which is why we also use them to visualize outliers in skewed variables.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we’ll begin by visualizing the variable distribution with boxplots,
    and then we’ll calculate the whisker’s limits manually to identify the points
    beyond which we could consider a value as an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will create boxplots utilizing the `seaborn` library. Let’s begin by importing
    the Python libraries and loading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the Python libraries and the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify the default background from `seaborn` (it makes prettier plots, but
    that’s subjective, of course):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the California house prices dataset from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make a boxplot of the `MedInc` variable to visualize its distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following boxplot, we identify the box containing the observations within
    the IQR, that is, the observations between the first and third quartiles. We also
    see the whiskers. On the left, the whisker extends to the minimum value of `MedInc`;
    on the right, the whisker goes up to the third quartile plus 1.5 times the IQR.
    Values beyond the right whisker are represented as dots and could constitute outliers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Boxplot of the MedInc variable highlighting potential outliers
    on the right tail of the distribution](img/B22396_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Boxplot of the MedInc variable highlighting potential outliers
    on the right tail of the distribution
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 5**.1*, the boxplot returns asymmetric boundaries denoted
    by the varying lengths of the left and right whiskers. This makes boxplots a suitable
    method for identifying outliers in highly skewed distributions. As we’ll see in
    the coming recipes, alternative methods to identify outliers create symmetric
    boundaries around the center of the distribution, which may not be the best option
    for asymmetric distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now create a function to plot a boxplot next to a histogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use the previous function to create the plots for the `MedInc` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, we can see the relationship between the boxplot and
    the variable’s distribution shown in the histogram. Note how most of `MedInc`’s
    observations are located within the IQR box. `MedInc`’s potential outliers lie
    on the right tail, corresponding to people with unusually high-income salaries:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Boxplot and histogram – two ways of displaying a variable’s
    distribution](img/B22396_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Boxplot and histogram – two ways of displaying a variable’s distribution
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen how we can visualize outliers, let’s see how to calculate
    the limits beyond which we find outliers at each side of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a function that returns the limits based on the IQR proximity
    rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the first and third quartiles are equivalent to the 25th and 75th
    percentiles. That’s why we use pandas’ `quantile` to determine those values.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the function from *step 7*, we’ll calculate the extreme limits for `MedInc`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we now execute `lower_limit` and `upper_limit`, we will see the values `-0.7063`
    and `8.013`. The lower limit is beyond `MedInc`’s minimum value, hence in the
    boxplot, the whisker only goes up to the minimum value. The upper limit, on the
    other hand, coincides with the right whisker’s limit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Common values to multiply the IQR are `1.5`, which is the default value in boxplots,
    or `3` if we want to be more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s display the box plot and histogram for the `HouseAge` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that this variable does not seem to contain outliers, and hence
    the whiskers in the box plot extend to the minimum and maximum values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Boxplot and histogram of the HouseAge variable](img/B22396_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Boxplot and histogram of the HouseAge variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s find the variable’s limits according to the IQR proximity rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we execute `lower_limit` and `upper_limit`, we will see the values `-10.5`
    and `65.5`, which are beyond the edges of the plots, and hence we don’t see any
    outliers.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we used the `boxplot` method from Seaborn to create the boxplots
    and then we calculated the limits beyond which a value could be considered an
    outlier based on the IQR proximity rule.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.2*, we saw that the box in the boxplot for `MedInc` extended
    from approximately 2 to 5, corresponding to the first and third quantiles (you
    can determine these values precisely by executing `X[`“`MedInc`”`].quantile(0.25)`
    and `X[`“`MedInc`”`].quantile(0.75)`). We also saw that the whiskers start at
    `MedInc`’s minimum on the left and extend up to `8.013` on the right (we know
    this value exactly because we calculated it in *step 8*). `MedInc` showed values
    greater than `8.013`, which were displayed in the boxplot as dots. Those are the
    values that could be considered outliers.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.3*, we displayed the boxplot for the `HouseAge` variable. The
    box included values ranging from approximately 18 to 35 (you can determine the
    precise values by executing `X[`“`HouseAge`”`].quantile(0.25)` and `X[`“`HouseAge`”`].quantile(0.75)`).
    The whiskers extended to the minimum and maximum values of the distribution. The
    limits of the whiskers in the plot did not coincide with those based on the IQR
    proximity rule (which we calculated in *step 10*) because these limits were far
    beyond the value range observed for this variable.
  prefs: []
  type: TYPE_NORMAL
- en: Finding outliers using the mean and standard deviation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In normally distributed variables, around 99.8% of the observations lie within
    the interval comprising the mean plus and minus three times the standard deviation.
    Thus, values beyond those limits can be considered outliers; they are rare.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Using the mean and standard deviation to detect outliers has some drawbacks.
    Firstly, it assumes a normal distribution, including outliers. Secondly, outliers
    strongly influence the mean and standard deviation. Therefore, a recommended alternative
    is the **Median Absolute Deviation** (**MAD**), which we’ll discuss in the next
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will identify outliers as those observations that lie outside
    the interval delimited by the mean plus and minus three times the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin the recipe by importing the Python libraries and loading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the Python libraries and dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the breast cancer dataset from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to plot a boxplot next to a histogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the function from *step 3* in the previous recipe, *Visualizing
    outliers with boxplots and the inter-quartile* *proximity rule*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the distribution of the `mean` `smoothness` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following boxplot, we see that the variable’s values show a distribution
    like the normal distribution, and it has six outliers – one on the left and five
    on the right tail:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Boxplot and histogram of the variable mean smoothness](img/B22396_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Boxplot and histogram of the variable mean smoothness
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a function that returns the mean plus and minus `fold` times the standard
    deviation, where `fold` is a parameter to the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the function to identify the extreme limits of the `mean` `smoothness`
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we now execute `lower_limit` or `upper_limit`, we will see the values `0.0541`
    and `0.13855`, corresponding to the limits beyond which we can consider a value
    an outlier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The interval between the mean plus and minus three times the standard deviation
    encloses 99.87% of the observations if the variable is normally distributed. For
    less conservative limits, we could multiply the standard deviation by 2 or 2.5,
    which would produce intervals that enclose 95.4% and 97.6% of the observations,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Boolean vector that flags observations with values beyond the limits
    determined in *step 6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we now execute `outliers.sum()`, we will see the value `5`, indicating that
    there are five outliers or observations that are smaller or greater than the extreme
    values found with the mean and the standard deviation. According to these limits,
    we’d identify one outlier less compared to the IQR rule.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s add red vertical lines to the histogram from *step 3* to highlight the
    limits determined by using the mean and the standard deviation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And now let’s make the plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following plot, we see that the limits observed by the IQR proximity
    rule in the box plot are less conservative than those identified by the mean and
    the standard deviation. Hence why we observe six potential outliers in the boxplot,
    but only five based on the mean and standard deviation calculations:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Comparison of the limits between the whiskers in the boxplot
    and those determined by using the mean and the standard deviation (vertical lines
    in the histogram)](img/B22396_05_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Comparison of the limits between the whiskers in the boxplot and
    those determined by using the mean and the standard deviation (vertical lines
    in the histogram)
  prefs: []
  type: TYPE_NORMAL
- en: The boundaries derived from the mean and standard deviation are symmetric. They
    extend equidistantly from the center of the distribution toward both tails. As
    previously mentioned, these boundaries are only suitable for normally distributed
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With pandas’ `mean()` and `std()`, we captured the mean and standard deviation
    of the variable. We determined the limits as the mean plus and minus three times
    the standard deviation. To highlight the outliers, we used NumPy’s `where()`.
    The `where()` function scanned the rows of the variable, and if the value was
    greater than the upper limit or smaller than the lower limit, it was assigned
    `True`, and alternatively `False`. Finally, we used pandas’ `sum()` over this
    Boolean vector to calculate the total number of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we compared the boundaries to determine outliers returned by the IQR
    proximity rule, which we discussed in the previous recipe, *Visualizing outliers
    with boxplots and the inter-quartile proximity rule*, and the mean and the standard
    deviation. We observed that the limits of the IQR rule are less conservative.
    That means that with the IQR rule, we’d flag more outliers in this particular
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Using the median absolute deviation to find outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mean and the standard deviation are heavily impacted by outliers. Hence,
    using these parameters to identify outliers can defeat the purpose. A better way
    to identify outliers is by using MAD. MAD is the median of the absolute deviation
    between each observation and the median value of the variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>M</mi><mi>A</mi><mi>D</mi><mo>=</mo><mi>b</mi><mo>×</mo><mi>M</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>n</mi><mo>(</mo><mfenced
    open="|" close="|"><mrow><mi>x</mi><mi>i</mi><mo>−</mo><mi>M</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>n</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced></mrow></mfenced><mo>)</mo></mrow></mrow></mrow></math>](img/24.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous equation, `xi` is each observation in the `X` variable. The
    beauty of MAD is that it uses the median instead of the mean, which is robust
    to outliers. The `b` constant is used to estimate the standard deviation from
    MAD, and if we assume normality, then `b =` `1.4826`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the variable is assumed to have a different distribution, `b` is then calculated
    as 1 divided by the 75th percentile. In the case of normality, 1/75th percentile
    = 1.4826.
  prefs: []
  type: TYPE_NORMAL
- en: After computing MAD, we use the median and MAD to establish distribution limits,
    designating values beyond these limits as outliers. The limits are set as the
    median plus and minus a multiple of MAD, typically ranging from 2 to 3.5\. The
    multiplication factor we choose reflects how stringent we want to be (the higher,
    the more conservative). In this recipe, we will identify outliers using MAD.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin the recipe by importing the Python libraries and loading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the Python libraries and dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the breast cancer dataset from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function that returns the limits based on MAD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use the function to capture the extreme limits of the `mean` `smoothness`
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we execute `lower_limit` or `upper_limit`, we will see the values `0.0536`
    and `0.13812`, corresponding to the limits beyond which we can consider a value
    an outlier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s create a Boolean vector that flags observations with values beyond the
    limits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we now execute `outliers.sum()`, we will see the value `5`, indicating that
    there are five outliers or observations that are smaller or greater than the extreme
    values found with MAD.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s make a function to plot a boxplot next to a histogram of a variable,
    highlighting in the histogram the limits calculated in *step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And now let’s make the plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following plot, we see that the limits observed by the IQR proximity
    rule in the box plot are less conservative than those identified by using MAD.
    MAD returns symmetric boundaries, while the boxplot generates asymmetric boundaries,
    which are more suitable for highly skewed distributions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Comparison of the limits between the whiskers in the boxplot
    and those determined by using MAD](img/B22396_05_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Comparison of the limits between the whiskers in the boxplot and
    those determined by using MAD
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Detecting outliers with MAD requires that the variable has certain variability.
    If more than 50% of the values in a variable are identical, the median coincides
    with the most frequent value, and MAD=0\. This means that all values different
    from the median will be flagged as outliers. This constitutes another limitation
    of using MAD in outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! You now know how to identify outliers using the median and MAD.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We determined the median with pandas’ `median()` and the absolute difference
    with pandas’ `abs()`. Next, we used the NumPy `where()` function to create a Boolean
    vector with `True` if a value was greater than the upper limit or smaller than
    the lower limit, otherwise `False`. Finally, we used pandas’ `sum()` over this
    Boolean vector to calculate the total number of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we compared the boundaries to determine outliers returned by the IQR
    proximity rule, which we discussed in the *Visualizing outliers with boxplots
    and the inter-quartile range proximity rule* recipe, and those returned by using
    MAD. The limits returned by the IQR rule were less conservative. This behavior
    can be changed by multiplying the IQR by 3 instead of 1.5, which is the default
    value in boxplots. In addition, we noted that MAD returns symmetric boundaries,
    whereas the boxplot provided asymmetric limits, which could be better suited for
    asymmetric distributions.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a thorough discussion of the advantages and limitations of the different
    methods to detect outliers, check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Rousseeuw PJ, Croux C. *Alternatives to the Median Absolute deviation*. Journal
    of the American Statistical Association, 1993\. [http://www.jstor.org/stable/2291267](https://www.jstor.org/stable/2291267).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leys C, et. al. *Detecting outliers: Do not use standard deviation around the
    mean, use absolute deviation around the median*. Journal of Experimental Social
    Psychology, 2013\. http://dx.doi.org/10.1016/j.jesp.2013.03.013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thériault R, et. al. *Check your outliers**! An introduction to identifying
    statistical outliers in R with easystats*. Behavior Research Methods, 2024\. [https://doi.org/10.3758/s13428-024-02356-w](https://link.springer.com/article/10.3758/s13428-024-02356-w).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recent studies distinguish three types of outliers: error outliers, interesting
    outliers, and random outliers. Error outliers are likely due to human or methodological
    errors and should be either corrected or removed from the data analysis. In this
    recipe, we’ll assume outliers are errors (you don’t want to remove interesting
    or random outliers) and remove them from the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use the IQR proximity rule to find the outliers and then remove them
    from the data using pandas and Feature-engine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the Python libraries, functions, and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the California housing dataset from scikit-learn and separate it into
    train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a function to find the limits beyond which we’ll consider a data
    point an outlier using the IQR proximity rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In *step 3*, we use the IQR proximity rule to find the limits beyond which data
    points will be considered outliers, which we discussed in the *Visualizing outliers
    with boxplots and the inter-quartile proximity rule* recipe. Alternatively, you
    can identify outliers with the mean and the standard deviation or MAD, as we covered
    in the *Finding outliers using the mean and standard deviation* and *Using the
    median absolute deviation to find* *outliers* recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the function from *step 3* , let’s determine the limits of the `MedInc`
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you execute `print(lower_limit, upper_limit)`, you’ll see the result of
    the previous command: `(-``3.925900000000002, 11.232600000000001)`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s retain the observations in the train and test sets whose values are greater
    than or equal to (`ge`) the lower limit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s retain the observations whose values are lower than or equal to (`le`)
    the upper limit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Go ahead and execute `X_train.shape` followed by `train_t.shape` to corroborate
    that the transformed DataFrame contains fewer observations than the original one
    after removing the outliers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can remove outliers across multiple variables simultaneously with `feature-engine`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set up a transformer to identify outliers in three variables by using the IQR
    rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`OutlierTrimmer` can identify boundaries using the IQR, as we show in this
    recipe, as well as by using the mean and standard deviation, or MAD. You need
    to change `capping_method` to `gaussian` or `mad`, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the transformer to the training set so that it learns those limits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'By executing `trimmer.left_tail_caps_`, we can visualize the lower limits for
    the three variables: `{''MedInc'': -0.6776500000000012, ''HouseAge'': -10.5, ''Population'':
    -626.0}`. By executing `trimmer.right_tail_caps_`, we can see the variables’ upper
    limits: `{''MedInc'': 7.984350000000001, ''HouseAge'': 65.5, ''``Population'':
    3134.0}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s remove outliers from the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To finish with the recipe, let’s compare the distribution of a variable before
    and after removing outliers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s create a function to display a boxplot on top of a histogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the code in *step 10* in the *Visualizing outliers with boxplots*
    recipe earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the distribution of `MedInc` before removing the outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following plot, we see that `MedInc` is skewed and observations greater
    than 8 are marked as outliers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.7– Boxplot and the histogram of MedInc before removing outliers.](img/B22396_05_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7– Boxplot and the histogram of MedInc before removing outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s plot the distribution of `MedInc` after removing the outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After removing outliers, `MedInc` seems less skewed and its values more evenly
    distributed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Boxplot and the histogram of MedInc after removing outliers](img/B22396_05_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Boxplot and the histogram of MedInc after removing outliers
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the IQR rule over the transformed variable reveals new outliers. This
    is not surprising; removing observations at the extremes of the distribution alters
    parameters such as the median and quartile values, which in turn determine the
    length of the whiskers, potentially identifying additional observations as outliers.
    The tools that we use to identify outliers are just that: tools. To unequivocally
    identify outliers, we need to support these tools with additional data analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: If thinking of removing error outliers from the dataset, make sure to compare
    and report the results with and without outliers, to see the extent of their impact
    on the models.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `ge()` and `le()` methods from pandas created Boolean vectors identifying
    observations exceeding or falling below thresholds set by the IQR proximity rule.
    We used these vectors with pandas `loc` to retain observations within the interval
    defined by the IQR.
  prefs: []
  type: TYPE_NORMAL
- en: The `feature-engine` library’s `OutlierTrimmer()` automates the procedure of
    removing outliers for multiple variables. `OutlierTrimmer()` can identify outliers
    based on the mean and standard deviation, IQR proximity rule, MAD, or quantiles.
    We can modify this behavior through the `capping_method` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The methods to identify outliers can be made more or less conservative by changing
    the factor by which we multiply the IQR, the standard deviation, or MAD. With
    `OutlierTrimmer()`, we can control the strength of the methods through the `fold`
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: With `tails` set to `"both"`, `OutlierTrimmer()` found and removed outliers
    at both ends of the variables’ distribution. To remove outliers just on one of
    the tails, we can pass `"left"` or `"right"` to the `tails` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '`OutlierTrimmer()` adopts the scikit-learn functionality with the `fit()` method,
    to learn parameters, and `transform()` to modify the dataset. With `fit()`, the
    transformer learned and stored the limits for each variable. With `transform()`,
    it removed the outliers from the data, returning `pandas` DataFrames.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the study that I mentioned earlier that classifies outliers into errors;
    it is interesting and random: Leys C, et.al. 2019\. *How to Classify, Detect,
    and Manage Univariate and Multivariate Outliers, with Emphasis on Pre-Registration*.
    International Review of Social Psychology. [https://doi.org/10.5334/irsp.289](https://rips-irsp.com/articles/10.5334/irsp.289).'
  prefs: []
  type: TYPE_NORMAL
- en: Bringing outliers back within acceptable limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Removing error outliers can be a valid strategy. However, this approach can
    reduce statistical power, in particular when there are outliers across many variables,
    because we end up removing big parts of the dataset. An alternative way to handle
    error outliers is by bringing outliers back within acceptable limits. In practice,
    what this means is replacing the value of the outliers with some thresholds identified
    with the IQR proximity rule, the mean and standard deviation, or MAD. In this
    recipe, we’ll replace outlier values using `pandas` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use the mean and standard deviation to find outliers and then replace
    their values using `pandas` and `feature-engine`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required Python libraries and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the breast cancer dataset from scikit-learn and separate it into train
    and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a function to find outliers using the mean and standard deviation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In *step 3*, we use the mean and standard deviation to find the limits beyond
    which data points will be considered outliers, as discussed in the *Finding outliers
    using the mean and standard deviation* recipe. Alternatively, you can identify
    outliers with the IQR rule or MAD, as we covered in the *Visualizing outliers
    with boxplots and the inter-quartile proximity rule* and *Using the median absolute
    deviation to find* *outliers* recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the function from *step 3*, let’s determine the limits of the `mean smoothness`
    variable, which follows approximately a Gaussian distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a copy of the original datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, replace outliers with the lower or upper limits from *step 4* in the new
    DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To corroborate that the outliers were replaced with the values determined in
    *step 4*, execute `train_t["worst smoothness"].agg(["min", "max"])` to obtain
    the new maximum and minimum values. They should coincide with the minimum and
    maximum values of the variable, or the limits returned in *step 4*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can replace outliers in multiple variables simultaneously by utilizing `feature-engine`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s set up a transformer to replace outliers in two variables, using limits
    determined with the mean and standard deviation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`Winsorizer` can identify boundaries using the mean and standard deviation,
    as we show in this recipe, as well as the IQR proximity rule and MAD. You need
    to change `capping_method` to `iqr` or `mad`, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the transformer to the data so that it learns those limits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By executing `capper.left_tail_caps_`, we can visualize the lower limits for
    the two variables: `{''worst smoothness'': 0.06364743973736293, ''worst texture'':
    7.115307053129349}`. By executing `capper.right_tail_caps_`, we can see the variables’
    upper limits: `{''worst smoothness'': 0.20149734880520967, ''worst` `texture'':
    43.97692158753917}`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, let’s replace the outliers with the limits from *step 8*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we now execute `train_t[capper.variables_].agg(["min", "max"])`, we’ll see
    that the maximum and minimum values of the transformed DataFrame coincide with
    either the maximum and minimum values of the variables or the identified limits,
    whatever comes first:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you are planning to cap variables, make sure you compare the performance
    of your models or the results of your analysis before and after replacing outliers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `clip()` function from pandas is used to cap values at lower or upper specified
    limits. In this recipe, we found those limits using the mean and standard deviation,
    and then clipped the variable so that all observations took values within those
    limits. The minimum value of the `worst smoothness` variable was actually greater
    than the lower limit we found in *step 4*, so no values were replaced at the left
    of its distribution. However, there were values greater than the upper limit from
    *step 4*, and those were replaced with the limit. This means that the minimum
    value of the transformed variable and the original variable coincide, but the
    maximum values do not.
  prefs: []
  type: TYPE_NORMAL
- en: We used `feature-engine` to replace outliers in multiple variables simultaneously.
    `Winsorizer()` can identify outliers based on the mean and standard deviation,
    the IQR proximity rule, MAD, or by using percentiles. We can modify this behavior
    through the `capping_method` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The methods to identify outliers can be made more or less conservative by changing
    the factor by which we multiply the IQR, the standard deviation, or MAD. With
    `Winsorizer()`, we can control the strength of the methods through the `fold`
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: With `tails` set to `"both"`, `Winsorizer()` found and replaced outliers at
    both ends of the variables’ distribution. To replace outliers at either end, we
    can pass `"left"` or `"right"` to the `tails` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '`Winsorizer()` adopts the scikit-learn functionality with the `fit()` method,
    to learn parameters, and `transform()` to modify the dataset. With `fit()`, the
    transformer learned and stored the limits for each variable. With `transform()`,
    it replaced the values of the outliers, returning pandas DataFrames.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`feature-engine` has `ArbitraryOutlierCapper()`, which caps variables at arbitrary
    minimum and maximum values: [https://feature-engine.readthedocs.io/en/latest/api_doc/outliers/ArbitraryOutlierCapper.html](https://feature-engine.readthedocs.io/en/latest/api_doc/outliers/ArbitraryOutlierCapper.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Applying winsorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Winsorizing, or winsorization, consists of replacing extreme, poorly known observations,
    that is, outliers, with the magnitude of the next largest (or smallest) observation.
    It’s similar to the procedure described in the previous recipe, *Bringing outliers
    back within acceptable limits*, but not exactly the same. Winsorization involves
    replacing the *same number of outliers* at both ends of the distribution, which
    makes Winsorization a symmetric process. This guarantees that the **Winsorized
    mean**, that is, the mean estimated after replacing outliers, remains a robust
    estimator of the central tendency of the variable.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, to remove a similar number of observations at both tails, we’d
    use percentiles. For example, the 5th percentile is the value below which 5% of
    the observations lie and the 95th percentile is the value beyond which 5% of the
    observations lie. Using these values as replacements might result in replacing
    a similar number of observations on both tails, but it’s not guaranteed. If the
    dataset contains repeated values, obtaining reliable percentiles is challenging
    and can lead to an uneven replacement of values at each tail. If this happens,
    then the winsorized mean is not a good estimator of the central tendency. In this
    recipe, we will apply winsorization.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will cap all variables of the breast cancer dataset at their 5th and 95th
    percentiles:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required Python libraries and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the breast cancer dataset from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Separate the data into a train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Capture the 5th and 95th percentiles of each variable in dictionaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now replace values beyond those percentiles with the respective percentiles
    for all variables at once:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s display the minimum, maximum, and mean values of one variable before
    winsorization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see the values in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the minimum, maximum, and mean values of the same variable after winsorization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the minimum and maximum values correspond
    to the percentiles. However, the mean is quite similar to the original mean of
    the variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to use winsorization as part of a scikit-learn pipeline, you can
    use the `feature-engine` library’s `Winsorizer()`, by setting it up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`capper =` `Winsorizer(`'
  prefs: []
  type: TYPE_NORMAL
- en: '`capping_method="quantiles",`'
  prefs: []
  type: TYPE_NORMAL
- en: '`tail="both",`'
  prefs: []
  type: TYPE_NORMAL
- en: '`fold=0.05,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`)`'
  prefs: []
  type: TYPE_NORMAL
- en: After this, proceed with the `fit()` and `transform()` methods as described
    in the *Bringing outliers back within acceptable* *limits* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that despite employing percentiles, the procedure didn’t precisely
    replace the same number of observations on both sides of the distribution. If
    you intend to winsorize your variables, compare the outcomes of your analyses
    before and after winsorization.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We used pandas’ `quantiles()` to obtain the 5th and 95th percentiles of all
    the variables in the dataset, and combined it with `to_dict()` to retain those
    percentiles in dictionaries, where the keys were the variables and the values
    were the percentiles. We then passed these dictionaries to pandas’ `clip()` to
    replace values smaller or larger than those percentiles by the percentiles. By
    using dictionaries, we capped multiple variables at once.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more details about how winsorization affects the mean and standard deviation
    in symmetric and asymmetric replacements, check out the original article:'
  prefs: []
  type: TYPE_NORMAL
- en: Dixon W. *Simplified Estimation from Censored Normal Samples. The Annals of
    Mathematical Statistics*, 1960\. [http://www.jstor.org/stable/2237953](https://www.jstor.org/stable/2237953)
  prefs: []
  type: TYPE_NORMAL
