- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes Container Orchestration Infrastructure Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While establishing a local data science setup for individual use for simple
    ML tasks may seem straightforward, creating a robust and scalable data science
    environment for multiple users (catering to diverse ML tasks and effectively tracking
    ML experiments) poses significant challenges. To overcome the scalability and
    control challenges of having a large number of users, companies normally implement
    ML platforms. There are different approaches to building ML platforms including
    build-your-own using open-source technologies or fully managed cloud ML platforms.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the open-source option, and specifically, Kubernetes,
    an indispensable open-source container orchestration platform that serves as a
    critical foundation for constructing open-source ML platforms. Kubernetes offers
    a wealth of capabilities, enabling the seamless management and orchestration of
    containers at scale. By leveraging Kubernetes, organizations can efficiently deploy
    and manage ML workloads, ensuring high availability, scalability, and resource
    utilization optimization.
  prefs: []
  type: TYPE_NORMAL
- en: We will delve into the core concepts of Kubernetes, gaining insights into its
    networking architecture and essential components. Furthermore, we will explore
    its robust security features and granular access control mechanisms, crucial for
    safeguarding ML environments and sensitive data. Through practical exercises,
    you will have the opportunity to build your own Kubernetes cluster and leverage
    its power to deploy containerized applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes overview and core concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes security and access control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on lab – building a Kubernetes infrastructure on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will continue to use services in your AWS account for the hands-on portion
    of the chapter. We will be using several AWS services, including the AWS **Elastic
    Kubernetes Service** (**EKS**), AWS **CloudShell**, and AWS **EC2**. All code
    files used in this chapter are located on GitHub: [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter06](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter06).'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand Kubernetes, we first need to understand containers as they are
    the core building blocks of Kubernetes. A **container** is a form of operating
    system virtualization and is a very popular computing platform for software deployment
    and running modern software based on microservices architecture. A container allows
    you to package and run computer software with isolated dependencies. Compared
    to server virtualization, such as Amazon EC2 or **VMware** virtual machines, containers
    are more lightweight and portable, as they share the same operating system and
    do not contain operating system images in each container. Each container has its
    own filesystem, shares of computing resources, and process space for the custom
    applications running inside it.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of containerization technology traced back to the 1970s with the
    **chroot system** and **Unix Version 7**. However, container technology did not
    gain much attention in the software development community for the next two decades
    and remained dormant. While it picked up some steam and made remarkable advances
    from 2000 to 2011, it was the introduction of **Docker** in 2013 that started
    a renaissance of container technology.
  prefs: []
  type: TYPE_NORMAL
- en: You can run all kinds of applications inside containers, such as simple programs
    like data processing scripts or complex systems like databases. The following
    diagram illustrates how container deployment is different from other types of
    deployment. With bare metal deployment, all different applications share the same
    hosting operating systems. If there is an issue with the hosting operating system,
    all applications will be impacted on the same physical machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the virtualized deployment, multiple guest operating systems can share
    the same hosting operating system. If one guest operating system has an issue,
    only the applications running in that guest operating system are impacted. Each
    guest operating system would have a full installation, thus consuming a lot of
    resources. With container deployment, a container runtime runs on a single host
    operating system, allowing the sharing of some common resources, while still providing
    the isolation of different environments for running different applications. A
    container is a lot more lightweight than a guest operating system, thus much more
    resource-efficient and faster. Note that a container runtime can also run in the
    guest operating system of a virtualized environment to host containerized applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The differences between bare metal, virtualized, and container
    deployment ](img/B20836_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: The differences between bare metal, virtualized, and container
    deployment'
  prefs: []
  type: TYPE_NORMAL
- en: Containers are packaged as Docker images, which are made of all the files (such
    as installation, application code, and dependencies) that are essential for running
    the containers and the applications in them. One way to build a Docker image is
    the use of a `Dockerfile` – a plain-text file that provides specifications on
    how to build a Docker image. Once a Docker image is created, it can be executed
    in a container runtime environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example `Dockerfile` for building a Docker image to create
    a runtime environment based on the **Ubuntu** operating system (the `FROM` instruction)
    and install various **Python** packages, such as `python3`, `numpy`, `scikit-learn`,
    and `pandas` (the `RUN` instructions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To build a Docker image from this `Dockerfile`, you can use the `Docker build
    - < Dockerfile` command, which is a utility that comes as part of the Docker installation.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have an understanding of containers, next, let’s dive into Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Kubernetes and its core concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing and orchestrating a small number of containers and containerized applications
    manually within a compute environment can be relatively manageable. However, as
    the number of containers and servers grows, the task becomes increasingly complex.
    Enter Kubernetes, a powerful open-source system specifically designed to address
    these challenges. First introduced in 2014, Kubernetes (commonly abbreviated to
    K8s, derived from replacing “ubernete” with the digit 8) offers a comprehensive
    solution for efficiently managing containers at scale across clusters of servers.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes follows a distributed architecture consisting of a master node and
    multiple worker nodes within a server cluster. Here, a server cluster refers to
    the set of machines and resources that Kubernetes manages, and a node is a single
    physical or virtual machine in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The master node, often referred to as the control plane, assumes the primary
    role in managing the entire Kubernetes cluster. It receives data about internal
    cluster events, external systems, and third-party applications, then processes
    the data and makes and executes decisions in response. It comprises four essential
    components, each playing a distinct role in the overall system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**API server**: The API server acts as the central communication hub for all
    interactions with the Kubernetes cluster. It provides a RESTful interface through
    which users, administrators, and other components can interact with the cluster.
    The API server handles authentication, authorization, and validation of requests,
    ensuring secure and controlled access to the cluster’s resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduler:** The scheduler component is responsible for determining the optimal
    placement of workloads or Pods across the available worker nodes within the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller:** The controller manager oversees the cluster’s overall state
    and manages various background tasks to maintain the desired system state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd:** etcd is a distributed key-value store that serves as the cluster’s
    reliable data store, ensuring consistent and consistent access to critical configuration
    and state information. It stores the cluster’s current state, configuration details,
    and other essential data, providing a reliable source of truth for the entire
    system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, worker nodes are machines that run containerized workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the core architecture components of a Kubernetes
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Kubernetes architecture ](img/B20836_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Kubernetes architecture'
  prefs: []
  type: TYPE_NORMAL
- en: The master node exposes the **API server** layer, which allows programmatic
    control of the cluster. An example of an API call could be the deployment of a
    web application on the cluster. The control plane also tracks and manages all
    configuration data in **etcd**, which is responsible for storing all the cluster
    data, such as the desired number of container images to run, compute resource
    specification, and size of storage volume for a web application running on the
    cluster. Kubernetes uses the **controller** to monitor the current states of Kubernetes
    resources and take the necessary actions (for example, request the change via
    the API server) to move the current states to the desired states if there are
    differences (such as the difference in the number of the running containers) between
    the two states. The controller manager in the master node is responsible for managing
    all the Kubernetes controllers. Kubernetes comes with a set of built-in controllers
    such as **scheduler**, which is responsible for scheduling **Pods** (units of
    deployment, which we will discuss in more detail later) to run on worker nodes
    when there is a change request.
  prefs: []
  type: TYPE_NORMAL
- en: Other examples include the **Job controller**, which is responsible for running
    and stopping one or more Pods for a task, and the **Deployment controller**, which
    is responsible for deploying Pods based on a deployment manifest, such as a deployment
    manifest for a web application.
  prefs: []
  type: TYPE_NORMAL
- en: To interact with a Kubernetes cluster control plane, you can use the `kubectl`
    command-line utility, the Kubernetes Python client ([https://github.com/kubernetes-client/python](https://github.com/kubernetes-client/python)),
    or access it directly using the RESTful API. You can find a list of supported
    `kubectl` commands at [https://kubernetes.io/docs/reference/kubectl/cheatsheet/](https://kubernetes.io/docs/reference/kubectl/cheatsheet/).
  prefs: []
  type: TYPE_NORMAL
- en: There are several fundamental technical concepts that form the core of the Kubernetes
    architecture. These concepts are essential to understanding and effectively working
    with Kubernetes. Let’s look at some of the key concepts in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Namespaces organize clusters of worker machines into virtual sub-clusters. They
    are used to provide logical separation of resources owned by different teams and
    projects while still allowing ways for different namespaces to communicate. A
    namespace can span multiple worker nodes, and it can be used to group a list of
    permissions under a single name to allow authorized users to access resources
    in a namespace. Resource usage controls can be enforced to namespaces such as
    quotas for CPU and memory resources. Namespaces also make it possible to name
    resources with identical names if the resources reside in different namespaces
    to avoid naming conflicts. By default, there is a `default` namespace in Kubernetes.
    You can create additional namespaces as needed. The default namespace is used
    if a namespace is not specified.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes deploys computing in a logical unit called a Pod. All Pods must belong
    to a Kubernetes namespace (either the default namespace or a specified namespace).
    One or more containers can be grouped into a Pod, and all containers in the Pod
    are deployed and scaled together as a single unit and share the same context,
    such as Linux namespaces and filesystems. Each Pod has a unique IP address that’s
    shared by all the containers in the Pod. A Pod is normally created as a workload
    resource, such as a Kubernetes Deployment or Kubernetes Job.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Namespaces, Pods, and containers ](img/B20836_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Namespaces, Pods, and containers'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows the relationship between namespaces, Pods, and containers
    in a Kubernetes cluster. In this figure, each namespace contains its own set of
    Pods and each Pod can contain one or more containers running in it.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A Deployment is used by Kubernetes to create or modify Pods that run containerized
    applications. For example, to deploy a containerized application, you create a
    configuration manifest file (usually in a `YAML` file format) that specifies details,
    such as the container deployment name, namespaces, container image URI, number
    of Pod replicas, and the communication port for the application. After the Deployment
    is applied using a Kubernetes client utility (`kubectl`), the corresponding Pods
    running the specified container images will be created on the worker nodes. The
    following example creates a Deployment of Pods for an `nginx` server with the
    desired specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the flow of applying the preceding deployment manifest
    file to a Kubernetes cluster and creating two Pods to host two copies of the `nginx`
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Creating an Nginx deployment ](img/B20836_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: Creating an nginx deployment'
  prefs: []
  type: TYPE_NORMAL
- en: After the Deployment, a Deployment controller monitors the deployed container
    instances. If an instance goes down, the controller will replace it with another
    instance on the worker node.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A Kubernetes Job is a controller that creates one or more Pods to run some
    tasks and ensures the job is successfully completed. If a number of Pods fail
    due to node failure or other system issues, a Kubernetes Job will recreate the
    Pods to complete the task. A Kubernetes Job can be used to run batch-oriented
    tasks, such as running batch data processing scripts, ML model training scripts,
    or ML batch inference scripts on a large number of inference requests. After a
    Job is completed, the Pods are not terminated, so you can access the Job logs
    and inspect the detailed status of the Job. The following is an example template
    for running a training job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this configuration contains a parameter called `restartPolicy`, which
    controls how the Pod is restarted when the container exits and fails. There are
    three configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`OnFailure`: Only restart the Pod if it fails, not if it succeeds. This is
    the default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Never`: Do not restart the Pod under any circumstances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Always`: Always restart the Pod regardless of its exit status.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use this parameter to control whether you want to restart training if
    the container terminates on failure.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes custom resources and operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes provides a list of built-in resources, such as Pods or Deployments
    for different needs. It also allows you to create **custom resources** (**CRs**)
    and manage them just like the built-in resources, and you can use the same tools
    (such as `kubectl`) to manage them. When you create the CR in Kubernetes, Kubernetes
    creates a new API (for example, `<custom resource name>/<version>`) for each version
    of the resource. This is also known as *extending* the Kubernetes APIs. To create
    a CR, you create a **custom resource definition** (**CRD**) `YAML` file. To register
    the CRD in Kubernetes, you simply run `kubectl apply -f <name of the CRD yaml
    file>` to apply the file. After that, you can use it just like any other Kubernetes
    resource. For example, to manage a custom model training job on Kubernetes, you
    can define a CRD with specifications such as algorithm name, data encryption setting,
    training image, input data sources, number of job failure retries, number of replicas,
    and job liveness probe frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Kubernetes operator is a controller that operates on a custom resource. The
    operator watches the CR types and takes specific actions to make the current state
    match the desired state, just as a built-in controller does. For example, if you
    want to create a training job for the training job CRD mentioned previously, you
    create an operator that monitors training job requests and performs application-specific
    actions to start up the Pods and run the training job throughout the lifecycle.
    The following figure shows the components involved with an operator deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – A Kubernetes custom resource and its interaction with the operator
    ](img/B20836_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: A Kubernetes custom resource and its interaction with the operator'
  prefs: []
  type: TYPE_NORMAL
- en: The most common way to deploy an operator is to deploy a CR definition and the
    associated controller. The controller runs outside of the Kubernetes control plane,
    similar to running a containerized application in a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes Services play a crucial role in enabling reliable and scalable communication
    between various components and applications within a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As applications in a cluster are often dynamic and can be scaled up or down,
    Services provide a stable and abstracted endpoint that other components can use
    to access the running instances of those applications.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, a Kubernetes Service is an abstraction layer that exposes a set
    of Pods as a single, well-defined network endpoint. It acts as a load balancer,
    distributing incoming network traffic to the available Pods behind the Service.
    This abstraction allows applications to interact with the Service without needing
    to know the specific details of the underlying Pods or their IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Networking on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes operates a flat private network among all the resources in a Kubernetes
    cluster. Within a cluster, all Pods can communicate with each other cluster-wide
    without a **network address translation** (**NAT**). Kubernetes gives each Pod
    its own cluster private IP address, which is the same IP address seen by the Pod
    itself and what others see it as. All containers inside a single Pod can reach
    each container’s port on the localhost. All nodes in a cluster have their individually
    assigned IP addresses as well and can communicate with all Pods without a NAT.
    The following figure shows the different IP assignments for Pods and nodes, and
    communication flows from different resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – IP assignments and communication flow ](img/B20836_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: IP assignments and communication flow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, you might need a set of Pods running the same application container
    (the container for an `nginx` application) for high availability and load balancing,
    for example. Instead of calling each Pod by its private IP address separately
    to access the application running in a Pod, you want to call an abstraction layer
    for this set of Pods, and this abstraction layer can dynamically send traffic
    to each Pod behind it. In this case, you can create a Kubernetes Service as an
    abstraction layer for a logical set of Pods. A Kubernetes Service can dynamically
    select the Pod behind it by matching an `app` label for the Pod using a Kubernetes
    feature called `selector`. The following example shows the specification that
    would create a Service called `nginx-service`, which sends traffic to Pods with
    the app `nginx` label on port `9376`. A service is also assigned with its own
    cluster private IP address, so it is reachable by other resources inside a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to using `selector` to automatically detect Pods behind the service,
    you can also manually create an `Endpoint` and map a fixed IP address and port
    to a service, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'While nodes, Pods, and services are all assigned with cluster private IPs,
    these IPs are not routable from outside of a cluster. To access Pods or services
    from outside of a cluster, you have the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Access from a node or Pod**: You can connect to the shell of a running Pod
    using the `kubectl exec` command and access other Pods, nodes, and services from
    the shell.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes proxy**: You can start a Kubernetes proxy to access services by
    running the `kubectl proxy --port=<port number>` command on your local machine.
    Once the proxy is running, you can access nodes, Pods, or services. For example,
    you can access a service using the following scheme:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**NodePort**: `NodePort` opens a specific port on all the worker nodes, and
    any traffic sent to this port on the IP address of any of the nodes is forwarded
    to the service behind the port. The nodes’ IP addresses need to be routable from
    external sources. The following figure shows the communication flow using `NodePort`:![Figure
    6.7 – Accessing Kubernetes Service via NodePort ](img/B20836_06_07.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 6.7: Accessing a Kubernetes Service via NodePort'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`NodePort` is simple to use, but it has some limitations, such as one service
    per `NodePort`, a fixed port range to use (`3000` to `32767`), and you need to
    know the IP addresses of individual worker nodes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Load balancer**: A load balancer is a way to expose services to the internet
    when you use a cloud provider such as AWS. With a load balancer, you get a public
    IP address that’s accessible to the internet, and all traffic sent to the IP address
    will be forwarded to the service behind the load balancer. A load balancer is
    not part of Kubernetes and it is provided by whatever cloud infrastructure a Kubernetes
    cluster resides on (for example, AWS). The following figure shows the communication
    flow from a load balancer to services and Pods:![Figure 6.8 – Accessing a Kubernetes
    service via a load balancer ](img/B20836_06_08.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 6.8: Accessing a Kubernetes Service via a load balancer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A load balancer allows you to choose the exact port to use and can support multiple
    ports per service. However, it does require a separate load balancer per service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Ingress**: An Ingress gateway is the entry point to a cluster. It acts as
    a load balancer and routes incoming traffic to the different services based on
    routing rules.![Figure 6.9 – Accessing a Kubernetes service via Ingress ](img/B20836_06_09.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 6.9: Accessing a Kubernetes Service via Ingress'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An Ingress is different from a load balancer and `NodePort` in that it acts
    as a proxy to manage traffic to clusters. It works with the `NodePort` and load
    balancer and routes the traffic to the different services. The Ingress way is
    becoming more commonly used, especially in combination with a load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to network traffic management from outside of the cluster, another
    important aspect of Kubernetes network management is to control the traffic flow
    between different Pods and services within a cluster. For example, you might want
    to allow certain traffic to access a Pod or service while denying traffic from
    other sources. This is especially important for applications built on microservices
    architecture, as there could be many services or Pods that need to work together.
    Such a network of microservices is also called a **service mesh**. As the number
    of services grows larger, it becomes challenging to understand and manage the
    networking requirements, such as *service discovery*, *network routing*, *network
    metrics*, and *failure recovery*. **Istio** is an open-source service mesh management
    software that makes it easy to manage a large service mesh on Kubernetes, and
    it provides the following core functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingress**: Istio provides an Ingress gateway that can be used to expose Pods
    and services inside a service mesh to the internet. It acts as a load balancer
    that manages the inbound and outbound traffic for the service mesh. A gateway
    only allows traffic to come in/out of a mesh – it does not do routing of the traffic.
    To route traffic from the gateway to the service inside the service mesh, you
    create an object called `VirtualService` to provide routing rules to route incoming
    traffic to different destinations inside a cluster, and you create a binding between
    virtual services and the gateway object to connect the two.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network traffic management**: Istio provides easy rule-based network routing
    to control the flow of traffic and API calls between different services. When
    Istio is installed, it automatically detects services and endpoints in a cluster.
    Istio uses an object called `VirtualService` to provide routing rules to route
    incoming traffic to different destinations inside a cluster. Istio uses a load
    balancer called `gateway` to manage the inbound and outbound traffic for the network
    mesh. The `gateway` load balancer only allows traffic to come in/out of a mesh
    – it does not do routing of the traffic. To route traffic from the gateway, you
    create a binding between virtual services and the `gateway` object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to manage the traffic in and out of a Pod, an Envoy proxy component
    (aka `sidecar`) is injected into a Pod, and it intercepts and decides how to route
    all traffic. The Istio component that manages the traffic configurations of the
    sidecars and service discovery is called the `Pilot`. The `Citadel` component
    manages authentication for service to service and end user. The `Gallery` component
    is responsible for insulating other Istio components from the underlying Kubernetes
    infrastructure. The following figure shows the architecture of Istio on Kubernetes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Istio architecture ](img/B20836_06_10.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.10: Istio architecture'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Istio also has support for security and observability, which are critical for
    building production systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security**: Istio provides authentication and authorization for inter-service
    communications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observability**: Istio captures metrics, logs, and traces of all service
    communications within a cluster. Examples of metrics include network latency,
    errors, and saturation. Examples of traces include call flows and service dependencies
    within a mesh.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Istio can handle a wide range of Deployment needs, such as load balancing and
    service-to-service authentication. It can even extend to other clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Security and access management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Security is a critical consideration for building production-grade systems on
    Kubernetes. As a practitioner planning to use Kubernetes as the foundational platform
    for ML, it is important to become familiar with the various security aspects of
    Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has many built-in security features. These security features allow
    you to implement fine-grained network traffic control and access control to different
    Kubernetes APIs and services. In this section, we will discuss network security,
    authentication, and authorization.
  prefs: []
  type: TYPE_NORMAL
- en: API authentication and authorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Access to Kubernetes APIs can be authenticated and authorized for both users
    and Kubernetes **service accounts** (a service account provides an identity for
    processes running in a Pod).
  prefs: []
  type: TYPE_NORMAL
- en: 'Users are handled outside of Kubernetes, and there are a number of user authentication
    strategies for Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**X.509 client certificate**: A signed certificate is sent to the API server
    for authentication. The API server verifies this with the certificate authority
    to validate the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single sign-on with OpenID Connect** (**OIDC**): The user authenticates with
    the OIDC provider and receives a bearer token (**JSON** **Web Token** (**JWT**))
    that contains information about the user. The user passes the bearer token to
    the API server, which verifies the validity of the token by checking the certificate
    in the token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTTP basic authentication**: HTTP basic authentication requires a user ID
    and password to be sent as part of the API request, and it validates the user
    ID and password against a password file associated with the API server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authentication proxy**: The API server extracts the user identity in the
    HTTP header and verifies the user with the certificate authority.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authentication webhook**: An external service is used for handling the authentication
    for the API server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service accounts are used to provide identity for processes running in a Pod.
    They are created and managed in Kubernetes. Service accounts need to reside within
    a namespace, by default. There is also a *default* service account in each namespace.
    If a Pod is not assigned a service account, the default service account will be
    assigned to the Pod. A service account has an associated authentication token,
    saved as a Kubernetes Secret, and used for API authentication. A Kubernetes Secret
    is used for storing sensitive information such as passwords, authentication tokens,
    and SSH keys. We will cover Secrets in more detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a user or service account is authenticated, the request needs to be authorized
    to perform allowed operations. Kubernetes authorizes authenticated requests using
    the API server in the control plane, and it has several modes for authorization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Attribute-based access control** (**ABAC**): Access rights are granted to
    users through policies. Note that every service account has a corresponding username.
    The following sample policy allows the `joe` user access to all APIs in all namespaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following policy allows the `system:serviceaccount:kube-system:default`
    service account access to all APIs in all namespaces:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Role-based access control** (**RBAC**): Access rights are granted based on
    the role of a user. RBAC authorizes using the `rbac.authorization.k8s.io` API
    group. The RBAC API works with four Kubernetes objects: `Role`, `ClusterRole`,
    `RoleBinding`, and `ClusterRoleBinding`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Role` and `ClusterRole` contain a set of permissions. The permissions are
    *additive*, meaning there are no deny permissions, and you need to explicitly
    add permission to resources. The `Role` object is namespaced and is used to specify
    permissions within a namespace. The `ClusterRole` object is non-namespaced but
    can be used for granting permission for a given namespace or cluster-scoped permissions.
    See the following illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_06_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Role versus ClusterRole'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following .`yaml` file provides get, watch, and list access to all Pods
    resources in the default namespace for the core API group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following policy allows get, watch, and list access for all Kubernetes
    nodes across the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`RoleBinding` and `ClusterRoleBinding` grant permissions defined in a `Role`
    or `ClusterRole` object to a user or set of users with reference to a `Role` or
    `ClusterRole` object. The following policy binds the `joe` user to the `pod-reader`
    role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `RoleBinding` object binds a service account, `SA-name`, to the
    `ClusterRole` `secret-reader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Kubernetes has a built-in feature for storing and managing sensitive information
    such as passwords. Instead of storing this sensitive information directly in plain
    text in a Pod, you can store this information as Kubernetes Secrets, and provide
    specific access to them using Kubernetes RBAC to create and/or read these Secrets.
    By default, Secrets are stored as unencrypted plain-text Base64-encoded strings,
    and data encryption at rest can be enabled for the Secrets. The following policy
    shows how to create a secret for storing AWS access credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several ways to use a secret in a Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As environment variables in the Pod specification template:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The application code inside the container can access the Secrets just like other
    environment variables.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As a file in a volume mounted on a Pod:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the previous examples, you will see files in the `/etc/aws` folder in a Pod
    for each corresponding secret name (such as `SECRET_AWS_ACCESS_KEY`) that contains
    the values for the Secrets.
  prefs: []
  type: TYPE_NORMAL
- en: We now know what containers are and how they can be deployed on a Kubernetes
    cluster. We also understand how to configure networking on Kubernetes to allow
    Pods to communicate with each other and how to expose a Kubernetes container for
    external access outside of the cluster using different networking options.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes can serve as the foundational infrastructure for running ML workloads.
    For example, you can run a **Jupyter** **notebook** as a containerized application
    on Kubernetes as your data science environment for experimentation and model building.
  prefs: []
  type: TYPE_NORMAL
- en: You can also run a model training job as a Kubernetes Job if you need additional
    resources, and then serve the model as a containerized web service application
    or run batch inferences on trained models as a Kubernetes Job. In the following
    hands-on exercise, you will learn how to use Kubernetes as the foundational infrastructure
    for running ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on – creating a Kubernetes infrastructure on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will create a Kubernetes environment using Amazon EKS,
    a managed Kubernetes environment on AWS, which makes it easier to set up a Kubernetes
    cluster. Let’s first look at the problem statement.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an ML solutions architect, you have been tasked with evaluating Kubernetes
    as a potential infrastructure platform for building an ML platform for one business
    unit in your bank. You need to build a sandbox environment on AWS and demonstrate
    that you can deploy a Jupyter notebook as a containerized application for your
    data scientists to use.
  prefs: []
  type: TYPE_NORMAL
- en: Lab instruction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this hands-on exercise, you are going to create a Kubernetes environment
    using Amazon EKS, which is a managed service for Kubernetes on AWS that creates
    and configures a Kubernetes cluster with both master and worker nodes automatically.
    EKS provisions and scales the control plane, including the API server and backend
    persistent layer. It also runs the open-source Kubernetes and is compatible with
    all Kubernetes-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: After the EKS cluster is created, you will explore the EKS environment to inspect
    some of its core components, and then you will learn how to deploy a containerized
    Jupyter Notebook application and make it accessible from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s complete the following steps to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch the AWS CloudShell service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log on to your AWS account, select the **Oregon** Region, and launch AWS CloudShell.
    CloudShell is an AWS service that provides a browser-based **Linux** terminal
    environment to interact with AWS resources. With CloudShell, you authenticate
    using your AWS console credential and can easily run **AWS** **CLI**, **AWS**
    **SDK**, and other tools.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Install the `eksctl` utility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Follow the installation instructions for Unix at [https://github.com/weaveworks/eksctl/blob/main/README.md#installation](https://github.com/weaveworks/eksctl/blob/main/README.md#installation)
    to install eksctl. The `eksctl` utility is a command-line utility for managing
    the EKS cluster. We will use the `eksctl` utility to create a Kubernetes cluster
    on Amazon EKS in *Step 3*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following command to start creating an EKS cluster in the **Oregon**
    Region inside your AWS account. It will take about 15 minutes to complete running
    the setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The command will launch a `cloudformation` template and this will create the
    following resources:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An Amazon EKS cluster with two worker nodes inside a new Amazon **virtual private
    cloud** (**VPC**). Amazon EKS provides fully managed Kubernetes master nodes,
    so you won’t see the master nodes inside your private VPC.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An EKS cluster configuration file saved in the `/home/cloudshell-user/.kube/config`
    directory on CloudShell. The `config` file contains details such as the API server
    `url` address, the name of the admin user for managing the cluster, and the client
    certificate for authenticating to the Kubernetes cluster. The `kubectl` utility
    uses information in the `config` file to connect and authenticate to the Kubernetes
    API server.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'EKS organizes worker nodes into logical groups called `nodegroup`. Run the
    following command to look up the `nodegroup` name. You can look up the name of
    the cluster in the EKS management console. The name of the node group should look
    something like `ng-xxxxxxxx`:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Install the `kubectl` utility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the instructions at [https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html](https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html)
    to install kubectl for Linux. You will need to know the version of the Kubernetes
    server to install the matching kubectl version. You can find the version of the
    Kubernetes server using the `kubectl version --short` command.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Explore the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now the cluster is up, let’s explore it a bit. Try running the following commands
    in the CloudShell terminal and see what is returned:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Table 6.1 – kubectl commands ](img/B20836_06_12.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.12: kubectl commands'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploy a Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s deploy a Jupyter Notebook server as a containerized application. Copy
    and run the following code block. It should create a file called `deploy_Jupyter_notebook.yaml`.
    We will use a container image from the Docker Hub image repository:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create a Deployment by running the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check to make sure the Pod is running by executing `kubectl get pods`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Check the logs of the Jupyter server Pod by running `kubectl logs <name of notebook
    pod>`. Find the section in the logs that contains `http://jupyter-notebook-598f56bf4b-spqn4:8888/?token=XXXXXXX...`,
    and copy the token (`XXXXXX…`) portion. We will use the token for *Step 8*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also access the Pod using an interactive shell by running `kubectl exec
    --stdin --tty <name of notebook pod> -- /bin/sh`. Run `ps aux` to see a list of
    running processes. You will see a process related to the Jupyter notebook.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Expose the Jupyter notebook to the internet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, we have a Jupyter server running in a Docker container in a Kubernetes
    Pod on top of two EC2 instances in an AWS VPC but we can’t get to it because the
    Kubernetes cluster doesn’t expose a route to the container. We will create a Kubernetes
    Service to expose the Jupyter Notebook server to the internet so it can be accessed
    from a browser.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following code block to create a specification file for a new Service.
    It should create a file called `jupyter_svc.yaml`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After the file is created, run `kubectl apply -f jupyter_svc.yaml` to create
    the service. A new Kubernetes Service called `jupyter-service`, as well as a new
    `LoadBalancer` object, should be created. You can verify the service by running
    `kubectl get service`. Note and copy the `EXTERNAL-IP` address associated with
    the `jupyter-service` service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Paste the `EXTERNAL-IP` address into a new browser window and enter the token
    you copied earlier into the **Password or token** field to log in as shown in
    the following screenshot. You should see a Jupyter Notebook window showing up:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Jupyter login screen ](img/B20836_06_13.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.13: Jupyter login screen'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The following diagram shows the environment that you have created after working
    through the hands-on exercise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Jupyter notebook deployment on the EKS cluster ](img/B20836_06_14.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.14: Jupyter Notebook deployment on the EKS cluster'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congratulations, you have successfully created a new Amazon EKS cluster on AWS
    and deployed a Jupyter server instance as a container on the cluster. We will
    reuse this EKS cluster for the next chapter. However, if you don’t plan to use
    this EKS for a period of time, it is recommended to shut down the cluster to avoid
    unnecessary costs. To shut down the cluster, run `kubectl delete svc <service
    name>` to delete the service first. Then run `eksctl delete cluster --name <cluster
    name>` to delete the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered Kubernetes, a robust container management platform
    that forms the infrastructure foundation for constructing open-source ML platforms.
    Throughout this chapter, you gained an understanding of containers and insights
    into the functioning of Kubernetes. Moreover, you acquired hands-on experience
    in establishing a Kubernetes cluster on AWS by leveraging AWS EKS. Additionally,
    we explored the process of deploying a containerized Jupyter Notebook application
    onto the cluster, thereby creating a fundamental data science environment. In
    the next chapter, we will shift our focus toward exploring a selection of open-source
    ML platforms that seamlessly integrate with the Kubernetes infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code7020572834663656.png)'
  prefs: []
  type: TYPE_IMG
