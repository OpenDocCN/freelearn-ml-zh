- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time Series Analysis and Forecasting with MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series data constitutes a sequence of measurements gathered over a certain
    period. These measurements, which are tied to a specific variable, occur at regular
    intervals. An essential characteristic of time series data lies in the significance
    of its order; the arrangement of observations on a timeline conveys meaningful
    patterns. Altering this order can completely reshape the data’s meaning. Sequential
    data is a broader concept that encompasses any data presented in a sequential
    manner, which includes time series data. In this chapter, we will delve into the
    fundamental concepts surrounding sequential data, elucidating how to construct
    models that capture patterns within time series or any sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the basic concepts of time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting statistics from sequential data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a model to predict stock market data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with imbalanced datasets in MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce basic machine learning concepts. To understand
    these topics, a basic knowledge of algebra and mathematical modeling is needed.
    You will also require a working knowledge of MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with the MATLAB code in this chapter, you will need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`Nile.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TSLA.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TeslaStockForecasting.m`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the basic concepts of time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A time series represents a chronological sequence of recorded observations,
    such as monthly revenue, daily stock prices, weekly interest rates, annual profits,
    and more. The primary objective of time series analysis is to examine the historical
    progression of a phenomenon over time to anticipate its future trajectory. This
    predictive insight is derived from the assumption that recurring patterns observed
    in the past will continue to manifest in the future. We will explore the concepts
    of predictive forecasting and various forecasting methodologies, providing a detailed
    description of both.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding predictive forecasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Predicting the trajectory of variables is immensely significant when it comes
    to formulating plans and policies for any endeavor. For instance, when planning
    a company’s production strategy, it isn’t sufficient to merely understand whether
    the demand for products or services is on the rise or declining. It’s imperative
    to forecast the future trends in product demand, pricing dynamics, and the costs
    of raw materials. Collectively, these elements wield substantial influence over
    production activities.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasts hold a pivotal position at the core of the entire decision-making
    procedure. When forecasts are imprecise or insufficient, there exists a substantial
    risk of rendering the conclusions drawn from intricate decision models ineffective.
    The phrase *forecasting process* encompasses a range of intricate activities,
    whether overt or covert, that ultimately culminate in the creation of a forecast.
  prefs: []
  type: TYPE_NORMAL
- en: The terms *forecast* and *prediction* are frequently used interchangeably, but
    it’s beneficial to differentiate their meanings. Forecasting involves associating
    probabilities with future events or defining confidence intervals to estimate
    the range of values likely to occur in the future. On the contrary, prediction
    entails identifying the exact value a measurable quantity will take on in the
    future. Consequently, it’s straightforward to connect forecasts with the predictions
    made by employing traditional inferential statistical tools to derive corresponding
    confidence intervals. The primary goal of all forecasting models is to determine
    an estimation of the anticipated value, coupled with an estimation of the potential
    error the forecasting model may generate.
  prefs: []
  type: TYPE_NORMAL
- en: The forecast horizon is a crucial factor that distinguishes the forecasting
    process significantly. Forecasts can pertain to the immediate future, spanning
    up to 12 months, and serving as a foundation for operational decision-making.
    This might involve predicting product demand for the next 2 months. Alternatively,
    forecasts may extend into the medium term, ranging from 12 to 24 months, supporting
    decisions related to production planning. In the third scenario, forecasts are
    aimed at a more distant future, beyond the 24-month mark. In this case, they are
    crafted to underpin managerial decisions concerning company development plans.
    Across these three scenarios, each characterized by a short, medium, or long prediction
    horizon, the objectives of decision-makers seeking to utilize these forecasts
    differ significantly. Likewise, the level of precision and detail required for
    these respective forecasts also varies.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing forecasting methodologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Forecasting methodologies refer to the various techniques and approaches that
    are used to predict future values, trends, or events based on historical data
    and patterns. These methodologies are essential in a wide range of fields, including
    economics, finance, business, and meteorology, among others. Forecasting methodologies
    vary primarily depending on the characteristics and intended purposes of the decisions
    they support. Factors such as the time horizon’s duration, the quality and consistency
    of historical data, and the specific attributes of the product being forecasted,
    such as its stage in the life cycle, play significant roles in determining the
    appropriate forecasting method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, forecasting methods can be categorized into two major groups: **quantitative
    methods** and **qualitative methods**.'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining quantitative forecasting methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Quantitative methods are employed when there is sufficient quantitative data
    available, enabling predictions of future outcomes based on historical data. Typically,
    these methods find application in making short - to intermediate-term decisions.
    Here are some examples of quantitative prediction methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time series analysis**: In this method, the phenomenon to be predicted is
    treated as an opaque entity, and the focus is not on identifying influencing factors.
    The objective of this approach is to recognize past patterns in the phenomenon’s
    evolution and extend those patterns into the future to make predictions. In simpler
    terms, predictions are based on the historical behavior of the phenomenon over
    time rather than being linked to explanatory variables. For example, this approach
    is often used in analyzing sales trends, GDP trends, and similar data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression analysis**: Regression models quantify the correlation between
    one or more predictor variables and a response variable. These methods operate
    under the assumption that the variable to be forecasted can be associated with
    one or more independent or explanatory variables. This method is suitable for
    predicting numerical outcomes and is commonly used in economics, finance, and
    social sciences. For instance, the demand for consumer goods in a household is
    believed to be influenced by factors such as income, the ages of household members,
    and other related variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exponential smoothing**: Exponential smoothing techniques allocate exponentially
    decreasing weights to previous observations, prioritizing the significance of
    more recent data points. This helps in smoothing out noise and identifying trends
    in time series data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning**: Machine learning algorithms, including decision trees,
    random forests, and neural networks, find application in forecasting tasks, especially
    when handling extensive datasets or intricate relationships. They can capture
    non-linear patterns and are increasingly employed in various forecasting applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These methods are applicable under the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: Sufficient historical data is accessible regarding the phenomenon’s previous
    patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to quantify this historical information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be reasonably assumed that the attributes characterizing past trends
    will persist into the future, allowing for accurate forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, quantitative methods are employed when there is an ample supply
    of quantifiable historical data.
  prefs: []
  type: TYPE_NORMAL
- en: Describing qualitative forecasting methodologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Qualitative methods primarily rely on judgments, making them dependent on the
    viewpoints and assessments of consumers and experts. These methods come into play
    when quantitative data is scarce, yet ample qualitative information is available.
    Here are a few instances illustrating the application of qualitative methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Expert judgment**: Qualitative forecasting often relies on the expertise
    and opinions of subject matter experts. The Delphi method and expert panels are
    examples of techniques that gather input from knowledgeable individuals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Market research**: Qualitative research methods, such as surveys, focus groups,
    and customer interviews, are used to gather information about consumer preferences,
    behaviors, and market trends. This data can be valuable for predicting future
    market conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario analysis**: Qualitative forecasting can involve the creation of
    multiple scenarios or narratives about possible future events. Decision-makers
    then consider these scenarios to make informed decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Historical analogy**: This method involves making predictions based on similarities
    with past events or situations. It can be useful when dealing with unique or novel
    situations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expert systems**: Expert systems and AI-driven tools can assist in qualitative
    forecasting by aggregating and interpreting data from various sources to provide
    insights and predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice between quantitative and qualitative methods for forecasting depends
    on various factors, including the nature of the data, the availability of historical
    information, the specific forecasting goals, and the level of subjectivity involved.
    In many cases, a combination of both quantitative and qualitative methods is used
    to improve the accuracy and robustness of forecasts, especially in complex decision-making
    contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A time series represents a collection of observations related to a phenomenon,
    recorded at consecutive time points or intervals. These intervals are typically,
    though not always, evenly spaced or of uniform duration. Instances of time series
    data include diverse elements such as trends in commodity prices, stock market
    indices, the BTP/BUND spread, and unemployment rates.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike classical statistics, which frequently assumes that independent observations
    stem from a single random variable, time series analysis presupposes *n* observations
    derived from a multitude of dependent random variables. Consequently, the analysis
    of a time series entails a procedure aimed at deciphering the underlying generating
    process behind the observed data, as opposed to treating each observation as independent
    and identically distributed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the data collected, time series data can be divided into two main
    types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous time series**: Data is recorded continuously over a specific period
    without any gaps or interruptions. This type of time series is common in applications
    such as sensor data, where measurements are taken at very short intervals, often
    in real time. Examples include temperature readings recorded every second, stock
    prices updated every minute, or heart rate monitoring over continuous intervals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discrete time series**: Data is recorded at specific, discrete time intervals,
    which may not necessarily be evenly spaced. These intervals can be daily, weekly,
    monthly, annually, or any other predetermined period. Discrete time series data
    is often used in economic, financial, and social sciences to analyze trends and
    patterns over time. Examples include monthly sales figures, annual GDP growth
    rates, or weekly website traffic statistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice between continuous and discrete time series depends on the nature
    of the data being collected and the specific objectives of the analysis. Each
    type of time series has its own set of statistical techniques and tools that are
    used for modeling, forecasting, and extracting meaningful insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series data can also be categorized into two main types based on the underlying
    nature of the data-generating process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deterministic time series**: This type is characterized by a clear and predictable
    pattern or relationship between data points. The data values in a deterministic
    time series follow a specific mathematical function or rule. Deterministic patterns
    can be expressed as mathematical equations or formulas and may include periodic
    or seasonal variations, trends, or specific mathematical relationships. Deterministic
    time series often exhibit little or no randomness, making them highly predictable.
    Examples of this type of time series are a sine wave representing daily temperature
    variations over a year, a linear trend in stock prices, where prices consistently
    increase or decrease over time, and monthly sales figures influenced by a known
    seasonal pattern, such as holiday sales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic time series**: This is characterized by randomness, and the data
    points are not governed by a specific deterministic rule. Instead, they are influenced
    by random or probabilistic factors. Stochastic time series exhibit inherent randomness,
    making it challenging to predict future values with certainty. They often involve
    elements of uncertainty, noise, and randomness that cannot be fully explained
    by a deterministic model. Statistical techniques, such as **Autoregressive Integrated
    Moving Average** (**ARIMA**) models or state-space models, are commonly used to
    analyze and forecast stochastic time series. Examples of this type of time series
    are stock prices, which are influenced by a multitude of unpredictable factors
    and exhibit daily fluctuations that cannot be precisely predicted, daily weather
    conditions, where variables such as rainfall or temperature can vary randomly,
    and daily website traffic, which is influenced by user behavior and external factors,
    resulting in random fluctuations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, many real-world time series data contains a combination of deterministic
    and stochastic components. Analyzing and modeling time series data effectively
    often requires identifying and separating these components to gain insights and
    make accurate forecasts or predictions. Deterministic models are useful when the
    underlying patterns are well defined and stable, while stochastic models are employed
    when randomness plays a significant role in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, when dealing with time series data that exhibits both deterministic
    and stochastic components, it is common to represent the series as the sum of
    these two contributions. This approach allows for a more comprehensive understanding
    of the data and facilitates modeling and analysis. Mathematically, it can be expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Y t = f(t) + w(t)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y**t*: This represents the observed time series data at time *t*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f(t)*: This represents the deterministic component or contribution at time
    *t*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w(t)*: This represents the stochastic (random) component or contribution at
    time *t*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By decomposing the time series into its deterministic and stochastic parts,
    analysts and researchers can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Analyze trends**: The deterministic component often includes trends, seasonality,
    and other structured patterns that can be analyzed separately to understand the
    underlying dynamics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model stochastic behavior**: The stochastic component captures the random
    fluctuations and noise in the data. Models such as **ARIMA** or **Generalized
    Autoregressive Conditional Heteroskedasticity** (**GARCH**) can be applied to
    this component for forecasting or risk assessment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This decomposition approach enhances the ability to make forecasts, identify
    anomalies, and gain insights into the factors influencing the time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Using time denoted as *t = 1 …. T*, we represent the sequence, yt. Time serves
    as the critical parameter governing the sequence of events that must not be overlooked.
    Consequently, understanding the temporal dimension’s position of observation becomes
    essential. Typically, this information is depicted as the pair of values (*t*,
    *yt*) on a Cartesian graph, creating a continuous line graph that conveys the
    impression of continuous detection of the phenomenon. This graphical representation
    is commonly known as a time series plot (*Figure 9**.1*).
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how a series plot can be drawn, we can use the `Nile.csv` file
    in this book’s GitHub repository. This file provides data on the annual flow of
    the Nile river at Aswan from 1871 to 1970, measured in units of 108 cubic meters.
    We can import this dataset into MATLAB using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To draw a time series plot, we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the dot format, where `time` and `Nile` are the names of the two features
    selected. The following time series plot will be drawn:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 9.1 – Measurements of the annual flow of the Nile \uFEFFriver at Aswan](img/B21156_09_01.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Measurements of the annual flow of the Nile river at Aswan
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 9**.1*, there appears to be a significant change point in
    the data around 1898, suggesting a notable shift or change in the river’s flow
    characteristics during that period.
  prefs: []
  type: TYPE_NORMAL
- en: A time series plot provides an immediate insight into trends, recurring patterns,
    and other systematic behaviors that evolve. In the previous graph, we can see
    annual data exhibiting a consistent declining trend spanning a considerable time
    frame. Notably, there is a recurring zigzag pattern due to the data being recorded
    monthly, which is indicative of seasonality. It’s worth noting that the high peaks
    occur consistently during months when rainfall is anticipated.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate analysis of time series aims to decipher the dynamic processes that
    underlie the series and predict future occurrences of the phenomenon. In this
    analysis, we focus exclusively on the data pairs (*t, yt*), where *t = 1, …..,T*.
    The crucial concept here is that both past and present data contain valuable information
    for forecasting the future development of the phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s essential to recognize that univariate analysis might be overly
    restrictive in some cases. Often, we possess information related to phenomena
    associated with the one under consideration, which should be suitably integrated
    to enhance the model’s predictive capabilities. Nevertheless, univariate analysis
    serves as a valuable baseline, allowing us to validate more sophisticated modeling
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a time series plot, four distinct patterns can be identified concerning
    the progression of time:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Horizontal pattern**: In this scenario, the series fluctuates around a constant
    value, which is typically the series average. Such a series is referred to as
    being stationary on average. This pattern is commonly encountered in quality control,
    where processes are consistently maintained around an average value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonal pattern**: A seasonal pattern emerges when the series is influenced
    by recurring seasonal factors, such as monthly, semi-annual, or quarterly variations.
    Products such as ice cream, soft drinks, and electricity consumption often exhibit
    seasonal patterns. These series, which are influenced by seasonality, are also
    known as periodic series since the seasonal cycle repeats at fixed intervals.
    In the case of annual data, seasonality may not be evident.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cyclic pattern**: The presence of a cyclic pattern is characterized by irregular
    increases and decreases in the series that do not follow a fixed period. This
    distinguishes cyclic fluctuations from seasonal ones. Furthermore, cyclic oscillations
    typically exhibit larger amplitudes compared to seasonal variations. In economic
    series, cyclic patterns are driven by economic expansions and contractions resulting
    from speculative phenomena.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trend**: Trends are recognized by sustained, long-term increases or decreases
    in the series. For instance, the global population series exemplifies an increasing
    trend, while the monthly beer sales series may not display any discernible trend
    and instead possesses a stable horizontal background pattern.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many time series exhibit a combination of these patterns, and it is precisely
    this kind of complexity that adds a high level of interest to the forecasting
    task. Forecasting methods must possess the capability to discern and replicate
    the various components of the series. This involves the assumption that past patterns
    will persist and continue to manifest their characteristic features in the future.
  prefs: []
  type: TYPE_NORMAL
- en: The classical approach to time series analysis involves dissecting the deterministic
    portion of the series into a set of signal components, which convey the structural
    information of the series while filtering out the negligible noise. In practical
    terms, our objective is to identify some of the previously mentioned patterns
    within the time series trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'In time series analysis, three fundamental components are used to model and
    understand the underlying patterns in the data: trend, seasonal, and residual
    components. These components help decompose a time series into its constituent
    parts, making it easier to analyze and forecast the data accurately:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trend component**: It represents the long-term, systematic, and often smooth
    movement or direction in the time series. It reflects the underlying, sustained
    changes or growth (increasing trend) or declines (decreasing trend) in the data
    over an extended period. The trend component aims to capture the overall behavior
    of the time series, making it essential for understanding its fundamental trajectory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonal component**: It accounts for the recurring patterns or fluctuations
    in the time series that follow a fixed and known period. These patterns repeat
    at regular intervals, such as daily, monthly, quarterly, or annually. Seasonality
    is often linked to external factors such as weather, holidays, or business cycles
    and tends to recur predictably over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual component**: This component, also known as the error or noise, represents
    the random fluctuations or unexplained variability in the time series data. It
    includes all the information not accounted for by the trend and seasonal components.
    Analyzing the residual component helps identify irregularities, outliers, or unexpected
    events in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematically, a time series (*Y**t*) can be expressed as the sum of these
    three components. Decomposing a time series into these components is a fundamental
    step in time series analysis. Once separated, analysts can model each component
    individually, apply appropriate statistical techniques, and make more accurate
    forecasts. This decomposition helps in understanding the underlying dynamics of
    the data and can provide valuable insights for decision-making and forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the different approaches to time series modeling and how it is possible
    to extract knowledge from these types of data.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting statistics from sequential data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series data represents a sequence of measurements gathered over a certain
    period. These measurements are linked to a specific variable and are obtained
    at regular intervals. An essential characteristic of time series data is its inherent
    order, where the arrangement of observations along a timeline conveys significant
    information. Altering the sequence can completely change the data’s meaning. Sequential
    data, on a broader scale, encompasses any data presented sequentially, including
    time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Our primary goal is to develop models that capture the underlying patterns within
    time series data or any sequential data. These models are instrumental in describing
    essential aspects of the time series patterns. They enable us to explore how past
    data influences the future, examine correlations between datasets, make future
    predictions, or control variables based on specific metrics. To visually represent
    time series data, we commonly employ line charts or bar graphs. Time series data
    analysis finds extensive use in various domains, including finance, signal processing,
    weather forecasting, trajectory prediction, earthquake prediction, and any field
    dealing with temporal data.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of time series and sequential data analysis, the models we construct
    must consider data ordering and extract relationships among adjacent data points.
    Now, let’s explore some MATLAB code examples for analyzing time series and sequential
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Converting a dataset into a time series format in MATLAB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A time series represents a sequence of observations of a phenomenon collected
    at consecutive moments or time intervals, typically evenly spaced or of consistent
    duration, although not necessarily so. It’s crucial to recognize that time plays
    a fundamental role in the analysis of a time series. To begin, we need to develop
    proficiency in handling data that portrays a prolonged observation of a specific
    phenomenon. Our initial step will involve learning how to transform a sequence
    of observations into time series data and then visualizing it.
  prefs: []
  type: TYPE_NORMAL
- en: In MATLAB, to adequately handle time series data, we can use the `timetable`
    object. A timetable is a specialized form of a table that’s designed for handling
    time series data. Similar to regular tables, timetables store data variables organized
    by columns, allowing for various data types and sizes, so long as they share the
    same number of rows. In addition to this flexibility, timetables offer specific
    functions tailored for time-related operations, enabling alignment, combination,
    and calculations involving timestamped data across one or more timetables.
  prefs: []
  type: TYPE_NORMAL
- en: The row times in a timetable consist of datetime or duration values that serve
    as labels for individual rows. You can access and manipulate a timetable by row
    time and variable. To access specific elements within a timetable, you can use
    round parentheses, *()*, to retrieve a subtable or curly braces, *{}*, to extract
    their contents. Furthermore, you can reference variables and the row times vector
    using their respective names. To identify and annotate events within a timetable,
    you can associate an event table with it. Event tables contain a record of event
    times, corresponding event labels, and additional event-related details.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we will see how it is possible to create a timetable from such
    data. Suppose we have stored data about the flow of a river, measured at different
    times, in a timetable. In addition to archiving, schedules provide functions to
    synchronize data with specified times. In addition, we can note the time to add
    more information about the measurement conditions and other variables to be measured.
    Therefore, we can associate a time with the variables present in the workspace.
    The values present in the variable containing the times become the times of the
    object lines. All the other input arguments become time-dependent variables. Let’s
    learn how to format timetable data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we must create the vector containing the variables, starting with
    the time vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, we have the time of the measurements, then the value of the temperatures
    measured in this place at this time, and finally the values of the river flow
    measured in units of 108 cubic meters. To set the time format, we used the `datetime()`
    function: a datetime array contains information about the year, month, day, hour,
    minute, and second components associated with each recorded point in time, adhering
    to the proleptic ISO calendar system. Additionally, datetime arrays offer versatile
    formats for displaying output and parsing input text, support for storing fractional
    seconds with precision down to nanoseconds, and properties to handle considerations
    such as time zones, daylight saving time, and leap seconds.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The data that’s used is corrected and added to the time vector in a format
    compatible with the type of data. Now, we can use the `timetable()` function to
    create the object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A new timetable object was created in the MATLAB workspace. We can check the
    object’s type as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To understand how the `timetable()` function handles the data, we will print
    the object that was created, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The timeline can also be constructed based on specific values of some parameters.
    For example, we can use the `RiverFlow` variable already available in the MATLAB
    workspace to associate each of its values with a specific time value defined in
    a specific timeline. For example, we can define a time step as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We set `TimeStep` to one year and `StartTime` to 1870\. By doing this, one value
    of `RiverFlow` is associated with a new year starting from 1870 with an increment
    of one year.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding time series slicing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Slicing and dicing are two expressions that are used in the context of datasets,
    signifying the process of partitioning a substantial dataset into smaller segments
    or examining it from various perspectives to gain deeper insights. These terms
    draw an analogy from culinary terminology, describing two fundamental knife techniques
    that chefs must proficiently execute. Slicing entails cutting, whereas dicing
    involves cutting food into finely uniform sections, often performed consecutively.
    In data analysis, the concept of slice and dice typically encompasses the systematic
    breakdown of a large dataset into more manageable portions to extract additional
    information.
  prefs: []
  type: TYPE_NORMAL
- en: In MATLAB, a timetable is a specialized form of table that links a specific
    time with each row. You can efficiently extract time-related subsets of its data
    through various methods, such as identifying times within a specified range or
    matching recurring time intervals.
  prefs: []
  type: TYPE_NORMAL
- en: 'After learning how to create a timetable object from variables in the workspace,
    we can learn how to import and manipulate an external file into MATLAB as a timetable
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `.csv` file we used in the *Exploring the basic concepts of
    time series* *data* section. I’m referring to the `Nile.csv` file; we can import
    this file using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let’s learn how to access this specific type of data. You can employ dot
    notation to retrieve the row times of a timetable. Furthermore, you can access
    specific variables by using dot notation individually or access all the data within
    a timetable using its name, as we just did.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To display the first two lines of the time series object, we can use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following data is printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To access only the data of one variable, we can use dot notation, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Retrieve the entirety of the timetable data as a matrix using the `NileRiverFlowData.Variables`
    syntax. This syntax relies on the second dimension name of the timetable and is
    functionally identical to accessing all contents through curly brace indexing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: However, it’s important to note that the resulting matrix does not incorporate
    row times as the row times vector is considered metadata within the timetable
    and not treated as a variable. In cases where the timetable data cannot be effectively
    concatenated into a matrix, an error message will be generated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To locate data within a particular range, you can utilize the `timerange` function,
    which establishes time-based subscripts for indexing. For example, you can set
    up a range that commences on January 1, 1880, and concludes on January 1, 1920\.
    It’s important to note that by default, the `timerange` function defines a half-open
    interval that includes the left endpoint but excludes the right endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can slice a portion of the dataset defined from this range, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The new dataset contains data starting from January 1, 1880, and ending on January
    1, 1919.
  prefs: []
  type: TYPE_NORMAL
- en: Resampling time series data in MATLAB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s see how it’s possible to resample time series data in MATLAB. This task
    involves changing the frequency or time intervals of the data while preserving
    the essential information. This can be useful for various purposes, such as aggregating
    data to a coarser time scale, interpolating data to a finer time scale, or aligning
    data from different sources with a common time grid. MATLAB provides several functions
    and methods for resampling time series data. Here’s a basic example of how to
    do it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by using the `retime()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This function resamples or consolidates data within a timetable while addressing
    issues related to duplicate or irregular time entries. The `retime()` function
    generates a timetable by incorporating variables from the initial data and enforces
    a consistent time step for the row times. This function resamples or consolidates
    data within the initial variables based on the specified method. `retime()` can
    be applied to interpolate data values from initial values at different timestamps,
    aggregate data into time intervals, eliminate rows with duplicate timestamps in
    the timetable, and transform an irregular timetable into a regular one by imposing
    regularly spaced row times, as defined by the new *time step*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we resampled the dataset by adding the weekly data starting
    from the monthly, but there are different ways to resample data. We performed
    data value interpolation based on the adjacent rows. It is essential that the
    input timetable has sorted and distinct row times. There are four different types
    of interpolation available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`linear`: Linear interpolation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spline`: Piecewise cubic spline interpolation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pchip`: Shape-preserving piecewise cubic interpolation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`makima`: Modified Akima cubic Hermite interpolation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are the possibilities to aggregate the time series data using the same
    function (`retime()`). The `retime()` function offers aggregation options, including
    the mean. For example, we can compute the mean values monthly for the given data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we aggregated the previously resampled weekly data using the average
    as the aggregation criterion.
  prefs: []
  type: TYPE_NORMAL
- en: Moving average
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A moving average is a commonly used method for analyzing time series data. It
    involves calculating the average of a set of data points within a specific window
    or time interval as it moves through the time series. This helps to smooth out
    fluctuations and highlight underlying trends in the data. There are different
    types of moving averages, such as the **simple moving average** (**SMA**) and
    the **exponential moving average** (**EMA**), each with its characteristics and
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'SMA is a basic and widely used method for analyzing time series data. It is
    a statistical calculation that helps smooth out fluctuations in data to identify
    trends or patterns. Here’s how you calculate SMA:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a specific period or window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each data point, take the average of the data points within that time window.
    This involves adding up the values over the chosen period and dividing by the
    number of data points in that window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move the window one time step forward and calculate the average again. Repeat
    this process for each data point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result is a series of averages that represent the trend in the data over
    the chosen period. SMAs are commonly used for various purposes, including identifying
    trends, smoothing out noise in time series data, and making forecasts. For example,
    if you are interested in calculating a 10-day SMA for daily stock prices, you
    would average the closing prices of the last 10 days to get a moving average value
    for each day. This moving average can help you identify the general direction
    of the stock’s price trend over those 10 days.
  prefs: []
  type: TYPE_NORMAL
- en: 'In MATLAB, you can calculate an SMA for a time series using the `movmean()`
    function, which is available in the Statistics and Machine Learning Toolbox. Let’s
    learn how to use the `movmean()` function with a practical example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by defining the window size for the SMA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We defined a five-year window to calculate the moving average.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can calculate the moving average:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `movmean()` function generates an array of local mean values for k-point
    neighborhoods. Each mean is computed by sliding a window with a length of *k*
    across adjacent elements of the array, *A*. If *k* is an odd number, the window
    centers around the current position’s element. For even values of *k*, the window
    centers around the current and preceding elements. The window size adjusts automatically
    at the array’s endpoints when there are insufficient elements to fill it. In such
    cases, the average is calculated based on the elements that fit within the window.
    The resulting array, denoted as *M*, maintains the same dimensions as array *A*.
    When *A* is a one-dimensional vector, `movmean()` processes it along the vector’s
    length. For multidimensional arrays, `movmean()` operates along the first dimension
    of *A* without a size of 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The EMA is a variant of the moving average, placing greater emphasis on recent
    data points. This characteristic renders it more responsive to short-term price
    fluctuations when compared to the SMA. To calculate an EMA, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a specific period, typically represented as *N*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the multiplier, which is often referred to as the “smoothing factor”
    or “weighting multiplier.” This is usually calculated as *2 / (N +* *1)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with the EMA for the first data point, which is typically the SMA for
    the first *N* data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the subsequent data points, calculate the EMA using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: EMA(current) = (Price(current) − EMA(previous)) * Multiplier + EMA(previous)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*EMA(current)* is the EMA for the current data point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Price(current)* is the price of the asset at the current time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*EMA(previous)* is the EMA for the previous data point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multiplier* is the smoothing factor calculated in *Step 2*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EMA assigns more weight to recent data points, and as a result, it reacts
    more quickly to price changes than the SMA. It’s commonly used in technical analysis
    to identify trends and potential reversal points in financial markets, such as
    stocks, currencies, and commodities. Traders and analysts often use different
    EMA periods to analyze short-term and long-term trends.
  prefs: []
  type: TYPE_NORMAL
- en: MATLAB, for instance, provides functions such as `movavg()` to calculate various
    types of moving averages, including the EMA. You can use this function to compute
    EMAs in MATLAB. Remember that the `movavg()` function requires the use of Financial
    Toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the type of moving average, after which we define the
    window size and calculate the EMA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The EMA also calculates an average over a specific time window, but it assigns
    more weight to recent data points. It gives exponentially decreasing weight to
    older data points. The EMA is more responsive to recent changes in the data. It
    reacts quickly to price changes, making it suitable for short-term trend analysis
    and capturing price reversals.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential smoothing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exponential smoothing is a time series forecasting method that is used to make
    predictions about future data points based on past observations. It is particularly
    useful for data with a trend or seasonal component. Exponential smoothing assigns
    exponentially decreasing weights to past data points, with the most recent observations
    receiving the highest weights. EMA and exponential smoothing are both techniques
    that are used in time series analysis to capture trends and patterns. However,
    they differ in their formulations and applications. EMA, a subtype of exponential
    smoothing, places higher emphasis on recent data points by assigning them greater
    weight in the calculation. The mathematical expression for the EMA explicitly
    includes a smoothing parameter (alpha), offering a straightforward approach to
    adjusting the impact of recent versus older observations. Conversely, exponential
    smoothing is a broader term that encompasses various smoothing techniques, with
    the EMA being a specific instance. While the EMA requires an initial value for
    the calculation, exponential smoothing, in general, demands an initial value that
    significantly influences the overall smoothing process.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several variations of exponential smoothing, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simple exponential smoothing** (**SES**): This is used for univariate time
    series data without a trend or seasonality. It uses a single smoothing parameter
    (alpha) to assign weights to past observations. The forecast for the next period
    is a weighted average of the most recent observation and the previous forecast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Holt’s linear exponential smoothing**: This is used for data with a linear
    trend but no seasonality. It extends SES by introducing two smoothing parameters,
    alpha for the level and beta for the trend. It calculates the level and trend
    separately and uses them to make forecasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Holt-Winters exponential smoothing**: This is used for data with both a trend
    and seasonality. It extends Holt’s linear exponential smoothing by adding a third
    smoothing parameter, gamma, for the seasonal component. It models and forecasts
    the level, trend, and seasonal components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basic idea behind exponential smoothing is to assign exponentially decreasing
    weights to past observations, giving more importance to recent data, and making
    the method adaptive to changes in the data pattern. Exponential smoothing is widely
    used in time series forecasting applications, such as sales forecasting, demand
    forecasting, and financial market predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Double and triple smoothing are advanced techniques in time series analysis,
    building upon the foundation of exponential smoothing. Double smoothing incorporates
    an additional level of smoothing to address trends, introducing a trend-smoothing
    parameter alongside the data-smoothing parameter. This method enhances the model’s
    ability to capture and forecast trends effectively. Triple smoothing, or the Holt-Winters
    method, goes a step further by incorporating seasonality components. It includes
    parameters for data smoothing, trend smoothing, and seasonality smoothing, making
    it particularly robust for time series data with consistent patterns. These techniques
    provide nuanced tools for capturing and forecasting complex trends and seasonal
    variations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In MATLAB, you can implement various types of exponential smoothing using functions
    and libraries related to time series analysis and forecasting. These functions
    often provide tools to estimate the smoothing parameters and make forecasts based
    on your data. Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the smoothing parameter (`alpha`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can adjust `alpha` based on our needs (`0` < `alpha` <= `1`). After that,
    we must initialize the forecast with the first data point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we must calculate the SES forecast:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can display the forecasts:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `alpha` parameter controls the smoothing level, with lower values making
    the forecast more responsive to recent data and higher values making it smoother.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we analyzed some statistical methods for extracting information
    from a time series. Now, let’s see a practical case in which we predict the performance
    of a company’s shares using recurrent networks.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a model to predict the stock market
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting stock market movements is a complex and challenging endeavor. It
    involves analyzing various factors and data points to forecast the future direction
    of stock prices. Fundamental analysts examine a company’s financial health, including
    its revenue, earnings, debt levels, and growth prospects. They also consider macroeconomic
    factors such as interest rates, inflation, and government policies that can impact
    the overall market. Technical analysts study historical price and volume data,
    looking for patterns and trends in stock charts. They use tools such as moving
    averages, support and resistance levels, and various technical indicators to make
    predictions. Identifying current market trends and understanding market cycles
    can provide insights into potential future movements. Bull markets, bear markets,
    and sideways markets can affect stock prices differently. Predictions are inherently
    uncertain, and risk management is crucial. Diversifying a portfolio, setting stop-loss
    orders, and using proper position sizing can help manage risks associated with
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Some traders and investors use machine learning and **artificial intelligence**
    (**AI**) algorithms to analyze vast amounts of data and identify potential patterns
    or trends in the market. Stock market predictions can range from short-term (days
    or weeks) to long-term (years). The chosen will depend on the investment strategy
    and goals. It’s important to remember that past stock market performance is not
    a guarantee of future results. Market conditions can change rapidly and unpredictably.
    Stock market predictions involve a combination of financial analysis, technical
    analysis, sentiment analysis, and consideration of various external factors. While
    some investors and traders may use predictions as part of their decision-making
    process, it’s essential to approach them with caution and maintain a diversified
    portfolio to manage risk effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will delve into the application of the **L****ong short-term
    memory** (**LSTM**) model for forecasting the future stock price of a highly renowned
    corporation: Tesla Inc. This is an American **electric** **vehicle** (**EV**)
    and clean energy company founded in 2003 by Elon Musk, JB Straubel, Martin Eberhard,
    Marc Tarpenning, and Ian Wright. It is headquartered in Palo Alto, California.'
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate Tesla’s stock price performance, we will utilize data encompassing
    the stock prices from November 6, 2010, to October 5, 2023, sourced from the NASDAQ
    GS stock quotes. It is also possible to adjust the timeframe if needed, beyond
    the default interval provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data was acquired from the Yahoo! Finance website: [https://finance.yahoo.com/quote/TSLA](https://finance.yahoo.com/quote/TSLA).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.2* shows a screenshot of the Yahoo! Finance website:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – The Yahoo! Finance website](img/B21156_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – The Yahoo! Finance website
  prefs: []
  type: TYPE_NORMAL
- en: 'This CSV file includes the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Date**: Date of quote'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open**: Open price'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High**: High price'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low**: Low price'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Close**: Close price adjusted for splits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adj Close**: Close price adjusted for both dividends and splits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume**: Exchange volume'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The information within the CSV file is labeled as `TSLA.csv`. To start, let’s
    investigate the process of importing this data into the MATLAB workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will import the data in timetable format using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As always, we should be in the folder containing the `TSLA.csv` file or add
    this folder to the MATLAB path.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before embarking on data prediction using the LSTM method, we will commence
    with an exploratory analysis to glean insights into the data distribution and
    extract preliminary knowledge. To glean preliminary insights from the imported
    dataset, we can utilize the `summary()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A summary of the data was returned with some statistics on all the variables
    in the dataset. For example, we can read the number of records (`157`) and the
    min, median, and max values of all variables. Based on the preliminary analysis
    of the results, it is evident that Tesla’s stock prices have undergone a significant
    transformation over the past 13 years. Specifically, the minimum value stands
    at $1.5927, while the maximum value reaches $381.59.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Upon reviewing the dataset’s content, our next step involves conducting an
    initial visual exploratory analysis, wherein we will generate a graphical representation
    of the stock prices over the years:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following chart will be drawn (*Figure 9**.3*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Stock prices of Tesla Inc. from 2010 to 2023](img/B21156_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Stock prices of Tesla Inc. from 2010 to 2023
  prefs: []
  type: TYPE_NORMAL
- en: Examining the preceding graph reveals a substantial increase in prices over
    time. Commencing in 2020, this upsurge follows an exponential trend. Let’s delve
    deeper into understanding the fluctuations that the Tesla stock has documented
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the evolution of a phenomenon becomes intriguing not only by examining
    its time series graph but also by drawing comparisons between the intensity of
    the phenomenon at distinct time points. This involves calculating variations in
    intensity from one period to another. Additionally, analyzing the trend of changes
    in the phenomenon between contiguous periods can offer valuable insights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We represent a time series as *Y1*, ..., *Yt*, ..., *Yn*. As explained in the
    *Time series analysis* section, a time series reflects the chronological recording
    of experimental observations of a variable. This variable can encompass various
    elements, such as price trends, stock market indices, spreads, and unemployment
    rates. Essentially, it functions as a sequence of time-ordered data points from
    which we endeavor to extract insights for characterizing the observed phenomenon
    and predicting future values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To measure the variation between two distinct time points (referred to as *t*
    and *t + 1*), we can compute the following ratio:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Y t+1 − Y t _ Y t
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This index, commonly known as “percentage change,” represents a percentage ratio.
    It specifically denotes the percentage rate of variation in the phenomenon, *Y*,
    at time *t + 1* compared to the preceding time, *t*. Utilizing the percentage
    change method provides a more accurate depiction of how data has evolved over
    a specified timeframe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This approach is applied not only in monitoring the prices of individual securities
    and major market indices but also in comparing the values of various currencies.
    When preparing balance sheets with comparative financial statements, it is typical
    to include specific asset prices at different time junctures, accompanied by the
    corresponding percentage changes during those periods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To calculate percentage changes in MATLAB, we can use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `diff()` function computes the differences between consecutive elements
    of an array along the first dimension where the size of that dimension is not
    equal to 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can plot the results obtained as a bar plot to see how the differences are
    distributed on the time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Figure 9**.4* shows the bar plot of percentage changes over the years:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Percentage changes over the years](img/B21156_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Percentage changes over the years
  prefs: []
  type: TYPE_NORMAL
- en: Percentage changes in the stock market play a crucial role in providing valuable
    insights and serving various functions. For example, percentage changes allow
    investors and analysts to measure the performance of individual stocks, portfolios,
    or entire market indices over specific periods. This helps in assessing whether
    investments have grown or declined in value. Understanding percentage changes
    helps investors gauge the volatility and risk associated with a particular stock
    or asset. Stocks with larger percentage fluctuations are generally considered
    riskier investments.
  prefs: []
  type: TYPE_NORMAL
- en: After calculating the percentage changes, we can move on to the return concept;
    return refers to the profit or loss that an investor realizes from their investments
    in stocks. Returns are typically expressed as a percentage and represent the change
    in the value of an investment over a specific period.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculating the return from stock prices in MATLAB involves taking the percentage
    change in prices over a specific period. We have just loaded stock price data
    into MATLAB as a matrix. We checked that the data was organized with prices in
    chronological order. After that, we calculated the percentage change in stock
    prices from one period to the next. Now, we can calculate various types of returns,
    such as price return (capital gain) or total return (including income). For price
    return, simply sum the percentage changes, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following result is obtained:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is a great return from the Tesla stock market that’s excellent for those
    who have held stocks for the last three years.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Our goal is to leverage the data within this dataset to forecast the Tesla
    stock price based on the information provided in the `.csv` file. To train the
    network, we require both input and output data. The input is defined by the data
    contained within the dataset. Consequently, we must generate our output. We will
    achieve this by assuming our objective is to forecast the Tesla stock price at
    time *t + 1* based on the information available at time *t*. As a result, we will
    establish the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Input = TeslaData . Close(t)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output = TeslaData . Close(t + 1)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is a sequence-to-sequence regression problem, such as a deep learning approach
    where the goal is to predict a sequence of continuous values as an output, given
    a sequence of input data. It is commonly used for tasks such as time series forecasting,
    natural language processing, and any other problem where the output is a sequence
    of numerical values. In this context, the model learns to map an input sequence
    to an output sequence, and the length of the input and output sequences can vary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A recurrent network possesses memory, and this memory is preserved by defining
    the time step. The time step determines how many steps into the past backpropagation
    are considered when computing gradients for weight updates during training. In
    this context, we establish *TimeStep = 1*. Let’s prepare the data based on this
    assumption:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Within this code, `XData` represents the input variable, equivalent to data
    at time *t*, while `YData` signifies the output value for the subsequent period,
    namely data at time *t +* *1*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To address this type of problem, we can use an LSTM network. LSTM is a type
    of **recurrent neural network** (**RNN**) architecture that’s designed to address
    the vanishing gradient problem that can occur when training traditional RNNs.
    LSTM models are especially effective for processing and making predictions on
    sequences of data. They are built around the concept of memory cells, which can
    store information over long sequences. These memory cells possess the capability
    to learn and remember patterns, rendering them well suited for tasks involving
    sequential data. Such tasks include time series analysis, natural language processing,
    and speech recognition.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The ideal lag value should be chosen based on the autocorrelation plot together
    with domain knowledge. To evaluate the performance of an LSTM model, long sequences
    (more lags) should be considered. Even an auto-regressive model would perform
    well for *lag=1*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'LSTM models have three fundamental gates that control the flow of information
    within the network:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Forget gate**: This is responsible for deciding which information from the
    previous cell state should be either forgotten or retained'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input gate**: This determines which new information should be stored in the
    cell state'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output gate**: This regulates the information to be output from the cell
    state'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The cell state runs horizontally across the top of the LSTM and acts as a conveyor
    belt, allowing information to flow from one time step to another. The gates help
    regulate the flow of information into and out of the cell state.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We start by setting some parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, we have to build the architecture of the deep network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Six layers were used:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sequenceInputLayer`: A sequence input layer receives sequential data and performs
    data normalization before feeding it into a neural network.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lstmLayer`: An LSTM layer, as part of an RNN, specializes in capturing long-term
    dependencies within time series and sequential data. This layer facilitates additive
    interactions, contributing to enhanced gradient flow over extended sequences during
    the training process. Two parameters were passed: the number of hidden units and
    the output mode as a sequence. One argument was supplied: the quantity of hidden
    units, also referred to as the hidden size, denoted as a positive integer. This
    parameter determines the volume of information retained by the layer across time
    steps, encapsulated in the hidden state. The hidden state can encompass data from
    all preceding time steps, irrespective of the sequence length. Oversizing the
    number of hidden units may lead to overfitting the training data. Additionally,
    the output mode is specified with one of two values: `sequence`, which produces
    the entire sequence as output, and `last`, which generates output for the last
    time step of the sequence.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fullyConnectedLayer`: A fully connected layer multiplies the input with a
    weight matrix and subsequently incorporates a bias vector.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropoutLayer`: This is used for regularization during training. It helps prevent
    overfitting by randomly setting a fraction of the input units to 0 during each
    forward and backward pass. This encourages the network to learn more robust and
    generalized features.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fullyConnectedLayer`: The last fully connected layer is often used for making
    predictions based on the features learned by the preceding layers in the network.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regressionLayer`: This is a layer that’s used in deep learning for regression
    tasks. Regression tasks involve predicting continuous numerical values, as opposed
    to classification tasks, which involve predicting categories or classes. `regressionLayer`
    is specifically designed for regression problems, and it is commonly used in neural
    networks for tasks such as predicting stock prices, house prices, temperature
    forecasting, and various other numerical predictions. `regressionLayer` is responsible
    for calculating the loss between the predicted values generated by the neural
    network and the actual target values during training. It employs a loss function
    to gauge the dissimilarity between the predicted values and the ground truth values.
    Common loss functions employed in regression tasks encompass **mean squared error**
    (**MSE**), **mean absolute error** (**MAE**), and Huber loss, among others.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After that, we have to specify the training options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the optimization algorithm, we set the Adam optimizer, a popular optimization
    algorithm used in training deep neural networks. It is known for its ability to
    efficiently handle non-stationary objectives, high-dimensional parameter spaces,
    and noisy gradient information. Adam combines the benefits of both the RMSprop
    optimizer and momentum-based methods. We also set the number of training epochs:
    an epoch is one complete pass through the entire training dataset. Training may
    stop early based on validation performance to avoid overfitting. We set an appropriate
    learning rate for the optimizer. You may need to experiment with different learning
    rates to find the one that works best for your problem.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can train the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following results are printed (*Figure 9**.5*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.5 – LSTM training process](img/B21156_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – LSTM training process
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use the network to predict the performance of stocks. Each value
    will be predicted starting from the previous one with a time step equal to 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `predict()` function forecasts outcomes by employing a well-trained deep
    learning neural network. To evaluate the performance of the prediction, we can
    use **root mean square error** (**RMSE**), which is calculated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: RMSE is a widely utilized metric in statistics and machine learning for quantifying
    the average magnitude of errors or differences between predicted and actual values.
    It calculates the square root of the mean of the squared differences between predicted
    values and true values. RMSE serves as a means to evaluate the accuracy of a predictive
    model or the quality of predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following value is returned:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To appreciate the prediction capacity of the model based on LSTMs, let’s draw
    a graph of the performance of the Tesla stock and compare it with that predicted
    by our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following graph is printed (*Figure 9**.6*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Actual values versus predicted values](img/B21156_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Actual values versus predicted values
  prefs: []
  type: TYPE_NORMAL
- en: We can appreciate the goodness of the forecast in periods in which the situation
    does not undergo major variations. On the contrary, a certain deviation is noted
    in correspondence with the periods of greatest fluctuation of the security, demonstrating
    that it is necessary to consider further information for a correct prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see what we can do when we must work with unbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with imbalanced datasets in MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dealing with imbalanced datasets is a common challenge in machine learning,
    particularly in classification tasks where one class significantly outnumbers
    the other(s). Handling imbalanced datasets is crucial because models trained on
    such data may exhibit bias toward the majority class and perform poorly in predicting
    the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding oversampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Oversampling is a method that’s employed to tackle class imbalance in a dataset
    by augmenting the number of instances belonging to the minority class. The aim
    is to balance the class distribution and prevent machine learning models from
    being biased toward the majority class. Oversampling is particularly useful when
    you have limited data for the minority class. There are several methods for oversampling,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random oversampling**: In random oversampling, you randomly select and duplicate
    instances from the minority class until the class distribution is balanced. This
    method is straightforward but may lead to overfitting as it introduces duplicate
    data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthetic Minority Over-sampling Technique** (**SMOTE**): SMOTE is a popular
    oversampling method that creates synthetic samples for the minority class. It
    works by selecting a minority class instance and its *k* nearest neighbors. Then,
    it generates synthetic data points along the line segments connecting the chosen
    instance and its neighbors. SMOTE helps to prevent overfitting and can be more
    effective than random oversampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive Synthetic Sampling** (**ADASYN**): ADASYN is an extension of SMOTE
    that’s designed to generate additional synthetic samples for minority class instances
    that pose greater difficulty in classification. It considers the distribution
    of the data and assigns different weights to different instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Borderline-SMOTE**: Borderline-SMOTE is a variation of SMOTE that specifically
    targets instances near the borderline between the minority and majority classes.
    It generates synthetic samples for these borderline instances to improve classification
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minority Synthetic Over-sampling Technique** (**MSMOTE**): MSMOTE is an extension
    of SMOTE that takes into account the density of minority class instances. It generates
    synthetic samples based on the local density of the data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random oversampling with noise**: This technique involves adding a small
    amount of random noise to the duplicated instances during random oversampling.
    This helps to reduce overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Informed oversampling**: Informed oversampling methods consider domain knowledge
    or specific characteristics of the data to guide the generation of synthetic samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of oversampling method depends on the characteristics of your dataset
    and the problem you are trying to solve. Experimenting with different techniques
    and evaluating the impact on model performance using appropriate metrics (for
    example, F1 score, precision-recall curves, and so on) is often necessary to determine
    the most effective oversampling strategy for your specific use case. It’s important
    to note that oversampling may not always be the best solution, and you should
    also consider other techniques, such as undersampling, cost-sensitive learning,
    and ensemble methods, to address class imbalance effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring undersampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Undersampling is a technique that’s used to address class imbalance in a dataset
    by reducing the number of instances in the majority class. The goal is to create
    a more balanced class distribution, which can help machine learning models avoid
    bias toward the majority class. Undersampling can be a useful approach when you
    have a large amount of data in the majority class and want to avoid the potential
    issues of overfitting that may arise with oversampling the minority class. Here
    are some key points to consider when exploring undersampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random undersampling**: In random undersampling, you randomly select and
    remove instances from the majority class until the class distribution is balanced.
    This method is simple but may lead to the loss of valuable data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster-centroid undersampling**: Cluster-centroid undersampling involves
    clustering the majority class instances and selecting a representative centroid
    from each cluster to form a smaller dataset. This method retains diversity within
    the majority class and is less likely to remove informative data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tomek links**: Tomek links are pairs of instances from different classes
    that are close to each other but classified incorrectly. Removing the majority
    class instance of a Tomek link can help improve the boundary between classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edited nearest neighbors** (**ENN**): ENN identifies and removes instances
    from the majority class that do not agree with their neighbors in terms of class
    labels. ENN can help in reducing noisy samples in the majority class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-sided selection**: One-sided selection combines both Tomek links and
    ENN. It uses Tomek links to identify problematic instances and then uses ENN to
    remove them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NearMiss undersampling**: This method selects the majority class instances
    that are closest to minority class instances based on distance metrics. NearMiss
    algorithms ensure that the selected majority class instances are close to the
    minority class, thus helping in improving class separation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Informed undersampling**: Informed undersampling methods consider domain
    knowledge or specific characteristics of the data to guide the selection of instances
    from the majority class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When exploring undersampling, it’s important to consider various aspects. First,
    choose the undersampling method that is most appropriate for your dataset. The
    method you choose should depend on the nature of your data, the problem you are
    trying to solve, and the potential impact on model performance. Also, undersampling
    may lead to a loss of information, especially if the majority class contains important
    data points. Be cautious when applying this technique, and consider whether you
    can obtain more data for the minority class or explore other methods such as oversampling,
    cost-sensitive learning, or ensemble methods. Remember to evaluate your model’s
    performance using appropriate metrics, and experiment with different approaches
    to find the best solution for your specific use case. For instance, suppose you
    have a dataset with 10,000 credit card transactions, out of which only 100 are
    fraudulent. In this case, you might undersample the non-fraudulent transactions
    to create a more balanced dataset. You could randomly select, say, 500 non-fraudulent
    transactions, resulting in a new dataset with 500 non-fraudulent and 100 fraudulent
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering cost-sensitive learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cost-sensitive learning is a machine learning approach that considers the varying
    costs of misclassifying different classes in a classification problem. In traditional
    machine learning, all misclassifications are treated equally. However, in many
    real-world applications, misclassifying one class may have more severe consequences
    or higher associated costs than misclassifying another class. Cost-sensitive learning
    aims to optimize the model to minimize these specific costs.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-sensitive learning is related to but different from handling class imbalance.
    While class imbalance focuses on the unequal distribution of classes, cost-sensitive
    learning considers the associated costs of misclassification. Cost-sensitive learning
    often involves the use of cost matrices, which specify the costs of different
    types of misclassifications. These matrices assign different costs for false positives,
    false negatives, true positives, and true negatives.
  prefs: []
  type: TYPE_NORMAL
- en: In cost-sensitive learning, the decision threshold for classification can be
    adjusted to minimize the expected cost. This means that the model might be more
    or less conservative in its predictions, depending on the cost matrix. Various
    machine learning algorithms can be adapted to handle cost-sensitive learning.
    Some algorithms allow you to specify class-specific misclassification costs. For
    example, in decision trees or ensemble methods, you can assign weights to classes
    based on their costs.
  prefs: []
  type: TYPE_NORMAL
- en: When working with cost-sensitive learning, it’s important to consider evaluation
    metrics that reflect the cost-effectiveness of the model, such as the weighted
    F1 score or cost-sensitive versions of precision and recall. Cost-sensitive learning
    is often applied to anomaly detection problems. In such cases, you want to detect
    rare and potentially costly events, such as fraud detection in financial transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that cost-sensitive learning involves trade-offs. Reducing the
    cost of one type of error may increase the cost of another. Careful consideration
    and domain knowledge are required when designing the cost matrix. Cost-sensitive
    learning is a valuable approach in situations where misclassification costs are
    not uniform, and it can help optimize machine learning models for specific real-world
    applications. It’s an essential technique for decision-making systems where the
    consequences of different errors can be significantly different.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored several key concepts in the field of data analysis
    and predictive modeling. We started by discussing the basics of time series data,
    which refers to data that is collected over a certain period and contains a sequential
    order. The extraction of statistics from such sequential data is then highlighted
    as an important step in analyzing and understanding patterns within the data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also emphasized the implementation of a model to predict stock
    market data. This involves using various techniques and algorithms to analyze
    historical stock market data, identify patterns and trends, and make predictions
    about future stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, this chapter addressed the challenge of dealing with imbalanced datasets
    in MATLAB. An imbalanced dataset refers to a situation where the distribution
    of classes within the dataset is significantly skewed, making it difficult to
    train a model accurately. We discussed methods and strategies to handle imbalanced
    datasets within the MATLAB programming environment.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this chapter focused on the importance of understanding and analyzing
    time series data, extracting meaningful statistics, implementing predictive models
    for stock market data, and addressing the challenges of imbalanced datasets in
    MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will understand the basic concepts of recommender systems,
    how to identify similar users in a dataset, and how to implement a practical case
    of recommender systems using MATLAB. Finally, we will understand model compression,
    pruning, and quantization for efficient inference on edge devices.
  prefs: []
  type: TYPE_NORMAL
