["```py\n<manifest ...>\n    <uses-permission android:name=\"android.permission.CAMERA\" />\n    <uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\" />\n    <application ...>\n```", "```py\npublic class Recorder extends AppCompatActivity {\n    private TextureView mPreview;  // For displaying the live camera preview\n    private Camera mCamera;        // Object to contact the camera hardware\n    private MediaRecorder mMediaRecorder;    // Store the camera's image stream as a video\n\n    private boolean isRecording = false; // Is video being recoded?\n    private Button btnRecord;            // Button that triggers recording\n```", "```py\nimport android.hardware.Camera;\nimport android.media.MediaRecorder;\nimport android.view.TextureView;\n```", "```py\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_recorder);\n\n        mPreview = (TextureView)findViewById(R.id.texturePreview);\n        btnRecord = (Button)findViewById(R.id.btnRecord);\n\n        btnRecord.setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View view) {\n                onCaptureClick(view);\n            }\n        });\n```", "```py\n    public void onCaptureClick(View view) {\n        if (isRecording) {\n            // Already recording? Release camera lock for others\n            mMediaRecorder.stop();\n            releaseMediaRecorder();\n            mCamera.lock();\n\n            isRecording = false;\n            releaseCamera();\n            mGyroFile.close();\n            mGyroFile = null;\n            btnRecord.setText(\"Start\");\n            mStartTime = -1;\n        } else {\n            // Not recording â€“ launch new \"thread\" to initiate!\n            new MediaPrepareTask().execute(null, null, null);\n        }\n    }\n```", "```py\n    private void releaseMediaRecorder() {\n        if(mMediaRecorder != null) {\n            mMediaRecorder.reset();\n            mMediaRecorder.release();\n            mMediaRecorder = null;\n            mCamera.lock();\n        }\n    }\n```", "```py\n    class MediaPrepareTask extends AsyncTask<Void, Void, Boolean>{\n        @Override\n        protected Boolean doInBackground(Void... voids) {\n            if(prepareVideoRecorder()) {\n                mMediaRecorder.start();\n                isRecording = true;\n            } else {\n                releaseMediaRecorder();\n                return false;\n            }\n            return true;\n        }\n\n        @Override\n        protected void onPostExecute(Boolean result) {\n            if(!result) {\n                Recorder.this.finish();\n            }\n\n            btnRecord.setText(\"Stop\");\n        }\n    }\n```", "```py\n    @TargetApi(Build.VERSION_CODES.HONEYCOMB)\n    private boolean prepareVideoRecorder() {\n        mCamera = Camera.open();\n\n        Camera.Parameters parameters = mCamera.getParameters();\n        List<Camera.Size> mSupportedPreviewSizes = parameters.getSupportedPreviewSizes();\n```", "```py\n        Camera.Size optimalSize = getOptimalPreviewSize(mSupportedPreviewSizes,mPreview.getWidth(),mPreview.getHeight());\n        parameters.setPreviewSize(optimalSize.width,optimalSize.height);\n```", "```py\n        CamcorderProfile profile = CamcorderProfile.get(CamcorderProfile.QUALITY_HIGH);\n        profile.videoFrameWidth = optimalSize.width;\n        profile.videoFrameHeight = optimalSize.height;\n```", "```py\n        mCamera.setParameters(parameters);\n        try {\n               mCamera.setPreviewTexture(mPreview.getSurfaceTexture());\n        } catch(IOException e) {\n            Log.e(TAG,\"Surface texture is unavailable or unsuitable\" + e.getMessage());\n            return false;\n        }\n```", "```py\n    mMediaRecorder = new MediaRecorder();\n        mCamera.unlock();\n        mMediaRecorder.setCamera(mCamera);\n\n        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);\n\n        mMediaRecorder.setOutputFormat(profile.fileFormat);\n        mMediaRecorder.setVideoFrameRate(profile.videoFrameRate);\n        mMediaRecorder.setVideoSize(profile.videoFrameWidth,profile.videoFrameHeight);\n        mMediaRecorder.setVideoEncodingBitRate(\n                                    profile.videoBitRate);\n\n        mMediaRecorder.setVideoEncoder(profile.videoCodec);\n        mMediaRecorder.setOutputFile(getOutputMediaFile().toString());\n```", "```py\n        try {\n            mMediaRecorder.prepare();\n        } catch (IllegalStateException e) {\n            Log.d(TAG, \"IllegalStateException preparing MediaRecorder: \" + e.getMessage());\n            releaseMediaRecorder();\n            return false;\n        } catch (IOException e) {\n            Log.d(TAG, \"IOException preparing MediaRecorder: \" + e.getMessage());\n            releaseMediaRecorder();\n            return false;\n        }\n        return true;\n    }\n```", "```py\n    private Camera.Size getOptimalPreviewSize(List<Camera.Size> sizes, int w, int h) {\n        final double ASPECT_TOLERANCE = 0.1;\n        double targetRatio = (double)w / h;\n\n        if(sizes == null) {\n            return null;\n        }\n\n        Camera.Size optimalSize = null;\n\n        double minDiff = Double.MAX_VALUE;\n\n        int targetHeight = h;\n\n        for (Camera.Size size : sizes) {\n            double ratio = (double)size.width / size.height;\n            double diff = Math.abs(ratio - targetRatio);\n\n            if(Math.abs(ratio - targetRatio) > ASPECT_TOLERANCE)\n                continue;\n\n            if(Math.abs(size.height - targetHeight) < minDiff) {\n                optimalSize = size;\n                minDiff = Math.abs(size.height - targetHeight);\n            }\n        }\n\n        if(optimalSize == null) {\n            minDiff = Double.MAX_VALUE;\n            for(Camera.Size size : sizes) {\n                if(Math.abs(size.height-targetHeight) < minDiff) {\n                    optimalSize = size;\n                    minDiff = Math.abs(size.height-targetHeight);\n                }\n            }\n        }\n\n        return optimalSize;\n    }\n```", "```py\n    private File getOutputMediaFile() {\n        if(!Environment.getExternalStorageState().equalsIgnoreCase(Environment.MEDIA_MOUNTED)) {\n            return null;\n        }\n\n        File mediaStorageDir = new File(Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES), \"Recorder\");\n\n        if(!mediaStorageDir.exists()) {\n            if(!mediaStorageDir.mkdirs()) {\n                Log.d(\"Recorder\", \"Failed to create directory\");\n                return null;\n            }\n        }\n\n        String timeStamp = new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date());\n        File mediaFile;\n        mediaFile = new File(mediaStorageDir.getPath() + File.separator + \"VID_\" + timeStamp + \".mp4\");\n\n        return mediaFile;\n    }\n```", "```py\n    private void releaseCamera() {\n        if(mCamera != null) {\n            mCamera.release();\n            mCamera = null;\n        }\n    }\n    @Override\n    protected void onPause() {\n        super.onPause();\n\n        releaseMediaRecorder();\n        releaseCamera();\n    }\n```", "```py\npublic class Recorder extends Activity implements SensorEventListener {\n    private TextureView mPreview;\n    private Camera mCamera;\n    ...\n```", "```py\n    ...\n    private Button btnRecord;\n\n    private SensorManager mSensorManager;\n    private Sensor mGyro;\n    private PrintStream mGyroFile;\n    private long mStartTime = -1;\n\n    private static String TAG = \"GyroRecorder\";\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n    ....\n```", "```py\n                onCaptureClick(view);\n            }\n        });\n\n        mSensorManager = (SensorManager)getSystemService(Context.SENSOR_SERVICE);\n        mGyro = mSensorManager.getDefaultSensor(Sensor.TYPE_GYROSCOPE);\n        mSensorManager.registerListener(this, mGyro, SensorManager.SENSOR_DELAY_FASTEST);\n    }\n```", "```py\n    private boolean prepareVideoRecorder() {\n        mCamera = Camera.open();\n        ...\n        mMediaRecorder.setOutputFile(getOutputMediaFile().toString());\n\n        try {\n            mGyroFile = new PrintStream(getOutputGyroFile());\n            mGyroFile.append(\"gyro\\n\");\n        } catch(IOException e) {\n            Log.d(TAG, \"Unable to create acquisition file\");\n            return false;\n        }\n\n        try {\n            mMediaRecorder.prepare();\n        ...\n```", "```py\n    private File getOutputGyroFile() {\n        if(!Environment.getExternalStorageState().equalsIgnoreCase(Environment.MEDIA_MOUNTED)) {\n            return null;\n        }\n\n        File gyroStorageDir = new File(Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES), \"Recorder\");\n\n        if(!gyroStorageDir.exists()) {\n            if(!gyroStorageDir.mkdirs()) {\n                Log.d(\"Recorder\", \"Failed to create directory\");\n                return null;\n            }\n        }\n\n        String timeStamp = new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date());\n        File gyroFile;\n        gyroFile = new File(gyroStorageDir.getPath() + File.separator + \"VID_\" + timeStamp + \"gyro.csv\");\n\n        return gyroFile;\n    }\n```", "```py\n    @Override\n    public void onAccuracyChanged(Sensor sensor, int accuracy) {\n        // Empty on purpose\n        // Required because we implement SensorEventListener\n    }\n\n    @Override\n    public void onSensorChanged(SensorEvent sensorEvent) {\n        if(isRecording) {\n            if(mStartTime == -1) {\n                mStartTime = sensorEvent.timestamp;\n            }\n            mGyroFile.append(sensorEvent.values[0] + \",\" +\n                             sensorEvent.values[1] + \",\" +\n                             sensorEvent.values[2] + \",\" +\n                             (sensorEvent.timestamp-mStartTime) + \"\\n\");\n        }\n    }\n```", "```py\npublic class Point3 {\n    public double x;\n    public double y;\n    public double z;\n}\n```", "```py\npublic class CameraOverlayWidget extends SurfaceView implements GestureDetector.OnGestureListener, SurfaceHolder.Callback {\n    public static String TAG= \"SFOCV::Overlay\";\n    protected Paint paintSafeExtents;\n    protected Button btn;\n    protected GestureDetector mGestureDetector;\n```", "```py\n    private long sizeWidth = 0, sizeHeight = 0;\n\n    // Stuff required to paint the recording sign\n    protected boolean mRecording = false;\n    protected Paint paintRecordCircle;\n    protected Paint paintRecordText;\n\n    // Calibrate button\n    private Paint paintCalibrateText;\n    private Paint paintCalibrateTextOutline;\n\n    private Paint paintTransparentButton;\n\n    private RenderThread mPainterThread;\n    private boolean bStopPainting = false;\n\n    private Point3 omega;\n    private Point3 drift;\n    private Point3 theta;\n\n    public static final double SAFETY_HORIZONTAL = 0.15;\n    public static final double SAFETY_VERTICAL = 0.15;\n```", "```py\n    protected GestureDetector.OnGestureListener mCustomTouchMethods = null;\n    protected OverlayEventListener mOverlayEventListener = null;\n```", "```py\n    public CameraOverlayWidget(Context ctx, AttributeSet attrs) {\n        super(ctx, attrs);\n\n        // Position at the very top and I'm the event handler\n        setZOrderOnTop(true);\n        getHolder().addCallback(this);\n\n        // Load all the required objects\n        initializePaints();\n\n        // Setup the required handlers/threads\n        mPainterThread = new RenderThread();\n        mGestureDetector = new GestureDetector(ctx, this);\n    }\n```", "```py\n    /**\n     * Initializes all paint objects.\n     */\n    protected void initializePaints() {\n        paintSafeExtents = new Paint();\n        paintSafeExtents.setColor(Color.WHITE);\n        paintSafeExtents.setStyle(Paint.Style.STROKE);\n        paintSafeExtents.setStrokeWidth(3);\n\n        paintRecordCircle = new Paint();\n        paintRecordCircle.setColor(Color.RED);\n        paintRecordCircle.setStyle(Paint.Style.FILL);\n\n        paintRecordText = new Paint();\n        paintRecordText.setColor(Color.WHITE);\n        paintRecordText.setTextSize(20);\n\n        paintCalibrateText = new Paint();\n        paintCalibrateText.setColor(Color.WHITE);\n        paintCalibrateText.setTextSize(35);\n        paintCalibrateText.setStyle(Paint.Style.FILL);\n\n        paintCalibrateTextOutline = new Paint();\n        paintCalibrateTextOutline.setColor(Color.BLACK);\n        paintCalibrateTextOutline.setStrokeWidth(2);\n        paintCalibrateTextOutline.setTextSize(35);\n        paintCalibrateTextOutline.setStyle(Paint.Style.STROKE);\n\n        paintTransparentButton = new Paint();\n        paintTransparentButton.setColor(Color.BLACK);\n        paintTransparentButton.setAlpha(128);\n        paintTransparentButton.setStyle(Paint.Style.FILL);\n    }\n```", "```py\n    class RenderThread extends Thread {\n        private long start = 0;\n        @Override\n        public void run() {\n            super.run();\n\n            start = SystemClock.uptimeMillis();\n\n            while(!bStopPainting && !isInterrupted()) {\n                long tick = SystemClock.uptimeMillis();\n                renderOverlay(tick);\n            }\n        }\n```", "```py\n        /**\n         * A renderer for the overlay with no state of its own.\n         * @returns nothing\n         */\n        public void renderOverlay(long tick) {\n            Canvas canvas = getHolder().lockCanvas();\n\n            long width = canvas.getWidth();\n            long height = canvas.getHeight();\n\n            // Clear the canvas\n            canvas.drawColor(Color.TRANSPARENT, PorterDuff.Mode.CLEAR);\n```", "```py\n            // Draw the bounds\n            long lSafeW = (long)(width * SAFETY_HORIZONTAL);\n            long lSafeH = (long)(height * SAFETY_VERTICAL);\n            canvas.drawRect(lSafeW, lSafeH, width-lSafeW, height-lSafeH, paintSafeExtents);\n```", "```py\n            if(mRecording) {\n                // Render this only on alternate 500ms intervals\n                if(((tick-start) / 500) % 2 == 1) {\n                    canvas.drawCircle(100, 100, 20, paintRecordCircle);\n                    final String s = \"Recording\";\n                    canvas.drawText(s, 0, s.length(), 130, 110, paintRecordText);\n                }\n            }\n```", "```py\n            canvas.drawRect((float)(1-SAFETY_HORIZONTAL)*sizeWidth, (float)(1-SAFETY_VERTICAL)*sizeHeight, sizeWidth , sizeHeight, paintTransparentButton);\n\n            final String strCalibrate = \"Calibrate\";\n            canvas.drawText(strCalibrate, 0, strCalibrate.length(), width-200, height-200, paintCalibrateText);\n            canvas.drawText(strCalibrate, 0, strCalibrate.length(), width-200, height-200, paintCalibrateTextOutline);\n```", "```py\n            if(omega!=null) {\n                final String strO = \"O: \";\n                canvas.drawText(strO, 0, strO.length(), width - 200, 200, paintCalibrateText);\n                String strX = Math.toDegrees(omega.x) + \"\";\n                String strY = Math.toDegrees(omega.y) + \"\";\n                String strZ = Math.toDegrees(omega.z) + \"\";\n                canvas.drawText(strX, 0, strX.length(), width - 200, 250, paintCalibrateText);\n                canvas.drawText(strY, 0, strY.length(), width - 200, 300, paintCalibrateText);\n                canvas.drawText(strZ, 0, strZ.length(), width - 200, 350, paintCalibrateText);\n            }\n\n            if(theta!=null) {\n                final String strT = \"T: \";\n                canvas.drawText(strT, 0, strT.length(), width - 200, 500, paintCalibrateText);\n                String strX = Math.toDegrees(theta.x) + \"\";\n                String strY = Math.toDegrees(theta.y) + \"\";\n                String strZ = Math.toDegrees(theta.z) + \"\";\n                canvas.drawText(strX, 0, strX.length(), width - 200, 550, paintCalibrateText);\n                canvas.drawText(strY, 0, strY.length(), width - 200, 600, paintCalibrateText);\n                canvas.drawText(strZ, 0, strZ.length(), width - 200, 650, paintCalibrateText);\n            }\n```", "```py\n            // Flush out the canvas\n            getHolder().unlockCanvasAndPost(canvas);\n        }\n    }\n```", "```py\n    public void setRecording() {\n        mRecording = true;\n    }\n\n    public void unsetRecording() {\n        mRecording = false;\n    }\n```", "```py\n    @Override\n    public void onSizeChanged(int w,int h,int oldw,int oldh) {\n        super.onSizeChanged(w, h, oldw, oldh);\n\n        sizeWidth = w;\n        sizeHeight = h;\n    }\n```", "```py\n    public void setCustomTouchMethods(GestureDetector.SimpleOnGestureListener c){\n        mCustomTouchMethods = c;\n    }\n\n    public void setOverlayEventListener(OverlayEventListener listener) {\n        mOverlayEventListener = listener;\n    }\n```", "```py\n    public void setOmega(Point3 omega) {\n        this.omega = omega;\n    }\n\n    public void setDrift(Point3 drift) {\n        this.drift = drift;\n    }\n\n    public void setTheta(Point3 theta) {\n        this.theta = theta;\n    }\n```", "```py\n    /**\n     * This method is called during the activity's onResume. This ensures a wakeup\n     * re-instantiates the rendering thread.\n     */\n    public void resume() {\n        bStopPainting = false;\n        mPainterThread = new RenderThread();\n    }\n\n    /**\n     * This method is called during the activity's onPause method. This ensures\n     * going to sleep pauses the rendering.\n     */\n    public void pause() {\n        bStopPainting = true;\n\n        try {\n            mPainterThread.join();\n        }\n        catch(InterruptedException e) {\n            e.printStackTrace();\n        }\n        mPainterThread = null;\n    }\n```", "```py\n    @Override\n    public void surfaceCreated(SurfaceHolder surfaceHolder) {\n        getHolder().setFormat(PixelFormat.RGBA_8888);\n\n        // We created the thread earlier - but we should start it only when\n        // the surface is ready to be drawn on.\n        if(mPainterThread != null && !mPainterThread.isAlive()) {\n            mPainterThread.start();\n        }\n    }\n\n    @Override\n    public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {\n        // Required for implementation\n    }\n    @Override\n    public void surfaceDestroyed(SurfaceHolder holder) {\n        // Required for implementation\n    }\n```", "```py\n    @Override\n    public boolean onTouchEvent(MotionEvent motionEvent) {\n        boolean result = mGestureDetector.onTouchEvent(motionEvent);\n        return result;\n    }\n\n    @Override\n    public boolean onDown(MotionEvent motionEvent) {\n        MotionEvent.PointerCoords coords =new MotionEvent.PointerCoords();\n\n        motionEvent.getPointerCoords(0, coords);\n\n        // Handle these only if there is an event listener\n        if(mOverlayEventListener!=null) {\n            if(coords.x >= (1-SAFETY_HORIZONTAL)*sizeWidth &&coords.x<sizeWidth &&\n               coords.y >= (1-SAFETY_VERTICAL)*sizeHeight &&coords.y<sizeHeight) {\n                return mOverlayEventListener.onCalibrate(motionEvent);\n            }\n        }\n\n        // Didn't match? Try passing a raw event - just in case\n        if(mCustomTouchMethods!=null)\n            return mCustomTouchMethods.onDown(motionEvent);\n\n        // Nothing worked - let it bubble up\n        return false;\n    }\n\n    @Override\n    public void onShowPress(MotionEvent motionEvent) {\n        if(mCustomTouchMethods!=null)\n            mCustomTouchMethods.onShowPress(motionEvent);\n    }\n\n    @Override\n    public boolean onFling(MotionEvent motionEvent,MotionEvent motionEvent2,float v, float v2) {\n        Log.d(TAG, \"onFling\");\n\n        if(mCustomTouchMethods!=null)\n            return mCustomTouchMethods.onFling(motionEvent,motionEvent2,v, v2);\n\n        return false;\n    }\n\n    @Override\n    public void onLongPress(MotionEvent motionEvent) {\n        Log.d(TAG, \"onLongPress\");\n\n        if(mCustomTouchMethods!=null)\n            mCustomTouchMethods.onLongPress(motionEvent);\n    }\n\n    @Override\n    public boolean onScroll(MotionEvent motionEvent,MotionEvent motionEvent2,float v, float v2) {\n        Log.d(TAG, \"onScroll\");\n\n        if(mCustomTouchMethods!=null)\n            return mCustomTouchMethods.onScroll(motionEvent,motionEvent2,v, v2);\n\n        return false;\n    }\n\n    @Override\n    public boolean onSingleTapUp(MotionEvent motionEvent) {\n        Log.d(TAG, \"onSingleTapUp\");\n\n        if(mCustomTouchMethods!=null)\n            return mCustomTouchMethods.onSingleTapUp(motionEvent);\n\n        return false;\n    }\n```", "```py\n    public interface OverlayEventListener {\n        public boolean onCalibrate(MotionEvent e);\n    }\n}\n```", "```py\npublic class SequentialFrameExtractor {\n    private String mFilename = null;\n    private CodecOutputSurface outputSurface = null;\n    private MediaCodec decoder = null;\n\n    private FrameAvailableListener frameListener = null;\n\n    private static final int TIMEOUT_USEC = 10000;\n    private long decodeCount = 0;\n}\n```", "```py\n    private long decodeCount = 0;\n    public SequentialFrameExtractor(String filename) {\n        mFilename = filename;\n    }\n\n    public void start() {\n        MediaExtractor mediaExtractor = new MediaExtractor();\n        try {\n            mediaExtractor.setDataSource(mFilename);\n        } catch(IOException e) {\n            e.printStackTrace();\n        }\n    }\n```", "```py\n            e.printStackTrace();\n        }\n        MediaFormat format = null;\n        int numTracks = mediaExtract.getTrackCount();\n        int track = -1;\n        for(int i=0;i<numTracks;i++) {\n            MediaFormat fmt = mediaExtractor.getTrackFormat(i);\n            String mime = fmt.getString(MediaFormat.KEY_MIME);\n            if(mime.startswith(\"video/\")) {\n                mediaExtractor.selectTrack(i);\n                track = i;\n                format = fmt;\n                break;\n            }\n        }\n        if(track==-1) {\n            // Did the user select an audio file?\n        }\n```", "```py\n        int frameWidth = format.getInteger(MediaFormat.KEY_WIDTH);\n        int frameHeight = format.getInteger(MediaFormat.KEY_HEIGHT);\n        outputSurface = new CodecOutputSurface(frameWidth,frameHeight);\n\n        String mime = format.getString(MediaFormat.KEY_MIME);\n        decoder = MediaCodec.createDecoderByType(mime);\n        decoder.configure(format,outputSurface.getSurface(),null,0);\n        decoder.start();\n\n        ByteBuffer[] decoderInputBuffers =\n                                       decoder.getInputBuffers();\n        MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();\n        int inputChunk = 0;\n        boolean outputDone = false, inputDone = false;\n        long presentationTimeUs = 0;\n```", "```py\n        while(!outputDone) {\n            if(!inputDone) {\n                int inputBufIndex =decoder.dequeueInputBuffer(TIMEOUT_USEC);\n                if(inputBufIndex >= 0) {\n                    ByteBuffer inputBuf =decoderInputBuffers[inputBufIndex];\n                    int chunkSize = mediaExtractor.readSampleData(inputBuf, 0);\n                    if(chunkSize < 0) {\n                        decoder.queueInputBuffer(inputBufIndex,0, 0, 0L,\n                               mediaCodec.BUFFER_FLAG_END_OF_STREAM);\n                        inputDone = true;\n                    } else {\n                        if(mediaExtractor.getSampleTrackIndex()!= track) {\n                            // We somehow got data that did not\n                            // belong to the track we selected\n                        }\n                        presentationTimeUs =mediaExtractor.getSampleTime();\n                        decoder.queueInputBuffer(inputBufIndex,0, chunkSize,                                                                                                                                                           presentationTimeUs, 0);\n                        inputChunk++;\n                        mediaExtractor.advance();\n                    }\n                }    \n            } else {\n                // We shouldn't reach here â€“ inputDone, protect us\n            }\n        }\n```", "```py\n            } else {\n                // We shouldn't reach here â€“ inputDone, protect us\n            }            \n            if(!outputDone) {\n                int decoderStatus = decoder.dequeueOutputBuffer(\n                                              info, TIMEOUT_USEC);\n                if(decoderStatus ==\n                                MediaCodec.INFO_TRY_AGAIN_LATER) {\n                    // Can't do anything here\n                } else if(decoderStatus ==\n                         MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {\n                    // Not important since we're using a surface\n                } else if(decoderStatus ==\n                          MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {\n                    MediaFormat newFormat = decoder.getOutputFormat();\n                    // Handled automatically for us\n                } else if(decoderStatus < 0) {\n                    // Something bad has happened\n                } else {\n                    if((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != ) {\n                        outputDone = true;\n                    }\n                }\n\n                boolean doRender = (info.size != 0);\n                decoder.releaseOutputBuffer(decoderStatus, doRender);\n                if(doRender) {\n                    outputSurface.awaitNewImage();\n                    outputSurface.drawImage(true);\n\n                    try {\n                        Mat img = outputSurface.readFrameAsMat();\n                        if(frameListener != null) {\n                            Frame frame = new Frame(img, presentationTimeUs,\n                                                    new Point3(), new Point());\n                            frameListener.onFrameAvailable(frame);\n                        }\n                    } catch(IOException e) {\n                        e.printStackTrace();\n                    }\n                    decodeCount++;\n                    if(frameListener!=null)\n                        frameListener.onFrameComplete(decodeCount);\n                }\n            }\n        }\n\n        medaiExtractor.release();\n        mediaExtractor = null;\n    }\n```", "```py\n    public void setFrameAvailableListener(FrameAvailableListener listener) {\n        frameListener = listener;\n    }\n\n    public interface FrameAvailableListener {\n        public void onFrameAvailable(Frame frame);\n        public void onFrameComplete(long frameDone);\n    }\n}\n```", "```py\nimport sys, numpy\n\nclass CalibrationParameters(object):\n    def __init__(self):\n        self.f = 0.0\n        self.td = 0.0\n        self.gb = (0.0, 0.0, 0.0)\n        self.ts = 0.0\n```", "```py\nclass GyroscopeDataFile(object):\n    def __init__(self, filepath):\n        self.filepath = filepath\n        self.omega = {}\n\n    def getfile_object(self):\n        return open(self.filepath)\n```", "```py\n    def parse(self):\n        with self._get_file_object() as fp:\n            firstline = fp.readline().strip()\n            if not firstline == 'utk':\n                raise Exception(\"The first line isn't valid\")\n```", "```py\n            for line in fp.readlines():\n                line = line.strip()\n                parts = line.split(\",\")\n```", "```py\n                timestamp = int(parts[3])\n                ox = float(parts[0])\n                oy = float(parts[1])\n                oz = float(parts[2])\n                print(\"%s: %s, %s, %s\" % (timestamp,\n                                          ox,\n                                          oy,\n                                          oz))\n                self.omega[timestamp] = (ox, oy, oz)\n    return\n```", "```py\n    def get_timestamps(self):\n        return sorted(self.omega.keys())\n\n    def get_signal(self, index):\n        return [self.omega[k][index] for k in self.get_timestamps()]\n```", "```py\n    def get_signal_x(self):\n        return self.get_signal(0)\n\n    def get_signal_y(self):\n        return self.get_signal(1)\n\n    def get_signal_z(self):\n        return self.get_signal(2)\n```", "```py\n    def fetch_approximate_omega(self, timestamp):\n        if timestamp in self.omega:\n            return self.omega[timestamp]\n```", "```py\n        i = 0\n        sorted_timestamps = self.get_timestamps()\n        for ts in sorted_timestamps:\n            if  ts > timestamp:\n                break\n            i += 1\n```", "```py\n        t_previous = sorted_timestamps[i]\n        t_current = sorted_timestamps[i+1]\n        dt = float(t_current â€“ t_previous)\n        slope = (timestamp â€“ t_previous) / dt\n\n        est_x = self.omega[t_previous][0]*(1-slope) + self.omega[t_current][0]*slope\n        est_y = self.omega[t_previous][1]*(1-slope) + self.omega[t_current][1]*slope\n        est_z = self.omega[t_previous][2]*(1-slope) + self.omega[t_current][2]*slope\n        return (est_x, est_y, est_z)\n```", "```py\nclass GyroVideo(object):def__init__(self, mp4):\n        self.mp4 = mp4\n        self.frameInfo = []\n        self.numFrames = 0\n        self.duration = 0\n        self.frameWidth = 0\n        self.frameHeight = 0\n```", "```py\n    def read_video(self, skip_keypoints=False):\n        vidcap = cv2.VideoCapture(self.mp4)\n        success, frame = vidcap.read()\n        prev_frame = None\n        previous_timestamp = 0\n        frameCount = 0\n```", "```py\n        while success:\n            current_timestamp = vidcap.get(0) * 1000 * 1000\n            print \"Processing frame#%d (%f ns)\" % (frameCount, current_timestamp)\n```", "```py\n            if not prev_frame:\n                self.frameInfo.append({'keypoints': None,\n                                       'timestamp': current_timestamp})\n                prev_frame = frame\n                previous_timestamp = current_timestamp\n                continue\n```", "```py\n            if skip_keypoints:\n                self.frameInfo.append({'keypoints': None,\n                                       'timestamp': current_timestamp})\n                continue\n```", "```py\n            old_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n            new_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n            old_corners = cv2.goodFeaturesToTrack(old_gray, 1000, 0.3, 30)\n```", "```py\n            if old_corners == None:\n                self.frameInfo.append({'keypoints': None,\n                                       'timestamp': current_timestamp})\n                frameCount += 1\n                previous_timestamp = current_timestamp\n                prev_frame = frame\n                success, frame = vidcap.read()\n                continue\n```", "```py\n            new_corners, status, err = cv2.calcOpticalFlowPyrLK(old_gray,\n                                           new_gray,\n                                           old_corners,\n                                           None,\n                                           winSize=(15,15)\n                                           maxLevel=2,\n                                           criteria=(cv2.TERM_CRITERIA_EPS\n                                                     | cv2.TERM_CRITERIA_COUNT,\n                                                     10, 0.03))\n```", "```py\n            if len(old_corners) > 4:\n                homography, mask = cv2.findHomography(old_corners, new_corners,\n                                                      cv2.RANSAC, 5.0)\n                mask = mask.ravel()\n                new_corners_homography = [new_corners[i] for i in xrange(len(mask)) if mask[i] == 1])\n                old_corners_homography = [old_corners[i] for i in xrange(len(mask)) if mask[i] == 1])\n                new_corners_homography = numpy.asarray(new_corners_homography)\n                old_corners_homography = numpy.asarray(old_corners_homography)\n            else:\n                new_corners_homography = new_corners\n                old_corners_homography = old_corners\n```", "```py\n            self.frameInfo.append({'keypoints': (old_corners_homography,\n                                                 new_corners_homography),\n                                   'timestamp': current_timestamp})\n            frameCount += 1\n            previous_timestamp = current_timestamp\n            prev_frame = frame\n            success, frame = vidcap.read()\n        self.numFrames = frameCount\n        self.duration = current_timstamp\n        return\n```", "```py\ndef rotateImage(src, rx, ry, rz, f, dx=0, dy=0, dz=0, convertToRadians=False):\n    if convertToRadians:\n        rx = (rx) * math.pi / 180\n        ry = (ry) * math.pi / 180\n        rz = (rz) * math.pi / 180\n\n    rx = float(rx)\n    ry = float(ry)\n    rz = float(rz)\n```", "```py\n    w = src.shape[1]\n    h = src.shape[0]\n```", "```py\n    smallR = cv2.Rodrigues(np.array([rx, ry, rz]))[0]\n    R = numpy.array([ [smallR[0][0], smallR[0][1], smallR[0][2], 0],\n                      [smallR[1][0], smallR[1][1], smallR[1][2], 0],\n                      [smallR[2][0], smallR[2][1], smallR[2][2], 0],\n                      [0,            0,            0,            1]])\n```", "```py\n    x = numpy.array([[1.0, 0,   0,   dx],\n                     [0,   1.0, 0,   dy],\n                     [0,   0,   1.0, dz],\n                     [0,   0,   0,   1]])\n    T = numpy.asmatrix(x)\n```", "```py\n    c = numpy.array([[f, 0, w/2, 0],\n                     [0, f, h/2, 0],\n                     [0, 0, 1,   0]])\n    cameraMatrix = numpy.asmatrix(c)\n```", "```py\n    transform = cameraMatrix * (T*R)\n```", "```py\n    output = cv2.warpPerspective(src, transform, (w, h))\n    return output\n```", "```py\n    w = src.shape[1]\n    h = src.shape[0]\n\n    # New code:\n    x = numpy.array([ [1, 0, -w/2],\n                      [0, 1, -h/2],\n                      [0, 0, 0],\n                      [0, 0, 1]]\n    A1 = numpy.asmatrix(x)\n    ...\n```", "```py\ntransform = cameraMatrix * (T*(R*A1))\n```", "```py\ndef getAccumulatedRotation(w, h,\n                           theta_x, theta_y, theta_z, timestamps,\n                           prev, current,\n                           f,\n                           gyro_delay=None, gyro_drift=None, shutter_duration=None):\n    if not gyro_delay:\n        gyro_delay = 0\n\n    if not gyro_drift:\n        gyro_drift = (0, 0, 0)\n\n    if not shutter_duration:\n        shutter_duration = 0\n```", "```py\n    x = numpy.array([[1, 0, -w/2],\n                  [0, 1, -h/2],\n                  [0, 0, 0],\n                  [0, 0, 1]])\n    A1 = numpy.asmatrix(x)\n    transform = A1.copy()\n```", "```py\n    prev = prev + gyro_delay\n    current = current + gyro_delay\n    if prev in timestamps and current in timestamps:\n        start_timestamp = prev\n        end_timestamp = current\n    else:\n        (rot, start_timestamp, t_next) = fetch_closest_trio(theta_x,\n                                                            theta_y,\n                                                            theta_z,\n                                                            timestamps,\n                                                            prev)\n        (rot, end_timestamp, t_next) = fetch_closest_trio(theta_x,\n                                                          theta_y,\n                                                          theta_z,\n                                                          timestamps,\n                                                          current)\n```", "```py\n    for time in xrange(timestamps.index(start_timestamp), timestamps.index(end_timestamp)):\n        time_shifted = timestamps[time] + gyro_delay\n        trio, t_previous, t_current = fetch_closest_trio(theta_x, theta_y, theta_z, timestamps, time_shifted)\n        gyro_drifted = (float(trio[0] + gyro_drift[0]),\n                        float(trio[1] + gyro_drift[1]),\n                        float(trio[2] + gyro_drift[2]))\n```", "```py\n        smallR = cv2.Rodrigues(numpy.array([-float(gyro_drifted[1]),\n                                            -float(gyro_drifted[0]),\n                                            -float(gyro_drifted[2])]))[0]\n        R = numpy.array([[smallR[0][0], smallR[0][1], smallR[0][2], 0],\n                         [smallR[1][0], smallR[1][1], smallR[1][2], 0],\n                         [smallR[2][0], smallR[2][1], smallR[2][2], 0],\n                         [0,            0,            0,            1]])\n        transform = R * transform\n```", "```py\n    x = numpy.array([[1, 0, 0, 0],\n                     [0, 1, 0, 0],\n                     [0, 0, 1, f],\n                     [0, 0, 0, 1]])\n    T = numpy.asmatrix(x)\n```", "```py\n    x = numpy.array([[f, 0, w/2, 0],\n                     [0, f, h/2, 0],\n                     [0, 0, 1,   0]])\n    A2 = numpy.asmatrix(x)\n    transform = A2 * (T*transform)\n    return transform\n```", "```py\nclass CalibrateGyroStabilize(object):\n    def __init__(self, mp4, csv):\n        self.mp4 = mp4\n        self.csv = csv\n```", "```py\n    def get_gaussian_kernel(sigma2, v1, v2, normalize=True):\n        gauss = [math.exp(-(float(x*x) / sigma2)) for x in range(v1, v2+1)]\n\n        if normalize:\n            total = sum(guass)\n            gauss = [x/total for x in gauss]\n\n        return gauss\n```", "```py\n    def gaussian_filter(input_array):\n        sigma = 10000\n        r = 256\n        kernel = get_gaussian_kernel(sigma, -r, r)\n        return numpy.convolve(input_array, kernel, 'same')\n```", "```py\n    def calcErrorScore(set1, set2):\n        if len(set1) != len(set2):\n            raise Exception(\"The given sets need to have the exact same length\")\n\n        score = 0\n        for first, second in zip(set1.tolist(), set2.tolist()):\n            diff_x = math.pow(first[0][0] â€“ second[0][0], 2)\n            diff_y = math.pow(first[0][1] â€“ second[0][1], 2)\n            score += math.sqrt(diff_x + diff_y)\n\n        return score\n```", "```py\n    def calcErrorAcrossVideo(videoObj, theta, timestamp, focal_length, gyro_delay=None, gyro_drift=None, rolling_shutter=None):\n        total_error = 0\n        for frameCount in xrange(videoObj.numFrames):\n            frameInfo = videoObj.frameInfo[frameCount]\n            current_timestamp = frameInfo['timestamp']\n\n            if frameCount == 0:\n                previous_timestamp = current_timestamp\n                continue\n            keypoints = frameInfo['keypoints']\n\n            if not keypoints:\n                continue\n```", "```py\n            old_corners = frameInfo['keypoints'][0]\n            new_corners = frameInfo['keypoints'][1]\n            transform = getAccumulatedRotation(videoObj.frameWidth,\n                                               videoObj.frameHeight,\n                                               theta[0], theta[1], theta[2],\n                                               timestamps,\n                                               int(previous_timestamp),\n                                               int(current_timestamp),\n                                               focal_length,\n                                               gyro_delay,\n                                               gyro_drift,\n                                               rolling_shutter)\n```", "```py\n            transformed_corners = cv2.perspectiveTransform(old_corners, transform)\n            error = calcErrorScore(new_corners, transformed_corners)\n            total_error += error\n            previous_timestamp = current_timestamp\n        return total_error\n```", "```py\n    def calibrate(self):\n        gdf = GyroscopeDataFile(csv)\n        gdf.parse()\n\n        signal_x = gdf.get_signal_x()\n        signal_y = gdf.get_signal_y()\n        signal_z = gdf.get_signal_z()\n        timestamps = gdf.get_timestamps()\n```", "```py\n        smooth_signal_x = self.gaussian_filter(signal_x)\n        smooth_signal_y = self.gaussian_filter(signal_y)\n        smooth_signal_z = self.gaussian_filter(signal_z)\n```", "```py\n        g = [ [], [] [] ]\n        g[0] = numpy.subtract(signal_x, smooth_signal_x).tolist()\n        g[1] = numpy.subtract(signal_y, smooth_signal_y).tolist()\n        g[2] = numpy.subtract(signal_z, smooth_signal_z).tolist()\n```", "```py\n        dgt = self.diff(timestamps)\n```", "```py\n        theta = [ [], [], [] ]\n        for component in [0, 1, 2]:\n            sum_of_consecutives = numpy.add( g[component][:-1], g[component][1:])\n            dx_0 = numpy.divide(sum_of_consecutives, 2 * 1000000000)\n            num_0 = numpy.multipy(dx_0, dgt)\n            theta[component] = [0]\n            theta[component].extend(numpy.cumsum(num_0))\n```", "```py\n        focal_length = 1080.0\n        gyro_delay = 0\n        gyro_drift = (0, 0, 0)\n        shutter_duration = 0\n\n        videoObj = GyroVideo(mp4)\n        videoObj.read_video()\n```", "```py\n        parameters = numpy.asarray([focal_length,\n                                    gyro_delay,\n                                    gyro_drift[0], gyro_drift[1], gyro_drift[2]])\n```", "```py\n        result = scipy.optimize.minimize(self.calcErrorAcrossVideoObjective,\n                                         parameters,\n                                         (videoObj, theta, timestamps),\n                                         'Nelder-Mead')\n```", "```py\n        focal_length = result['x'][0]\n        gyro_delay = result['x'][1]\n        gyro_drift = ( result['x'][2], result['x'][3], result['x'][4] )\n        print \"Focal length = %f\" % focal_length\n        print \"Gyro delay   = %f\" % gyro_delay\n        print \"Gyro drift   = (%f, %f, %f)\" % gyro_drift\n```", "```py\n    return (delta_theta, timestamps, focal_length, gyro_delay, gyro_drift, shutter_duration)\n```", "```py\ndef stabilize_video(mp4, csv):\n    calib_obj = CalibrateGyroStabilize(mp4, csv)\n```", "```py\n    delta_theta, timestamps, focal_length, gyro_delay, gyro_drift, shutter_duration = calib_obj.calibrate()\n```", "```py\n    vidcap = cv2.VideoCapture(mp4)\n```", "```py\n    frameCount = 0\n    success, frame = vidcap.read()\n    previous_timestamp = 0\n    while success:\n        print \"Processing frame %d\" % frameCount\n```", "```py\n        current_timestamp = vidcap.get(cv2.CAP_PROP_POS_MSEC) * 1000 * 1000\n```", "```py\n        rot, prev, current = fetch_closest_trio(delta_theta[0],delta_theta[1],delta_theta[2],timestamps,current_timestamps)\n```", "```py\n        rot = accumulateRotation(frame, delta_theta[0],delta_theta[1],delta_theta[2],timestamps, previous_timestamp,prev,focal_length,gyro_delay,gyro_drift,shutter_duration)\n```", "```py\n        cv2.imwrite(\"/tmp/rotated%04d.png\" % frameCount, rot)\n        frameCount += 1\n        previous_timestamp = prev\n        success, frame = vidcap.read()\n    return\n```", "```py\nffmpeg -f image2 -i image%04d.jpg output.mp4\n```", "```py\ndef calcErrorAcrossVideo(videoObj, theta, timestamp, focal_length, gyro_delay=None, gyro_drift=None, rolling_shutter=None):\n    total_error = 0\n    ...\n        transform = getAccumulatedRotation(...)\n        transformed_corners = cv2.perspectiveTransform(old_corners, transform)\n    ...\n```", "```py\nfor pt in old_corners:\n    x = pt[0][0]\n    y = pt[0][1]\n\n    pt_timestamp = int(current_timestamp) + rolling_shutter * (y-frame_height/2) / frame_height\n```", "```py\n    transform = getAccumulatedRotation(videoObj.frameWidth, videoObj.frameHeight, theta[0], theta[1], theta[2], timestamps, int(previous_timestamp), int(pt_timestamp), focal_length, gyro_delay, gyro_drift, doSub=True)\n```", "```py\n    output = transform * np.matrix(\"%f;%f;1.0\" % (x, y)).tolist()\n    tx = (output[0][0] / output[2][0]).tolist()[0][0]\n    ty = (output[1][0] / output[2][0]).tolist()[0][0]\n    transformed_corners.append( np.array([tx, ty]) )\n```", "```py\ndef meshwarp(src, distorted_grid):\n    \"\"\"\n    src: The original image\n    distorted_grid: The list of points that have been distorted\n    \"\"\"\n    size = src.shape\n```", "```py\n    mapsize = (size[0], size[1], 1)\n    dst = np.zeros(size, dtype=np.uint8)\n```", "```py\n    quads_per_row = len(distorted_grid[0]) â€“ 1\n    quads_per_col = len(distorted_grid) â€“ 1\n    pixels_per_row = size[1] / quads_per_row\n    pixels_per_col = size[0] / quads_per_col\n```", "```py\n    pt_src_all = []\n    pt_dst_all = []\n```", "```py\n    for ptlist in distorted_grid:\n        pt_src_all.extend(ptlist)\n```", "```py\n    for x in range(quads_per_row+1):\n        for y in range(quads_per_col+1):\n            pt_dst_all.append( [x*pixels_per_col,\n                                y*pixels_per_row])\n```", "```py\ngx, gt = np.mgrid[0:size[1], 0:size[0]]\n```", "```py\ng_out = scipy.interpolate.griddata(np.array(pt_dst_all),\n                                   np.array(pt_src_all),\n                                   (gx, gy), method='linear')\n```", "```py\nmapx = np.append([], [ar[:,0] for ar in g_out]).reshape(mapsize).astype('float32')\nmapy = np.append([], [ar[:,1] for ar in g_out]).reshape(mapsize).astype('float32')\n```", "```py\n    dst = cv2.remap(src, mapx, mapy, cv2.INTER_LINEAR)\n    return dst\n```", "```py\ndef accumulateRotation(src, theta_x, theta_y, theta_z, timestamps, prev, current, f, gyro_delay=None, gyro_drift=None, shutter_duration=None):\n    ...\n    transform = getAccumulatedRotation(src.shape[1], src.shape[0], theta_x, theta_y, theta_z, timestamps, prev, current, f, gyro_delay, gyro_drift)\n    o = cv2.warpPerspective(src, transform (src.shape[1], src.shape[0]))\n    return o\n```", "```py\n    ...\n    pts = []\n    transformed_pts = []\n    for x in range(10):\n        current_row = []\n        current_row_transformed = []\n        pixel_x = x * (src.shape[1] / 10)\n        for y in range(10):\n            pixel_y = y * (src.shape[0] / 10)\n            current_row.append( [pixel_x, pixel_y] )\n        pts.append(current_row)\n```", "```py\n        ...\n        for y in range(10):\n            pixel_y = y * (src.shape[0] / 10\n            if shutter_duration:\n                y_timestamp = current + shutter_duration*(pixel_y - src.shape[0]/2)\n            else:\n                y_timestamp = current\n        ...\n```", "```py\n        ...\n        transform = getAccumulatedRotation(src.shape[1], src.shape[0], theta_x, theta_y, theta_z, timestamps, prev, y_timestamp, f, gyro_delay, gyro_drift)\n        output = cv2.perspectiveTransform(np.array([[pixel_x, pixel_y]], transform)\n        current_row_transformed.append(output)\n    pts.append(current_row)\n    pts_transformed.append(current_row_transformed)\n        ...\n```", "```py\n    o = meshwarp(src, pts_transformed)\n    return o\n```", "```py\nimport pickle\nfp = open(\"/path/to/file.data\", \"w\")\nvideoObj = GyroVideo(mp4)\npickle.dump(videoObj, fp)\nfp.close()\n```", "```py\nimport pickle\nfp = open(\"/path/to/file.data\", \"r\")\nvideoObj = pickle.load(fp)\nfp.close()\n```", "```py\nffmpeg -i /tmp/image%04d.png -f image2 output.mp4\n\n```"]