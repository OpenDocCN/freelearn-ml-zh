<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer246">
    <h1 class="chapterNumber">7</h1>
    <h1 class="chapterTitle" id="_idParaDest-168">Personalized Product Recommendations</h1>
    <p class="normal">With the advancements in technology and the rising amount of data being collected, personalization is everywhere. From streaming services, such as Netflix and Hulu, to marketing messages and advertisements you see on your phones, most of the content shown to you is personalized nowadays. In marketing, personalized or targeted marketing campaigns have proven to work significantly better for driving customer engagements and conversions compared to generic or mass marketing campaigns.</p>
    <p class="normal">In this chapter, we are going to discuss building personalized recommendation models with which we can better target customers with the products that interest them the most. We will be examining how to <a id="_idIndexMarker606"/>conduct <strong class="keyWord">market basket analysis</strong> in Python, which helps marketers better understand which items are frequently bought together, how to<a id="_idIndexMarker607"/> build <strong class="keyWord">collaborative filtering</strong> algorithms in two approaches for personalized product recommendations, and what other approaches are taken for recommending products that are personalized to individual customers.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Product analytics with market basket analysis</li>
      <li class="bulletList">User-based versus item-based collaborative filtering</li>
      <li class="bulletList">Other frequently used recommendation methods</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-169">Product analytics with market basket analysis</h1>
    <p class="normal">The classic example<a id="_idIndexMarker608"/> of <strong class="keyWord">market basket analysis</strong> is that <a id="_idIndexMarker609"/>researchers have found that customers who buy diapers tend to buy beer as well. Although beer and diapers seem too distant items to go together, this finding shows that there are hidden associations between different products that we can find from data. The goal of market basket analysis is to find these hidden relationships between products and efficiently arrange for or recommend the products to customers. Market basket analysis helps answer questions like the following:</p>
    <ul>
      <li class="bulletList">Which products should be recommended to customers who are buying product X?</li>
      <li class="bulletList">What products should be close together in the store so that customers can easily find the products that they want to buy?</li>
    </ul>
    <p class="normal">To answer these questions, association rules of the market basket analysis should be found. An association rule, simply put, shows how confidently or significantly an itemset occurs in a transaction using a rule-based machine learning approach. An association rule has two parts: the antecedent, which is the condition of a rule, and the consequent, which is the result of a rule. Consider the following statement:</p>
    <blockquote class="packt_quote">
      <p class="quote"> “Customers who buy diapers are likely to buy beer.”</p>
    </blockquote>
    <p class="normal">In this statement, diapers are the antecedent and beer is the consequent. Five metrics are used to evaluate the strengths of these association rules:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Support</strong>: Support signals how frequently an itemset occurs in the dataset. The equation to calculate the support for an itemset (<em class="italic">X</em>, <em class="italic">Y</em>) is:</li>
    </ul>
    <p class="center"><img alt="" src="../Images/B30999_07_001.png"/><a id="_idIndexMarker610"/></p>
    <p class="normal">For example, if there are 3 transactions of diapers in all 10 transactions in the data, the support of diapers is 3/10 or 0.3. If there are 2 transactions of diapers and beer in all 10 transactions in the data, the support of diapers and beer is 2/10 or 0.2.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Confidence</strong>: Confidence is a conditional probability of the itemset (<em class="italic">X</em>,<em class="italic"> Y</em>) occurring given that the antecedent <em class="italic">X</em> has occurred. The equation to calculate the confidence for the consequent <em class="italic">Y</em> given the antecedent <em class="italic">X</em> is:</li>
    </ul>
    <p class="center"><img alt="" src="../Images/B30999_07_002.png"/><a id="_idIndexMarker611"/></p>
    <p class="normal">For example, if there are 3 transactions of diapers and 2 transactions of diapers and beer in all 10 transactions, the confidence of buying beer given diapers is 2/3 or 0.67. The confidence value will be 1 if the consequent always occurs with the antecedent.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Lift</strong>: Lift is a metric to measure how much more often the itemset (X, Y) occurs together than if the antecedent and consequent were independent events. The equation for the lift for the antecedent X and consequent Y is:</li>
    </ul>
    <p class="center"><img alt="" src="../Images/B30999_07_003.png"/><a id="_idIndexMarker612"/></p>
    <p class="normal">For example, if the confidence of diapers and beer was 2/3 and the support of beer was 5/10, then the lift of diapers to beer would be 2/3 divided by 5/10, which is 20/15 or 4/3. If the antecedent and the consequent are independent, the lift score will be 1.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Leverage</strong>: Leverage is a metric used to measure the difference between the frequency <a id="_idIndexMarker613"/>of the antecedent<a id="_idIndexMarker614"/> and the consequent occurring together and the frequency of the antecedent and the consequent if they were independent. The equation for the leverage is:</li>
    </ul>
    <p class="center"><img alt="" src="../Images/B30999_07_004.png"/><a id="_idIndexMarker615"/></p>
    <p class="normal">For example, if diapers and beer occurred in 4 transactions out of 10 total transactions and the support of diapers and beer were 0.6 and 0.5 respectively, then the leverage will be 0.4 – (0.6 * 0.5), which is 0.1. Leverage values range between –1 and 1 and if the antecedent and the consequent were independent, the leverage value will be 0.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Conviction</strong>: Conviction measures how much the consequent depends on the antecedent. The equation for the conviction is:</li>
    </ul>
    <p class="center"><img alt="" src="../Images/B30999_07_005.png"/><a id="_idIndexMarker616"/></p>
    <p class="normal">For example, if the confidence between diapers and beer is <code class="inlineCode">0.67</code> and the support of beer is <code class="inlineCode">0.2</code>, meaning there were 2 transactions of beer in 10 total transactions, the conviction would have been about <code class="inlineCode">2.42</code>. If the antecedent and the consequent are independent, then the conviction will be 1. On the other hand, if the confidence of the antecedent and consequent is 1 or if the consequent always occurs together with the antecedent, then the denominator becomes 0 and the conviction value becomes infinite.</p>
    <p class="normal">With this foundation, let’s dive into finding the association rules within an e-commerce dataset. We will be using an online retail dataset and the <code class="inlineCode">mlxtend</code> package to show how market basket <a id="_idIndexMarker617"/>analysis can be performed <a id="_idIndexMarker618"/>with Python.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Source code and data</strong>: <a href="https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.7 "><span class="url">https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.7</span></a></p>
      <p class="normal"><strong class="keyWord">Data source</strong>: <a href="https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.7/data.csv "><span class="url">https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.7/data.csv</span></a></p>
      <p class="normal">Here is the link for the original data source: <a href="https://archive.ics.uci.edu/dataset/352/online+retail"><span class="url">https://archive.ics.uci.edu/dataset/352/online+retail</span></a></p>
      <p class="normal">The following is the command for installing the <code class="inlineCode">mlxtend</code> package:</p>
      <pre class="programlisting con"><code class="hljs-con">pip install mlxtend
</code></pre>
    </div>
    <h2 class="heading-2" id="_idParaDest-170">Apriori algorithm – finding frequent itemsets</h2>
    <p class="normal">The Apriori algorithm is used to identify and generate the association rules that we discussed previously. We will discuss how to run the Apriori algorithm for market basket analysis in Python with an example. Let’s first load the data into a DataFrame using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
df = pd.read_csv(<span class="hljs-string">"./data.csv"</span>)
df = df.dropna(subset=[<span class="hljs-string">'CustomerID'</span>])
</code></pre>
    <p class="normal">Once we run this code, we now have all the purchases that the customers made. If you recall what we have discussed previously about the market basket analysis, the two key components are finding the itemsets and finding the rules among the itemsets. In this section, we are going to focus on finding the itemsets, which are the combinations of items.</p>
    <p class="normal">When you run the <code class="inlineCode">df["StockCode"].nunique()</code> command, you will see that there are 3,684 unique items or products in this data. As you can imagine, the number of combinations of the items you can create grows exponentially with the number of items. The number of all possible combinations of items is:</p>
    <p class="center"><img alt="" src="../Images/B30999_07_006.png"/><a id="_idIndexMarker619"/></p>
    <p class="normal">With over 3,000 items in our dataset, it will be unrealistic to check all the possible itemsets, as there are 2<sup class="superscript">3684</sup>-1 itemsets to check. This is where the <strong class="keyWord">Apriori</strong> algorithm comes in. Intuitively, the <a id="_idIndexMarker620"/>items <a id="_idIndexMarker621"/>or itemsets with low transaction frequency or low support are going to have less value in the findings. Thus, the Apriori algorithm extracts the itemsets that are considered frequent enough by the predefined threshold of support.</p>
    <p class="normal">To find the frequent itemsets using the Apriori algorithm, we need to convert the DataFrame in a way that it is a matrix where each row represents the customers and each column represents the items or products. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">customer_item_matrix = df.pivot_table(
    index=<span class="hljs-string">'CustomerID'</span>,
    columns=<span class="hljs-string">'StockCode'</span>,
    values=<span class="hljs-string">'Quantity'</span>,
    aggfunc=<span class="hljs-string">'sum'</span>
)
</code></pre>
    <p class="normal">Here, we are using the <code class="inlineCode">pivot_table</code> function to create a customer-to-item matrix and each value will be the total quantity of each item that each customer bought. With this customer-to-item matrix, we can run the Apriori algorithm to find the frequent itemsets using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> mlxtend.frequent_patterns <span class="hljs-keyword">import</span> apriori
frequent_items = apriori(
    customer_item_matrix,
    min_support=<span class="hljs-number">0.03</span>,
    use_colnames=<span class="hljs-literal">True</span>
)
frequent_items[<span class="hljs-string">"n_items"</span>] = frequent_items[<span class="hljs-string">"itemsets"</span>].apply(
<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x)
)
</code></pre>
    <p class="normal">As you can see from this code, we are using the <code class="inlineCode">apriori</code> function within the <code class="inlineCode">mlxtend</code> package. We have defined the minimum support required for each itemset to be <code class="inlineCode">0.03</code> with the <code class="inlineCode">min_support</code> parameter, which means we are going to ignore those itemsets that occur in less than 3% of the transactions. Then, we add a column, <code class="inlineCode">n_items</code>, which simply counts the number of items in each itemset.</p>
    <p class="normal">The final <a id="_idIndexMarker622"/>DataFrame <a id="_idIndexMarker623"/>looks like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_01.png"/></figure>
    <p class="packt_figref">Figure 7.1: The frequent_items DataFrame</p>
    <p class="normal">If you recall, the total number of possible combinations of itemsets was 2<sup class="superscript">3684</sup>-1. However, as you can see from this result of applying the Apriori algorithm, we now have 1,956 items, which <a id="_idIndexMarker624"/>is <a id="_idIndexMarker625"/>a manageable and reasonable amount of itemsets that we can examine the association rules for.</p>
    <h2 class="heading-2" id="_idParaDest-171">Association rules</h2>
    <p class="normal">With the frequent<a id="_idIndexMarker626"/> itemsets <a id="_idIndexMarker627"/>we have built, we can now generate the association rules that will tell us which itemsets are frequently bought together. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> mlxtend.frequent_patterns <span class="hljs-keyword">import</span> association_rules
rules = association_rules(
    frequent_items,
    metric=<span class="hljs-string">"confidence"</span>,
    min_threshold=<span class="hljs-number">0.6</span>,
    support_only=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">Here, we are using the <code class="inlineCode">association_rules</code> function in the <code class="inlineCode">mlxtend</code> package. There are a few key parameters to <a id="_idIndexMarker628"/>consider:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">metric</code>: This parameter defines which metric to use for selecting the association rules. Here, we <a id="_idIndexMarker629"/>are using <code class="inlineCode">confidence</code> to choose association rules with high confidence, but you can also use <code class="inlineCode">lift</code> if you are interested in finding the rules with high lift.</li>
      <li class="bulletList"><code class="inlineCode">min_threshold</code>: This parameter defines the threshold for the selection of association rules based on the metric you have chosen. Here, we are only interested in finding the rules with confidence above the 60% threshold.</li>
      <li class="bulletList"><code class="inlineCode">support_only</code>: You can set this parameter to <code class="inlineCode">True</code> if you are only interested in computing the support of the rules or if other metrics cannot be computed due to some data missing. Here, we set this to <code class="inlineCode">False</code> as we are interested in finding the rules with high confidence.</li>
    </ul>
    <p class="normal">The output of these rules should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_02.png"/></figure>
    <p class="packt_figref">Figure 7.2: Association rules based on confidence above 60%</p>
    <p class="normal">The association rule output contains antecedents, consequents, and key metrics, such as <code class="inlineCode">support</code>, <code class="inlineCode">confidence</code>, and <code class="inlineCode">lift</code>. You can sort by each metric and generate insights on which itemsets have close relationships. For example, you may want to find the top 20 itemsets that <a id="_idIndexMarker630"/>have the<a id="_idIndexMarker631"/> highest lift or the top 10 itemsets that have the highest confidence. You can also visualize these relationships so that you can easily view the relationships and the strengths of the relationships. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">most_lift = rules.sort_values(
    by=<span class="hljs-string">"lift"</span>, ascending=<span class="hljs-literal">False</span>
).head(<span class="hljs-number">20</span>).pivot_table(
    index=<span class="hljs-string">'antecedents'</span>,
    columns=<span class="hljs-string">'consequents'</span>,
    values=<span class="hljs-string">'lift'</span>,
    aggfunc=<span class="hljs-string">'sum'</span>
)
most_lift.index = [
    <span class="hljs-string">" + "</span>.join([
        df.loc[df[<span class="hljs-string">"</span><span class="hljs-string">StockCode"</span>] == item][<span class="hljs-string">"Description"</span>].unique()[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(x)
    ]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> most_lift.index
]
most_lift.columns = [
    <span class="hljs-string">" + "</span>.join([
        df.loc[df[<span class="hljs-string">"StockCode"</span>] == item][<span class="hljs-string">"</span><span class="hljs-string">Description"</span>].unique()[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(x)
    ]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> most_lift.columns
]
</code></pre>
    <p class="normal">Here, we first select the top 20 rules with the highest lift by using the <code class="inlineCode">sort_values</code> function. Then, we pivot the table so that each row is the antecedent, each column is the consequent, and the value of a cell is the lift. A heatmap can be used to visualize the strengths <a id="_idIndexMarker632"/>of these <a id="_idIndexMarker633"/>rules easily. The following code can be used to visualize these 20 rules with the highest lift:</p>
    <pre class="programlisting code"><code class="hljs-code">ax = plt.subplot()
sns.heatmap(
    most_lift,
    annot=<span class="hljs-literal">True</span>,
    annot_kws={<span class="hljs-string">"</span><span class="hljs-string">size"</span>: <span class="hljs-number">6</span>},
    <span class="hljs-comment"># fmt=".1f",</span>
    ax=ax
)
ax.set_title(<span class="hljs-string">"Top 20 Rules by Lift"</span>)
ax.set_xlabel(<span class="hljs-string">"Consequent"</span>)
ax.set_ylabel(<span class="hljs-string">"Antecedent"</span>)
ax.yaxis.set_ticklabels(<span class="hljs-built_in">list</span>(most_lift.index), fontsize=<span class="hljs-number">6</span>)
ax.xaxis.set_ticklabels(<span class="hljs-built_in">list</span>(most_lift.columns), fontsize=<span class="hljs-number">6</span>)
plt.show()
</code></pre>
    <p class="normal">This will generate a heatmap that looks like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_03.png"/></figure>
    <p class="packt_figref">Figure 7.3: The top 20 association rules with the highest lift</p>
    <p class="normal">In this chart, the<a id="_idIndexMarker634"/> lighter the<a id="_idIndexMarker635"/> color is, the higher the lift is for a given rule. For example, when the antecedent itemset is <code class="inlineCode">GREEN REGENCY TEACUP AND SAUCER</code>, <code class="inlineCode">REGENCY TEA PLATE ROSES</code> and the consequent itemset is <code class="inlineCode">ROSES REGENCY TEACUP AND SAUCER</code>, <code class="inlineCode">REGENCY TEA PLATE GREEN</code>, the lift is about 26 and the highest. On the other hand, when the antecedent itemset is <code class="inlineCode">ROSES REGENCY TEACUP AND SAUCER</code>, <code class="inlineCode">REGENCY TEA PLATE ROES</code> and the consequent itemset is <code class="inlineCode">GREEN REGENCY TEACUP AND SAUCER</code>, <code class="inlineCode">REGENCY TEA PLATE GREEN</code>, the lift is about 20 and the lowest among the top 20 rules.</p>
    <p class="normal">As you can see from this example, with the association rules, we can easily find out which products are bought together and their relationships with one another. This can be used in personalized marketing, where certain products are recommended based on these rules. If a customer purchased itemset A, then naturally you can recommend itemset B, which has high confidence or lift as the association rule suggests that A and B are frequently bought together with significance. This is one step closer to recommending individualized product sets in marketing messages rather than blindly recommending random product sets in the hopes that customers express interest in purchasing them.</p>
    <p class="normal">Lastly, if you <a id="_idIndexMarker636"/>recall, lift<a id="_idIndexMarker637"/> has a linear relationship with confidence and an inverse relationship with support, as lift is defined as:</p>
    <p class="center"><img alt="" src="../Images/B30999_07_007.png"/><a id="_idIndexMarker638"/></p>
    <p class="normal">This relationship can be easily shown from the rules we have found with the <code class="inlineCode">mlxtend</code> package. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">fig = plt.figure()
ax1 = fig.add_subplot(<span class="hljs-number">111</span>)
ax2 = ax1.twiny()
rules[[<span class="hljs-string">"support"</span>,<span class="hljs-string">"lift"</span>]].plot(
    kind=<span class="hljs-string">"scatter"</span>, x=<span class="hljs-string">"support"</span>, y=<span class="hljs-string">"lift"</span>, ax=ax1
)
rules[[<span class="hljs-string">"confidence"</span>,<span class="hljs-string">"lift"</span>]].plot(
    kind=<span class="hljs-string">"scatter"</span>, x=<span class="hljs-string">"confidence"</span>, y=<span class="hljs-string">"lift"</span>, ax=ax2, color=<span class="hljs-string">"orange"</span>, alpha=<span class="hljs-number">0.3</span>
)
ax1.legend([<span class="hljs-string">"lift vs. support"</span>,], loc=<span class="hljs-string">"upper right"</span>)
ax2.legend([<span class="hljs-string">"lift vs. confidence"</span>,], loc=<span class="hljs-string">"upper left"</span>)
plt.grid()
plt.show()
</code></pre>
    <p class="normal">Here, we have lift values on the y-axis and support and confidence values on the x-axis. The chart will look as the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_04.png"/></figure>
    <p class="packt_figref">Figure 7.4: The relationships between lift and confidence and between lift and support</p>
    <p class="normal">As you can see<a id="_idIndexMarker639"/> from this<a id="_idIndexMarker640"/> chart, the top x-axis shows the confidence values and the scatter plot of lift against confidence shows a linear relationship, where the lift values increase as the confidence values increase. On the other hand, the bottom x-axis shows the support values and the scatter plot of lift against support shows an inverse relationship, where the lift values decrease as the support values increase.</p>
    <h1 class="heading-1" id="_idParaDest-172">Collaborative filtering</h1>
    <p class="normal">There are various<a id="_idIndexMarker641"/> approaches in building recommendation systems and often multiple approaches take part. Some of the frequently used AI/ML-driven approaches in building recommendation systems are:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Collaborative filtering</strong>: This<a id="_idIndexMarker642"/> method uses previous user behaviors, such as pages they viewed, products they purchased, or ratings they have given previously. This algorithm leverages this kind of data to find similar products or content to those that the users have shown interest in previously. For example, if a user viewed a few thriller movies on a streaming platform, some other thriller movies may be recommended to this user. Or, if a customer bought dress shirts and dress shoes on an e-commerce platform, dress pants may be recommended to this customer. The collaborative filtering algorithms are often built based on the similarities between users or items:<ul>
          <li class="bulletList"><strong class="keyWord">User-based</strong> collaborative filtering uses data to find similar users based on the pages viewed or products purchased previously.</li>
          <li class="bulletList"><strong class="keyWord">Item-based</strong> collaborative filtering uses data to find items that are often bought or viewed together.</li>
        </ul>
      </li>
      <li class="bulletList"><strong class="keyWord">Content-based filtering</strong>: As the name suggests, this method uses the characteristics of products, contents, or users. Where the collaborative filtering algorithm focuses on user-to-user or item-to-item similarities, the content-based filtering algorithm focuses on characteristic similarities, such as genres, keywords, and metadata. For example, if you are recommending movies to watch, this approach may look at the descriptions, genres, or actors of movies and recommend those that match a user’s preferences. On the other hand, if you are recommending fashion items, then this approach may look at the product categories, descriptions, and brands to recommend certain items to users.</li>
      <li class="bulletList"><strong class="keyWord">Predictive modeling</strong>: As discussed in <em class="chapterRef">Chapter 6</em>, predictive models can be built for recommendation systems as well. Previous content that the users viewed, products that they have purchased, or web session history can be the features to identify the items that the users are highly likely to be interested in seeing or purchasing. The probability output of these predictive models can be used to rank the items so that more likely items are shown to the users before other less likely items.</li>
      <li class="bulletList"><strong class="keyWord">Hybrid models</strong>: Oftentimes, recommendation systems are built with all of the previously mentioned approaches. Blending different approaches for recommendations can help improve the recommendation accuracy that single-approach recommendation systems may miss.</li>
    </ul>
    <p class="normal">In this chapter, we are going to focus on the <em class="italic">collaborative filtering</em> algorithm as it is a frequently used backbone of lots of recommender systems, and more specifically, how to build recommendation systems using user-based collaborative filtering and item-based collaborative filtering algorithms. This technique is frequently used in recommendation systems as it captures the interactions among the users and items very well in a very intuitive way. Lots of organizations where recommendations are critical in their businesses, such as Netflix, Amazon, and Spotify, use collaborative filtering algorithms as part of their recommendation systems.</p>
    <p class="normal">The key to the <a id="_idIndexMarker643"/>collaborative filtering algorithms is to find similar users or items. There can be various metrics that can be used to measure the similarities between users or items, such as Euclidean distance, Manhattan distance, or Jaccard distance. However, cosine similarity is one of the most frequently used similarity metrics in collaborative filtering, as it focuses more on the directional similarity over the magnitude of the distance.</p>
    <p class="normal"><strong class="keyWord">Cosine similarity</strong> is simply the cosine of the angle between two vectors. Here, vectors can be user vectors or item vectors in our case of collaborative filtering algorithms. As cosine similarity has strength in high-dimensional space, it is often chosen as the distance metric for collaborative filtering algorithms. The equation for computing the cosine similarity between two vectors looks as follows:</p>
    <p class="center"><img alt="" src="../Images/B30999_07_008.png"/></p>
    <p class="normal">In this equation,<em class="italic">V</em><sub class="subscript">1</sub><sub class="italic">i</sub> and <em class="italic">V</em><sub class="subscript">2</sub><sub class="italic">i</sub> represent each item in each vector, which can be a user vector or an item vector. These cosine similarity values range between -1 and 1. If the 2 vectors are similar, the cosine similarity value will be close to 1; if the 2 vectors are independent, then the cosine similarity value will be 0; if the 2 vectors are the opposite vectors, then the cosine similarity value will be close to -1.</p>
    <p class="normal">To build collaborative filtering algorithms, we will first need to build the customer-to-item matrix. We will be using the same code that we have used for the market basket analysis, which looks like the following:</p>
    <pre class="programlisting code"><code class="hljs-code">customer_item_matrix = df.pivot_table(
    index=<span class="hljs-string">'CustomerID'</span>,
    columns=<span class="hljs-string">'StockCode'</span>,
    values=<span class="hljs-string">'Quantity'</span>,
    aggfunc=<span class="hljs-string">'sum'</span>
)
</code></pre>
    <p class="normal">As before, this matrix shows the quantities purchased for each item (column) by each customer (row). Instead of using the raw quantities of items purchased, we are going to one-hot encode so that the values are 1 if a given customer bought a given item or 0 if not, as in the<a id="_idIndexMarker644"/> following code:</p>
    <pre class="programlisting code"><code class="hljs-code">customer_item_matrix = customer_item_matrix.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>
)
</code></pre>
    <p class="normal">The <code class="inlineCode">customer_item_matrix</code> matrix should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_05.png"/></figure>
    <p class="packt_figref">Figure 7.5: The customer-to-item matrix</p>
    <p class="normal">As expected, each row represents whether a given user bought each item or not. This matrix will be the foundational matrix that we will use to build user-based collaborative filtering and<a id="_idIndexMarker645"/> item-based collaborative filtering algorithms in the following sections.</p>
    <h2 class="heading-2" id="_idParaDest-173">User-based collaborative filtering</h2>
    <p class="normal">The first <a id="_idIndexMarker646"/>step to building a user-based <a id="_idIndexMarker647"/>collaborative filtering model is to build a matrix of similar users. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.metrics.pairwise <span class="hljs-keyword">import</span> cosine_similarity
user_user_sim_matrix = pd.DataFrame(
    cosine_similarity(customer_item_matrix)
)
user_user_sim_matrix.columns = customer_item_matrix.index
user_user_sim_matrix[<span class="hljs-string">'CustomerID'</span>] = customer_item_matrix.index
user_user_sim_matrix = user_user_sim_matrix.set_index(<span class="hljs-string">'CustomerID'</span>)
</code></pre>
    <p class="normal">If you recall, the cosine similarity measure is one of the most frequently used similarity metrics for collaborative filtering algorithms. Here, we are using the scikit-learn package’s <code class="inlineCode">cosine_similarity</code> function to compute cosine similarities between users. The newly created variable, <code class="inlineCode">user_user_sim_matrix</code>, will be a matrix where each row and column represents a user and the similarities between two users.</p>
    <p class="normal">Take a look at the following example:</p>
    <pre class="programlisting code"><code class="hljs-code">user_user_sim_matrix.loc[
    [<span class="hljs-number">12347.0</span>, <span class="hljs-number">12348.0</span>, <span class="hljs-number">12349.0</span>, <span class="hljs-number">12350.0</span>, <span class="hljs-number">12352.0</span>],
    [<span class="hljs-number">12347.0</span>, <span class="hljs-number">12348.0</span>, <span class="hljs-number">12349.0</span>, <span class="hljs-number">12350.0</span>, <span class="hljs-number">12352.0</span>]
]
</code></pre>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_06.png"/></figure>
    <p class="packt_figref">Figure 7.6: A sample of user-to-user similarities</p>
    <p class="normal">This example shows the user-to-user cosine similarity metrics. As expected, the diagonal values are 1 since they represent the similarities between a user and itself. For example, users <code class="inlineCode">12349</code> and <code class="inlineCode">12350</code> have a cosine similarity of about <code class="inlineCode">0.057</code> and users <code class="inlineCode">12349</code> and <code class="inlineCode">12352</code> have a cosine similarity of <code class="inlineCode">0.138</code>. This suggests that user <code class="inlineCode">12349</code> is more similar to user <code class="inlineCode">12352</code> than to user <code class="inlineCode">12350</code>. This way, you can identify and rank the users by how similar they are to a target user.</p>
    <p class="normal">Let’s pick one customer and see how we may be able to build a recommendation system. We will select the user with ID <code class="inlineCode">14806</code> as an example for this exercise:</p>
    <pre class="programlisting code"><code class="hljs-code">TARGET_CUSTOMER = <span class="hljs-number">14806.0</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Similar Customers to TARGET"</span>)
user_user_sim_matrix.loc[
TARGET_CUSTOMER
].sort_values(ascending=<span class="hljs-literal">False</span>).head(<span class="hljs-number">10</span>)
</code></pre>
    <p class="normal">Here, we select the<a id="_idIndexMarker648"/> top 10 users that have the<a id="_idIndexMarker649"/> highest cosine similarity measures, by selecting <code class="inlineCode">TARGET_CUSTOMER</code> in the <code class="inlineCode">user_user_sim_matrix</code> matrix and sorting the values in descending order. The output should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_07.png"/></figure>
    <p class="packt_figref">Figure 7.7: Similar customers to the target customer</p>
    <p class="normal">Here, you can see <a id="_idIndexMarker650"/>that customer <code class="inlineCode">13919</code> is the most similar to the target customer, <code class="inlineCode">14806</code>, customer <code class="inlineCode">12561</code> comes second, and customer <code class="inlineCode">13711</code> follows in third.</p>
    <h3 class="heading-" id="_idParaDest-174">Recommending by the most similar customer</h3>
    <p class="normal">The simplest<a id="_idIndexMarker651"/> approach to recommend products to the target customer is to see which items the target customer has bought already and compare them against the most similar customer, <code class="inlineCode">13919</code>. Then, recommend the products that the target customer has not bought yet that the most similar customer has bought. This is based on the assumption, as discussed previously, that customers similar to each other are likely to behave similarly or share similar interests and they are likely to purchase similar products.</p>
    <p class="normal">To do this, we can follow these steps:</p>
    <ol>
      <li class="numberedList" value="1">We will first find out the items that the target customer has bought already, using the following code:
        <pre class="programlisting code"><code class="hljs-code">items_bought_by_target = <span class="hljs-built_in">set</span>(
    df.loc[
        df[<span class="hljs-string">"CustomerID"</span>] == TARGET_CUSTOMER
    ][<span class="hljs-string">"StockCode"</span>].unique()
)
</code></pre>
      </li>
    </ol>
    <p class="normal">This will get all the items that the target customer has bought and store them in the <code class="inlineCode">items_bought_by_target</code> variable.</p>
    <ol>
      <li class="numberedList" value="2">Next, we need to get all the items the most similar customer, <code class="inlineCode">13919</code>, has bought. Take a look at the following code:
        <pre class="programlisting code"><code class="hljs-code">items_bought_by_sim = <span class="hljs-built_in">set</span>(
    df.loc[
        df[<span class="hljs-string">"CustomerID"</span>] == <span class="hljs-number">13919.0</span>
    ][<span class="hljs-string">"StockCode"</span>].unique()
)
</code></pre>
      </li>
    </ol>
    <p class="normal">This finds out all the items that the most similar customer, <code class="inlineCode">13919</code>, has bought and stores them in the <code class="inlineCode">items_bought_by_sim</code> variable.</p>
    <ol>
      <li class="numberedList" value="3">Using the <code class="inlineCode">set</code> operation, it is easy to find out the items that the most similar customer has bought but not bought by the target customer, as shown in the following code:
        <pre class="programlisting code"><code class="hljs-code">items_bought_by_sim_but_not_by_target = items_bought_by_sim - items_bought_by_target
</code></pre>
      </li>
    </ol>
    <p class="normal">As you can see, we simply subtract the <code class="inlineCode">items_bought_by_target</code> set from the other set, <code class="inlineCode">items_bought_by_sim</code>. This will get all the items that the target customer has not yet bought but were bought by the most similar customer and store them in <code class="inlineCode">items_bought_by_sim_but_not_by_target</code>.</p>
    <ol>
      <li class="numberedList" value="4">We can<a id="_idIndexMarker652"/> get the details about these items using the following code:
        <pre class="programlisting code"><code class="hljs-code">df.loc[
    df[<span class="hljs-string">"StockCode"</span>].isin(items_bought_by_sim_but_not_by_target)
][[<span class="hljs-string">"StockCode"</span>, <span class="hljs-string">"Description"</span>]].drop_duplicates()
</code></pre>
      </li>
    </ol>
    <p class="normal">This output should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_08.png"/></figure>
    <p class="packt_figref">Figure 7.8: Items to recommend to the target customer</p>
    <p class="normal">This is the list<a id="_idIndexMarker653"/> of items that the most similar customer, <code class="inlineCode">13919</code>, has bought but have not been bought by the target customer, and their descriptions. In the most simple approach to recommending products, this list can be shown to the target customer as the recommended items. </p>
    <p class="normal">Since these two customers have shown similar interests by having bought similar items in the past, it is more likely for the target customer to purchase some of these items rather than a random selection of products. You can apply the same process<a id="_idIndexMarker654"/> for each customer and build sets of products to recommend based on the most similar customers’ historical purchases.</p>
    <h3 class="heading-" id="_idParaDest-175">Recommending by the top products bought by similar customers</h3>
    <p class="normal">Another approach <a id="_idIndexMarker655"/>is to rank the products by how frequently they were bought by the most similar customers.</p>
    <p class="normal">We can start with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">top10_similar_users = user_user_sim_matrix.loc[
    TARGET_CUSTOMER
].sort_values(
    ascending=<span class="hljs-literal">False</span>
).head(<span class="hljs-number">11</span>).to_dict()
potential_rec_items = {}
<span class="hljs-keyword">for</span> user, cos_sim <span class="hljs-keyword">in</span> top10_similar_users.items():
    <span class="hljs-keyword">if</span> user == TARGET_CUSTOMER:
        <span class="hljs-keyword">continue</span>
       
    items_bought_by_sim = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(
        df.loc[
            df[<span class="hljs-string">"CustomerID"</span>] == user
        ][<span class="hljs-string">"StockCode"</span>].unique()
    ))
    <span class="hljs-keyword">for</span> each_item <span class="hljs-keyword">in</span> items_bought_by_sim:
        <span class="hljs-keyword">if</span> each_item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> potential_rec_items:
            potential_rec_items[each_item] = <span class="hljs-number">0</span>
        potential_rec_items[each_item] += cos_sim
potential_rec_items = [(key, val) <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> potential_rec_items.items()]
potential_rec_items = <span class="hljs-built_in">sorted</span>(
    potential_rec_items, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">Here, we first get the top 10 most similar customers by the cosine similarity. Then, for each similar customer, we iterate through the items that were bought by the similar customer but not by the target customer. We count the number of times similar customers bought the given product, sort by the purchase frequency in the reverse order, and store it in the <code class="inlineCode">potential_rec_items</code> variable. The result should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_09.png"/></figure>
    <p class="packt_figref">Figure 7.9: Top products purchased by similar customers</p>
    <p class="normal">This suggests<a id="_idIndexMarker656"/> that 7 out of 10 customers have bought the <code class="inlineCode">21500</code> and <code class="inlineCode">21499</code> products, 4 out of 10 customers have bought the <code class="inlineCode">21498</code> and <code class="inlineCode">POST</code> products, and so forth. We can query to get the product descriptions from these product IDs using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">top_10_items = [x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> potential_rec_items[:<span class="hljs-number">10</span>]]
df.loc[
    df[<span class="hljs-string">"StockCode"</span>].isin(top_10_items)
][[<span class="hljs-string">"StockCode"</span>, <span class="hljs-string">"</span><span class="hljs-string">Description"</span>]].drop_duplicates().set_index(
    <span class="hljs-string">"StockCode"</span>
).loc[top_10_items]
</code></pre>
    <p class="normal">This output should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_10.png"/></figure>
    <p class="packt_figref">Figure 7.10: Top products purchased by similar customers</p>
    <p class="normal">This approach, however, does not account for how similar or dissimilar each customer was to the target customer. It just counted the number of top 10 similar customers for each product and recommended based on the simple sum.</p>
    <p class="normal">Since we have similarity measures for each of the top 10 similar customers, we may also want to consider<a id="_idIndexMarker657"/> that when we recommend products. Instead of simple counts, we can do weighted counts. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">potential_rec_items = {}
<span class="hljs-keyword">for</span> user, cos_sim <span class="hljs-keyword">in</span> top10_similar_users.items():
    <span class="hljs-keyword">if</span> user == TARGET_CUSTOMER:
        <span class="hljs-keyword">continue</span>
       
    items_bought_by_sim = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(
        df.loc[
            df[<span class="hljs-string">"CustomerID"</span>] == user
        ][<span class="hljs-string">"StockCode"</span>].unique()
    ))
    <span class="hljs-keyword">for</span> each_item <span class="hljs-keyword">in</span> items_bought_by_sim:
        <span class="hljs-keyword">if</span> each_item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> potential_rec_items:
            potential_rec_items[each_item] = <span class="hljs-number">0</span>
        potential_rec_items[each_item] += cos_sim
potential_rec_items = [(key, val) <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> potential_rec_items.items()]
potential_rec_items = <span class="hljs-built_in">sorted</span>(
    potential_rec_items, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">As you may notice, the only part that is different from the previous code is <code class="inlineCode">potential_rec_items[each_item] += cos_sim</code>. Instead of simply counting, we are now adding the cosine similarity metric that measures how similar or dissimilar each customer is to the target customer.</p>
    <p class="normal">When we query<a id="_idIndexMarker658"/> the top 10 items weighted by the cosine similarity, the results look as in the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_11.png"/></figure>
    <p class="packt_figref">Figure 7.11: Top products purchased by similar customers (weighted by cosine similarity)</p>
    <p class="normal">In this example, the output from the simple count is the same as the output from the weighted count. Not only can you do the weighted sum of cosine similarities but you can also do the weighted average. There are many other ways you can aggregate the scores and recommend products, so being creative is the key to building the final recommendation output from the cosine similarity measures.</p>
    <p class="normal">As you can see from these examples, we can build product sets to recommend based on the customers who have shown similar behaviors in the past. Oftentimes, people with similar tastes buy similar items, so this user-based collaborative filtering works well when recommending products based on the similarities among your customer base. On top of recommending based on customer similarities, we can also recommend based on how similar individual items are to others. We are going to discuss how to build recommendation<a id="_idIndexMarker659"/> systems based on item-based collaborative filtering in the following section.</p>
    <h2 class="heading-2" id="_idParaDest-176">Item-based collaborative filtering</h2>
    <p class="normal">As in the case of<a id="_idIndexMarker660"/> user-based <a id="_idIndexMarker661"/>collaborative filtering, the key starting point of building an item-based collaborative filtering algorithm is to build an item-to-item similarity matrix. From the customer-item matrix we built previously, we can use the following code to build an item-to-item similarity matrix:</p>
    <pre class="programlisting code"><code class="hljs-code">item_item_sim_matrix = pd.DataFrame(
    cosine_similarity(customer_item_matrix.T)
)
</code></pre>
    <p class="normal">As shown in this code, we transpose the customer-item matrix, <code class="inlineCode">customer_item_matrix</code>, first, which will make it an item-to-customer matrix where each row is an item, each column is a customer, and each value is 0 or 1 representing whether a given item is bought by a given column. Then, we compute cosine similarity by using the <code class="inlineCode">cosine_similarity</code> function and save the results as a pandas DataFrame in an <code class="inlineCode">item_item_sim_matrix</code> variable. </p>
    <p class="normal">The results should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_12.png"/></figure>
    <p class="packt_figref">Figure 7.12: Item-to-item similarity matrix</p>
    <h3 class="heading-" id="_idParaDest-177">Recommending by the most similar items</h3>
    <p class="normal">The simplest <a id="_idIndexMarker662"/>approach to generating recommendations based on item similarities is to find the most similar items for a given item and recommend them. When a person is viewing a certain product and you want to show related or similar items, this approach can be used. For example, when you search for a product on Amazon and click on one of the products shown on the search page, Amazon will show related or similar items at the bottom of the page that are often bought together. This is where item-based collaborative filtering can be used and, more specifically, when you want to recommend products based on one given product.</p>
    <p class="normal">Using our example dataset, let’s assume a customer is looking at a product, <code class="inlineCode">MEDIUM CERAMIC TOP STORAGE JAR</code>, which has a product code of <code class="inlineCode">23166</code>. From the item-to-item matrix, <code class="inlineCode">item_item_sim_matrix</code>, that we have just built, we can find the top 10 most similar items with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">most_similar_items = item_item_sim_matrix.loc[
    <span class="hljs-string">"23166"</span>
].sort_values(ascending=<span class="hljs-literal">False</span>).head(<span class="hljs-number">10</span>)
</code></pre>
    <p class="normal">As you can see in this code, we are selecting the top 10 most similar items by ordering the items by the cosine similarity in descending order. The output should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_13.png"/></figure>
    <p class="packt_figref">Figure 7.13: Top 10 similar items to product 23166</p>
    <p class="normal">We can also query <a id="_idIndexMarker663"/>the descriptions based on these product codes as in the following:</p>
    <pre class="programlisting code"><code class="hljs-code">rec_items = [
    x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> most_similar_items.index <span class="hljs-keyword">if</span> x != <span class="hljs-string">"23166"</span>
]
df.loc[
    df[<span class="hljs-string">'StockCode'</span>].isin(rec_items),
    [<span class="hljs-string">'StockCode'</span>, <span class="hljs-string">'Description'</span>]
].drop_duplicates().set_index(<span class="hljs-string">'StockCode'</span>).loc[rec_items]
</code></pre>
    <p class="normal">As you can see, we are excluding product <code class="inlineCode">23166</code> as it is the target item or the item that the customer is currently viewing and showing the rest of the 10 most similar items. The output should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_14.png"/></figure>
    <p class="packt_figref">Figure 7.14: Top 10 items similar to product 23166, including itself</p>
    <p class="normal">As you can imagine, we can show these 10 items, including itself, on the page while the customer is viewing the target product. This way, we are showing the most similar or the most frequently bought<a id="_idIndexMarker664"/> together items so that the customer has more options to choose from or more items to purchase.</p>
    <h3 class="heading-" id="_idParaDest-178">Recommending by the purchase history</h3>
    <p class="normal">Expanding on the<a id="_idIndexMarker665"/> previous example of recommending products based on a single item, we can also build an item-based collaborative filtering recommendation system for multiple items. Especially when you may be sending out marketing emails or newsletters, you may want to send product recommendations along with them. In this case, it will be useful to consider the purchase history of customers, such as what kinds of products they have purchased, what product pages they have viewed online, or what products they have put in the cart. Using this information, we can build personalized product recommendations that are different for each customer using the item-based collaborative filtering algorithm.</p>
    <p class="normal">Using our example dataset as an example, let’s assume a customer has bought three items with product codes <code class="inlineCode">23166</code>, <code class="inlineCode">22720</code>, and <code class="inlineCode">23243</code>. These items on our dataset are <code class="inlineCode">SET OF 3 CAKE TINS PANTRY DESIGN</code>, <code class="inlineCode">MEDIUM CERAMIC TOP STORAGE JAR</code>, and <code class="inlineCode">SET OF TEA COFFEE SUGAR TINS PANTRY</code>. You can get the top 10 most similar items for these items using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">item_item_sim_matrix[[
    <span class="hljs-string">"23166"</span>, <span class="hljs-string">"22720"</span>, <span class="hljs-string">"23243"</span>
]].mean(axis=<span class="hljs-number">1</span>).sort_values(
    ascending=<span class="hljs-literal">False</span>
).head(<span class="hljs-number">10</span>)
</code></pre>
    <p class="normal">As you can see from this code, we query the item-to-item matrix for those three items that the customer has bought and take the mean of the cosine similarities. Then, we sort these average values in descending order to get the top 10 most similar items to the given 3 items. The output should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_15.png"/></figure>
    <p class="packt_figref">Figure 7.15: Top 10 similar items to the given 3 items</p>
    <p class="normal">Excluding the <a id="_idIndexMarker666"/>items that were already bought, product <code class="inlineCode">22722</code> comes first and is the most similar item to the given itemset, and <code class="inlineCode">23165</code> follows the second. We can query the data to get the product descriptions of these top 10 similar items. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">rec_items = [
    x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> item_item_sim_matrix[[
        <span class="hljs-string">"23166"</span>, <span class="hljs-string">"22720"</span>, <span class="hljs-string">"23243"</span>
    ]].mean(axis=<span class="hljs-number">1</span>).sort_values(ascending=<span class="hljs-literal">False</span>).head(<span class="hljs-number">13</span>).index
    <span class="hljs-keyword">if</span> x <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">"23166"</span>, <span class="hljs-string">"22720"</span>, <span class="hljs-string">"23243"</span>]
]
df.loc[
    df[<span class="hljs-string">'StockCode'</span>].isin(rec_items),
    [<span class="hljs-string">'StockCode'</span>, <span class="hljs-string">'Description'</span>]
].drop_duplicates().set_index(<span class="hljs-string">'StockCode'</span>).loc[rec_items]
</code></pre>
    <p class="normal">Here, we exclude the 3 items that the customer has already bought from the recommendation list and retrieve the top 10 items that are the most similar or the most frequently bought together with these 3 items. The output of this query should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_07_16.png"/></figure>
    <p class="packt_figref">Figure 7.16: Top 10 similar items to the given 3 items</p>
    <p class="normal">As you can imagine, these <a id="_idIndexMarker667"/>product recommendations based on the item-based collaborative filtering algorithm results can be used for any marketing campaign. If you are building an email marketing campaign, you can run this item-based collaborative filtering algorithm to select the top similar products based on previous purchases, page views, or cart items and include them as part of the marketing emails. You can also have pop-up windows on web pages for when this user logs into your online page next time. Giving discounts or some other promotions on these items can also result in higher chances of conversions.</p>
    <p class="normal">There are numerous ways you can apply user-based and item-based collaborative filtering for recommendation systems. It will be wise to keep track of how your recommendations perform and tune how you use them as you progress with your recommendation systems. You will most likely want to track, of those items that you have recommended, how many of them have converted. </p>
    <p class="normal">As discussed in <em class="chapterRef">Chapter 2</em> when we discussed KPIs, tracking conversion and other KPIs among the recommended items can tell you the<a id="_idIndexMarker668"/> effectiveness of your recommendation systems and which approach works better.</p>
    <h1 class="heading-1" id="_idParaDest-179">Other frequently used recommendation methods</h1>
    <p class="normal">We have discussed the market basket analysis and collaborative filtering in depth for building personalized<a id="_idIndexMarker669"/> recommendation systems. However, there are various other ways these recommendation systems can be built. As previously mentioned, some of the common AI/ML-based approaches are association rules and collaborative filtering algorithms, which we have covered in this chapter; predictive modeling approaches are often used as well, and nowadays a hybrid of all these approaches is a typical method of building more comprehensive recommendation systems.</p>
    <p class="normal">Not only are there AI/ML-driven approaches for recommendation systems but there can be various other ways to recommend products or content without even using AI/ML. The following are some common methods used for making recommendations:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Bestsellers or top views</strong>: As the name suggests, recommendations based on the bestselling products or the most frequently viewed content are still used frequently. This helps new users or customers acquaint themselves with your products or platforms by easily viewing what others view or purchase the most on your platform. These can also be a great way to build your introductory marketing content.</li>
      <li class="bulletList"><strong class="keyWord">Trending</strong>: Similar to bestsellers, trending items show what is rising in popularity at a given moment. There can be some events, such as disasters in certain regions, breaking news at certain moments, or holiday events happening around the neighborhoods, that spike customer interest sporadically. It’s wise to adjust for these events as these special events trigger certain customer behaviors and can be great marketing opportunities.</li>
      <li class="bulletList"><strong class="keyWord">New arrivals</strong>: As marketers, you most likely would not want to miss an opportunity to promote new products. Customers, existing or new, are often attracted to new arrival items, and this can be a great marketing opportunity to retain existing customers, as well as gain new customers. By recommending new arrivals to customers who have shown interest in similar items, the marketing campaign can result in great success.</li>
      <li class="bulletList"><strong class="keyWord">Promotions</strong>: Not only can you recommend products or content by their popularity or relevance but you can also recommend products or content that are going on promotion. Customers are often attracted to special deals and you would want to expose and market special promotions as actively as possible. These are great ways to clear out overstocked inventories, raise brand awareness, bring in new customers, and retain inactive customers.</li>
    </ul>
    <p class="normal">As you can see, there can be various ways you can build recommendation systems on top of utilizing AI/ML-driven methodologies. Given the competitive landscape and abundance of marketing campaigns basically by all businesses around the world, using only one approach in recommending products or content is not going to be that successful. The more comprehensive recommendation system you have, the more successful you will be <a id="_idIndexMarker670"/>in attracting and retaining customers. When you are designing recommendation systems, it would be wise to consider all the approaches mentioned in this chapter and apply them at the right moments and places.</p>
    <h1 class="heading-1" id="_idParaDest-180">Summary</h1>
    <p class="normal">In this chapter, we discussed building personalized product recommendation systems. First, we discussed how to identify interrelationships between products by analyzing which itemsets are frequently bought together. We covered how to conduct market basket analysis in Python using the Apriori algorithm and association rules. Then, we dove deep into one of the AI/ML-driven approaches to building recommendation systems. We saw how user-based and item-based collaborative filtering algorithms can be used to identify similar users or items and products that are frequently bought together. In turn, these findings can then be used to recommend certain products that other customers are highly likely to be interested in and purchase. Lastly, we discussed various other approaches that can be used for recommending products and content. We showed how combining non-AI/ML approaches can result in a more comprehensive and diverse recommendation system.</p>
    <p class="normal">In the following chapter, we will continue our discussion around personalized and targeted marketing efforts. More specifically, we are going to cover how customer segmentations can be done in Python and why having an in-depth understanding of different customer segments within your customer base helps and affects the successes and failures of your marketing efforts.</p>
    <h1 class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 5000 members at:</p>
    <p class="normal"><a href="https://packt.link/genai"><span class="url">https://packt.link/genai</span></a></p>
    <p class="normal"><img alt="" src="../Images/QR_Code12856128601808671.png"/></p>
  </div>
</body></html>