- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Application Services for AI/ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the AWS AI services for building chatbots,
    advanced text analysis, document analysis, transcription, and so on. This chapter
    has been designed in such a way that you can solve different use cases by integrating
    AWS AI services and get an idea of how they work. AWS is growing every day, and
    they are adding new AI services regularly.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will approach different use cases programmatically or from
    the console. This will help you understand different APIs and how to use them.
    You will use S3 for storage and AWS Lambda to execute any code. The examples in
    this chapter are in Python, but you can use other supported languages such as
    Java, Node.js, .NET, PowerShell, Ruby, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing images and videos with Amazon Rekognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text to speech with Amazon Polly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech to text with Amazon Transcribe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing natural language processing with Amazon Comprehend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating documents with Amazon Translate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting text from documents with Amazon Textract
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating chatbots on Amazon Lex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series forecasting with Amazon Forecast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All you need for this chapter is an AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the code examples for this chapter from GitHub at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing images and videos with Amazon Rekognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you need to add powerful visual analysis to your applications, then **Amazon
    Rekognition** is the service to choose. **Rekognition Image** lets you easily
    build powerful applications to search, verify, and organize millions of images.
    It lets you extract motion-based context from stored or live stream videos, and
    helps you analyze them. Rekognition Video also allows you to index metadata such
    as objects, activities, scenes, celebrities, and faces, making video searches
    easy. Rekognition Image uses deep neural network models to detect and label numerous
    objects and scenes in your images. It helps you capture text in an image, a bit
    like **Optical Character Recognition (OCR)**. A perfect example is a T-shirt with
    quotes on it. If you were to take a picture of one and ask Amazon Rekognition
    to extract the text from it, it would be able to tell you what the text says.
    You can also perform celebrity recognition using Amazon Rekognition. Somebody
    who is not a celebrity won’t use the celebrity recognition API for their face;
    instead, they will use the face comparison API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The official documentation, available at [https://aws.amazon.com/rekognition/faqs/](https://aws.amazon.com/rekognition/faqs/),
    states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“With Rekognition Image, you only pay for the images you analyze and the face
    metadata you store. You will not be charged for the compute resources if, at any
    point of time, your* *training fails.”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Rekognition include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Image and video analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searchable image library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face-based user verification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text in image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image moderation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search index for video archives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy filtering of explicit and suggestive content in videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of explicit nudity – sexual activity, graphical nudity, adult toys,
    and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of suggestive content – partial nudity, swimwear or underwear, and
    so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Rekognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some of the benefits of using Amazon Rekognition:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS manages the infrastructure it runs on. In short, just use the API for your
    image analysis. You need to only focus on building and managing your deep learning
    pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With or without knowledge of image processing, you can perform image and video
    analysis just by using the APIs provided in Amazon Rekognition, which can be used
    for any application or service on several platforms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Labels API’s response will identify real-world entities within an image
    through the DetectLabels API. These labels include city, town, table, home, garden,
    animal, pets, food, drink, electronics, flowers, and more. The entities are classified
    based on their **confidence score**, which indicates the probability that a given
    prediction is correct — the higher the score, the better. Similarly, you can use
    the DetectText API to extract the text in an image. Amazon Rekognition may detect
    multiple lines based on the gap between words. Periods do not represent the end
    of a line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Rekognition can be integrated with AWS Kinesis Video Stream, AWS S3,
    and AWS Lambda for seamless and affordable image and video analysis. With the
    AWS IAM service, Amazon Rekognition API calls can easily be secured and controlled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Low cost: you only pay for the images and videos that are analyzed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through AWS CloudTrail, all the API calls for Amazon Rekognition can be captured
    as events. It captures all calls made from the console, the CLI, or code calls
    for APIs, which further enables the user to create Amazon SNS notifications based
    on CloudTrail events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can create a VPC endpoint policy for specific API calls to establish a private
    connection between your VPC and Amazon Rekognition. This helps you leverage enhanced
    security. As per the AWS Shared Responsibility Model, AWS takes care of the security
    of the infrastructure and software, and you have to take care of the security
    of your content in the cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Rekognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will learn how to integrate AWS Lambda with Amazon Rekognition
    to detect the labels in our image (uploaded at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Rekognition%20Demo/images](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Rekognition%20Demo/images))
    and print the detected objects in the CloudWatch console. You will use the `detect_labels`
    API from Amazon Rekognition in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will begin by creating an IAM role for Lambda:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the IAM console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Roles** from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Lambda** from the **Choose a use** **case** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following managed policies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AmazonS3ReadOnlyAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonRekognitionFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudWatchLogsFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Name the role `rekognition-lambda-role`:![Figure 8.1 – The Create role dialog](img/B21197_08_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 8.1 – The Create role dialog
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will create a Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS Lambda console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create function**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `lambda-rekognition`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choose `Python 3.6` from the `rekognition-lambda-role`:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Creating the Lambda function](img/B21197_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Creating the Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following code in `lambda_function.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, you will create a trigger for the Lambda Function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Navigate to the AWS S3 console page. Create a bucket, for example, `rekognition-test-baba`,
    as shown in *Figure 8**.3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.3 – AWS S3 console page](img/B21197_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – AWS S3 console page
  prefs: []
  type: TYPE_NORMAL
- en: Click on `images`. Click **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Properties** tab of our bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll to **Events** for that bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inside the `rekognition_event`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`All object` `create events`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`images`*/*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Lambda Function`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`lambda-rekognition`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.4 – S3 bucket Events window](img/B21197_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – S3 bucket Events window
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will upload the image from the shared GitHub repository to the S3
    bucket `images` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as you upload, you can check the **Monitoring** tab in the Lambda console
    to monitor the events, as shown in *Figure 8**.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.5 – CloudWatch monitoring the event in the Lambda console](img/B21197_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – CloudWatch monitoring the event in the Lambda console
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to `CloudWatch > CloudWatch Logs > Log groups > /aws/lambda/lambda-rekognition`.
    Select the latest stream from all the streams listed on the AWS console and scroll
    down in the logs to see your output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, you learned how to implement the Amazon Rekognition AI service
    to detect objects in an image and get a confidence score for each. You will see
    more use cases for Amazon Rekognition in the upcoming sections, where you will
    detect text in images. In the next section, you will learn about Amazon’s text-to-speech
    service and implement it.
  prefs: []
  type: TYPE_NORMAL
- en: Text to speech with Amazon Polly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Polly** is all about converting text into speech, and it does so by
    using pretrained deep learning models. It is a fully managed service, so you do
    not have to do anything. You provide the plain text as input for synthesizing
    or in **Speech Synthesis** **Markup Language (SSML)** format so that an audio
    stream is returned. It also gives you different languages and voices to choose
    from, with both male and female options. The output audio from Amazon Polly can
    be saved in MP3 format for further use in the application (web or mobile) or can
    be a JSON output for written speech.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you were to input the text “Baba went to the library” into
    Amazon Polly, the output speech mark object would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{"``time":370,"type":"word","start":5,"end":9,"value":"went"}`'
  prefs: []
  type: TYPE_NORMAL
- en: The word `"went"` begins 370 milliseconds after the audio stream begins, and
    starts at byte 5 and ends at byte 9 of the given input text.
  prefs: []
  type: TYPE_NORMAL
- en: It also returns output in `ogg_vorbis` and `pcm` format. When `pcm` is used,
    the content that is returned as an output is `audio/pcm` in a signed 16-bit, 1-channel
    (mono), little-endian format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Polly include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Can be used as an accessibility tool for reading web content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be integrated with Amazon Rekognition to help visually impaired people read
    signs. You can click a picture of the sign with text and feed it to Amazon Rekognition
    to extract text. The output text can be used as input for Polly, and it will return
    a voice as output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used in a public address system, where the admin team can just pass on
    the text to be announced and Amazon Polly does the magic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining Amazon Polly with `audio/video receiver` **(****AVR)** system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smart devices such as smart TVs, smart watches, and **Internet of Things (IoT)**
    devices can use this for audio output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narration generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When combined with Amazon Lex, full-blown voice user interfaces for applications
    can be developed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s explore the benefits of Amazon Polly.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Polly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the benefits of using Amazon Polly include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: This service is fully managed and does not require any admin cost to maintain
    or manage resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides an instant speech correction and enhancement facility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can develop your own access layer using the HTTP API from Amazon Polly.
    Development is easy due to the huge amount of language support that’s available,
    such as Python, Ruby, Go, C++, Java, and Node.js.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For certain neural voices, speech can be synthesized using the Newscaster style,
    to make them sound like a TV or radio broadcaster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Polly also allows you to modify the pronunciation of particular words
    or the use of new words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you’ll get hands-on with Amazon Polly.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Polly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will build a pipeline where you can integrate AWS Lambda
    with Amazon Polly. The pipeline reads a text file and generates an MP3 file, saving
    it to another folder in the same bucket. You will monitor the task’s progress
    in CloudWatch logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will begin by creating an IAM role for Lambda. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the IAM console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Roles** from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Lambda** as the trusted entity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following managed policies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AmazonS3FullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonPollyFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudWatchFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the role as `polly-lambda-role`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, you will create a Lambda function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to `Lambda > Functions >` `Create Function`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the function `polly-lambda`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the runtime to `python 3.6`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an existing role; that is, `polly-lambda-role`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Paste the code at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Rekognition%20Demo/lambda_code](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Rekognition%20Demo/lambda_code)
    into your Lambda function and check its progress in the CloudWatch console. You
    will be using the `start_speech_synthesis_task` API from Amazon Polly for this
    code; it is an asynchronous synthesis task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll down and in the `59` `sec`, as shown in *Figure 8**.6*, and click **Save**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The default is 3 seconds. Since this is an asynchronous operation, any retried
    attempts will create more files.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Edit basic settings window](img/B21197_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Edit basic settings window
  prefs: []
  type: TYPE_NORMAL
- en: Now, you will create a bucket to trigger an event.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS S3 console and create a bucket called `polly-test-baba`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder called `input-text` (in this example, you will only upload .`txt`
    files).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to `Properties > Events > Add notification`. Fill in the required fields,
    as shown here, and click on `polly_event`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`All object` `create events`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`input-text/`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.txt`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Lambda Function`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`polly-lambda`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, you will upload a file to trigger an event and check its progress in
    CloudWatchUpload, in this case, a file called `test_file.txt` in `input-text`,
    as shown in *Figure 8**.7*. You can download the sample file from this book’s
    GitHub repository at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Polly%20Demo/text_file](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Polly%20Demo/text_file):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The S3 bucket after uploading a text file for further processing](img/B21197_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – The S3 bucket after uploading a text file for further processing
  prefs: []
  type: TYPE_NORMAL
- en: This will trigger the Lambda function. You can monitor your logs by going to
    `CloudWatch> CloudWatch Logs> Log` `groups> /aws/lambda/polly-lambda`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the latest stream; the log will look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The logs sample is shown in *Figure 8**.8*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.8 – The logs in the CloudWatch console](img/B21197_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – The logs in the CloudWatch console
  prefs: []
  type: TYPE_NORMAL
- en: 'It will create output in MP3 format, as shown in *Figure 8**.9*. Download and
    listen to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.9 – The output file that was created in the S3 bucket](img/B21197_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – The output file that was created in the S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The most scalable and cost-effective way for your mobile apps or web apps is
    to generate an AWS pre-signed URL for S3 buckets and provide it to your users.
    These S3 Put events asynchronously invoke downstream AI workflows to generate
    results and send a response to the end users. Many users can be served at the
    same time through this approach, and it may increase performance and throughput.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to implement text to speech. In the next section,
    you will learn about Amazon Transcribe, a speech-to-text AI service.
  prefs: []
  type: TYPE_NORMAL
- en: Speech to text with Amazon Transcribe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, you learned about text to speech. In this section,
    you will learn about speech to text and the service that provides it: **Amazon
    Transcribe**. It is an automatic speech recognition service that uses pre-trained
    deep learning models, which means that you do not have to train on petabytes of
    data to produce a model; Amazon does this for us. You just have to use the APIs
    that are available to transcribe audio files or video files; it supports a number
    of different languages and custom vocabulary too. Accuracy is the key, and through
    custom vocabulary, you can enhance it based on the desired domain or industry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Block diagram of Amazon Transcribe’s input and output](img/B21197_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Block diagram of Amazon Transcribe’s input and output
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Transcribe include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Real-time audio streaming and transcription
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transcripting pre-recorded audio files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable text searching from a media file by combining AWS Elasticsearch and Amazon
    Transcribe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing sentiment analysis on recorded audio files for voice helpdesk (contact
    center analytics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Channel identification separation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you will explore the benefits of Amazon Transcribe.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Transcribe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s look at some of the benefits of using Amazon Transcribe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`VocabularyFilterName` and `VocabularyFilterMethod`, which are provided by
    the `StratTranscriptionJob` operation. For example, in financial organizations,
    this can be used to redact a caller’s details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language identification**: It can automatically identify the most used language
    in an audio file and generate transcriptions. If you have several audio files,
    then this service will help you classify them by language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming transcription**: You can send recorded audio files or live audio
    streams to Amazon Transcribe and output a stream of text in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom vocabulary or customized transcription**: You can use your custom
    vocabulary list as per your custom needs to generate accurate transcriptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timestamp generation**: If you want to build or add subtitles to your videos,
    then Amazon Transcribe can return the timestamp for each word or phrase from the
    audio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost effectiveness**: Being a managed service, there is no infrastructure
    cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s get hands-on with Amazon Transcribe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Transcribe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will build a pipeline where you can integrate AWS Lambda
    with Amazon Transcribe to read an audio file stored in a folder in an S3 bucket,
    and then store the output JSON file in another S3 bucket. You will monitor the
    task’s progress in CloudWatch Logs too. You will use the `start_transcription_job`
    asynchronous function to start our job and you will constantly monitor the job
    through `get_transcription_job` until its status becomes `COMPLETED`. Let’s get
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create an IAM role called `transcribe-demo-role` for the Lambda function
    to execute. Ensure that it can read and write from/to S3, use Amazon Transcribe,
    and print the output in CloudWatch logs. Add the following policies to the IAM
    role:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AmazonS3FullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudWatchFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonTranscribeFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, you will create a Lambda function called `transcribe-lambda` with our existing
    IAM role, `transcribe-demo-role`, and save it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please make sure you change the default timeout to a higher value in the `start_transcription_job`
    to start the task and monitor it by using the `get_transcription_job` API.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Paste the code available at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/blob/main/Chapter08/Amazon%20Transcribe%20Demo/lambda_function/lambda_function.py](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/blob/main/Chapter08/Amazon%20Transcribe%20Demo/lambda_function/lambda_function.py)
    and click on **Deploy**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This should give us the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.11 – The Basic settings section of our created lambda function](img/B21197_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – The Basic settings section of our created lambda function
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will be creating an S3 bucket called `transcribe-demo-101` and a folder
    called `input`. Create an event by going to the `audio-event`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`All object` `create events`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`input/`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Lambda Function`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`transcribe-lambda`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the audio file in `.mp4` format to the `input` folder. This will trigger
    the Lambda function. As per the code, the output will be stored in the S3 bucket
    in JSON format, which you can then use to read the contents of the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to `CloudWatch > CloudWatch Logs > Log groups > aws/lambda/transcribe-lambda`.
    Choose the latest stream from the list. It will look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.12 – The logs in a Log Stream for the specified log groups in the
    CloudWatch console](img/B21197_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – The logs in a Log Stream for the specified log groups in the CloudWatch
    console
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is saved to the S3 bucket in JSON format, as per the job name mentioned
    in your code (you can use the S3 `getObject` API to download and read it):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.13 – The output JSON file in an S3 bucket](img/B21197_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – The output JSON file in an S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is a best practice not to overprovision your function’s timeout settings.
    Always understand your code performance and set a function timeout accordingly.
    Overprovisioning a function timeout results in Lambda functions running longer,
    causing unexpected costs. If you are using asynchronous API calls in your Lambda
    function, then it is good to write them into SNS topics on success and trigger
    another Lambda function from that. If it needs human intervention, then it is
    suggested that you use AWS Step Functions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned and applied Amazon Transcribe to convert speech
    into text. In the next section, you will learn about one of the most powerful
    AWS AI services you can use to get the maximum amount of insight from our text
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing natural language processing with Amazon Comprehend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This service helps you extract insights from unstructured text. Unstructured
    text information is growing exponentially. A few data source examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer engagement**: Call center, issue triage, customer surveys, and product
    reviews'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business processes**: Customer/vendor emails, product support messages, and
    operation support feedback'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Records and research**: Whitepapers and medical records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**News and social media**: Social media analytics, brand trends, and correlated
    events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, the question is, what can you do with this data? How can you analyze it
    and extract any value out of it? The answer is Amazon Comprehend, which is used
    to get insights from unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Comprehend include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Information management system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More accurate search system on organized topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis of users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support ticket classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language detection from a document and then translating it into English using
    Amazon Translate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a system to label unstructured clinical data to assist in research
    and analysis purposes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting topics from saved audio files of company meetings or TV news
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you’ll explore the benefits of Amazon Comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Comprehend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the advantages of using Comprehend can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – A block diagram showing Amazon Comprehend’s capabilities](img/B21197_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – A block diagram showing Amazon Comprehend’s capabilities
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at these in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: It detects the language of the text and extracts key phrases. Amazon Comprehend
    can be used for sentiment analysis and topic modeling too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Comprehend Medical can be used to extract medical information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You pay for what you use since this is a fully managed service; you do not have
    to pay for the infrastructure. You do not need to train, develop, and deploy your
    own model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The topic modeling service works by extracting up to 100 topics. A topic is
    a keyword bucket so that you can see what is in the actual corpus of documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is accurate, continuously trained, and easy to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you’ll get hands-on with Amazon Comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Comprehend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will build a pipeline where you can integrate AWS Lambda
    with Amazon Rekognition and Amazon Comprehend. You will then read an image file
    stored in an S3 bucket and detect the language of the text that has been extracted
    from the image. You will also use CloudWatch to print out the output. The following
    is a diagram of our use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Architecture diagram of the required use case](img/B21197_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – Architecture diagram of the required use case
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by creating an IAM role:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the IAM console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Roles** from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create role.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Lambda** as the trusted entity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following managed `policies`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AmazonS3ReadOnlyAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonRekognitionFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ComprehendFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudWatchFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the role as `language-detection-from-image-role`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let’s create the Lambda function. Navigate to `Lambda > Functions >` `Create
    Function`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the function `language-detection-from-image`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the runtime to `Python 3.6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use our existing role; that is, `language-detection-from-image-role`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the code from [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Transcribe%20Demo/lambda_function](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Transcribe%20Demo/lambda_function),
    paste it into the function, and click `detect_text` API from Amazon Rekognition
    to detect text from an image and the `batch_detect_dominant_language` API from
    Amazon Comprehend to detect the language of the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, go to your AWS S3 console and create a bucket called *language-detection-image*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder called `input-image` (in this example, you will only upload
    `.``jpg` files).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to `Properties > Events>` `Add notification`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill in the required fields in the `image-upload-event`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`All object` `create events`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`input-image/`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.jpg`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Lambda Function`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`language-detection-from-image`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to `Amazon S3>language-detection-image>input-image`. Upload the `sign-image.jpg`
    image in the folder. (This file is available in this book’s GitHub repository
    at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Comprehend%20Demo/input_image](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Comprehend%20Demo/input_image)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This file upload will trigger the Lambda function. You can monitor the logs
    from `CloudWatch> CloudWatch Logs> Log` `groups> /aws/lambda/language-detection-from-image`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the streams and select the latest one. The detected language is printed
    in the logs, as shown in *Figure 8**.16*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.16 – The logs in CloudWatch for verifying the output](img/B21197_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – The logs in CloudWatch for verifying the output
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'It is suggested that you use batch operations such as `BatchDetectSentiment`or
    `BatchDetectDominantLanguage` in your production environment. This is because
    single API operations can cause API-level throttling. More details are available
    here: [https://docs.aws.amazon.com/comprehend/latest/dg/functionality.html](https://docs.aws.amazon.com/comprehend/latest/dg/functionality.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to use Amazon Comprehend to detect the language
    of texts. The text is extracted into our Lambda function using Amazon Rekognition.
    In the next section, you will learn about translating the same text into English
    via Amazon Translate.
  prefs: []
  type: TYPE_NORMAL
- en: Translating documents with Amazon Translate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time, people prefer to communicate in their own language, even on
    digital platforms. Amazon Translate is a text translation service. You can provide
    documents or strings of text in various languages and get it back in a different
    language. It uses pre-trained deep learning techniques, so you should not be worried
    about the models, nor how they are managed. You can make API requests and get
    the results back.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Translate include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: If there is an organization-wide requirement to prepare documents in different
    languages, then Translate is the solution for converting one language into many.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online chat applications can be translated in real time to provide a better
    customer experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To localize website content faster and more affordably into more languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis can be applied to different languages once they have been
    translated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To provide non-English language support for a news publishing website.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you will explore the benefits of Amazon Translate.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Translate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the benefits of using Amazon Translate include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses neural machine translation, which mimics the way the human brain works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You do not need to maintain resources or infrastructures for the Translation
    action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Produces high-quality results and maintains their consistency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can customize brand names and model names. Other unique terms too can get
    translated using the custom terminology feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be easily integrated with applications through APIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Translate scales itself when you need it to do more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you will get hands-on with Amazon Translate.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Translate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will build a product by integrating AWS Lambda with Amazon
    Rekognition, Amazon Comprehend, and Amazon Translate to read an image file stored
    in an S3 bucket. Then, you will detect the language of the text that has been
    extracted from the image so that you can translate it into English. You will also
    use CloudWatch to print the translated output. The following is a diagram of our
    use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Architecture diagram of the required use case](img/B21197_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Architecture diagram of the required use case
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating an IAM role:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the IAM console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Roles** from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Lambda** as the trusted entity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following managed policies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AmazonS3ReadOnlyAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonRekognitionFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ComprehendFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudWatchFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TranslateFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the role as `language-translation-from-image`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next immediate step is to create a Lambda function. Navigate to `Lambda
    > Functions >` `Create Function`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the function `language-detection-from-image`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the runtime to `Python 3.6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use an existing role; that is, `language-detection-from-image-role`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Paste the code available at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/blob/main/Chapter08/Amazon%20Translate%20Demo/lambda_function/lambda_function.py](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/blob/main/Chapter08/Amazon%20Translate%20Demo/lambda_function/lambda_function.py)
    and click `translate_text` API to translate the input text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to create a bucket called `language-translation-from-image`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder named `image`. Then, navigate to `Properties > Events>` `Add
    notification`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fill in the required fields, as shown here, and click on Save (please make
    sure you select `.jpg` as the suffix; otherwise, it will trigger the Lambda function
    for any object creation process):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`All object` `create events`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image/`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.jpg`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Lambda Function`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`language-translation-from-image`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Navigate to `Amazon S3 > language-detection-image > input-image`. Upload the
    `sign-image.jpg` image into the folder. This file is available in this book’s
    GitHub repository: [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Translate%20Demo/input_image](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Translate%20Demo/input_image).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uploading this image will trigger the Lambda function. You can monitor the logs
    by going to `CloudWatch > CloudWatch Logs > Log groups > /``aws/lambda/language-translation-from-image`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the streams and select the latest one. It will look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.18 – The logs in CloudWatch for verifying the output](img/B21197_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – The logs in CloudWatch for verifying the output
  prefs: []
  type: TYPE_NORMAL
- en: 'The translation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For production use cases, it is recommended to use AWS Lambda with AWS Step
    Functions if you have dependent services or a chain of services.
  prefs: []
  type: TYPE_NORMAL
- en: Using the same S3 bucket to store input and output objects is not recommended.
    Output object creation in the same bucket may trigger recursive Lambda invocation.
    If you are using the same bucket, then you recommend that you use a prefix and
    suffix to trigger events. Similarly, you recommend using a prefix to store output
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to combine multiple services and chain their
    output to achieve a particular use case outcome. You learned how to integrate
    Amazon Rekognition to detect text in an image. The language can then be detected
    by using Amazon Comprehend. Then, you used the same input and translated it into
    English with the help of Amazon Translate. The translated output was then printed
    on CloudWatch logs for verification. In the next section, you will learn about
    Amazon Textract, which can be used to extract text from a document.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting text from documents with Amazon Textract
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Manually extracting information from documents is slow, expensive, and prone
    to errors. Traditional optical character recognition software needs a lot of customization,
    and it will still give erroneous output. To avoid such manual processes and errors,
    you should use **Amazon Textract**. Generally, you convert the documents into
    images to detect bounding boxes around the texts in images. You then apply character
    recognition to read the text from it. Textract does all this for you, and also
    extracts text, tables, forms, and other data for you with minimal effort. If you
    get low-confidence results from Amazon Textract, then Amazon A2I is the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Textract reduces the manual effort of extracting text from millions of scanned
    document pages. Once the information has been captured, actions can be taken on
    the text, such as storing it in different data stores, analyzing sentiments, or
    searching for keywords. The following diagram shows how Amazon Textract works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Block diagram representation of Amazon Textract and how it
    stores its output](img/B21197_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – Block diagram representation of Amazon Textract and how it stores
    its output
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Textract include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Documenting processing workflows to extract tables or forms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating search indexes from documents using Amazon Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redacting personally identifiable information in a workflow; Textract identifies
    data types and form labels automatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you will explore the benefits of Amazon Textract.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Textract
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several reasons to use Textract:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero infrastructure cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully managed service (reduced development and management overhead)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helps you extract both structured and unstructured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwritten reviews can be analyzed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Textract performs better than OCR apps, which use a flat bag of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you will get hands-on with Amazon Textract.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Textract
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will use the Amazon Textract API to read an image file
    from our S3 bucket and print the FORM details on Cloudwatch. The same can be stored
    in S3 in your desired format for further use or can be stored in DynamoDB as a
    key-value pair. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create an IAM role called `textract-use-case-role` with the following
    policies. This will allow the Lambda function to execute so that it can read from
    S3, use Amazon Textract, and print the output in CloudWatch logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`CloudWatchFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonTextractFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonS3ReadOnlyAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s create an S3 bucket called `textract-document-analysis` and upload the
    `receipt.png` image file. This will be used to contain the FORM details that will
    be extracted. The image file is available at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Textract%20Demo/input_doc](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Textract%20Demo/input_doc):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.20 – An S3 bucket with an image (.png) file uploaded to the input
    folder](img/B21197_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – An S3 bucket with an image (.png) file uploaded to the input folder
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create a Lambda function called `read-scanned-doc`, as
    shown in *Figure 8**.21*, with an existing execution role called `textract-use-case-role`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.21 – The AWS Lambda Create function dialog](img/B21197_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 – The AWS Lambda Create function dialog
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the function has been created, paste the following code and deploy it.
    Scroll down to `analyze_document` API from Amazon Textract to get the `Table and
    Form` details via the `FeatureTypes` parameter of the API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Unlike the previous examples, you will create a test configuration to run our
    code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the dropdown left of the **Test** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Configure test events** and choose **Create new** **test event**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Amazon S3 Put** from the **Event** **template** dropdown.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the JSON body, change the highlighted values as per our bucket name and
    key, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.22 – The Event template for testing the Lambda function](img/B21197_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.22 – The Event template for testing the Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: In the `TextractDemo`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select your test configuration (`TextractDemo`) and click on **Test**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.23 – Selecting the test configuration before running your test](img/B21197_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.23 – Selecting the test configuration before running your test
  prefs: []
  type: TYPE_NORMAL
- en: This will trigger the Lambda function. You can monitor the logs from `CloudWatch
    > CloudWatch Logs > Log groups > /``aws/lambda/ read-scanned-doc`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the streams and select the latest one. It will look as follows; the
    key-value pairs can be seen in *Figure 8**.24*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.24 – The logs in CloudWatch for verifying the output](img/B21197_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.24 – The logs in CloudWatch for verifying the output
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The most scalable and cost-effective way to generate S3 PJUT events for asynchronously
    invocating downstream AI workflows via Lambda is to generate an AWS pre-signed
    URL, and then provide it to your mobile or web application users. Many users can
    be served at the same time via this approach, and it may increase performance
    and throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the same region for your AWS AI services and S3 bucket may improve
    performance and reduce network latency. AWS VPC endpoints can leverage enhanced
    security without using the public internet. You can store the AWS AI results in
    an AWS S3 bucket and encrypt the rest to attain better security.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to extract text from a scanned document and
    print the form data out of it. Unlike the other sections, you used the testing
    feature of a Lambda function by creating a test configuration that includes an
    event template. In the next section, you will learn about creating a chatbot for
    organizations and learn how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating chatbots on Amazon Lex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the features that are available in Alexa are powered by **Amazon Lex**.
    You can easily build a chatbot using Amazon Lex. It uses natural language understanding
    and automatic speech recognition behind the scenes. An Amazon Lex bot can be created
    either from the console or via APIs. Its basic requirements are shown in the upcoming
    diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Lex include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Apps that both listen and take input as text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatbots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversational AI products to provide a better customer and sales experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom business bots for assistance through AWS Lambda functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voice assistants for your call center, which can speak to a user, schedule a
    meeting, or request the details of your account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By integrating with Amazon Cognito, a few aspects such as user management, authentication,
    and sync across all your devices can be controlled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you will explore the benefits of Amazon Lex.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Lex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some reasons for using Lex include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots can be directly built and tested from the AWS Management Console. These
    chatbots can be easily integrated into Facebook Messenger, Slack, and Twilio SMS
    via its rich formatting capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversation logs can be stored in Amazon CloudWatch for further analysis. You
    can use them to monitor your bot and derive insights to improve your user experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Lex can be integrated into other AWS services such as Amazon Cognito,
    AWS Lambda, Amazon DynamoDB, Amazon CloudWatch, and AWS Mobile Hub to leverage
    application security, monitoring, user authentication, business logic, storage,
    and mobile app development in AWS platforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Lex chatbots can be integrated into your custom web applications too.
    You just need to build a chatbot widget and integrate it into your UI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you’ll get hands-on with Amazon Lex.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Lex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to [https://console.aws.amazon.com/lex/](https://console.aws.amazon.com/lex/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Get Started** and select **Custom bot**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fill in the following details and click on **Create**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.25 – The Create dialog of Amazon Lex](img/B21197_08_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.25 – The Create dialog of Amazon Lex
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Create Intent**. A dialog will appear. Select **Create Intent**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the new intent `MovieIntent` and click on **Add**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the `movie_type`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Slot type: `AMAZON.Genre`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prompt: `Which movie do` `you like?`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the `movie_type` is my variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.26 – The Sample utterances section](img/B21197_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.26 – The Sample utterances section
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll down to the **Response** section to add a message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.27 – The Response section of Amazon Lex](img/B21197_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.27 – The Response section of Amazon Lex
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll down to **Save Intent** and click on **Build**. Upon successfully building
    the prompt, the following success message will appear:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.28 – The Response section of Amazon Lex](img/B21197_08_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.28 – The Response section of Amazon Lex
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you can test your bot, as shown in *Figure 8**.29*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.29 – The Test bot dialog](img/B21197_08_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.29 – The Test bot dialog
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn about Amazon Forecast and learn how to use
    it for different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Forecast
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Forecast is a powerful service that enables you to build highly accurate
    time-series forecasting models without the need for deep expertise in machine
    learning. Whether you are predicting sales, demand for inventory, or any time-dependent
    metric, Amazon Forecast simplifies the process, making it accessible to a broader
    audience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon Forecast is designed to tackle a variety of forecasting challenges,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Demand forecasting**: Predict future demand for products or services based
    on historical data, helping optimize inventory and supply chain management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Financial planning**: Forecast financial metrics, such as revenue and expenses,
    aiding in budgeting and financial decision-making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource planning**: Efficiently plan resources like workforce scheduling
    based on predicted demand patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traffic and user engagement**: Predict website or application traffic, enhancing
    resource allocation and user experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you will explore the benefits of Amazon Lex.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Forecast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some reasons for using Amazon Forecast are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ease of use**: Amazon Forecast abstracts the complexity of building accurate
    forecasting models. With just a few clicks, you can create, train, and deploy
    models without deep machine learning expertise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated machine learning**: Amazon Forecast employs advanced machine learning
    techniques, automating the selection of algorithms and hyperparameter tuning to
    deliver the best possible model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forecast backtesting**: Enhance the reliability of your forecasts through
    backtesting. Amazon Forecast enables you to assess the accuracy of your models
    by comparing predictions against historical data. This iterative process helps
    fine-tune your models, adjusting hyperparameters and algorithms to achieve optimal
    forecasting performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Amazon Forecast seamlessly scales with your data, ensuring
    accurate predictions even with vast datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with AWS**: Leverage the power of integration with other AWS
    services like Amazon S3, AWS Lambda, and Amazon CloudWatch to create end-to-end
    forecasting solutions. Easily integrate Amazon Forecast into your existing applications
    and workflows, ensuring a seamless forecasting experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy and precision**: Amazon Forecast utilizes cutting-edge forecasting
    algorithms to deliver accurate and precise predictions, minimizing errors in your
    forecasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-effective**: Pay only for what you use. The pay-as-you-go pricing model
    ensures cost-effectiveness, especially for businesses with varying forecasting
    needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization**: Tailor forecasting models to your specific business needs,
    accommodating various forecasting scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you’ll get hands-on with Amazon Lex.
  prefs: []
  type: TYPE_NORMAL
- en: Sales Forecasting Model with Amazon Forecast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s dive into a hands-on example of using Amazon Forecast to build a sales
    forecasting model. In this example, you’ll predict future sales based on historical
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Set up your dataset**: Prepare a dataset containing historical sales data,
    ensuring it includes relevant timestamps and corresponding sales figures.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a dataset group**: Use the Amazon Forecast console or API to create
    a dataset group, grouping related datasets for forecasting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Import your data**: Upload your historical sales dataset to Amazon Forecast,
    allowing the service to learn patterns from the provided data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train your model**: Initiate model training using the Forecast console or
    API. Amazon Forecast will automatically select suitable algorithms and optimize
    hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate forecasts**: Once the model is trained, generate forecasts for future
    sales based on the patterns identified in your historical data.'
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging advanced features and implementing optimization strategies, you
    can elevate your Amazon Forecast experience. The flexibility and adaptability
    of the service allow you to tailor forecasting solutions to the specific needs
    of your business. For example, you can improve the precision of your forecasts
    by integrating external variables. Amazon Forecast allows you to include additional
    information, such as promotions, holidays, or economic indicators, that might
    impact the time series you are forecasting. By considering these external factors,
    your models can adapt to changing circumstances and provide more nuanced predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about a few of the AWS AI services that can be
    used to solve various problems. You used the Amazon Rekognition service, which
    detects objects and faces (including celebrity faces), and can also extract text
    from images. For text to speech, you used Amazon Polly, while for speech to text,
    you used Amazon Transcribe. Toward the end of this chapter, you built a chatbot
    in Amazon Lex and learned the usage and benefits of Amazon Forecast.
  prefs: []
  type: TYPE_NORMAL
- en: For language detection and translation in an image, you used Amazon Rekognition,
    Amazon Comprehend, and Amazon Translate. You learned how to combine all of them
    into one Lambda function to solve our problem.
  prefs: []
  type: TYPE_NORMAL
- en: For the certification exam, you do not need to remember all the APIs you used
    in this chapter. There may be questions on a few of the best practices that you
    learned or on the names of services that solve a specific problem. It is always
    good to practice using these AWS AI services as it will enhance your architecting
    skills.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about data preparation and transformation,
    which is the most important aspect of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH08](https://packt.link/MLSC01E2_CH08).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 8**.30*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.30 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_08_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.30 – QR code that opens Chapter Review Questions for logged-in users
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 8**.31*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.31 – Chapter Review Questions for Chapter 8](img/B21197_08_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.31 – Chapter Review Questions for Chapter 8
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – Sample timing practice drills on the online platform
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  prefs: []
  type: TYPE_NORMAL
