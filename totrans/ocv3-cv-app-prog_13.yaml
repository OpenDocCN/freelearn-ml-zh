- en: Chapter 13. Tracking Visual Motion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Tracing feature points in a video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the optical flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking an object in a video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Video sequences are interesting because they show scenes and objects in motion.
    The preceding chapter introduced the tools for reading, processing, and saving
    videos. In this chapter, we will look at different algorithms that track the visible
    motion in a sequence of images. This visible or **apparent motion** can be caused
    by objects that move in different directions and at various speeds or by the motion
    of the camera (or a combination of both).
  prefs: []
  type: TYPE_NORMAL
- en: Tracking apparent motion is of utmost importance in many applications. It allows
    you to follow specific objects while they are moving in order to estimate their
    speed and determine where they are going. It also permits you to stabilize videos
    taken from handheld cameras by removing or reducing the amplitude of camera jitters.
    Motion estimation is also used in video coding to compress a video sequence in
    order to facilitate its transmission or storage. This chapter will present a few
    algorithms that track the motion in an image sequence, and as we will see, this
    tracking can be achieved either sparsely (that is, at few image locations, this
    is **sparse motion**) or densely (at every pixel of an image, this is **dense
    motion**).
  prefs: []
  type: TYPE_NORMAL
- en: Tracing feature points in a video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned in previous chapters that analyzing an image through some of its
    most distinctive points can lead to effective and efficient computer vision algorithms.
    This is also true for image sequences in which the motion of some interest points
    can be used to understand how the different elements of a captured scene move.
    In this recipe, you will learn how to perform a temporal analysis of a sequence
    by tracking feature points as they move from frame to frame.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start the tracking process, the first thing to do is to detect the feature
    points in an initial frame. You then try to track these points in the next frame.
    Obviously, since we are dealing with a video sequence, there is a good chance
    that the object, on which the feature points are found, has moved (this motion
    can also be due to camera movement). Therefore, you must search around a point's
    previous location in order to find its new location in the next frame. This is
    what accomplishes the `cv::calcOpticalFlowPyrLK` function. You input two consecutive
    frames and a vector of feature points in the first image; the function then returns
    a vector of new point locations. To track the points over a complete sequence,
    you repeat this process from frame to frame. Note that as you follow the points
    across the sequence, you will unavoidably lose track of some of them such that
    the number of tracked feature points will gradually reduce. Therefore, it could
    be a good idea to detect new features from time to time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now take advantage of the video-processing framework we defined in
    [Chapter 12](ch12.html "Chapter 12. Processing Video Sequences") , *Processing
    Video Sequences*, and we will define a class that implements the `FrameProcessor`
    interface introduced in the *Processing the video frames* recipe of this chapter.
    The data attributes of this class include the variables that are required to perform
    both the detection of feature points and their tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the `process` method that will be called for each frame of
    the sequence. Basically, we need to proceed as follows. First, the feature points
    are detected if necessary. Next, these points are tracked. You reject the points
    that you cannot track or you no longer want to track. You are now ready to handle
    the successfully tracked points. Finally, the current frame and its points become
    the previous frame and points for the next iteration. Here is how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This method makes use of four utility methods. It should be easy for you to
    change any of these methods in order to define a new behavior for your own tracker.
    The first of these methods detects the feature points. Note that we have already
    discussed the `cv::goodFeatureToTrack` function in the first recipe of [Chapter
    8](ch08.html "Chapter 8. Detecting Interest Points") , *Detecting Interest Points*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The second method determines whether new feature points should be detected.
    This will happen when a negligible number of tracked points remain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The third method rejects some of the tracked points based on a criteria defined
    by the application. Here, we decided to reject the points that do not move (in
    addition to those that cannot be tracked by the `cv::calcOpticalFlowPyrLK` function).
    We consider that non-moving points belong to the background scene and are therefore
    uninteresting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the fourth method handles the tracked feature points by drawing all
    the tracked points with a line that joins them to their initial position (that
    is, the position where they were detected the first time) on the current frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple main function to track the feature points in a video sequence would
    then be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting program will show you the evolution of the moving tracked features
    over time. Here are, for example, two such frames at two different instants. In
    this video, the camera is fixed. The young cyclist is therefore the only moving
    object. Here is the result that is obtained after a few frames have been processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_13_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A few seconds later, we obtain the following frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_13_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To track the feature points from frame to frame, we must locate the new position
    of a feature point in the subsequent frame. If we assume that the intensity of
    the feature point does not change from one frame to the next one, we are looking
    for a displacement `(u,v)` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/B05388_13_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, `I[t]` and `I[t+1]` are the current frame and the one at the next instant,
    respectively. This constant intensity assumption generally holds for small displacement
    in images that are taken at two nearby instants. We can then use the Taylor expansion
    in order to approximate this equation by an equation that involves the image derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/B05388_13_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This latter equation leads us to another equation (as a consequence of the
    constant intensity assumption that cancels the two intensity terms):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/B05388_13_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This constraint is the fundamental **optical flow** constraint equation and
    is known as the **brightness constancy equation**.
  prefs: []
  type: TYPE_NORMAL
- en: This constraint is exploited by the so-called **Lukas-Kanade feature tracking**
    algorithm. In addition to using this constraint, the Lukas-Kanade algorithm also
    makes an assumption that the displacement of all the points in the neighborhood
    of the feature point is the same. We can therefore impose the optical flow constraint
    on all these points with a unique `(u,v)` unknown displacement. This gives us
    more equations than the number of unknowns (two), and therefore, we can solve
    this system of equations in a mean-square sense. In practice, it is solved iteratively,
    and the OpenCV implementation also offers us the possibility to perform this estimation
    at a different resolution in order to make the search more efficient and more
    tolerant to a larger displacement. By default, the number of image levels is `3`
    and the window size is `15`. These parameters can obviously be changed. You can
    also specify the termination criteria, which define the conditions that stop the
    iterative search. The sixth parameter of `cv::calcOpticalFlowPyrLK` contains the
    residual mean-square error that can be used to assess the quality of the tracking.
    The fifth parameter contains binary flags that tell us whether tracking the corresponding
    point was considered successful or not.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding description represents the basic principles behind the Lukas-Kanade
    tracker. The current implementation contains other optimizations and improvements
    that make the algorithm more efficient in the computation of the displacement
    of a large number of feature points.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Chapter 8](ch08.html "Chapter 8. Detecting Interest Points") , *Detecting
    Interest Points*, where there is a discussion on feature point detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Tracking an object in a video* recipe of this chapter uses feature point
    tracking in order to track objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classic article by *B. Lucas* and *T. Kanade*, *An Iterative Image Registration
    Technique with an Application to Stereo Vision*, at the *Int. Joint Conference
    in **Artificial Intelligence*, pp. 674-679, 1981, describes the original feature
    point tracking algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article by J. Shi and C. Tomasi, *Good Features to Track*, at the *IEEE
    Conference on Computer Vision and Pattern Recognition*, pp. 593-600, 1994, describes
    an improved version of the original feature point tracking algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the optical flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a scene is observed by a camera, the observed brightness pattern is projected
    on the image sensor and thus forms an image. In a video sequence, we are often
    interested in capturing the motion pattern, that is the projection of the 3D motion
    of the different scene elements on an image plane. This image of projected 3D
    motion vectors is called the **motion field**. However, it is not possible to
    directly measure the 3D motion of scene points from a camera sensor. All we observe
    is a brightness pattern that is in motion from frame to frame. This apparent motion
    of the brightness pattern is called the **optical flow**. One might think that
    the motion field and optical flow should be equal, but this is not always true.
    An obvious case would be the observation of a uniform object; for example, if
    a camera moves in front of a white wall, then no optical flow is generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another classical example is the illusion produced by a rotating barber pole:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating the optical flow](img/image_13_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the motion field should show motion vectors in the horizontal
    direction as the vertical cylinder rotates around its main axis. However, observers
    perceive this motion as red and blue strips moving up and this is what the optical
    flow will show. In spite of these differences, the optical flow is considered
    to be a valid approximation of the motion field. This recipe will explain how
    the optical flow of an image sequence can be estimated.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Estimating the optical flow means quantifying the apparent motion of the brightness
    pattern in an image sequence. So let's consider one frame of the video at one
    given instant. If we look at one particular pixel `(x,y)` on the current frame,
    we would like to know where this point is moving in the subsequent frames. That
    is to say that the coordinates of this point are moving over time-a fact that
    can be expressed as `(x(t),y(t))`-and our goal is to estimate the velocity of
    this point `(dx/dt,dy/dt)`. The brightness of this particular point at time `t`
    can be obtained by looking at the corresponding frame of the sequence, that is,
    `I(x(t),y(t),t)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'From our **image brightness constancy** assumption, we can write that the brightness
    of this point does not vary with respect to time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B05388_13_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The chain rule allows us to write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B05388_13_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This equation is known as the **brightness constancy equation** and it relates
    the optical flow components (the derivatives of `x` and `y` with respect to time)
    with the image derivatives. This is exactly the equation we derived in the previous
    recipe; we simply demonstrated it differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'This single equation (composed of two unknowns) is however insufficient to
    compute the optical flow at a pixel location. We therefore need to add an additional
    constraint. A common choice is to assume the smoothness of the optical flow, which
    means that the neighboring optical flow vectors should be similar. Any departure
    from this assumption should therefore be penalized. One particular formulation
    for this constraint is based on the Laplacian of the optical flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/equation.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The objective is therefore to find the optical flow field that minimizes both
    the deviations from the brightness constancy equation and the Laplacian of the
    flow vectors.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Several approaches have been proposed to solve the dense optical flow estimation
    problem, and OpenCV implements a few of them. Let''s use the `cv::DualTVL1OpticalFlow`
    class that is built as a subclass of the generic `cv::Algorithm` base class. Following
    the implemented pattern, the first thing to do is to create an instance of this
    class and obtain a pointer to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the object we just created is in a ready-to-use state, we simply call
    the method that calculates an optical flow field between the two frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is an image of 2D vectors (`cv::Point`) that represents the displacement
    of each pixel between the two frames. In order to display the result, we must
    therefore show these vectors. This is why we created a function that generates
    an image map for an optical flow field. To control the visibility of the vectors,
    we used two parameters. The first one is a stride value that is defined such that
    only one vector over a certain number of pixels will be displayed. This stride
    makes space for the display of the vectors. The second parameter is a scale factor
    that extends the vector length to make it more apparent. Each drawn optical flow
    vector is then a simple line that ends with a plain circle to symbolize the tip
    of an arrow. Our mapping function is therefore as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider the following two frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_13_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If these frames are used, then the estimated optical flow field can be visualized
    by calling our drawing function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_13_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We explained in the first section of this recipe that an optical flow field
    can be estimated by minimizing a function that combines the brightness constancy
    constraint and a smoothness function. The equations we presented then constitute
    the classical formulation of the problem, and this one has been improved in many
    ways.
  prefs: []
  type: TYPE_NORMAL
- en: The method we used in the previous section is known as the **Dual TV L1** method.
    It has two main ingredients. The first one is the use of a smoothing constraint
    that aims at minimizing the absolute value of the optical flow gradient (instead
    of the square of it). This choice reduces the impact of the smoothing term, especially
    at regions of discontinuity where, for example, the optical flow vectors of a
    moving object are quite different from the ones of its background. The second
    ingredient is the use of a **first-order Taylor approximation**; this linearizes
    the formulation of the brightness constancy constraint. We will not enter into
    the details of this formulation here; it is suffice to say that this linearization
    facilitates the iterative estimation of the optical flow field. However, since
    the linear approximation is only valid for small displacements, the method requires
    a coarse-to-fine estimation scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we used this method with its default parameters. A number of
    setters and getters methods allow you to modify the ones which can have an impact
    on the quality of the solution and on the speed of the computation. For example,
    one can modify the number of scales used in the pyramidal estimation or specify
    a more or less strict stopping criterion to be adopted during each iterative estimation
    step. Another important parameter is the weight associated with the brightness
    constancy constraint versus the smoothness constraint. For example, if we reduce
    the importance given to brightness constancy by two, we then obtain a smoother
    optical flow field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![How it works...](img/image_13_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The article by *B.K.P. Horn* and *B.G. Schunck*, *Determining optical flow,
    in Artificial Intelligence*, 1981, is the classical reference on optical flow
    estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article by *C. Zach*, *T. Pock*, and *H. Bischof*, *A duality based approach
    for real time tv-l 1 optical flow*, at *IEEE conference on Computer Vision and
    Pattern Recognition* 2007, describes the details of the `Dual TV-L1` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking an object in a video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two recipes, we learned how to track the motion of points and
    pixels in an image sequence. In many applications, however, the requirement is
    rather to track a specific moving object in a video. An object of interest is
    first identified and then it must be followed over a long sequence. This is challenging
    because as it evolves in the scene, the image of this object will undergo many
    changes in appearance due to viewpoint and illumination variations, non-rigid
    motion, occlusion, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe presents some of the object-tracking algorithms implemented in the
    OpenCV library. These implementations are based on a common framework, which facilitates
    the substitution of one method by another. Contributors have also made available
    a number of new methods. Note that, we have already presented a solution to the
    object-tracking problem in the *Counting pixels with integral images* recipe in
    [Chapter 4](ch04.html "Chapter 4. Counting the Pixels with Histograms") , *Counting
    the Pixels with Histograms*; this one was based on the use of histograms computed
    through integral images.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The visual object-tracking problem generally assumes that no prior knowledge
    about the objects to be tracked is available. Tracking is therefore initiated
    by identifying the object in a frame, and tracking must start at this point. The
    initial identification of the object is achieved by specifying a bounding box
    inside which the target is inscribed. The objective of the tracker module is then
    to reidentify this object in a subsequent frame.
  prefs: []
  type: TYPE_NORMAL
- en: The `cv::Tracker` class of OpenCV that defines the object-tracking framework
    has therefore, two main methods. The first one is the `init` method used to define
    the initial target bounding box. The second one is the `update` method that outputs
    a new bounding box, given a new frame. Both the methods accept a frame (a `cv::Mat`
    instance) and a bounding box (a `cv::Rect2D` instance) as arguments; in one case,
    the bounding box is an input, while for the second method, the bounding box is
    an output parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to test one of the proposed object tracker algorithms, we use the
    video-processing framework that has been presented in the previous chapter. In
    particular, we define a frame-processing subclass that will be called by our `VideoProcessor`
    class when each frame of the image sequence is received. This subclass has the
    following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reset` attribute is set to `true` whenever the tracker has been reinitiated
    through the specification of a new target''s bounding box. It is the `setBoundingBox`
    method that is used to store a new object position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The callback method used to process each frame then simply calls the appropriate
    method of the tracker and draws the new computed bounding box on the frame to
    be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate how an object can be tracked using the `VideoProcessor` and
    `FrameProcessor` instances, we use the **Median Flow tracker** defined in OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The first bounding box identifies one goose in our test image sequence. This
    one is then automatically tracked in the subsequent frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_13_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Unfortunately, as the sequence progresses, the tracker will unavoidably make
    errors. The accumulation of these small errors will cause the tracker to slowly
    drift from the real target position. Here is, for example, the estimated position
    of our target after `130` frames have been processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_13_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Eventually, the tracker will lose track of the object. The ability of a tracker
    to follow an object over a long period of time is the most important criteria
    that characterizes the performance of an object tracker.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we showed how the generic `cv::Tracker` class can be used to
    track an object in an image sequence. We selected the Median Flow tracker algorithm
    to illustrate the tracking result. This is a simple but effective method to track
    a textured object as long as its motion is not too rapid and it is not too severely
    occluded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Median Flow tracker is based on feature point tracking. It first starts
    by defining a grid of points over the object to be tracked. One could have instead
    detected interest points on the object using, for instance, the `FAST` operator
    presented in [Chapter 8](ch08.html "Chapter 8. Detecting Interest Points") , *Detecting
    Interest Points*. However, using points at predefined locations presents a number
    of advantages. It saves time by avoiding the computation of interest points. It
    guarantees that a sufficient number of points will be available for tracking.
    It also makes sure that these points will be well distributed over the whole object.
    The Median Flow implementation uses, by default, a grid of `10x10` points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_13_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next step is to use the Lukas-Kanade feature-tracking algorithm presented
    in the first recipe of this chapter, *Tracing feature points in a video*. Each
    point of the grid is then tracked over the next frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_13_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Median Flow algorithm then estimates the errors made when tracking these
    points. These errors can be estimated, for example, by computing the sum of absolute
    pixel difference in a window around the point at its initial and tracked position.
    This is the type of error that is conveniently computed and returned by the `cv::calcOpticalFlowPyrLK`
    function. Another error measure proposed by the Median Flow algorithm is to use
    the so-called forward-backward error. After the points have been tracked between
    a frame and the next one, these points at their new position are backward-tracked
    to check whether they will return to their original position in the initial image.
    The difference between the thus obtained forward-backward position and the initial
    one is the error in tracking.
  prefs: []
  type: TYPE_NORMAL
- en: Once the tracking error of each point has been computed, only 50 percent of
    the points having the smallest error are considered. This group is used to compute
    the new position of the bounding box in the next image. Each of these points votes
    for a displacement value, and the median of these possible displacements is retained.
    For the change in scale, the points are considered in pairs. The ratio of the
    distance between the two points in the initial frame and the next one is estimated.
    Again, it is the median of these scales that is finally applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Median Tracker is one of many other visual object trackers based on feature
    point tracking. Another family of solutions is the one that is based on template
    matching, a concept we discussed in the *Matching local templates* recipe in [Chapter
    9](ch09.html "Chapter 9. Describing and Matching Interest Points") , *Describing
    and Matching Interest Points*. A good representative of these kinds of approaches
    is the **Kernelized Correlation Filter** (**KCF**) algorithm, implemented as the
    `cv::TrackerKCF` class in OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Basically, this one uses the target''s bounding box as a template to search
    for the new object position in the next view. This is normally computed through
    a simple correlation, but KCF uses a special trick based on the Fourier transform
    that we briefly mentioned in the introduction of [Chapter 6](ch06.html "Chapter 6. Filtering
    the Images") , *Filtering the Images*. Without entering into any details, the
    signal-processing theory tells us that correlating a template over an image corresponds
    to simple image multiplication in the frequency domain. This considerably speeds
    up the identification of the matching window in the next frame and makes KCF one
    of the fastest and robust trackers. As an example, here is the position of the
    bounding box after a tracking of `130` frames using KCF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_13_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The article by *Z. Kalal*, *K. Mikolajczyk*, and *J. Matas*, *Forward-backward
    error: Automatic detection of tracking failures*, in *Int. Conf. on Pattern Recognition*,
    2010, describes the Median Flow algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article by *Z. Kalal*, *K. Mikolajczyk*, and *J. Matas*, *Tracking-learning-detection*,
    in *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol 34, no
    7, 2012, is an advanced tracking method that uses the Median Flow algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article by *J.F. Henriques*, *R. Caseiro*, *P. Martins*, *J. Batist*a, *High-Speed
    Tracking with Kernelized Correlation Filters*, in *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol 37, no 3, 2014, describes the KCF tracker
    algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
