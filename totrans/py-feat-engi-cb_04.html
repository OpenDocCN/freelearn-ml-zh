<html><head></head><body>
		<div id="_idContainer080">
			<h1 id="_idParaDest-119" class="chapter-number"><a id="_idTextAnchor516"/><st c="0">4</st></h1>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor517"/><a id="_idTextAnchor518"/><st c="2">Performing Variable Discretization</st></h1>
			<p><st c="36">Discretization is the </st><a id="_idTextAnchor519"/><st c="59">process of transforming continuous variables into discrete features by creating a set of contiguous intervals, also </st><a id="_idIndexMarker276"/><st c="175">called </st><strong class="bold"><st c="182">bins</st></strong><st c="186">, which span the range of the variable values. </st><st c="233">Subsequently, these intervals are treated as </st><span class="No-Break"><st c="278">categorical data.</st></span></p>
			<p><st c="295">Many machine learning models, such as decision trees and Naïve Bayes, work better with discrete attributes. </st><st c="404">In fact, decision tree-based models make decisions based on discrete partitions over the attributes. </st><st c="505">During induction, a decision tree evaluates all possible feature values to find the best cut-point. </st><st c="605">Therefore, the more values the feature has, the longer the induction time of the tree is. </st><st c="695">In this sense, discretization can reduce the time it takes to train </st><span class="No-Break"><st c="763">the models.</st></span></p>
			<p><st c="774">Discretization has additional advantages. </st><st c="817">Data is reduced and simplified; discrete features can be easier to understand by domain experts. </st><st c="914">Discretization can change the distribution of skewed variables; when sorting observations across bins with equal-frequency, the values are spread more homogeneously across the range. </st><st c="1097">Additionally, discretization can minimize the influence of outliers by placing them at lower or higher intervals, together with the remaining </st><strong class="bold"><st c="1239">inlier</st></strong><st c="1245"> values of the distribution. </st><st c="1274">Overall, discretization reduces and simplifies data, making the learning process faster and potentially yielding more </st><span class="No-Break"><st c="1392">accurate results.</st></span></p>
			<p><st c="1409">Discretization can also lead to a loss of information, for example, by combining values that are strongly associated with different classes or target values into the same bin. </st><st c="1586">Therefore, the aim of a discretization algorithm is to find the minimal number of intervals without incurring a significant loss of information. </st><st c="1731">In practice, many discretization procedures require the user to input the number of intervals into which the values will be sorted. </st><st c="1863">Then, the job of the algorithm is to find the cut points for those intervals. </st><st c="1941">Among these procedures, we find the most widely used equal-width and equal-frequency discretization methods. </st><st c="2050">Discretization methods based on decision trees are, otherwise, able to find the optimal number of partitions, as well as the </st><span class="No-Break"><st c="2175">cut points.</st></span></p>
			<p><st c="2186">Discretization procedures can be classified as </st><strong class="bold"><st c="2234">supervised</st></strong><st c="2244"> and </st><strong class="bold"><st c="2249">unsupervised</st></strong><st c="2261">. Unsupervised discretization methods only use the variable’s distribution to determine the limits of the contiguous bins. </st><st c="2384">On the other hand, supervised methods use target information to create </st><span class="No-Break"><st c="2455">the intervals.</st></span></p>
			<p><st c="2469">In this chapter, we will discuss widely used supervised and unsupervised discretization procedures that are available in established open source libraries. </st><st c="2626">Among these, we will cover equal-width, equal-frequency, arbitrary, k-means, and decision tree-based discretization. </st><st c="2743">More elaborate methods, such as ChiMerge and CAIM, are out of the scope of this chapter, as their implementation is not yet open </st><span class="No-Break"><st c="2872">source available.</st></span></p>
			<p><st c="2889">This chapter contains the </st><span class="No-Break"><st c="2916">following recipes:</st></span></p>
			<ul>
				<li><st c="2934">Performing </st><span class="No-Break"><st c="2946">equal-width discretization</st></span></li>
				<li><st c="2972">Implementing </st><span class="No-Break"><st c="2986">equal-frequency discretization</st></span></li>
				<li><st c="3016">Discretizing the variable into </st><span class="No-Break"><st c="3048">arbitrary intervals</st></span></li>
				<li><st c="3067">Performing discretization with </st><span class="No-Break"><st c="3099">k-means clustering</st></span></li>
				<li><st c="3117">Implementing </st><span class="No-Break"><st c="3131">feature binarization</st></span></li>
				<li><st c="3151">Using decision trees </st><span class="No-Break"><st c="3173">for discretization</st></span><a id="_idTextAnchor520"/></li>
			</ul>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor521"/><st c="3191">Technical requirements</st></h1>
			<p><st c="3214">In this chapter, we will use the numerical computing libraries </st><strong class="source-inline"><st c="3278">pandas</st></strong><st c="3284">, </st><strong class="source-inline"><st c="3286">numpy</st></strong><st c="3291">, </st><strong class="source-inline"><st c="3293">matplotlib</st></strong><st c="3303">, </st><strong class="source-inline"><st c="3305">scikit-learn</st></strong><st c="3317">, and</st><a id="_idTextAnchor522"/> <strong class="source-inline"><st c="3322">feature-engine</st></strong><st c="3337">. We will also use the </st><strong class="source-inline"><st c="3360">yellowbrick</st></strong><st c="3371"> Python open source library, which you can install </st><span class="No-Break"><st c="3422">with </st></span><span class="No-Break"><strong class="source-inline"><st c="3427">pip</st></strong></span><span class="No-Break"><st c="3430">:</st></span></p>
			<pre class="console"><st c="3432">
 pip install yellowbrick</st></pre>			<p><st c="3456">For more details about </st><strong class="source-inline"><st c="3480">yellowbrick</st></strong><st c="3491">, visit the </st><span class="No-Break"><st c="3503">documentation here:</st></span></p>
			<p><span class="No-Break"><st c="3522">https://www.scikit-yb.org/en/latest/index.html</st></span><a id="_idTextAnchor523"/></p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor524"/><st c="3569">Performing equal-width discretization</st></h1>
			<p><st c="3607">Equal-width</st><a id="_idTextAnchor525"/><st c="3619"> discretization</st><a id="_idIndexMarker277"/><st c="3634"> consists of dividing the range of</st><a id="_idIndexMarker278"/><st c="3668"> observed values for a variable into </st><em class="italic"><st c="3705">k</st></em><st c="3706"> equally sized intervals, where </st><em class="italic"><st c="3738">k</st></em><st c="3739"> is supplied by the user. </st><st c="3765">The interval width for the </st><em class="italic"><st c="3792">X</st></em><st c="3793"> variable is given by </st><span class="No-Break"><st c="3815">the followin</st><a id="_idTextAnchor526"/><st c="3827">g:</st></span></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><img src="image/20.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;W&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;h&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;M&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;X&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;M&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;k&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.398em;height:1.510em;width:9.768em"/><st c="3830"/></p>
			<p><st c="3860">Then, if the </st><a id="_idIndexMarker279"/><st c="3873">values of the variable vary between 0 and 100, we can create five bins like this: </st><em class="italic"><st c="3955">width = (100-0) / 5 = 20</st></em><st c="3979">. The bins will be 0–20, 20–40, 40–60, and 80–100. </st><st c="4030">The first and final bins (0–20 and 80–100) can be expanded to accommodate values smaller than 0 or greater than 100 by extending the limits to minus and </st><span class="No-Break"><st c="4183">plus infinity.</st></span></p>
			<p><st c="4197">In this recipe, we will carry out equal-width discretization using </st><strong class="source-inline"><st c="4265">pandas</st></strong><st c="4271">, </st><strong class="source-inline"><st c="4273">scikit-learn</st></strong><st c="4285">, </st><span class="No-Break"><st c="4287">and </st></span><span class="No-Break"><strong class="source-inline"><st c="4291">feature-engi</st><a id="_idTextAnchor527"/><a id="_idTextAnchor528"/><st c="4303">ne</st></strong></span><span class="No-Break"><st c="4306">.</st></span></p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor529"/><st c="4307">How to do it...</st></h2>
			<p><st c="4323">First, le</st><a id="_idTextAnchor530"/><st c="4333">t’s import the necessary Python libraries and get the </st><span class="No-Break"><st c="4388">dataset ready:</st></span></p>
			<ol>
				<li><st c="4402">Let’s import the libraries </st><span class="No-Break"><st c="4430">and functions:</st></span><pre class="source-code"><st c="4444">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="4622">Let’s load the predictor and target variables of the California </st><span class="No-Break"><st c="4687">housing dataset:</st></span><pre class="source-code"><st c="4703">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="4768">Note</st></p>
			<p class="callout"><st c="4773">To avoid data leakage, we will find the intervals’ limits by using the variables in the train set. </st><st c="4873">Then, we will use these limits to discretize the variables in train and </st><span class="No-Break"><st c="4945">test sets.</st></span></p>
			<ol>
				<li value="3"><st c="4955">Let’s divide the data into train and </st><span class="No-Break"><st c="4993">test sets:</st></span><pre class="source-code"><st c="5003">
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre><p class="list-inset"><st c="5093">Next, we will divide the continuous </st><strong class="source-inline"><st c="5130">HouseAge</st></strong><st c="5138"> variable into 10 intervals using </st><strong class="source-inline"><st c="5172">pandas</st></strong><st c="5178"> and the formula described at the beginning of </st><span class="No-Break"><st c="5225">the recipe.</st></span></p></li>				<li><st c="5236">Let’s capture the minimum and maximum values </st><span class="No-Break"><st c="5282">of </st></span><span class="No-Break"><strong class="source-inline"><st c="5285">HouseAge</st></strong></span><span class="No-Break"><st c="5293">:</st></span><pre class="source-code"><st c="5295">
min_value = int(X_train["HouseAge"].min())
max_value = int(X_train["HouseAge"].max())</st></pre></li>				<li><st c="5381">Let’s </st><a id="_idIndexMarker280"/><st c="5388">determine the interval width, which is the variable’s value range divided by the number </st><span class="No-Break"><st c="5476">of bins:</st></span><pre class="source-code"><st c="5484">
width = int((max_value - min_value) / 10)</st></pre><p class="list-inset"><st c="5526">If we execute </st><strong class="source-inline"><st c="5541">print(width)</st></strong><st c="5553">, we will obtain </st><strong class="source-inline"><st c="5570">5</st></strong><st c="5571">, which is the size of </st><span class="No-Break"><st c="5594">the intervals.</st></span></p></li>				<li><st c="5608">No</st><a id="_idTextAnchor531"/><st c="5611">w we need to define the interval limits and store them in </st><span class="No-Break"><st c="5670">a list:</st></span><pre class="source-code"><st c="5677">
interval_limits = [i for i in range(
    min_value, max_value, width)]</st></pre><p class="list-inset"><st c="5744">If we now execute </st><strong class="source-inline"><st c="5763">print(interval_limits)</st></strong><st c="5785">, we will see the </st><span class="No-Break"><st c="5803">interval limits:</st></span></p><pre class="source-code"><strong class="bold"><st c="5819">[1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51]</st></strong></pre></li>				<li><st c="5862">Let’s expand the limits of the first and last intervals to accommodate smaller or greater values that we could find in the test set or in future </st><span class="No-Break"><st c="6008">data sources:</st></span><pre class="source-code"><st c="6021">
interval_limits[0] = -np.inf
interval_limits[-1] = np.inf</st></pre></li>				<li><st c="6079">Let’s make a copy of the DataFrames so we don’t overwrite the original ones, which we will need for later steps in </st><span class="No-Break"><st c="6195">the recipe:</st></span><pre class="source-code"><st c="6206">
train_t = X_train.copy()
test_t = X_test.copy()</st></pre></li>				<li><st c="6254">Let’s sort </st><a id="_idIndexMarker281"/><st c="6266">the </st><strong class="source-inline"><st c="6270">HouseAge</st></strong><st c="6278"> variable into the intervals that we defined in </st><span class="No-Break"><em class="italic"><st c="6326">step 6</st></em></span><span class="No-Break"><st c="6332">:</st></span><pre class="source-code"><st c="6334">
train_t["HouseAge_disc"] = pd.cut(
    x=X_train["HouseAge"],
    bins=interval_limits,
    include_lowest=True)
test_t["HouseAge_disc"] = pd.cut(
    x=X_test["HouseAge"],
    bins=interval_limits,
    include_lowest=True)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="6534">Note</st></p>
			<p class="callout"><st c="6539">We have set </st><strong class="source-inline"><st c="6552">include_lowest=True</st></strong><st c="6571"> to include the lowest value in the first interval. </st><st c="6623">Note that we used the train set to find the intervals and then used those limits to sort the variable in </st><span class="No-Break"><st c="6728">both datasets.</st></span></p>
			<ol>
				<li value="10"><st c="6742">Let’s print the top </st><strong class="source-inline"><st c="6763">5</st></strong><st c="6764"> observations of the discretized and </st><span class="No-Break"><st c="6801">original variables:</st></span><pre class="source-code"><st c="6820">
print(train_t[["HouseAge", "HouseAge_disc"]].head(5))</st></pre><p class="list-inset"><st c="6874">In the foll</st><a id="_idTextAnchor532"/><st c="6886">owing output, we can see that the </st><strong class="source-inline"><st c="6921">52</st></strong><st c="6923"> value was allocated to the 46–infinite interval, the </st><strong class="source-inline"><st c="6977">43</st></strong><st c="6979"> value was allocated to the 41–46 interval, and </st><span class="No-Break"><st c="7027">so on:</st></span></p><pre class="source-code"><strong class="bold"><st c="7033">         HouseAge HouseAge_disc</st></strong>
<strong class="bold"><st c="7056">1989         52.0   (46.0, inf]</st></strong>
<strong class="bold"><st c="7078">256           43.0  (41.0, 46.0]</st></strong>
<strong class="bold"><st c="7100">7887         17.0  (16.0, 21.0]</st></strong>
<strong class="bold"><st c="7123">4581</st></strong><strong class="bold"><st c="7128">         17.0  (16.0, 21.0]</st></strong>
<strong class="bold"><st c="7146">1993         50.0   (46.0, inf]</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="7168">Note</st></p>
			<p class="callout"><st c="7173">The parentheses and brackets in the intervals indicate whether a value is included in the interval or not. </st><st c="7281">For example, the (41, 46] interval contains all values greater than 41 and smaller than or equal </st><span class="No-Break"><st c="7378">to 46.</st></span></p>
			<p class="list-inset"><st c="7384">Equal-width discretization</st><a id="_idIndexMarker282"/><st c="7411"> allocates a different number of observations to </st><span class="No-Break"><st c="7460">each i</st><a id="_idTextAnchor533"/><st c="7466">nterval.</st></span></p>
			<ol>
				<li value="11"><st c="7475">Let’s make a bar plot with the proportion of observations across the intervals of  </st><strong class="source-inline"><st c="7558">HouseAge</st></strong><st c="7566"> in the train and </st><span class="No-Break"><st c="7584">test sets:</st></span><pre class="source-code"><st c="7594">
t1 = train_t["HouseAge_disc"].value_counts(
    normalize=True, sort=False)
t2 = test_t["HouseAge_disc"].value_counts(
    normalize=True, sort=False)
tmp = pd.concat([t1, t2], axis=1)
tmp.columns = ["train", "test"]
tmp.plot.bar(figsize=(8, 5))
plt.xticks(rotation=45)
plt.ylabel("Number of observations per bin")
plt.xlabel('Discretized HouseAge')
plt.title("HouseAge")
plt.show()</st></pre><p class="list-inset"><a id="_idTextAnchor534"/><st c="7969">In the following output, we can see that the proportion of observations per interval is approximately the same in the train and test sets, but different </st><span class="No-Break"><st c="8123">across inter</st><a id="_idTextAnchor535"/><st c="8135">vals:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B22396_04_1.jpg" alt="Figure 4.1 – The proportion of observations per interval after the discretization"/><st c="8141"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="8299">Figure 4.1 – The proportion of observations per interval after the discretization</st></p>
			<p class="list-inset"><st c="8380">With </st><strong class="source-inline"><st c="8386">feature-engine</st></strong><st c="8400">, we </st><a id="_idIndexMarker283"/><st c="8405">can perform equal-width discretization in fewer lines of code and for many variables at </st><span class="No-Break"><st c="8493">a time.</st></span></p>
			<ol>
				<li value="12"><st c="8500">First, let’s import </st><span class="No-Break"><st c="8521">the discretizer:</st></span><pre class="source-code"><st c="8537">
from feature_engine.discretisation import EqualWidthDiscretiser</st><a id="_idTextAnchor536"/></pre></li>				<li><st c="8601">Let’s set up the discretizer to sort three continuous variables into </st><span class="No-Break"><st c="8671">eight intervals:</st></span><pre class="source-code"><st c="8687">
variables = ['MedInc', 'HouseAge', 'AveRooms']
disc = EqualWidthDiscretiser(
    bins=8, variables=variables)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="8793">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="8798">EqualWidthDiscretiser()</st></strong><st c="8822"> returns an integer indicating w</st><a id="_idTextAnchor537"/><st c="8854">hether the value was sorted into the first, second, or eighth bin by default. </st><st c="8933">That is the equivalent of ordinal encoding, which we described in the </st><em class="italic"><st c="9003">Replacing categories with ordinal numbers</st></em><st c="9044"> recipe of </st><a href="B22396_02.xhtml#_idTextAnchor182"><span class="No-Break"><em class="italic"><st c="9055">Chapter 2</st></em></span></a><st c="9064">, </st><em class="italic"><st c="9066">Encoding Categorical Variables</st></em><st c="9096">. To carry out a different encoding with the </st><strong class="source-inline"><st c="9141">feature-engine</st></strong><st c="9155"> or </st><strong class="source-inline"><st c="9159">category</st></strong> <strong class="source-inline"><st c="9167">encoders </st></strong><st c="9177">Python libraries, cast the returned variables as objects by setting </st><strong class="source-inline"><st c="9245">return_object</st></strong><st c="9258"> to </st><strong class="source-inline"><st c="9262">True</st></strong><st c="9266">. Alternatively, make the transformer return the interval limits by setting </st><strong class="source-inline"><st c="9342">return_boundaries</st></strong> <span class="No-Break"><st c="9359">to </st></span><span class="No-Break"><strong class="source-inline"><st c="9363">True</st></strong></span><span class="No-Break"><st c="9367">.</st></span></p>
			<ol>
				<li value="14"><st c="9368">Let’s fit the </st><a id="_idIndexMarker284"/><st c="9383">discretizer to the train set so that it learns the cut points for </st><span class="No-Break"><st c="9449">each variable:</st></span><pre class="source-code"><st c="9463">
disc.fit(X_train)</st></pre><p class="list-inset"><st c="9481">After fitting, we can inspect the cut points in the </st><strong class="source-inline"><st c="9534">binner_dict_</st></strong><st c="9546"> attribute by </st><span class="No-Break"><st c="9560">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="9570">print(disc.binner_dict_)</st></strong></span><span class="No-Break"><st c="9594">.</st></span></p></li>			</ol>
			<p class="callout-heading"><st c="9595">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="9600">feature-engine</st></strong><st c="9615"> will automatically extend the limits of the lower and upper intervals to infinite to accommodate potential outliers in </st><span class="No-Break"><st c="9735">future data.</st></span></p>
			<ol>
				<li value="15"><st c="9747">Let’s discretize the variables in the train and </st><span class="No-Break"><st c="9796">test sets:</st></span><pre class="source-code"><st c="9806">
train_t = disc.transform(X_train)
test_t = disc.transform(X_test)</st></pre><p class="list-inset"><strong class="source-inline"><st c="9872">EqualWidthDiscretiser()</st></strong><st c="9896"> returns a DataFrame where the selected variables are discretized. </st><st c="9963">If we run </st><strong class="source-inline"><st c="9973">test_t.head()</st></strong><st c="9986">, </st><a id="_idTextAnchor538"/><st c="9988">we will see the following output where the original values of </st><strong class="source-inline"><st c="10050">MedInc</st></strong><st c="10056">, </st><strong class="source-inline"><st c="10058">HouseAge</st></strong><st c="10066">, and </st><strong class="source-inline"><st c="10072">AveRooms</st></strong><st c="10080"> are replaced by the </st><span class="No-Break"><st c="10101">interval nu</st><a id="_idTextAnchor539"/><st c="10112">mbers:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B22396_04_2.jpg" alt="Figure 4.2 – A DataFrame with three discretized variables: HouseAge, MedInc, and AveRooms"/><st c="10119"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="10446">Figure 4.2 – A DataFrame with three discretized variables: HouseAge, MedInc, and AveRooms</st></p>
			<ol>
				<li value="16"><st c="10535">Now, let’s</st><a id="_idIndexMarker285"/><st c="10546"> make bar plots with the proportion of observations per interval to better understand the effect of </st><span class="No-Break"><st c="10646">equal-width discretization:</st></span><pre class="source-code"><st c="10673">
plt.figure(figsize=(6, 12), constrained_layout=True)
for i in range(3):
    # location of plot in figure
    ax = plt.subplot(3, 1, i + 1)
    # the variable to plot
    var = variables[i]
    # determine proportion of observations per bin
    t1 = train_t[var].value_counts(normalize=True,
        sort=False)
    t2 = test_t[var].value_counts(normalize=True,
        sort=False)
    # concatenate proportions
    tmp = pd.concat([t1, t2], axis=1)
    tmp.columns = ['train', 'test']
    # sort the intervals
    tmp.sort_index(inplace=True)
    # make plot
    tmp.plot.bar(ax=ax)
    plt.xticks(rotation=0)
    plt.ylabel('Observations per bin')
    ax.set_title(var)
plt.show()</st></pre><p class="list-inset"><st c="11271">The in</st><a id="_idTextAnchor540"/><st c="11278">tervals contain</st><a id="_idIndexMarker286"/><st c="11294"> a different number of observations, as shown in the </st><span class="No-Break"><st c="11347">following plots:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B22396_04_3.jpg" alt="Figure 4.3 – Bar plots with the proportion of observations per interval after the discretization"/><st c="11363"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="11674">Figure 4.3 – Bar plots with the proportion of observations per interval after the discretization</st></p>
			<p class="list-inset"><st c="11770">Now, let’s </st><a id="_idIndexMarker287"/><st c="11782">implement equal-width discretization </st><span class="No-Break"><st c="11819">with scikit-learn.</st></span></p>
			<ol>
				<li value="17"><st c="11837">Let’s import the classes </st><span class="No-Break"><st c="11863">from scikit-learn:</st></span><pre class="source-code"><st c="11881">
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import kBinsDiscretiz</st><a id="_idTextAnchor541"/><st c="11976">er</st></pre></li>				<li><st c="11979">Let’s set up an equal-width discretizer by setting its </st><strong class="source-inline"><st c="12035">strategy</st></strong> <span class="No-Break"><st c="12043">to </st></span><span class="No-Break"><strong class="source-inline"><st c="12047">uniform</st></strong></span><span class="No-Break"><st c="12054">:</st></span><pre class="source-code"><st c="12056">
disc = KBinsDiscretizer(
    n_bins=8, encode='ordinal', strategy='uniform')</st></pre></li>			</ol>
			<p class="callout-heading"><st c="12129">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="12134">KBinsDiscretiser()</st></strong><st c="12153"> can return the bins as integers by setting </st><strong class="source-inline"><st c="12197">encoding</st></strong><st c="12205"> to </st><strong class="source-inline"><st c="12209">'ordinal'</st></strong><st c="12218"> or one-hot encoded by setting </st><strong class="source-inline"><st c="12249">encoding</st></strong> <span class="No-Break"><st c="12257">to </st></span><span class="No-Break"><strong class="source-inline"><st c="12261">'onehot-dense'</st></strong></span><span class="No-Break"><st c="12275">.</st></span></p>
			<ol>
				<li value="19"><st c="12276">Let’s use </st><strong class="source-inline"><st c="12287">ColumnTransformer()</st></strong><st c="12306"> to restrict the discretization to the selected variables from </st><span class="No-Break"><em class="italic"><st c="12369">step 13</st></em></span><span class="No-Break"><st c="12376">:</st></span><pre class="source-code">
<strong class="source-inline"><st c="12378">ct = ColumnTransformer(</st></strong>
<strong class="source-inline"><st c="12401">    [("discretizer", disc, variables)],</st></strong>
<strong class="source-inline"><st c="12437">    remainder="passthrough",</st></strong>
<strong class="source-inline"><st c="12462">).set_output(transform="pandas")</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="12495">Note</st></p>
			<p class="callout"><st c="12500">With </st><strong class="source-inline"><st c="12506">remainder</st></strong><st c="12515"> set to </st><strong class="source-inline"><st c="12523">passthrough</st></strong><st c="12534">, </st><strong class="source-inline"><st c="12536">ColumnTransformer()</st></strong><st c="12555"> returns all the variables in the input DataFrame after the transformation. </st><st c="12631">To return only the transformed variables, set </st><strong class="source-inline"><st c="12677">remainder</st></strong> <span class="No-Break"><st c="12686">to </st></span><span class="No-Break"><strong class="source-inline"><st c="12690">drop</st></strong></span><span class="No-Break"><st c="12694">.</st></span></p>
			<ol>
				<li value="20"><st c="12695">Let’s fit the discretizer to the train set so that it learns the </st><span class="No-Break"><st c="12761">interval limits:</st></span><pre class="source-code"><st c="12777">
ct.fit(X_train)</st></pre></li>				<li><st c="12793">Finall</st><a id="_idTextAnchor542"/><st c="12800">y, let’s </st><a id="_idIndexMarker288"/><st c="12810">discretize the selected variables in the train and </st><span class="No-Break"><st c="12861">test sets:</st></span><pre class="source-code"><st c="12871">
train_t = ct.transform(X_train)
test_t = ct.transform(X_test)</st></pre><p class="list-inset"><st c="12933">We can inspect the cut points learned by the transformer by </st><span class="No-Break"><st c="12994">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="13004">ct.named_transformers_["discretizer"].bin_edges_</st></strong></span><span class="No-Break"><st c="13052">.</st></span></p></li>			</ol>
			<p class="callout-heading"><st c="13053">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="13058">ColumnTransformer()</st></strong><st c="13078"> will append </st><strong class="source-inline"><st c="13091">discretize</st></strong><st c="13101"> to the variables that were discretized and </st><strong class="source-inline"><st c="13145">remainder</st></strong><st c="13154"> to those that were </st><span class="No-Break"><st c="13174">not modified.</st></span></p>
			<p><st c="13187">We can check the output by </st><span class="No-Break"><st c="13215">executing </st></span><span class="No-Break"><strong class="source-inline"><st c="13225">test_</st><a id="_idTextAnchor543"/><a id="_idTextAnchor544"/><st c="13230">t.head()</st></strong></span><span class="No-Break"><st c="13239">.</st></span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor545"/><st c="13240">How it works…</st></h2>
			<p><st c="13254">In this recipe, we sorted the variable values into equidistant intervals. </st><st c="13329">To perform discretization with </st><strong class="source-inline"><st c="13360">pandas</st></strong><st c="13366">, we first found the maximum and minimum values of the </st><strong class="source-inline"><st c="13421">HouseAge</st></strong><st c="13429"> variable using the </st><strong class="source-inline"><st c="13449">max()</st></strong><st c="13454"> and </st><strong class="source-inline"><st c="13459">min()</st></strong><st c="13464"> methods. </st><st c="13474">Then, we estimated the interval width by dividing the value range by the number of arbitrary bins. </st><st c="13573">With the width and the minimum and maximum values, we determined the interval limits and stored them in a list. </st><st c="13685">We</st><a id="_idIndexMarker289"/><st c="13687"> used this list with pandas </st><strong class="source-inline"><st c="13715">cut()</st></strong><st c="13720"> to sort the variable values into </st><span class="No-Break"><st c="13754">the intervals.</st></span></p>
			<p class="callout-heading"><st c="13768">Note</st></p>
			<p class="callout"><st c="13773">Pandas </st><strong class="source-inline"><st c="13781">cut()</st></strong><st c="13786"> sorts the variable into intervals of equal size by default. </st><st c="13847">It will extend the variable range by .1% on each side to include the minimum and maximum values. </st><st c="13944">The reason why we generated the intervals manually is to accommodate potentially smaller or larger values than those seen in the dataset in future data sources when we deploy </st><span class="No-Break"><st c="14119">our model.</st></span></p>
			<p><st c="14129">After discretization, we normally treat the intervals as categorical values. </st><st c="14207">By default, pandas </st><strong class="source-inline"><st c="14226">cut()</st></strong><st c="14231"> returns the interval values as ordered integers, which is the equivalent of ordinal encoding. </st><st c="14326">Alternatively, we can return the interval limits by setting the </st><strong class="source-inline"><st c="14390">labels</st></strong><st c="14396"> parameter </st><span class="No-Break"><st c="14407">to </st></span><span class="No-Break"><strong class="source-inline"><st c="14410">None</st></strong></span><span class="No-Break"><st c="14414">.</st></span></p>
			<p><st c="14415">To display the </st><a id="_idIndexMarker290"/><st c="14431">number of observations per interval, we created a bar plot. </st><st c="14491">We used the pandas </st><strong class="source-inline"><st c="14510">value_counts()</st></strong><st c="14524"> function to obtain the fraction of observations per interval, which returns the result in pandas Series, where the index is the interval and the counts are the values. </st><st c="14693">To plot these proportions, first, we concatenated the train and test set series using the pandas </st><strong class="source-inline"><st c="14790">concat()</st></strong><st c="14798">function in a DataFrame, and then we assigned the </st><strong class="source-inline"><st c="14849">train</st></strong><st c="14854"> and </st><strong class="source-inline"><st c="14859">test</st></strong><st c="14863"> column names to it. </st><st c="14884">Finally, we used </st><strong class="source-inline"><st c="14901">plot.bar()</st></strong><st c="14911"> to display a bar plot. </st><st c="14935">We rotated the labels with Matplotlib’s </st><strong class="source-inline"><st c="14975">xticks()</st></strong><st c="14983">function, and added the </st><em class="italic"><st c="15008">x</st></em><st c="15009"> and </st><em class="italic"><st c="15014">y</st></em><st c="15015"> legend with </st><strong class="source-inline"><st c="15028">xlabels()</st></strong><st c="15037"> and </st><strong class="source-inline"><st c="15042">ylabel()</st></strong><st c="15050">, as well as the title </st><span class="No-Break"><st c="15073">with </st></span><span class="No-Break"><strong class="source-inline"><st c="15078">title()</st></strong></span><span class="No-Break"><st c="15085">.</st></span></p>
			<p><st c="15086">To perform equal-width discretization with </st><strong class="source-inline"><st c="15130">feature-engine</st></strong><st c="15144">, we </st><span class="No-Break"><st c="15149">used </st></span><span class="No-Break"><strong class="source-inline"><st c="15154">EqualWidth</st></strong></span><strong class="source-inline"><st c="15164">
Discretiser()</st></strong><st c="15178">, which takes the number of bins and the variables to discretize as arguments. </st><st c="15257">With </st><strong class="source-inline"><st c="15262">fit()</st></strong><st c="15267">, the discretizer learned the interval limits for each variable. </st><st c="15332">With </st><strong class="source-inline"><st c="15337">transform()</st></strong><st c="15348">, it sorted the values into </st><span class="No-Break"><st c="15376">each bin.</st></span></p>
			<p><strong class="source-inline"><st c="15385">EqualWidthDiscretiser()</st></strong><st c="15409"> returns the bins as sorted integers by default, which is the equivalent of ordinal encoding. </st><st c="15503">To follow up the discretization with any other encoding procedure available in the </st><strong class="source-inline"><st c="15586">feature-engine</st></strong><st c="15600"> or </st><strong class="source-inline"><st c="15604">category encoders</st></strong><st c="15621"> libraries, we need to return the bins cast as objects by setting </st><strong class="source-inline"><st c="15687">return_object</st></strong><st c="15700"> to </st><strong class="source-inline"><st c="15704">True</st></strong><st c="15708"> when we set up </st><span class="No-Break"><st c="15724">the transformer.</st></span></p>
			<p class="callout-heading"><st c="15740">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="15745">EqualWidthDiscretiser()</st></strong><st c="15769"> extends the values of the first and last interval to minus and plus infinity by default to automatically accommodate smaller and greater values than those seen in the </st><span class="No-Break"><st c="15937">training set.</st></span></p>
			<p><st c="15950">We followed the discretization with bar plots to display the fraction of observations per interval for each of the transformed variables. </st><st c="16089">We could see that if the original variable was skewed, the bar plot was also skewed. </st><st c="16174">Note how some of the intervals of the </st><strong class="source-inline"><st c="16212">MedInc</st></strong><st c="16218"> and </st><strong class="source-inline"><st c="16223">AveRooms</st></strong><st c="16231"> variables, which had skewed distributions, contained very few observations. </st><st c="16308">In particular, even though we wanted to create eight bins for </st><strong class="source-inline"><st c="16370">AveRooms</st></strong><st c="16378">, there were only enough values to create five, and most values of the variables were allocated to the </st><span class="No-Break"><st c="16481">first interval.</st></span></p>
			<p><st c="16496">Finally, we </st><a id="_idTextAnchor546"/><st c="16509">discretized</st><a id="_idIndexMarker291"/><st c="16520"> three continuous variables into equal-width bins with </st><strong class="source-inline"><st c="16575">KBinsDiscretizer()</st></strong><st c="16593"> from scikit-learn. </st><st c="16613">To create equal-width bins, we set the </st><strong class="source-inline"><st c="16652">strategy</st></strong><st c="16660"> argument to </st><strong class="source-inline"><st c="16673">uniform</st></strong><st c="16680">. With </st><strong class="source-inline"><st c="16687">fit()</st></strong><st c="16692">, the transformer learned the limits of the intervals, and with </st><strong class="source-inline"><st c="16756">transform()</st></strong><st c="16767">, it sorted the values into </st><span class="No-Break"><st c="16795">each interval.</st></span></p>
			<p><st c="16809">We used the </st><strong class="source-inline"><st c="16822">ColumnTransformer()</st></strong><st c="16841"> to restrict the discretization to the selected variables, setting the transform output to pandas to obtain a DataFrame after the transformation. </st><strong class="source-inline"><st c="16987">KBinsDiscretizer()</st></strong><st c="17005"> can return the intervals as ordinal numbers, as we had it do in the recipe, or as one-hot-encoded variables. </st><st c="17115">The behavior can be modified through the </st><span class="No-Break"><strong class="source-inline"><st c="17156">encod</st><a id="_idTextAnchor547"/><a id="_idTextAnchor548"/><st c="17161">e</st></strong></span><span class="No-Break"><st c="17163"> parameter.</st></span></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor549"/><st c="17174">See also</st></h2>
			<p><st c="17183">For a comparison of equal-width discretization with more sophisticated methods, see Dougherty J, Kohavi R, Sahami M. </st><em class="italic"><st c="17301">Supervised and unsupervised discretization of continuous features</st></em><st c="17366">. In: Proceedings of the 12th international conference on machine learning. </st><st c="17442">San Francisco: Morgan Kaufma</st><a id="_idTextAnchor550"/><st c="17470">nn; 1995.</st><a id="_idTextAnchor551"/><a id="_idTextAnchor552"/> <span class="No-Break"><st c="17480">p. </st><st c="17484">194–202.</st></span></p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor553"/><st c="17492">Implementing equal-frequency discretization</st></h1>
			<p><st c="17536">Equal-width discretization</st><a id="_idIndexMarker292"/><st c="17563"> is intuitive and easy to compute. </st><st c="17598">However, if the variables are skewe</st><a id="_idTextAnchor554"/><st c="17633">d, then there will be many empty bins or bins with only a few values, while most observations will be allocated to a few intervals. </st><st c="17766">This could result in a loss of information. </st><st c="17810">This problem can be solved by adaptively finding the interval cut-points so that each interval contains a similar fraction </st><span class="No-Break"><st c="17933">of observations.</st></span></p>
			<p><st c="17949">Equal-frequency discretization</st><a id="_idIndexMarker293"/><st c="17980"> divides the values of the variable into intervals that carry the same proportion of observations. </st><st c="18079">The interval width is</st><a id="_idIndexMarker294"/><st c="18100"> determined by </st><strong class="bold"><st c="18115">quantiles</st></strong><st c="18124">. Quantiles are values that divide data into equal portions. </st><st c="18185">For example, the median is a quantile that divides the data into two halves. </st><st c="18262">Quartiles divide the data into four equal portions, and percentiles divide the data into 100 equal-sized portions. </st><st c="18377">As a result, the intervals will most likely have different widths, but a similar number of observations. </st><st c="18482">The number of intervals is defined by </st><span class="No-Break"><st c="18520">the user.</st></span></p>
			<p><st c="18529">In this recipe, we will perform equal-frequency discretization using </st><strong class="source-inline"><st c="18599">pandas</st></strong><st c="18605">, </st><strong class="source-inline"><st c="18607">scikit-learn</st></strong><st c="18619">, </st><span class="No-Break"><st c="18621">and </st><a id="_idTextAnchor555"/><a id="_idTextAnchor556"/></span><span class="No-Break"><strong class="source-inline"><st c="18625">feature-engine</st></strong></span><span class="No-Break"><st c="18639">.</st></span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor557"/><st c="18640">How to do it...</st></h2>
			<p><st c="18656">First, let’s</st><a id="_idIndexMarker295"/><st c="18669"> import the necessary Python libraries and get the </st><span class="No-Break"><st c="18720">dataset ready:</st></span></p>
			<ol>
				<li><st c="18734">Let’s import the required Python libraries </st><span class="No-Break"><st c="18778">and functions:</st></span><pre class="source-code"><st c="18792">
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="18951">Let’s load the California housing dataset into </st><span class="No-Break"><st c="18999">a DataFrame:</st></span><pre class="source-code"><st c="19011">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="19076">Note</st></p>
			<p class="callout"><st c="19081">To avoid data leakage, we will determine the interval boundaries or quantiles from the </st><span class="No-Break"><st c="19169">train set.</st></span></p>
			<ol>
				<li value="3"><st c="19179">Let’s divide the data into train and </st><span class="No-Break"><st c="19217">test sets:</st></span><pre class="source-code"><st c="19227">
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="19317">Let’s make a copy of </st><span class="No-Break"><st c="19339">the DataFrames:</st></span><pre class="source-code"><st c="19354">
train_t = X_train.copy()
test_t = X_test.copy()</st></pre></li>				<li><st c="19402">We’ll use pandas </st><strong class="source-inline"><st c="19420">qcut()</st></strong><st c="19426">to obtain a discretized copy of the </st><strong class="source-inline"><st c="19463">HouseAge</st></strong><st c="19471"> variable, which we will store as a new column in the training set, and the limits of eight </st><span class="No-Break"><st c="19563">equal-frequency intervals:</st></span><pre class="source-code"><st c="19589">
train_t["House_disc"], interval_limits = pd.qcut(
    x=X_train["HouseAge"],
    q=8,
    labels=None,
    retbins=True,
)</st></pre><p class="list-inset"><st c="19696">If you execute </st><strong class="source-inline"><st c="19712">print(interval_limits)</st></strong><st c="19734">, you’ll see the following interval limits: </st><strong class="source-inline"><st c="19778">array([ 1., 14., 18., 24., 29., 34., 37., </st></strong><span class="No-Break"><strong class="source-inline"><st c="19820">44., 52.])</st></strong></span><span class="No-Break"><st c="19830">.</st></span></p></li>				<li><st c="19831">Let’s </st><a id="_idIndexMarker296"/><st c="19838">print the top five observations of the discretized and </st><span class="No-Break"><st c="19893">original variables:</st></span><pre class="source-code"><st c="19912">
print(train_t[["HouseAge", "House_disc"]].head(5))</st></pre><p class="list-inset"><st c="19963">In the following output, we see that the </st><strong class="source-inline"><st c="20005">52</st></strong><st c="20007"> value was allocated to the 44–52 interval, the </st><strong class="source-inline"><st c="20055">43</st></strong><st c="20057"> value was allocated to the 37–44 interval, and </st><span class="No-Break"><st c="20105">so on:</st></span></p><pre class="source-code"><strong class="bold"><st c="20111">       HouseAge     House_disc</st></strong>
<strong class="bold"><st c="20131">1989       52.0   (44.0, 52.0]</st></strong>
<strong class="bold"><st c="20154">256        43.0   (37.0, 44.0]</st></strong>
<strong class="bold"><st c="20176">7887       17.0   (14.0, 18.0]</st></strong>
<strong class="bold"><st c="20199">4581       17.0   (14.0, 18.0]</st></strong>
<strong class="bold"><st c="20222">1993       50.0   (44.0,</st><a id="_idTextAnchor558"/><st c="20239"> 52.0]</st></strong></pre></li>				<li><st c="20245">Now, let’s discretize </st><strong class="source-inline"><st c="20268">HouseAge</st></strong><st c="20276"> in the test set, using pandas </st><strong class="source-inline"><st c="20307">cut()</st></strong><st c="20312"> with the interval limits determined in </st><span class="No-Break"><em class="italic"><st c="20352">step 5</st></em></span><span class="No-Break"><st c="20358">:</st></span><pre class="source-code"><st c="20360">
test_t["House_disc"] = pd.cut(
    x=X_test["HouseAge"],
    bins=interval_limits,
    include_lowest=True)</st></pre></li>				<li><st c="20456">Let’s </st><a id="_idIndexMarker297"/><st c="20463">make a bar plot with the proportion of observations per interval in the train and </st><span class="No-Break"><st c="20545">test sets:</st></span><pre class="source-code"><st c="20555">
# determine proportion of observations per bin
t1 = train_t["House_disc"].value_counts(
    normalize=True)
t2 = test_t["House_disc"].value_counts(normalize=True)
# concatenate proportions
tmp = pd.concat([t1, t2], axis=1)
tmp.columns = ["train", "test"]
tmp.sort_index(inplace=True)
# plot
tmp.plot.bar()
plt.xticks(rotation=45)
plt.ylabel("Number of observations per bin")
plt.title("HouseAge")
plt.show()</st></pre><p class="list-inset"><st c="20959">In the following plot, we can see that the bins contain a similar fraction</st><a id="_idTextAnchor559"/> <span class="No-Break"><st c="21034">of observations:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B22396_04_4.jpg" alt="Figure 4.4 – The proportion of observations per interval of HouseAge after equal-frequency discretization"/><st c="21051"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="21186">Figure 4.4 – The proportion of observations per interval of HouseAge after equal-frequency discretization</st></p>
			<p class="list-inset"><st c="21291">With </st><strong class="source-inline"><st c="21297">feature-engine</st></strong><st c="21311">, we can apply equal-frequency discretization to </st><span class="No-Break"><st c="21360">multiple </st><a id="_idTextAnchor560"/><st c="21369">variables.</st></span></p>
			<ol>
				<li value="9"><st c="21379">Let’s </st><a id="_idIndexMarker298"/><st c="21386">import </st><span class="No-Break"><st c="21393">the discretizer:</st></span><pre class="source-code"><st c="21409">
from feature_engine.discretisation import EqualFrequencyDiscretiser</st></pre></li>				<li><st c="21477">Let’s set up the transformer to discretize three continuous variables into </st><span class="No-Break"><st c="21553">eight bins:</st></span><pre class="source-code"><st c="21564">
variables = ['MedInc', 'HouseAge', 'AveRooms']
disc = EqualFrequencyDiscretiser(
    q=8, variables=variables, return_boundaries=True)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="21695">Note</st></p>
			<p class="callout"><st c="21700">With </st><strong class="source-inline"><st c="21706">return_boundaries=True</st></strong><st c="21728">, the transformer will return the interval boundaries after the discretization. </st><st c="21808">To return the interval number, set it </st><span class="No-Break"><st c="21846">to </st></span><span class="No-Break"><strong class="source-inline"><st c="21849">False</st></strong></span><span class="No-Break"><st c="21854">.</st></span></p>
			<ol>
				<li value="11"><st c="21855">Let’s fit the </st><a id="_idIndexMarker299"/><st c="21870">discretizer to the train set so that it learns the </st><span class="No-Break"><st c="21921">interval limits:</st></span><pre class="source-code">
<strong class="bold"><st c="21937">disc.fit(X_train)</st></strong></pre><p class="list-inset"><st c="21955">The transformer stores the limits of the intervals for each variable in a dictionary in its </st><span class="No-Break"><strong class="source-inline"><st c="22048">disc.binner_dict_</st></strong></span><span class="No-Break"><st c="22065"> attribute.</st></span></p></li>			</ol>
			<p class="callout-heading"><st c="22076">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="22081">feature-engine</st></strong><st c="22096"> will automatically extend the limits of the lower and upper intervals to infinite to accommodate potential outliers in </st><span class="No-Break"><st c="22216">future data.</st></span></p>
			<ol>
				<li value="12"><st c="22228">Let’s transform the variables in the train and </st><span class="No-Break"><st c="22276">test sets:</st></span><pre class="source-code"><st c="22286">
train_t = disc.transform(X_train)
test_t = disc.tran</st><a id="_idTextAnchor561"/><st c="22339">sform(X_test)</st></pre></li>				<li><st c="22353">Let’s make bar plots with the fraction of observations per interval to better understand the effect of </st><span class="No-Break"><st c="22457">equal-frequency discretization:</st></span><pre class="source-code"><st c="22488">
plt.figure(figsize=(6, 12), constrained_layout=True)
for i in range(3):
    # location of plot in figure
    ax = plt.subplot(3, 1, i + 1)
    # the variable to plot
    var = variables[i]
    # determine proportion of observations per bin
    t1 = train_t[var].value_counts(normalize=True)
    t2 = test_t[var].value_counts(normalize=True)
    # concatenate proportions
    tmp = pd.concat([t1, t2], axis=1)
    tmp.columns = ['train', 'test']
    # sort the intervals
    tmp.sort_index(inplace=True)
    # make plot
    tmp.plot.bar(ax=ax)
    plt.xticks(rotation=45)
    plt.ylabel("Observations per bin")
    # add variable name as title
    ax.set_title(var)
 plt.show()</st></pre><p class="list-inset"><st c="23092">In the following </st><a id="_idIndexMarker300"/><st c="23110">figure, we can se</st><a id="_idTextAnchor562"/><st c="23127">e that the intervals have a similar fracti</st><a id="_idTextAnchor563"/><st c="23170">on </st><span class="No-Break"><st c="23174">of observations:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B22396_04_5.jpg" alt="Figure 4.5 – The proportion of observations per interval after  equal-frequency discretization of three va﻿riables."/><st c="23190"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="23721">Figure 4.5 – The proportion of observations per interval after equal-frequency discretization of three va</st><a id="_idTextAnchor564"/><st c="23826">riables.</st></p>
			<p class="list-inset"><st c="23835">Now, let’s carry </st><a id="_idIndexMarker301"/><st c="23853">out equal-frequency discretization </st><span class="No-Break"><st c="23888">with scikit-learn:</st></span></p>
			<ol>
				<li value="14"><st c="23906">Let’s import </st><span class="No-Break"><st c="23920">the transformer:</st></span><pre class="source-code"><st c="23936">
from sklearn.preprocessing import KBinsDiscretizer</st></pre></li>				<li><st c="23987">Let’s set up the discretizer to sort variables into eight </st><span class="No-Break"><st c="24046">equal-frequency bins:</st></span><pre class="source-code"><st c="24067">
disc = KBinsDiscretizer(
    n_bins=8, encode='ordinal', strategy='quantile')</st></pre></li>				<li><st c="24141">Let’s fit the </st><a id="_idIndexMarker302"/><st c="24156">discretizer to a slice of the train set containing the variables from </st><em class="italic"><st c="24226">step 10</st></em><st c="24233"> so that it learns the </st><span class="No-Break"><st c="24256">interval limits:</st></span><pre class="source-code"><st c="24272">
disc.fit(X_train[variables])</st></pre></li>			</ol>
			<p class="callout-heading"><st c="24301">Note</st></p>
			<p class="callout"><st c="24306">scikit-learn’s </st><strong class="source-inline"><st c="24322">KBinsDiscretiser()</st></strong><st c="24340"> will discretize all the variables in the dataset. </st><st c="24391">To discretize only a subset, we apply the transformer to the slice of the DataFrame that contains the variables of interest. </st><st c="24516">Alternatively, we can restrict the discretization to a subset of variables by using the </st><strong class="source-inline"><st c="24604">ColumnTransformer()</st></strong><st c="24623">, as we did in the </st><em class="italic"><st c="24642">Performing equal-width </st></em><span class="No-Break"><em class="italic"><st c="24665">discretization</st></em></span><span class="No-Break"><st c="24679"> recipe.</st></span></p>
			<ol>
				<li value="17"><st c="24687">Let’s make a copy of the DataFrames where we’ll store the </st><span class="No-Break"><st c="24746">discretized variables:</st></span><pre class="source-code"><st c="24768">
train_t = X_train.copy()
test_t = X_test.copy()</st></pre></li>				<li><st c="24816">Finally, let’s transform the variables in both the train and </st><span class="No-Break"><st c="24878">test sets:</st></span><pre class="source-code"><st c="24888">
train_t[variables] = disc.transform(
    X_train[variables])
test_t[variables] = disc.transform(X_test[variables])</st></pre></li>			</ol>
			<p><st c="24999">We can inspect the cut points by </st><span class="No-Break"><st c="25033">execu</st><a id="_idTextAnchor565"/><a id="_idTextAnchor566"/><st c="25038">ting </st></span><span class="No-Break"><strong class="source-inline"><st c="25044">disc.bin_edges_</st></strong></span><span class="No-Break"><st c="25059">.</st></span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor567"/><st c="25060">How it works…</st></h2>
			<p><st c="25074">In this recipe, we sorted the variable values into intervals with a similar proportion </st><span class="No-Break"><st c="25162">of observations.</st></span></p>
			<p><st c="25178">We used pandas </st><strong class="source-inline"><st c="25194">qcut()</st></strong><st c="25200"> to identify the interval limits from the train set and sort the values of the </st><strong class="source-inline"><st c="25279">HouseAge</st></strong><st c="25287"> variable into those intervals. </st><st c="25319">Next, we passed those interval limits to pandas </st><strong class="source-inline"><st c="25367">cut()</st></strong><st c="25372"> to discretize </st><strong class="source-inline"><st c="25387">HouseAge</st></strong><st c="25395"> in the test set. </st><st c="25413">Note that pandas </st><strong class="source-inline"><st c="25430">qcut()</st></strong><st c="25436">, like pandas </st><strong class="source-inline"><st c="25450">cut()</st></strong><st c="25455">, returned the interval values as ordered integers, which is the equivalent of </st><a id="_idIndexMarker303"/><span class="No-Break"><st c="25534">ordinal encoding,</st></span></p>
			<p class="callout-heading"><st c="25551">Note</st></p>
			<p class="callout"><st c="25556">With equal-frequency discretization, many occurrences of values within a small continuous range could cause observations with very similar values, resulting in different intervals. </st><st c="25738">The problem with this is that it can introduce artificial distinctions between data points that are actually quite similar in nature, biasing models or subsequent </st><span class="No-Break"><st c="25901">data analysis.</st></span></p>
			<p><st c="25915">With Feature-engine’s </st><strong class="source-inline"><st c="25938">EqualFrequencyDiscretiser()</st></strong><st c="25965">, we discretized three variables into eight bins. </st><st c="26015">With </st><strong class="source-inline"><st c="26020">fit()</st></strong><st c="26025">, the discretizer learned the interval limits and stored them in the </st><strong class="source-inline"><st c="26094">binner_dict_</st></strong><st c="26106"> attribute. </st><st c="26118">With </st><strong class="source-inline"><st c="26123">transform()</st></strong><st c="26134">, the observations were allocated to </st><span class="No-Break"><st c="26171">the bins.</st></span></p>
			<p class="callout-heading"><st c="26180">Note</st></p>
			<p class="callout"><strong class="source-inline"><st c="26185">EqualFrequencyDiscretiser()</st></strong><st c="26213"> returns an integer indicating whether the value was sorted into the first, second, or eighth bin by default. </st><st c="26323">That is the equivalent of ordinal encoding, which we described in the </st><em class="italic"><st c="26393">Replacing categories with ordinal numbers</st></em><st c="26434"> recipe in </st><a href="B22396_02.xhtml#_idTextAnchor182"><span class="No-Break"><em class="italic"><st c="26445">Chapter 2</st></em></span></a><st c="26454">, </st><em class="italic"><st c="26456">Encoding </st></em><span class="No-Break"><em class="italic"><st c="26465">Categorical Variables</st></em></span><span class="No-Break"><st c="26486">.</st></span></p>
			<p class="callout"><st c="26487">To follow up the discretization with a different type of encoding, we can return the variables cast as objects by setting </st><strong class="source-inline"><st c="26610">return_object</st></strong><st c="26623"> to </st><strong class="source-inline"><st c="26627">True</st></strong><st c="26631"> and then use any of the </st><strong class="source-inline"><st c="26656">feature-engine</st></strong><st c="26670"> or </st><strong class="source-inline"><st c="26674">category encoders</st></strong><st c="26691">  transformers</st><span class="Annotation-reference"> </span><st c="26704">. Alternatively, we can return the interval limits, as we did in </st><span class="No-Break"><st c="26769">this recipe.</st></span></p>
			<p><st c="26781">Finally, we discretized variables into eight equal-frequency bins using </st><strong class="source-inline"><st c="26854">scikit-learn</st></strong><st c="26866">’s </st><strong class="source-inline"><st c="26870">KBinsDiscretizer()</st></strong><st c="26888">. With </st><strong class="source-inline"><st c="26895">fit()</st></strong><st c="26900">, the transformer learned the cut points and stored them in its </st><strong class="source-inline"><st c="26964">bin_edges_</st></strong><st c="26974"> attribute. </st><st c="26986">With </st><strong class="source-inline"><st c="26991">transform()</st></strong><st c="27002">, it sorted the values into each interval. </st><st c="27045">Note that, differently from </st><strong class="source-inline"><st c="27073">EqualFrequencyDiscretiser()</st></strong><st c="27100">, </st><strong class="source-inline"><st c="27102">KBinsDiscretizer()</st></strong><st c="27120"> will transform all of the variables in the dataset. </st><st c="27173">To avoid this, we only applied the discretizer on a slice of the data with the variables </st><span class="No-Break"><st c="27262">to modify.</st></span></p>
			<p class="callout-heading"><st c="27272">Note</st></p>
			<p class="callout"><st c="27277">scikit-learn’s </st><strong class="source-inline"><st c="27293">KbinsDiscretizer</st></strong><st c="27309"> has the option to return the intervals as ordinal numbers or one-hot encoded. </st><st c="27388">The behavior can be modified through</st><a id="_idTextAnchor568"/><st c="27424"> the </st><span class="No-Break"><strong class="source-inline"><st c="27429">encode</st></strong></span><span class="No-Break"><st c="27435"> parameter.</st></span></p>
			<h1 id="_idParaDest-129"><st c="27446">Discretizing the va</st><a id="_idTextAnchor569"/><a id="_idTextAnchor570"/><st c="27466">riable into arbitrar</st><a id="_idTextAnchor571"/><st c="27487">y intervals</st></h1>
			<p><st c="27499">In various</st><a id="_idIndexMarker304"/><st c="27510"> industries</st><a id="_idTextAnchor572"/><st c="27521">, it is common to group variable values into segments that make sense for the business. </st><st c="27609">For example, we might want to group the variable age in intervals representing children, young adults, middle-aged people, and retirees. </st><st c="27746">Alternatively, we might group ratings into bad, good, and excellent. </st><st c="27815">On occasion, if we know that the variable is in a certain scale (for example, logarithmic), we might want to define the interval cut points within </st><span class="No-Break"><st c="27962">that scale.</st></span></p>
			<p><st c="27973">In this recipe, we will discretize a variable into pre-defined user intervals using </st><strong class="source-inline"><st c="28058">pan</st><a id="_idTextAnchor573"/><a id="_idTextAnchor574"/><st c="28061">das</st></strong> <span class="No-Break"><st c="28065">and </st></span><span class="No-Break"><strong class="source-inline"><st c="28070">feature-engine</st></strong></span><span class="No-Break"><st c="28084">.</st></span></p>
			<h2 id="_idParaDest-130"><st c="28085">How t</st><a id="_idTextAnchor575"/><st c="28091">o do it...</st></h2>
			<p><st c="28102">First, let’s </st><a id="_idIndexMarker305"/><st c="28116">imp</st><a id="_idTextAnchor576"/><st c="28119">ort the necessary Python libraries and get the </st><span class="No-Break"><st c="28167">dataset ready:</st></span></p>
			<ol>
				<li><st c="28181">Import Python libraries </st><span class="No-Break"><st c="28206">and classes:</st></span><pre class="source-code"><st c="28218">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing</st></pre></li>				<li><st c="28343">Let’s load the California housing dataset into a </st><span class="No-Break"><strong class="source-inline"><st c="28393">pandas</st></strong></span><span class="No-Break"><st c="28399"> DataFrame:</st></span><pre class="source-code"><st c="28410">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)</st></pre></li>				<li><st c="28475">Let’s plot a</st><a id="_idIndexMarker306"/><st c="28488"> histogram of the </st><strong class="source-inline"><st c="28506">Population</st></strong><st c="28516"> variable to find out its </st><span class="No-Break"><st c="28542">value range:</st></span><pre class="source-code"><st c="28554">
X["Population"].hist(bins=30)
plt.title("Population")
plt.ylabel("Number of observations")
plt.show()</st></pre><p class="list-inset"><st c="28656">Population </st><a id="_idIndexMarker307"/><st c="28668">values vary between 0 </st><a id="_idTextAnchor577"/><st c="28690">and </st><span class="No-Break"><st c="28694">approximately 40,000:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B22396_04_6.jpg" alt="Figure 4.6 – Histogram of the ﻿Population variable"/><st c="28715"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28831">Figure 4.6 – Histogram of the </st><a id="_idTextAnchor578"/><st c="28861">Population variable</st></p>
			<ol>
				<li value="4"><st c="28880">Let’s cre</st><a id="_idTextAnchor579"/><st c="28890">ate a list with arbitrary interval limits, setting the upper limit to infinity to accommodate </st><span class="No-Break"><st c="28985">bigger values:</st></span><pre class="source-code"><st c="28999">
intervals = [0, 200, 500, 1000, 2000, np.inf]</st></pre></li>				<li><st c="29045">Let’s create a list with the interval limits </st><span class="No-Break"><st c="29091">as strings:</st></span><pre class="source-code"><st c="29102">
labels = ["0-200", "200-500", "500-1000", "1000-2000",
    "&gt;2000"]</st></pre></li>				<li><st c="29166">Let’s make</st><a id="_idIndexMarker308"/><st c="29177"> a copy of the dataset and discretize the </st><strong class="source-inline"><st c="29219">Population</st></strong><st c="29229"> variable into the pre-defined limits from </st><span class="No-Break"><em class="italic"><st c="29272">step 4</st></em></span><span class="No-Break"><st c="29278">:</st></span><pre class="source-code"><st c="29280">
X_t = X.copy()
X_t[«Population_limits»] = pd.cut(
    X["Population"],
    bins=intervals,
    labels=None,
    include_lowest=True)</st></pre></li>				<li><st c="29397">Now, let’s </st><a id="_idIndexMarker309"/><st c="29409">discretize </st><strong class="source-inline"><st c="29420">Population</st></strong><st c="29430"> into pre-defined intervals and name the intervals with the labels that we defined in </st><em class="italic"><st c="29516">step 5</st></em> <span class="No-Break"><st c="29522">for comparison:</st></span><pre class="source-code"><st c="29538">
X_t[«Population_range»] = pd.cut(
    X[„Population"],
    bins=intervals,
    labels=labels,
    include_lowest=True)</st></pre></li>				<li><st c="29641">Let’s </st><a id="_idTextAnchor580"/><st c="29648">inspect the first five rows of the original and </st><span class="No-Break"><st c="29696">discretized variables:</st></span><pre class="source-code"><st c="29718">
X_t[['Population', 'Population_range',
    'Population_limits']].head()</st></pre><p class="list-inset"><st c="29786">In the last two columns of the DataFrame, we can see the discretized variables: the first one with the strings that we created in </st><em class="italic"><st c="29917">step 5</st></em><st c="29923"> as values, and the second one with the </st><span class="No-Break"><st c="29963">interval limits:</st></span></p><pre class="source-code"><strong class="bold"><st c="29979">   Population Population_range Population_limits</st></strong>
<strong class="bold"><st c="30025">0       322.0          200-500    (200.0, 500.0]</st></strong>
<strong class="bold"><st c="30056">1      2401.0            &gt;2000     (2000.0, inf]</st></strong>
<strong class="bold"><st c="30085">2       496.0          200-500    (200.0, 500.0]</st></strong>
<strong class="bold"><st c="30116">3       558.0         500-1000   (500.0, 1000.0]</st></strong>
<strong class="bold"><st c="30149">4       565.0         500-1000   (500.0, 1000.0]</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="30182">Note</st></p>
			<p class="callout"><st c="30187">We only need one of the variable versions, either the one with the value range or the one with the interval limits. </st><st c="30304">In this recipe, I created both to highlight the different options offered </st><span class="No-Break"><st c="30378">by </st></span><span class="No-Break"><strong class="source-inline"><st c="30381">pandas</st></strong></span><span class="No-Break"><st c="30387">.</st></span></p>
			<ol>
				<li value="9"><st c="30388">Finally, we </st><a id="_idIndexMarker310"/><st c="30401">can count and plot the </st><a id="_idIndexMarker311"/><st c="30424">number of observations within </st><span class="No-Break"><st c="30454">each interval:</st></span><pre class="source-code"><st c="30468">
X_t['Population_range'
    ].value_counts().sort_index().plot.bar()
plt.xticks(rotation=0)
plt.ylabel("Number of observations")
plt.title("Population")
plt.show()</st></pre><p class="list-inset"><st c="30627">In the foll</st><a id="_idTextAnchor581"/><st c="30639">owing figure, we can see that t</st><a id="_idTextAnchor582"/><st c="30671">he number of obser</st><a id="_idTextAnchor583"/><st c="30690">vations per </st><span class="No-Break"><st c="30703">interval varies:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B22396_04_7.jpg" alt="Figure 4.7 – The proportion of observations per interval after the discretization."/><st c="30719"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="30824">Figure 4.7 – The proportion of observations per interval after the discretization.</st></p>
			<p class="list-inset"><st c="30906">To wrap up</st><a id="_idIndexMarker312"/><st c="30917"> the recipe, let’s discretize multiple variables </st><span class="No-Break"><st c="30966">ut</st><a id="_idTextAnchor584"/><st c="30968">ilizing </st></span><span class="No-Break"><strong class="source-inline"><st c="30977">feature-engine</st></strong></span><span class="No-Break"><st c="30991">:</st></span></p>
			<ol>
				<li value="10"><st c="30993">Let’s</st><a id="_idIndexMarker313"/><st c="30998"> import </st><span class="No-Break"><st c="31006">the transformer:</st></span><pre class="source-code"><st c="31022">
from feature_engine.discretisation import
    ArbitraryDiscretiser</st></pre></li>				<li><st c="31085">Let’s create a dictionary with the variables as keys and the interval limits </st><span class="No-Break"><st c="31163">as values:</st></span><pre class="source-code"><st c="31173">
intervals = {
    "Population": [0, 200, 500, 1000, 2000, np.inf],
    "MedInc": [</st><a id="_idTextAnchor585"/><st c="31248">0, 2, 4, 6, np.inf]}</st></pre></li>				<li><st c="31269">Let’s set up the discretizer with the limits from </st><span class="No-Break"><em class="italic"><st c="31320">step 11</st></em></span><span class="No-Break"><st c="31327">:</st></span><pre class="source-code"><st c="31329">
discretizer = ArbitraryDiscretiser(
    binning_dict=intervals, return_boundaries=True)</st></pre></li>				<li><st c="31413">Now, we can go ahead and discretize </st><span class="No-Break"><st c="31450">the variables:</st></span><pre class="source-code"><st c="31464">
X_t = discretizer.fit_transform(X)</st></pre><p class="list-inset"><st c="31499">If we execute </st><strong class="source-inline"><st c="31514">X_t.head()</st></strong><st c="31524">, we will see the following output, where the </st><strong class="source-inline"><st c="31570">Population</st></strong><st c="31580"> and </st><strong class="source-inline"><st c="31585">MedInc</st></strong><st c="31591"> var</st><a id="_idTextAnchor586"/><st c="31595">iables have </st><span class="No-Break"><st c="31608">been discretized:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B22396_04_8.jpg" alt="Figure 4.8 – A DataFrame containing the discretized variables"/><st c="31625"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="32062">Figure 4.8 – A DataFrame containing the discretized variables</st></p>
			<p class="list-inset"><st c="32123">The advantage</st><a id="_idIndexMarker314"/><st c="32137"> of using </st><strong class="source-inline"><st c="32147">feature-engine</st></strong><st c="32161"> is that we </st><a id="_idIndexMarker315"/><st c="32173">can discretize multiple variables at the same time and apply arbitrary discretization as p</st><a id="_idTextAnchor587"/><a id="_idTextAnchor588"/><st c="32263">art of a </st><span class="No-Break"><st c="32273">scikit-learn </st></span><span class="No-Break"><strong class="source-inline"><st c="32286">Pipeline</st></strong></span><span class="No-Break"><st c="32294">.</st></span></p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor589"/><st c="32295">How it works...</st></h2>
			<p><st c="32311">In this recipe, we sorted the values of a variable into user-defin</st><a id="_idTextAnchor590"/><st c="32378">ed intervals. </st><st c="32393">First, we plotted a histogram of the </st><strong class="source-inline"><st c="32430">Population</st></strong><st c="32440"> variable to get an idea of its va</st><a id="_idTextAnchor591"/><st c="32474">lue range. </st><st c="32486">Next, we arbitrarily determined the limits of the intervals and captured them in a list. </st><st c="32575">We created intervals that included 0–200, 200–500, 500–1000, 1000–2000, and more than 2,000 by setting the upper limit to infinite with </st><strong class="source-inline"><st c="32711">np.inf</st></strong><st c="32717">. Next, we created a list with the interval names as strings. </st><st c="32779">Using pandas </st><strong class="source-inline"><st c="32792">cut()</st></strong><st c="32797"> and passing the list with the interval limits, we sorted the variable values into the pre-defined bins. </st><st c="32902">We executed the command twice; in the first run, we set the </st><strong class="source-inline"><st c="32962">labels</st></strong><st c="32968"> argument to </st><strong class="source-inline"><st c="32981">None</st></strong><st c="32985">, returning the interval limits as a result. </st><st c="33030">In the second run, we set the </st><strong class="source-inline"><st c="33060">labels</st></strong><st c="33066"> argument to the list of strings. </st><st c="33100">We captured the returned output in two variables: the first one displays the interval limits as values and the second one has strings as values. </st><st c="33245">Finally, we counted the number of observations per variable using </st><span class="No-Break"><st c="33311">pandas </st></span><span class="No-Break"><strong class="source-inline"><st c="33318">value_counts()</st></strong></span><span class="No-Break"><st c="33332">.</st></span></p>
			<p><st c="33333">Finally, we automated the procedure with </st><strong class="source-inline"><st c="33375">feature-engine</st></strong><st c="33389">’s </st><strong class="source-inline"><st c="33393">ArbitraryDiscretiser()</st></strong><st c="33415">. This transformer takes a dictionary with the variables to discretize as keys and the interval limits in a list as values, and then uses pandas </st><strong class="source-inline"><st c="33560">cut()</st></strong><st c="33565"> under the hood to discretize the variables. </st><st c="33610">With </st><strong class="source-inline"><st c="33615">fit()</st></strong><st c="33620">, the transformer does not learn any parameters but checks that the variables are numerical. </st><st c="33713">With </st><strong class="source-inline"><st c="33718">transform()</st><a id="_idTextAnchor592"/><a id="_idTextAnchor593"/></strong><st c="33729">, it discretizes </st><span class="No-Break"><st c="33746">the variables.</st></span></p>
			<h1 id="_idParaDest-132"><st c="33760">Performing discretization </st><a id="_idTextAnchor594"/><st c="33787">with k-means clustering</st></h1>
			<p><st c="33810">The aim o</st><a id="_idTextAnchor595"/><st c="33820">f a discretization </st><a id="_idIndexMarker316"/><st c="33840">procedure is to find a set of cut points that</st><a id="_idIndexMarker317"/><st c="33885"> partition a variable into a small number of intervals that have good class coherence. </st><st c="33972">To create partitions that group similar observations, we can use clustering algorithms such </st><span class="No-Break"><st c="34064">as k-means.</st></span></p>
			<p><st c="34075">In</st><a id="_idIndexMarker318"/><st c="34078"> discretization using k-means clustering, the partitions are the clusters identified by the k-means algorithm. </st><st c="34189">The k-means clustering algorithm has two main steps. </st><st c="34242">In the initialization step, </st><em class="italic"><st c="34270">k</st></em><st c="34271"> observations are chosen randomly as the initial centers of the </st><em class="italic"><st c="34335">k</st></em><st c="34336"> clusters, and the remaining data points are assigned to the closest cluster. </st><st c="34414">The proximity to the cluster is measured by a distance measure, such as the Euclidean distance. </st><st c="34510">In the iteration step, the centers of the clusters are re-computed as the average of all of the observations within the cluster, and the observations are reassigned to the newly created closest cluster. </st><st c="34713">The iteration step continues until the optimal </st><em class="italic"><st c="34760">k</st></em><st c="34761"> centers </st><span class="No-Break"><st c="34770">are found.</st></span></p>
			<p><st c="34780">Discretization with k-means requires one parameter, which is </st><em class="italic"><st c="34842">k</st></em><st c="34843">, the number of clusters. </st><st c="34869">There are a few methods to determine the optimal number of clusters. </st><st c="34938">One of them is the elbow method, which we will use in this recipe. </st><st c="35005">This m</st><a id="_idTextAnchor596"/><st c="35011">ethod consists of training s</st><a id="_idTextAnchor597"/><st c="35040">everal k-means algorithms over the data using different values of </st><em class="italic"><st c="35107">k</st></em><st c="35108">, and then determining the explained variation returned by the clustering. </st><st c="35183">In the next step, we plot the explained variation as a function of the number of clusters, </st><em class="italic"><st c="35274">k</st></em><st c="35275">, and pick the </st><em class="italic"><st c="35290">elbow</st></em><st c="35295"> of the curve as the number of clusters to use. </st><st c="35343">The elbow is the inflection point that indicates that increasing the number of </st><em class="italic"><st c="35422">k</st></em><st c="35423"> further does not significantly increase the variance explained by the model. </st><st c="35501">There are different metrics to quantify the explained variation. </st><st c="35566">We will use the sum of the square distances from each point to its </st><span class="No-Break"><st c="35633">assigned center.</st></span></p>
			<p><st c="35649">In this recipe, we will use the Python library </st><strong class="source-inline"><st c="35697">yellowbrick</st></strong><st c="35708"> to determine the optimal number of clusters and then carry out k-means</st><a id="_idTextAnchor598"/><a id="_idTextAnchor599"/><st c="35779"> discretization </st><span class="No-Break"><st c="35795">with scikit-learn.</st></span></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor600"/><st c="35813">How to do it...</st></h2>
			<p><st c="35829">Let’s start by importing the necessary Python libraries and get the </st><span class="No-Break"><st c="35898">dataset ready:</st></span></p>
			<ol>
				<li><st c="35912">Import </st><a id="_idIndexMarker319"/><st c="35920">the required Python libraries </st><span class="No-Break"><st c="35950">and classes:</st></span><pre class="source-code"><st c="35962">
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer
from yellowbrick.cluster import KElbowVisualizer</st></pre></li>				<li><st c="36224">Let’s </st><a id="_idIndexMarker320"/><st c="36231">load the California housing dataset into a </st><span class="No-Break"><strong class="source-inline"><st c="36274">pandas</st></strong></span><span class="No-Break"><st c="36280"> DataFrame:</st></span><pre class="source-code"><st c="36291">
X, y = fetch_california_housing(
    return_</st><a id="_idTextAnchor601"/><st c="36332">X_y=True, as_fram</st><a id="_idTextAnchor602"/><st c="36350">e=True)</st></pre></li>				<li><st c="36358">The k-means optimal clusters should be determined using the train set, so let’s divide the data into train and </st><span class="No-Break"><st c="36470">test sets:</st></span><pre class="source-code"><st c="36480">
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="36570">Let’s make a list with the variables </st><span class="No-Break"><st c="36608">to transform:</st></span><pre class="source-code"><st c="36621">
variables = ['MedInc', 'HouseAge', 'AveRooms']</st></pre></li>				<li><st c="36668">Let’s set up a k-means </st><span class="No-Break"><st c="36692">clustering algorithm:</st></span><pre class="source-code"><st c="36713">
k_means = KMeans(random_state=10)</st></pre></li>				<li><st c="36747">Now, using Yellowbrick’s visualizer and the elbow method, let’s find the optimal number of clusters for </st><span class="No-Break"><st c="36852">each variable:</st></span><pre class="source-code"><st c="36866">
for variable in variables:
    # set up a visualizer
    visualizer = KElbowVisualizer(
        k_means, k=(4,12),
        metric='distortion',
        timings=False)
    visualizer.fit(X_train[variable].to_f</st><a id="_idTextAnchor603"/><st c="37039">rame())
    visualiz</st><a id="_idTextAnchor604"/><st c="37056">er.show()</st></pre><p class="list-inset"><st c="37066">In the </st><a id="_idIndexMarker321"/><st c="37074">following </st><a id="_idIndexMarker322"/><st c="37084">plots, we see that the optimal number of clusters is six for the first t</st><a id="_idTextAnchor605"/><st c="37156">wo variables and seven for </st><span class="No-Break"><st c="37184">the third:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B22396_04_9.jpg" alt="Figure 4.9 – The number of clusters versus the explained variation for the MedInc, HouseAge, and AveRooms variables, from top to bottom"/><st c="37194"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="37557">Figure 4.9 – The number of clusters versus the explained variation for the MedInc, HouseAge, and AveRooms variables, from top to bottom</st></p>
			<ol>
				<li value="7"><st c="37692">Let’s set </st><a id="_idIndexMarker323"/><st c="37703">up a discretizer that</st><a id="_idIndexMarker324"/><st c="37724"> uses k-means clustering to create six partitions and returns the clusters as </st><span class="No-Break"><st c="37802">one-hot-encoded variables:</st></span><pre class="source-code"><st c="37828">
disc = KBinsDiscretizer(
    n_bins=6,
    encode="onehot-dense",
    strategy="kmeans",
    subsample=None,
).set_output(transform="pandas")</st></pre></li>				<li><st c="37954">Let’s fit </st><a id="_idIndexMarker325"/><st c="37965">the discretizer to the slice of the</st><a id="_idIndexMarker326"/><st c="38000"> DataFrame that contains the variables to discretize so that it finds the clusters for </st><span class="No-Break"><st c="38087">each variable:</st></span><pre class="source-code"><st c="38101">
disc.fit(X_train[variables])</st></pre></li>			</ol>
			<p class="callout-heading"><st c="38130">Note</st></p>
			<p class="callout"><st c="38135">In this recipe, we sort the values of all three of the variables into six clusters. </st><st c="38220">To discretize </st><strong class="source-inline"><st c="38234">MedInc</st></strong><st c="38240"> and </st><strong class="source-inline"><st c="38245">HouseAge</st></strong><st c="38253"> into six partitions and </st><strong class="source-inline"><st c="38278">AveRooms</st></strong><st c="38286"> into seven, we would set up one instance of the discretizer for each variable group and use the </st><strong class="source-inline"><st c="38383">ColumnTransformer()</st></strong><st c="38402"> to restrict the discretization to </st><span class="No-Break"><st c="38437">each group.</st></span></p>
			<ol>
				<li value="9"><st c="38448">Let’s inspect the </st><span class="No-Break"><st c="38467">cut points:</st></span><pre class="source-code"><st c="38478">
disc.bin_edges_</st></pre><p class="list-inset"><st c="38494">Each array contains the cut points for the six clusters for </st><strong class="source-inline"><st c="38555">MedInc</st></strong><st c="38561">, </st><strong class="source-inline"><st c="38563">HouseAge</st></strong><st c="38571">, </st><span class="No-Break"><st c="38573">and </st></span><span class="No-Break"><strong class="source-inline"><st c="38577">AveRooms</st></strong></span><span class="No-Break"><st c="38585">:</st></span></p><pre class="source-code"><strong class="bold"><st c="38587">array([array([0.4999, 2.49587954, 3.66599029, 4.95730115, 6.67700141, 9.67326677, 15.0001]),</st></strong>
<strong class="bold"><st c="38679">array([1., 11.7038878, 19.88430419, 27.81472503, 35.39424098, 43.90930314, 52.]),</st></strong>
<strong class="bold"><st c="38761">array([0.84615385, 4.84568771, 6.62222005, 15.24138445, 37.60664483, 92.4473438, 132.53333333])], dtype=object)</st></strong></pre></li>				<li><st c="38873">Let’s obtain the discretized form of the variables in the train </st><span class="No-Break"><st c="38938">test sets:</st></span><pre class="source-code"><st c="38948">
train_features = disc.transform(X_train[variables])
test_features = disc.transform(X_test[variables])</st></pre><p class="list-inset"><st c="39050">With </st><strong class="source-inline"><st c="39056">print(test_features)</st></strong><st c="39076">, we can </st><a id="_idTextAnchor606"/><st c="39085">inspect the DataFrame that is returned by the discretizer. </st><st c="39144">It contains 18 binary variables correspond</st><a id="_idTextAnchor607"/><st c="39186">ing to the one-hot-encoded </st><a id="_idIndexMarker327"/><st c="39214">transformation </st><a id="_idIndexMarker328"/><st c="39229">of the six clusters returned for each of the three </st><span class="No-Break"><st c="39280">numerical variables:</st></span></p><pre class="source-code"><strong class="bold"><st c="39300">         MedInc_0.0  MedInc_1.0  MedInc_2.0  MedInc_3.0  MedInc_4.0  MedInc_5.0  \</st></strong>
<strong class="bold"><st c="39368">14740            0.0            0.0            </st></strong><strong class="bold"><st c="39383">1.0            0.0            0.0            0.0</st></strong>
<strong class="bold"><st c="39398">10101            0.0            0.0            0.0            1.0            0.0            </st></strong><strong class="bold"><st c="39425">0.0</st></strong>
<strong class="bold"><st c="39428">20566            0.0            0.0            1.0            0.0            0.0            0.0</st></strong>
<strong class="bold"><st c="39458">2670              1.0            </st></strong><strong class="bold"><st c="39468">0.0            0.0            0.0            0.0            0.0</st></strong>
<strong class="bold"><st c="39487">15709            0.0            0.0            0.0            1.0            </st></strong><strong class="bold"><st c="39510">0.0            0.0</st></strong>
<strong class="bold"><st c="39517">         HouseAge_0.0  HouseAge_1.0  HouseAge_2.0  HouseAge_3.0  HouseAge_4.0  \</st></strong>
<strong class="bold"><st c="39584">14740               0.0</st></strong><strong class="bold"><st c="39594">               0.0               1.0               0.0               0.0</st></strong>
<strong class="bold"><st c="39610">10101               0.0               0.0               0.0               1.0               0.0</st></strong>
<strong class="bold"><st c="39636">20566               0.0               0.0               0.0               1.0               0.0</st></strong>
<strong class="bold"><st c="39662">2670                 0.0               0.0               0.0</st></strong><strong class="bold"><st c="39679">               0.0               1.0</st></strong>
<strong class="bold"><st c="39687">15709               0.0               0.0               1.0               0.0               0.0</st></strong>
<strong class="bold"><st c="39713">         HouseAge_5.0</st></strong><strong class="bold"><st c="39726">  AveRooms_0.0  AveRooms_1.0  AveRooms_2.0  AveRooms_3.0  \</st></strong>
<strong class="bold"><st c="39780">14740               0.0               0.0               1.0               0.0               </st></strong><strong class="bold"><st c="39803">0.0</st></strong>
<strong class="bold"><st c="39806">10101               0.0               0.0               1.0               0.0               0.0</st></strong>
<strong class="bold"><st c="39832">20566               0.0               0.0               </st></strong><strong class="bold"><st c="39847">1.0               0.0               0.0</st></strong>
<strong class="bold"><st c="39858">2670                 0.0               0.0               1.0               0.0               0.0</st></strong>
<strong class="bold"><st c="39883">15709               </st></strong><strong class="bold"><st c="39890">0.0               1.0               0.0               0.0               0.0</st></strong>
<strong class="bold"><st c="39909">         AveRooms_4.0  AveRooms_5.0</st></strong>
<strong class="bold"><st c="39935">14740               0.0               </st></strong><strong class="bold"><st c="39946">0.0</st></strong>
<strong class="bold"><st c="39949">10101               0.0               0.0</st></strong>
<strong class="bold"><st c="39963">20566               0.0               0.0</st></strong>
<strong class="bold"><st c="39977">2670                 0.0               0.0</st></strong>
<strong class="bold"><st c="39990">15709               0.0               0.0</st></strong></pre></li>			</ol>
			<p><st c="40004">You</st><a id="_idIndexMarker329"/><st c="40008"> can </st><a id="_idIndexMarker330"/><st c="40013">concatenate the result to the original DataFrame using </st><strong class="source-inline"><st c="40068">pandas</st></strong><st c="40074"> and then drop the original numerical variables. </st><st c="40123">Alternatively, use the </st><strong class="source-inline"><st c="40146">ColumnTransformer()</st></strong><st c="40165"> class to restrict the discretization to the selected variables and add the result to th</st><a id="_idTextAnchor608"/><a id="_idTextAnchor609"/><st c="40253">e data by setting </st><strong class="source-inline"><st c="40272">remainder</st></strong> <span class="No-Break"><st c="40281">to </st></span><span class="No-Break"><strong class="source-inline"><st c="40285">"passthrough"</st></strong></span><span class="No-Break"><st c="40298">.</st></span></p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor610"/><st c="40299">How it works...</st></h2>
			<p><st c="40315">In this recipe, we performed discretization</st><a id="_idIndexMarker331"/><st c="40359"> with k-means clustering. </st><st c="40385">First</st><a id="_idTextAnchor611"/><st c="40390">, we identified the optimal number of clusters utilizing the elbow method by using </st><span class="No-Break"><st c="40473">Yellowbrick’s </st></span><span class="No-Break"><strong class="source-inline"><st c="40487">KElbowVisualizer()</st></strong></span><span class="No-Break"><st c="40505">.</st></span></p>
			<p><st c="40506">To perform k-means</st><a id="_idIndexMarker332"/><st c="40525"> discretization, we used scikit-learn’s </st><strong class="source-inline"><st c="40565">KBinsDiscretizer()</st></strong><st c="40583">, setting </st><strong class="source-inline"><st c="40593">strategy</st></strong><st c="40601"> to </st><strong class="source-inline"><st c="40605">kmeans</st></strong><st c="40611"> and the number of clusters to six in the </st><strong class="source-inline"><st c="40653">n_bins</st></strong><st c="40659"> argument. </st><st c="40670">With </st><strong class="source-inline"><st c="40675">fit()</st></strong><st c="40680">, the transformer learned the cluster boundaries using the k-means algorithm. </st><st c="40758">With </st><strong class="source-inline"><st c="40763">transform()</st></strong><st c="40774">, it sorted the variable values to their corresponding cluster. </st><st c="40838">We set </st><strong class="source-inline"><st c="40845">encode</st></strong><st c="40851"> to </st><strong class="source-inline"><st c="40855">"onehot-dense"</st></strong><st c="40869">; hence, after the discretization, the transformer applied one-hot encoding to the clusters. </st><st c="40963">We also set the output of the discretizer to </st><strong class="source-inline"><st c="41008">pandas</st></strong><st c="41014">, and with that, the transformer returned the one-hot encoded ver</st><a id="_idTextAnchor612"/><a id="_idTextAnchor613"/><st c="41079">sion of the clustered variables as </st><span class="No-Break"><st c="41115">a DataFrame.</st></span></p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor614"/><st c="41127">See also</st></h2>
			<ul>
				<li><st c="41136">Discretization with k-means is described in the article found  in </st><em class="italic"><st c="41202">Palaniappan and Hong, Discretization of Continuous Valued Dimensions in OLAP Data Cube</st></em><st c="41288">s. </st><st c="41292">International Journal of Computer Science and Network Security, VOL.8 No.11, November </st><span class="No-Break"><st c="41378">2008. </st></span><a href="http://paper.ijcsns.org/07_book/200811/20081117.pdf"><span class="No-Break"><st c="41384">http://paper.i</st><span id="_idTextAnchor615"/><st c="41398">jcsns.org/07_book/200811/20081117.pdf</st></span></a><span class="No-Break"><st c="41436">.</st></span></li>
				<li><st c="41437">To learn more about the elbow method, visit Yellowbrick’s documentation and references </st><span class="No-Break"><st c="41525">at </st></span><a href="https://www.scikit-yb.org/en/latest/api/cluster/elbow.html"><span class="No-Break"><st c="41528">https://www.scikit-yb.org/en/latest/api/cluster/elbow.html</st></span></a><span class="No-Break"><st c="41586">.</st></span></li>
				<li><st c="41587">For other ways of determining the fit of k-means clustering, check out the additional visualizers in Yellowbrick </st><span class="No-Break"><st c="41701">at </st><a id="_idTextAnchor616"/></span><a href="https://www.scikit-yb.org/en/latest/api/cluster/index.html"><span class="No-Break"><st c="41704">https://www.</st><span id="_idTextAnchor617"/><span id="_idTextAnchor618"/><st c="41716">scikit-yb.org/en/latest/api/cluster/index.html</st></span></a><span class="No-Break"><st c="41763">.</st></span></li>
			</ul>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor619"/><st c="41764">Implementing feature binarization</st></h1>
			<p><st c="41798">Some datasets</st><a id="_idIndexMarker333"/><st c="41812"> contain sparse variables. </st><st c="41839">Sparse variables are those where the majority of the values are 0. </st><st c="41906">The classical example of sparse va</st><a id="_idTextAnchor620"/><st c="41940">riables are those derived from text data through the bag-of-words model, where each variable is a word and each value represents the number of times the word appears in a certain document. </st><st c="42130">Given that a document contains a limited number of words, whereas the feature space contains the words that appear across all documents, most documents, that is, most rows, will show a value of 0 for most columns. </st><st c="42344">However, words are not the </st><a id="_idIndexMarker334"/><st c="42371">sole example. </st><st c="42385">If we think about house details data, the </st><em class="italic"><st c="42427">number of saunas</st></em><st c="42443"> variable will also be 0 for most houses. </st><st c="42485">In summary, some variables have very skewed distributions, where most observations show the same value, usually 0, and only a few observations show different, usually </st><span class="No-Break"><st c="42652">higher, values.</st></span></p>
			<p><st c="42667">For a simpler representation of these sparse or highly skewed variables, we can binarize them by clipping all values greater than 1 to 1. </st><st c="42806">In fact, binarization is commonly performed on text count data, where we consider the presence or absence of a feature rather than a quantified number of occurrences of </st><span class="No-Break"><st c="42975">a word.</st></span></p>
			<p><st c="42982">In this recipe,</st><a id="_idTextAnchor621"/><a id="_idTextAnchor622"/><st c="42998"> we will perform binarizat</st><a id="_idTextAnchor623"/><st c="43024">ion </st><span class="No-Break"><st c="43029">using </st></span><span class="No-Break"><strong class="source-inline"><st c="43035">scikit-learn</st></strong></span><span class="No-Break"><st c="43047">.</st></span></p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor624"/><st c="43048">Getting ready</st></h2>
			<p><st c="43062">We will use a dataset consisting of a bag of words, which is available in the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Bag+of+Words). </st><st c="43229">It is licensed under CC BY </st><span class="No-Break"><st c="43256">4.0 (</st></span><a href="https://creativecommons.org/licenses/by/4.0/legalcode"><span class="No-Break"><st c="43261">https://creativecommons.org/licenses/by/4.0/legalcode</st></span></a><span class="No-Break"><st c="43315">).</st></span></p>
			<p><st c="43318">I downloaded and prepared a small bag of words representing a simplified version of one of those datasets. </st><st c="43426">You will find this dataset in the accompanying </st><span class="No-Break"><st c="43473">GitHub repository:</st></span></p>
			<p><span class="No-Break"><st c="43491">https://github.com/PacktPublishing/Python-Feature-Engineering-Coo</st><a id="_idTextAnchor625"/><a id="_idTextAnchor626"/><st c="43557">kbook-Third-Edition/tr</st><a id="_idTextAnchor627"/><st c="43580">ee/main/ch04-discretization</st></span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor628"/><st c="43608">How to do it...</st></h2>
			<p><st c="43624">Let’s begin by importing the libraries and loading </st><span class="No-Break"><st c="43676">the data:</st></span></p>
			<ol>
				<li><st c="43685">Let’s import the required Python libraries, classes, </st><span class="No-Break"><st c="43739">and datasets:</st></span><pre class="source-code"><st c="43752">
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import Binarizer</st></pre></li>				<li><st c="43901">Let’s load the bag of words dataset, which contains words as columns and different texts </st><span class="No-Break"><st c="43991">as rows:</st></span><pre class="source-code"><st c="43999">
data = pd.read_csv("bag_of_words.csv")</st></pre></li>				<li><st c="44038">Let’s</st><a id="_idIndexMarker335"/><st c="44044"> display histograms to visualize the sparsity of </st><span class="No-Break"><st c="44093">the variables:</st></span><pre class="source-code"><st c="44107">
data.hist(bins=30, figsize=(20, 20), layout=(3,4))
plt.show()</st></pre><p class="list-inset"><st c="44169">In the following histograms, we can see that the dif</st><a id="_idTextAnchor629"/><st c="44222">ferent words appear zero times in </st><span class="No-Break"><st c="44257">most documents:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B22396_04_10.jpg" alt="Figure 4.10 – Histograms representing th﻿e number of times each word appears in a document"/><st c="44272"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="44633">Figure 4.10 – Histograms representing th</st><a id="_idTextAnchor630"/><st c="44673">e number of times each word appears in a document</st></p>
			<ol>
				<li value="4"><st c="44723">Let’s </st><a id="_idIndexMarker336"/><st c="44730">set up </st><strong class="source-inline"><st c="44737">binarizer</st></strong><st c="44746"> to clip all values greater than 1 to 1 and return DataFrames as </st><span class="No-Break"><st c="44811">a result:</st></span><pre class="source-code"><st c="44820">
binarizer = Binarizer(th</st><a id="_idTextAnchor631"/><st c="44845">reshold = 0) .set_output(transform="pandas")</st></pre></li>				<li><st c="44890">Let’s binarize </st><span class="No-Break"><st c="44906">the variables:</st></span><pre class="source-code"><st c="44920">
data_t = binarizer.fit_transform(data)</st></pre><p class="list-inset"><st c="44959">Now we can explore the distribution of the binarized variables by displaying the histograms as </st><a id="_idTextAnchor632"/><st c="45055">in </st><em class="italic"><st c="45058">step 3</st></em><st c="45064">, or better, by creating </st><span class="No-Break"><st c="45089">bar plots.</st></span></p></li>				<li><st c="45099">Let’s</st><a id="_idIndexMarker337"/><st c="45105"> create a bar plot with the number of observations per bin </st><span class="No-Break"><st c="45164">per variable:</st></span><pre class="source-code"><st c="45177">
variables = data_t.columns.to_list()
plt.figure(figsize=(20, 20), constrained_layout=True)
for i in range(10):
    ax = plt.subplot(3, 4, i + 1)
    var = variables[i]
    t = data_t[var].value_counts(normalize=True)
    t.plot.bar(ax=ax)
    plt.xticks(rotation=0)
    plt.ylabel("Observations per bin")
    ax.set_title(var)
plt.show()</st></pre><p class="list-inset"><st c="45487">In the following plot, we can see the binarized v</st><a id="_idTextAnchor633"/><st c="45537">ariables, where most occurrences show the </st><span class="No-Break"><strong class="source-inline"><st c="45580">0</st></strong></span><span class="No-Break"><st c="45581"> value:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B22396_04_11.jpg" alt="Figure 4.11 – Bar plots containing the number of documents that eithe﻿r show each one of the words or not"/><st c="45588"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="46228">Figure 4.11 – Bar plots containing the number of documents that eithe</st><a id="_idTextAnchor634"/><st c="46297">r show each one of the words or not</st></p>
			<p><st c="46333">That’s it;</st><a id="_idTextAnchor635"/><a id="_idTextAnchor636"/><st c="46344"> now</st><a id="_idIndexMarker338"/><st c="46348"> we have a simpler representation of </st><span class="No-Break"><st c="46385">the data.</st></span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor637"/><st c="46394">How it works…</st></h2>
			<p><st c="46408">In this recipe, we changed the representation of sparse variables to consider the presence or absence of an occurrence, which, in our case, is a word. </st><st c="46560">The data consisted of a bag of words, where each variable (column) is a wor</st><a id="_idTextAnchor638"/><st c="46635">d, each row is a document, and the values represent the number of times the word appears in a document. </st><st c="46740">Most words do not appear in</st><a id="_idIndexMarker339"/><st c="46767"> most documents; therefore, most values in the data are 0. </st><st c="46826">We corroborated the sparsity of our data </st><span class="No-Break"><st c="46867">with histograms.</st></span></p>
			<p><st c="46883">scikit-learn’s </st><strong class="source-inline"><st c="46899">Binarizer()</st></strong><st c="46910"> mapped values greater than the threshold, which, in our case, was 0, to the </st><strong class="source-inline"><st c="46987">1</st></strong><st c="46988"> value, while values less than or equal to the threshold were mapped to 0. </st><strong class="source-inline"><st c="47063">Binarizer()</st></strong><st c="47074"> has the </st><strong class="source-inline"><st c="47083">fit()</st></strong><st c="47088"> and </st><strong class="source-inline"><st c="47093">transform()</st></strong><st c="47104"> methods, where </st><strong class="source-inline"><st c="47120">fit()</st></strong><st c="47125"> does not do anything and </st><strong class="source-inline"><st c="47151">transform()</st></strong><st c="47162"> binarizes </st><span class="No-Break"><st c="47173">the variables.</st></span></p>
			<p><strong class="source-inline"><st c="47187">Binarizer()</st></strong><st c="47199"> modifies all variables in a dataset returning NumPy arrays by default. </st><st c="47271">To return </st><strong class="source-inline"><st c="47281">pandas</st></strong><st c="47287"> DataFr</st><a id="_idTextAnchor639"/><a id="_idTextAnchor640"/><st c="47294">ames instead, we set the transform output </st><span class="No-Break"><st c="47337">to </st></span><span class="No-Break"><strong class="source-inline"><st c="47340">pandas</st></strong></span><span class="No-Break"><st c="47346">.</st></span></p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor641"/><st c="47347">Using decision trees for discretization</st></h1>
			<p><st c="47387">In all previous</st><a id="_idIndexMarker340"/><st c="47403"> recipes in this chapter, we determined the number of intervals arbitrarily, and then the discretization algorithm would find the interval limits one way or another. </st><st c="47569">Decision trees can find the interval limits and th</st><a id="_idTextAnchor642"/><st c="47619">e optimal number of </st><span class="No-Break"><st c="47640">bins autom</st><a id="_idTextAnchor643"/><st c="47650">atically.</st></span></p>
			<p><st c="47660">Decision</st><a id="_idIndexMarker341"/><st c="47669"> tree methods discretize continuous attributes during the learning process. </st><st c="47745">At each node, a decision tree evaluates all possible values of a feature and selects the cut point that maximizes the class separation, or sample coherence, by utilizing a performance metric such as entropy or Gini impurity for classification, or the squared or absolute error for regression. </st><st c="48038">As a result, the observations end up in certain leaves based on whether their feature values are greater or smaller than certain </st><span class="No-Break"><st c="48167">cut points.</st></span></p>
			<p><st c="48178">In the following figure, we can see the diagram of a decision tree that is trained to predict house prices based on the property’s average number </st><span class="No-Break"><st c="48325">of rooms:</st></span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B22396_04_12.jpg" alt="Figure 4.12 – A diagram of a decision tree trained to predict house price based on the property’s average number of rooms"/><st c="48334"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="48749">Figure 4.12 – A diagram of a decision tree trained to predict house price based on the property’s average number of rooms</st></p>
			<p><st c="48870">Based on this decision tree, houses with a smaller mean number of rooms than 5.5 will go to the first leaf, houses with a mean number of rooms between 5.5 and 6.37 will fall into the second leaf, houses with mean values between 6.37 and 10.77 will end up in the third leaf, and </st><a id="_idIndexMarker342"/><st c="49149">houses with mean values greater than 10.77 will land in the </st><span class="No-Break"><st c="49209">fourth leaf.</st></span></p>
			<p><st c="49221">As you see, by design, decision trees</st><a id="_idIndexMarker343"/><st c="49259"> can find the set of cut points that partition a variable into intervals with good </st><span class="No-Break"><st c="49342">class coherence.</st></span></p>
			<p><st c="49358">In this recipe, </st><a id="_idTextAnchor644"/><st c="49375">we will perform d</st><a id="_idTextAnchor645"/><a id="_idTextAnchor646"/><st c="49392">ecision tree-based discretization </st><span class="No-Break"><st c="49427">using Feature-engine.</st></span></p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor647"/><st c="49448">How to do it...</st></h2>
			<p><st c="49464">Let’s begin by importing some libraries and loading </st><span class="No-Break"><st c="49517">the data:</st></span></p>
			<ol>
				<li><st c="49526">Let’s import the required Python libraries, classes, </st><span class="No-Break"><st c="49580">and datasets:</st></span><pre class="source-code"><st c="49593">
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.tree import plot_tree
from feature_engine.discretisation import DecisionTreeDiscretiser</st></pre></li>				<li><st c="49853">Let’s load the California housing dataset into a </st><strong class="source-inline"><st c="49903">pandas</st></strong><st c="49909"> DataFrame and then split it into train and </st><span class="No-Break"><st c="49953">test sets:</st></span><pre class="source-code"><st c="49963">
X, y = fetch_california_housing(return_X_y=True,
    as_frame=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="50117">Let’s make a list with the names of the variables </st><span class="No-Break"><st c="50168">to discretize:</st></span><pre class="source-code"><st c="50182">
variables = list(X.columns)[:-2]</st></pre><p class="list-inset"><st c="50215">If we execute </st><strong class="source-inline"><st c="50230">print(variables)</st></strong><st c="50246">, we’ll see the following variable names: </st><strong class="source-inline"><st c="50288">['MedInc'</st></strong><st c="50297">, </st><strong class="source-inline"><st c="50299">'HouseAge'</st></strong><st c="50309">, </st><strong class="source-inline"><st c="50311">'AveRooms'</st></strong><st c="50321">, </st><strong class="source-inline"><st c="50323">'AveBedrms'</st></strong><st c="50334">, </st><strong class="source-inline"><st c="50336">'</st></strong><span class="No-Break"><strong class="source-inline"><st c="50337">Population'</st></strong></span><span class="No-Break"><st c="50348">, </st></span><span class="No-Break"><strong class="source-inline"><st c="50350">'AveOccup']</st></strong></span><span class="No-Break"><st c="50361">.</st></span></p></li>				<li><st c="50362">Let’s set </st><a id="_idIndexMarker344"/><st c="50373">up the transformer to discretize</st><a id="_idIndexMarker345"/><st c="50405"> the variables from </st><em class="italic"><st c="50425">step 3</st></em><st c="50431">. We want the transformer to optimize the hyperparameter’s maximum depth and minimum samples per leaf of each tree based on the negative mean square error metric using three-fold cross-validation. </st><st c="50628">As the output of the discretization, we want the limits of </st><span class="No-Break"><st c="50687">the intervals:</st></span><pre class="source-code"><st c="50701">
disc = DecisionTreeDiscretiser(
    bin_output="boundaries",
    precision=3,
    cv=3,
    scoring="neg_mean_squared_error",
    variables=variables,
    regression=True,
    param_grid={
        "max_depth": [1, 2, 3],
        "min_samples_leaf": [10, 20, 50]},
)</st></pre></li>				<li><st c="50923">Let’s fit the discretizer using the train set so that it finds the best decision trees for each of </st><span class="No-Break"><st c="51023">the variables:</st></span><pre class="source-code"><st c="51037">
disc.fit(X_train, y_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="51064">Note</st></p>
			<p class="callout"><st c="51069">You can inspect the limits of the found intervals for each variable in the </st><strong class="source-inline"><st c="51145">binner_dict_</st></strong><st c="51157"> attribute by executing </st><strong class="source-inline"><st c="51181">disc.binner_dict_</st></strong><st c="51198">. Note how the discretizer appended minus and plus infinity to the limits to accommodate smaller and greater values than those observed in the </st><span class="No-Break"><st c="51341">training set.</st></span></p>
			<ol>
				<li value="6"><st c="51354">Let’s discretize the variables and then display the first five rows of the transformed </st><span class="No-Break"><st c="51442">training set:</st></span><pre class="source-code"><st c="51455">
train_t = disc.transform(X_train)
test_t = disc.transform(X_test)
train_t[variables].head()</st></pre><p class="list-inset"><st c="51547">In the following</st><a id="_idIndexMarker346"/><st c="51564"> output, we can see the limits of the</st><a id="_idIndexMarker347"/><st c="51601"> intervals to which each observation </st><span class="No-Break"><st c="51638">was allocated:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B22396_04_13.jpg" alt="Figure 4.13 – The first five rows of the transformed training set containing the discretized variables"/><st c="51652"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="52162">Figure 4.13 – The first five rows of the transformed training set containing the discretized variables</st></p>
			<p class="callout-heading"><st c="52264">Note</st></p>
			<p class="callout"><st c="52269">If you choose to return the interval limits and want to use these datasets to train machine learning models, you will need to follow up the discretization with one-hot encoding or ordinal encoding. </st><st c="52468">Check the recipes in </st><a href="B22396_02.xhtml#_idTextAnchor182"><span class="No-Break"><em class="italic"><st c="52489">Chapter 2</st></em></span></a><st c="52498">, </st><em class="italic"><st c="52500">Encoding Categorical Variables</st></em><st c="52530">, for </st><span class="No-Break"><st c="52536">more details.</st></span></p>
			<ol>
				<li value="7"><st c="52549">Instead of returning the interval limits, we can return the interval number to which</st><a id="_idIndexMarker348"/><st c="52634"> each observation is allocated by setting</st><a id="_idIndexMarker349"/><st c="52675"> up the transformer </st><span class="No-Break"><st c="52695">like this:</st></span><pre class="source-code"><st c="52705">
disc = DecisionTreeDiscretiser(
    bin_output="bin_number",
    cv=3,
    scoring="neg_mean_squared_error",
    variables=variables,
    regression=True,
    param_grid={
        "max_depth": [1, 2, 3],
        "min_samples_leaf": [10, 20, 50]})</st></pre></li>				<li><st c="52912">We can now fit and then transform the training and </st><span class="No-Break"><st c="52964">testing sets:</st></span><pre class="source-code"><st c="52977">
train_t = disc.fit_transform(X_train, y_train)
test_t = disc.transform(X_test)</st></pre><p class="list-inset"><st c="53056">If you now execute </st><strong class="source-inline"><st c="53076">train_t[variables].head()</st></strong><st c="53101">, you will see integers as a result instead of the </st><span class="No-Break"><st c="53152">interval limits:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B22396_04_14.jpg" alt="Figure 4.14 – The first five rows of the transformed training set containing the discretized variables"/><st c="53168"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="53292">Figure 4.14 – The first five rows of the transformed training set containing the discretized variables</st></p>
			<p class="list-inset"><st c="53394">To wrap up the recipe, we will make the discretizer return the predictions of the trees as replacement values for the </st><span class="No-Break"><st c="53513">discretized variables:</st></span></p>
			<ol>
				<li value="9"><st c="53535">Let’s set up</st><a id="_idIndexMarker350"/><st c="53548"> the transformer to return the </st><a id="_idIndexMarker351"/><st c="53579">predictions, then fit it to the training set, and finally transform </st><span class="No-Break"><st c="53647">both datasets:</st></span><pre class="source-code"><st c="53661">
disc = DecisionTreeDiscretiser(
    bin_output="prediction",
    precision=1,
    cv=3,
    scoring="neg_mean_squared_error",
    variables=variables,
    regression=True,
    param_grid=
        {"max_depth": [1, 2, 3],
            "min_samples_leaf": [10, 20, 50]},
)
train_t = disc.fit_transform(X_train, y_train)
test_t = disc.transform(X_test)</st></pre></li>				<li><st c="53962">Let’s explore the number of unique values of the </st><strong class="source-inline"><st c="54012">AveRooms</st></strong><st c="54020"> variable before and after </st><span class="No-Break"><st c="54047">the discretization:</st></span><pre class="source-code"><st c="54066">
X_test["AveRooms"].nunique(), test_t["AveRooms"].nunique()</st></pre><p class="list-inset"><st c="54125">In the following output, we can see that the predictions of the decision trees are also discrete or finite because the trees contain a finite number of end leaves; </st><strong class="source-inline"><st c="54290">7</st></strong><st c="54291">, while the original</st><a id="_idIndexMarker352"/><st c="54311"> variable </st><a id="_idIndexMarker353"/><st c="54321">contained more than 6000 </st><span class="No-Break"><st c="54346">different values:</st></span></p><pre class="source-code"><st c="54363">(6034, 7)</st></pre></li>				<li><st c="54373">To better understand the structure of the tree, we can capture it into </st><span class="No-Break"><st c="54445">a variable:</st></span><pre class="source-code"><st c="54456">
tree = disc.binner_dict_["AveRooms"].best_estimator_</st></pre></li>			</ol>
			<p class="callout-heading"><st c="54509">Note</st></p>
			<p class="callout"><st c="54514">When we set the transformer to return integers or bin limits, we will obtain the bin limits in the </st><strong class="source-inline"><st c="54614">binner_dict_</st></strong><st c="54626"> attribute. </st><st c="54638">If we set the transformer to return the tree predictions, </st><strong class="source-inline"><st c="54696">binner_dict_</st></strong><st c="54708"> will contain the trained tree for </st><span class="No-Break"><st c="54743">each variable.</st></span></p>
			<ol>
				<li value="12"><st c="54757">Now, we can display the </st><span class="No-Break"><st c="54782">tree structure:</st></span><pre class="source-code"><st c="54797">
fig = plt.figure(figsize=(20, 6))
plot_tree(tree, fontsize=10, proportion=True)
plt.show()</st></pre></li>				<li><st c="54888">In the following figure, we can see the values used by the tree to allocate samples to the different end leaves based on the mean number </st><span class="No-Break"><st c="55026">of rooms:</st></span></li>
			</ol>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B22396_04_15.jpg" alt="Figure 4.15 – The structure of the decision tree trained to discretize AveRooms"/><st c="55035"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="55862">Figure 4.15 – The structure of the decision tree trained to discretize AveRooms</st></p>
			<ol>
				<li value="14"><st c="55941">To wrap up the recipe, we can plot the number of observations per bin for three of </st><span class="No-Break"><st c="56025">the </st></span><span class="No-Break"><a id="_idIndexMarker354"/></span><span class="No-Break"><st c="56029">variables:</st></span><pre class="source-code"><st c="56039">
plt.figure(figsize=(6, 12), constrained_layout=True)
for i in range(3):
    ax = plt.subplot(3, 1, i + 1)
    var = variables[i]
    t1 = train_t[var].value_counts(normalize=True)
    t2 = test_t[var].value_counts(normalize=True)
    tmp = pd.concat([t1, t2], axis=1)
    tmp.columns = ["train", "test"]
    tmp.sort_index(inplace=True)
    tmp.plot.bar(ax=ax)
    plt.xticks(rotation=0)
    plt.ylabel(</st><a id="_idTextAnchor648"/><st c="56403">"Observations per bin")
    ax.</st><a id="_idTextAnchor649"/><st c="56431">set_title(var)
plt.show()</st></pre><p class="list-inset"><st c="56457">We can s</st><a id="_idTextAnchor650"/><st c="56466">ee the </st><a id="_idIndexMarker355"/><st c="56474">number of observations per bin in the </st><span class="No-Break"><st c="56512">following output:</st></span></p></li>			</ol>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B22396_04_16.jpg" alt="Figure 4.16 – The proportion of observations ﻿per bin after discretizing the variables with decision trees"/><st c="56529"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="56827">Figure 4.16 – The proportion of observations </st><a id="_idTextAnchor651"/><st c="56872">per bin after discretizing the variables with decision trees</st></p>
			<p class="list-inset"><st c="56932">As </st><a id="_idIndexMarker356"/><st c="56936">evidenced in </st><a id="_idIndexMarker357"/><st c="56949">the plots, discretization with decision tree</st><a id="_idTextAnchor652"/><st c="56993">s </st><a id="_idTextAnchor653"/><a id="_idTextAnchor654"/><st c="56996">returns a different fraction of observations at each node </st><span class="No-Break"><st c="57054">or bin.</st></span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor655"/><st c="57061">How it works...</st></h2>
			<p><st c="57077">To perform discretization with decision trees, we used </st><span class="No-Break"><st c="57133">f</st></span><span class="No-Break"><strong class="source-inline"><st c="57134">eature-engine</st></strong></span><span class="No-Break"><st c="57147">’s </st></span><span class="No-Break"><strong class="source-inline"><st c="57151">Decision</st></strong></span><strong class="source-inline"><st c="57159">
TreeDiscretiser()</st></strong><st c="57177">. This transformer fitted a decision tree using each variable to discretize as input and optimized the hyperparameters of the model to find the best </st><a id="_idIndexMarker358"/><st c="57326">partitions </st><a id="_idIndexMarker359"/><st c="57337">based on a performance metric. </st><st c="57368">It automatically found the optimal number of intervals, as well as their limits, returning</st><a id="_idTextAnchor656"/><a id="_idTextAnchor657"/><a id="_idTextAnchor658"/><st c="57458"> either the limits, the bin number, or the predictions as </st><span class="No-Break"><st c="57516">a result.</st></span></p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor659"/><st c="57525">There’s more...</st></h2>
			<p><st c="57541">The implementation of </st><strong class="source-inline"><st c="57564">feat</st><a id="_idTextAnchor660"/><st c="57568">ure-engine</st></strong><st c="57579"> is inspired by the winning solution of the KDD 2009 data science competition. </st><st c="57658">The winners created new features by obtaining predictions of decision trees based on continuous features. </st><st c="57764">You can find more details in the </st><em class="italic"><st c="57797">Winning the KDD Cup Orange Challenge with Ensemble Selection</st></em><st c="57857"> article on </st><em class="italic"><st c="57869">page 27</st></em><st c="57876"> of the article series </st><span class="No-Break"><st c="57899">at </st></span><a href="http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf"><span class="No-Break"><st c="57902">http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf</st></span></a><span class="No-Break"><st c="57957">.</st></span></p>
			<p><st c="57958">For a review of discretization techniques, you might find the following </st><span class="No-Break"><st c="58031">articles useful:</st></span></p>
			<ul>
				<li><st c="58047">Dougherty et al, </st><em class="italic"><st c="58065">Supervised and Unsupervised Discretization of Continuous Features, Machine Learning: Proceedings of the 12th International Conference</st></em><st c="58198">, </st><span class="No-Break"><st c="58200">1995, (</st></span><a href="https://ai.stanford.edu/~ronnyk/disc.pdf"><span class="No-Break"><st c="58207">https://ai.stanford.edu/~ronnyk/disc.pdf</st></span></a><span class="No-Break"><st c="58248">).</st></span></li>
				<li><st c="58251">Lu et al, </st><em class="italic"><st c="58262">Discretization: An Enabling Technique, Data Mining, and Knowledge Discovery</st></em><st c="58337">, 6, 393–423, </st><span class="No-Break"><st c="58351">2002, (</st></span><a href="https://www.researchgate.net/publication/220451974_Discretization_An_Enabling_Technique"><span class="No-Break"><st c="58358">https://www.researchgate.net/publication/220451974_Discretization_An_Enabling_Technique</st></span></a><span class="No-Break"><st c="58446">).</st></span></li>
				<li><st c="58449">Garcia et al, </st><em class="italic"><st c="58464">A Survey of Discretization Techniques: Taxonomy and Empirical Analysis in Supervised Learning, IEEE Transactions on Knowledge in Data</st><a id="_idTextAnchor661"/><st c="58597"> Engineering 25 (4)</st></em><st c="58616">, </st><span class="No-Break"><st c="58618">2013, (</st></span><a href="https://ieeexplore.ieee.org/document/6152258"><span class="No-Break"><st c="58625">https://ieeexplore.ieee.org/document/6152258</st></span></a><span class="No-Break"><st c="58670">).</st></span></li>
			</ul>
		</div>
	<div id="charCountTotal" value="58673"/></body></html>