<html><head></head><body>
		<div id="_idContainer077">
			<h1 class="chapter-number"><a id="_idTextAnchor074"/>6</h1>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor075"/>Processing Data in Machine Learning Systems</h1>
			<p>We talked about data in <a href="B19548_03.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, where we introduced the types of data that are used in machine learning systems. In this chapter, we’ll dive deeper into ways in which data and algorithms are entangled. We’ll talk about data in generic terms, but in this chapter, we’ll explain what kind of data is needed in machine learning systems. I’ll explain the fact that all kinds of data are used in numerical form – either as a feature vector or as more complex feature matrices. Then, I’ll explain the need to transform unstructured data (for example, text) into structured data. This chapter will lay the foundations for diving deeper into each type of data, which is the content of the next <span class="No-Break">few chapters.</span></p>
			<p>In this chapter, we will do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Discuss the process of measurement (obtaining numerical data) and the measurement instruments that are used in <span class="No-Break">that process</span></li>
				<li>Visualize numerical data using the Matplotlib and <span class="No-Break">Seaborn libraries</span></li>
				<li>Reduce dimensions using <strong class="bold">principal component </strong><span class="No-Break"><strong class="bold">analysis</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">PCA</strong></span><span class="No-Break">)</span></li>
				<li>Work with Hugging Face’s Dataset module to download and process image and <span class="No-Break">text data</span></li>
			</ul>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor076"/>Numerical data</h1>
			<p>Numerical data usually <a id="_idIndexMarker223"/>comes in the form of tables of numbers, kind of like database tables. One of the most common data in this form is metrics data – for example, the standard object-oriented metrics that have been used since <span class="No-Break">the 1980s.</span></p>
			<p>Numerical data is often the result of a measurement process. The measurement process is a process where we quantify the empirical properties of an entity using measurement instruments to a number. The process must guarantee that important empirical properties are preserved in the mathematical domain – that is, in the numbers. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em> shows an example of <span class="No-Break">this process:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div class="IMG---Figure" id="_idContainer063">
					<img alt="Figure 6.1 – The measurement process with an example of quality measurement using defects" src="image/B19548_06_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – The measurement process with an example of quality measurement using defects</p>
			<p>The<a id="_idIndexMarker224"/> important part of this process consists of three elements. First is the measurement instrument, which needs to map the empirical properties to numbers in a true way. Then, there are the measurement standards, such as the ISO <strong class="bold">Vocabulary in Metrology</strong> (<strong class="bold">VIM</strong>), which is called the measurement’s trueness. Finally, we have the result of the measurement process – the process of applying the measurement instrument to a specific measured entity – which results in a number, a quantification of the measured property. A single number, however, cannot characterize an entire software product or even a part of it, regardless of how true it is to the measured entity. Therefore, in practice, we use several measurement instruments to create a holistic view of the <span class="No-Break">measured entity.</span></p>
			<p>This is where numerical data comes in. Each measurement that characterizes the measured entity is stored in a database or a table – each entity becomes one row and each metric becomes one column. The more columns we have, the better the characteristics of the measured entity. However, at the same time, the more measures we collect, the higher the risk that they will be interconnected, correlated (positively and negatively), and that they will overlap. So, we need to work a bit with that data to get some orientation of it. So, first, we must visualize <span class="No-Break">the data.</span></p>
			<p>The data we’ll use in this part of this chapter comes from a paper by Alhustain, Sultan (Predicting Relative Thresholds for Object Oriented Metrics." 2021 IEEE/ACM International Conference on Technical Debt (TechDebt). IEEE, 2021) and is available from Zenodo (<a href="https://zenodo.org/records/4625975">https://zenodo.org/records/4625975</a>), one of the most commonly used open data repositories in software engineering research. The data contains the values of measures for typical <span class="No-Break">object-oriented metrics:</span></p>
			<ul>
				<li><strong class="bold">Coupling between objects</strong> (<strong class="bold">CB</strong>): The number of references to other classes from the measured <span class="No-Break">entity (class)</span></li>
				<li><strong class="bold">Direct class coupling</strong> (<strong class="bold">DCC</strong>): The number of connections from this class to other classes (for <span class="No-Break">example, associations)</span></li>
				<li><strong class="bold">ExportCoupling</strong>: The number of outgoing connections from <span class="No-Break">the class</span></li>
				<li><strong class="bold">ImportCoupling</strong>: The number of incoming connections to <span class="No-Break">the class</span></li>
				<li><strong class="bold">Number of methods</strong> (<strong class="bold">NOM</strong>): The number of methods in <span class="No-Break">the class</span></li>
				<li><strong class="bold">Weighted methods per class</strong> (<strong class="bold">WMC</strong>): The number of methods in the class, weighted by <span class="No-Break">their size</span></li>
				<li><strong class="bold">Defect count</strong> (<strong class="bold">defect</strong>): The number of defects discovered for <span class="No-Break">this class</span></li>
			</ul>
			<p>The dataset <a id="_idIndexMarker225"/>describes several software projects from the Apache foundation – for example, the Ant tool. For each product, the measured entities are classes in <span class="No-Break">the project.</span></p>
			<p>So, let’s start with the next best practice, which will lead us to <span class="No-Break">the visualization.</span></p>
			<p class="callout-heading">Best practice #34</p>
			<p class="callout">When working with numerical data, visualize it first, starting with the summary views of <span class="No-Break">the data.</span></p>
			<p>When I work with numerical data, I usually start by visualizing it. I start with some overview of the data and then work my way toward <span class="No-Break">the details.</span></p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor077"/>Summarizing the data</h2>
			<p>Summarizing <a id="_idIndexMarker226"/>the data can be done using tables and pivots, as well as charts. One of the charts that I usually start working with is the correlogram – it’s a diagram that shows correlations between each variable/measure in <span class="No-Break">the dataset.</span></p>
			<p>So, let’s read our data into the notebook and start <span class="No-Break">visualizing it:</span></p>
			<pre class="source-code">
# read the file with data using openpyxl
import pandas as pd
# we read the data from the excel file,
# which is the defect data from the ant 1.3 system
dfDataAnt13 = pd.read_excel('./chapter_6_dataset_numerical.xlsx',
                            sheet_name='ant_1_3',
                            index_col=0)</pre>			<p>Once the<a id="_idIndexMarker227"/> dataset is in memory, we can use Python’s Seaborn library to visualize it using the correlogram. The following code does <span class="No-Break">just that:</span></p>
			<pre class="source-code">
# now, let's visualize the data using correlograms
# for that, we use the seaborn library
import seaborn as sns
import matplotlib.pyplot as plt
# in seaborn, the correlogram is called
# pairplot
sns.pairplot(dfDataAnt13)</pre>			<p>The result of this code fragment is the <span class="No-Break">following correlogram:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer064">
					<img alt="Figure 6.2 – Correlogram for the dataset from the paper of Alhusain" src="image/B19548_06_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Correlogram for the dataset from the paper of Alhusain</p>
			<p>The<a id="_idIndexMarker228"/> interesting part here is the distribution of each of the measures presented in the cells on the diagonal. In the case of our data, this distribution is hard to interpret for some of the variables, so we can visualize it a bit differently. When we replace the last line of the code fragment with <strong class="source-inline">sns.pairplot(dfDataAnt13, diag_kind="kde")</strong>, we get a new visualization, with a better view of the distribution. This is shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer065">
					<img alt="Figure 6.3 – Correlogram with a better visualization of the distribution of each measure" src="image/B19548_06_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Correlogram with a better visualization of the distribution of each measure</p>
			<p>These <a id="_idIndexMarker229"/>correlograms provide us with a quick orientation of which variables can be correlated with one another. These correlations are something that we can use later in <span class="No-Break">our work.</span></p>
			<p>We can also look at the data by visualizing the numbers using heatmaps. Heatmaps are tabular visualizations where the intensity of the color indicates the strength of the value of each variable. We can use the following code to create <span class="No-Break">a heatmap:</span></p>
			<pre class="source-code">
# heatmap
p1 = sns.heatmap(dfDataAnt13, cmap="Reds")</pre>			<p>The resulting diagram is presented in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer066">
					<img alt="Figure 6.4 – Summary heatmap for the measures" src="image/B19548_06_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Summary heatmap for the measures</p>
			<p>Before <a id="_idIndexMarker230"/>diving deeper into correlation analysis, I often dive a bit deeper into pairwise comparisons. I also recommend my students to do that since working with pairs allows us to understand the connections <span class="No-Break">between variables.</span></p>
			<p>So, here is my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #35</p>
			<p class="callout">When visualizing data at the aggregate level, focus on the strength of relationships and connections between <span class="No-Break">the values.</span></p>
			<p>Visualizations at the aggregate level can provide us with many different views, but what we should look for is the connections between the variables. Correlograms and heatmaps provide us with this kind of visualization and understanding of <span class="No-Break">the data.</span></p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor078"/>Diving deeper into correlations</h2>
			<p>A good set of <a id="_idIndexMarker231"/>diagrams to work with are scatter plots. However, I often use diagrams that are called KDE plots, also called <em class="italic">density plots</em>. They provide a nicer overview of the variables. The following code fragment visualizes the data in <span class="No-Break">this way:</span></p>
			<pre class="source-code">
# now, let's make some density plots
# set seaborn style
sns.set_style("white")
# Basic 2D density plot
sns.kdeplot(x=dfDataAnt13.CBO, y=dfDataAnt13.DCC)
plt.show()</pre>			<p>The result of this code fragment is the diagram presented in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer067">
					<img alt="Figure 6.5 – Density plot for two measures – DCC and CBO" src="image/B19548_06_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Density plot for two measures – DCC and CBO</p>
			<p>This diagram indicates that two measures – CBO and DCC – are quite strongly dependent on one another (or they quantify similar/same <span class="No-Break">measurable concepts).</span></p>
			<p>We can make this diagram a bit nicer if we want to use it in a dashboard by using the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# Custom the color, add shade and bandwidth
sns.kdeplot(x=dfDataAnt13.WMC,
            y=dfDataAnt13.ImportCoupling,
            cmap="Reds",
            shade=True,
            bw_adjust=.5)
plt.show()</pre>			<p>This code<a id="_idIndexMarker232"/> fragment results in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer068">
					<img alt="Figure 6.6 – Density plot with a colormap" src="image/B19548_06_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Density plot with a colormap</p>
			<p>The preceding diagram shows both the correlation and the number of points that are in each area – the more intense the color, the more data points are in that area. The same diagram for the DCC and CBO measures is shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer069">
					<img alt="Figure 6.7 – Density plot with a colormap for the DCC and CBO measures" src="image/B19548_06_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Density plot with a colormap for the DCC and CBO measures</p>
			<p>Finally, we <a id="_idIndexMarker233"/>can use bubble diagrams to visualize the correlations and the number of data points per group. The following code creates the <span class="No-Break">bubble diagram:</span></p>
			<pre class="source-code">
# now a bubble diagram
# use the scatterplot function to build the bubble map
sns.scatterplot(data=dfDataAnt13,
                x="NOM",
                y="DCC",
                size="Defect",
                legend=False,
                sizes=(20, 2000))
# show the graph
plt.show()</pre>			<p>This code results in the diagram presented in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer070">
					<img alt="Figure 6.8 – Scatter plot – a variant called a bubble plot" src="image/B19548_06_8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Scatter plot – a variant called a bubble plot</p>
			<p>This plot lets<a id="_idIndexMarker234"/> us see the number of points in each area of the scatterplot, which helps us track the <span class="No-Break">correlations visually.</span></p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor079"/>Summarizing individual measures</h2>
			<p>Scatterplots <a id="_idIndexMarker235"/>and density plots are good for tracking dependencies between variables. However, we often need to summarize individual measures. For that, we can use boxplots. The following code creates a boxplot for the data in <span class="No-Break">our example:</span></p>
			<pre class="source-code">
# boxplot
sns.boxplot( x=dfDataAnt13.Defect, y=dfDataAnt13.CBO )</pre>			<p>The result is the boxplot presented in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer071">
					<img alt="Figure 6.9 – Boxplot summarizing the CBO measure for classes with and without defects" src="image/B19548_06_9.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Boxplot summarizing the CBO measure for classes with and without defects</p>
			<p>The <a id="_idIndexMarker236"/>summary provides us with a quick visual indication that the classes with defects are often more connected to other classes than the classes without defects. This is not that surprising because usually, the classes are connected and the ones that are <em class="italic">not</em> connected are often very simple and therefore <span class="No-Break">not error-prone.</span></p>
			<p>A variation of the boxplot is the violin plot, which we get if we change the last line of the last code fragment to <strong class="source-inline">sns.violinplot( x='Defect', y='CBO', data=dfDataAnt13)</strong>. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.10</em> presents such a <span class="No-Break">violin diagram:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer072">
					<img alt="Figure 6.10 – A violin diagram, which is a variation of a boxplot" src="image/B19548_06_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – A violin diagram, which is a variation of a boxplot</p>
			<p>Visualization<a id="_idIndexMarker237"/> is a good way to understand the numerical data that we have at our disposal. We can go even further and start working with it by using methods such as <span class="No-Break">dimensionality reduction.</span></p>
			<p>So, here is the next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #36</p>
			<p class="callout">Diving deeper into individual analyses should be guided by the machine learning task <span class="No-Break">at hand.</span></p>
			<p>Although we have not discussed the tasks for our numerical data explicitly, it is always there. In the case of defect-related data, the most common task is predicting the number of defects per module or class. This means that charts such as the violin plot are very useful, providing us with a visual understanding of whether there is some sort of difference – a difference that a machine learning model <span class="No-Break">can capture.</span></p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor080"/>Reducing the number of measures – PCA</h2>
			<p>The<a id="_idIndexMarker238"/> final analysis that we’ll do <a id="_idIndexMarker239"/>regarding numerical data in this chapter is about reducing the number of variables. It comes from the field of statistics and has been used for reducing the number of variables in the experiment: PCA (Wold, 1987 #104). In short, PCA is a technique that finds the best possible fit of a pre-defined number of vectors to the data at hand. It does not remove any variables; instead, it recalculates them in such a way that the correlation among the new set of variables – called principal components – <span class="No-Break">is minimalized.</span></p>
			<p>Let’s apply this to our dataset using the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# before we use PCA, we need to remove the variable "defect"
# as this is the variable which we predict
dfAnt13NoDefects = dfDataAnt13.drop(['Defect'], axis=1)
# PCA for the data at hand
from sklearn.decomposition import PCA
# we instantiate the PCA class with two parameters
# the first one is the number of principal components
# and the second is the random state
pcaComp = PCA(n_components=2,
              random_state=42)
# then we find the best fit for the principal components
# and fit them to the data
vis_dims = pcaComp.fit_transform(dfAnt13NoDefects)</pre>			<p>Now, we can visualize <span class="No-Break">the data:</span></p>
			<pre class="source-code">
# and of course, we could visualize it
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
colors = ["red", "darkgreen"]
x = [x for x,y in vis_dims]
y = [y for x,y in vis_dims]
# please note that we use the dataset with defects to
# assign colors to the data points in the diagram
color_indices = dfDataAnt13.Defect
colormap = matplotlib.colors.ListedColormap(colors)
plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)
for score in [0,1]:
    color = colors[score]
plt.rcParams['figure.figsize'] = (20,20)</pre>			<p>This code<a id="_idIndexMarker240"/> fragment results in the diagram presented in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer073">
					<img alt="Figure 6.11 – PCA results for reducing the dimensionality of the defect dataset to two. The red data points are the classes that have defects and the green data points are the classes without defects" src="image/B19548_06_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – PCA results for reducing the dimensionality of the defect dataset to two. The red data points are the classes that have defects and the green data points are the classes without defects</p>
			<p>What is typical <a id="_idIndexMarker241"/>about a PCA transformation is its linearity. We can see that this diagram contains trails of that – it looks like a triangle with one horizontal dimension along the <em class="italic">x axis</em>, one vertical dimension along the <em class="italic">y axis</em>, and a 0-point on the <span class="No-Break">left-hand side.</span></p>
			<p>For this dataset, the diagram shows that the red-marked data points are grouped toward the left, and the green-marked points are spread a bit more to the right. This means that there is some difference between the classes that have defects and the classes that do not<a id="_idIndexMarker242"/> have defects. However, there is no clear-cut distinction. That would indicate that the machine learning model can’t find a pattern – at least, not a pattern that would <span class="No-Break">be robust.</span></p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor081"/>Other types of data – images</h1>
			<p>In <a href="B19548_03.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, we looked at image data, mostly from the perspective of what kind of image data exists. Now, we will take a more pragmatic approach and introduce a better way of working with images than just <span class="No-Break">using files.</span></p>
			<p>Let’s look at how<a id="_idIndexMarker243"/> image data is stored in a popular repository – Hugging Face. The library has a specific module for working with datasets – conveniently called <em class="italic">Dataset</em>. It can be installed using the <strong class="source-inline">pip install -q datasets</strong> command. So, let’s load a dataset and visualize one of the images from there using the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# importing the images library
from datasets import load_dataset, Image
# loading a dataset "food101", or more concretely it's split for training
dataset = load_dataset("food101", split="train")</pre>			<p>Now, the variable dataset contains all the images. Well, not all of them – just the part that the designer of the dataset specified as the training set (see the last line of the code fragment). We can visualize one of the images using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# visualizing the first image
dataset[0]["image"]</pre>			<p>Since the images are under unknown copyright, we won’t visualize them in this book. However, the preceding line will show the first image in the dataset. We can also take a look at what else is in that dataset by simply typing <strong class="source-inline">dataset</strong>. We will see the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
Dataset({ features: ['image', 'label'], num_rows: 75750 })</pre>			<p>This means<a id="_idIndexMarker244"/> that the dataset contains two columns – images and their labels. It contains 75,750 of them. Let’s see what the distribution of labels looks like in this dataset using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# we can also plot the histogram
# to check the distribution of labels in the dataset
import seaborn as sns
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (20,10)
sns.histplot(data=dataset['label'], x=dataset['label'])</pre>			<p>This gives us a nice histogram, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer074">
					<img alt="Figure 6.12 – Histogram with the distributions of labels. Each column is the number of images labeled with the appropriate label – 0 to 100" src="image/B19548_06_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Histogram with the distributions of labels. Each column is the number of images labeled with the appropriate label – 0 to 100</p>
			<p>This diagram shows classes of images that are larger than others – the ones that contain over 2,000 images in them. However, it is difficult to check what these labels mean without<a id="_idIndexMarker245"/> understanding the dataset. We can do that by manually visualizing the images. So, here is my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #37</p>
			<p class="callout">When visualizing the metadata for images, make sure you visualize the <span class="No-Break">images themselves.</span></p>
			<p>We mustn’t forget to visualize the image data by plotting the images. We need to ensure that we know what the labels mean and what we use <span class="No-Break">them for.</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor082"/>Text data</h1>
			<p>For the <a id="_idIndexMarker246"/>text data, we’ll use the same Hugging Face hub to obtain two kinds of data – unstructured text, as we did in <a href="B19548_03.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, and structured data – programming <span class="No-Break">language code:</span></p>
			<pre class="source-code">
# import Hugging Face Dataset
from datasets import load_dataset
# load the dataset with text classification labels
dataset = load_dataset('imdb')</pre>			<p>The preceding code fragment loads the dataset of movie<a id="_idIndexMarker247"/> reviews from the <strong class="bold">Internet Movie Database</strong> (<strong class="bold">IMDb</strong>). We can get an example of the data by using an interface that’s similar to what we used <span class="No-Break">for images:</span></p>
			<pre class="source-code">
# show the first example
dataset['train'][0]</pre>			<p>We can visualize it using a similar <span class="No-Break">one too:</span></p>
			<pre class="source-code">
# plot the distribution of the labels
sns.histplot(dataset['train']['label'], bins=2)</pre>			<p>The preceding code fragment creates the following diagram, showing that both positive and negative comments are <span class="No-Break">perfectly balanced:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer075">
					<img alt="Figure 6.13 – Balanced classes in the IMDb movie database reviews" src="image/B19548_06_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – Balanced classes in the IMDb movie database reviews</p>
			<p>We can<a id="_idIndexMarker248"/> do all kinds of processing on the text data in the next steps. However, this processing is related to feature extraction, so we’ll talk about it in the next <span class="No-Break">few chapters.</span></p>
			<p>Before we do that, though, let’s look at datasets that are closer to the domain of software engineering – programming language code. We used similar data in <a href="B19548_03.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, so let’s focus on how we could obtain a larger corpus of programming language code from Hugging Face. We could use the following code to obtain the data and check the <span class="No-Break">first program:</span></p>
			<pre class="source-code">
# now, let us import the code to the text summarization dataset
dsCode = load_dataset('code_x_glue_ct_code_to_text', 'java', split='test')
# and see the first example of the code
dsCode[0]</pre>			<p>This code fragment shows us the first program, which is already tokenized and prepared for further analysis. So, let’s take a peek at the frequency of tokens in this dataset. We can use the<a id="_idIndexMarker249"/> following code <span class="No-Break">for that:</span></p>
			<pre class="source-code">
import pandas as pd
import matplotlib.pyplot as plt
# create a list of tokens
lstCodeLines = dsCode['code_tokens']
# flatten the list of lists to one list
lstCodeLines = [item for sublist in lstCodeLines for item in sublist]
#print the first elements of the list
print(lstCodeLines[:10])
dfCode = pd.DataFrame(lstCodeLines, columns=['token'])
# group the tokens and count the number of occurences
# which will help us to visualize the frequency of tokens in the next step
dfCodeCounts = dfCode.groupby('token').size().reset_index(name='counts')
# sort the counts by descending order
dfCodeCounts = dfCodeCounts.sort_values(by='counts', ascending=False)
fig, ax = plt.subplots(figsize=(12, 6))
# plot the frequency of tokens as a barplot
# for the simplicity, we only take the first 20 tokens
sns.barplot(x='token',
            y='counts',
            data=dfCodeCounts[:20],
            palette=sns.color_palette("BuGn_r", n_colors=20),
            ax=ax)
# rotate the x-axis labels to make sure that
# we see the full token names, i.e. lines of code
ax.set_xticklabels(ax.get_xticklabels(),
                   rotation=45,
                   horizontalalignment='right')</pre>			<p>The<a id="_idIndexMarker250"/> preceding code extracts the tokens, counts them, and creates a diagram of the frequency of the top 20 tokens. The result is presented in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer076">
					<img alt="Figure 6.14 – Token frequencies for the top 20 most common tokens in the code dataset" src="image/B19548_06_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – Token frequencies for the top 20 most common tokens in the code dataset</p>
			<p>Interestingly, we<a id="_idIndexMarker251"/> can observe that brackets, commas, semicolons, and curly brackets are the most commonly used tokens in the dataset. This isn’t surprising as these kinds of characters have special meaning <span class="No-Break">in Java.</span></p>
			<p>The other tokens in the top 20 list are, unsurprisingly, keywords in Java or have special meanings (such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">==</strong></span><span class="No-Break">).</span></p>
			<p>So, my last best practice in this chapter is about understanding the <span class="No-Break">text data.</span></p>
			<p class="callout-heading">Best practice #38</p>
			<p class="callout">Summary statistics for text data help us perform a sanity check of <span class="No-Break">the data.</span></p>
			<p>Even though textual data is quite unstructured by nature, we can visualize some of the properties of the data. For example, token frequency analysis can reveal whether our empirical understanding of the data makes sense and whether we can <span class="No-Break">trust it.</span></p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor083"/>Toward feature engineering</h1>
			<p>In this chapter, we explored methods for visualizing data. We learned how to create diagrams and identify dependencies in the data. We also learned how we can use dimensionality reduction techniques to plot multidimensional data on a two <span class="No-Break">dimensional diagram.</span></p>
			<p>In the next<a id="_idIndexMarker252"/> few chapters, we’ll dive into feature engineering different types of data. Sometimes, it is easy to mix feature engineering with data extraction. In practice, it is not that difficult to tell one from <span class="No-Break">the other.</span></p>
			<p>Extracted data is data that has been collected by applying some sort of measurement instrument. Raw text or images are good examples of this kind of data. Extracted data is close to the domain where the data comes from – or how it <span class="No-Break">is measured.</span></p>
			<p>Features describe the data based on the analysis that we want to perform – they are closer to what we want to do with the data. It is closer to what we want to achieve and which form of machine learning analysis we want <span class="No-Break">to do.</span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor084"/>References</h1>
			<ul>
				<li><em class="italic">International Standardization Organization, International vocabulary of basic and general terms in metrology (VIM). In International Organization. 2004. </em><span class="No-Break"><em class="italic">p. 09-14.</em></span></li>
				<li><em class="italic">Alhusain, S. Predicting Relative Thresholds for Object Oriented Metrics. In 2021 IEEE/ACM International Conference on Technical Debt (TechDebt). </em><span class="No-Break"><em class="italic">2021. IEEE.</em></span></li>
				<li><em class="italic">Feldt, R., et al. Supporting software decision meetings: Heatmaps for visualising test and code measurements. In 2013 39th Euromicro Conference on Software Engineering and Advanced Applications. </em><span class="No-Break"><em class="italic">2013. IEEE.</em></span></li>
				<li><em class="italic">Staron, M., et al. Measuring and visualizing code stability – a case study at three companies. In 2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement. </em><span class="No-Break"><em class="italic">2013. IEEE.</em></span></li>
				<li><em class="italic">Wen, S., C. Nilsson, and M. Staron. Assessing the release readiness of engine control software. In Proceedings of the 1st International Workshop on Software Qualities and Their </em><span class="No-Break"><em class="italic">Dependencies. 2018.</em></span></li>
			</ul>
		</div>
	</body></html>