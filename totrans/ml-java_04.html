<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Customer Relationship Prediction with Ensembles</h1>
                </header>
            
            <article>
                
<p>Any type of company that offers a service, product, or experience needs a solid understanding of their relationship with their customers; therefore, <strong>customer relationship management</strong> (<strong>CRM</strong>) is a key element of modern marketing strategies. One of the biggest challenges that businesses face is the need to understand exactly what causes a customer to buy new products.</p>
<p>In this chapter, we will work on a real-world marketing database provided by the French telecom company, Orange. The task will be to estimate the likelihood of the following customer actions:</p>
<ul>
<li>Switch provider (churn)</li>
<li>Buy new products or services (appetency)</li>
<li>Buy upgrades or add-ons proposed to them to make the sale more profitable (upselling)</li>
</ul>
<p>We will tackle the <strong>Knowledge Discovery and Data Mining</strong> (<strong>KDD</strong>) Cup 2009 challenge and show the steps to process the data using Weka. First, we will parse and load the data and implement the basic baseline models. Later, we will address advanced modeling techniques, including data preprocessing, attribute selection, model selection, and evaluation.</p>
<div class="packt_infobox"><span>The KDD Cup is the leading data mining competition in the world. It is organized annually by the ACM</span> <strong>Special Interest Group on Knowledge Discovery and Data Mining</strong><span>. The winners are announced at the</span> Conference on Knowledge Discovery and Data Mining<span>, which is usually held in August. Yearly archives, including all of the corresponding datasets, are available at</span> <span class="URLPACKT"><a href="http://www.kdd.org/kdd-cup">http://www.kdd.org/kdd-cup</a></span><span>.</span></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The customer relationship database</h1>
                </header>
            
            <article>
                
<p>The most practical way to build knowledge on customer behavior is to produce scores that explain a target variable, such as churn, appetency, or upselling. The score is computed by a model using input variables that describe customers; for example, their current subscription, purchased devices, consumed minutes, and so on. The scores are then used by the information system for things like providing relevant personalized marketing actions.</p>
<p>A customer is the main entity in most of the customer-based relationship databases; getting to know the customer's behavior is important. The customer's behavior produces a score in relation to the churn, appetency, or upselling. The basic idea is to produce a score using a computational model, which may use different parameters, such as the current subscription of the customer, devices purchased, minutes consumed, and so on. Once the score is formed, it is used by the information system to decide on the next strategy, which is especially designed for the customer, based on his or her behavior.</p>
<p>In 2009, the conference on KDD organized a machine learning challenge on customer relationship prediction.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Challenge</h1>
                </header>
            
            <article>
                
<p>Given a large set of customer attributes, the task in the challenge was to estimate the following target variables:</p>
<ul>
<li><strong>Churn probability</strong>: This is the likelihood that a customer will switch providers. The churn rate is also known as the attrition rate or the participant turnover rate, and is a measure used to find the number of individuals, objects, terms, or items moving into or out of a given collection, over a given time period. The term is heavily used in industries that are driven by customers and use subscriber-based models; for example, the cell phone industry and cable TV operators.</li>
<li><strong>Appetency probability</strong>: This is the propensity to buy a service or product.</li>
<li><strong>Upselling probability</strong>: This is the likelihood that a customer will buy an add-on or upgrade. Upselling implies selling something in addition to what the customer is already using. Consider it like the value-added services that are provided by most cell phone operators. Using sales techniques, salesmen try to make customers opt for value-added services, which will bring more revenue. Many times, customers are not aware of other options, and the salesmen convince them to use or consider those options.</li>
</ul>
<p>The challenge was to beat the in-house system developed by Orange Labs. This was an opportunity for the participants to prove that they could handle a large database, including heterogeneous, noisy data, and unbalanced class distributions.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dataset</h1>
                </header>
            
            <article>
                
<p>For the challenge, Orange released a large dataset of customer data, containing about one million customers, described in ten tables with hundreds of fields. In the first step, they resampled the data to select a less unbalanced subset, containing 100,000 customers. In the second step, they used an automatic feature construction tool that generated 20,000 features describing the customers, which was then narrowed down to 15,000 features. In the third step, the dataset was anonymized by randomizing the order of features, discarding the attribute names, replacing the nominal variables with randomly generated strings, and multiplying the continuous attributes by a random factor. Finally, all of the instances were split randomly into training and testing datasets.</p>
<p>The KDD Cup provided two sets of data, a large set and a small set, corresponding to fast and slow challenges, respectively. Both the training and testing sets contained 50,000 examples, and the data was split similarly, but the samples were ordered differently for each set.</p>
<p>In this chapter, we will work with the small dataset, consisting of 50,000 instances, each described with 230 variables. Each of the 50,000 rows of data corresponds to a client, and they are associated with three binary outcomes, one for each of the three challenges (upselling, churn, and appetency).</p>
<p>To make this clearer, the following table illustrates the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-645 image-border" src="Images/fda8bb7a-606b-453b-9097-f8d72776f4eb.png" style="width:36.58em;height:25.42em;" width="789" height="548"/></p>
<p>The table depicts the first 25 instances, that is, customers, each described with 250 attributes. For this example, only a selected subset of 10 attributes is shown. The dataset contains many missing values, and even empty or constant attributes. The last three columns of the table correspond to the three distinct class labels involving the ground truth, that is, if the customer indeed switched providers (churn), bought a service (appetency), or bought an upgrade (upsell). However, note that the labels are provided separately from the data in three distinct files, hence it is essential to retain the order of the instances and the corresponding class labels to ensure proper correspondence.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluation</h1>
                </header>
            
            <article>
                
<p>The submissions were evaluated according to the arithmetic mean of the area under the ROC curve for the three tasks (churn, appetency, and upselling). The ROC curve shows the performance of the model as a curve obtained by plotting the sensitivity against specificity for various threshold values used to determine the classification result (refer to <a href="11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Applied Machine Learning Quick Start</em>, in the section <em>ROC curves</em>). Now, the <strong>area under the ROC</strong> <strong>curve</strong> (<strong>AUC</strong>) is related to the area under this curve <span>â€“</span> the larger the area, the better the classifier). Most toolboxes, including Weka, provide an API to calculate the AUC score.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic Naive Bayes classifier baseline</h1>
                </header>
            
            <article>
                
<p>As per the rules of the challenge, the participants had to outperform the basic Naive Bayes classifier in order to qualify for prizes, which makes an assumption that features are independent (refer to <a href="11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Applied Machine Learning Quick Start</em>).</p>
<p>The KDD Cup organizers ran the vanilla Naive Bayes classifier, without any feature selection or hyperparameter adjustments. For the large dataset, the overall scores of the Naive Bayes on the test set were as follows:</p>
<ul>
<li><strong>Churn problem</strong>: AUC = 0.6468</li>
<li><strong>Appetency problem</strong>: AUC = 0.6453</li>
<li><strong>Upselling problem</strong>: AUC=0.7211</li>
</ul>
<p>Note that the baseline results are only reported for the large dataset. Moreover, while both the training and testing datasets are provided at the KDD Cup site, the actual true labels for the test set are not provided. Therefore, when we process the data with our models, there is no way to know how well the models will perform on the test set. What we will do is only use the training data, and evaluate our models with cross-validation. The results will not be directly comparable, but nevertheless, we will have an idea about what a reasonable magnitude of the AUC score should be.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting the data</h1>
                </header>
            
            <article>
                
<p>At the KDD Cup web page (<span class="URLPACKT"><a href="http://kdd.org/kdd-cup/view/kdd-cup-2009/Data">http://kdd.org/kdd-cup/view/kdd-cup-2009/Data</a></span>), you should see a page that looks similar to the following screenshot. First, under the <span class="packt_screen">Small version (230 var.)</span> header, download <kbd>orange_small_train.data.zip</kbd>. Next, download the three sets of true labels associated with this training data. The following files are found under the <span class="packt_screen">Real binary targets (small)</span> header:</p>
<ul>
<li><kbd>orange_small_train_appentency.labels</kbd></li>
<li><kbd>orange_small_train_churn.labels</kbd></li>
<li><kbd>orange_small_train_upselling.labels</kbd></li>
</ul>
<p>Save and unzip all of the files marked in the red boxes, as shown in the screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-646 image-border" src="Images/80d5b9c9-2bfa-43a1-9351-51a883eae104.png" style="width:92.83em;height:96.92em;" width="1114" height="1163"/></p>
<p>In the following sections, first, we will load the data into Weka and apply basic modeling with the Naive Bayes classifier, in order to obtain our own baseline AUC scores. Later, we will look at more advanced modeling techniques and tricks.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading the data</h1>
                </header>
            
            <article>
                
<p>We will load the data to Weka directly from the <kbd>.csv</kbd> format. For this purpose, we will write a function that accepts the path to the data file and the true labels file. The function will load and merge both datasets and remove empty attributes. We will begin with the following code block:</p>
<pre>public static Instances loadData(String pathData, String <br/>  pathLabeles) throws Exception { </pre>
<p>First, we load the data using the <kbd>CSVLoader()</kbd> class. Additionally, we specify the <kbd>\t</kbd> tab as a field separator and force the last 40 attributes to be parsed as nominal:</p>
<pre>// Load data 
CSVLoader loader = new CSVLoader(); 
loader.setFieldSeparator("\t"); 
loader.setNominalAttributes("191-last"); 
loader.setSource(new File(pathData)); 
Instances data = loader.getDataSet(); </pre>
<div class="packt_infobox">The <kbd>CSVLoader</kbd> class accepts many additional parameters, specifying the column separator, string enclosures, whether a header row is present, and so on. The complete documentation is available at <span class="URLPACKT"><a href="http://weka.sourceforge.net/doc.dev/weka/core/converters/CSVLoader.html">http://weka.sourceforge.net/doc.dev/weka/core/converters/CSVLoader.html</a>.</span></div>
<p>Some of the attributes do not contain a single value, and Weka automatically recognizes them asÂ <kbd>String</kbd> attributes. We actually do not need them, so we can safely remove them by using the <kbd>RemoveType</kbd> filter. Additionally, we specify the <kbd>-T</kbd> parameters, which removes an attribute of a specific type and specifies the attribute type that we want to remove:</p>
<pre>// remove empty attributes identified as String attribute  
RemoveType removeString = new RemoveType(); 
removeString.setOptions(new String[]{"-T", "string"}); 
removeString.setInputFormat(data); 
Instances filteredData = Filter.useFilter(data, removeString); </pre>
<p>Alternatively, we could use the <kbd>void deleteStringAttributes()</kbd> method, implemented within the <kbd>Instances</kbd> class, which has the same effect; for example, <kbd>data.removeStringAttributes()</kbd>.</p>
<p>Now, we will load and assign class labels to the data. We will utilize <kbd>CVSLoader</kbd> again, where we specify that the file does not have any header line, that is, <kbd>setNoHeaderRowPresent(true)</kbd>:</p>
<pre>// Load labeles 
loader = new CSVLoader(); 
loader.setFieldSeparator("\t"); 
loader.setNoHeaderRowPresent(true); 
loader.setNominalAttributes("first-last"); 
loader.setSource(new File(pathLabeles)); 
Instances labels = loader.getDataSet(); </pre>
<p>Once we have loaded both files, we can merge them together by calling the <kbd>Instances.mergeInstances (Instances, Instances)</kbd> static method. The method returns a new dataset that has all of the attributes from the first dataset, plus the attributes from the second set. Note that the number of instances in both datasets must be the same:</p>
<pre>// Append label as class value 
Instances labeledData = Instances.mergeInstances(filteredData, <br/>   labeles); </pre>
<p>Finally, we set the last attribute, that is, the label attribute that we just added, as a target variable, and return the resulting dataset:</p>
<pre>// set the label attribute as class  
labeledData.setClassIndex(labeledData.numAttributes() - 1); 
 
System.out.println(labeledData.toSummaryString()); 
return labeledData; 
} </pre>
<p>The function provides a summary as output, as shown in the following code block, and returns the labeled dataset:</p>
<pre>    Relation Name:  orange_small_train.data-weka.filters.unsupervised.attribute.RemoveType-Tstring_orange_small_train_churn.labels.txt
    Num Instances:  50000
    Num Attributes: 215
    
    Name          Type  Nom  Int Real     Missing      Unique  Dist
    1 Var1        Num   0%   1%   0% 49298 / 99%     8 /  0%    18 
    2 Var2        Num   0%   2%   0% 48759 / 98%     1 /  0%     2 
    3 Var3        Num   0%   2%   0% 48760 / 98%   104 /  0%   146 
    4 Var4        Num   0%   3%   0% 48421 / 97%     1 /  0%     4
    ...
  </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic modeling</h1>
                </header>
            
            <article>
                
<p>In this section, we will implement our own baseline model by following the approach that the KDD Cup organizers took. However, before we get to the model, let's first implement the evaluation engine that will return the AUC on all three problems.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluating models</h1>
                </header>
            
            <article>
                
<p>Now, let's take a closer look at the evaluation function. The evaluation function accepts an initialized model, cross-validates the model on all three problems, and reports the results as an area under the ROC curve (AUC), as follows:</p>
<pre>public static double[] evaluate(Classifier model) <br/>   throws Exception { 
 
  double results[] = new double[4]; 
 
  String[] labelFiles = new String[]{ 
    "churn", "appetency", "upselling"}; 
 
  double overallScore = 0.0; 
  for (int i = 0; i &lt; labelFiles.length; i++) { </pre>
<p>First, we call the <kbd>Instance loadData(String, String)</kbd> function that we implemented earlier to load the training data and merge it with the selected labels:</p>
<pre>    // Load data 
    Instances train_data = loadData( 
     path + "orange_small_train.data", 
      path+"orange_small_train_"+labelFiles[i]+".labels.txt"); </pre>
<p>Next, we initialize the <kbd>weka.classifiers.Evaluation</kbd> class and pass our dataset. (The dataset is only used to extract data properties; the actual data is not considered.) We call the <kbd>void crossValidateModel(Classifier, Instances, int, Random)</kbd> method to begin cross-validation, and we create five folds. As validation is done on random subsets of the data, we need to pass a random seed, as well:</p>
<pre>    // cross-validate the data 
    Evaluation eval = new Evaluation(train_data); 
    eval.crossValidateModel(model, train_data, 5,  
    new Random(1)); </pre>
<p>After the evaluation completes, we read the results by calling the <kbd>double areUnderROC(int)</kbd> method. As the metric depends on the target value that we are interested in, the method expects a class value index, which can be extracted by searching the index of the <kbd>"1"</kbd> value in the class attribute, as follows:</p>
<pre>    // Save results 
    results[i] = eval.areaUnderROC( 
      train_data.classAttribute().indexOfValue("1")); 
    overallScore += results[i]; 
  } </pre>
<p>Finally, the results are averaged and returned:</p>
<pre>  // Get average results over all three problems 
  results[3] = overallScore / 3; 
  return results; 
}</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing the Naive Bayes baseline</h1>
                </header>
            
            <article>
                
<p>Now, when we have all of the ingredients, we can replicate the Naive Bayes approach that we are expected to outperform. This approach will not include any additional data preprocessing, attribute selection, or model selection. As we do not have true labels for the test data, we will apply five-fold cross-validation to evaluate the model on a small dataset.</p>
<p>First, we initialize a Naive Bayes classifier, as follows:</p>
<pre>Classifier baselineNB = new NaiveBayes(); </pre>
<p>Next, we pass the classifier to our evaluation function, which loads the data and applies cross-validation. The function returns an area under the ROC curve score for all three problems, and the overall results:</p>
<pre>double resNB[] = evaluate(baselineNB); 
System.out.println("Naive Bayes\n" +  
"\tchurn:     " + resNB[0] + "\n" +  
"\tappetency: " + resNB[1] + "\n" +  
"\tup-sell:   " + resNB[2] + "\n" +  
"\toverall:   " + resNB[3] + "\n"); </pre>
<p>In our case, the model returns the following results:</p>
<pre>    Naive Bayes
      churn:     0.5897891153549814
      appetency: 0.630778394752436
      up-sell:   0.6686116692438094
      overall:   0.6297263931170756
  </pre>
<p>These results will serve as a baseline when we tackle the challenge with more advanced modeling. If we process the data with significantly more sophisticated, time-consuming, and complex techniques, we expect the results to be much better. Otherwise, we are simply wasting resources. In general, when solving machine learning problems, it is always a good idea to create a simple baseline classifier that serves us as an orientation point.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Advanced modeling with ensembles</h1>
                </header>
            
            <article>
                
<p>In the previous section, we implemented an orientation baseline; now, let's focus on heavy machinery. We will follow the approach taken by the KDD Cup 2009 winning solution, developed by the IBM research team (Niculescu-Mizil and others).</p>
<p>To address this challenge, they used the ensemble selection algorithm (Caruana and Niculescu-Mizil, 2004). This is an ensemble method, which means it constructs a series of models and combines their output in a specific way, in order to provide the final classification. It has several desirable properties that make it a good fit for this challenge, as follows:</p>
<ul>
<li>It was proven to be robust, yielding excellent performance.</li>
<li>It can be optimized for a specific performance metric, including AUC.</li>
<li>It allows for different classifiers to be added to the library.</li>
<li>It is an anytime method, meaning that if we run out of time, we have a solution available.</li>
</ul>
<p>In this section, we will loosely follow the steps as they are described in their report. Note that this is not an exact implementation of their approach, but rather a solution overview that will include the necessary steps to dive deeper.</p>
<p>A general overview of the steps is as follows:</p>
<ol>
<li>First, we will preprocess the data by removing attributes that clearly do not bring any value <span>â€“ f</span>or example, all of the missing or constant values; fixing missing values, in order to help machine learning algorithms, which cannot deal with them; and converting categorical attributes to numerical attributes.</li>
<li>Next, we will run the attribute selection algorithm to select only a subset of attributes that can help in the prediction of tasks.</li>
<li>In the third step, we will instantiate the ensemble selection algorithms with a wide variety of models, and finally, we will evaluate the performance.</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Before we start</h1>
                </header>
            
            <article>
                
<p>For this task, we will need an additional Weka package, <kbd>ensembleLibrary</kbd>. Weka 3.7.2 and higher versions support external packages, mainly developed by the academic community. A list of <span class="packt_screen">WEKA Packages</span> is available at <span class="URLPACKT"><a href="http://weka.sourceforge.net/packageMetaData">http://weka.sourceforge.net/packageMetaData</a></span>, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-647 image-border" src="Images/fbd6a8f5-6251-4690-a3f0-014f6baab657.png" style="width:90.00em;height:68.58em;" width="1080" height="823"/></div>
<p>Find and download the latest available version of the <kbd>ensembleLibrary</kbd> package at <span class="URLPACKT"><a href="http://prdownloads.sourceforge.net/weka/ensembleLibrary1.0.5.zip?download">http://prdownloads.sourceforge.net/weka/ensembleLibrary1.0.5.zip?download</a></span>.</p>
<p>After you unzip the package, locate <kbd>ensembleLibrary.jar</kbd> and import it into your code, as follows:</p>
<pre>import weka.classifiers.meta.EnsembleSelection; </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data preprocessing</h1>
                </header>
            
            <article>
                
<p>First, we will utilize Weka's built-in <kbd>weka.filters.unsupervised.attribute.RemoveUseless</kbd> filter, which works exactly as its name suggests. It removes the attributes that do not vary much, for instance, all constant attributes are removed. The maximum variance, which is only applied to nominal attributes, is specified with the <kbd>-M</kbd> parameter. The default parameter is 99%, which means that if more than 99% of all instances have unique attribute values, the attribute is removed, as follows:</p>
<pre>RemoveUseless removeUseless = new RemoveUseless(); 
removeUseless.setOptions(new String[] { "-M", "99" });// threshold 
removeUseless.setInputFormat(data); 
data = Filter.useFilter(data, removeUseless); </pre>
<p>Next, we will replace all of the missing values in the dataset with the modes (nominal attributes) and means (numeric attributes) from the training data, by using the <kbd>weka.filters.unsupervised.attribute.ReplaceMissingValues</kbd> filter. In general, missing value replacement should be proceeded with caution, while taking into consideration the meaning and context of the attributes:</p>
<pre>ReplaceMissingValues fixMissing = new ReplaceMissingValues(); 
fixMissing.setInputFormat(data); 
data = Filter.useFilter(data, fixMissing); </pre>
<p>Finally, we will discretize numeric attributes, that is, we will transform numeric attributes into intervals by using the <kbd>weka.filters.unsupervised.attribute.Discretize</kbd> filter. With the <kbd>-B</kbd> option, we set splitting numeric attributes into four intervals, and the <kbd>-R</kbd> option specifies the range of attributes (only numeric attributes will be discretized):</p>
<pre>Discretize discretizeNumeric = new Discretize(); 
discretizeNumeric.setOptions(new String[] { 
    "-B",  "4",  // no of bins 
    "-R",  "first-last"}); //range of attributes 
fixMissing.setInputFormat(data); 
data = Filter.useFilter(data, fixMissing); </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Attribute selection</h1>
                </header>
            
            <article>
                
<p>In the next step, we will select only informative attributes, that is, attributes that are more likely to help with prediction. A standard approach to this problem is to check the information gain carried by each attribute. We will use the <kbd>weka.attributeSelection.AttributeSelection</kbd> filter, which requires two additional methods: an evaluator (how attribute usefulness is calculated) and search algorithms (how to select a subset of attributes).</p>
<p>In our case, first, we initialize <kbd>weka.attributeSelection.InfoGainAttributeEval</kbd>, which implements the calculation of information gain:</p>
<pre>InfoGainAttributeEval eval = new InfoGainAttributeEval(); 
Ranker search = new Ranker(); </pre>
<p>To only select the top attributes above a threshold, we initialize <kbd>weka.attributeSelection.Ranker</kbd>, in order to rank the attributes with information gain above a specific threshold. We specify this with the <kbd>-T</kbd> parameter, while keeping the value of the threshold low, in order to keep the attributes with at least some information:</p>
<pre>search.setOptions(new String[] { "-T", "0.001" }); </pre>
<div class="packt_tip"><span>The general rule for setting this threshold is to sort the attributes by information gain and pick the threshold where the information gain drops to a negligible value.</span></div>
<p>Next, we can initialize the <kbd>AttributeSelection</kbd> class, set the evaluator and ranker, and apply the attribute selection to our dataset, as follows:</p>
<pre>AttributeSelection attSelect = new AttributeSelection(); 
attSelect.setEvaluator(eval); 
attSelect.setSearch(search); 
 
// apply attribute selection 
attSelect.SelectAttributes(data); </pre>
<p>Finally, we remove the attributes that were not selected in the last run by calling the <kbd>reduceDimensionality(Instances)</kbd> method:</p>
<pre>// remove the attributes not selected in the last run 
data = attSelect.reduceDimensionality(data); </pre>
<p>In the end, we are left with 214 out of 230 attributes.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Model selection</h1>
                </header>
            
            <article>
                
<p>Over the years, practitioners in the field of machine learning have developed a wide variety of learning algorithms and improvements for existing ones. There are so many unique supervised learning methods that it is challenging to keep track of all of them. As the characteristics of the datasets vary, no one method is the best in all of the cases, but different algorithms are able to take advantage of the different characteristics and relationships of a given dataset.</p>
<p>First, we need to create the model library by initializing the <kbd>weka.classifiers.EnsembleLibrary</kbd> class, which will help us define the models:</p>
<pre>EnsembleLibrary ensembleLib = new EnsembleLibrary(); </pre>
<p>Next, we add the models and their parameters to the library as string values; for example, we can add three decision tree learners with different parameters, as follows:</p>
<pre>ensembleLib.addModel("weka.classifiers.trees.J48 -S -C 0.25 -B -M <br/>   2"); 
ensembleLib.addModel("weka.classifiers.trees.J48 -S -C 0.25 -B -M <br/>   2 -A"); </pre>
<p>If you are familiar with the Weka graphical interface, you can also explore the algorithms and their configurations there and copy the configuration, as shown in the following screenshot. Right-click on the algorithm name and navigate to <span class="packt_screen">Edit configuration</span> | <span class="packt_screen">Copy configuration string</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-648 image-border" src="Images/0824df77-7948-4eea-bc5a-dcf2f64fe21e.png" style="width:41.33em;height:23.17em;" width="917" height="513"/></p>
<p>To complete this example, we added the following algorithms and their parameters:</p>
<ul>
<li>The Naive Bayes that was used as the default baseline:</li>
</ul>
<pre>ensembleLib.addModel("weka.classifiers.bayes.NaiveBayes"); </pre>
<ul>
<li>The k-nearest neighbors, based on lazy models:</li>
</ul>
<pre>ensembleLib.addModel("weka.classifiers.lazy.IBk"); </pre>
<ul>
<li>Logistic regression as a simple logistic with default parameters:</li>
</ul>
<pre>ensembleLib.addModel("weka.classifiers.functions.SimpleLogi<br/>   stic"); </pre>
<ul>
<li>Support vector machines with default parameters:</li>
</ul>
<pre>ensembleLib.addModel("weka.classifiers.functions.SMO"); </pre>
<ul>
<li><kbd>AdaBoost</kbd>, which is, in itself, an ensemble method:</li>
</ul>
<pre>ensembleLib.addModel("weka.classifiers.meta.AdaBoostM1"); </pre>
<ul>
<li><kbd>LogitBoost</kbd>, an ensemble method based on logistic regression:</li>
</ul>
<pre>ensembleLib.addModel("weka.classifiers.meta.LogitBoost"); </pre>
<ul>
<li><kbd>DecisionStump</kbd>, an ensemble method based on one-level decision trees:</li>
</ul>
<pre>ensembleLib.addModel("classifiers.trees.DecisionStump"); </pre>
<p>As the <kbd>EnsembleLibrary</kbd> implementation is primarily focused on GUI and console users, we have to save the models into a file by calling the <kbd>saveLibrary(File, EnsembleLibrary, JComponent)</kbd> method, as follows:</p>
<pre>EnsembleLibrary.saveLibrary(new <br/>   File(path+"ensembleLib.model.xml"), ensembleLib, null); 
System.out.println(ensembleLib.getModels()); </pre>
<p>Next, we can initialize the ensemble selection algorithm by instantiating the <kbd>weka.classifiers.meta.EnsembleSelection</kbd> class. First, let's review the following method options:</p>
<ul>
<li><kbd>-L &lt;/path/to/modelLibrary&gt;</kbd>: This specifies the <kbd>modelLibrary</kbd> file, continuing the list of all models.</li>
<li><kbd>-W &lt;/path/to/working/directory&gt;</kbd>: This specifies the working directory, where all models will be stored.</li>
<li><kbd>-B &lt;numModelBags&gt;</kbd>: This sets the number of bags, that is, the number of iterations to run the ensemble selection algorithm.</li>
<li><kbd>-E &lt;modelRatio&gt;</kbd>: This sets the ratio of library models that will be randomly chosen to populate each bag of models.</li>
<li><kbd>-V &lt;validationRatio&gt;</kbd>: This sets the ratio of the training dataset that will be reserved for validation.</li>
<li><kbd>-H &lt;hillClimbIterations&gt;</kbd>: This sets the number of hill climbing iterations to be performed on each model bag.</li>
<li><kbd>-I &lt;sortInitialization&gt;</kbd>: This sets the ratio of the ensemble library that the sort initialization algorithm will be able to choose from, while initializing the ensemble for each model bag.</li>
<li><kbd>-X &lt;numFolds&gt;</kbd>: This sets the number of cross-validation folds.</li>
<li><kbd>-P &lt;hillclimbMetric&gt;</kbd>: This specifies the metric that will be used for model selection during the hill climbing algorithm. Valid metrics include the accuracy, rmse, roc, precision, recall, fscore, and all.</li>
<li><kbd>-A &lt;algorithm&gt;</kbd>: This specifies the algorithm to be used for ensemble selection. Valid algorithms include forward (default) for forward selection, backward for backward elimination, both for both forward and backward elimination, best to simply print the top performer from the ensemble library, and library to only train the models in the ensemble library.</li>
<li><kbd>-R</kbd>: This flags whether the models can be selected more than once for an ensemble.</li>
<li><kbd>-G</kbd>: This states whether the sort initialization greedily stops adding models when the performance degrades.</li>
<li><kbd>-O</kbd>: This is a flag for verbose output. This prints the performance of all of the selected models.</li>
<li><kbd>-S &lt;num&gt;</kbd>: This is a random number seed (the default is <kbd>1</kbd>).</li>
<li><kbd>-D</kbd>: If set, the classifier is run in debug mode, and may provide additional information to the console as output.</li>
</ul>
<p>We initialize the algorithm with the following initial parameters, where we specify optimizing the ROC metric:</p>
<pre>EnsembleSelection ensambleSel = new EnsembleSelection(); 
ensambleSel.setOptions(new String[]{ 
  "-L", path+"ensembleLib.model.xml", // &lt;/path/to/modelLibrary&gt;<br/>     "-W", path+"esTmp", // &lt;/path/to/working/directory&gt; -  
"-B", "10", // &lt;numModelBags&gt;  
  "-E", "1.0", // &lt;modelRatio&gt;. 
  "-V", "0.25", // &lt;validationRatio&gt; 
  "-H", "100", // &lt;hillClimbIterations&gt;  
"-I", "1.0", // &lt;sortInitialization&gt;  
  "-X", "2", // &lt;numFolds&gt; 
  "-P", "roc", // &lt;hillclimbMettric&gt; 
  "-A", "forward", // &lt;algorithm&gt;  
  "-R", "true", // - Flag to be selected more than once 
  "-G", "true", // - stops adding models when performance degrades 
  "-O", "true", // - verbose output. 
  "-S", "1", // &lt;num&gt; - Random number seed. 
  "-D", "true" // - run in debug mode  
}); </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Performance evaluation</h1>
                </header>
            
            <article>
                
<p>The evaluation is heavy, both computationally and memory-wise, so make sure that you initialize the JVM with extra heap space (for instance, <kbd>java -Xmx16g</kbd>). The computation can take a couple of hours or days, depending on the number of algorithms that you include in the model library. This example took 4 hours and 22 minutes on a 12-core Intel Xeon E5-2420 CPU with 32 GB of memory and utilizing 10% CPU and 6 GB of memory on average.</p>
<p>We call our evaluation method and provide the results as output, as follows:</p>
<pre>double resES[] = evaluate(ensambleSel); 
System.out.println("Ensemble Selection\n"  
+ "\tchurn:     " + resES[0] + "\n" 
+ "\tappetency: " + resES[1] + "\n"  
+ "\tup-sell:   " + resES[2] + "\n"  
+ "\toverall:   " + resES[3] + "\n"); </pre>
<p>The specific set of classifiers in the model library achieved the following result:</p>
<pre>    Ensamble
      churn:     0.7109874158176481
      appetency: 0.786325687118347
      up-sell:   0.8521363243575182
      overall:   0.7831498090978378
  </pre>
<p>Overall, the approach has brought us to a significant improvement of more than 15 percentage points, compared to the initial baseline that we designed at the beginning of this chapter. While it is hard to give a definite answer, the improvement was mainly due to three factors: data preprocessing and attribute selection, the exploration of a large variety of learning methods, and the use of an ensemble-building technique that is able to take advantage of the variety of base classifiers without overfitting. However, the improvement requires a significant increase in processing time, as well as working memory.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Ensemble methods â€“ MOA</h1>
                </header>
            
            <article>
                
<p>To ensemble, as the word suggests, is to view together, or at the same time. It is used to combine multiple learner algorithms, in order to obtain better results and performance. There are various techniques that you can use for an ensemble. Some commonly used ensemble techniques or classifiers include bagging, boosting, stacking, a bucket of models, and so on.</p>
<p><strong>Massive Online Analysis</strong> (<strong>MOA</strong>) supports ensemble classifiers, such as accuracy weighted ensembles, accuracy updated ensembles, and many more. In this section, we will show you how to use the leveraging bagging algorithm:</p>
<ol>
<li>Open the Terminal and execute the following command:</li>
</ol>
<pre><strong>java -cp moa.jar -javaagent:sizeofag-1.0.4.jar moa.gui.GUI</strong></pre>
<ol start="2">
<li class="CDPAlignLeft CDPAlign"><span>Select the <span class="packt_screen">Classification</span> tab and click on the <span class="packt_screen">Configure</span> button:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-649 image-border" src="Images/902e7a65-f7d5-486b-840f-dfd65d228b1d.png" style="width:72.08em;height:76.67em;" width="865" height="920"/></p>
<p style="padding-left: 60px">This will open the <span class="packt_screen">Configure task</span> option.</p>
<ol start="3">
<li><span>I</span><span>n the <span class="packt_screen">learner</span> option, select <span class="packt_screen">bayes.NaiveBayes</span></span>, and then, i<span>n the <span class="packt_screen">stream</span> o</span><span>ption, click on <span class="packt_screen">Edit</span>,</span> as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-650 image-border" src="Images/1fd2f759-4284-4077-a77d-e5802c8f3fac.png" style="width:27.67em;height:35.08em;" width="434" height="550"/></p>
<ol start="4">
<li><span>Select</span> <span class="packt_screen">ConceptDriftStream</span>, <span>and, in <span class="packt_screen">stream</span> and <span class="packt_screen">driftstream</span>, select the <span class="packt_screen">AgrawalGenerator</span>; it will use the Agrawal dataset for the stream generator:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-651 image-border" src="Images/d730b01b-0e88-4452-94a4-c46d1f63eff9.png" style="width:35.42em;height:34.75em;" width="549" height="538"/></p>
<ol start="5">
<li><span>Close all of the windows and click on the <span class="packt_screen">Run</span> button:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-652 image-border" src="Images/7ffe2372-c8cb-4cb3-a1df-404c63e883da.png" style="width:72.08em;height:77.08em;" width="865" height="925"/></p>
<p style="padding-left: 60px">This will run the task and generate the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-653 image-border" src="Images/9795215c-7212-4bb4-ba51-471c42c203bf.png" style="width:71.92em;height:76.83em;" width="863" height="922"/></p>
<ol start="6">
<li>Let's use the <span class="packt_screen">LeveragingBag</span> option. For this, open the <span class="packt_screen">Configure task</span> window and select the <span class="packt_screen">Edit</span> option in <span class="packt_screen">baseLearner</span>, which will show the following; select <span class="packt_screen">LeveragingBag</span> from the first drop-down box. You can find other options, such as boosting and average weight ensembles, in the first drop-down box:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-654 image-border" src="Images/04f39f1f-151f-496c-ba95-3c1db7d83177.png" style="width:32.17em;height:30.00em;" width="583" height="544"/></p>
<p style="padding-left: 60px">Leave the <span class="packt_screen">stream</span> as <span class="packt_screen">AgrawalGenerator</span>, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-655 image-border" src="Images/eeba95e7-a148-45d0-aeb7-30083591f15a.png" style="width:24.42em;height:30.75em;" width="432" height="543"/></p>
<ol start="7">
<li>Close the <span class="packt_screen">Configure task</span> window and click on the <span class="packt_screen">Run</span> button; this will take some time to complete:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-656 image-border" src="Images/7f2c9091-b62a-4db9-aed2-5b14052603a8.png" style="width:91.50em;height:81.83em;" width="1098" height="982"/></p>
<p>The output shows the evaluation after every 10,000 instances, how much RAM time is taken with classification correctness, as well as Kappa statistics. As you can see, over time, the classification correctness increases, along with the increasing instances. The graph in the preceding screenshot shows the correctness and the number of instances.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we tackled the KDD Cup 2009 challenge on customer relationship predictions, implementing the data preprocessing steps and addressing the missing values and redundant attributes. We followed the winning KDD Cup solution and studied how to leverage ensemble methods by using a basket of learning algorithms, which can significantly boost classification performance.</p>
<p>In the next chapter, we will tackle another problem concerning customer behavior: purchasing behavior. You will learn how to use algorithms that detect frequently occurring patterns.</p>


            </article>

            
        </section>
    </div>



  </body></html>