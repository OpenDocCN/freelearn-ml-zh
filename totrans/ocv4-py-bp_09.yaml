- en: Learning to Classify and Localize Objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have studied a range of algorithms and approaches where you have
    learned how to solve real-world problems with the help of computer vision. In
    recent years, in parallel with the considerable hardware computational power that
    is provided with devices such as **Graphical Processing Units **(**GPUs**), a
    lot of algorithms arose that utilized this power and achieved state-of-the-art
    results in computer vision tasks. Usually, these algorithms are based on neural
    networks, which enable the creator of the algorithm to squeeze quite a lot of
    meaningful information from data.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, in contrast to the classical approaches, this information is often
    quite hard to interpret. From that point of view, you might say that we are getting
    closer to artificial intelligence—that is, we are giving a computer an approach
    and it figures out how to do the rest. In order for all of this to not appear
    so mysterious, let's learn about deep learning models in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: As you have already seen, a few of the classical problems in computer vision
    include object detection and localization. Let's look at how to classify and localize
    objects with the help of deep learning models in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of this chapter is to learn important deep learning concepts such
    as transfer learning and how to apply them to build your own object classifier
    and localizer. Specifically, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a large dataset for training a deep learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding **Convolutional Neural Networks** (**CNNs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying and localizing with CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start by preparing a dataset for training. Then, we will go on to understand
    how to use a pretrained model for creating a new classifier. Once you have understood
    how it is done, we will move forward and build more complex architectures that
    will perform localization.
  prefs: []
  type: TYPE_NORMAL
- en: During these steps, we will use the **Oxford-IIIT-Pet** dataset. Finally, we
    will run the app that will use our trained localizer network for inference. Although
    the network will be trained only using the bounding boxes of the heads of pets,
    you will see that it will also be good for localization of the human head position.
    The latter will show the power of generalization of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about these concepts of deep learning and seeing them in action will
    be very useful in the future when you make your own applications using deep learning
    models or when you start to work on completely new deep learning architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have mentioned in all of the chapters of this book, you will need to have
    OpenCV installed. Besides that, you will need to install TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: The Oxford-IIIT-Pet dataset is available for download at [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/), along
    with our dataset preparation script, which will be downloaded automatically for
    you.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code that we present in this chapter (from the GitHub repository)
    at [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition). You
    can also use the Docker files available in the repository to run the code in the
    chapter. Refer to the [Appendix B](c86bca68-4b4a-4be6-8edd-67b1d43f0bfa.xhtml),
    *Setting Up a Docker Container*, for more information on the Docker files.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final app will consist of modules to prepare the dataset, train the models,
    and run an inference with the models using input from your camera. This will require
    the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`main.py`: This is the main script for starting the application and localizing
    the head (of the pets) in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data.py`: This is a module to download and prepare the dataset for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`classification.py`: This is a script to train a classifier network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`localization.py`: This is a script to train and save a localization network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After preparing the dataset for training, we will do the following to complete
    our app:'
  prefs: []
  type: TYPE_NORMAL
- en: We will first train a classification network using transfer learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will train an object localization network, again using transfer learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After we create and train our localization network, we will run our `main.py`
    script to localize the heads in real time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start by learning how to prepare the inference script that will run our
    app. The script will connect to your camera, find a head position in each frame
    of the video stream using the localization model that we will create, and then
    illustrate the results in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing an inference script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our inference script is quite simple. It will first prepare a drawing function,
    then load the model and connect it to the camera. Then, it will loop over the
    frames from the video stream. In the loop, for each frame of the stream, it will
    use the imported model to make an inference and the drawing function to display
    the results. Let''s create a complete script using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the required modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this code, besides importing NumPy and OpenCV, we have also imported **Keras**.
    We are going to use Keras to make predictions in this script; additionally, we
    will use it to create and train our models throughout the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define a function to draw localization bounding boxes on a frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding `draw_box` function accepts `frame` and the normalized coordinates
    of the two corners of a bounding box as an array of four numbers. The function
    first reshapes the one-dimensional array of the box into a two-dimensional array,
    where the first index represents the point and the second represents the *x* and
    *y* coordinates. Then, it transforms the normalized coordinates to the coordinates
    of the image by multiplying them with an array composed of the width and height
    of the image and translates the result into integer values in the same line. Finally,
    it draws the bounding box with the color green using the `cv2.rectangle` function
    and returns `frame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we import the model that we will prepare throughout the chapter and connect
    to the camera:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`model` will be stored in a binary file, which is imported using a convenient
    function from Keras.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we iterate over the frames from the camera, resize each `frame`
    to a standard size (that is, the default image size for the models that we will
    create), and convert `frame` to the **RGB** (**red**, **green**, **blue**) color
    space as we will train our models on RGB images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the same loop, we normalize the image and add one to the shape of the frame
    as the model accepts batches of images. Then, we pass the result to `model` for
    inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We continue the loop by drawing the predicted bounding box using the previously
    defined function, show the results, and then set the termination criteria:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our inference script ready, let's start the journey of creating
    our model by first preparing the dataset in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned previously, in this chapter, we are going to use the Oxford-IIIT-Pet
    dataset. It will be a good idea to encapsulate the preparation of the dataset
    in a separate `data.py` script, which can then be used throughout the chapter.
    As with any other script, first of all, we have to import all the required modules,
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In order to prepare our dataset for use, we will first download and parse the
    dataset into memory. Then, out of the parsed data, we will create a TensorFlow
    dataset, which allows us to work with a dataset in a convenient manner as well
    as prepare the data in the background so that the preparation of the data does
    not interrupt the neural network training process. So, let's move on to download
    and parse the dataset in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and parsing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we first download the dataset from the official website and
    then parse it into a convenient format. During this stage, we will leave out the
    images, which occupy quite a lot of memory. We cover this procedure in the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define where we want to store our pets dataset and download it using a convenient `get_file` function
    in Keras:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As our dataset resides in an archive, we have also extracted it by passing `untar=True`.
    We also pointed `cache_dir` to the current directory. Once the files are saved,
    consequent executions of the `get_file` function will result in no action.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset weighs more than half a gigabyte, and, on the first run, you will
    need a stable internet connection with good bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have downloaded and extracted our dataset, let''s define constants
    for the dataset and annotation folders and set the image size that we want to
    resize our images to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Size `224` is often the default size on which image classification networks
    are trained. Hence, it's a good idea to keep to that size for better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Annotations of this dataset contain information about the image in XML format.
    Before parsing the XML, let''s first define what data we want to have:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`namedtuple` is an extension of a standard tuple in Python and allows you to
    refer to an element of a tuple by its name. The names that we have defined correspond
    to the data elements that we are interested in. Namely, those are the image itself
    (`image`), the head bounding box of the pet ( `box`), the image size, `type` (cat
    or dog), and `breed` (there are 37 breeds).'
  prefs: []
  type: TYPE_NORMAL
- en: '`breeds` and `types` are strings in the annotation; what we want are numbers
    corresponding to `breeds`. For that purpose, we define two dictionaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`defaultdict` is a dictionary that returns default values for the undefined
    keys. Here, it will return the next number starting from zero when requested.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define a function that, given a path to an XML file, will return an
    instance of our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The previously defined function covers the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the XML file and parse it:'
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The contents of the XML file are parsed using the `ElementTree` module, which
    represents the XML in a format that is convenient to navigate through.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, get the name of the corresponding image and extract the name of the breed:'
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, convert the breed to a number using `breeds` that was previously
    defined, which assigns the next number for each undefined key:'
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, get the ID of `types`:'
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, extract the bounding box and normalize it:'
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Return the results as an instance of `Data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have downloaded the dataset and prepared a parser, let''s go on
    to parse the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We have also sorted the paths so that they appear in the same order in different
    runtime environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have parsed our dataset, we might want to print out available breeds
    and types for illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code snippet outputs two types, namely `cat` and `dog`, and their `breeds`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Later on in this chapter, we will have to split the dataset on training and
    test sets. In order to perform a good split, we should randomly pick data elements
    from the dataset in order to have a proportional number of `breeds` in the train
    and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can mix the dataset now so that we don''t have to worry about it later,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The previous code first sets a random seed, which is required to get the same
    result every time we execute the code. The `seed` method accepts one argument,
    which is a number specifying a random sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Once the `seed` method is set, we have the same sequence of random numbers in
    functions that use random numbers. Such numbers are called **pseudorandom**. This
    means that, although they look random, they are predefined. In our case, we use
    the `shuffle` method, which mixes the order of elements in the `parsed` array.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have parsed our dataset into a convenient NumPy array, let's move
    on and create a TensorFlow dataset out of it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a TensorFlow dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to use the TensorFlow dataset adapter in order to train our models.
    Of course, we could create a NumPy array from our dataset, but imagine how much
    memory it would require to keep all the images in the memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the dataset adapter allows you to load the data into memory when
    required. Moreover, the data is loaded and prepared in the background so that
    it will not be a bottleneck in our training process. We transform our parsed array
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: From the previous code snippet, `from_tensor_slices` creates `Dataset` whose
    elements are slices of the given tensors. In our case, the tensors are NumPy arrays
    of labels (box, breed, image location, and more).
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, it is a similar concept to the Python `zip` function. First,
    we have prepared the input accordingly. Let''s now print one element from the
    dataset to see how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It is the TensorFlow—`tensor` that contains all the information that we have
    parsed from a single XML file. Given the dataset, we can check whether all our
    bounding boxes are correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As we have normalized the boxes, they should be in the range of `[0,1]`. Additionally,
    we make sure that the coordinates of the first point of the box are less than
    or equal to the coordinates of the second point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we define a function that will transform our data element so that we can
    feed it into a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The function first loads the corresponding image and resizes it to the standard
    size and normalizes it to `[0,1]`. Then, it creates a `one_hot` vector out of `types`
    and `breeds` using the `tf.one_hot` method and returns the result as an instance
    of `Data`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now what remains is to `map` our dataset with the function, and we are ready
    to go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We have also called the `prefetch` method, which makes sure that some amount
    of data is prefetched so that our networks will not have to wait for the data
    to be loaded from the hard drive.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are running the data preparation script directly, it might be a good
    idea to illustrate some samples of the data. First, we create a function that
    creates an illustration image when it is given a sample of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The function converts the `breed` one-hot vector back to a number, finds the
    name of the breed in the `breeds` dictionary, and plots the bounding box of the
    head together with the breed name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we concatenate several such illustrations and show the resulting image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8a17387-fbe6-4919-8f02-5520a8e469c8.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows nice pets with the bounding boxes around their
    heads as expected. Note that, although we have used random numbers to mix the
    dataset in our script, you obtain the same result as illustrated previously. So,
    you can now see the power of pseudorandom numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have prepared the dataset, let's move on to creating and training
    classifiers in the next section. We will build two classifiers—one for the pet
    type and the other for the breed.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying with CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start with the classification, first of all, we have to import the required
    modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We have to import our prepared dataset and Keras, which we will use to build
    our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: However, before we build our classifier, let's first learn about convolutional
    networks, as we are going to use them to build our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](2e878463-75f1-40a5-b263-0c5aa9627328.xhtml), *Fun with Filters*, you
    learned about filters and convolution. In particular, you learned how filters
    can be used to create a pencil sketch image. In the pencil sketch, you could see
    the points in the image that had a sharp change in value, that is, they were darker
    than those that had a smooth change.
  prefs: []
  type: TYPE_NORMAL
- en: From that point of view, the filters that we applied can be thought of as filters
    for edge detection. In other words, the filters act as a feature detector, where
    the feature is an edge. Alternatively, you could compose a different filter that
    is activated on the corners or that is activated when there is no change in the
    color value.
  prefs: []
  type: TYPE_NORMAL
- en: The filters that we have used act on a single-channel image and have two dimensions;
    however, we can extend the filter with the third dimension, which can then be
    applied to a multichannel image. For example, if a single-channel filter has size
    *3 x 3*, the corresponding 3-channel (for example, RGB) filter will have size
    *3 x 3 x 3*, where the last value is the depth of the filter.
  prefs: []
  type: TYPE_NORMAL
- en: Such filters can already be used for more complex features. For example, you
    might think of a filter that works with the color green, meanwhile ignoring the
    values in red and blue by setting zeros in the corresponding elements of the filter.
  prefs: []
  type: TYPE_NORMAL
- en: Once you come up with a good set of filters, you can apply them to the original
    image and then stack them into a new multichannel image. For example, if we apply
    100 filters on an image, we will obtain 100 single-channel images, which will
    result in a 100-channel image after stacking. Hence, we have built a layer that
    accepts 3 channels and outputs 100 channels.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can compose new filters that have a depth of 100 and act on the composed
    100-channel image. These filters can also be activated on more complex features.
    For example, if there were filters in previous layers that are activated on edges,
    we can compose a filter that is activated on an intersection of edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a range of layers, we might see filters that are activated, for example,
    on the noses of people, heads, the wheels of vehicles, or so on. That is actually
    how the convolutional network works. Surely, a question arises: how do we compose
    those filters? The answer is we don''t, because they are learned.'
  prefs: []
  type: TYPE_NORMAL
- en: We provide the data and the network learns which filters it needs to make good
    predictions. Another difference between the convolutional filters that you have
    used is that, besides the learnable parameter of the filters, there is one more
    learnable value called, which is a constant term added to the output of a filter.
  prefs: []
  type: TYPE_NORMAL
- en: Besides that, after the convolutional filters in each layer, usually, a nonlinear
    function is applied to the output of the filters called the **activation function**.
    As a result of the nonlinearity, the network represents quite a wider class of
    functions so that there is a relatively higher chance of building a good model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some understanding of how a convolutional network works, let's
    start by building a classifier. While building the networks in this chapter, you
    will see how the convolutional layers are built and used. As mentioned previously,
    we use a pretrained model for our new models, or, in other words, we use transfer
    learning. Let's understand what this is in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Usually, a CNN has millions of parameters. Let's make an estimation to find
    out where all of those parameters come from.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a 10-layer network and each layer has 100 filters of size *3
    x 3*. These numbers are quite low and networks that have good performance usually
    have dozens of layers and hundreds of filters in each layer. For our case, each
    filter has a depth of 100.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, each filter has 3 x 3 x 3 = 900 parameters (excluding biases, the number
    of which is 100), which results in *900 x 100* parameters for each layer and,
    therefore, about 900,000 parameters for the complete network. To learn so many
    parameters from scratch without overfitting would require quite a large annotated
    dataset. A question arises: what can we do instead?'
  prefs: []
  type: TYPE_NORMAL
- en: You have learned that layers of a network act as feature extractors. Besides
    this, natural images have quite a lot in common. Therefore, it would be a good
    idea to use the feature extractors of a network that was trained on a large dataset
    to achieve good performance on a different, smaller dataset. This technique is
    called **transfer learning**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s pick a pretrained model as our base model, which is a single line of
    code with Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use the `MobileNetV2` pretrained network, which is a robust and lightweight
    network. Of course, you can use other available models instead, which can be found
    on the Keras website or by simply listing them with `dir(K.applications)`.
  prefs: []
  type: TYPE_NORMAL
- en: We have taken the version of the network that excludes the top layers responsible
    for classification by passing in `include_top=False`, as we are going to build
    a new classifier on top of it. But still, the network includes all the other layers
    that were trained on **ImageNet**. ImageNet is a dataset that includes millions
    of images and each of the images is annotated with one of 1,000 classes of the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the shape of the output of our base model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The first number is undefined and denotes the batch size or, in other words,
    the number of input images. Suppose we simultaneously pass a stack of 10 images
    to the network; then, the output here would have a shape of `(10,7,7,1280)` and
    the first dimension of the tensor will correspond to the input image number.
  prefs: []
  type: TYPE_NORMAL
- en: The next two indexes are the size of the output shape and the last is the number
    of channels. In the original model, this output represents features from the input
    images that are later used to classify the images of the ImageNet dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, they are quite a good representation of all the images so that the
    network can classify the images of ImageNet based on them. Let's try to use these
    features to classify the types and breeds of our pets. In order to do this, let's
    first prepare a classifier in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the pet type and breed classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we are going to use the features as they are, let''s first freeze the weights
    of the network layers so that they don''t update during the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In general, each location of an activation map specifies whether there is a
    feature of the corresponding type in that location. As we work on the last layers
    of the network, we can suppose that different locations on the activation map
    contain similar information and reduce the dimensionality of our features by averaging
    the activation maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The operation is called `AveragePooling2D`—we pool the average of the tensor
    in two dimensions of our feature tensor. You can see the results by printing the
    shapes of the input and output of the operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have just `1280` features per image, let''s add the classification
    layer right away and prepare our dataset for training either on the types or the
    breeds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Training on the types and `breeds` differs only by the number of output neurons
    and the labels. In the case of `breeds`, the number of labels is `37`, and, in
    the case of types, this is `2` (namely cat or dog), which you can see in the code.
    A dense layer represents densely connected neurons. The latter means that each
    neuron in the layer is connected to all 1,280 inputs to the layer.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, each neuron has *1280 + 1* learnable parameters, where 1 is for the bias.
    Mathematically, for the complete layer, the weights of the kernel are represented
    with a matrix that has a size (1,280 for the number of classes) and a column of
    height 1280.
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear part of the layer can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d26cdbef-7778-47fc-941b-c1661e2aa0d5.png),'
  prefs: []
  type: TYPE_NORMAL
- en: Here, **x** is the output of the previous layer (1,280 averaged features, in
    our case), **a** is the matrix, and **b** is the column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we have set a **softmax** function as the activation, which is a good
    choice with classification tasks. The latter is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2b6cfaf-9977-473b-87ee-011f3fc1f660.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **x** is the input to the activation (output of the linear part).
  prefs: []
  type: TYPE_NORMAL
- en: You can see that it sums up to one across all outputs; hence, the output can
    be thought of as the probability of the corresponding class.
  prefs: []
  type: TYPE_NORMAL
- en: The mapping that we defined on the dataset will set the image as data and the
    breeds or types as the label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to define our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here, you can see that the input of the network is our base model and the output
    is our classifier layer. Hence, we have successfully built our classification
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now that we have prepared our classifier network, let''s train and evaluate
    it in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to train the classifier, we have to configure it for training. We
    have to specify an objective function (the `loss` function) and a training method.
    Additionally, we might want to specify some metrics in order to see how the model
    performs. We can configure the classifier using the `compile` method of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We have passed `metrics` as `categorical_accuracy`, which will show which part
    of the dataset is classified with the right class. Besides this, we have passed
    one more metric called `top_k_categorical_accuracy`, which shows which part of
    the dataset is correct in the top `k` prediction of the network.
  prefs: []
  type: TYPE_NORMAL
- en: The default value of `k` is five, so the metric shows which part of the dataset
    is in the most probable five classes predicted by the neural network. We have
    also passed `optimizer="adam"`, which forces the model to use **Adam Optimizer **as
    a training algorithm. You will learn how neural networks are usually trained in
    the *Understanding backpropagation* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before training, we also split the dataset into training and test sets in order
    to see how the network performs on unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Here, we take the first `1000` elements of the dataset for test purposes. And
    the remaining part is used for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training part is mixed by calling the `shuffle` method, which will make
    sure that we have a different order of the data in each epoch of training. Finally,
    we train our network by calling the `fit` method of the dataset and then evaluate
    this on the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: First, the `fit` method accepts the dataset itself, which we pass with batches
    of `32`. The latter means that, on each step of the training process, `32` images
    from the dataset will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have also passed a number of `epochs`, which means that our dataset will
    be iterated for `4` times until the training procedure stops. The output of the
    last `epoch` looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Our categorical accuracy on the train set is more than 97%. So, we are pretty
    good at differentiating between cats and dogs. Of course, the **top-K accuracy**
    will be 100 percent as we have just two classes. Now, let's see how we are performing
    on the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'After training, the model is evaluated and you should obtain results similar
    to the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We again get the categorical accuracy of more than 97%. Therefore, our model
    does not overfit and performs well on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we train on breeds, the same output for training looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Meanwhile, the output for testing looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: For breeds, we get worse results, which is expected as it is much more difficult
    to differentiate a breed than just state whether it is a cat or a dog. In any
    case, the model does not perform too badly. Its first-attempt guess is more than
    80 percent right, and we can also be about 99 percent sure that it will guess
    the breed if it has 5 attempts.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to use a pretrained classifier network
    to build a new classifier. In the next section, let's move ahead with our deep
    learning journey and create an object localization network using the same base
    model—a task that the base model was never trained to accomplish.
  prefs: []
  type: TYPE_NORMAL
- en: Localizing with CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Being able to create your own localizer is a good way to acquire intuition on
    how an object detection network might work. This is because the only conceptual
    difference between object detection and localization networks is that a localization
    network predicts a single bounding box, while an object detection network predicts
    multiple boxes. Also, it is a good way to start understanding how to build a neural
    network that accomplishes other regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are going to use the same pretrained classifier network, `MobileNetV2`,
    as the previous section. However, this time we are going to use the network for
    localizing objects instead of classifying. Let''s import the required modules
    and the base model in the same way that we did in the previous section—although,
    this time, we are not going to freeze the layers of the base model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have everything ready, let's go on to prepare our localizer model.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let's think about how we can make a localizer using the output of the
    base model.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, the output tensor of the base model has a shape of `(None,
    7, 7, 1280)`. The output tensor represents features obtained using a convolutional
    network. We can suppose that some spatial information is encoded in the spatial
    indexes *(7,7)*.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try to reduce the dimensionality of our feature map using a couple of
    convolutional layers and create a regressor that should predict the corner coordinates
    of the pets' head bounding boxes provided by the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our convolutional layers will have several options that are the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: First of all, they will both use the **Rectified Linear Unit** (**ReLU**) as
    an activation function. The latter is a simple function, which is zero when the
    input is less than zero and is equal to the input when the input is greater than
    or equal to zero.
  prefs: []
  type: TYPE_NORMAL
- en: '`padding=same` specifies that we do not want the convolution operation to reduce
    the size of the feature map. The feature maps will be padded with zeros such that
    the feature maps do not reduce size. This is in contrast to `padding=''valid''`,
    which applies the convolutional kernel only up to the margins of the feature maps.'
  prefs: []
  type: TYPE_NORMAL
- en: It is often a good idea to regularize trained parameters, normalize them, or
    do both. The latter often allows you to train easier, faster, and generalize better.
    Regularizers allow you to apply penalties on layer parameters during optimization.
    These penalties are incorporated in the loss function that the network optimizes.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we use the `l2` kernel regularizer, which regularizes the **Euclidian**
    norm of the convolutional kernel weights. The regularization is accomplished by
    adding the ![](img/36015bb2-55c9-49d9-aded-2922fb7f138e.png) term to the loss
    function (the objective function). Here, ![](img/41764ce8-db3a-4c8c-9556-f1ba8a681902.png) is
    a small constant and ![](img/1fafba1e-c15e-42e9-aec3-821c0ef3eb13.png) is the
    *L2* norm, which is equal to the square root of the sum of squares of the parameters
    of the layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is one of the most widely used regularization terms. Now we are ready
    to define our convolutional layers. The first layer is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Here, the first parameter is the number of output channels, which is also the
    number of convolutional filters. The second parameter describes the size of the
    convolutional filters. At first glance, it might seem that a single-pixel convolutional
    kernel does not make much sense as it cannot encode the contextual information
    of a feature map.
  prefs: []
  type: TYPE_NORMAL
- en: That is surely correct; however, in this case, it is used for a different purpose.
    It is a fast operation that allows encoding the depth of the input feature maps
    in a lower dimensionality. The depth is reduced from 1280 to `256`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next layer looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Here, besides the default options that we use, we use strides, which specify
    the number of pixels shifts over the input. In [Chapter 1](2e878463-75f1-40a5-b263-0c5aa9627328.xhtml),
    *Fun with Filters,* the convolutional operation was applied in each location,
    which means the filter was moved one pixel at a time and is equivalent to strides
    equal to one.
  prefs: []
  type: TYPE_NORMAL
- en: When the `strides` option is `2`, then we move the filters by two pixels at
    each step. The option is in plural form as we might want to have different strides
    in different directions, which can be done by passing a tuple of numbers. The
    application of a `stride` with a value greater than 1 is a means to reduce the
    size of the activation map without losing spatial information.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are other operations that can reduce the size of the activation
    maps. For example, an operation called **max pooling** can be used, which is one
    of the most widely used operations in modern convolutional networks. The latter
    takes a small window size (for example, *2 x 2*), picks a single maximal value
    from that window, moves by a specified number of pixels (for example, 2), and
    repeats the procedure throughout the activation map. Therefore, as a result of
    this procedure, the size of the activation map will be reduced by a factor of
    2.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the approach with strides, the max-pooling operation is more
    suitable for tasks where we are not very interested in spatial information. Such
    tasks are, for example, classification tasks, in which we are not interested where
    an object is exactly but are simply interested in what it is. The loss of the
    spatial information in max pooling happens when we simply take the maximal value
    in a window without considering its position in the window.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last thing that we want to do is to connect a dense layer of four neurons
    to the convolutional layer, which will be regressed to the two corner coordinates
    of the bounding boxes (`(x,y)` for each corner):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: As the coordinates of bounding boxes are normalized, it's a good idea to use
    an activation function, which has values in the range of `(0,1)` such as a `sigmoid`
    function, in our case.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the required layers are ready. Now, let''s define the model with the new
    layers and compile it for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We use the **Mean Squared Error** (**MSE**) as a `loss` function, which is the
    squared difference between the ground truth and the predicted value. During training,
    this value will be minimized; hence, the model is supposed to predict the corner
    coordinates after training.
  prefs: []
  type: TYPE_NORMAL
- en: The regularization terms that we have added to the convolutional layers are
    also added to `loss` as discussed. The latter is done automatically by Keras.
    Also, we use the **Root of MSE** (**RMSE**) along with the **Mean Absolute Error**
    (**MAE**), which measures the average magnitude of the errors, as our metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now split the dataset, in the same way that we did in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: What is left to do is to train our model, just like we did in the previous section.
    However, before proceeding with our training, you might be interested in learning
    how exactly the training of our new layers is accomplished. In multilayer neural
    networks, training is usually done using the **backpropagation** **algorithm**,
    so let's first learn about that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network is considered to be trained when we have some optimal weights
    of the network so that the network makes good predictions on our data. So, the
    question is how do we reach these optimal weights? Neural networks are usually
    trained using a **gradient descent** algorithm. This might be either the pure
    gradient descent algorithm or some improved optimization method such as **Adam
    optimizer**, which is again based on computing the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: In all of these algorithms, we need to compute the gradient of the loss function
    relative to all the weights. As a neural network is a complex function, it might
    not appear to be straightforward. This is where the backpropagation algorithm
    jumps in, which allows us to calculate the gradients easily in complex networks
    and understand what the gradient looks like. Let's dive into the details of the
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a neural network consisting of an *N* sequential layer. Generally
    speaking, the *i^(th)* layer in such a network is a function that can be defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fb2f53d-17c4-497c-999c-22c2343ef5ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/6b337984-58aa-48f5-843f-ebfdf60098d1.png) is the weight of the
    layer and ![](img/3b4433fc-eda7-4fd8-b358-e97534e0dd42.png) is the function corresponding
    to the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: We can define ![](img/0eedcb66-349e-4f28-9927-52467d336169.png) to be the input
    of our network so that the formula holds for the complete neural network including
    the first layer.
  prefs: []
  type: TYPE_NORMAL
- en: We can also define ![](img/16351790-0066-4bbe-8664-ebab0df88d75.png) to be our
    loss function so that the formula defines not only all the layers but also the
    loss function. Of course, such a generalization excludes the weight normalization
    term that we have already used. However, this is a simple term that just adds
    up to the loss and hence can be omitted for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compute the gradient of the loss function by setting ![](img/cecf9042-e73b-4400-b08b-c297da168364.png) and using
    the chain rule as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/577e2914-0581-45de-9448-52b995b0d006.png)'
  prefs: []
  type: TYPE_IMG
- en: According to our definition, this formula holds not only for the loss function,
    but it is also general for all of the layers. In this formula, we can see that
    the partial derivative of a certain layer with respect to all the weight in the
    previous layer including the current layer is expressed in terms of the same derivative
    of the previous layer, which is the ![](img/207667cb-a836-4e3f-b064-8ce5f4d66ed0.png) term
    in the formula, and terms that can be calculated using only the current layer,
    namely, ![](img/c9dea053-54e9-4b00-abaf-4b7c0aa70465.png) and ![](img/6b7913d5-3b59-4749-9fd1-e1beec1d0088.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the formula, we can now numerically compute the gradient. In order to
    do that, we first define a variable representing an error signal and assign its
    initial value to one. It should be clear in a moment why it represents an error
    signal. Then, we start from the last layer (the loss function, in our case) and
    repeat the following steps until we reach the input of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the partial derivative of the current layer with respect to its weights
    and multiply by the error signal. This will be the part of the gradient corresponding
    to the weights of the current layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the partial derivative with respect to the previous layer, multiply
    by the error signal, and then update the error signal with the resulting value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the input of the network is not reached, move to the previous layer and repeat
    the steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we reach the input, we have all the partial derivatives of the loss with
    respect to the learnable weights; therefore, we have the gradient of our loss
    function. Now we can note that this is the partial derivative of a layer with
    respect to the previous layer that propagates backward throughout the network
    during the gradient computation process.
  prefs: []
  type: TYPE_NORMAL
- en: That is a **propagating signal**, which influences the contribution of each
    layer to the gradient of the loss function. For example, if it becomes all zero
    somewhere during the propagation, then the contribution of all the remaining layers
    to the gradient will also be zero. Such a phenomenon is called a **vanishing-gradient
    problem**. This algorithm can be generalized to acyclic networks with different
    kinds of branches.
  prefs: []
  type: TYPE_NORMAL
- en: In order to train our network, all that is left to do is to update our weight
    in the direction of the gradient and repeat the procedure until convergence. If
    the pure gradient descent algorithm is used, we simply subtract the gradient multiplied
    by some small constant from the weights; however, usually, more advanced optimization
    algorithms are used such as Adam optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with the pure gradient descent algorithm is that, first, we should
    find some optimal value for the small constant so that the update of the weights
    is neither too small, which will result in slow learning, or too large, as too
    large a value results in instability. Another problem is that once we have found
    an optimal value, we have to start to decrease it once the network starts to converge.
    What is more important, it's often wise to update different weights with different
    factors as different weights might be at different distances from their optimal
    values.
  prefs: []
  type: TYPE_NORMAL
- en: These are some of the reasons why we might want to use more advanced optimization
    techniques such as Adam optimizer or **RMSProp**, which take some or all of these
    mentioned issues, and even some unmentioned issues, into account. Meanwhile, while
    creating your networks, you should note that there is still ongoing research in
    the field of optimization algorithms and that one of the existing optimizers might
    be better in some cases than others, although the Adam optimizer should be a good
    choice for many tasks.
  prefs: []
  type: TYPE_NORMAL
- en: You might also note that in the algorithm, we did not mention exactly how the
    partial derivatives in a layer can be computed. Of course, they can be numerically
    computed by varying the values and measuring the response as is done with numerical
    methods for computing a derivative. The problem is that such computations would
    be heavy and error-prone. A better way to do it is to define a symbolic representation
    for each operation used and then, again, use the chain rule as in the backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: So, we now understand how the complete gradient is calculated. Actually, most
    of the modern deep learning frameworks do the differentiation for you. You usually
    don't need to worry about how exactly it is accomplished, but understanding the
    backgrounds of the computation might be very helpful if you are planning to work
    on new, that is, your own, models.
  prefs: []
  type: TYPE_NORMAL
- en: But for now, let's train our prepared model in the next section and see how
    it performs.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we proceed with the actual training, it is a good idea to have some
    means to save the model with the best weights. For that purpose, we will use a
    callback from Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The callback will be called after each epoch of training; it will calculate
    the `root_mean_square_error` metric of predictions on the validation data and
    will save the model to `localization.h5` if the metric has improved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we train our model in the same way that we did with classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Here, the difference is that we train with more `epochs` this time, as well
    as pass our callbacks and the validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: During training, you will first see a gradual decrease in the loss and metrics,
    both on the train and validation data. After several `epochs`, you might see that
    the metrics on the validation data increase. The latter might be thought of as
    a sign of overfitting, but after more `epochs`, you might see that the metrics
    on `validation_data` suddenly drop. The latter phenomenon is because the model
    switches to a better minimum metric during the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the result of the lowest value of the monitored metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: You can note that, in this case, it was the eighth `epoch` that performed best
    on the validation data. You can note that the RMSE deviation on the validation
    data is about 6 percent. The MAE is less than 6 percent. We can interpret this
    result as follows—given an image from the validation data, the corner coordinates
    of the bounding box are usually shifted by a factor of 1/20 of the size of the
    image, which is not a bad result as the size of the bounding box is comparable
    with the size of the image.
  prefs: []
  type: TYPE_NORMAL
- en: You might also want to try to train the model with the frozen layers of the
    base models. If you do so, you will notice a far worse performance than with an
    unfrozen model. It will perform about twice as badly on the validation dataset
    according to the metrics. Given these numbers, we can conclude that the layers
    of the base model were able to learn on the dataset so that our model performs
    better on the localization task.
  prefs: []
  type: TYPE_NORMAL
- en: So, now that we have our model ready, let's use our inference script to see
    what it can do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Seeing inference in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we run our inference script, it will connect to the camera and localize
    a box on each frame, as depicted in the following photograph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6a24176-0086-4809-8cc9-bc69254f672d.png)'
  prefs: []
  type: TYPE_IMG
- en: Although the model was trained on the location of heads of pets, we can see
    that it's quite good at localizing the head of a person. This is where you can
    note the power of generalization of the model.
  prefs: []
  type: TYPE_NORMAL
- en: When you create your own deep learning apps, you might discover that you have
    a lack of data for your particular application. However, if you relate your specific
    case to other available datasets, you might be able to find some applicable dataset
    that, although different, might allow you to successfully train your model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we have created and trained classification and localization
    models using the Oxford-IIIT-Pet dataset. We have learned how to create deep learning
    classifiers and localizers using transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: You have started to understand how to solve real-world problems using deep learning.
    You have understood how CNNs work and you know how to create a new CNN using a
    base model.
  prefs: []
  type: TYPE_NORMAL
- en: We have also covered the backpropagation algorithm for computing gradients.
    Understanding this algorithm will allow you to make wiser decisions on the architecture
    of models that you might want to build in the future.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue our deep learning journey. We will create
    an application that will detect and track objects with high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset attribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Oxford-IIIT-Pet dataset**: *Cats and Dogs*, O. M. Parkhi, A. Vedaldi, A.
    Zisserman, C. V. Jawahar in IEEE Conference on Computer Vision and Pattern Recognition,
    2012.'
  prefs: []
  type: TYPE_NORMAL
