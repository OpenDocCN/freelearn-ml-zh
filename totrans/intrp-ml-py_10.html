<html><head></head><body>
  <div id="_idContainer259" class="Basic-Text-Frame">
    <h1 class="chapterNumber">10</h1>
    <h1 id="_idParaDest-262" class="chapterTitle">Feature Selection and Engineering for Interpretability</h1>
    <p class="normal">In the first three chapters, we discussed how complexity hinders <strong class="keyWord">Machine Learning</strong> (<strong class="keyWord">ML</strong>) interpretability. There’s a trade-off because you may need some complexity to maximize predictive performance, yet not to the extent that you cannot rely on the model to satisfy the tenets of interpretability: fairness, accountability, and transparency. This chapter is the first of four focused on how to tune for interpretability. One of the easiest ways to improve interpretability is through feature selection. It has many benefits, such as faster training and making the model easier to interpret. But if these two reasons don’t convince you, perhaps another one will.</p>
    <p class="normal">A common misunderstanding is that complex models can self-select features and perform well nonetheless, so why even bother to select features? Yes, many model classes have mechanisms that can take care of useless features, but they aren’t perfect. And the potential for overfitting increases with each one that remains. Overfitted models aren’t reliable, even if they are more accurate. So, while employing model mechanisms such as regularization is still highly recommended to avoid overfitting, feature selection is still useful.</p>
    <p class="normal">In this chapter, we will comprehend how irrelevant features adversely weigh on the outcome of a model and thus, the importance of feature selection for model interpretability. Then, we will review filter-based feature selection methods such as <strong class="keyWord">Spearman’s correlation</strong> and <a id="_idIndexMarker1006"/>learn about embedded methods such <a id="_idIndexMarker1007"/>as <strong class="keyWord">LASSO and ridge regression</strong>. Then, we will discover wrapper methods<a id="_idIndexMarker1008"/> such as <strong class="keyWord">sequential feature selection</strong>, and hybrid ones such as <strong class="keyWord">Recursive Feature Elimination</strong> (<strong class="keyWord">RFE</strong>). Lastly, even <a id="_idIndexMarker1009"/>though feature engineering is typically conducted before selection, there’s value in exploring feature engineering for many reasons after the dust has settled and features have been selected.</p>
    <p class="normal">These are the main topics we are going to cover in this chapter:</p>
    <ul>
      <li class="bulletList">Understanding the effect of irrelevant features</li>
      <li class="bulletList">Reviewing filter-based feature selection methods</li>
      <li class="bulletList">Exploring embedded feature selection methods</li>
      <li class="bulletList">Discovering wrapper, hybrid, and advanced feature selection methods</li>
      <li class="bulletList">Considering feature engineering</li>
    </ul>
    <p class="normal">Let’s begin!</p>
    <h1 id="_idParaDest-263" class="heading-1">Technical requirements</h1>
    <p class="normal">This chapter’s example uses the <code class="inlineCode">mldatasets</code>, <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">scipy</code>, <code class="inlineCode">mlxtend</code>, <code class="inlineCode">sklearn-genetic-opt</code>, <code class="inlineCode">xgboost</code>, <code class="inlineCode">sklearn</code>, <code class="inlineCode">matplotlib</code>, and <code class="inlineCode">seaborn</code> libraries. Instructions on how to install all these libraries are in the <em class="italic">Preface</em>.</p>
    <div class="note">
      <p class="normal">The GitHub code for this chapter is located here: <a href="https://packt.link/1qP4P"><span class="url">https://packt.link/1qP4P</span></a>.</p>
    </div>
    <h1 id="_idParaDest-264" class="heading-1">The mission</h1>
    <p class="normal">It has<a id="_idIndexMarker1010"/> been estimated that there are over 10 million non-profits worldwide, and while a large portion of them have public funding, most of them depend mostly on private donors, both corporate and individual, to continue operations. As such, fundraising is mission-critical and carried out throughout the year.</p>
    <p class="normal">Year over year, donation revenue has grown, but there are several problems non-profits face: donor interests evolve, so a charity popular one year might be forgotten the next; competition is fierce between non-profits, and demographics are shifting. In the United States, the average donor only gives two charitable gifts per year and is over 64 years old. Identifying potential donors is challenging, and campaigns to reach them can be expensive.</p>
    <p class="normal">A National Veterans Organization non-profit arm has a large mailing list of about 190,000 past donors and would like to send a special mailer to ask for donations. However, even with a special bulk discount rate, it costs them $0.68 per address. This adds up to over $130,000. They only have a marketing budget of $35,000. Given that they have made this a high priority, they are willing to extend the budget but only if the <strong class="keyWord">Return On Investment</strong> (<strong class="keyWord">ROI</strong>) is <a id="_idIndexMarker1011"/>high enough to justify the additional cost.</p>
    <p class="normal">To minimize<a id="_idIndexMarker1012"/> the use of their limited budget, instead of mass mailing, they’d like to try direct mailing, which aims to identify potential donors using what is already known, such as past donations, geographic location, and demographic data. They will reach other donors via email instead, which is much cheaper, costing no more than $1,000 per month for their entire list. They hope this hybrid marketing plan will yield better results. They also recognize that high-value donors respond better to personalized paper mailers, while smaller donors respond better to email anyway.</p>
    <p class="normal">No more than six percent of the mailing list donates to any given campaign. Using ML to predict human behavior is by no means an easy task, especially when the data categories are imbalanced. Nevertheless, success is not measured by the highest predictive accuracy but by profit lift. In other words, the direct mailing model evaluated on the test dataset should produce more profit than if they mass-mailed the entire dataset.</p>
    <p class="normal">They have sought your assistance to use ML to produce a model that identifies the most probable donors, but also in a way that <em class="italic">guarantees</em> an ROI.</p>
    <p class="normal">You received the dataset from the non-profit, which is approximately evenly split into training and test data. If you send the mailer to absolutely everybody in the test dataset, you make a profit of $11,173, but if you manage to identify only those who will donate, the maximum yield of $73,136 will be attained. Your goal is to achieve a high-profit lift and reasonable ROI. When the campaign runs, it will identify the most probably donors for the entire mailing list, and the non-profit hopes to spend not much more than $35,000 in total. However, the dataset has 435 columns, and some simple statistical tests and modeling exercises show that the data is too noisy to identify the potential donors’ reliability because of overfitting.</p>
    <h1 id="_idParaDest-265" class="heading-1">The approach</h1>
    <p class="normal">You’ve <a id="_idIndexMarker1013"/>decided to first fit a base model with all the features and assess it at different levels of complexity to understand the relationship between the increased number of features and the propensity for the predictive model to overfit to the training data. Then, you will employ a series of feature selection methods ranging from simple filter-based methods to the most advanced ones to determine which one achieves the profitability and reliability goals sought by the client. Lastly, once a list of final features has been selected, you can try feature engineering.</p>
    <p class="normal">Given the cost-sensitive nature of the problem, thresholds are important to optimize the profit lift. We will get into the role of thresholds later on, but one significant effect is that even though this is a classification problem, it is best to use regression models, and then use predictions to classify so that there’s only one threshold to tune. That is, for classification models, you would need a threshold for the label, say those that donated over $1, and then another one for probabilities predicted. On the other hand, regression predicts the donation, and the threshold can be optimized based on that.</p>
    <h1 id="_idParaDest-266" class="heading-1">The preparations</h1>
    <p class="normal">The code for this<a id="_idIndexMarker1014"/> example can be found at <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/10/Mailer.ipynb"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/10/Mailer.ipynb</span></a>.</p>
    <h2 id="_idParaDest-267" class="heading-2">Loading the libraries</h2>
    <p class="normal">To run this <a id="_idIndexMarker1015"/>example, we need to install the following libraries:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">mldatasets</code> to load the dataset</li>
      <li class="bulletList"><code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, and <code class="inlineCode">scipy</code> to manipulate it</li>
      <li class="bulletList"><code class="inlineCode">mlxtend</code>, <code class="inlineCode">sklearn-genetic-opt</code>, <code class="inlineCode">xgboost</code>, and <code class="inlineCode">sklearn</code> (scikit-learn) to fit the models</li>
      <li class="bulletList"><code class="inlineCode">matplotlib</code> and <code class="inlineCode">seaborn</code> to create and visualize the interpretations</li>
    </ul>
    <p class="normal">To load the libraries, use the following code block:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> mldatasets
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> timeit
<span class="hljs-keyword">from</span> tqdm.notebook <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> VarianceThreshold,\
                                    mutual_info_classif, SelectKBest
<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression,\
                                    LassoCV, LassoLarsCV, LassoLarsIC
<span class="hljs-keyword">from</span> mlxtend.feature_selection <span class="hljs-keyword">import</span> SequentialFeatureSelector
<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFECV
<span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA <span class="hljs-keyword">import</span> shap
<span class="hljs-keyword">from</span> sklearn-genetic-opt <span class="hljs-keyword">import</span> GAFeatureSelectionCV
<span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> rankdata
<span class="hljs-keyword">from</span> sklearn.discriminant_analysis <span class="hljs-keyword">import</span>
LinearDiscriminantAnalysis
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestRegressor
<span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
</code></pre>
    <p class="normal">Next, we will load <a id="_idIndexMarker1016"/>and prepare the dataset.</p>
    <h2 id="_idParaDest-268" class="heading-2">Understanding and preparing the data</h2>
    <p class="normal">We load the data<a id="_idIndexMarker1017"/> like this into two DataFrames (<code class="inlineCode">X_train</code> and <code class="inlineCode">X_test</code>) with the features and two <code class="inlineCode">numpy</code> arrays with corresponding labels (<code class="inlineCode">y_train</code> and <code class="inlineCode">y_test</code>). Please note that these DataFrames have already been previously prepared for us to remove sparse or unnecessary features, treat missing values, and encode categorical features:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train, X_test, y_train, y_test = mldatasets.load(
    <span class="hljs-string">"nonprofit-mailer"</span>,
    prepare=<span class="hljs-literal">True</span>
)
y_train = y_train.squeeze()
y_test = y_test.squeeze()
</code></pre>
    <p class="normal">All features are numeric with no missing values and categorical features have already been one-hot encoded for us. Between both train and test mailing lists, there should be over 191,500 records and 435 features. You can check this is the case like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(X_train.shape)
<span class="hljs-built_in">print</span>(y_train.shape)
<span class="hljs-built_in">print</span>(X_test.shape)
<span class="hljs-built_in">print</span>(y_test.shape)
</code></pre>
    <p class="normal">The preceding code should output the following:</p>
    <pre class="programlisting con"><code class="hljs-con">(95485, 435)
(95485,)
(96017, 435)
(96017,)
</code></pre>
    <p class="normal">Next, we can<a id="_idIndexMarker1018"/> verify that the test labels have the right number of donors (<code class="inlineCode">test_donors</code>), donations (<code class="inlineCode">test_donations</code>), and hypothetical profit ranges (<code class="inlineCode">test_min_profit</code> and <code class="inlineCode">test_max_profit</code>) using the variable cost of $0.68 (<code class="inlineCode">var_cost</code>). We can print these, and then do the same for the training dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">var_cost = <span class="hljs-number">0.68</span>
y_test_donors = y_test[y_test &gt; <span class="hljs-number">0</span>]
test_donors = <span class="hljs-built_in">len</span>(y_test_donors)
test_donations = <span class="hljs-built_in">sum</span>(y_test_donors)
test_min_profit = test_donations - (<span class="hljs-built_in">len</span>(y_test)*var_cost)
test_max_profit = test_donations - (test_donors*var_cost)
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">'%s test donors totaling $%.0f (min profit: $%.0f,\</span>
<span class="hljs-string">    max profit: $%.0f)'</span>
    %(test_donors, test_donations, test_min_profit,\
       test_max_profit))
y_train_donors = y_train[y_train &gt; <span class="hljs-number">0</span>]
train_donors = <span class="hljs-built_in">len</span>(y_train_donors)
train_donations = <span class="hljs-built_in">sum</span>(y_train_donors)
train_min_profit = train_donations – (<span class="hljs-built_in">len</span>(y_train)*var_cost)
train_max_profit = train_donations – (train_donors*var_cost)
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">'%s train donors totaling $%.0f (min profit: $%.0f,\</span>
<span class="hljs-string">    max profit: $%.0f)'</span>
    %(train_donors, train_donations, train_min_profit,\
    train_max_profit))
</code></pre>
    <p class="normal">The preceding code should output the following:</p>
    <pre class="programlisting con"><code class="hljs-con">4894 test donors totaling $76464 (min profit: $11173, max profit: $73136)
4812 train donors totaling $75113 (min profit: $10183, max profit: $71841)
</code></pre>
    <p class="normal">Indeed, if the <a id="_idIndexMarker1019"/>non-profit mass-mailed to everyone on the test mailing list, they’d make about $11,000 profit but would have to go grossly over budget to achieve this. The non-profit recognizes that making the max profit by identifying and targeting only donors is nearly an impossible feat. Therefore, they would be content with producing a model that reliably can yield more than the min profit but with a smaller cost, preferably under budget.</p>
    <h1 id="_idParaDest-269" class="heading-1">Understanding the effect of irrelevant features</h1>
    <p class="normal"><strong class="keyWord">Feature selection</strong> is <a id="_idIndexMarker1020"/>also known as <strong class="keyWord">variable</strong> or <strong class="keyWord">attribute selection</strong>. It is<a id="_idIndexMarker1021"/> the method by which you can automatically or manually select a subset of specific features useful to the construction of ML models.</p>
    <p class="normal">It’s not necessarily<a id="_idIndexMarker1022"/> true that more features lead to better models. Irrelevant features can impact the learning process, leading to overfitting. Therefore, we need some strategies to remove any features that might adversely affect learning. Some <a id="_idIndexMarker1023"/>of the advantages of selecting a smaller subset of features include the following:</p>
    <ul>
      <li class="bulletList"><em class="italic">It’s easier to understand simpler models</em>: For instance, feature importance for a model that uses 15 variables is much easier to grasp than one that uses 150 variables.</li>
      <li class="bulletList"><em class="italic">Shorter training time</em>: Reducing the number of variables decreases the cost of computing, speeds up model training, and perhaps most notably, simpler models have quicker inference times.</li>
      <li class="bulletList"><em class="italic">Improved generalization by reducing overfitting</em>: Sometimes, with little prediction value, many of the variables are just noise. The ML model, however, learns from this noise and triggers overfitting to the training data while minimizing generalization simultaneously. We may significantly enhance the generalization of ML models by removing these irrelevant or noisy features.</li>
      <li class="bulletList"><em class="italic">Variable redundancy</em>: It is common for datasets to have collinear features, which could mean some are redundant. In cases like these, as long as no significant information<a id="_idIndexMarker1024"/> is lost, we can retain only one of the correlated features and delete the others.</li>
    </ul>
    <p class="normal">Now, we will fit some models to demonstrate the effect of too many features.</p>
    <h2 id="_idParaDest-270" class="heading-2">Creating a base model</h2>
    <p class="normal">Let’s create a<a id="_idIndexMarker1025"/> base model for our mailing list dataset to see how this plays out. But first, let’s set our random seed for reproducibility:</p>
    <pre class="programlisting code"><code class="hljs-code">rand = <span class="hljs-number">9</span>
os.environ[<span class="hljs-string">'PYTHONHASHSEED'</span>]=<span class="hljs-built_in">str</span>(rand)
np.random.seed(rand)
</code></pre>
    <p class="normal">We will use <a id="_idIndexMarker1026"/>XGBoost’s <strong class="keyWord">Random Forest</strong> (<strong class="keyWord">RF</strong>) regressor (<code class="inlineCode">XGBRFRegressor</code>) throughout this chapter. It’s just like scikit-learn’s but faster because it uses second-order approximations of the objective function. It also has more options, such as setting the learning rate and monotonic constraints, examined in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>. We initialize <code class="inlineCode">XGBRFRegressor</code> with a conservative initial <code class="inlineCode">max_depth</code> value of <code class="inlineCode">4</code> and always use <code class="inlineCode">200</code> estimators for consistency. Then, we fit it with our training data. We will use <code class="inlineCode">timeit</code> to measure how long it takes, which we save in a variable (<code class="inlineCode">baseline_time</code>) for later reference:</p>
    <pre class="programlisting code"><code class="hljs-code">stime = timeit.default_timer()
reg_mdl = xgb.XGBRFRegressor(max_depth=<span class="hljs-number">4</span>, n_estimators=<span class="hljs-number">200</span>, seed=rand)
fitted_mdl = reg_mdl.fit(X_train, y_train)
etime = timeit.default_timer()
baseline_time = etime-stime
</code></pre>
    <p class="normal">Now that we have a base model, let’s evaluate it.</p>
    <h2 id="_idParaDest-271" class="heading-2">Evaluating the model</h2>
    <p class="normal">Next, let’s <a id="_idIndexMarker1027"/>create a dictionary (<code class="inlineCode">reg_mdls</code>) to house all the models we will fit in this chapter to test which feature subsets produce the best models. Here, we can evaluate the RF model with all the features and a <code class="inlineCode">max_depth</code> value of <code class="inlineCode">4</code> (<code class="inlineCode">rf_4_all</code>) using <code class="inlineCode">evaluate_reg_mdl</code>. It will make a summary and a scatter plot with a regression line:</p>
    <pre class="programlisting code"><code class="hljs-code">reg_mdls = {}
reg_mdls[<span class="hljs-string">'rf_4_all'</span>] = mldatasets.evaluate_reg_mdl(
    fitted_mdl,
    X_train,
    X_test,
    y_train,
    y_test,
    plot_regplot=<span class="hljs-literal">True</span>,
    ret_eval_dict=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1028"/>preceding code produces the metrics and plot shown in <em class="italic">Figure 10.1</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_01.png" alt="Chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.1: Base model predictive performance</p>
    <p class="normal">For a <a id="_idIndexMarker1029"/>plot like the one in <em class="italic">Figure 10.1</em>, usually, a diagonal line is expected, so one glance at this plot would tell you that the model is not predictive. Also, the RMSEs may not seem bad but in the context of such a lopsided problem, they are dismal. Consider this: only 5% of the list makes a donation, and only 20% of those are over $20, so an average error of $4.3 – $4.6 is enormous.</p>
    <p class="normal">So, is this model useless? The answer lies in what thresholds we use to classify with it. Let’s start by defining an array of thresholds (<code class="inlineCode">threshs</code>), ranging from $0.40 to $25. We start spacing these out by a cent until it reaches $1, then by 10 cents until it reaches $3, and after that, space by $1:</p>
    <pre class="programlisting code"><code class="hljs-code">threshs = np.hstack(
    [
      np.linspace(<span class="hljs-number">0.40</span>,<span class="hljs-number">1</span>,<span class="hljs-number">61</span>),
      np.linspace(<span class="hljs-number">1.1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">20</span>),
      np.linspace(<span class="hljs-number">4</span>,<span class="hljs-number">25</span>,<span class="hljs-number">22</span>)
    ]
)
</code></pre>
    <p class="normal">There’s a<a id="_idIndexMarker1030"/> function in <code class="inlineCode">mldatasets</code> that can compute profit at every threshold (<code class="inlineCode">profits_by_thresh</code>). All it needs is the actual (<code class="inlineCode">y_test</code>) and predicted labels, followed by the thresholds (<code class="inlineCode">threshs</code>), the variable cost (<code class="inlineCode">var_costs</code>), and the <code class="inlineCode">min_profit</code> required. It produces a <code class="inlineCode">pandas</code> DataFrame with the revenue, costs, profit, and ROI for every threshold, as long as the profit is above <code class="inlineCode">min_profit</code>. Remember, we had set this minimum at the beginning of the chapter as $11,173 because it makes no sense to target donors under this amount. After we generate these profit DataFrames for the test and train datasets, we can place the maximum and minimum amounts in the model’s dictionary for later use. And then, we employ <code class="inlineCode">compare_df_plots</code> to plot the costs, profits, and ROI ratio for testing and training for every threshold where it exceeded the profit minimum:</p>
    <pre class="programlisting code"><code class="hljs-code">y_formatter = plt.FuncFormatter(
    <span class="hljs-keyword">lambda</span> x, loc: <span class="hljs-string">"${:,}K"</span>.<span class="hljs-built_in">format</span>(x/<span class="hljs-number">1000</span>)
)
profits_test = mldatasets.profits_by_thresh(
    y_test,
    reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'</span><span class="hljs-string">preds_test'</span>],
    threshs,
    var_costs=var_cost,
    min_profit=test_min_profit
)
profits_train = mldatasets.profits_by_thresh(
    y_train,
    reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'preds_train'</span>],
    threshs,
    var_costs=var_cost,
    min_profit=train_min_profit
)
reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'max_profit_train'</span>] =profits_train.profit.<span class="hljs-built_in">max</span>()
reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'max_profit_test'</span>] = profits_test.profit.<span class="hljs-built_in">max</span>()
reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'</span><span class="hljs-string">max_roi'</span>] = profits_test.roi.<span class="hljs-built_in">max</span>()
reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'min_costs'</span>] = profits_test.costs.<span class="hljs-built_in">min</span>()
reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'profits_train'</span>] = profits_train
reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'profits_test'</span>] = profits_test
mldatasets.compare_df_plots(
    profits_test[[<span class="hljs-string">'costs'</span>, <span class="hljs-string">'</span><span class="hljs-string">profit'</span>, <span class="hljs-string">'roi'</span>]],
    profits_train[[<span class="hljs-string">'costs'</span>, <span class="hljs-string">'profit'</span>, <span class="hljs-string">'roi'</span>]],
    <span class="hljs-string">'Test'</span>,
    <span class="hljs-string">'Train'</span>,
    y_formatter=y_formatter,
    x_label=<span class="hljs-string">'Threshold'</span>,\
    plot_args={<span class="hljs-string">'secondary_y'</span>:<span class="hljs-string">'roi'</span>}
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1031"/>preceding snippet generates the plots in <em class="italic">Figure 10.2</em>. You can tell that <strong class="keyWord">Test</strong> and <strong class="keyWord">Train</strong> are almost identical. Costs decrease steadily at a high rate and profit at a lower rate, while ROI increases steadily. However, some differences exist, such as ROI, which becomes a bit higher eventually, and although viable thresholds start at the same point, <strong class="keyWord">Train</strong> does end at a different threshold. It turns out the model can turn a profit, so despite the appearance of the plot in <em class="italic">Figure 10.1</em>, the model is far from useless:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_02.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.2: Comparison between profit, costs, and ROI for the test and train datasets for the base model across thresholds</p>
    <p class="normal">The difference in RMSEs for the train and test sets didn’t lie. The model did not overfit. The main reason for this is that we used relatively shallow trees by setting our <code class="inlineCode">max_depth</code> value at <code class="inlineCode">4</code>. We can easily see this effect of using shallow trees by computing how many features had a <code class="inlineCode">feature_importances_</code> value of over 0:</p>
    <pre class="programlisting code"><code class="hljs-code">reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'total_feat'</span>] =\
    reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'fitted'</span>].feature_importances_.shape[<span class="hljs-number">0</span>] reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'num_feat'</span>] = <span class="hljs-built_in">sum</span>(
    reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'fitted'</span>].feature_importances_ &gt; <span class="hljs-number">0</span>
)
<span class="hljs-built_in">print</span>(reg_mdls[<span class="hljs-string">'rf_4_all'</span>][<span class="hljs-string">'num_feat'</span>])
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1032"/>preceding code outputs <code class="inlineCode">160</code>. In other words, only 160 were used out of 435—there are only so many features that can be accommodated in such a shallow tree! Naturally, this leads to lowering overfitting, but at the same time, the choice of features with measures of impurity over a random selection of features is not necessarily the most optimal.</p>
    <h2 id="_idParaDest-272" class="heading-2">Training the base model at different max depths</h2>
    <p class="normal">So, what <a id="_idIndexMarker1033"/>happens if we make the trees deeper? Let’s repeat all the steps we did for the shallow one but for max depths between 5 and 12:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> depth <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>, <span class="hljs-number">13</span>)):
mdlname = <span class="hljs-string">'rf_'</span>+<span class="hljs-built_in">str</span>(depth)+<span class="hljs-string">'_all'</span>
stime = timeit.default_timer()
reg_mdl = xgb.XGBRFRegressor(
    max_depth=depth,
    n_estimators=<span class="hljs-number">200</span>,
    seed=rand
)
fitted_mdl = reg_mdl.fit(X_train, y_train)
etime = timeit.default_timer()
reg_mdls[mdlname] = mldatasets.evaluate_reg_mdl(
    fitted_mdl,
    X_train,
    X_test,
    y_train,
    y_test,
    plot_regplot=<span class="hljs-literal">False</span>,
    show_summary=<span class="hljs-literal">False</span>,
    ret_eval_dict=<span class="hljs-literal">True</span>
)
reg_mdls[mdlname][<span class="hljs-string">'speed'</span>] = (etime - stime)/baseline_time
reg_mdls[mdlname][<span class="hljs-string">'depth'</span>] = depth
reg_mdls[mdlname][<span class="hljs-string">'fs'</span>] = <span class="hljs-string">'all'</span>
profits_test = mldatasets.profits_by_thresh(
    y_test,
    reg_mdls[mdlname][<span class="hljs-string">'preds_test'</span>],
    threshs,
    var_costs=var_cost,
    min_profit=test_min_profit
)
profits_train = mldatasets.profits_by_thresh(
    y_train,
    reg_mdls[mdlname][<span class="hljs-string">'preds_train'</span>],
    threshs,
    var_costs=var_cost,
    min_profit=train_min_profit
)
reg_mdls[mdlname][<span class="hljs-string">'max_profit_train'</span>] = profits_train.profit.<span class="hljs-built_in">max</span>()
reg_mdls[mdlname][<span class="hljs-string">'max_profit_test'</span>] = profits_test.profit.<span class="hljs-built_in">max</span>()
reg_mdls[mdlname][<span class="hljs-string">'max_roi'</span>] = profits_test.roi.<span class="hljs-built_in">max</span>()
reg_mdls[mdlname][<span class="hljs-string">'min_costs'</span>] = profits_test.costs.<span class="hljs-built_in">min</span>()
reg_mdls[mdlname][<span class="hljs-string">'profits_train'</span>] = profits_train
reg_mdls[mdlname][<span class="hljs-string">'profits_test'</span>] = profits_test
reg_mdls[mdlname][<span class="hljs-string">'total_feat'</span>] =\
reg_mdls[mdlname][<span class="hljs-string">'fitted'</span>].feature_importances_.shape[<span class="hljs-number">0</span>]
reg_mdls[mdlname][<span class="hljs-string">'</span><span class="hljs-string">num_feat'</span>] = <span class="hljs-built_in">sum</span>(
    reg_mdls[mdlname][<span class="hljs-string">'fitted'</span>].feature_importances_ &gt; <span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Now, let’s <a id="_idIndexMarker1034"/>plot the details in the profits DataFrames for the “deepest” model (with a max depth of 12) as we did before with <code class="inlineCode">compare_df_plots</code>, producing <em class="italic">Figure 10.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_03.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.3: Comparison between profit, costs, and ROI for the test and train datasets for a “deep” base model across thresholds</p>
    <p class="normal">See how different <strong class="keyWord">Test</strong> and <strong class="keyWord">Train</strong> are this time in <em class="italic">Figure 10.3</em>. <strong class="keyWord">Test</strong> reaches a max of about $15,000 and <strong class="keyWord">Train</strong> exceeds $20,000. <strong class="keyWord">Train</strong>’s costs dramatically fall, making the ROI orders of magnitude much higher than <strong class="keyWord">Test</strong>. Also, the ranges of thresholds are much different. Why is this a problem, you ask? If we had to guess what threshold to use to pick who to target in the next mailer, the optimal for <strong class="keyWord">Train</strong> is higher than for <strong class="keyWord">Test</strong>—meaning that by using an overfit model, we could miss the mark and underperform on unseen data.</p>
    <p class="normal">Next, let’s<a id="_idIndexMarker1035"/> convert our model dictionary (<code class="inlineCode">reg_mdls</code>) into a DataFrame and extract some details from it. Then, we can sort it by depth, format it, color-code it, and output it:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">display_mdl_metrics</span>(<span class="hljs-params">reg_mdls, sort_by=</span><span class="hljs-string">'depth'</span><span class="hljs-params">, max_depth=</span><span class="hljs-literal">None</span>):
    reg_metrics_df = pd.DataFrame.from_dict( reg_mdls, <span class="hljs-string">'index'</span>)\
                        [[<span class="hljs-string">'depth'</span>, <span class="hljs-string">'fs'</span>, <span class="hljs-string">'rmse_train'</span>, <span class="hljs-string">'rmse_test'</span>,\
                          <span class="hljs-string">'max_profit_train'</span>,\
                          <span class="hljs-string">'max_profit_test'</span>, <span class="hljs-string">'max_roi'</span>,\
                          <span class="hljs-string">'min_costs'</span>, <span class="hljs-string">'speed'</span>, <span class="hljs-string">'num_feat'</span>]]
    pd.set_option(<span class="hljs-string">'precision'</span>, <span class="hljs-number">2</span>) 
    html = reg_metrics_df.sort_values(
        by=sort_by, ascending=<span class="hljs-literal">False</span>).style.\
        <span class="hljs-built_in">format</span>({<span class="hljs-string">'max_profit_train'</span>:<span class="hljs-string">'${0:,.0f}'</span>,\
        <span class="hljs-string">'max_profit_test'</span>:<span class="hljs-string">'${0:,.0f}'</span>, <span class="hljs-string">'min_costs'</span>:<span class="hljs-string">'${0:,.0f}'</span>}).\
        background_gradient(cmap=<span class="hljs-string">'plasma'</span>, low=<span class="hljs-number">0.3</span>, high=<span class="hljs-number">1</span>,
                            subset=[<span class="hljs-string">'rmse_train'</span>, <span class="hljs-string">'rmse_test'</span>]).\
        background_gradient(cmap=<span class="hljs-string">'viridis'</span>, low=<span class="hljs-number">1</span>, high=<span class="hljs-number">0.3</span>,
                            subset=[
                                <span class="hljs-string">'max_profit_train'</span>, <span class="hljs-string">'max_profit_test'</span>
                                ]
                            )
    <span class="hljs-keyword">return</span> html
display_mdl_metrics(reg_mdls)
</code></pre>
    <p class="normal">The preceding snippet leverages the <code class="inlineCode">display_mdl_metrics</code> function to output the DataFrame shown in <em class="italic">Figure 10.4</em>. Something that should be immediately visible is how RMSE train and RMSE test are inverses. One decreases dramatically, and another increases slightly as the depth increases. The same can be said for profit. ROI tends to increase with depth and training speed and the number of features used as well:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_04.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.4: Comparing metrics for all base RF models with different depths</p>
    <p class="normal">We could <a id="_idIndexMarker1036"/>be tempted to use <code class="inlineCode">rf_11_all</code> with the highest profitability, but it would be risky to use it! A common misunderstanding is that black-box models can effectively cut through any number of irrelevant features. While they are often be able to find something of value and make the most out of it, too many features could hinder their reliability by overfitting to noise in the training dataset. Fortunately, there is a sweet spot where you can reach high profitability with minimal overfitting, but to get there, we have to reduce the number of features first!</p>
    <h1 id="_idParaDest-273" class="heading-1">Reviewing filter-based feature selection methods</h1>
    <p class="normal"><strong class="keyWord">Filter-based methods</strong> independently <a id="_idIndexMarker1037"/>select features from a dataset without employing any ML. These methods depend only on the variables’ characteristics and are relatively effective, computationally inexpensive, and quick to perform. Therefore, being the low-hanging fruit of feature selection methods, they are usually the first step in any feature selection pipeline.</p>
    <p class="normal">Filter-based methods can be categorized as:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Univariate</strong>: Individually <a id="_idIndexMarker1038"/>and independently of the feature space, they evaluate and rate a single feature at a time. One problem that can occur with univariate methods is that they may filter out too much since they don’t take into consideration the relationship between features.</li>
      <li class="bulletList"><strong class="keyWord">Multivariate</strong>: These<a id="_idIndexMarker1039"/> take into account the entire feature space and how features interact with each other.</li>
    </ul>
    <p class="normal">Overall, for the removal of obsolete, redundant, constant, duplicated, and uncorrelated features, filter methods are very strong. However, by not accounting for complex, non-linear, non-monotonic correlations and interactions that only ML models can find, they aren’t effective whenever these relationships are prominent in the data.</p>
    <p class="normal">We will review three categories of filter-based methods:</p>
    <ul>
      <li class="bulletList">Basic</li>
      <li class="bulletList">Correlation</li>
      <li class="bulletList">Ranking</li>
    </ul>
    <p class="normal">We will explain them further in their own sections.</p>
    <h2 id="_idParaDest-274" class="heading-2">Basic filter-based methods</h2>
    <p class="normal">We <a id="_idIndexMarker1040"/>employ <strong class="keyWord">basic filter methods</strong> in the<a id="_idIndexMarker1041"/> data preparation stage, specifically, the data cleaning stage, before any modeling. The reason for this is there’s a low risk of making feature selection decisions that would adversely impact models. They involve common-sense operations such as removing features that carry no information or duplicate it.</p>
    <h3 id="_idParaDest-275" class="heading-3">Constant features with a variance threshold</h3>
    <p class="normal"><strong class="keyWord">Constant features</strong> don’t<a id="_idIndexMarker1042"/> change<a id="_idIndexMarker1043"/> in the training dataset and, therefore, carry no information, and the model can’t learn from them. We can use a univariate method called <code class="inlineCode">VarianceThreshold</code>, which removes low-variance features. We will use a threshold of zero because we want to filter out only features<a id="_idIndexMarker1044"/> with <strong class="keyWord">zero variance</strong>—in other words, constant features. It only works with numeric features, so we must first identify which features are numeric and which are categorical. Once we fit the method on the numeric columns, <code class="inlineCode">get_support()</code> returns the list of features that aren’t constant, and we can use set algebra to return only the constant features (<code class="inlineCode">num_const_cols</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">num_cols_l = X_train.select_dtypes([np.number]).columns
cat_cols_l = X_train.select_dtypes([np.<span class="hljs-built_in">bool</span>, np.<span class="hljs-built_in">object</span>]).columns
num_const = VarianceThreshold(threshold=<span class="hljs-number">0</span>)
num_const.fit(X_train[num_cols_l])
num_const_cols = <span class="hljs-built_in">list</span>(
    <span class="hljs-built_in">set</span>(X_train[num_cols_l].columns) -
    <span class="hljs-built_in">set</span>(num_cols_l[num_const.get_support()])
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1045"/>preceding snippet produced a list of constant numeric features, but how about categorical features? Categorical features would only have one category or unique value. We can easily check this by applying the <code class="inlineCode">nunique()</code> function on categorical features. It will return a <code class="inlineCode">pandas</code> series, and then a <code class="inlineCode">lambda</code> function can filter out only those with one unique value. Then, <code class="inlineCode">.index.tolist()</code> returns the name of the features as a list. Now, we just join both lists of constant features, and voilà! We have all constants (<code class="inlineCode">all_const_cols</code>). We can print them; there should be three:</p>
    <pre class="programlisting code"><code class="hljs-code">cat_const_cols = X_train[cat_cols_l].nunique()[<span class="hljs-keyword">lambda</span> x:\
                                               x&lt;<span class="hljs-number">2</span>].index.tolist()
all_const_cols = num_const_cols + cat_const_cols
<span class="hljs-built_in">print</span>(all_const_cols)
</code></pre>
    <p class="normal">In most cases, removing constant features isn’t good enough. A redundant feature might be almost constant or <strong class="keyWord">quasi-constant</strong>.</p>
    <h3 id="_idParaDest-276" class="heading-3">Quasi-constant features with value_counts</h3>
    <p class="normal"><strong class="keyWord">Quasi-constant features</strong> are <a id="_idIndexMarker1046"/>almost<a id="_idIndexMarker1047"/> entirely the same value. Unlike constant filtering, using a variance threshold won’t work because high variance and quasi-constantness aren’t mutually exclusive. Instead, we will iterate all features and get <code class="inlineCode">value_counts()</code>, which returns the number of rows for each value. Then, divide these counts by the total number of rows to get a percentage and sort by the highest. If the top value is higher than the predetermined threshold (<code class="inlineCode">thresh</code>), we append it to a list of quasi-constant columns (<code class="inlineCode">quasi_const_cols</code>). Please note that choosing this threshold must be done with a lot of care and understanding of the problem. For instance, in this case, we know that it’s lopsided because only 5% donate, most of whom donate a low amount, so even a tiny percentage of a feature might make an impact, which is why our threshold is so high at 99.9%:</p>
    <pre class="programlisting code"><code class="hljs-code">thresh = <span class="hljs-number">0.999</span>
quasi_const_cols = []
num_rows = X_train.shape[<span class="hljs-number">0</span>]
<span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> tqdm(X_train.columns):
    top_val = (
        X_train[col].value_counts() / num_rows
        ).sort_values(ascending=<span class="hljs-literal">False</span>).values[<span class="hljs-number">0</span>]
    <span class="hljs-keyword">if</span> top_val &gt;= thresh:
        quasi_const_cols.append(col)
<span class="hljs-built_in">print</span>(quasi_const_cols)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1048"/>preceding code should have printed five features, which include the three that were previously obtained. Next, we will deal with another form of irrelevant features: duplicates!</p>
    <h3 id="_idParaDest-277" class="heading-3">Duplicating features</h3>
    <p class="normal">Usually, when<a id="_idIndexMarker1049"/> we discuss duplicates with data, we immediately think of duplicate rows, but <strong class="keyWord">duplicate columns</strong> are also <a id="_idIndexMarker1050"/>problematic. We can find them just as you would find duplicate rows with the <code class="inlineCode">pandas duplicated()</code> function, except we would transpose the DataFrame first, inversing columns and rows:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train_transposed = X_train.T
dup_cols = X_train_transposed[
    X_train_transposed.duplicated()].index.tolist()
<span class="hljs-built_in">print</span>(dup_cols)
</code></pre>
    <p class="normal">The preceding snippet outputs a list with the two duplicated columns.</p>
    <h3 id="_idParaDest-278" class="heading-3">Removing unnecessary features</h3>
    <p class="normal">Unlike <a id="_idIndexMarker1051"/>other feature selection methods, which you should test with models, you can apply basic filter-based feature selection methods right away by removing the features you deemed useless. But just in case, it’s good practice to make a copy of the original data. Please note that we don’t include constant columns (<code class="inlineCode">all_constant_cols</code>) in the columns we are to drop (<code class="inlineCode">drop_cols</code>) because the quasi-constant ones already include them:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train_orig = X_train.copy()
X_test_orig = X_test.copy()
drop_cols = quasi_const_cols + dup_cols
X_train.drop(labels=drop_cols, axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)
X_test.drop(labels=drop_cols, axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Next, we will<a id="_idIndexMarker1052"/> explore multivariate filter-based methods on the remaining features.</p>
    <h2 id="_idParaDest-279" class="heading-2">Correlation filter-based methods</h2>
    <p class="normal"><strong class="keyWord">Correlation filter-based methods</strong> quantify <a id="_idIndexMarker1053"/>the strength of the relationship between two features. It is useful for feature selection because we might want to filter out extremely correlated<a id="_idIndexMarker1054"/> features or those that aren’t correlated with others at all. Either way, it is a multivariate feature selection method—bivariate, to be precise.</p>
    <p class="normal">But first, we ought to choose a correlation method:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pearson’s correlation coefficient</strong>: Measures<a id="_idIndexMarker1055"/> the linear correlation between two features. It outputs a coefficient between -1 (negative) and 1 (positive), with 0 meaning no linear correlation. Like linear regression, it assumes linearity, normality, and homoscedasticity—that is, the error term around the linear regression line is similarly sized across all values.</li>
      <li class="bulletList"><strong class="keyWord">Spearman’s rank correlation coefficient</strong>: Measures the strength of monotonicity<a id="_idIndexMarker1056"/> of two features regardless of whether they are linearly related or not. Monotonicity is the degree to which as one feature increases, the other one consistently increases or decreases. It is measured between -1 and 1, with 0 meaning no monotonic correlation. It makes no distribution assumptions and can work with both continuous and discrete features. However, its weakness is with non-monotonic relationships.</li>
      <li class="bulletList"><strong class="keyWord">Kendall’s tau correlation coefficient</strong>: Measures<a id="_idIndexMarker1057"/> the ordinal association between features—that is, it computes the similarity between lists of ordered numbers. It also ranges between -1 and 1, but they mean low and high, respectively. It’s useful with discrete features.</li>
    </ul>
    <p class="normal">The dataset is a mix of continuous and discrete, and we cannot make any linear assumptions about it, so <code class="inlineCode">spearman</code> is the right choice. All three can be used with the <code class="inlineCode">pandas</code> <code class="inlineCode">corr</code> function though:</p>
    <pre class="programlisting code"><code class="hljs-code">corrs = X_train.corr(method=<span class="hljs-string">'spearman'</span>)
<span class="hljs-built_in">print</span>(corrs.shape)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1058"/>preceding code should output the shape of the correlation matrix, which is <code class="inlineCode">(428, 428)</code>. This dimension makes sense because there are 428 features left, and each feature has a relationship with 428 features, including itself.</p>
    <p class="normal">We can <a id="_idIndexMarker1059"/>now look for features to remove in the correlation matrix (<code class="inlineCode">corrs</code>). Note that to do so, we must establish thresholds. For instance, we can say that an extremely correlated feature has an absolute value coefficient of over 0.99 and less than 0.15 for an uncorrelated feature. With these thresholds in mind, we can find features that are correlated to only one feature and extremely correlated to more than one feature. Why one feature? Because the diagonals in a correlation matrix are always 1 because a feature is always perfectly correlated with itself. The <code class="inlineCode">lambda</code> functions in the following code make sure we are accounting for this:</p>
    <pre class="programlisting code"><code class="hljs-code">extcorr_cols = (<span class="hljs-built_in">abs</span>(corrs) &gt; <span class="hljs-number">0.99</span>).<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>)[<span class="hljs-keyword">lambda</span> x: x&gt;<span class="hljs-number">1</span>]\
.index.tolist()
<span class="hljs-built_in">print</span>(extcorr_cols)
uncorr_cols = (<span class="hljs-built_in">abs</span>(corrs) &gt; <span class="hljs-number">0.15</span>).<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>)[<span class="hljs-keyword">lambda</span> x: x==<span class="hljs-number">1</span>]\
.index.tolist()
<span class="hljs-built_in">print</span>(uncorr_cols)
</code></pre>
    <p class="normal">The preceding code outputs the two lists as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">['MAJOR', 'HHAGE1', 'HHAGE3', 'HHN3', 'HHP1', 'HV1', 'HV2', 'MDMAUD_R', 'MDMAUD_F', 'MDMAUD_A']
['TCODE', 'MAILCODE', 'NOEXCH', 'CHILD03', 'CHILD07', 'CHILD12', 'CHILD18', 'HC15', 'MAXADATE']
</code></pre>
    <p class="normal">The first list contains features that are extremely correlated with ones other than themselves. While this is useful to know, you shouldn’t remove features from this list without understanding what features they are correlated with and how, as well as with the target. Then, only if redundancy is found, make sure you only remove one of them. The second list is of features uncorrelated to any others than themself, which, in this case, is <a id="_idIndexMarker1060"/>suspicious given the sheer number of features. That being said, we also should inspect them one by one, especially to measure them against the target to see whether they are redundant. However, we will take a chance and make a feature subset (<code class="inlineCode">corr_cols</code>) excluding the uncorrelated ones:</p>
    <pre class="programlisting code"><code class="hljs-code">corr_cols = X_train.columns[
    ~X_train.columns.isin(uncorr_cols)
].tolist()
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(corr_cols))
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker1061"/>code should output <code class="inlineCode">419</code>. Let’s now fit the RF model with only these features. Given that there are still over 400 features, we will use a <code class="inlineCode">max_depth</code> value of <code class="inlineCode">11</code>. Except for that and a different model name (<code class="inlineCode">mdlname</code>), it’s the same code as before:</p>
    <pre class="programlisting code"><code class="hljs-code">mdlname = <span class="hljs-string">'rf_11_f-corr'</span>
stime = timeit.default_timer()
reg_mdl = xgb.XGBRFRegressor(
    max_depth=<span class="hljs-number">11</span>,
    n_estimators=<span class="hljs-number">200</span>,
    seed=rand
)
fitted_mdl = reg_mdl.fit(X_train[corr_cols], y_train)
reg_mdls[mdlname][<span class="hljs-string">'num_feat'</span>] = <span class="hljs-built_in">sum</span>(
    reg_mdls[mdlname][<span class="hljs-string">'fitted'</span>].feature_importances_ &gt; <span class="hljs-number">0</span>
)
</code></pre>
    <p class="normal">Before we compare the results for the preceding model, let’s learn about ranking filter methods.</p>
    <h2 id="_idParaDest-280" class="heading-2">Ranking filter-based methods</h2>
    <p class="normal"><strong class="keyWord">Ranking filter-based methods</strong> are<a id="_idIndexMarker1062"/> based on <a id="_idIndexMarker1063"/>statistical univariate ranking tests, which assess the strength of the dependency between a feature and the target. These are some of the most popular methods:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">ANOVA F-test: Analysis of Variance</strong> (<strong class="keyWord">ANOVA</strong>) F-test measures the linear dependency<a id="_idIndexMarker1064"/> between<a id="_idIndexMarker1065"/> features and the target. As the name suggests, it does this by decomposing the variance. It makes similar assumptions to linear regression, such as normality, independence, and homoscedasticity. In scikit-learn, you can use <code class="inlineCode">f_regression</code> and <code class="inlineCode">f_classification</code> for regression and classification, respectively, to rank features by the F-score yielded by the F-test.</li>
      <li class="bulletList"><strong class="keyWord">Chi-square test of independence</strong>: This<a id="_idIndexMarker1066"/> test measures the association between<a id="_idIndexMarker1067"/> non-negative categorical variables and binary targets, so it’s only suitable for classification problems. In scikit-learn, you can use <code class="inlineCode">chi2</code>.</li>
      <li class="bulletList"><strong class="keyWord">Mutual information</strong> (<strong class="keyWord">MI</strong>): Unlike<a id="_idIndexMarker1068"/> the two previous methods, this one is derived from information theory rather than classical statistical hypothesis testing. It’s a different name but a concept we have already discussed in this book as<a id="_idIndexMarker1069"/> the <strong class="keyWord">Kullback-Leibler</strong> (<strong class="keyWord">KL</strong>) <strong class="keyWord">divergence</strong> because it’s the KL for feature <em class="italic">X</em> and target <em class="italic">Y</em>. The Python implementation in scikit-learn uses a numerically stable and symmetric offshoot of KL <a id="_idIndexMarker1070"/>called <strong class="keyWord">Jensen-Shannon</strong> (<strong class="keyWord">JS</strong>) divergence instead and leverages k-nearest neighbors to compute distances. Features can be ranked by MI with <code class="inlineCode">mutual_info_regression</code> and <code class="inlineCode">mutual_info_classif</code> for regression and classification, respectively.</li>
    </ul>
    <p class="normal">Of the three options mentioned, the one that is most appropriate for this dataset is MI because we cannot assume linearity among our features, and most of them aren’t categorical either. We can try classification with a threshold of $0.68, which at least covers the cost of sending the mailer. To that end, we must first create a binary classification target (<code class="inlineCode">y_train_class</code>) with that threshold:</p>
    <pre class="programlisting code"><code class="hljs-code">y_train_class = np.where(y_train &gt; <span class="hljs-number">0.68</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Next, we can use <code class="inlineCode">SelectKBest</code> to get the top 160 features according<a id="_idIndexMarker1071"/> to <strong class="keyWord">MI Classification</strong> (<strong class="keyWord">MIC</strong>). We then employ <code class="inlineCode">get_support()</code> to obtain a Boolean vector (or mask), which tells us which features are in the top 160, and we subset the list of features with this mask:</p>
    <pre class="programlisting code"><code class="hljs-code">mic_selection = SelectKBest(
    mutual_info_classif, k=<span class="hljs-number">160</span>).fit(X_train, y_train_class)
mic_cols = X_train.columns[mic_selection.get_support()].tolist()
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(mic_cols))
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1072"/>preceding code should confirm that there are 160 features in the <code class="inlineCode">mic_cols</code> list. Incidentally, this is an arbitrary number. Ideally, we could test different thresholds for the <a id="_idIndexMarker1073"/>classification target and <em class="italic">k</em>s for the MI, looking for the model that achieved the highest profit lift while underfitting the least. Next, we can fit the RF model as we’ve done before with the MIC features. This time, we will use a max depth of <code class="inlineCode">5</code> because there are significantly fewer features:</p>
    <pre class="programlisting code"><code class="hljs-code">mdlname = <span class="hljs-string">'rf_5_f-mic'</span>
stime = timeit.default_timer()
reg_mdl = xgb.XGBRFRegressor(max_depth=<span class="hljs-number">5</span>, n_estimators=<span class="hljs-number">200</span>, seed=rand)
fitted_mdl = reg_mdl.fit(X_train[mic_cols], y_train)
reg_mdls[mdlname][<span class="hljs-string">'num_feat'</span>] = <span class="hljs-built_in">sum</span>(
    reg_mdls[mdlname][<span class="hljs-string">'fitted'</span>].feature_importances_ &gt; <span class="hljs-number">0</span>
)
</code></pre>
    <p class="normal">Now, let’s plot the profits for <strong class="keyWord">Test</strong> and <strong class="keyWord">Train</strong> as we did in <em class="italic">Figure 10.3</em>, but for the MIC model. It will produce what’s shown in <em class="italic">Figure 10.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_05.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.5: Comparison between profit, costs, and ROI for the test and train datasets for a model with MIC features across thresholds</p>
    <p class="normal">In <em class="italic">Figure 10.5</em>, you<a id="_idIndexMarker1074"/> can tell that there is quite a bit of difference between <strong class="keyWord">Test</strong> and <strong class="keyWord">Train</strong>, yet similarities indicate minimal overfitting. For instance, the highest profitability can be found between 0.66 and 0.75 for <strong class="keyWord">Train</strong>, and while <strong class="keyWord">Test</strong> is mostly between 0.66 and 0.7, it only gradually <a id="_idIndexMarker1075"/>decreases afterward.</p>
    <p class="normal">Although we have visually examined the MIC model, it’s nice to have some reassurance by looking at raw metrics. Next, we will compare all the models we have trained so far using consistent metrics.</p>
    <h2 id="_idParaDest-281" class="heading-2">Comparing filter-based methods</h2>
    <p class="normal">We <a id="_idIndexMarker1076"/>have been saving metrics into a dictionary (<code class="inlineCode">reg_mdls</code>), which we easily convert to a DataFrame and output as we have done before, but this time we sort by <code class="inlineCode">max_profit_test</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">display_mdl_metrics(reg_mdls, <span class="hljs-string">'max_profit_test'</span>)
</code></pre>
    <p class="normal">The preceding snippet generated what is shown in <em class="italic">Figure 10.6</em>. It is evident that the filter MIC model is the least overfitted of all. It ranked higher than more complex models with more features and took less time to train than any model. Its speed is an advantage for hyperparameter tuning. What if we wanted to find the best classification target thresholds or MIC <em class="italic">k</em>? We won’t do this now, but we would likely get a better model if we ran every combination, but it would take time to do and even more with more features:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_06.png" alt="Application, table  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 10.6: Comparing metrics for all base models and filter-based feature-selected models</p>
    <p class="normal">In <em class="italic">Figure 10.6</em>, we<a id="_idIndexMarker1077"/> can tell that the correlation filter model (<code class="inlineCode">rf_11_f-corr</code>) performs worse than the model with more features and an equal amount of <code class="inlineCode">max_depth</code> <code class="inlineCode">(rf_11_all)</code>, which suggests that we must have removed an important feature. As cautioned in that section, the problem with blindly setting thresholds and removing anything above it is that you can inadvertently remove something useful. Not all extremely correlated and uncorrelated features are useless, so further inspection is required. Next, we will explore some embedded methods that, when combined with cross-validation, require less oversight.</p>
    <h1 id="_idParaDest-282" class="heading-1">Exploring embedded feature selection methods</h1>
    <p class="normal"><strong class="keyWord">Embedded methods</strong> exist <a id="_idIndexMarker1078"/>within models themselves by naturally selecting features during training. You can leverage the intrinsic properties of any model that has them to capture the features selected:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Tree-based models</strong>: For<a id="_idIndexMarker1079"/> instance, we have used the following code many times to count the number of features used by the RF models, which is evidence of feature selection naturally occurring in the learning process:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">sum</span>(reg_mdls[mdlname][<span class="hljs-string">'fitted'</span>].feature_importances_ &gt; <span class="hljs-number">0</span>)
</code></pre>
     
    <p class="normal">XGBoost’s RF uses gain by default, which is the average decrease in error in all splits where it used the feature to compute feature importance. We can increase the <a id="_idIndexMarker1080"/>threshold above 0 to select even fewer features according to their relative contribution. However, by constraining the trees’ depth, we forced the model to choose even fewer features already.</p> </li>
    </ul>
    <ul>
      <li class="bulletList"><strong class="keyWord">Regularized models with coefficients</strong>: We will study this further in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>, but many model classes <a id="_idIndexMarker1081"/>can incorporate penalty-based regularization, such as L1, L2, and elastic net. However, not all of them have intrinsic parameters such as coefficients that can be extracted to determine which features were penalized.</li>
    </ul>
    <p class="normal">This section <a id="_idIndexMarker1082"/>will only cover regularized models given that we are using a tree-based model already. It’s best to leverage different model classes to get different perspectives on what features matter the most.</p>
    <p class="normal">We covered some of these models in <em class="chapterRef">Chapter 3</em>, <em class="italic">Interpretation Challenges</em>, but these are a few model classes that incorporate penalty-based regularization and output feature-specific coefficients:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Least Absolute Shrinkage and Selection Operator</strong> (<strong class="keyWord">LASSO</strong>): Because it uses L1 penalty in the<a id="_idIndexMarker1083"/> loss function, LASSO can set coefficients to 0.</li>
      <li class="bulletList"><strong class="keyWord">Least-Angle Regression</strong> (<strong class="keyWord">LARS</strong>): Similar<a id="_idIndexMarker1084"/> to LASSO but is vector-based and is more suitable for high-dimensional data. It is also fairer toward equally correlated features.</li>
      <li class="bulletList"><strong class="keyWord">Ridge regression</strong>: Uses<a id="_idIndexMarker1085"/> L2 penalty in the loss function and because of this, can only shrink coefficients of irrelevance close to 0 but not to 0.</li>
      <li class="bulletList"><strong class="keyWord">Elastic net regression</strong>: Uses <a id="_idIndexMarker1086"/>a mix of both L1 and L2 norms as penalties.</li>
      <li class="bulletList"><strong class="keyWord">Logistic regression</strong>: Contingent<a id="_idIndexMarker1087"/> on the solver, it can handle L1, L2, or elastic net penalties.</li>
    </ul>
    <p class="normal">There are also several variations of the preceding models, such<a id="_idIndexMarker1088"/> as <strong class="keyWord">LASSO LARS</strong>, which is a LASSO fit using the LARS algorithm, or<a id="_idIndexMarker1089"/> even <strong class="keyWord">LASSO LARS IC</strong>, which is the same but uses AIC or BIC criteria for the model section:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Akaike’s Information Criteria</strong> (<strong class="keyWord">AIC</strong>): A <a id="_idIndexMarker1090"/>relative goodness of fit measure founded in information theory</li>
      <li class="bulletList"><strong class="keyWord">Bayesian Information Criteria</strong> (<strong class="keyWord">BIC</strong>): Has <a id="_idIndexMarker1091"/>a similar formula to AIC but has a different penalty term</li>
    </ul>
    <p class="normal">OK, now let’s <a id="_idIndexMarker1092"/>use <code class="inlineCode">SelectFromModel</code> to extract top features from a LASSO model. We will use <code class="inlineCode">LassoCV</code> because it can automatically cross-validate to find the optimal penalty strength. Once you fit it, we can get the feature mask with <code class="inlineCode">get_support()</code>. We can then print the number of features and list of features:</p>
    <pre class="programlisting code"><code class="hljs-code">lasso_selection = SelectFromModel(
    LassoCV(n_jobs=-<span class="hljs-number">1</span>, random_state=rand)
)
lasso_selection.fit(X_train, y_train)
lasso_cols = X_train.columns[lasso_selection.get_support()].tolist()
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(lasso_cols))
<span class="hljs-built_in">print</span>(lasso_cols)
</code></pre>
    <p class="normal">The preceding code outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">7
['ODATEDW', 'TCODE', 'POP901', 'POP902', 'HV2', 'RAMNTALL', 'MAXRDATE']
</code></pre>
    <p class="normal">Now, let’s try the same but with <code class="inlineCode">LassoLarsCV</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">llars_selection = SelectFromModel(LassoLarsCV(n_jobs=-<span class="hljs-number">1</span>))
llars_selection.fit(X_train, y_train)
llars_cols = X_train.columns[llars_selection.get_support()].tolist()
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(llars_cols))
<span class="hljs-built_in">print</span>(llars_cols)
</code></pre>
    <p class="normal">The preceding snippet produces the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">8
['RECPGVG', 'MDMAUD', 'HVP3', 'RAMNTALL', 'LASTGIFT', 'AVGGIFT', 'MDMAUD_A', 'DOMAIN_SOCIALCLS']
</code></pre>
    <p class="normal">LASSO shrunk coefficients for all but seven features to 0, and LASSO LARS did the same but for eight. However, notice how there’s no overlap between both lists! OK, so let’s try incorporating AIC model selection into LASSO LARS with <code class="inlineCode">LassoLarsIC</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">llarsic_selection = SelectFromModel(LassoLarsIC(criterion=<span class="hljs-string">'aic'</span>))
llarsic_selection.fit(X_train, y_train)
llarsic_cols = X_train.columns[
    llarsic_selection.get_support()
].tolist()
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(llarsic_cols))
<span class="hljs-built_in">print</span>(llarsic_cols)
</code></pre>
    <p class="normal">The preceding snippet generates the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">111
['TCODE', 'STATE', 'MAILCODE', 'RECINHSE', 'RECP3', 'RECPGVG', 'RECSWEEP',..., 'DOMAIN_URBANICITY', 'DOMAIN_SOCIALCLS', 'ZIP_LON']
</code></pre>
    <p class="normal">It’s the same <a id="_idIndexMarker1093"/>algorithm but with a different method for selecting the value of the regularization parameter. Note how this less-conservative approach expands the number of features to 111. Now, so far, all of the methods we have used have the L1 norm. Let’s try one with L2—more specifically, L2-penalized logistic regression. We do exactly what we did before, but this time, we fit with the binary classification targets (<code class="inlineCode">y_train_class</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">log_selection = SelectFromModel(
    LogisticRegression(
        C=<span class="hljs-number">0.0001</span>,
        solver=<span class="hljs-string">'sag'</span>,
        penalty=<span class="hljs-string">'l2'</span>,
        n_jobs=-<span class="hljs-number">1</span>,
        random_state=rand
    )
)
log_selection.fit(X_train, y_train_class)
log_cols = X_train.columns[log_selection.get_support()].tolist()
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(log_cols))
<span class="hljs-built_in">print</span>(log_cols)
</code></pre>
    <p class="normal">The preceding code produces the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">87
['ODATEDW', 'TCODE', 'STATE', 'POP901', 'POP902', 'POP903', 'ETH1', 'ETH2', 'ETH5', 'CHIL1', 'HHN2',..., 'AMT_7', 'ZIP_LON']
</code></pre>
    <p class="normal">Now that we have a few feature subsets to test, we can place their names into a list (<code class="inlineCode">fsnames</code>) and the feature subset lists into another list (<code class="inlineCode">fscols</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">fsnames = [<span class="hljs-string">'e-lasso'</span>, <span class="hljs-string">'e-llars'</span>, <span class="hljs-string">'e-llarsic'</span>, <span class="hljs-string">'e-logl2'</span>]
fscols = [lasso_cols, llars_cols, llarsic_cols, log_cols]
</code></pre>
    <p class="normal">We can then<a id="_idIndexMarker1094"/> iterate across all list names and fit and evaluate our <code class="inlineCode">XGBRFRegressor</code> model as we have done before, but increasing <code class="inlineCode">max_depth</code> at every iteration:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">train_mdls_with_fs</span>(<span class="hljs-params">reg_mdls, fsnames, fscols, depths</span>):
    <span class="hljs-keyword">for</span> i, fsname <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">enumerate</span>(fsnames), total=<span class="hljs-built_in">len</span>(fsnames)):
       depth = depths[i]
       cols = fscols[i]
       mdlname = <span class="hljs-string">'rf_'</span>+<span class="hljs-built_in">str</span>(depth)+<span class="hljs-string">'</span><span class="hljs-string">_'</span>+fsname
       stime = timeit.default_timer()
       reg_mdl = xgb.XGBRFRegressor(
           max_depth=depth, n_estimators=<span class="hljs-number">200</span>, seed=rand
       )
       fitted_mdl = reg_mdl.fit(X_train[cols], y_train)
       reg_mdls[mdlname][<span class="hljs-string">'num_feat'</span>] = <span class="hljs-built_in">sum</span>(
           reg_mdls[mdlname][<span class="hljs-string">'fitted'</span>].feature_importances_ &gt; <span class="hljs-number">0</span>
<span class="hljs-number">       </span>)
train_mdls_with_fs(reg_mdls, fsnames, fscols, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])
</code></pre>
    <p class="normal">Now, let’s see how our embedded feature-selected models fare in comparison to the filtered ones. We will rerun the code we ran to output what was shown in <em class="italic">Figure 10.6</em>. This time, we will get what is shown in <em class="italic">Figure 10.7</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_07.png" alt="Table  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 10.7: Comparing metrics for all base models and filter-based and embedded feature-selected models</p>
    <p class="normal">According<a id="_idIndexMarker1095"/> to <em class="italic">Figure 10.7</em>, three of the four embedded methods we tried produced models with the lowest test RMSE (<code class="inlineCode">rf_5_e-llarsic</code>, <code class="inlineCode">rf_e-lasso</code>, and <code class="inlineCode">rf_4_e-llars</code>). They also all trained much faster than the others and are more profitable than any other model of equal complexity. One of them (<code class="inlineCode">rf_5_e-llarsic</code>) is even highly profitable. Compare this with <code class="inlineCode">rf_9_all</code> with similar test profitability to see how performance diverges from the training data.</p>
    <h1 id="_idParaDest-283" class="heading-1">Discovering wrapper, hybrid, and advanced feature selection methods</h1>
    <p class="normal">The feature <a id="_idIndexMarker1096"/>selection methods studied so far are computationally inexpensive because they require no model fitting or fitting simpler white-box models. In this section, we will learn about other, more exhaustive methods with many possible tuning options. The categories of methods included here are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Wrapper</strong>: Exhaustively <a id="_idIndexMarker1097"/>searches for the best subset of features by fitting an ML model using a search strategy that measures improvement on a metric.</li>
      <li class="bulletList"><strong class="keyWord">Hybrid</strong>: A<a id="_idIndexMarker1098"/> method<a id="_idIndexMarker1099"/> that combines embedded and filter methods with wrapper methods.</li>
      <li class="bulletList"><strong class="keyWord">Advanced</strong>: A method <a id="_idIndexMarker1100"/>that doesn’t fall into any of the previously<a id="_idIndexMarker1101"/> discussed categories. Examples include dimensionality reduction, model-agnostic <a id="_idIndexMarker1102"/>feature importance, and <strong class="keyWord">Genetic Algorithms</strong> (<strong class="keyWord">GAs</strong>).</li>
    </ul>
    <p class="normal">And now, let’s get started with wrapper methods!</p>
    <h2 id="_idParaDest-284" class="heading-2">Wrapper methods</h2>
    <p class="normal">The<a id="_idIndexMarker1103"/> concept behind <strong class="keyWord">wrapper methods</strong> is reasonably simple: evaluate different subsets of features on the ML model and choose the one that achieves the best score in a predetermined objective function. What varies here is the search strategy:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Sequential Forward Selection</strong> (<strong class="keyWord">SFS</strong>): This <a id="_idIndexMarker1104"/>approach<a id="_idIndexMarker1105"/> begins without a feature and adds one, one at a time.</li>
      <li class="bulletList"><strong class="keyWord">Sequential Forward Floating Selection</strong> (<strong class="keyWord">SFFS</strong>): The same <a id="_idIndexMarker1106"/>as the previous except for<a id="_idIndexMarker1107"/> every feature it adds, it can remove one, as long as the objective function increases.</li>
      <li class="bulletList"><strong class="keyWord">Sequential Backward Selection</strong> (<strong class="keyWord">SBS</strong>): This<a id="_idIndexMarker1108"/> process begins with all features<a id="_idIndexMarker1109"/> present and eliminates one feature at a time.</li>
      <li class="bulletList"><strong class="keyWord">Sequential Floating Backward Selection</strong> (<strong class="keyWord">SFBS</strong>): The same <a id="_idIndexMarker1110"/>as the previous except for<a id="_idIndexMarker1111"/> every feature it removes, it can add one, as long as the objective function increases.</li>
      <li class="bulletList"><strong class="keyWord">Exhaustive Feature Selection</strong> (<strong class="keyWord">EFS</strong>): This<a id="_idIndexMarker1112"/> approach<a id="_idIndexMarker1113"/> seeks all possible combinations of features.</li>
      <li class="bulletList"><strong class="keyWord">BiDirectional Search</strong> (<strong class="keyWord">BDS</strong>): This<a id="_idIndexMarker1114"/> last <a id="_idIndexMarker1115"/>one simultaneously allows both forward and backward function selection to get one unique solution.</li>
    </ul>
    <p class="normal">These methods are greedy algorithms because they solve the problem piece by piece, choosing pieces based on their immediate benefit. Even though they may arrive at a global maximum, they take an approach more suited for finding local maxima. Depending on the number of features, they might be too computationally expensive to be practical, especially EFS, which grows exponentially. Another important distinction is the difference between forward methods’ accuracy increases as features are added and backward ones, monitor accuracy decreases as features are removed. To allow for shorter search times, we will do two things:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Start <a id="_idIndexMarker1116"/>our search with the features collectively selected by other methods to have a smaller feature space to choose from. To that end, we combine feature lists from several methods into a single <code class="inlineCode">top_cols</code> list:
        <pre class="programlisting code"><code class="hljs-code">top_cols = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(mic_cols).union(<span class="hljs-built_in">set</span>(llarsic_cols)\
).union(<span class="hljs-built_in">set</span>(log_cols)))
<span class="hljs-built_in">len</span>(top_cols)
</code></pre>
      </li>
      <li class="numberedList">Sample our datasets so that ML models speed up. We can use <code class="inlineCode">np.random.choice</code> to do a random selection of row indexes without replacement:
        <pre class="programlisting code"><code class="hljs-code">sample_size = <span class="hljs-number">0.1</span>
sample_train_idx = np.random.choice(
    X_train.shape[<span class="hljs-number">0</span>],
    math.ceil(X_train.shape[<span class="hljs-number">0</span>]*sample_size),
    replace=<span class="hljs-literal">False</span>
)
sample_test_idx = np.random.choice(
    X_test.shape[<span class="hljs-number">0</span>],
    math.ceil(X_test.shape[<span class="hljs-number">0</span>]*sample_size),
    replace=<span class="hljs-literal">False</span>
)
</code></pre>
      </li>
    </ol>
    <p class="normal">Out of the wrapper methods presented, we will only perform SFS, given how time-consuming they are. Still, with an even smaller dataset, you can try the other options, which the <code class="inlineCode">mlextend</code> library also supports.</p>
    <h3 id="_idParaDest-285" class="heading-3">Sequential forward selection (SFS)</h3>
    <p class="normal">The<a id="_idIndexMarker1117"/> first argument of a wrapper <a id="_idIndexMarker1118"/>method is an unfitted estimator (a model). In <code class="inlineCode">SequentialFeatureSelector</code>, we are placing a <code class="inlineCode">LinearDiscriminantAnalysis</code> model. Other arguments include the direction (<code class="inlineCode">forward=true</code>), whether it’s floating (<code class="inlineCode">floating=False</code>), which means it might undo the previous exclusion or inclusion of a feature, the number of features we wish to select (<code class="inlineCode">k_features=27</code>), the number of cross-validations (<code class="inlineCode">cv=3</code>), and the loss function to use (<code class="inlineCode">scoring=f1</code>). Some recommended optional arguments to enter are the verbosity (<code class="inlineCode">verbose=2</code>) and the number of jobs to run in parallel (<code class="inlineCode">n_jobs=-1</code>). Since it could take a while, we’ll definitely want it to output something and use as many processors as possible:</p>
    <pre class="programlisting code"><code class="hljs-code">sfs_lda = SequentialFeatureSelector(
    LinearDiscriminantAnalysis(n_components=<span class="hljs-number">1</span>),
    forward=<span class="hljs-literal">True</span>,
    floating=<span class="hljs-literal">False</span>,
    k_features=<span class="hljs-number">100</span>,
    cv=<span class="hljs-number">3</span>,
    scoring=<span class="hljs-string">'f1'</span>,
    verbose=<span class="hljs-number">2</span>,
    n_jobs=-<span class="hljs-number">1</span>
)
sfs_lda = sfs_lda.fit(X_train.iloc[sample_train_idx][top_cols],\
                      y_train_class[sample_train_idx])
sfs_lda_cols = X_train.columns[<span class="hljs-built_in">list</span>(sfs_lda.k_feature_idx_)].tolist()
</code></pre>
    <p class="normal">Once we fit the SFS, it will return the index of features that have been selected with <code class="inlineCode">k_feature_idx_</code>, and we can use those to subset the columns and obtain the list of feature names.</p>
    <h2 id="_idParaDest-286" class="heading-2">Hybrid methods</h2>
    <p class="normal">Starting <a id="_idIndexMarker1119"/>with<a id="_idIndexMarker1120"/> 435 features, there are over 10<sup class="superscript">42</sup> combinations of 27 feature subsets alone! So, you can see how EFS would be impractical in such a large feature space. Therefore, except for EFS on the entire dataset, wrapper methods will invariably take some shortcuts to select the features. Whether you are going forward, backward, or both, as long as you are not assessing every single combination of features, you could easily miss out on the best one.</p>
    <p class="normal">However, we can leverage the more rigorous, exhaustive search approach of wrapper methods with filter and embedded methods’ efficiency. The result of this is <strong class="keyWord">hybrid methods</strong>. For instance, you could employ filter or embedded methods to derive only the top 10 features and perform EFS or SBS on only those.</p>
    <h3 id="_idParaDest-287" class="heading-3">Recursive Feature Elimination (RFE)</h3>
    <p class="normal">Another, more<a id="_idIndexMarker1121"/> common approach is something such as SBS, but instead of removing features based on improving a metric <a id="_idIndexMarker1122"/>alone, using the model’s intrinsic parameters to rank the features and only removing the least ranked. The name of this approach is <strong class="keyWord">Recursive Feature Elimination</strong> (<strong class="keyWord">RFE</strong>), and it is a hybrid between embedded and wrapper methods. We can only use models with <code class="inlineCode">feature_importances_</code> or coefficients (<code class="inlineCode">coef_</code>) because this is how the method knows what features to remove. Model classes in scikit-learn with these attributes are classified under <code class="inlineCode">linear_model</code>, <code class="inlineCode">tree</code>, and <code class="inlineCode">ensemble</code>. Also, scikit-learn-compatible versions of XGBoost, LightGBM, and CatBoost also have <code class="inlineCode">feature_importances_</code>.</p>
    <p class="normal">We will use the cross-validated version of RFE because it’s more reliable. <code class="inlineCode">RFECV</code> takes the estimator first (<code class="inlineCode">LinearDiscriminantAnalysis</code>). We can then define <code class="inlineCode">step</code>, which sets how many features it should remove in every iteration, the number of cross-validations (<code class="inlineCode">cv</code>), and the metric used for evaluation (<code class="inlineCode">scoring</code>). Lastly, it is recommended to set the verbosity (<code class="inlineCode">verbose=2</code>) and leverage as many processors as possible (<code class="inlineCode">n_jobs=-1</code>). To speed it up, we will use a sample again for the training and start with the 267 for <code class="inlineCode">top_cols</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">rfe_lda = RFECV(
    LinearDiscriminantAnalysis(n_components=<span class="hljs-number">1</span>),
    step=<span class="hljs-number">2</span>, cv=<span class="hljs-number">3</span>, scoring=<span class="hljs-string">'f1'</span>, verbose=<span class="hljs-number">2</span>, n_jobs=-<span class="hljs-number">1</span>
)
rfe_lda.fit(
    X_train.iloc[sample_train_idx][top_cols],
    y_train_class[sample_train_idx]
)
rfe_lda_cols = np.array(top_cols)[rfe_lda.support_].tolist()
</code></pre>
    <p class="normal">Next, we will try different methods that don’t relate to the main three feature selection categories: filter, embedded, and wrapper.</p>
    <h2 id="_idParaDest-288" class="heading-2">Advanced methods</h2>
    <p class="normal">Many<a id="_idIndexMarker1123"/> methods <a id="_idIndexMarker1124"/>can be categorized under advanced feature selection methods, including the following subcategories:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Model-agnostic feature importance</strong>: Any<a id="_idIndexMarker1125"/> feature importance method covered in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>, can be used to<a id="_idIndexMarker1126"/> obtain the top features of a model for feature selection purposes.</li>
      <li class="bulletList"><strong class="keyWord">Genetic algorithms</strong>: This<a id="_idIndexMarker1127"/> is a wrapper method in the sense that it “wraps” a model assessing predictive performance across many feature subsets. However, unlike the wrapper methods we examined, it doesn’t make the most locally optimal choice. It’s more optimized to work with large feature spaces. It’s called genetic because it’s inspired by biology—natural selection, specifically.</li>
      <li class="bulletList"><strong class="keyWord">Dimensionality reduction</strong>: Some<a id="_idIndexMarker1128"/> dimensionality reduction methods, such as <strong class="keyWord">Principal Component Analysis</strong> (<strong class="keyWord">PCA</strong>), can <a id="_idIndexMarker1129"/>return explained variance on a feature basis. For others, such as factor analysis, it can be derived from other outputs. Explained variance can be used to rank features.</li>
      <li class="bulletList"><strong class="keyWord">Autoencoders</strong>: We <a id="_idIndexMarker1130"/>won’t delve into this one, but deep learning can be leveraged for feature selection with autoencoders. This method has many variants you can find in Google Scholar and is not widely adopted in industry.</li>
    </ul>
    <p class="normal">We will briefly cover the first two in this section so you can understand how they can be implemented. Let’s dive right in!</p>
    <h3 id="_idParaDest-289" class="heading-3">Model-agnostic feature importance</h3>
    <p class="normal">A popular <a id="_idIndexMarker1131"/>model-agnostic feature importance<a id="_idIndexMarker1132"/> method that we have used throughout this book is SHAP, and it has many properties that make it more reliable than other methods. In the following code, we can take our best model and extract <code class="inlineCode">shap_values</code> for it using <code class="inlineCode">TreeExplainer</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">fitted_rf_mdl = reg_mdls[<span class="hljs-string">'rf_11_all'</span>][<span class="hljs-string">'fitted'</span>]
shap_rf_explainer = shap.TreeExplainer(fitted_rf_mdl)
shap_rf_values = shap_rf_explainer.shap_values(
    X_test_orig.iloc[sample_test_idx]
)
shap_imps = pd.DataFrame(
    {<span class="hljs-string">'col'</span>:X_train_orig.columns, <span class="hljs-string">'imp'</span>:np.<span class="hljs-built_in">abs</span>(shap_rf_values).mean(<span class="hljs-number">0</span>)}
).sort_values(by=<span class="hljs-string">'imp'</span>,ascending=<span class="hljs-literal">False</span>)
shap_cols = shap_imps.head(<span class="hljs-number">120</span>).col.tolist()
</code></pre>
    <p class="normal">Then, the average for the absolute value of the SHAP values across the first dimension is what provides us with a ranking for each feature. We put this value in a DataFrame and sort it as we did for PCA. Lastly, also take the top 120 and place them in a list (<code class="inlineCode">shap_cols</code>).</p>
    <h3 id="_idParaDest-290" class="heading-3">Genetic algorithms</h3>
    <p class="normal">GAs are a<a id="_idIndexMarker1133"/> stochastic global optimization<a id="_idIndexMarker1134"/> technique inspired by natural selection, which wrap a model much like wrapper methods do. However, they don’t follow a sequence on a step-by-step basis. GAs don’t have iterations but generations, which include populations of chromosomes. Each chromosome is a binary representation of your feature space, where 1 means to select a feature and 0 to not. Each generation is produced with the following operations:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Selection</strong>: Like <a id="_idIndexMarker1135"/>with natural selection, this is partially random (exploration) and partially based on what has already worked (exploitation). What has worked is its fitness. Fitness is assessed with a “scorer” much like wrapper methods. Poor fitness chromosomes are removed, whereas good ones get to reproduce through “crossover.”</li>
      <li class="bulletList"><strong class="keyWord">Crossover</strong>: Randomly, some<a id="_idIndexMarker1136"/> good bits (or features) of each parent go to a child.</li>
      <li class="bulletList"><strong class="keyWord">Mutation</strong>: Even when a <a id="_idIndexMarker1137"/>chromosome has proved effective, given a low mutation rate, it will occasionally mutate or flip one of its bits, in other words, features.</li>
    </ul>
    <p class="normal">The<a id="_idIndexMarker1138"/> Python implementation we will use has many options. We won’t explain all of them here, but they are documented well in the code should you be interested. The first attribute is the estimator. We can also define the cross-validation iterations (<code class="inlineCode">cv=3</code>) and <code class="inlineCode">scoring</code> to determine whether chromosomes are fit. There are some important probabilistic properties, such as the probability for a mutated bit (<code class="inlineCode">mutation_probability</code>) and that bits will get exchanged (<code class="inlineCode">crossover_probability</code>). Generation-wise, <code class="inlineCode">n_gen_no_change</code> provides a means for early stopping if generations haven’t improved, and <code class="inlineCode">generations</code> by default is 40, but we will use 5. We can fit <code class="inlineCode">GeneticSelectionCV</code> as you would any model. It can take a while, so it is best to define the verbosity and allow it to use all the processing capacity. Once finished, we can use the Boolean mask (<code class="inlineCode">support_</code>) to subset the features:</p>
    <pre class="programlisting code"><code class="hljs-code">ga_rf = GAFeatureSelectionCV(
    RandomForestRegressor(random_state=rand, max_depth=<span class="hljs-number">3</span>),
    cv=<span class="hljs-number">3</span>,
    scoring=<span class="hljs-string">'neg_root_mean_squared_error'</span>,
    crossover_probability=<span class="hljs-number">0.8</span>,
    mutation_probability=<span class="hljs-number">0.1</span>,
    generations=<span class="hljs-number">5</span>, n_jobs=-<span class="hljs-number">1</span>
)
ga_rf = ga_rf.fit(
    X_train.iloc[sample_train_idx][top_cols].values,
    y_train[sample_train_idx]
)
ga_rf_cols = np.array(top_cols)[ga_rf.best_features_].tolist()
</code></pre>
    <p class="normal">OK, now that we have covered a wide variety of wrapper, hybrid, and advanced feature selection methods in this section, let’s evaluate all of them at once and compare the results.</p>
    <h2 id="_idParaDest-291" class="heading-2">Evaluating all feature-selected models</h2>
    <p class="normal">As we have<a id="_idIndexMarker1139"/> done with embedded methods, we can place feature subset names (<code class="inlineCode">fsnames</code>), lists (<code class="inlineCode">fscols</code>), and corresponding <code class="inlineCode">depths</code> in lists:</p>
    <pre class="programlisting code"><code class="hljs-code">fsnames = [<span class="hljs-string">'w-sfs-lda'</span>, <span class="hljs-string">'h-rfe-lda'</span>, <span class="hljs-string">'a-shap'</span>, <span class="hljs-string">'a-ga-rf'</span>]
fscols = [sfs_lda_cols, rfe_lda_cols, shap_cols, ga_rf_cols]
depths = [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]
</code></pre>
    <p class="normal">Then, we can use the two functions we created to first iterate across all feature subsets, training and evaluating a model with them. Then the second function outputs the results of the evaluation in a DataFrame with previously trained models:</p>
    <pre class="programlisting code"><code class="hljs-code">train_mdls_with_fs(reg_mdls, fsnames, fscols, depths) 
display_mdl_metrics(reg_mdls, <span class="hljs-string">'max_profit_test'</span>, max_depth=<span class="hljs-number">7</span>)
</code></pre>
    <p class="normal">This time, we are limiting the models to those with no more than a depth of 7 since those are very overfitted. The result of the snippet is depicted in <em class="italic">Figure 10.8</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_08.png" alt="Graphical user interface, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.8: Comparing metrics for all feature-selected models</p>
    <p class="normal"><em class="italic">Figure 10.8</em> shows how feature-selected models are more profitable than ones that include all the features compared at the same depths. Also, the embedded LASSO LARS with AIC (<code class="inlineCode">e-llarsic</code>) method and the MIC (<code class="inlineCode">f-mic</code>) filter method outperform all wrapper, hybrid, and advanced methods with the same depths. Still, we also impeded these methods by using a sample of the training dataset, which was necessary to speed up the process. Maybe they would have outperformed the top ones otherwise. However, the three feature selection methods that follow are pretty competitive:</p>
    <ul>
      <li class="bulletList">RFE with LDA: Hybrid method (<code class="inlineCode">h-rfe-lda</code>)</li>
      <li class="bulletList">Logistic regression with L2 regularization: Embedded method (<code class="inlineCode">e-logl2</code>)</li>
      <li class="bulletList">GAs with RF: Advanced method (<code class="inlineCode">a-ga-rf</code>)</li>
    </ul>
    <p class="normal">It <a id="_idIndexMarker1140"/>would make sense to spend many days running many variations of the methods reviewed in this book. For instance, perhaps RFE with L1 regularized logistic regression or GA with support vector machines with additional mutation yields the best model. There are so many different possibilities! Nevertheless, if you were forced to make a recommendation based on <em class="italic">Figure 10.8</em>, by profit alone, the 111-feature <code class="inlineCode">e-llarsic</code> is the best option, but it also has higher minimum costs and lower maximum ROI than any of the top models. There’s a trade-off. And even though it has among the highest test RMSEs, the 160-feature model (<code class="inlineCode">f-mic</code>) has a similar spread between max profit train and test and beat it in max ROI and min costs. Therefore, these are the two reasonable options. But before making a final determination, profitability would have to be compared side by side across different thresholds to assess when each model can make the most reliable predictions and at what costs and ROIs.</p>
    <h1 id="_idParaDest-292" class="heading-1">Considering feature engineering</h1>
    <p class="normal">Let’s assume that<a id="_idIndexMarker1141"/> the non-profit has chosen to use the model whose features were selected with LASSO LARS with AIC (<code class="inlineCode">e-llarsic</code>) but would like to evaluate whether you can improve it further. Now that you have removed over 300 features that might have only marginally improved predictive performance but mostly added noise, you are left with more relevant features. However, you also know that 8 features selected by <code class="inlineCode">e-llars</code> produced the same amount of RMSE as the 111 features. This means that while there’s something in those extra features that improves profitability, it does not improve the RMSE.</p>
    <p class="normal">From a feature selection standpoint, many things can be done to approach this problem. For instance, examine the overlap and difference of features between <code class="inlineCode">e-llarsic</code> and <code class="inlineCode">e-llars</code>, and do feature selection variations strictly on those features to see whether the RMSE dips on any combination while keeping or improving on current profitability. However, there’s also another possibility, which is feature engineering. There are a few important reasons you <a id="_idIndexMarker1142"/>would want to perform feature engineering at this stage:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Make model interpretation easier to understand</strong>: For instance, sometimes features have a scale that is not intuitive, or the scale is intuitive, but the distribution makes it hard to understand. As long as transformations to these features don’t worsen model performance, there’s value in transforming the features to understand the outputs of interpretation methods better. As you train models on more engineered features, you realize what works and why it does. This will help you understand the model and, more importantly, the data.</li>
      <li class="bulletList"><strong class="keyWord">Place guardrails on individual features</strong>: Sometimes, features have an uneven distribution, and models tend to overfit in sparser areas of the feature’s histogram or where influential outliers exist.</li>
      <li class="bulletList"><strong class="keyWord">Clean up counterintuitive interactions</strong>: Some interactions that models find make no sense and only exist because the features correlate, but not for the right reasons. They could be confounding variables or perhaps even redundant ones (such as the one we found in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>). You could decide to engineer an interaction feature or remove a redundant one.</li>
    </ul>
    <p class="normal">In reference to the last two reasons, we will examine feature engineering strategies in more detail in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>. This section will focus on the first reason, particularly because it’s a good place to start since it will allow you to understand the data better until you know it well enough to make more transformational changes.</p>
    <p class="normal">So, we are left with 111 features but have no idea how they relate to the target or each other. The first thing we ought to do is run a feature importance method. We can use SHAP’s <code class="inlineCode">TreeExplainer</code> on the <code class="inlineCode">e-llarsic</code> model. An advantage of <code class="inlineCode">TreeExplainer</code> is that it can compute SHAP interaction values, <code class="inlineCode">shap_interaction_values</code>. Instead of outputting an array of <code class="inlineCode">(N, 111)</code> dimensions where <em class="italic">N</em> is the number of observations as <code class="inlineCode">shap_values</code> does, it will output <code class="inlineCode">(N, 111, 111)</code>. We can produce a <code class="inlineCode">summary_plot</code> graph with it that ranks both individual features and interactions. The only difference for interaction values is you use <code class="inlineCode">plot_type="compact_dot"</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">winning_mdl = <span class="hljs-string">'rf_5_e-llarsic'</span>
fitted_rf_mdl = reg_mdls[winning_mdl][<span class="hljs-string">'fitted'</span>]
shap_rf_explainer = shap.TreeExplainer(fitted_rf_mdl)
shap_rf_interact_values = \
    shap_rf_explainer.shap_interaction_values(
        X_test.iloc[sample_test_idx][llarsic_cols]
    )
shap.summary_plot(
    shap_rf_interact_values,
    X_test.iloc[sample_test_idx][llarsic_cols],
    plot_type=<span class="hljs-string">"compact_dot"</span>,
    sort=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker1143"/>snippet produces the SHAP interaction summary plot shown in <em class="italic">Figure 10.9</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_09.png" alt="Graphical user interface, application, table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.9: SHAP interaction summary plot</p>
    <p class="normal">We can read <em class="italic">Figure 10.9</em> as we<a id="_idIndexMarker1144"/> would any summary plot except it includes bivariate interactions twice—first with one feature and then with another. For instance, <code class="inlineCode">MDMAUD_A* - CLUSTER</code> is the interaction SHAP values for that interaction from <code class="inlineCode">MDMAUD_A</code>'s perspective, so the feature values correspond to that feature alone, but the SHAP values are for the interaction. One thing that we can agree on here is that the plot is hard to read given the scale of the importance values and complexity of comparing bivariate interactions in no order. We will address this later.</p>
    <p class="normal">Throughout<a id="_idIndexMarker1145"/> this book, chapters with tabular data have started with a data dictionary. This one was an exception, given that there were 435 features to begin with. Now, it makes sense to at the very least understand what the top features are. The complete data dictionary can be found at <a href="https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98dic.txt"><span class="url">https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98dic.txt</span></a>, but some of the features have already been changed because of categorical encoding, so we will explain them in more detail here:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">MAXRAMNT</code>: Continuous, the dollar amount of the largest gift to date</li>
      <li class="bulletList"><code class="inlineCode">HVP2</code>: Discrete, percentage of homes with a value of &gt;= $150,000 in the neighborhoods of donors (values between 0 and 100)</li>
      <li class="bulletList"><code class="inlineCode">LASTGIFT</code>: Continuous, the dollar amount of the most recent gift</li>
      <li class="bulletList"><code class="inlineCode">RAMNTALL</code>: Continuous, the dollar amount of lifetime gifts to date</li>
      <li class="bulletList"><code class="inlineCode">AVGGIFT</code>: Continuous, the average dollar amount of gifts to date</li>
      <li class="bulletList"><code class="inlineCode">MDMAUD_A</code>: Ordinal, the donation amount code for donors who have given a $100 + gift at any time in their giving history (values between 0 and 3, and -1 for those who have never exceeded $100). The amount code is the third byte of an <strong class="keyWord">RFA</strong> (<strong class="keyWord">recency/frequency/amount</strong>) major customer matrix code, which is the amount given. The categories are as follows:</li>
    </ul>
    <p class="normal">0: Less than $100 (low dollar)</p>
    <p class="normal">1: $100 – 499 (core)</p>
    <p class="normal">2: $500 – 999 (major)</p>
    <p class="normal">3: $1,000 + (top)</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">NGIFTALL</code>: Discrete, number of lifetime gifts to date</li>
      <li class="bulletList"><code class="inlineCode">AMT_14</code>: Ordinal, donation amount code of the RFA for the 14th previous promotion (2 years prior), which corresponds to the last dollar amount given back then:</li>
    </ul>
    <p class="normal">0: $0.01 – 1.99</p>
    <p class="normal">1: $2.00 – 2.99</p>
    <p class="normal">2: $3.00 – 4.99</p>
    <p class="normal">3: $5.00 – 9.99</p>
    <p class="normal">4: $10.00 – 14.99</p>
    <p class="normal">5: $15.00 – 24.99</p>
    <p class="normal">6: $25.00 and above</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">DOMAIN_SOCIALCLS</code>: Nominal, <strong class="keyWord">Socio-Economic Status</strong> (<strong class="keyWord">SES</strong>) of <a id="_idIndexMarker1146"/>the neighborhood, which combines with <code class="inlineCode">DOMAIN_URBANICITY</code> (0: Urban, 1: City, 2: Suburban, 3: Town, and 4: Rural), meaning the following:</li>
    </ul>
    <p class="normal">1: Highest SES</p>
    <p class="normal">2: Average SES, except above average for urban communities</p>
    <p class="normal">3: Lowest SES, except below average for urban communities</p>
    <p class="normal">4: Lowest SES for urban communities only</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">CLUSTER</code>: Nominal, code indicating which cluster group the donor falls in</li>
      <li class="bulletList"><code class="inlineCode">MINRAMNT</code>: Continuous, dollar amount of the smallest gift to date</li>
      <li class="bulletList"><code class="inlineCode">LSC2</code>: Discrete, percentage of Spanish-speaking families in the donor’s neighborhood (values between 0 and 100)</li>
      <li class="bulletList"><code class="inlineCode">IC15</code>: Discrete, percentage of families with an income of &lt; $15,000 in the donor’s neighborhood (values between 0 and 100)</li>
    </ul>
    <p class="normal">The following insights <a id="_idIndexMarker1147"/>can be distilled from the preceding dictionary and <em class="italic">Figure 10.9</em>:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Gift amounts prevail</strong>: Seven of the top features pertain to gift amounts, whether it’s a total, min, max, average, or last. If you include the count of gifts (<code class="inlineCode">NGIFTALL</code>), there are eight features involving donation history, making complete sense. So, why is this relevant? Because they are likely highly correlated and understanding how could hold the keys to improving the model. Perhaps other features can be created that distill these relationships much better.</li>
      <li class="bulletList"><strong class="keyWord">High values of continuous gift amount features have high SHAP values</strong>: Plot a box plot of any of those features like this, <code class="inlineCode">plt.boxplot(X_test.MAXRAMNT)</code>, and you’ll see how right-skewed these features are. Perhaps a transformation such as breaking them into bins—called “discretization”—or using a different scale, such as logarithmic (try <code class="inlineCode">plt.boxplot(np.log(X_test.MAXRAMNT))</code>), can help interpret these features but also help find the pockets where the likelihood of donation dramatically increases.</li>
      <li class="bulletList"><strong class="keyWord">Relationship with the 14th previous promotion</strong>: What happened two years before they made that promotion connect to the one denoted in the dataset labels? Were the promotional materials similar? Is there a seasonality factor occurring at the same time every couple of years? Maybe you can engineer a feature that better identifies this phenomenon.</li>
      <li class="bulletList"><strong class="keyWord">Inconsistent classifications</strong>: <code class="inlineCode">DOMAIN_SOCIALCLS</code> has different categories depending on the <code class="inlineCode">DOMAIN_URBANICITY</code> value. We can make this consistent by using all five categories in the scale (Highest, Above Average, Average, Below Average, and Lowest) even if this means non-urban donors would be using only three. The advantage to doing this would be easier interpretation, and it’s highly unlikely it would adversely impact the model’s performance.</li>
    </ul>
    <p class="normal">The<a id="_idIndexMarker1148"/> SHAP interaction summary plot can be useful for identifying feature and interaction rankings and some commonalities between them, but in this case (see <em class="italic">Figure 10.9</em>), it was hard to read. But to dig deeper into interactions, you first need to quantify their impact. To this end, let’s create a heatmap with only the top interactions as measured by their mean absolute SHAP value (<code class="inlineCode">shap_rf_interact_avgs</code>). We should then set all the diagonal values to 0 (<code class="inlineCode">shap_rf_interact_avgs_nodiag</code>) because these aren’t interactions but feature SHAP values, and it’s easier to observe the interactions without them. We can place this matrix in a DataFrame, but it’s a DataFrame of 111 columns and 111 rows, so to filter it by those features with the most interactions, we sum them and rank them with <code class="inlineCode">scipy</code>'s <code class="inlineCode">rankdata</code>. Then, we use the ranking to identify the 12 most interactive features (<code class="inlineCode">most_interact_cols</code>) and subset the DataFrame by them. Finally, we plot the DataFrame as a heatmap:</p>
    <pre class="programlisting code"><code class="hljs-code">shap_rf_interact_avgs = np.<span class="hljs-built_in">abs</span>(shap_rf_interact_values).mean(<span class="hljs-number">0</span>)
shap_rf_interact_avgs_nodiag = shap_rf_interact_avgs.copy()
np.fill_diagonal(shap_rf_interact_avgs_nodiag, <span class="hljs-number">0</span>)
shap_rf_interact_df = pd.DataFrame(shap_rf_interact_avgs_nodiag)
shap_rf_interact_df.columns = X_test[llarsic_cols].columns
shap_rf_interact_df.index = X_test[llarsic_cols].columns
shap_rf_interact_ranks = <span class="hljs-number">112</span> -rankdata(np.<span class="hljs-built_in">sum</span>(
     shap_rf_interact_avgs_nodiag, axis=<span class="hljs-number">0</span>)
)
most_interact_cols = shap_rf_interact_df.columns[
    shap_rf_interact_ranks &lt; <span class="hljs-number">13</span>
]
shap_rf_interact_df = shap_rf_interact_df.loc[
most_interact_cols,most_interact_cols
]
sns.heatmap(
    shap_rf_interact_df,
    cmap=<span class="hljs-string">'Blues'</span>,
    annot=<span class="hljs-literal">True</span>,
    annot_kws={<span class="hljs-string">'size'</span>:<span class="hljs-number">10</span>},
    fmt=<span class="hljs-string">'.3f'</span>,
    linewidths=<span class="hljs-number">.5</span>
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1149"/>preceding snippet outputs what is shown in <em class="italic">Figure 10.10</em>. It depicts the most salient feature interactions according to SHAP interaction absolute mean values. Note that these are averages, so given how right-skewed most of these features are, it is likely much higher for many observations. However, it’s still a good indication of relative impact:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_10.png" alt="Chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.10: SHAP interactions heatmap</p>
    <p class="normal">One way in<a id="_idIndexMarker1150"/> which we can understand feature interactions one by one is with SHAP’s <code class="inlineCode">dependence_plot</code>. For instance, we can take our top feature, <code class="inlineCode">MAXRAMNT</code>, and plot it with color-coded interactions with features such as <code class="inlineCode">RAMNTALL</code>, <code class="inlineCode">LSC4</code>, <code class="inlineCode">HVP2</code>, and <code class="inlineCode">AVGGIFT</code>. But first, we will need to compute <code class="inlineCode">shap_values</code>. There are a couple of problems though that need to be addressed, which we mentioned earlier. They have to do with the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">The prevalence of outliers</strong>: We can cut them out of the plot by limiting the <em class="italic">x</em>- and <em class="italic">y</em>-axes using percentiles for the feature and SHAP values, respectively, with <code class="inlineCode">plt.xlim</code> and <code class="inlineCode">plt.ylim</code>. This essentially zooms in on cases that lie between the 1st and 99th percentiles.</li>
      <li class="bulletList"><strong class="keyWord">Lopsided distribution of dollar amount features</strong>: It is common in any feature involving money for it to be right-skewed. There are many ways to simplify it, such as using percentiles to bin the feature, but a quick way to make it easier to appreciate is by using a logarithmic scale. In <code class="inlineCode">matplotlib</code>, you can do this with <code class="inlineCode">plt.xscale('log')</code> without any need to transform the feature.</li>
    </ul>
    <p class="normal">The <a id="_idIndexMarker1151"/>following code accounts for the two issues. You can try commenting out <code class="inlineCode">xlim</code>, <code class="inlineCode">ylim</code>, or <code class="inlineCode">xscale</code> to see the big difference they individually make in understanding <code class="inlineCode">dependence_plot</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">shap_rf_values = shap_rf_explainer.shap_values(
    X_test.iloc[sample_test_idx] [llarsic_cols]
)
maxramt_shap = shap_rf_values[:,llarsic_cols.index(<span class="hljs-string">"MAXRAMNT"</span>)]
shap.dependence_plot(
    <span class="hljs-string">"MAXRAMNT"</span>,
    shap_rf_values,
    X_test.iloc[sample_test_idx][llarsic_cols],
    interaction_index=<span class="hljs-string">"AVGGIFT"</span>,
    show=<span class="hljs-literal">False</span>, alpha=<span class="hljs-number">0.1</span>
)
plt.xlim(xmin=np.percentile(X_test.MAXRAMNT, <span class="hljs-number">1</span>),\
         xmax=np.percentile(X_test.MAXRAMNT, <span class="hljs-number">99</span>))
plt.ylim(ymin=np.percentile(maxramt_shap, <span class="hljs-number">1</span>),\
         ymax=np.percentile(maxramt_shap, <span class="hljs-number">99</span>))
plt.xscale(<span class="hljs-string">'log'</span>)
</code></pre>
    <p class="normal">The preceding code generates what is shown in <em class="italic">Figure 10.11</em>. It shows how there’s a tipping point somewhere between 10 and 100 for <code class="inlineCode">MAXRAMNT</code> where the mean impact on the model output starts to creep out, and these correlate with a higher <code class="inlineCode">AVGGIFT</code> value:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_11.png" alt="Chart  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 10.11: SHAP interaction plot between MAXRAMNT and AVGGIFT</p>
    <p class="normal">A <a id="_idIndexMarker1152"/>lesson you could take from <em class="italic">Figure 10.11</em> is that a cluster is formed by certain values of these features and possibly a few others that increase the likelihood of a donation. From a feature engineering standpoint, we could take unsupervised methods to create special cluster features solely based on the few features you have identified as related. Or we could take a more manual route, comparing different plots to understand how to best identify clusters. We could derive binary features from this process or even a ratio between features that more clearly depict interactions or cluster belonging.</p>
    <p class="normal">The idea here is not to reinvent the wheel trying to do what the model already does so well but to, first and foremost, aim for a more straightforward model interpretation. Hopefully, that will even have a positive impact on predictive performance by tidying up the features, because if you understand them better, maybe the model does too! It’s like smoothing a grainy image; it might confuse you less and the model too (see <em class="chapterRef">Chapter 13</em>, <em class="italic">Adversarial Robustness</em>, for more on that)! But understanding the data better through the model has other positive side effects.</p>
    <p class="normal">In fact, the lessons don’t stop with feature engineering or modeling but can be directly applied to promotions. What<a id="_idIndexMarker1153"/> if tipping points identified could be used to encourage donations? Perhaps get a free mug if you donate over $<em class="italic">X</em>? Or set up a recurring donation of $<em class="italic">X</em> and be on the exclusive list of “silver” patrons?</p>
    <p class="normal">We will end this topic on that curious note, but hopefully, this inspires you to appreciate how we can apply lessons from model interpretation to feature selection, engineering, and much more.</p>
    <h1 id="_idParaDest-293" class="heading-1">Mission accomplished</h1>
    <p class="normal">To approach this mission, you have reduced overfitting using primarily the toolset of feature selection. The non-profit is pleased with a profit lift of roughly 30%, costing a total of $35,601, which is $30,000 less than it would cost to send everyone in the test dataset the mailer. However, they still want assurance that they can safely employ this model without worries that they’ll experience losses.</p>
    <p class="normal">In this chapter, we’ve examined how overfitting can cause the profitability curves not to align. Misalignment is critical because it could mean that choosing a threshold based on training data would not be reliable on out-of-sample data. So, you use <code class="inlineCode">compare_df_plots</code> to compare profitability between the test and train sets as you’ve done before, but this time, for the chosen model (<code class="inlineCode">rf_5_e-llarsic</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">profits_test = reg_mdls[<span class="hljs-string">'rf_5_e-llarsic'</span>][<span class="hljs-string">'profits_test'</span>]
profits_train = reg_mdls[<span class="hljs-string">'rf_5_e-llarsic'</span>][<span class="hljs-string">'profits_train'</span>]
mldatasets.compare_df_plots(
    profits_test[[<span class="hljs-string">'costs'</span>, <span class="hljs-string">'</span><span class="hljs-string">profit'</span>, <span class="hljs-string">'roi'</span>]],
    profits_train[[<span class="hljs-string">'costs'</span>, <span class="hljs-string">'profit'</span>, <span class="hljs-string">'roi'</span>]],
    <span class="hljs-string">'Test'</span>,
    <span class="hljs-string">'Train'</span>,
    x_label=<span class="hljs-string">'Threshold'</span>,
    y_formatter=y_formatter,
    plot_args={<span class="hljs-string">'secondary_y'</span>:<span class="hljs-string">'roi'</span>}
)
</code></pre>
    <p class="normal">The preceding code generates what is shown in <em class="italic">Figure 10.12</em>. You can show this to the non-profit to prove that there’s a sweet spot at $0.68 that is the second highest profit attainable in <strong class="keyWord">Test</strong>. It is also within reach of their budget and achieves an ROI of 41%. More importantly, these numbers are not far from what they are for <strong class="keyWord">Train</strong>. Another thing that is great to see is that the profit curve slowly slides down for both <strong class="keyWord">Train</strong> and <strong class="keyWord">Test</strong> instead of dramatically falling off a cliff. The non-profit can be assured that the operation would still be profitable if they choose to increase the threshold. After all, they want to target donors from the entire mailing list, and for that to be financially feasible, they have to be more exclusive. Say they are using a threshold of $0.77 on the entire mailing list, the campaign would cost about $46,000 but return over $24,000 in profit:</p>
    <figure class="mediaobject"><img src="../Images/B18406_10_12.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.12: Comparison between profit, costs, and ROI for the test and train datasets for the model with LASSO LARS via AIC features across different thresholds</p>
    <p class="normal">Congratulations! You have accomplished this mission!</p>
    <p class="normal">But there’s one crucial detail that we’d be careless if we didn’t bring up.</p>
    <p class="normal">Although we trained this model with the next campaign in mind, the model will likely be used in future direct marketing campaigns without retraining. This model reusing presents a problem. There’s a concept <a id="_idIndexMarker1154"/>called <strong class="keyWord">data drift</strong>, also known as <strong class="keyWord">feature drift</strong>, which<a id="_idIndexMarker1155"/> is that, over time, what the model learned about the features concerning the target variable no longer holds true. Another concept, <strong class="keyWord">concept drift</strong>, is<a id="_idIndexMarker1156"/> about how the definition of the target feature changes over time. For instance, what constitutes a profitable donor can change. Both drifts can happen simultaneously, and with problems involving human behavior, this is to be expected. Behavior is shaped by cultures, habits, attitudes, technologies, and fashions, which are always evolving. You can caution the non-profit that you can only assure them that the model will be reliable for the next campaign, but they can’t afford to hire you for model retraining every single time!</p>
    <p class="normal">You can propose to the client creating a script that monitors drift directly on their mailing list database. If it finds significant changes in the features used by the model, it will alert both them and you. You could, at this point, trigger automatic retraining of the model. However, if the drift is due to data corruption, you won’t have an opportunity to address the problem. And even if automatic retraining is done, it can’t be deployed if performance metrics don’t meet predetermined standards. Either way, you should keep a close eye on predictive performance to be able to guarantee reliability. Reliability is an essential theme in model interpretability because it relates heavily to accountability. We won’t cover drift detection in this book, but future chapters discuss data augmentation (<em class="chapterRef">Chapter 11</em>, <em class="italic">Bias Mitigation and Causal Inference Methods</em>) and adversarial robustness (<em class="chapterRef">Chapter 13</em>, <em class="italic">Adversarial Robustness</em>), which pertain to reliability.</p>
    <h1 id="_idParaDest-294" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we have learned about how irrelevant features impact model outcomes and how feature selection provides a toolset to solve this problem. We then explored many different methods in this toolset, from the most basic filter methods to the most advanced ones. Lastly, we broached the subject of feature engineering for interpretability. Feature engineering can make for a more interpretable model that will perform better. We will cover this topic in more detail in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>.</p>
    <p class="normal">In the next chapter, we will discuss methods for bias mitigation and causal inference.</p>
    <h1 id="_idParaDest-295" class="heading-1">Dataset sources</h1>
    <ul>
      <li class="bulletList">Ling, C., and Li, C., 1998, <em class="italic">Data Mining for Direct Marketing: Problems and Solutions</em>. In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (KDD’98). AAAI Press, 73–79: <a href="https://dl.acm.org/doi/10.5555/3000292.3000304"><span class="url">https://dl.acm.org/doi/10.5555/3000292.3000304</span></a></li>
      <li class="bulletList">UCI Machine Learning Repository, 1998, KDD Cup 1998 Data Data Set: <a href="https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data"><span class="url">https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data</span></a></li>
    </ul>
    <h1 id="_idParaDest-296" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Ross, B.C., 2014<em class="italic">, Mutual Information between Discrete and Continuous Data Sets</em>. PLoS ONE, 9: <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357"><span class="url">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357</span></a></li>
      <li class="bulletList">Geurts, P., Ernst, D., and Wehenkel, L., 2006, <em class="italic">Extremely randomized trees</em>. Machine Learning, 63(1), 3-42: <a href="https://link.springer.com/article/10.1007/s10994-006-6226-1"><span class="url">https://link.springer.com/article/10.1007/s10994-006-6226-1</span></a></li>
      <li class="bulletList">Abid, A., Balin, M.F., and Zou, J., 2019, <em class="italic">Concrete Autoencoders for Differentiable Feature Selection and Reconstruction</em>. ICML: <a href="https://arxiv.org/abs/1901.09346"><span class="url">https://arxiv.org/abs/1901.09346</span></a></li>
      <li class="bulletList">Tan, F., Fu, X., Zhang, Y., and Bourgeois, A.G., 2008, <em class="italic">A genetic algorithm-based method for feature subset selection</em>. Soft Computing, 12, 111-120: <a href="https://link.springer.com/article/10.1007/s00500-007-0193-8"><span class="url">https://link.springer.com/article/10.1007/s00500-007-0193-8</span></a></li>
      <li class="bulletList">Calzolari, M., 2020, October 12, manuel-calzolari/sklearn-genetic: sklearn-genetic 0.3.0 (Version 0.3.0). Zenodo: <a href="http://doi.org/10.5281/zenodo.4081754"><span class="url">http://doi.org/10.5281/zenodo.4081754</span></a></li>
    </ul>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask the author questions, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="Chapter_10.xhtml"><span class="url">https://packt.link/inml</span></a></p>
    <p class="normal"><img src="../Images/QR_Code107161072033138125.png" alt="" role="presentation"/></p>
  </div>
</body></html>