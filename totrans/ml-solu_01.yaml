- en: Chapter 1. Credit Risk Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章：信用风险建模
- en: All the chapters in this book are practical applications. We will develop one
    application per chapter. We will understand about the application, and choose
    the proper dataset in order to develop the application. After analyzing the dataset,
    we will build the base-line approach for the particular application. Later on,
    we will develop a revised approach that resolves the shortcomings of the baseline
    approach. Finally, we will see how we can develop the best possible solution using
    the appropriate optimization strategy for the given application. During this development
    process, we will learn necessary key concepts about Machine Learning techniques.
    I would recommend my reader run the code which is given in this book. That will
    help you understand concepts really well.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的所有章节都是实际应用。我们将每章开发一个应用。我们将了解应用，并选择合适的数据库以开发应用。在分析数据库后，我们将为特定应用构建基线方法。随后，我们将开发一个改进的方法，以解决基线方法的不足。最后，我们将了解如何使用适当的优化策略为给定应用开发最佳解决方案。在这个过程中，我们将学习有关机器学习技术的必要关键概念。我建议我的读者运行本书中给出的代码。这将帮助你真正理解这些概念。
- en: In this chapter, we will look at one of the many interesting applications of
    predictive analysis. I have selected the finance domain to begin with, and we
    are going to build an algorithm that can predict loan defaults. This is one of
    the most widely used predictive analysis applications in the finance domain. Here,
    we will look at how to develop an optimal solution for predicting loan defaults.
    We will cover all of the elements that will help us build this application.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨预测分析众多有趣应用之一。我选择了金融领域作为起点，我们将构建一个能够预测贷款违约的算法。这是金融领域最广泛使用的预测分析应用之一。在这里，我们将探讨如何为预测贷款违约开发一个最优解。我们将涵盖所有有助于我们构建此应用的因素。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Introducing the problem statement
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: Understanding the dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集
- en: Understanding attributes of the dataset
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集的特征
- en: Data analysis
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析
- en: Features engineering for the baseline model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线模型的特征工程
- en: Selecting an ML algorithm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择机器学习算法
- en: Training the baseline model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练基线模型
- en: Understanding the testing matrix
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: Testing the baseline model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试基线模型
- en: Problems with the existing approach
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有方法的问题
- en: How to optimize the existing approach
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何优化现有方法
- en: Understanding key concepts to optimize the approach
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解关键概念以优化方法
- en: Hyperparameter tuning
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Implementing the revised approach
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施改进的方法
- en: Testing the revised approach
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试改进的方法
- en: Understanding the problem with the revised approach
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解改进方法中的问题
- en: The best approach
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳方法
- en: Implementing the best approach
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施最佳方法
- en: Summary
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述
- en: Introducing the problem statement
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: First of all, let's try to understand the application that we want to develop
    or the problem that we are trying to solve. Once we understand the problem statement
    and it's use case, it will be much easier for us to develop the application. So
    let's begin!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们尝试理解我们想要开发的应用或我们试图解决的问题。一旦我们理解了问题陈述及其用例，开发应用将变得容易得多。所以，让我们开始吧！
- en: Here, we want to help financial companies, such as banks, NBFS, lenders, and
    so on. We will make an algorithm that can predict to whom financial institutes
    should give loans or credit. Now you may ask *what is the significance of this
    algorithm?* Let me explain that in detail. When a financial institute lends money
    to a customer, they are taking some kind of risk. So, before lending, financial
    institutes check whether or not the borrower will have enough money in the future
    to pay back their loan. Based on the customer's current income and expenditure,
    many financial institutes perform some kind of analysis that helps them decide
    whether the borrower will be a good customer for that bank or not. This kind of
    analysis is manual and time-consuming. So, it needs some kind of automation. If
    we develop an algorithm, that will help financial institutes gauge their customers
    efficiently and effectively.Your next question may be *what is the output of our
    algorithm?* Our algorithm will generate probability. This probability value will
    indicate the chances of borrowers defaulting. Defaulting means borrowers cannot
    repay their loan in a certain amount of time. Here, probability indicates the
    chances of a customer not paying their loan EMI on time, resulting in default.
    So, a higher probability value indicates that the customer would be a bad or inappropriate
    borrower (customer) for the financial institution, as they may default in the
    next 2 years. A lower probability value indicates that the customer will be a
    good or appropriate borrower (customer) for the financial institution and will
    not default in the next 2 years.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望帮助金融机构，如银行、NBFS、贷款人等。我们将开发一个算法，可以预测金融机构应该向谁发放贷款或信用。现在你可能想知道*这个算法的意义是什么？*让我详细解释一下。当金融机构向客户贷款时，他们是在承担某种风险。所以，在贷款之前，金融机构会检查借款人未来是否有足够的钱来偿还贷款。基于客户的当前收入和支出，许多金融机构会进行某种分析，帮助他们决定借款人是否是那家银行的优质客户。这种分析是手动且耗时的。因此，需要某种自动化。如果我们开发一个算法，将帮助金融机构高效且有效地评估他们的客户。你的下一个问题可能是*我们的算法的输出是什么？*我们的算法将生成概率。这个概率值将表示借款人违约的可能性。违约意味着借款人在一定时间内无法偿还贷款。在这里，概率表示客户不能按时支付贷款EMI的可能性，从而导致违约。所以，较高的概率值表示客户可能是一个不良或不合适的借款人（客户），因为他们在接下来的两年内可能会违约。较低的概率值表示客户将是一个良好或合适的借款人（客户），在接下来的两年内不会违约。
- en: 'Here, I have given you information regarding the problem statement and its
    output, but there is an important aspect of this algorithm: its input. So, let''s
    discuss what our input will be!'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我已经提供了关于问题陈述及其输出的信息，但这个算法的一个重要方面是：它的输入。所以，让我们来讨论我们的输入将是什么！
- en: Understanding the dataset
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据集
- en: Here, we are going to discuss our input dataset in order to develop the application.
    You can find the dataset at [https://github.com/jalajthanaki/credit-risk-modelling/tree/master/data](https://github.com/jalajthanaki/credit-risk-modelling/tree/master/data).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论我们的输入数据集，以便开发应用程序。你可以在[https://github.com/jalajthanaki/credit-risk-modelling/tree/master/data](https://github.com/jalajthanaki/credit-risk-modelling/tree/master/data)找到数据集。
- en: 'Let''s discuss the dataset and its attributes in detail. Here, in the dataset,
    you can find the following files:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论数据集及其属性。在这里，在数据集中，你可以找到以下文件：
- en: '`cs-training.csv`'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cs-training.csv`'
- en: Records in this file are used for training, so this is our training dataset.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个文件中的记录用于训练，因此这是我们训练数据集。
- en: '`cs-test.csv`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cs-test.csv`'
- en: Records in this file are used for testing our machine learning models, so this
    is our testing dataset.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个文件中的记录用于测试我们的机器学习模型，因此这是我们测试数据集。
- en: '`Data Dictionary.xls`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Data Dictionary.xls`'
- en: This file contains information about each of the attributes of the dataset.
    So, this file is referred to as our data dictionary.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个文件包含了关于数据集中每个属性的信息。因此，这个文件被称为我们的数据字典。
- en: '`sampleEntry.csv`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampleEntry.csv`'
- en: This file gives us an idea about the format in which we need to generate our
    end output for our testing dataset. If you open this file, then you will see that
    we need to generate the probability of each of the records present in the testing
    dataset. This probability value indicates the chances of borrowers defaulting.
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个文件让我们了解我们需要为测试数据集生成最终输出的格式。如果你打开这个文件，你会看到我们需要生成测试数据集中每个记录的概率。这个概率值表示借款人违约的可能性。
- en: Understanding attributes of the dataset
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据集的属性
- en: 'The dataset has 11 attributes, which are shown as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有11个属性，如下所示：
- en: '![Understanding attributes of the dataset](img/B08394_01_01.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![理解数据集的属性](img/B08394_01_01.jpg)'
- en: 'Figure 1.1: Attributes (variables) of the dataset'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：数据集的属性（变量）
- en: 'We will look at each of the attributes one by one and understand their meaning
    in the context of the application:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个查看每个属性，并理解其在应用程序上下文中的含义：
- en: '**SeriousDlqin2yrs**:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**SeriousDlqin2yrs**:'
- en: In the dataset, this particular attribute indicates whether the borrower has
    experienced any past dues until 90 days in the previous 2 years.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据集中，此特定属性表示借款人是否在过去两年内有过90天以上的逾期还款记录。
- en: The value of this attribute is Yes if the borrower has experienced past dues
    of more than 90 days in the previous 2 years. If the EMI was not paid by the borrower
    90 days after the due date of the EMI, then this flag value is Yes.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果借款人在过去两年内有过超过90天的逾期还款记录，则此属性值为Yes。如果借款人在EMI到期日90天后仍未支付EMI，则此标志值是Yes。
- en: The value of this attribute is No if the borrower has not experienced past dues
    of more than 90 days in the previous 2 years. If the EMI was paid by the borrower
    before 90 days from the due date of the EMI, then this flag value is No.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果借款人在过去两年内没有超过90天的逾期还款记录，则此属性值为No。如果借款人在EMI到期日90天前支付了EMI，则此标志值是No。
- en: This attribute has target labels. In other words, we are going to predict this
    value using our algorithm for the test dataset.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此属性具有目标标签。换句话说，我们将使用我们的算法来预测测试数据集中的此值。
- en: '**RevolvingUtilizationOfUnsecuredLines**:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**RevolvingUtilizationOfUnsecuredLines**:'
- en: This attribute indicates the credit card limits of the borrower after excluding
    any current loan debt and real estate.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此属性表示在排除任何当前贷款债务和房地产后，借款人的信用卡额度。
- en: Suppose I have a credit card and its credit limit is $1,000\. In my personal
    bank account, I have $1,000\. My credit card balance is $500 out of $1,000.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我有一张信用卡，其信用额度为$1,000。在我的个人银行账户中，我有$1,000。我的信用卡余额是$500，总额度为$1,000。
- en: So, the total maximum balance I can have via my credit card and personal bank
    account is $1,000 + $1,000 = $2,000; I have used $500 from my credit card limit,
    so the total balance that I have is $500 (credit card balance) + $1,000 (personal
    bank account balance) = $1,500.
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我通过信用卡和个人银行账户可以拥有的总最大余额是$1,000 + $1,000 = $2,000；我已经使用了信用卡额度的$500，所以我拥有的总余额是$500（信用卡余额）+
    $1,000（个人银行账户余额）= $1,500。
- en: If account holder have taken home loan or other property loan and paying EMIs
    for those loan then we are not considering EMI value for property loan. Here,
    for this data attribute we have considered account holder's credit card balance
    and personal account balance.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果账户持有人已经申请了住房贷款或其他财产贷款并支付了这些贷款的EMI，则我们不考虑财产贷款的EMI价值。在这里，对于这个数据属性，我们考虑了账户持有人的信用卡余额和个人账户余额。
- en: So, the RevolvingUtilizationOfUnsecuredLines value is = $1,500 / $2,000 = 0.7500
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，循环未担保线使用率的价值是 = $1,500 / $2,000 = 0.7500
- en: '**Age**:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Age**:'
- en: This attribute is self-explanatory. It indicates the borrower's age.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此属性值一目了然。它表示借款人的年龄。
- en: '**NumberOfTime30-59DaysPastDueNotWorse**:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**NumberOfTime30-59DaysPastDueNotWorse**:'
- en: The number of this attribute indicates the number of times borrowers have paid
    their EMIs late but have paid them 30 days after the due date or 59 days before
    the due date.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此属性值的数量表示借款人支付EMI逾期但支付时间在到期日30天后或到期日前59天内的次数。
- en: '**DebtRatio**:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**DebtRatio**:'
- en: This is also a self-explanatory attribute, but we will try and understand it
    better with an example.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这也是一个一目了然的属性，但我们将通过一个例子来尝试更好地理解它。
- en: If my monthly debt is $200 and my other expenditure is $500, then I spend $700
    monthly. If my monthly income is $1,000, then the value of the DebtRatio is $700/$1,000
    = 0.7000
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我的月度债务是$200，我的其他支出是$500，那么我每月支出$700。如果我的月收入是$1,000，那么债务比率的价值是$700/$1,000 =
    0.7000
- en: '**MonthlyIncome**:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**MonthlyIncome**:'
- en: This attribute contains the value of the monthly income of borrowers.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此属性包含借款人每月收入的价值。
- en: '**NumberOfOpenCreditLinesAndLoans**:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**NumberOfOpenCreditLinesAndLoans**:'
- en: This attribute indicates the number of open loans and/or the number of credit
    cards the borrower holds.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此属性表示借款人持有的开放贷款和/或信用卡的数量。
- en: '**NumberOfTimes90DaysLate**:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**NumberOfTimes90DaysLate**:'
- en: This attribute indicates how many times a borrower has paid their dues 90 days
    after the due date of their EMIs.
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此属性表示借款人支付其EMI到期日90天后的欠款次数。
- en: '**NumberRealEstateLoansOrLines**:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**NumberRealEstateLoansOrLines**:'
- en: This attribute indicates the number of loans the borrower holds for their real
    estate or the number of home loans a borrower has.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此属性表示借款人持有的房地产贷款数量或借款人的住房贷款数量。
- en: '**NumberOfTime60-89DaysPastDueNotWorse**:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**NumberOfTime60-89DaysPastDueNotWorse**：'
- en: This attribute indicates how many times borrowers have paid their EMIs late
    but paid them 60 days after their due date or 89 days before their due date.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此属性表示借款人支付EMI（等额本息）的次数，但支付时间是在到期日后的60天或到期日前的89天。
- en: '**NumberOfDependents**:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**NumberOfDependents**：'
- en: This attribute is self-explanatory as well. It indicates the number of dependent
    family members the borrowers have. The dependent count is excluding the borrower.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此属性也是不言自明的。它表示借款人拥有的依赖家庭成员数量。依赖人数不包括借款人本人。
- en: These are basic attribute descriptions of the dataset, so you have a basic idea
    of the kind of dataset we have. Now it's time to get hands-on. So from the next
    section onward, we will start coding. We will begin exploring our dataset by performing
    basic data analysis so that we can find out the statistical properties of the
    dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是数据集的基本属性描述，因此您可以对我们所拥有的数据集类型有一个基本的概念。现在，是时候动手实践了。所以从下一节开始，我们将开始编写代码。我们将通过执行基本数据分析来探索我们的数据集，以便我们可以找出数据集的统计特性。
- en: Data analysis
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分析
- en: 'This section is divided into two major parts. You can refer to the following
    figure to see how we will approach this section:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本节分为两个主要部分。您可以参考以下图表，了解我们将如何处理本节：
- en: '![Data analysis](img/B08394_01_02.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![数据分析](img/B08394_01_02.jpg)'
- en: 'Figure 1.2: Parts and steps of data analysis'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：数据分析的各个部分和步骤
- en: In the first part, we have only one step. In the preceding figure, this is referred
    to as step 1.1\. In this first step, we will do basic data preprocessing. Once
    we are done with that, we will start with our next part.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分中，我们只有一个步骤。在前面的图中，这被称为步骤1.1。在这个第一步中，我们将进行基本数据预处理。一旦完成，我们将开始下一部分。
- en: 'The second part has two steps. In the figure*,* this is referred to as step
    2.1\. In this step, we will perform basic data analysis using statistical and
    visualization techniques, which will help us understand the data. By doing this
    activity, we will get to know some statistical facts about our dataset. After
    this, we will jump to the next step, which is referred to as step 2.2 in *Figure
    1.2*. In this step, we will once again perform data preprocessing, but, this time,
    our preprocessing will be heavily based on the findings that we have derived after
    doing basic data analysis on the given training dataset. You can find the code
    at this GitHub Link: [https://github.com/jalajthanaki/credit-risk-modelling/blob/master/basic_data_analysis.ipynb](https://github.com/jalajthanaki/credit-risk-modelling/blob/master/basic_data_analysis.ipynb).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分有两个步骤。在图中，这被称为步骤2.1。在这个步骤中，我们将使用统计和可视化技术进行基本数据分析，这将帮助我们理解数据。通过这个活动，我们将了解一些关于数据集的统计事实。之后，我们将跳到下一个步骤，这在图1.2中被称为步骤2.2。在这个步骤中，我们将再次进行数据预处理，但这次我们的预处理将高度基于我们在给定训练数据集上进行基本数据分析后得出的发现。您可以在以下GitHub链接中找到代码：[https://github.com/jalajthanaki/credit-risk-modelling/blob/master/basic_data_analysis.ipynb](https://github.com/jalajthanaki/credit-risk-modelling/blob/master/basic_data_analysis.ipynb)。
- en: So let's begin!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧！
- en: Data preprocessing
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: In this section, we will perform a minimal amount of basic preprocessing. We
    will look at the approaches as well as their implementation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将执行最小量的基本预处理。我们将探讨方法及其实现。
- en: First change
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一次更改
- en: If you open the `cs-training.csv` file, then you will find that there is a column
    without a heading, so we will add a heading there. Our heading for that attribute
    is `ID`. If you want to drop this column, you can because it just contains the
    `sr.no` of the records.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打开`cs-training.csv`文件，您会发现其中有一列没有标题，因此我们将在那里添加一个标题。该属性的标题为`ID`。如果您想删除此列，您可以这样做，因为它只包含记录的`sr.no`。
- en: Second change
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第二次更改
- en: This change is not a mandatory one. If you want to skip it, you can, but I personally
    like to perform this kind of preprocessing. The change is related to the heading
    of the attributes, we are removing "-" from the headers. Apart from this, I will
    convert all the column heading into lowercase. For example, the attribute named
    `NumberOfTime60-89DaysPastDueNotWorse` will be converted into `numberoftime6089dayspastduenotworse`.
    These kinds of changes will help us when we perform in-depth data analysis. We
    do not need to take care of this hyphen symbols while processing.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种更改不是强制性的。如果你想跳过它，你可以，但我个人喜欢执行这种预处理。这个更改与属性的标题有关，我们将从标题中移除"-"。除此之外，我还会将所有列标题转换为小写。例如，名为`NumberOfTime60-89DaysPastDueNotWorse`的属性将被转换为`numberoftime6089dayspastduenotworse`。这些类型的更改将有助于我们在进行深入数据分析时。在处理时我们不需要注意这些连字符符号。
- en: Implementing the changes
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实施更改
- en: Now, you may ask *how will I perform the changes described?* Well, there are
    two ways. One is a manual approach. In this approach, you will open the `cs-training.csv`
    file and perform the changes manually. This approach certainly isn't great. So,
    we will take the second approach. With the second approach, we will perform the
    changes using Python code. You can find all the changes in the following code
    snippets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道*我将如何执行所描述的更改？* 好吧，有两种方法。一种是手动方法。在这种方法中，你将打开`cs-training.csv`文件并手动执行更改。这种方法当然不是很好。所以，我们将采取第二种方法。在第二种方法中，我们将使用Python代码执行更改。你可以在以下代码片段中找到所有更改。
- en: 'Refer to the following screenshot for the code to perform the first change:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下截图以获取执行第一个更改的代码：
- en: '![Implementing the changes](img/B08394_01_03.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![实施更改](img/B08394_01_03.jpg)'
- en: 'Figure 1.3: Code snippet for implementing the renaming or dropping of the index
    column'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：实现重命名或删除索引列的代码片段
- en: For the second change, you can refer to *Figure 1.4:*
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个更改，你可以参考*图1.4*：
- en: '![Implementing the changes](img/B08394_01_04.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![实施更改](img/B08394_01_04.jpg)'
- en: 'Figure 1.4: Code snippet for removing "-" from the column heading and converting
    all the column headings into lowercase'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：删除列标题中的"-"并将所有列标题转换为小写的代码片段
- en: The same kind of preprocessing needs to be done on the `cs-test.csv` file. This
    is because the given changes are common for both the training and testing datasets.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的预处理需要在`cs-test.csv`文件上执行。这是因为给定的更改对训练集和测试集都是通用的。
- en: 'You can find the entire code on GitHub by clicking on this link: [https://github.com/jalajthanaki/credit-risk-modelling/blob/master/basic_data_analysis.ipynb](https://github.com/jalajthanaki/credit-risk-modelling/blob/master/basic_data_analysis.ipynb).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击此链接在GitHub上找到整个代码：[https://github.com/jalajthanaki/credit-risk-modelling/blob/master/basic_data_analysis.ipynb](https://github.com/jalajthanaki/credit-risk-modelling/blob/master/basic_data_analysis.ipynb)。
- en: You can also move hands-on along with reading.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在阅读的同时动手操作。
- en: I'm using Python 2.7 as well as a bunch of different Python libraries for the
    implementation of this code. You can find information related to Python dependencies
    as well as installation in the *README* section. Now let's move on to the basic
    data analysis section.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用Python 2.7以及一系列不同的Python库来实现此代码。你可以在*README*部分找到有关Python依赖项以及安装的信息。现在让我们继续到基本数据分析部分。
- en: Basic data analysis followed by data preprocessing
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本数据分析随后是数据预处理
- en: Let's perform some basic data analysis, which will help us find the statistical
    properties of the training dataset. This kind of analysis is also called exploratory
    data analysis (EDA), and it will help us understand how our dataset represents
    the facts. After deriving some facts, we can use them in order to derive feature
    engineering. So let's explore some important facts!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一些基本数据分析，这将帮助我们找到训练数据集的统计特性。这种分析也称为探索性数据分析（EDA），它将帮助我们了解我们的数据集如何表示事实。在推导出一些事实之后，我们可以使用它们来进行特征工程。所以，让我们探索一些重要的事实！
- en: 'From this section onward, all the code is part of one iPython notebook. You
    can refer to the code using this GitHub Link: [https://github.com/jalajthanaki/credit-risk-modelling/blob/master/Credit%20Risk%20Analysis.ipynb](https://github.com/jalajthanaki/credit-risk-modelling/blob/master/Credit%20Risk%20Analysis.ipynb).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从本节开始，所有代码都是iPython笔记本的一部分。你可以使用此GitHub链接引用代码：[https://github.com/jalajthanaki/credit-risk-modelling/blob/master/Credit%20Risk%20Analysis.ipynb](https://github.com/jalajthanaki/credit-risk-modelling/blob/master/Credit%20Risk%20Analysis.ipynb)。
- en: 'The following are the steps we are going to perform:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将要执行的步骤：
- en: Listing statistical properties
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出统计属性
- en: Finding the missing values
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找缺失值
- en: Replacing missing values
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 替换缺失值
- en: Correlation
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相关性
- en: Detecting Outliers
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检测异常值
- en: Listing statistical properties
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 列出统计属性
- en: 'In this section, we will get an idea about the statistical properties of the
    training dataset. Using pandas'' describe function, we can find out the following
    basic things:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解训练数据集的统计属性。使用pandas的describe函数，我们可以找出以下基本事项：
- en: '`count`: This will give us an idea about the number of records in our training
    dataset.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count`：这将让我们了解训练数据集中记录的数量。'
- en: '`mean`: This value gives us an indication of the mean of each of the data attributes.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean`：这个值给我们每个数据属性的均值指示。'
- en: '`std`: This value indicates the standard deviation for each of the data attributes.
    You can refer to this example: [http://www.mathsisfun.com/data/standard-deviation.html](http://www.mathsisfun.com/data/standard-deviation.html).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std`：这个值表示每个数据属性的标准差。您可以参考以下示例：[http://www.mathsisfun.com/data/standard-deviation.html](http://www.mathsisfun.com/data/standard-deviation.html)。'
- en: '`min`: This value gives us an idea of what the minimum value for each of the
    data attributes is.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min`：这个值让我们了解每个数据属性的最低值是多少。'
- en: '`25%`: This value indicates the 25th percentile. It should fall between 0 and
    1.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`25%`：这个值表示第25百分位数。它应该在0和1之间。'
- en: '`50%`: This value indicates the 50th percentile. It should fall between 0 and
    1.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`50%`：这个值表示第50百分位数。它应该在0和1之间。'
- en: '`75%`: This value indicates the 75th percentile. It should fall between 0 and
    1.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`75%`：这个值表示第75百分位数。它应该在0和1之间。'
- en: '`max`: This value gives us an idea of what the maximum value for each of the
    data attributes is.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max`：这个值让我们了解每个数据属性的最高值是多少。'
- en: 'Take a look at the code snippet in the following figure:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下图中的代码片段：
- en: '![Listing statistical properties](img/B08394_01_05.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![列出统计属性](img/B08394_01_05.jpg)'
- en: 'Figure 1.5: Basic statistical properties using the describe function of pandas'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：使用pandas的describe函数的基本统计属性
- en: 'We need to find some other statistical properties for our dataset that will
    help us understand it. So, here, we are going to find the median and mean for
    each of the data attributes. You can see the code for finding the median in the
    following figure:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为我们的数据集找到一些其他的统计属性，这将帮助我们理解它。因此，在这里，我们将找到每个数据属性的均值和中位数。您可以在下面的图中看到查找中位数的代码：
- en: '![Listing statistical properties](img/B08394_01_06.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![列出统计属性](img/B08394_01_06.jpg)'
- en: 'Figure 1.6: Code snippet for generating the median and the mean for each data
    attribute'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6：生成每个数据属性的均值和中位数的代码片段
- en: 'Now let''s check out what kind of data distribution is present in our dataset.
    We draw the frequency distribution for our target attribute, `seriousdlqin2yrs`,
    in order to understand the overall distribution of the target variable for the
    training dataset. Here, we will use the `seaborn` visualization library. You can
    refer to the following code snippet:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查我们的数据集中存在什么样的数据分布。我们绘制了目标属性`seriousdlqin2yrs`的频率分布，以便理解训练数据集中目标变量的整体分布。在这里，我们将使用`seaborn`可视化库。您可以通过以下代码片段进行参考：
- en: '![Listing statistical properties](img/B08394_01_07.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![列出统计属性](img/B08394_01_07.jpg)'
- en: 'Figure 1.7: Code snippet for understanding the target variable distribution
    as well as the code snippet for the visualization of the distribution'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7：理解目标变量分布以及分布可视化的代码片段
- en: 'You can refer to the visualization chart in the following figure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下图中的可视化图表：
- en: '![Listing statistical properties](img/B08394_01_69.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![列出统计属性](img/B08394_01_69.jpg)'
- en: 'Figure 1.8: Visualization of the variable distribution of the target data attribute'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8：目标数据属性变量分布的可视化
- en: From this chart, you can see that there are many records with the target label
    *0* and fewer records with the target label *1*. You can see that the data records
    with a *0* label are about 93.32%, whereas 6.68% of the data records are labeled
    *1*. We will use all of these facts in the upcoming sections. For now, we can
    consider our outcome variable as imbalanced.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图表中，您可以看到有许多带有目标标签*0*的记录，而带有目标标签*1*的记录较少。您可以看到带有*0*标签的数据记录大约占93.32%，而6.68%的数据记录被标记为*1*。我们将在接下来的章节中使用所有这些事实。现在，我们可以将我们的结果变量视为不平衡的。
- en: Finding missing values
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查找缺失值
- en: In order to find the missing values in the dataset, we need to check each and
    every data attribute. First, we will try to identify which attribute has a missing
    or null value. Once we have found out the name of the data attribute, we will
    replace the missing value with a more meaningful value. There are a couple of
    options available for replacing the missing values. We will explore all of these
    possibilities.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在数据集中找到缺失值，我们需要检查每个数据属性。首先，我们将尝试识别哪个属性有缺失或空值。一旦我们找到了数据属性的名字，我们将用更有意义的价值替换缺失值。有几种选项可用于替换缺失值。我们将探索所有这些可能性。
- en: 'Let''s code for our first step. Here, we will see which data attribute has
    missing values as well count how many records there are for each data attribute
    with a missing value. You can see the code snippet in the following figure:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写第一步的代码。在这里，我们将查看哪些数据属性存在缺失值，以及每个具有缺失值的数据属性有多少条记录。您可以在下面的图中查看代码片段：
- en: '![Finding missing values](img/B08394_01_08.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![寻找缺失值](img/B08394_01_08.jpg)'
- en: 'Figure 1.9: Code snippet for identifying which data attributes have missing
    values'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9：识别哪些数据属性有缺失值的代码片段
- en: 'As displayed in the preceding figure, the following two data attributes have
    missing values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，以下两个数据属性存在缺失值：
- en: '`monthlyincome`: This attribute contains 29,731 records with a missing value.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monthlyincome`：此属性包含29,731条记录，其中存在缺失值。'
- en: '`numberofdependents`: This attribute contains 3,924 records with a missing
    value.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numberofdependents`：此属性包含3,924条记录，其中存在缺失值。'
- en: 'You can also refer to the code snippet in the following figure for the graphical
    representation of the facts described so far:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以参考下面的图中的代码片段，以图形方式表示到目前为止描述的事实：
- en: '![Finding missing values](img/B08394_01_09.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![寻找缺失值](img/B08394_01_09.jpg)'
- en: 'Figure 1.10: Code snippet for generating a graph of missing values'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10：生成缺失值图表的代码片段
- en: 'You can view the graph itself in the following figure:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在下面的图中查看图表本身：
- en: '![Finding missing values](img/B08394_01_10.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![寻找缺失值](img/B08394_01_10.jpg)'
- en: 'Figure 1.11: A graphical representation of the missing values'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11：缺失值的图形表示
- en: 'In this case, we need to replace these missing values with more meaningful
    values. There are various standard techniques that we can use for that. We have
    the following two options:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要用更有意义的价值替换这些缺失值。我们可以使用各种标准技术来完成这项工作。我们有以下两种选项：
- en: Replace the missing value with the mean value of that particular data attribute
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用该特定数据属性的平均值替换缺失值
- en: Replace the missing value with the median value of that particular data attribute
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用该特定数据属性的中位数替换缺失值
- en: In the previous section, we already derived the mean and median values for all
    of our data attributes, and we will use them. Here, our focus will be on the attributes
    titled `monthlyincome` and `numberofdependents` because they have missing values.
    We have found out which data attributes have missing values, so now it's time
    to perform the actual replacement operation. In the next section, you will see
    how we can replace the missing values with the mean or the median.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们已经为所有数据属性推导出了平均值和中位数，我们将使用它们。在这里，我们的重点是标题为`monthlyincome`和`numberofdependents`的属性，因为它们有缺失值。我们已经确定了哪些数据属性有缺失值，现在是时候执行实际的替换操作了。在下一节中，您将看到我们如何用平均值或中位数替换缺失值。
- en: Replacing missing values
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 替换缺失值
- en: In the previous section, we figured out which data attributes in our training
    dataset contain missing values. We need to replace the missing values with either
    the mean or the median value of that particular data attribute. So in this section,
    we will focus particularly on how we can perform the actual replacement operation.
    This operation of replacing the missing value is also called imputing the missing
    data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们确定了训练数据集中哪些数据属性包含缺失值。我们需要用该特定数据属性的平均值或中位数来替换缺失值。因此，在本节中，我们将特别关注我们如何执行实际的替换操作。这种替换缺失值的操作也称为缺失数据插补。
- en: 'Before moving on to the code section, I feel you guys might have questions
    such as these: *should I replace missing values with the mean or the median?*
    *Are there any other options available?* Let me answer these questions one by
    one.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入代码部分之前，我觉得你们可能会有一些问题，比如：*我应该用平均值还是中位数替换缺失值？* *还有其他选项吗？* 让我逐一回答这些问题。
- en: The answer to the first question, practically, will be a trial and error method.
    So you first replace missing values with the mean value, and during the training
    of the model, measure whether you get a good result on the training dataset or
    not. Then, in the second iteration, we need to try to replace the values with
    the median and measure whether you get a good result on the training dataset or
    not.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题的答案实际上将是一个试错法。所以你首先用平均值替换缺失值，并在模型训练过程中测量是否在训练数据集上得到好的结果。然后，在第二次迭代中，我们需要尝试用中位数替换值，并测量是否在训练数据集上得到好的结果。
- en: 'In order to answer the second question, there are many different imputation
    techniques available, such as the deletion of records, replacing the values using
    the KNN method, replacing the values using the most frequent value, and so on.
    You can select any of these techniques, but you need to train the model and measure
    the result. Without implementing a technique, you can''t really say with certainty
    that a particular imputation technique will work for the given training dataset.
    Here, we are talking in terms of the credit-risk domain, so I would not get into
    the theory much, but just to refresh your concepts, you can refer to the following
    articles:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答第二个问题，有许多不同的插补技术可用，例如删除记录、使用KNN方法替换值、使用最频繁的值替换值等等。你可以选择这些技术中的任何一种，但你需要训练模型并测量结果。没有实施技术，你无法真正确定特定的插补技术是否适用于给定的训练数据集。在这里，我们谈论的是信用风险领域，所以我不太会深入理论，只是为了刷新你的概念，你可以参考以下文章：
- en: '[https://machinelearningmastery.com/handle-missing-data-python/](https://machinelearningmastery.com/handle-missing-data-python/)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/handle-missing-data-python/](https://machinelearningmastery.com/handle-missing-data-python/)'
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html)'
- en: '[https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/)'
- en: 'We can see the code for replacing the missing values using the attribute''s
    mean value and its median value in the following figure:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图中看到使用属性的平均值和中位数替换缺失值的代码：
- en: '![Replacing missing values](img/B08394_01_11.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![替换缺失值](img/B08394_01_11.jpg)'
- en: 'Figure 1.12: Code snippet for replacing the mean values'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12：替换平均值代码片段
- en: In the preceding code snippet, we replaced the missing value with the mean value,
    and in the second step, we verified that all the missing values have been replaced
    with the mean of that particular data attribute.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们用平均值替换了缺失值，在第二步中，我们验证了所有缺失值都已用该特定数据属性的平均值替换。
- en: 'In the next code snippet, you can see the code that we have used for replacing
    the missing values with the median of those data attributes. Refer to the following
    figure:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个代码片段中，你可以看到我们用来用这些数据属性的中位数替换缺失值的代码。请参考以下图：
- en: '![Replacing missing values](img/B08394_01_12.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![替换缺失值](img/B08394_01_12.jpg)'
- en: 'Figure 1.13: Code snippet for replacing missing values with the median'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13：用中位数替换缺失值的代码片段
- en: In the preceding code snippet, we have replaced the missing value with the median
    value, and in second step, we have verified that all the missing values have been
    replaced with the median of that particular data attribute.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们已经用中位数替换了缺失值，在第二步中，我们已经验证了所有缺失值都已用该特定数据属性的中位数替换。
- en: In the first iteration, I would like to replace the missing value with the median.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代中，我希望用中位数替换缺失值。
- en: 'In the next section, we will see one of the important aspects of basic data
    analysis: finding correlations between data attributes. So, let''s get started
    with correlation.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到基本数据分析的一个重要方面：寻找数据属性之间的相关性。所以，让我们从相关性开始吧。
- en: Correlation
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 相关系数
- en: I hope you basically know what correlation indicates in machine learning. The
    term correlation refers to a mutual relationship or association between quantities.
    If you want to refresh the concept on this front, you can refer to [https://www.investopedia.com/terms/c/correlation.asp](https://www.investopedia.com/terms/c/correlation.asp).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望您基本上知道在机器学习中相关性表示什么。术语相关性指的是数量之间的相互关系或关联。如果您想在这方面刷新概念，可以参考 [https://www.investopedia.com/terms/c/correlation.asp](https://www.investopedia.com/terms/c/correlation.asp)。
- en: 'So, here, we will find out what kind of association is present among the different
    data attributes. Some attributes are highly dependent on one or many other attributes.
    Sometimes, values of a particular attribute increase with respect to its dependent
    attribute, whereas sometimes values of a particular attribute decrease with respect
    to its dependent attribute. So, correlation indicates the positive as well as
    negative associations among data attributes. You can refer to the following code
    snippet for the correlation:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这里，我们将找出不同数据属性之间存在的关联类型。有些属性高度依赖于一个或多个其他属性。有时，特定属性的值相对于其依赖属性增加，而有时特定属性的值相对于其依赖属性减少。因此，相关性表示数据属性之间的正负关联。您可以参考以下代码片段来了解相关性：
- en: '![Correlation](img/B08394_01_13.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![相关性](img/B08394_01_13.jpg)'
- en: 'Figure 1.14: Code snippet for generating correlation'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14：生成相关性的代码片段
- en: 'You can see the code snippet of the graphical representation of the correlation
    in the following figure:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下图中看到表示相关性的图形表示的代码片段：
- en: '![Correlation](img/B08394_01_14.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![相关性](img/B08394_01_14.jpg)'
- en: 'Figure 1.15: Code snippet for generating a graphical snippet'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15：生成图形片段的代码片段
- en: 'You can see the graph of the correlation in the following figure:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下图中看到相关性的图表：
- en: '![Correlation](img/B08394_01_15.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![相关性](img/B08394_01_15.jpg)'
- en: 'Figure 1.16: Heat map for correlation'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16：相关性热图
- en: 'Let''s look at the preceding graph because it will help you understand correlation
    in a great way. The following facts can be derived from the graph:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看前面的图表，因为它将极大地帮助您理解相关性。以下事实可以从图表中得出：
- en: Cells with 1.0 values are highly associated with each other.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值为 1.0 的单元格彼此高度相关。
- en: Each attribute has a very high correlation with itself, so all the diagonal
    values are 1.0.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个属性与其自身都有非常高的相关性，因此所有对角线值都是 1.0。
- en: The data attribute **numberoftime3059dayspastduenotworse** (refer to the data
    attribute given on the vertical line or on the *y* *axis*) is highly associated
    with two attributes, **numberoftimes90dayslate** and **numberoftime6089dayspastduenotworse**.
    These two data attributes are given on the *x* *axis* (or on the horizontal line).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据属性 **numberoftime3059dayspastduenotworse**（参考垂直线或 *y* 轴上的数据属性）与两个属性高度相关，**numberoftimes90dayslate**
    和 **numberoftime6089dayspastduenotworse**。这两个数据属性位于 *x* 轴（或水平线上）。
- en: The data attribute numberoftimes90dayslate is highly associated with numberoftime3059dayspastduenotworse
    and numberoftime6089dayspastduenotworse. These two data attributes are given on
    the *x* axis (or on the horizontal line).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据属性 numberoftimes90dayslate 与 numberoftime3059dayspastduenotworse 和 numberoftime6089dayspastduenotworse
    高度相关。这两个数据属性位于 *x* 轴（或水平线上）。
- en: The data attribute numberoftime6089dayspastduenotworse is highly associated
    with numberoftime3059dayspastduenotworse and numberoftimes90dayslate. These two
    data attributes are given on the *x* axis (or on the horizontal line).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据属性 numberoftime6089dayspastduenotworse 与 numberoftime3059dayspastduenotworse
    和 numberoftimes90dayslate 高度相关。这两个数据属性位于 *x* 轴（或水平线上）。
- en: The data attribute **numberofopencreditlinesandloans** also has an association
    with **numberrealestateloansorlines** and vice versa. Here, the data attribute
    numberrealestateloansorlines is present on the *x* axis (or on the horizontal
    line).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据属性 **numberofopencreditlinesandloans** 也与 **numberrealestateloansorlines**
    相关，反之亦然。在这里，数据属性 numberrealestateloansorlines 位于 *x* 轴（或水平线上）。
- en: Before moving ahead, we need to check whether these attributes contain any outliers
    or insignificant values. If they do, we need to handle these outliers, so our
    next section is about detecting outliers from our training dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续前进之前，我们需要检查这些属性是否包含任何异常值或不显著值。如果有的话，我们需要处理这些异常值，因此我们下一节将介绍如何从我们的训练数据集中检测异常值。
- en: Detecting outliers
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检测异常值
- en: 'In this section, you will learn how to detect outliers as well as how to handle
    them. There are two steps involved in this section:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何检测异常值以及如何处理它们。本节涉及两个步骤：
- en: Outliers detection techniques
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值检测技术
- en: Handling outliers
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理异常值
- en: First, let's begin with detecting outliers. Now you guys might have wonder *why
    should we detect outliers*. In order to answer this question, I would like to
    give you an example. Suppose you have the weights of 5-year-old children. You
    measure the weight of five children and you want to find out the average weight.
    The children weigh 15, 12, 13, 10, and 35 kg. Now if you try to find out the average
    of these values, you will see that the answer 17 kg. If you look at the weight
    range carefully, then you will realize that the last observation is out of the
    normal range compared to the other observations. Now let's remove the last observation
    (which has a value of 35) and recalculate the average of the other observations.
    The new average is 12.5 kg. This new value is much more meaningful in comparison
    to the last average value. So, the outlier values impact the accuracy greatly;
    hence, it is important to detect them. Once that is done, we will explore techniques
    to handle them in upcoming section named handling outlier.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从检测异常值开始。现在你们可能想知道*为什么我们应该检测异常值*。为了回答这个问题，我想给你们举一个例子。假设你有5岁孩子的体重。你测量了五个孩子的体重，并想找出平均体重。孩子们的体重分别是15、12、13、10和35公斤。现在如果你尝试找出这些值的平均值，你会看到答案是17公斤。如果你仔细观察体重范围，你会发现最后一个观测值与其他观测值相比超出了正常范围。现在让我们移除最后一个观测值（其值为35）并重新计算其他观测值的平均值。新的平均值是12.5公斤。这个新值与最后一个平均值相比更有意义。因此，异常值对准确性有很大影响；因此，检测它们非常重要。一旦完成，我们将在名为处理异常值的下一节中探讨处理它们的技术。
- en: Outliers detection techniques
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异常值检测技术
- en: 'Here, we are using the following outlier detection techniques:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了以下异常值检测技术：
- en: Percentile-based outlier detection
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于百分比的异常值检测
- en: Median Absolute Deviation (MAD)-based outlier detection
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于中位数绝对偏差（MAD）的异常值检测
- en: Standard Deviation (STD)-based outlier detection
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于标准差（STD）的异常值检测
- en: Majority-vote-based outlier detection
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于多数投票的异常值检测
- en: Visualization of outliers
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值的可视化
- en: Percentile-based outlier detection
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于百分比的异常值检测
- en: 'Here, we have used percentile-based outlier detection, which is derived based
    on the basic statistical understanding. We assume that we should consider all
    the data points that lie under the percentile range from 2.5 to 97.5\. We have
    derived the percentile range by deciding on a threshold of 95\. You can refer
    to the following code snippet:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了基于百分比的异常值检测，这是基于基本的统计理解推导出来的。我们假设我们应该考虑所有位于2.5到97.5百分比范围内的数据点。我们通过决定一个95%的阈值来推导出百分比范围。你可以参考以下代码片段：
- en: '![Percentile-based outlier detection](img/B08394_01_16.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![基于百分比的异常值检测](img/B08394_01_16.jpg)'
- en: 'Figure 1.17: Code snippet for percentile-based outlier detection'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.17：基于百分比的异常值检测代码片段
- en: We will use this method for each of the data attributes and detect the outliers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这种方法对每个数据属性进行检测异常值。
- en: Median Absolute Deviation (MAD)-based outlier detection
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于中位数绝对偏差（MAD）的异常值检测
- en: 'MAD is a really simple statistical concept. There are four steps involved in
    it. This is also known as modified Z-score. The steps are as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 中位数绝对偏差（MAD）是一个非常简单的统计概念。它涉及四个步骤。这也被称为修改后的Z分数。步骤如下：
- en: Find the median of the particular data attribute.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到特定数据属性的中位数。
- en: For each of the given values for the data attribute, subtract the previously
    found median value. This subtraction is in the form of the absolute value. So,
    for each data point, you will get the absolute value.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于数据属性给出的每个值，减去之前找到的中位数值。这个减法是以绝对值的形式进行的。因此，对于每个数据点，你将得到一个绝对值。
- en: In the third step, generate the median of the absolute values that we derived
    in the second step. We will perform this operation for each data point for each
    of the data attributes. This value is called the MAD value.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第三步中，生成我们在第二步中得到的绝对值的平均值。我们将对每个数据点以及每个数据属性执行此操作。这个值被称为MAD值。
- en: In the fourth step, we will use the following equation to derive the modified
    Z-score:![Median Absolute Deviation (MAD)-based outlier detection](img/B08394_01_17.jpg)![Median
    Absolute Deviation (MAD)-based outlier detection](img/B08394_01_18.jpg)![Median
    Absolute Deviation (MAD)-based outlier detection](img/B08394_01_19.jpg)
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第四步中，我们将使用以下公式来推导修改后的Z分数：![基于中值绝对偏差（MAD）的异常值检测](img/B08394_01_17.jpg)![基于中值绝对偏差（MAD）的异常值检测](img/B08394_01_18.jpg)![基于中值绝对偏差（MAD）的异常值检测](img/B08394_01_19.jpg)
- en: 'Now it''s time to refer to the following code snippet:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候参考以下代码片段了：
- en: '![Median Absolute Deviation (MAD)-based outlier detection](img/B08394_01_20.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![基于中值绝对偏差（MAD）的异常值检测](img/B08394_01_20.jpg)'
- en: 'Figure 1.18: Code snippet for MAD-based outlier detection'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.18：基于MAD的异常值检测代码片段
- en: Standard Deviation (STD)-based outlier detection
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于标准差（STD）的异常值检测
- en: 'In this section, we will use standard deviation and the mean value to find
    the outlier. Here, we select a random threshold value of 3\. You can refer to
    the following code snippet:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用标准差和平均值来查找异常值。在这里，我们选择了一个随机的阈值值为3。你可以参考以下代码片段：
- en: '![Standard Deviation (STD)-based outlier detection](img/B08394_01_21.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![基于标准差（STD）的异常值检测](img/B08394_01_21.jpg)'
- en: 'Figure 1.19: Standard Deviation (STD) based outlier detection code'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.19：基于标准差（STD）的异常值检测代码
- en: 'Majority-vote-based outlier detection:'
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于多数投票的异常值检测：
- en: 'In this section, we will build the voting mechanism so that we can simultaneously
    run all the previously defined methods—such as percentile-based outlier detection,
    MAD-based outlier detection, and STD-based outlier detection—and get to know whether
    the data point should be considered an outlier or not. We have seen three techniques
    so far. So, if two techniques indicate that the data should be considered an outlier,
    then we consider that data point as an outlier; otherwise, we don''t. So, the
    minimum number of votes we need here is two. Refer to the following figure for
    the code snippet:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建投票机制，以便我们可以同时运行之前定义的所有方法——例如基于百分比的异常值检测、基于MAD的异常值检测和基于STD的异常值检测——并了解数据点是否应该被视为异常值。到目前为止，我们已经看到了三种技术。因此，如果两种技术指示数据应该被视为异常值，那么我们就将这个数据点视为异常值；否则，我们不将其视为异常值。因此，这里所需的最低投票数是两个。请参考以下图中的代码片段：
- en: '![Majority-vote-based outlier detection:](img/B08394_01_22.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![基于多数投票的异常值检测：](img/B08394_01_22.jpg)'
- en: 'Figure 1.20: Code snippet for the voting mechanism for outlier detection'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.20：异常值检测的投票机制代码片段
- en: Visualization of outliers
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异常值可视化
- en: 'In this section, we will plot the data attributes to get to know about the
    outliers visually. Again, we are using the `seaborn` and `matplotlib` library
    to visualize the outliers. You can find the code snippet in the following figure:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将绘制数据属性以直观地了解异常值。同样，我们使用`seaborn`和`matplotlib`库来可视化异常值。你可以在以下图中找到代码片段：
- en: '![Visualization of outliers](img/B08394_01_23.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![异常值可视化](img/B08394_01_23.jpg)'
- en: 'Figure 1.21: Code snippet for the visualization of the outliers'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.21：异常值可视化代码片段
- en: Refer to the preceding figure for the graph and learn how our defined methods
    detect the outlier. Here, we chose a sample size of 5,000\. This sample was selected
    randomly.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考前面的图来了解我们的定义方法如何检测异常值。在这里，我们选择了5,000个样本的大小。这个样本是随机选择的。
- en: '![Visualization of outliers](img/B08394_01_24.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![异常值可视化](img/B08394_01_24.jpg)'
- en: 'Figure 1.22: Graph for outlier detection'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.22：异常值检测的图表
- en: Here, you can see how all the defined techniques will help us detect outlier
    data points from a particular data attribute. You can see all the attribute visualization
    graphs on this GitHub link at [https://github.com/jalajthanaki/credit-risk-modelling/blob/master/Credit%20Risk%20Analysis.ipynb](https://github.com/jalajthanaki/credit-risk-modelling/blob/master/Credit%20Risk%20Analysis.ipynb).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到所有定义的技术如何帮助我们从一个特定的数据属性中检测异常数据点。你可以在以下GitHub链接上看到所有属性可视化图表：[https://github.com/jalajthanaki/credit-risk-modelling/blob/master/Credit%20Risk%20Analysis.ipynb](https://github.com/jalajthanaki/credit-risk-modelling/blob/master/Credit%20Risk%20Analysis.ipynb)。
- en: So far, you have learned how to detect outliers, but now it's time to handle
    these outlier points. In the next section, we will look at how we can handle outliers.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了如何检测异常值，但现在你需要处理这些异常值点。在下一节中，我们将探讨如何处理异常值。
- en: Handling outliers
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理异常值
- en: In this section, you will learn how to remove or replace outlier data points.
    This particular step is important because if you just identify the outlier but
    aren't able to handle it properly, then at the time of training, there will be
    a high chance that we over-fit the model. So, let's learn how to handle the outliers
    for this dataset. Here, I will explain the operation by looking at the data attributes
    one by one.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何删除或替换异常数据点。这一步骤尤为重要，因为如果您只是识别了异常值，但无法正确处理，那么在训练时，我们有很大的可能性会过度拟合模型。因此，让我们学习如何处理这个数据集的异常值。在这里，我将通过逐个查看数据属性来解释这个操作。
- en: Revolving utilization of unsecured lines
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 未担保线路的循环利用率
- en: In this data attribute, when you plot an outlier detection graph, you will come
    to know that values of more than 0.99999 are considered outliers. So, values greater
    than 0.99999 can be replaced with 0.99999\. So for this data attribute, we perform
    the replacement operation. We have generated new values for the data attribute
    `revolvingutilizationofunsecuredlines`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据属性中，当您绘制异常值检测图时，您将了解到大于0.99999的值被认为是异常值。因此，大于0.99999的值可以被替换为0.99999。因此，对于这个数据属性，我们执行替换操作。我们为数据属性`revolvingutilizationofunsecuredlines`生成了新的值。
- en: 'For the code, you can refer to the following figure:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代码，您可以参考以下图表：
- en: '![Revolving utilization of unsecured lines](img/B08394_01_25.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![未担保线路的循环利用率](img/B08394_01_25.jpg)'
- en: 'Figure 1.23: Code snippet for replacing outlier values with 0.99999'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.23：用0.99999替换异常值的代码片段
- en: Age
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 年龄
- en: In this attribute, if you explore the data and see the percentile-based outlier,
    then you see that there is an outlier with a value of 0 and the youngest age present
    in the data attribute is 21\. So, we replace the value of 0 with 22\. We code
    the condition such that the age should be more than 22\. If it is not, then we
    will replace the age with 22\. You can refer to the following code and graph.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个属性中，如果您探索数据并查看基于百分比的异常值，那么您会看到有一个值为0的异常值，数据属性中最年轻的年龄是21岁。因此，我们将0的值替换为22。我们编写了这样的条件，即年龄应该大于22。如果不是，那么我们将年龄替换为22。您可以参考以下代码和图表。
- en: 'The following figure shows how the frequency distribution of age is given in
    the dataset. By looking at the data, we can derive the fact that 0 is the outlier
    value:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了数据集中年龄的频率分布。通过观察数据，我们可以得出结论，0是异常值：
- en: '![Age](img/B08394_01_26.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![年龄](img/B08394_01_26.jpg)'
- en: 'Figure 1.24: Frequency for each data value shows that 0 is an outlier'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.24：每个数据值的频率显示0是异常值
- en: 'Refer to the following box graph, which gives us the distribution indication
    of the age:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下箱形图，它给出了年龄的分布指示：
- en: '![Age](img/B08394_01_27.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![年龄](img/B08394_01_27.jpg)'
- en: 'Figure 1.25: Box graph for the age data attribute'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.25：年龄数据属性的箱形图
- en: 'Before removing the outlier, we got the following outlier detection graph:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在删除异常值之前，我们得到了以下异常值检测图：
- en: '![Age](img/B08394_01_28.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![年龄](img/B08394_01_28.jpg)'
- en: 'Figure 1.26: Graphical representation of detecting outliers for data attribute
    age'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.26：数据属性年龄检测异常的图形表示
- en: 'The code for replacing the outlier is as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 替换异常值的代码如下：
- en: '![Age](img/B08394_01_29.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![年龄](img/B08394_01_29.jpg)'
- en: 'Figure 1.27: Replace the outlier with the minimum age value 21'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.27：用最小年龄值21替换异常值
- en: In the code, you can see that we have checked each data point of the age column,
    and if the age is greater than 21, then we haven't applied any changes, but if
    the age is less than 21, then we have replaced the old value with 21\. After that,
    we put all these revised values into our original dataframe.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，您可以看到我们已经检查了年龄列的每个数据点，如果年龄大于21，则我们没有应用任何更改，但如果年龄小于21，则我们用21替换了旧值。之后，我们将所有这些修改后的值放入我们的原始数据框中。
- en: Number of time 30-59 days past due not worse
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 30-59天逾期次数未恶化
- en: In this data attribute, we explore the data as well as referring to the outlier
    detection graph. Having done that, we know that values 96 and 98 are our outliers.
    We replace these values with the media value. You can refer to the following code
    and graph to understand this better.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据属性中，我们不仅探索了数据，还参考了异常值检测图。在这样做之后，我们知道96和98是我们的异常值。我们将这些值替换为平均值。您可以参考以下代码和图表来更好地理解这一点。
- en: 'Refer to the outlier detection graph given in the following figure:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图表中的异常值检测图：
- en: '![Number of time 30-59 days past due not worse](img/B08394_01_30.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![30-59天逾期次数未恶化](img/B08394_01_30.jpg)'
- en: 'Figure 1.28: Outlier detection graph'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.28：异常值检测图
- en: 'Refer to the frequency analysis of the data in the following figure:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 参考下图中数据的频率分析：
- en: '![Number of time 30-59 days past due not worse](img/B08394_01_31.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![30-59天逾期次数未恶化](img/B08394_01_31.jpg)'
- en: 'Figure 1.29: Outlier values from the frequency calculation'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.29：频率计算中的异常值
- en: 'The code snippet for replacing the outlier values with the median is given
    in the following figure:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 用中位数替换异常值代码片段如下所示：
- en: '![Number of time 30-59 days past due not worse](img/B08394_01_32.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![30-59天逾期次数未恶化](img/B08394_01_32.jpg)'
- en: 'Figure 1.30: Code snippet for replacing outliers'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.30：替换异常值的代码片段
- en: Debt ratio
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 负债比率
- en: 'If we look at the graph of the outlier detection of this attribute, then it''s
    kind of confusing. Refer to the following figure:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看这个属性的异常值检测图，那么它有点令人困惑。请参考下图：
- en: '![Debt ratio](img/B08394_01_33.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![负债比率](img/B08394_01_33.jpg)'
- en: 'Figure 1.31: Graph of outlier detection for the debt ratio column'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.31：负债比率列的异常值检测图
- en: 'Why? It''s confusing because we are not sure which outlier detection method
    we should consider. So, here, we do some comparative analysis just by counting
    the number of outliers derived from each of the methods. Refer to the following
    figure:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？这很令人困惑，因为我们不确定应该考虑哪种异常值检测方法。所以，在这里，我们只通过计算每种方法产生的异常值数量来进行一些比较分析。请参考下图：
- en: '![Debt ratio](img/B08394_01_34.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![负债比率](img/B08394_01_34.jpg)'
- en: 'Figure 1.32: Comparison of various outlier detection techniques'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.32：各种异常值检测技术的比较
- en: 'The maximum number of outliers was detected by the MAD-based method, so we
    will consider that method. Here, we will find the minimum upper bound value in
    order to replace the outlier values. The minimum upper bound is the minimum value
    derived from the outlier value. Refer to the following code snippet:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最大异常值检测是通过基于MAD的方法进行的，因此我们将考虑该方法。在这里，我们将找到最小上界值以替换异常值。最小上界是从异常值中得出的最小值。请参考以下代码片段：
- en: '![Debt ratio](img/B08394_01_35.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![负债比率](img/B08394_01_35.jpg)'
- en: 'Figure 1.33: The code for the minimum upper bound'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.33：最小上界代码
- en: Monthly income
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 月收入
- en: 'For this data attribute, we will select the voting-based outlier detection
    method, as shown in the following figure:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据属性，我们将选择基于投票的异常值检测方法，如下图所示：
- en: '![Monthly income](img/B08394_01_36.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![月收入](img/B08394_01_36.jpg)'
- en: 'Figure 1.34: Outlier detection graph'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.34：异常值检测图
- en: 'In order to replace the outlier, we will use the same logic that we have for
    the `debt ratio` data attribute. We replace the outliers by generating a minimum
    upper bound value. You can refer to the code given in the following figure:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了替换异常值，我们将使用与“负债比率”数据属性相同的逻辑。我们通过生成一个最小上界值来替换异常值。你可以参考下图中给出的代码：
- en: '![Monthly income](img/B08394_01_37.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![月收入](img/B08394_01_37.jpg)'
- en: 'Figure 1.35: Replace the outlier value with the minimum upper bound value'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.35：用最小上界值替换异常值
- en: Number of open credit lines and loans
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 开放信用额度及贷款数量
- en: 'If you refer to the graph given in the following figure, you will see that
    there are no highly deviated outlier values present in this column:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你参考下图中给出的图表，你会看到在这个列中没有出现高度偏离的异常值：
- en: '![Number attributes, outliersmonthly incomeof open credit lines and loans](img/B08394_01_38.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![数量属性、异常值、月收入、开放信用额度及贷款](img/B08394_01_38.jpg)'
- en: 'Figure 1.36: Outlier detection graph'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.36：异常值检测图
- en: So, we will not perform any kind of replacement operation for this data attribute.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将不对这个数据属性执行任何替换操作。
- en: Number of times 90 days late
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 90天逾期次数
- en: For this attribute, when you analyze the data value frequency, you will immediately
    see that the values 96 and 98 are outliers. We will replace these values with
    the median value of the data attribute.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个属性，当你分析数据值频率时，你会立即看到96和98是异常值。我们将用数据属性的中位数替换这些值。
- en: 'Refer to the frequency analysis code snippet in the following figure:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 参考下图中频率分析的代码片段：
- en: '![Number of times 90 days late](img/B08394_01_39.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![90天逾期次数](img/B08394_01_39.jpg)'
- en: 'Figure 1.37: Frequency analysis of the data points'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.37：数据点的频率分析
- en: 'The outlier replacement code snippet is shown in the following figure:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值替换代码片段如下所示：
- en: '![Number of times 90 days late](img/B08394_01_40.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![90天逾期次数](img/B08394_01_40.jpg)'
- en: 'Figure 1.38: Outlier replacement using the median value'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.38：使用中位数替换异常值
- en: Number of real estate loans or lines
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 房地产贷款或信用额度数量
- en: When we see the frequency of value present in the data attribute, we will come
    to know that a frequency value beyond 17 is too less. So, here we replace every
    value less than 17 with 17.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到数据属性中值的频率时，我们会知道超过17的频率值太低。所以，在这里，我们将每个小于17的值替换为17。
- en: 'You can refer to the code snippet in the following figure:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下图中的代码片段：
- en: '![Number of real estate loans or lines](img/B08394_01_41.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![房地产贷款或信用额度数量](img/B08394_01_41.jpg)'
- en: 'Figure 1.39: Code snippet for replacing outliers'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.39：替换异常值代码片段
- en: Number of times 60-89 days past due not worse
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 60-89天逾期次数未变差
- en: For this attribute, when you analyze the data value frequency, you will immediately
    see that the values 96 and 98 are outliers. We will replace these values with
    the median value of the data attribute.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个属性，当你分析数据值频率时，你会立即看到96和98是异常值。我们将用数据属性的均值替换这些值。
- en: 'Refer to the frequency analysis code snippet in the following figure:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图中的频率分析代码片段：
- en: '![Number of times 60-89 days past due not worse](img/B08394_01_42.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![60-89天逾期次数未变差的情况](img/B08394_01_42.jpg)'
- en: 'Figure 1.40: Frequency analysis of the data'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.40：数据频率分析
- en: 'The outlier replacement code snippet is shown in the following figure:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值替换代码片段如下所示：
- en: '![Number of times 60-89 days past due not worse](img/B08394_01_43.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![60-89天逾期次数未变差的情况](img/B08394_01_43.jpg)'
- en: 'Figure 1.41: Code snippet for replacing outliers using the median value'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.41：使用中位数替换异常值的代码片段
- en: You can refer to the `removeSpecificAndPutMedian` method code from *Figure 1.38.*
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考*图1.38*中的`removeSpecificAndPutMedian`方法代码。
- en: Number of dependents
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 受抚养人数
- en: For this attribute, when you see the frequency value of the data points, you
    will immediately see that data values greater than 10 are outliers. We replace
    values greater than 10 with 10.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个属性，当你看到数据点的频率值时，你会立即看到大于10的数据值是异常值。我们将大于10的值替换为10。
- en: 'Refer to the code snippet in the following figure:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图中的代码片段：
- en: '![Number of dependents](img/B08394_01_44.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![受抚养人数](img/B08394_01_44.jpg)'
- en: 'Figure 1.42: Code snippet for replacing outlier values'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.42：替换异常值代码片段
- en: This is the end of the outlier section. In this section, we've replaced the
    value of the data points in a more meaningful way. We have also reached the end
    of our basic data analysis section. This analysis has given us a good understanding
    of the dataset and its values. The next section is all about feature engineering.
    So, we will start with the basics first, and later on in this chapter, you will
    learn how feature engineering will impact the accuracy of the algorithm in a positive
    manner.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '这就结束了异常值部分。在本节中，我们以更有意义的方式替换了数据点的值。我们也到达了基本数据分析部分的终点。这次分析让我们对数据集及其值有了很好的理解。下一节全是关于特征工程。所以，我们首先从基础知识开始，然后在本章的后面，你将学习到特征工程如何以积极的方式影响算法的准确性。 '
- en: Feature engineering for the baseline model
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基线模型的特征工程
- en: In this section, you will learn how to select features that are important in
    order to develop the predictive model. So right now, just to begin with, we won't
    focus much on deriving new features at this stage because first, we need to know
    which input variables / columns / data attributes / features give us at least
    baseline accuracy. So, in this first iteration, our focus is on the selection
    of features from the available training dataset.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何选择重要的特征来开发预测模型。所以现在，首先，我们不会过多关注在这一阶段推导新的特征，因为首先，我们需要知道哪些输入变量/列/数据属性/特征至少能给我们提供基线准确率。因此，在这个第一次迭代中，我们的重点是选择可用的训练数据集中的特征。
- en: Finding out Feature importance
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定特征重要性
- en: 'We need to know which the important features are. In order to find that out,
    we are going to train the model using the Random Forest classifier. After that,
    we will have a rough idea about the important features for us. So let''s get straight
    into the code. You can refer to the code snippet in the following figure:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要知道哪些是重要的特征。为了找出这些特征，我们将使用随机森林分类器来训练模型。之后，我们将对我们来说重要的特征有一个大致的了解。所以让我们直接进入代码。你可以参考以下图中的代码片段：
- en: '![Finding out Feature importance](img/B08394_01_45.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![确定特征重要性](img/B08394_01_45.jpg)'
- en: 'Figure 1.43: Derive the importance of features'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.43：推导特征的重要性
- en: In this code, we are using Random Forest Classifier from scikit-learn. We use
    the `fit()` function to perform training, and then, in order to generate the importance
    of the features, we will use the `feature_importances`_ function, which is available
    in the scikit-learn library. Then, we will print the features with the highest
    importance value to the lowest importance value.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们使用scikit-learn的RandomForest分类器。我们使用`fit()`函数进行训练，然后，为了生成特征的重要性，我们将使用scikit-learn库中可用的`feature_importances`_函数。然后，我们将按重要性值从高到低打印出特征。
- en: 'Let''s draw a graph of this to get a better understanding of the most important
    features. You can find the code snippet in the following figure:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制一个图表，以便更好地理解最重要的特征。您可以在以下图中找到代码片段：
- en: '![Finding out Feature importance](img/B08394_01_46.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![找出特征重要性](img/B08394_01_46.jpg)'
- en: 'Figure 1.44: Code snippet for generating a graph for feature importance'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.44：生成特征重要性图的代码片段
- en: 'In this code snippet, we are using the matplotlib library to draw the graph.
    Here, we use a bar graph and feed in the values of all the data attributes and
    their importance values, which we previously derived. You can refer to the graph
    in the following figure:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码片段中，我们使用matplotlib库来绘制图表。在这里，我们使用条形图并输入所有数据属性及其重要性值，这些值是我们之前推导出来的。您可以参考以下图中的图表：
- en: '![Finding out Feature importance](img/B08394_01_47.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![找出特征重要性](img/B08394_01_47.jpg)'
- en: 'Figure 1.45: Graph of feature importance'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.45：特征重要性图
- en: For the first iteration, we did this quite some work on the feature engineering
    front. We will surely revisit feature engineering in the upcoming sections. Now
    it's time to implement machine learning algorithms to generate the baseline predictive
    model, which will give us an idea of whether a person will default on a loan in
    the next 2 years or not. So let's jump to the next section.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代中，我们在特征工程方面做了一些相当多的工作。我们将在接下来的章节中重新审视特征工程。现在，是时候实现机器学习算法来生成基线预测模型了，这将让我们了解一个人在接下来的两年内是否会违约。所以，让我们跳到下一节。
- en: Selecting machine learning algorithms
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择机器学习算法
- en: This section is the most important one. Here, we will try a couple of different
    ML algorithms in order to get an idea about which ML algorithm performs better.
    Also, we will perform a training accuracy comparison.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节是最重要的。在这里，我们将尝试几种不同的机器学习算法，以便了解哪种机器学习算法表现更好。我们还将进行训练准确度比较。
- en: 'By this time, you will definitely know that this particular problem is considered
    a classification problem. The algorithms that we are going to choose are as follows
    (this selection is based on intuition):'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 到这时，你肯定知道这个特定问题被认为是分类问题。我们将选择的算法如下（这个选择基于直觉）：
- en: K-Nearest Neighbor (KNN)
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-Nearest Neighbor (KNN)
- en: Logistic Regression
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: AdaBoost
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost
- en: GradientBoosting
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GradientBoosting
- en: RandomForest
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RandomForest
- en: 'Our first step is to generate the training data in a certain format. We are
    going to split the training dataset into a training and testing dataset. So, basically,
    we are preparing the input for our training. This is common for all the ML algorithms.
    Refer to the code snippet in the following figure:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是生成特定格式的训练数据。我们将把训练数据集分成训练集和测试集。所以，基本上，我们正在为训练准备输入。这对于所有机器学习算法都是常见的。请参考以下图中的代码片段：
- en: '![Selecting machine learning algorithms](img/B08394_01_48.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![选择机器学习算法](img/B08394_01_48.jpg)'
- en: 'Figure 1.46: Code snippet for generating a training dataset in the key-value
    format for training'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.46：生成用于训练的关键值格式训练数据集的代码片段
- en: As you can see in the code, variable x contains all the columns except the target
    column entitled `seriousdlqin2yrs`, so we have dropped this column. The reason
    behind dropping this attribute is that this attribute contains the answer/target/label
    for each row. ML algorithms need input in terms of a key-value pair, so a target
    column is key and all other columns are values. We can say that a certain pattern
    of values will lead to a particular target value, which we need to predict using
    an ML algorithm.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码所示，变量x包含除了名为`seriousdlqin2yrs`的目标列之外的所有列，因此我们已删除此列。删除此属性的原因是，该属性包含每行的答案/目标/标签。机器学习算法需要以键值对的形式输入，因此目标列是键，所有其他列是值。我们可以这样说，某种值模式将导致特定的目标值，这是我们需要使用机器学习算法进行预测的。
- en: Here, we also split the training data. We will use 75% of the training data
    for actual training purposes, and once training is completed, we will use the
    remaining 25% of the training data to check the training accuracy of our trained
    ML model. So, without wasting any time, we will jump to the coding of the ML algorithms,
    and I will explain the code to you as and when we move forward. Note that here,
    I'm not get into the mathematical explanation of the each ML algorithm but I am
    going to explain the code.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们也分割了训练数据。我们将使用75%的训练数据用于实际训练目的，一旦训练完成，我们将使用剩余的25%训练数据来检查我们训练的ML模型的训练准确度。所以，我们不浪费任何时间，将直接跳到ML算法的编码，随着我们的前进，我会解释代码。注意，在这里，我不会深入到每个ML算法的数学解释，但我将解释代码。
- en: K-Nearest Neighbor (KNN)
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-Nearest Neighbor (KNN)
- en: In this algorithm, generally, our output prediction follows the same tendency
    as that of its neighbor. K is the number of neighbors that we are going to consider.
    If K=3, then during the prediction output, check the three nearest neighbor points,
    and if one neighbor belongs to *X* category and two neighbors belongs to *Y* category,
    then the predicted label will be *Y,* as the majority of the nearest points belongs
    to the *Y* category.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，通常我们的输出预测遵循其邻居的相同趋势。K是我们将要考虑的邻居数量。如果K=3，那么在预测输出时，检查三个最近的邻居点，如果一个邻居属于*X*类别，而两个邻居属于*Y*类别，那么预测标签将是*Y*，因为大多数最近的点属于*Y*类别。
- en: 'Let''s see what we have coded. Refer to the following figure:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们编写了什么。参考以下图：
- en: '![K-Nearest Neighbor (KNN)](img/B08394_01_49.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![K-Nearest Neighbor (KNN)](img/B08394_01_49.jpg)'
- en: 'Figure 1.47: Code snippet for defining the KNN classifier'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.47：定义KNN分类器的代码片段
- en: 'Let''s understand the parameters one by one:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个了解这些参数：
- en: As per the code, K=5 means our prediction is based on the five nearest neighbors.
    Here, `n_neighbors=5`.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据代码，K=5意味着我们的预测基于五个最近的邻居。在这里，`n_neighbors=5`。
- en: Weights are selected uniformly, which means all the points in each neighborhood
    are weighted equally. Here, weights='uniform'.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重均匀选择，这意味着每个邻域中的所有点都被同等加权。在这里，weights='uniform'。
- en: 'algorithm=''auto'': This parameter will try to decide the most appropriate
    algorithm based on the values we passed.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'algorithm=''auto'': 此参数将尝试根据我们传递的值决定最合适的算法。'
- en: 'leaf_size = 30: This parameter affects the speed of the construction of the
    model and query. Here, we have used the default value, which is 30.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'leaf_size = 30: 此参数影响模型构建和查询的速度。在这里，我们使用了默认值，即30。'
- en: 'p=2: This indicates the power parameter for the Minkowski metric. Here, p=2
    uses `euclidean_distance`.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'p=2: 这表示Minkowski度量的幂参数。在这里，p=2使用`euclidean_distance`。'
- en: 'metric=''minkowski'': This is the default distance metric, which helps us build
    the tree.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'metric=''minkowski'': 这是默认的距离度量，它帮助我们构建树。'
- en: 'metric_params=None: This is the default value that we are using.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'metric_params=None: 这是我们在使用的默认值。'
- en: Logistic regression
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is one of most widely used ML algorithms and is also one
    of the oldest. This algorithm generates probability for the target variable using
    sigmod and other nonlinear functions in order to predict the target labels.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是最广泛使用的ML算法之一，也是最古老的算法之一。该算法使用sigmoid和其他非线性函数生成目标变量的概率，以预测目标标签。
- en: 'Let''s refer to the code and the parameter that we have used for Logistic regression.
    You can refer to the code snippet given in the following figure:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们参考代码和用于逻辑回归的参数。您可以参考以下图中的代码片段：
- en: '![Logistic regression](img/B08394_01_50.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/B08394_01_50.jpg)'
- en: 'Figure 1.48: Code snippet for the Logistic regression ML algorithm'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.48：逻辑回归ML算法的代码片段
- en: 'Let''s understand the parameters one by one:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个了解这些参数：
- en: 'penalty=''l1'': This parameter indicates the choice of the gradient descent
    algorithm. Here, we have selected the Newton-Conjugate_Gradient method.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'penalty=''l1'': 此参数表示梯度下降算法的选择。在这里，我们选择了牛顿-共轭梯度方法。'
- en: 'dual=False: If we have number of sample > number of features, then we should
    set this parameter as false.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'dual=False: 如果样本数量大于特征数量，我们应该将此参数设置为false。'
- en: 'tol=0.0001: This is one of the stopping criteria for the algorithm.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'tol=0.0001: 这是算法的一个停止标准。'
- en: 'c=1.0: This value indicates the inverse of the regularization strength. This
    parameter must be a positive float value.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'c=1.0: 此值表示正则化强度的倒数。此参数必须是一个正浮点值。'
- en: 'fit_intercept = True: This is a default value for this algorithm. This parameter
    is used to indicate the bias for the algorithm.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'solver=''liblinear'': This algorithm performs well for small datasets, so we
    chose that.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'intercept_scaling=1: If we select the liblinear algorithm and fit_intercept
    = True, then this parameter helps us generate the feature weight.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'class_weight=None: There is no weight associated with the class labels.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'random_state=None: Here, we use the default value of this parameter.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'max_iter=100: Here, we iterate 100 times in order to converge our ML algorithm
    on the given dataset.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'multi_class=''ovr'': This parameter indicates that the given problem is the
    binary classification problem.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'verbose=2: If we use the liblinear in the solver parameter, then we need to
    put in a positive number for verbosity.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaBoost
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The AdaBoost algorithm stands for Adaptive Boosting. Boosting is an ensemble
    method in which we will build strong classifier by using multiple weak classifiers.
    AdaBoost is boosting algorithm giving good result for binary classification problems.
    If you want to learn more about it then refer this article [https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/](https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: This particular algorithm has N number of iterations. In the first iteration,
    we start by taking random data points from the training dataset and building the
    model. After each iteration, the algorithm checks for data points in which the
    classifier doesn't perform well. Once those data points are identified by the
    algorithm based on the error rate, the weight distribution is updated. So, in
    the next iteration, there are more chances that the algorithm will select the
    previously poorly classified data points and learn how to classify them. This
    process keeps running for the given number of iterations you provide.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s refer to the code snippet given in the following figure:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '![AdaBoost](img/B08394_01_51.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.49: Code snippet for the AdaBosst classifier'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter-related description is given as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '**base_estimator = None**: The base estimator from which the boosted ensemble
    is built.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n_estimators=200**: The maximum number of estimators at which boosting is
    terminated. After 200 iterations, the algorithm will be terminated.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**learning_rate=1.0**: This rate decides how fast our model will converge.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GradientBoosting
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This algorithm is also a part of the ensemble of ML algorithms. In this algorithm,
    we use basic regression algorithm to train the model. After training, we will
    calculate the error rate as well as find the data points for which the algorithm
    does not perform well, and in the next iteration, we will take the data points
    that introduced the error and retrain the model for better prediction. The algorithm
    uses the already generated model as well as a newly generated model to predict
    the values for the data points.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code snippet in the following figure:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '![GradientBoosting](img/B08394_01_52.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.50: Code snippet for the Gradient Boosting classifier'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the parameters of the classifier:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '**loss=''deviance''**: This means that we are using logistic regression for
    classification with probabilistic output.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**learning_rate = 0.1**: This parameter tells us how fast the model needs to
    converge.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n_estimators = 200**: This parameter indicates the number of boosting stages
    that are needed to be performed.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**subsample = 1.0**: This parameter helps tune the value for bias and variance.
    Choosing subsample < 1.0 leads to a reduction in variance and an increase in bias.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**min_sample_split=2**: The minimum number of samples required to split an
    internal node.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**min_weight_fraction_leaf=0.0**: Samples have equal weight, so we have provided
    the value 0.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_depth=3**: This indicates the maximum depth of the individual regression
    estimators. The maximum depth limits the number of nodes in the tree.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**init=None**: For this parameter, loss.init_estimator is used for the initial
    prediction.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**random_state=None**: This parameter indicates that the random state is generated
    using the `numpy.random` function.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_features=None**: This parameter indicates that we have N number of features.
    So, `max_features=n_features`.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**verbose=0**: No progress has been printed.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RandomForest
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This particular ML algorithm generates the number of decision trees and uses
    the voting mechanism to predict the target label. In this algorithm, there are
    a number of decision trees generated, creating a forest of trees, so it's called
    RandomForest.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, note how we have declared the RandomForest classifier:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![RandomForest](img/B08394_01_53.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.51: Code snippet for Random Forest Classifier'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the parameters here:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators=10`: This indicates the number of trees in the forest.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`criterion=''gini''`: Information gained will be calculated by gini.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth=None`: This parameter indicates that nodes are expanded until all
    leaves are pure or until all leaves contain less than min_samples_split samples.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split=2`: This parameter indicates that there is a minimum of
    two samples required to perform splitting in order to generate the tree.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_leaf=1`: This indicates the sample size of the leaf node.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_weight_fraction_leaf=0.0`: This parameter indicates the minimum weighted
    fraction of the sum total of weights (of all the input samples) required to be
    at a leaf node. Here, weight is equally distributed, so a sample weight is zero.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features=''auto''`: This parameter is considered using the auto strategy.
    We select the auto value, and then we select max_features=sqrt(n_features).'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_leaf_nodes=None`: This parameter indicates that there can be an unlimited
    number of leaf nodes.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bootstrap=True`: This parameter indicates that the bootstrap samples are used
    when trees are being built.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`oob_score=False`: This parameter indicates whether to use out-of-the-bag samples
    to estimate the generalization accuracy. We are not considering out-of-the-bag
    samples here.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_jobs=1`: Both fit and predict job can be run in parallel if `n_job = 1`.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state=None`: This parameter indicates that random state is generated
    using the `numpy.random` function.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose=0`: This controls the verbosity of the tree building process. 0 means
    we are not printing the progress.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up until now, we have seen how we declare our ML algorithm. We have also defined
    some parameter values. Now, it's time to train this ML algorithm on the training
    dataset. So let's discuss that.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Training the baseline model
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will perform actual training using the following ML algorithms.
    This step is time-consuming as it needs more computation power. We use 75% of
    the training dataset for actual training and 25% of the dataset for testing in
    order to measure the training accuracy.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code snippet in the following figure:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the baseline model](img/B08394_01_54.jpg)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.52: Code snippet for performing training'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code snippet, you can see that we performed the actual training
    operation using the `fit()` function from the scikit-learn library. This function
    uses the given parameter and trains the model by taking the input of the target
    data attribute and other feature columns.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Once you are done with this step, you'll see that our different ML algorithms
    generate different trained models. Now it's time to check how good our trained
    model is when it comes to prediction. There are certain techniques that we can
    use on 25% of the dataset. In the next section, we will understand these techniques.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at some of the widely used testing matrices that
    we can use in order to get an idea about how good or bad our trained model is.
    This testing score gives us a fair idea about which model achieves the highest
    accuracy when it comes to the prediction of the 25% of the data.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are using two basic levels of the testing matrix:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: The mean accuracy of the trained models
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ROC-AUC score
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mean accuracy of the trained models
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will understand how scikit-learn calculates the accuracy
    score when we use the scikit-learn function `score()` to generate the training
    accuracy. The function score() returns the mean accuracy. More precisely, it uses
    residual standard error. Residual standard error is nothing but the positive square
    root of the mean square error. Here, the equation for calculating accuracy is
    as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mean accuracy of the trained models](img/B08394_01_55.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
- en: The best possible score is 1.0 and the model can have a negative score as well
    (because the model can be arbitrarily worse). If a constant model always predicts
    the expected value of y, disregarding the input features, it will get a residual
    standard error score of 0.0.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: The ROC-AUC score
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ROC-AUC score is used to find out the accuracy of the classifier. ROC and
    AUC are two different terms. Let's understand each of the terms one by one.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: ROC
  id: totrans-435
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ROC stands for *Receiver Operating Characteristic*. It's is a type of curve.
    We draw the ROC curve to visualize the performance of the binary classifier. Now
    that I have mentioned that ROC is a curve, you may want to know which type of
    curve it is, right? The ROC curve is a 2-D curve. It's *x* *axis* represents the
    *False Positive Rate* (FPR) and its y *axis* represents the *True Positive Rate*
    (TPR). TPR is also known as sensitivity, and FPR is also known as specificity
    (SPC). You can refer to the following equations for FPR and TPR.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '*TPR = True Positive / Number of positive samples = TP / P*'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '*FPR = False Positive / Number of negative samples = FP / N = 1 - SPC*'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: 'For any binary classifier, if the predicted probability is ≥ 0.5, then it will
    get the class label X, and if the predicted probability is < 0.5, then it will
    get the class label Y. This happens by default in most binary classifiers. This
    cut-off value of the predicted probability is called the threshold value for predictions.
    For all possible threshold values, FPR and TPR have been calculated. This FPR
    and TPR is an x,y value pair for us. So, for all possible threshold values, we
    get the x,y value pairs, and when we put the points on an ROC graph, it will generate
    the ROC curve. If your classifier perfectly separates the two classes, then the
    ROC curve will hug the upper-right corner of the graph. If the classifier performance
    is based on some randomness, then the ROC curve will align more to the diagonal
    of the ROC curve. Refer to the following figure:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '![ROC](img/B08394_01_56.jpg)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.53: ROC curve for different classification scores'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, the leftmost ROC curve is for the perfect classifier.
    The graph in the center shows the classifier with better accuracy in real-world
    problems. The classifier that is very random in its guess is shown in the rightmost
    graph. When we draw an ROC curve, how can we quantify it? In order to answer that
    question, we will introduce AUC.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: AUC
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AUC stands for Area Under the Curve. In order to quantify the ROC curve, we
    use the AUC. Here, we will see how much area has been covered by the ROC curve.
    If we obtain a perfect classifier, then the AUC score is 1.0, and if we have a
    classifier that is random in its guesses, then the AUC score is 0.5\. In the real
    world, we don''t expect an AUC score of 1.0, but if the AUC score for the classifier
    is in the range of 0.6 to 0.9, then it will be considered a good classifier. You
    can refer to the following figure:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '![AUC](img/B08394_01_57.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.54: AUC for the ROC curve'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, you can see how much area under the curve has been
    covered, and that becomes our AUC score. This gives us an indication of how good
    or bad our classifier is performing.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: These are the two matrices that we are going to use. In the next section, we
    will implement actual testing of the code and see the testing matrix for our trained
    ML models.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Testing the baseline model
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will implement the code, which will give us an idea about
    how good or how bad our trained ML models perform in a validation set. We are
    using the mean accuracy score and the AUC-ROC score.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have generated five different classifiers and, after performing testing
    for each of them on the validation dataset, which is 25% of held-out dataset from
    the training dataset, we will find out which ML model works well and gives us
    a reasonable baseline score. So let's look at the code:.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the baseline model](img/B08394_01_58.jpg)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.55: Code snippet to obtain a test score for the trained ML model'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code snippet, you can see the scores for three classifiers.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the code snippet in the following figure:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the baseline model](img/B08394_01_59.jpg)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.56: Code snippet to obtain the test score for the trained ML model'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: In the code snippet, you can see the score of the two classifiers.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Using the `score()` function of scikit-learn, you will get the mean accuracy
    score, whereas, the `roc_auc_score()` function will provide you with the ROC-AUC
    score, which is more significant for us because the mean accuracy score considers
    only one threshold value, whereas the ROC-AUC score takes into consideration all
    possible threshold values and gives us the score.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the code snippets given above, the AdaBoost and GradientBoosting
    classifiers get a good ROC-AUC score on the validation dataset. Other classifiers,
    such as logistic regression, KNN, and RandomForest do not perform well on the
    validation set. From this stage onward, we will work with AdaBoost and GradientBoosting
    classifiers in order to improve their accuracy score.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see what we need to do in order to increase classification
    accuracy. We need to list what can be done to get good accuracy and what are the
    current problems with the classifiers. So let's analyze the problem with the existing
    classifiers and look at their solutions.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the existing approach
  id: totrans-462
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We got the baseline score using the AdaBoost and GradientBoosting classifiers.
    Now, we need to increase the accuracy of these classifiers. In order to do that,
    we first list all the areas that can be improvised but that we haven't worked
    upon extensively. We also need to list possible problems with the baseline approach.
    Once we have the list of the problems or the areas on which we need to work, it
    will be easy for us to implement the revised approach.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I''m listing some of the areas, or problems, that we haven''t worked
    on in our baseline iteration:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem: We haven''t used cross-validation techniques extensively in order
    to check the overfitting issue.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solution: If we use cross-validation techniques properly, then we will know
    whether our trained ML model suffers from overfitting or not. This will help us
    because we don''t want to build a model that can''t even be generalized properly.'
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Problem: We also haven''t focused on hyperparameter tuning. In our baseline
    approach, we mostly use the default parameters. We define these parameters during
    the declaration of the classifier. You can refer to the code snippet given in
    *Figure 1.52*, where you can see the classifier taking some parameters that are
    used when it trains the model. We haven''t changed these parameters.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solution: We need to tune these hyperparameters in such a way that we can increase
    the accuracy of the classifier. There are various hyperparameter-tuning techniques
    that we need to use.'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will look at how these optimization techniques actually
    work as well as discuss the approach that we are going to take. So let's begin!
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the existing approach
  id: totrans-470
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will gain an understanding of the basic technicality regarding
    cross-validation and hyperparameter tuning. Once we understand the basics, it
    will be quite easy for us to implement them. Let's start with a basic understanding
    of cross-validation and hyperparameter tuning.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: Understanding key concepts to optimize the approach
  id: totrans-472
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this revised iteration, we need to improve the accuracy of the classifier.
    Here, we will cover the basic concepts first and then move on to the implementation
    part. So, we will understand two useful concepts:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cross-validation is also referred to as rotation estimation. It is basically
    used to track a problem called overfitting. Let me start with the overfitting
    problem first because the main purpose of using cross-validation is to avoid the
    overfitting situation.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Basically, when you train the model using the training dataset and check its
    accuracy, you find out that your training accuracy is quite good, but when you
    apply this trained model on an as-yet-unseen dataset, you realize that the trained
    model does not perform well on the unseen dataset and just mimics the output of
    the training dataset in terms of its target labels. So, we can say that our trained
    model is not able to generalize properly. This problem is called overfitting,
    and in order to solve this problem, we need to use cross-validation.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: In our baseline approach, we didn't use cross-validation techniques extensively.
    The good part is that, so far, we generated our validation set of 25% of the training
    dataset and measured the classifier accuracy on that. This is a basic technique
    used to get an idea of whether the classifier suffers from overfitting or not.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other cross validation techniques that will help us with two
    things:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: 'Tracking the overfitting situation using CV: This will give us a perfect idea
    about the overfitting problem. We will use K-fold CV.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model selection using CV: Cross-validation will help us select the classification
    models. This will also use K-fold CV.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's look at the single approach that will be used for both of these tasks.
    You will find the implementation easy to understand.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: The approach of using CV
  id: totrans-484
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The scikit-learn library provides great implementation of cross-validation.
    If we want to implement cross-validation, we just need to import the cross-validation
    module. In order to improvise on accuracy, we will use K-fold cross-validation.
    What this K-fold cross-validation basically does is explained here.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: When we use the train-test split, we will train the model by using 75% of the
    data and validate the model by using 25% of the data. The main problem with this
    approach is that, actually, we are not using the whole training dataset for training.
    So, our model may not be able to come across all of the situations that are present
    in the training dataset. This problem has been solved by K-fold CV.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: In K-fold CV, we need to provide the positive integer number for K. Here, you
    divide the training dataset into the K sub-dataset. Let me give you an example.
    If you have 125 data records in your training dataset and you set the value as
    k = 5, then each subset of the data gets 25 data records. So now, we have five
    subsets of the training dataset with 25 records each.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand how these five subsets of the dataset will be used. Based
    on the provided value of K, it will be decided how many times we need to iterate
    over these subsets of the data. Here we have taken K=5\. So, we iterate over the
    dataset K-1 = 5-1 =4 times. Note that the number of iterations in K-fold CV is
    calculated by the equation K-1\. Now let''s see what happens to each of the iterations:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: '**First iteration**: We take one subset for testing and the remaining four
    subsets for training.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second iteration**: We take two subsets for testing and the remaining three
    subsets for training.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Third iteration**: We take three subsets for testing and the remaining two
    subsets for training.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fourth iteration**: We take four subsets for testing and the remaining subset
    for training. After this fourth iteration, we don''t have any subsets left for
    training or testing, so we stop after iteration K-1.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This approach has the following advantages:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: K-fold CV uses all the data points for training, so our model takes advantage
    of getting trained using all of the data points.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: After every iteration, we get the accuracy score. This will help us decide how
    models perform.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We generally consider the mean value and standard deviation value of the cross-validation
    after all the iterations have been completed. For each iteration, we track the
    accuracy score, and once all iterations have been done, we take the mean value
    of the accuracy score as well as derive the standard deviation (std) value from
    the accuracy scores. This CV mean and standard deviation score will help us identify
    whether the model suffers from overfitting or not.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you perform this process for multiple algorithms then based on this mean
    score and the standard score, you can also decide which algorithm works best for
    the given dataset.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantage of this approach is as follows:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: This k-fold CV is a time-consuming and computationally expensive method.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So after reading this, you hopefully understand the approach and, by using this
    implementation, we can ascertain whether our model suffers from overfitting or
    not. This technique will also help us select the ML algorithm. We will check out
    the implementation of this in the Implementing the Revised Approach section.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: Now let's check out the next optimization technique, which is hyperparameter
    tuning.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will look at how we can use a hyperparameter-tuning technique
    to optimize the accuracy of our model. There are some kind of parameters whose
    value cannot be learnt during training process. These parameters are expressing
    higher-level properties of the ML model. These higher-level parameters are called
    hyperparameters. These are tuning nobs for ML model. We can obtain the best value
    for hyperparameter by trial and error. You can refer more on this by using this
    link: [https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/),
    If we come up with the optimal value of the hyperparameters, then we will able
    to achieve the best accuracy for our model, but the challenging part is that we
    don''t know the exact values of these parameters over our head. These parameters
    are the tuning knobs for our algorithm. So, we need to apply some techniques that
    will give us the best possible value for our hyperparameter, which we can use
    when we perform training.'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, there are two functions that we can use in order to find these
    hyperparameter values, which are as follows:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Grid search parameter tuning
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random search parameter tuning
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid search parameter tuning
  id: totrans-507
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we will look at how grid search parameter tuning works. We
    specify the parameter values in a list called grid. Each value specified in grid
    has been taken in to consideration during the parameter tuning. . The model has
    been built and evaluated based on the specified grid value. This technique exhaustively
    considers all parameter combinations and generates the final optimal parameters.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have five parameters that we want to optimize. Using this technique,
    if we want to try 10 different values for each of the parameters, then it will
    take 105 evaluations. Assume that, on average, for each parameter combination,
    10 minutes are required for training; then, for the evaluation of 105, it will
    take years. Sounds crazy, right? This is the main disadvantage of this technique.
    This technique is very time consuming. So, a better solution is random search.
    '
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: Random search parameter tuning
  id: totrans-510
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The intuitive idea is the same as grid search, but the main difference is that
    instead of trying out all possible combinations, we will just randomly pick up
    the parameter from the selected subset of the grid. If I want to add on to my
    previous example, then in random search, we will take a random subset value of
    the parameter from 105 values. Suppose that we take only 1,000 values from 105
    values and try to generate the optimal value for our hyperparameters. This way,
    we will save time.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: In the revised approach, we will use this particular technique to optimize the
    hyperparameters.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: From the next section, we will see the actual implementation of K-fold cross-validation
    and hyperparameter tuning. So let's start implementing our approach.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the revised approach
  id: totrans-514
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see the actual implementation of our revised approach,
    and this revised approach will use K-fold cross-validation and hyperparameter
    optimization. I have divided the implementation part into two sections so you
    can connect the dots when you see the code. The two implementation parts are as
    follows:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a cross-validation based approach
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing hyperparameter tuning
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a cross-validation based approach
  id: totrans-518
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will see the actual implementation of K-fold CV. Here,
    we are using the scikit-learn cross-validation score module. So, we need to choose
    the value of K-fold. By default, the value is 3\. I''m using the value of K =
    5\. You can refer to the code snippet given in the following figure:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing a cross-validation based approach](img/B08394_01_60.jpg)'
  id: totrans-520
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.57: Code snippet for the implementation of K-fold cross validation'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding figure, we obtain `cvScore.mean()` and `cvScore.std()`
    scores to evaluate our model performance. Note that we have taken the whole training
    dataset into consideration. So, the values for these parameters are `X_train =
    X` and `y_train = y`. Here, we define the `cvDictGen` function , which will track
    the mean value and the standard deviation of the accuracy. We have also implemented
    the `cvDictNormalize` function, which we can use if we want to obtain a normalized
    mean and a standard deviation (std) score. For the time being, we are not going
    to use the `cvDictNormalize` function.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to run the `cvDictGen` method. You can see the output in the
    following figure:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing a cross-validation based approach](img/B08394_01_61.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.58: Code snippet for the output of K-fold cross validation'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: We have performed cross-validation for five different ML algorithms to check
    which ML algorithm works well. As we can see, in our output given in the preceding
    figure, GradietBoosting and Adaboot classifier work well. We have used the cross-validation
    score in order to decide which ML algorithm we should select and which ones we
    should not go with. Apart from that, based on the mean value and the std value,
    we can conclude that our ROC-AUC score does not deviate much, so we are not suffering
    from the overfitting issue.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to see the implementation of hyperparameter tuning.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: Implementing hyperparameter tuning
  id: totrans-528
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will look at how we can obtain optimal values for hyperparameters.
    Here, we are using the `RandomizedSearchCV` hyperparameter tuning method. We have
    implemented this method for the AdaBoost and GradientBossting algorithms. You
    can see the implementation of hyperparameter tuning for the Adaboost algorithm
    in the following figure:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing hyperparameter tuning](img/B08394_01_62.jpg)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.59: Code snippet of hyperparameter tuning for the Adaboost algorithm'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: After running the `RandomizedSearchCV` method on the given values of parameters,
    it will generate the optimal parameter value. As you can see in the preceding
    figure, we want the optimal value for the parameter; `n_estimators`.`RandomizedSearchCV`
    obtains the optimal value for `n_estimators`, which is 100.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the implementation of hyperparameter tuning for the GradientBoosting
    algorithm in the following figure:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing hyperparameter tuning](img/B08394_01_63.jpg)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.60: Code snippet of hyperparameter tuning for the GradientBoosting
    algorithm'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding figure, the `RandomizedSearchCV` method obtains
    the optimal value for the following hyperparameters:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '''loss'': ''deviance'''
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '''max_depth'': 2'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '''n_estimators'': 449'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now it's time to test our revised approach. Let's see how we will test the model
    and what the outcome of the testing will be.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: Implementing and testing the revised approach
  id: totrans-541
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we need to plug the optimal values of the hyperparameters, and then we
    will see the ROC-AUC score on the validation dataset so that we know whether there
    will be any improvement in the accuracy of the classifier or not.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the implementation and how we have performed training using the
    best hyperparameters by referring to the following figure:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing and testing the revised approach](img/B08394_01_64.jpg)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.61: Code snippet for performing training by using optimal hyperparameter
    values'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we are done with the training, we can use the trained model to predict
    the target labels for the validation dataset. After that, we can obtain the ROC-AUC
    score, which gives us an idea of how much we are able to optimize the accuracy
    of our classifier. This score also helps validate our direction, so if we aren''t
    able to improve our classifier accuracy, then we can identify the problem and
    improve accuracy in the next iteration. You can see the ROC-AUC score in the following
    figure:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing and testing the revised approach](img/B08394_01_65.jpg)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.62: Code snippet of the ROC-AUC score for the revised approach'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the output, after hyperparameter tuning, we have an improvement
    in the ROC-AUC score compared to our baseline approach. In our baseline approach,
    the ROC-AUC score for AdaBoost is 0.85348539, whereas after hyperparameter tuning,
    it is 0.86572352\. In our baseline approach, the ROC-AUC score for GradientBoosting
    is 0.85994964, whereas after hyperparameter tuning, it is 0.86999235\. These scores
    indicate that we are heading in the right direction.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: 'The question remains: *can we further improve the accuracy of the classifiers?*
    Sure, there is always room for improvement, so we will follow the same approach.
    We list all the possible problems or areas we haven''t touched upon yet. We try
    to explore them and generate the best possible approach that can give us good
    accuracy on the validation dataset as well as the testing dataset.'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: So let's see what our untouched areas in this revised approach will be.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: Understanding problems with the revised approach
  id: totrans-552
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up until the revised approach, we did not spend a lot of time on feature engineering.
    So in our best possible approach, we spent time on the transformation of features
    engineering. We need to implement a voting mechanism in order to generate the
    final probability of the prediction on the actual test dataset so that we can
    get the best accuracy score.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the two techniques that we need to apply:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: Feature transformation
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ensemble ML model with a voting mechanism
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we implement these techniques, we will check our ROC-AUC score on the validation
    dataset. After that, we will generate a probability score for each of the records
    present in the real test dataset. Let's start with the implementation.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: Best approach
  id: totrans-558
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the previous section, in this iteration, we will focus on feature
    transformation as well as implementing a voting classifier that will use the AdaBoost
    and GradientBoosting classifiers. Hopefully, by using this approach, we will get
    the best ROC-AUC score on the validation dataset as well as the real testing dataset.
    This is the best possible approach in order to generate the best result. If you
    have any creative solutions, you can also try them as well. Now we will jump to
    the implementation part.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  id: totrans-560
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will implement the following techniques:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: Log transformation of features
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voting-based ensemble model
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's implement feature transformation first.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: Log transformation of features
  id: totrans-565
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will apply log transformation to our training dataset. The reason behind
    this is that we have some attributes that are very skewed and some data attributes
    that have values that are more spread out in nature. So, we will be taking the
    natural log of one plus the input feature array. You can refer to the code snippet
    shown in the following figure:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '![Log transformation of features](img/B08394_01_66.jpg)'
  id: totrans-567
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.63: Code snippet for log(p+1) transformation of features.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: I have also tested the ROC-AUC accuracy on the validation dataset, which gives
    us a minor change in accuracy.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: Voting-based ensemble ML model
  id: totrans-570
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will use a voting-based ensemble classifier. The scikit-learn
    library already has a module available for this. So, we implement a voting-based
    ML model for both untransformed features as well as transformed features. Let''s
    see which version scores better on the validation dataset. You can refer to the
    code snippet given in the following figure:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: '![Voting-based ensemble ML model](img/B08394_01_67.jpg)'
  id: totrans-572
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.64: Code snippet for a voting based ensemble classifier'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are using two parameters: weight 2 for GradientBoosting and 1 for
    the AdaBoost algorithm. I have also set the voting parameter as soft so classifiers
    can be more collaborative.'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: We are almost done with trying out our best approach using a voting mechanism.
    In the next section, we will run our ML model on a real testing dataset. So let's
    do some real testing!
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: Running ML models on real test data
  id: totrans-576
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we will be testing the accuracy of a voting-based ML model on our testing
    dataset. In the first iteration, we are not going to take log transformation for
    the test dataset, and in the second iteration, we are going to take log transformation
    for the test dataset. In both cases, we will generate the probability for the
    target class. Here, we are generating probability because we want to know how
    much of a chance there is of a particular person defaulting on their loan in the
    next 2 years. We will save the predicted probability in a `csv` file.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code for performing testing in the following figure:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: '![Running ML models on real test data](img/B08394_01_68.jpg)'
  id: totrans-579
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.65: Code snippet for testing'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: If you can see *Figure 1.64* then you come to know that here, we have achieved
    86% accuracy. This score is by far the most efficient accuracy as per industry
    standards.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-582
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to analyze a dataset using various statistical
    techniques. After that, we obtained a basic approach and, by using that approach,
    we developed a model that didn't even achieve the baseline. So, we figured out
    what had gone wrong in the approach and tried another approach, which solved the
    issues of our baseline model. Then, we evaluated that approach and optimized the
    hyper parameters using cross-validation and ensemble techniques in order to achieve
    the best possible outcome for this application. Finally, we found out the best
    possible approach, which gave us state-of-the-art results. You can find all of
    the code for this on GitHub at [https://github.com/jalajthanaki/credit-risk-modelling](https://github.com/jalajthanaki/credit-risk-modelling).
    You can find all the installation related information at [https://github.com/jalajthanaki/credit-risk-modelling/blob/master/README.md](https://github.com/jalajthanaki/credit-risk-modelling/blob/master/README.md).
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will look at another very interesting application of
    the analytics domain: predicting the stock price of a given share. Doesn''t that
    sound interesting? We will also use some modern machine learning (ML) and deep
    learning (DL) approaches in order to develop stock price prediction application,
    so get ready for that as well!'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
