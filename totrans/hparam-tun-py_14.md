# *第11章*: 理解流行算法的超参数

大多数**机器学习**（**ML**）算法都有自己的超参数。在不了解模型超参数的情况下，知道如何实施许多花哨的超参数调整方法，就像医生在诊断病人之前就开处方一样。

在本章中，我们将学习几种流行机器学习算法的超参数。对于每个算法，都会有广泛的解释，包括但不限于每个超参数的定义、当每个超参数的值发生变化时将产生什么影响，以及基于影响的超参数优先级列表。

到本章结束时，你将了解几种流行机器学习算法的重要超参数。理解机器学习算法的超参数至关重要，因为并非所有超参数在影响模型性能时都同等重要。我们不必对模型的所有超参数进行超参数调整；我们只需要关注更关键的超参数。

本章将涵盖以下主要内容：

+   探索随机森林超参数

+   探索XGBoost超参数

+   探索LightGBM超参数

+   探索CatBoost超参数

+   探索SVM超参数

+   探索人工神经网络超参数

# 探索随机森林超参数

**随机森林**是一种基于树的模型，它使用一系列**决策树**构建而成。它是一个非常强大的集成机器学习模型，可以用于分类和回归任务。随机森林利用决策树集合的方式是通过执行一种称为**自助聚集**（**bagging**）的集成方法，并进行一些修改。为了理解随机森林的每个超参数如何影响模型性能，我们首先需要了解模型是如何工作的。

在讨论随机森林如何集成一系列决策树之前，让我们先讨论一下决策树在高级别是如何工作的。决策树可以通过构建一系列决策（以规则和分割点形式）来执行分类或回归任务，这些决策可以以树的形式可视化。这些决策是通过查看给定训练数据中的所有特征和特征值来做出的。决策树的目标是使每个叶节点具有高度的纯度。可以使用几种方法来衡量纯度；对于分类任务，最流行的方法是计算**基尼**或**熵**值，而对于回归任务，最流行的方法是计算**均方误差**值。

随机森林利用袋装法来集成决策树集合。袋装法是一种集成方法，通过结合多个机器学习模型的预测，希望生成更准确和鲁棒的预测。在这种情况下，随机森林结合了多个决策树的预测输出，这样我们就不太关注单个树的预测。这是因为决策树很可能对训练数据进行过拟合。然而，随机森林不仅仅利用了传统的袋装集成方法，它还确保只利用那些彼此之间高度不相关的决策树集合的预测输出。随机森林是如何做到这一点的呢？它不是要求每个决策树在选择分割点时查看所有特征及其值，而是随机森林定制了这个过程，使得每个决策树只查看特征的一个随机样本。

在Python中，最受欢迎且维护得最好的随机森林实现可以在scikit-learn包中找到。它包括回归（`RandomForestRegressor`）和分类（`RandomForestClassifier`）任务的实现。这两个实现具有非常相似的超参数，只有少数小的差异。以下是最重要的超参数，按照对模型性能影响从大到小排序。请注意，这个优先级列表是主观的，基于我们过去开发随机森林模型的经验：

1.  `n_estimators`：这指定了用于构建随机森林的决策树数量。一般来说，树的数量越多，模型的性能越好，但这也意味着计算时间会更长。然而，存在一个阈值，超过这个阈值添加更多的树对模型性能的提升影响不大。甚至可能由于过拟合问题而产生负面影响。

1.  `max_features`：这指定了随机森林用于在每个决策树中选择最佳分割点的随机采样特征的数量。值越高，方差减少越少，因此偏差增加越少。更高的值也会导致计算时间更长。scikit-learn默认情况下，对于回归任务将使用所有特征，而对于分类任务则只使用`sqrt(n_features)`数量的特征。

1.  `criterion`：这用于衡量每个决策树的同质性。scikit-learn为回归和分类任务实现了几种方法。对于回归任务有`squared_error`、`absolute_error`和`poisson`，而对于分类任务有`gini`、`entropy`和`log_loss`。不同的方法将对模型性能产生不同的影响；对于这个超参数没有明确的经验法则。

1.  `max_depth`: 这指定了每个决策树的最大深度。此超参数的默认值为`None`，意味着每个树的节点将继续分支，直到我们得到纯叶节点或直到所有叶子节点包含的样本数少于`min_samples_split`。值越低越好，因为这可以防止过拟合。然而，一个过低的值可能导致欠拟合问题。有一点可以肯定——值越高意味着计算时间越长。

1.  `min_samples_split`: 这指定了树能够进一步分裂内部节点（可以分裂成子节点的节点）所需的最小样本数。值越高，越容易防止过拟合。

1.  `min_samples_leaf`: 这指定了叶节点中所需的最小样本数。较高的值可以帮助我们防止过拟合。

scikit-learn中的随机森林超参数

如需了解scikit-learn中随机森林实现的每个超参数的更多信息，请访问官方文档页面：https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html 和 [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)。

其他有用的模板参数可以在不同的scikit-learn估计器实现中找到。以下是一些您在训练scikit-learn估计器时需要了解的重要参数，这些参数可以帮助您：

1.  `class_weight`: 这指定了训练数据中每个类别的权重。此参数仅适用于分类任务。当您面临类别不平衡问题时，此参数非常重要。我们需要给样本较少的类别赋予更高的权重。

1.  `n_jobs`: 这指定了在训练估计器时使用的并行进程数。scikit-learn在后台使用`joblib`包。

1.  `random_state`: 这指定了随机种子数，以确保代码的可重复性。

1.  `verbose`: 此参数用于控制任何日志活动。将`verbose`设置为大于零的整数可以让我们看到在训练估计器时发生了什么。

在本节中，我们学习了随机森林在高级别上的工作原理，并查看了一些重要的超参数，以及它们如何影响模型性能的解释。我们还了解了主要超参数。此外，我们还了解了scikit-learn中的一些有用参数，这些参数可以简化训练过程。在下一节中，我们将讨论XGBoost算法。

# 探索XGBoost超参数

**极端梯度提升**（**XGBoost**）也是一个基于树的模型，它通过一系列决策树的集合构建，类似于随机森林。它也可以用于分类和回归任务。XGBoost与随机森林之间的区别在于它们如何进行集成。与使用袋装集成方法的随机森林不同，XGBoost使用另一种称为**提升**的集成方法。

提升是一种集成算法，其目标是通过一系列单独的弱模型，通过克服先前模型的弱点（参见*图11.1*）来提高性能。它不是一个特定的模型；它只是一个通用的集成算法。弱度的定义可能因不同的提升集成实现类型而异。在XGBoost中，它是基于先前决策树模型的梯度误差来定义的。请看以下图表：

![Figure 11.1 – Boosting ensemble algorithm]

![img/B18753_11_001.jpg]

图11.1 – 提升集成算法

XGBoost是一个非常流行且广泛采用的机器学习模型，它使用提升集成算法和一系列决策树构建。每个决策树都是逐个添加的，并拟合到前一个树的预测误差，以纠正这些误差。值得注意的是，由于XGBoost是梯度提升算法的一部分，所有弱模型（决策树）都需要使用可微分的损失函数和梯度下降优化方法进行拟合。

XGBoost有自己的包，不仅可以在Python中使用，还可以在其他编程语言中使用，如R和JVM。在Python中，您可以通过`pip install xgboost`安装XGBoost。此包还实现了scikit-learn包装器，用于回归（`XGBRegressor`）和分类（`XGBClassifier`）任务。该包提供了许多超参数，但并非所有参数都对模型性能有重大影响。以下是最重要的超参数，按其对模型性能影响的重要性从高到低排序：

1.  `n_estimators`：这指定了用于构建XGBoost模型要使用的决策树数量。它也可以解释为提升轮数，这与神经网络中的epoch概念相似。一般来说，值越高，模型的性能越好，但代价是计算时间会更长。然而，我们需要小心过高的值，因为它可能导致过拟合问题。

1.  `learning_rate`：这是梯度下降优化算法的学习率。值越低，模型找到最优解的机会越高，但代价是计算时间更长。如果你在训练的最后几次迭代中没有发现过拟合的迹象，你可以增加这个超参数的值；如果有过拟合，你可以减少它。

1.  `max_depth`：这是每个决策树的最大深度。较低的值可以帮助我们防止过拟合。然而，过低的值可能导致欠拟合问题。有一点可以肯定——较高的值会导致更长的计算时间。

1.  `min_child_weight`：这是在子节点中需要的、使用 Hessian 计算的实例权重最小总和。这个超参数作为正则化器，确保每个树在达到一定程度的纯度后停止尝试分裂节点。换句话说，它是一个通过限制树深度来防止过拟合的正则化参数。较高的值可以帮助我们防止过拟合。然而，过高的值可能导致欠拟合问题。

1.  `gamma`：这是一个基于损失值减少的伪正则化参数。这个超参数的值指定了在树的叶子节点上进行进一步划分所需的最小损失减少量。你可以给这个超参数一个较高的值来防止过拟合问题。然而，请小心，不要使用过高的值；它可能导致欠拟合问题。

1.  `colsample_bytree`：这是 scikit-learn 实现的随机森林中 `max_features` 超参数的分数版本。这个超参数负责告诉 XGBoost 在每个决策树中选择最佳分裂点需要多少随机采样的特征。较低的值可以帮助我们防止过拟合并降低计算时间。然而，过低的值可能导致欠拟合问题。

1.  `subsample`：这是 `colsample_bytree` 超参数的观察版本。这个超参数负责告诉 XGBoost 在训练每个树时需要使用多少训练样本。这个超参数可以用来防止过拟合问题。然而，如果我们使用过低的值，它也可能导致欠拟合问题。

XGBoost 超参数完整列表

如需了解 XGBoost 其他超参数的更多信息，请访问官方文档页面：https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn。

在本节中，我们讨论了 XGBoost 的高层次工作原理，并查看了一些重要的超参数，以及它们如何影响模型性能的解释。我们还看了主要超参数。在下一节中，我们将讨论 LightGBM 算法。

# 探索 LightGBM 超参数

**轻量梯度提升机**（**LightGBM**）也是一种基于决策树集合的增强算法，类似于XGBoost。它既可以用于分类任务，也可以用于回归任务。然而，它在树的生长方式上与XGBoost不同。在LightGBM中，树是以叶节点的顺序生长的，而XGBoost是以层级的顺序生长的（见*图11.2*）。我们所说的“叶节点优先”是指LightGBM通过优先增长那些分裂导致同质性增加最大的节点来生长树：

![Figure 11.2 – Level-wise versus leaf-wise tree growth

![img/B18753_11_002.jpg]

图11.2 – Level-wise versus leaf-wise tree growth

除了XGBoost和LightGBM在树的生长方式上的不同之外，它们在处理分类特征方面也有不同的方法。在XGBoost中，我们需要在将特征传递给模型之前对分类特征进行编码。这通常是通过使用独热编码或整数编码方法来完成的。在LightGBM中，我们只需告诉哪些特征是分类的，它就会通过执行等值分裂自动处理这些特征。在分布式学习中进行优化时，XGBoost和LightGBM在执行优化方面还有其他几个不同之处。总的来说，与XGBoost相比，LightGBM的计算时间要快得多。

与XGBoost类似，LightGBM也有自己的包，不仅可以在Python中使用，也可以在R语言中使用。在Python中，您可以通过`pip install lightgbm`来安装LightGBM。此包还实现了scikit-learn包装器，用于回归（`LGBMRegressor`）和分类（`LGBMClassifier`）任务。以下是LightGBM最重要的超参数，从对模型性能影响最大到最小排序：

1.  `max_depth`：这指定了每个决策树的最大深度。较低的值可以帮助我们防止过拟合。然而，过低的值可能导致欠拟合问题。有一点可以肯定——较高的值意味着更长的计算时间。

1.  `num_leaves`：这指定了每棵树的最大叶节点数。它的值应该小于`max_depth`的2的幂，因为对于固定数量的叶节点，叶节点优先的树比深度优先的树要深得多。一般来说，值越高，模型的性能越好，但这也意味着更长的计算时间。然而，存在一个阈值，增加更多叶节点对模型性能的影响不会很大，甚至可能由于过拟合而产生负面影响。

1.  `Learning_rate`：这指定了梯度下降优化算法的学习率。值越低，模型找到更优解的可能性越高，但这也意味着更长的计算时间。如果在训练的最后几次迭代中没有发现过拟合的迹象，则可以增加此超参数的值，反之亦然。

1.  `min_child_samples`：这指定了叶节点中所需的最小样本数。较高的值可以帮助我们防止过拟合。然而，一个过高的值可能导致欠拟合问题。

1.  `特征分数`：这与XGBoost中的`colsample_bytree`类似。这个超参数告诉LightGBM在每棵决策树中选择最佳分割点时需要使用多少随机采样的特征。这个超参数对于防止过拟合可能很有用。然而，如果我们使用一个过低的值，也可能导致欠拟合问题。

1.  `bagging_fraction`：这是`feature_fraction`超参数的观测版本。这个超参数负责告诉LightGBM在每棵树的训练过程中需要使用多少训练样本。较低的值可以帮助我们防止过拟合并降低计算时间。然而，一个过低的值可能导致欠拟合问题。

LightGBM超参数完整列表

想了解更多关于其他LightGBM超参数的信息，请访问官方文档页面：[https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api](https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api)。

在本节中，我们讨论了LightGBM在高级别的工作方式，并查看了一些重要的超参数，以及它们如何影响模型性能的解释。我们还看了主要超参数。在下一节中，我们将讨论CatBoost算法。

# 探索CatBoost超参数

**分类提升**（**CatBoost**）是另一种基于决策树集合的增强算法，类似于XGBoost和LightGBM。它也可以用于分类和回归任务。CatBoost与XGBoost或LightGBM的主要区别在于其生长树的方式。在XGBoost和LightGBM中，树是非对称生长的，而在CatBoost中，树是对称生长的，使得所有树都是平衡的。这种平衡树特性提供了几个好处，包括控制过拟合问题的能力、降低推理时间和在CPU上的高效实现。CatBoost通过在每个节点的每个分割中使用相同的条件来实现这一点，如下面的图所示：

![图11.3 – 非对称与对称树

![img/B18753_11_003.jpg](img/B18753_11_003.jpg)

图11.3 – 非对称与对称树

CatBoost的主要卖点是其自动处理多种类型特征的能力，包括数值、分类和文本，特别是对于分类特征。我们只需通过`cat_features`参数告诉CatBoost哪些特征是分类特征，它就会自动处理这些特征。默认情况下，CatBoost会对只有两个类别的分类特征执行独热编码。对于更高基数特征，它将执行目标编码，并组合几个分类特征，甚至分类和数值特征。有关CatBoost如何处理分类特征的更多信息，请参阅官方文档页面：[https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic)。

与XGBoost和LightGBM类似，CatBoost也有自己的包，不仅可以在Python中使用，也可以在R语言中使用。在Python中，你可以通过`pip install catboost`安装CatBoost。你可以利用实现的scikit-learn兼容类来处理回归（`CatBoostRegressor`）和分类（`CatBoostClassifier`）任务。以下是根据每个超参数对模型性能的重要性按降序排列的CatBoost最重要的超参数列表：

1.  `iterations`: 这指定了用于构建CatBoost模型的决定树的数量。它也可以解释为提升轮数，类似于神经网络中的epoch概念。一般来说，值越高，模型的性能越好，但代价是计算时间更长。然而，存在一个阈值，增加更多树对模型性能的影响不会太大，甚至可能由于过拟合而产生负面影响。

1.  `depth`: 这指定了每个决策树的最大深度。一个较低的值可以帮助我们防止过拟合。然而，一个过低的值可能导致欠拟合问题。有一点可以肯定——较高的值意味着更长的计算时间。

1.  `learning_rate`: 这指定了梯度下降优化算法的学习率。值越低，模型找到更优解的机会越高，但代价是计算时间更长。如果在训练的最后几次迭代中没有发现过拟合的迹象，你可以增加这个超参数的值，反之亦然。

1.  `l2_leaf_reg`: 这是在成本函数上的正则化参数。这个超参数可以防止过拟合问题。然而，如果我们使用一个过高的值，它也可能导致欠拟合问题。

1.  `one_hot_max_size`：这是CatBoost决定何时对分类特征进行one-hot编码的阈值。任何基数小于或等于给定值的分类特征将通过one-hot编码方法转换为数值。

CatBoost超参数完整列表

关于其他CatBoost超参数的更多信息，请访问官方文档页面([https://catboost.ai/en/docs/concepts/parameter-tuning](https://catboost.ai/en/docs/concepts/parameter-tuning))。

在本节中，我们讨论了CatBoost在高级别的工作原理，并查看了一些重要的超参数，以及它们如何影响模型性能的解释。我们还看了主要超参数。在下一节中，我们将讨论SVM算法。

# 探索SVM超参数

**支持向量机**(**SVM**)是一种机器学习模型，它利用线或超平面，以及一些线性代数变换，来执行分类或回归任务。前几节中讨论的所有算法都可以归类为基于树的算法，而SVM不属于*基于树*的机器学习算法组。它是*基于距离*的算法组的一部分。我们通常称SVM中的线性代数变换为**内核**。这负责将任何问题转化为线性问题。

Python中最受欢迎和最维护良好的SVM实现可以在scikit-learn包中找到。它包括回归(`SVR`)和分类(`SVC`)任务的实现。它们都有非常相似的超参数，只有一些小的差异。以下是最重要的SVM超参数，从对模型性能影响最大到最小排序：

1.  `kernel`：这是线性代数变换，其目标是把给定问题转化为线性问题。我们可以从五个内核中选择，包括线性(`linear`)、多项式(`poly`)、径向基函数(`rbf`)和sigmoid(`sigmoid`)内核。不同的内核将对模型性能产生不同的影响，对于这个超参数没有明确的经验法则。

1.  `C`：这是控制过拟合的正则化参数。值越低，正则化对模型的影响越强，因此预防过拟合的可能性越高。

1.  `degree`：这个超参数是特定于多项式内核函数的。这个超参数的值对应于模型所使用的多项式函数的次数。

1.  `gamma`：这是径向基、多项式和sigmoid内核函数的系数。scikit-learn提供了两种选项，即`scale`和`auto`。

scikit-learn中的SVM超参数

如需了解SVM中每个超参数在scikit-learn中的实现方式，您可以访问官方文档页面：https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html 和 [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)。

在本节中，我们讨论了SVM在高级别上是如何工作的，并查看了一些重要的超参数，以及它们如何影响模型性能的解释。我们还探讨了主要超参数。在下一节中，我们将讨论人工神经网络。

# 探索人工神经网络超参数

**人工神经网络**，也称为**深度学习**，是一种模仿人类大脑工作方式的机器学习算法。深度学习可以用于回归和分类任务。该模型的主要卖点之一是其能够从原始数据中自动执行特征工程和选择。一般来说，为了确保此算法表现良好，我们需要向模型提供大量训练数据。神经网络的最简单形式称为**感知器**（见*图11.4*）。感知器只是在所有特征上应用的一个线性组合，并在计算的末尾添加一个偏差：

![图11.4 – 感知器]

![img/B18753_11_004.jpg]

图11.4 – 感知器

如果将感知器的输出传递给一个非线性函数，这通常被称为**激活函数**，然后再传递给另一个感知器，那么我们可以称之为具有一层**多层感知器**（**MLP**）。神经网络的训练过程包括两个主要步骤，即**前向传播**和**反向传播**。在前向传播中，我们只是让神经网络根据定义的架构在给定的输入上执行计算。在反向传播中，模型将根据定义的损失函数使用基于梯度的优化过程更新权重和偏差参数。

除了MLP之外，还有其他类型的神经网络，例如**卷积神经网络**（**CNNs**）、**长短期记忆网络**（**LSTMs**）、**循环神经网络**（**RNNs**）和**变换器**。当我们处理图像数据时，通常会采用CNN，但当我们处理文本数据时，也可以使用一维CNN。RNNs和LSTMs通常用于处理时间序列或自然语言数据。变换器主要用于与文本相关的项目，但最近，它们也被用于图像和声音数据。

几个包提供了在Python中实现神经网络的实现，包括PyTorch、TensorFlow和Scikit Learn。以下是最重要的超参数，根据每个超参数对模型性能的重要性按降序排列。请注意，这个优先级列表是基于我们过去开发随机森林模型的经验而主观制定的。由于不同包中超参数的命名可能不同，我们将只使用超参数的一般名称：

1.  **优化器**：这是将要使用的基于梯度的优化算法。我们有几个优化器可供选择。然而，可能最流行和最广泛采用的优化器是**Adam**。还有其他选项，包括但不限于SGD和RMSProp。不同的优化器可能对模型性能有不同的影响，并且没有明确的经验法则来选择哪个是最好的。值得注意的是，每个优化器都有自己的超参数。

1.  **学习率**：这个超参数控制优化器在优化过程中从给定的训练数据中“学习”时步的大小。在选择其他超参数之前，首先选择最佳的学习率范围非常重要。值越低，模型找到更优解的可能性越高，但会以更长的计算时间为代价。如果在训练的最后几次迭代中没有发现过拟合的迹象，则可以增加这个超参数的值，反之亦然。

1.  **批量大小**：这指定了每个训练步骤中将传递给神经网络的训练样本数量。一般来说，值越高，模型的性能越好。然而，过大的批量大小通常会受到设备内存的限制。

1.  在XGBoost中的`n_estimators`和CatBoost中的`iterations`，高值可以导致更好的模型性能，但会以更长的计算时间为代价。然而，我们需要小心使用过高的值，因为它可能导致过拟合。

1.  **层数数量**：值越高，模型的复杂性越高，因此过拟合的可能性也越高。通常，一或两层就足以构建一个好的模型。

1.  **节点数量**：每个层中单元或节点的数量。值越高，模型的复杂性越高，因此过拟合的可能性也越高。

1.  **激活函数**：非线性变换函数。有多个激活函数可供选择。实践中最广泛采用的激活函数包括**修正线性激活函数**（**ReLU**）、**指数线性单元**（**ELU**）、**Sigmoid**、**Softmax**和**Tanh**。

1.  **Dropout 率**：dropout 层的比率。dropout 层是神经网络中的一个特殊层，通过随机将单元值设为零来充当正则化器。这个超参数控制了多少个单元被设为零。较高的值可以帮助我们防止过拟合。然而，过高的值可能导致欠拟合问题。

1.  **L1/L2 正则化**：这些是应用于损失函数的正则化参数。这个超参数可以帮助防止过拟合。然而，如果其值过高，也可能导致欠拟合问题。

在本节中，我们讨论了神经网络在高级层面上的工作原理，神经网络的变体，并查看了一些重要的超参数，以及它们如何影响模型性能的解释。我们还探讨了主要超参数。现在，让我们总结本章内容。

# 摘要

在本章中，我们讨论了几个流行的算法在高级层面上的工作原理，解释了它们的重要超参数以及它们如何影响性能，并提供了按性能影响降序排列的超参数优先级列表。此时，你应该能够通过关注最重要的超参数来更有效地设计你的超参数调整实验。你也应该了解每个重要超参数对模型性能的影响。

在下一章中，我们将把这里讨论的超参数调整方法总结成一个简单的决策图，帮助你选择最适合你问题的方法。此外，我们还将涵盖几个案例研究，展示如何在实际中利用这个决策图。
