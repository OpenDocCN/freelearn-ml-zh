- en: Choosing the Best Algorithm for Your Application
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为您的应用程序选择最佳算法
- en: 'There are three distinct phases in the software-engineering process: conception,
    implementation, and deployment. This book has primarily focused on the implementation
    phase of the process, which is when a software engineer develops the core functionality
    (that is, a **machine learning** (**ML**) algorithm) and features of the project.
    In the last chapter, we discussed matters concerning the deployment phase. Our
    learning is nearly complete.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程过程有三个不同的阶段：构思、实施和部署。本书主要关注过程的实施阶段，这是软件工程师开发项目核心功能（即机器学习算法）和特性的阶段。在最后一章，我们讨论了与部署阶段相关的问题。我们的学习几乎已经完成。
- en: In this final chapter, we'll turn to the conception phase in order to round
    out our understanding of the full ML development process. Specifically, we'll
    discuss how to choose the best algorithm for a given problem. The ML ecosystem
    is evolving, intimidating, and full of jargon unfamiliar even to experienced software
    developers. I often see students of ML get stuck at the beginning of the process,
    not knowing where to start in a vast and unfamiliar landscape. What they don't
    know yet is that once you get past the initial hurdle of choosing an algorithm
    and decrypting the jargon, the rest of the journey is much easier.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章的最后，我们将转向构思阶段，以完善我们对整个机器学习开发过程的理解。具体来说，我们将讨论如何为给定问题选择最佳算法。机器学习生态系统正在演变，令人畏惧，充满了连经验丰富的软件开发者都感到陌生的术语。我经常看到机器学习的初学者在过程的开始阶段陷入困境，不知道在广阔而陌生的领域中从何入手。他们还没有意识到，一旦你克服了选择算法和解读术语的初步障碍，剩下的旅程就会容易得多。
- en: The goal of this chapter is to provide a compass, a simple guide one can use
    to find a way around the landscape. It's not always easy to select the right algorithm,
    but sometimes it is. The first sections of this chapter will teach you four simple
    decision points—essentially four multiple-choice questions—that you can use to
    focus in on the most suitable algorithms for your project. Most of the time, you'll
    end up with only one or two algorithms to choose from after going through this
    process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是提供一个指南针，一个简单的指南，你可以用它来在领域中找到自己的道路。选择正确的算法并不总是容易，但有时也是可能的。本章的前几节将教你四个简单的决策点——本质上就是四个多项选择题——你可以使用这些决策点来专注于最适合你项目的算法。在经过这个过程后，大多数情况下，你将只剩下一个或两个算法可供选择。
- en: We'll then continue our education by discussing other topics related to planning
    an ML system. We'll discuss the telltale signs that you've chosen the wrong algorithm
    so that you can identify mistakes early on. You'll also learn how to tell the
    difference between using the wrong algorithm versus a bad implementation of one.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将继续通过讨论与规划机器学习系统相关的其他主题来继续我们的教育。我们将讨论你选择了错误算法的明显迹象，这样你就可以尽早识别错误。你还将学习如何区分使用错误的算法与一个糟糕的实现之间的区别。
- en: I'll also show you an example of combining two different ML models, so that
    you can compose larger systems out of individual models that are best suited to
    their own tasks. This approach can yield great results if designed carefully.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我还会给你展示如何结合两种不同的机器学习模型（ML models）的例子，这样你就可以从适合各自任务的独立模型中组合出更大的系统。如果设计得当，这种方法可以产生非常好的结果。
- en: I called this chapter a compass—not a map—for a reason. It is not a comprehensive
    guide that covers every ML algorithm known to computer scientists. As with a compass,
    you must also use your guile and skills to find your way. Use this chapter to
    find your own project's starting point, and then follow up with your own research.
    While the 20 or so algorithms and techniques we've discussed in this book give
    you a pretty wide view of the landscape, they're only a fraction of the ecosystem.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我之所以称这一章为指南针而不是地图，是有原因的。它不是一本涵盖计算机科学家所知的所有机器学习算法的全面指南。就像指南针一样，你也必须运用你的智慧和技能来找到自己的道路。使用本章来找到你自己的项目的起点，然后继续进行你自己的研究。虽然本书中讨论的20多种算法和技术为你提供了对整个领域的广泛视角，但它们只是生态系统的一小部分。
- en: As we come to the beginning of the end of this book, I'd like to give you one
    final piece of advice. To become an expert at something requires a consistent
    dedication to *both *practice and play. If you want to become a world-class pianist,
    you must spend countless hours doing meticulous rote practice with a metronome,
    practicing fingering exercises and learning challenging *études*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们来到这本书的结尾时，我想给你一条最后的建议。要成为某个领域的专家，需要持续致力于**练习和玩耍**。如果你想成为一名世界级的钢琴家，你必须花无数个小时用节拍器进行细致的死记硬背练习，练习指法练习，学习有挑战性的**练习曲**。
- en: But you also have to *play*, which is where exploration happens and creativity
    is developed. After thirty minutes of running drills, a pianist might spend thirty
    minutes improvising jazz and experimenting with melody and counterpoint, learning
    the *je ne sais quoi* or the emotive essence of the scales and patterns in the
    music. That playful exploration, the experimentation involved in creativity, develops
    the intuitive sense of music in a way that rote practice does not. Rote practice—meticulous
    work and study—in turn develops the mechanical sense and skill that play cannot.
    Practice and play elevate each other in a virtuous cycle. The skilled are able
    to explore farther and deeper than the unskilled, and the excitement of what lies
    even deeper still is what motivates the practice that develops the skill. Time
    and patience, practice and play, motivation and discipline are the only things
    you need to go from being a novice to an expert.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但你还得去“玩”，这是探索发生和创造力发展的地方。经过三十分钟的练习后，钢琴家可能会花三十分钟即兴创作爵士乐，尝试旋律和对位，学习音乐的“je ne sais
    quoi”或音阶和模式中的情感本质。这种玩耍的探索，创造力中的实验，以死记硬背所不具备的方式发展了音乐的直觉感。死记硬背——细致的工作和学习——反过来又发展了机械感和技能，这是玩耍所不能做到的。练习和玩耍在良性循环中相互提升。有技能的人能够比没有技能的人探索得更远、更深入，而更深层次的东西所带来的兴奋感正是推动技能发展的练习的动力。时间和耐心、练习和玩耍、动机和纪律是你从新手到专家所需要的一切。
- en: ML is the opposite of jazz piano, but the path to expertise is the same. The
    rote practice of ML—the equivalent of practicing scales—is building and implementing
    algorithms. I particularly recommend writing algorithms from scratch as practice;
    that's the only true way to understand what's really going on inside. Don't just
    write an algorithm from scratch once to prove to yourself that you can. Write
    the algorithm several times, in different environments, different programming
    languages, different architectures, with different datasets, and keep doing that
    until you can write nearly the whole thing off the top of your head. I'm fairly
    certain I can write a Naive Bayes classifier blindfolded in any of three programming
    languages, just as you would be able to after you've written dozens of them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与爵士钢琴相反，但成为专家的道路是相同的。机器学习的死记硬背——相当于练习音阶——是构建和实现算法。我特别推荐从头开始编写算法作为练习；这是真正理解内部发生情况的唯一方法。不要只写一次算法来证明自己可以做到。要在不同的环境中、不同的编程语言、不同的架构、不同的数据集上多次编写算法，并且一直这样做，直到你能够几乎从头到尾地写出整个算法。我相当确信我可以在任何三种编程语言中闭着眼睛编写一个朴素贝叶斯分类器，就像你在编写了数十个之后也能做到的那样。
- en: The play of ML lies in experimentation. This chapter is about choosing the best
    algorithm for your application, but it isn't rule of law. If you never experiment
    you'll never develop a rich intuition for the algorithms or data. Experiment with
    other approaches, parameters, or variations on algorithms and learn from the experimentation.
    You'll be surprised at how often an experiment can end up successful, but more
    importantly than that, experimentation should be part of your practice and education.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的乐趣在于实验。本章是关于为你的应用程序选择最佳算法，但这并不是法律规则。如果你从不实验，你就永远不会对算法或数据发展出丰富的直觉。尝试其他方法、参数或算法的变体，并从实验中学习。你会惊讶于实验成功有多频繁，但更重要的是，实验应该成为你的练习和教育的一部分。
- en: 'Let''s kick the chapter off by discussing the four major decision points you
    can use to hone your skills on an algorithm:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论四个主要决策点开始，这些决策点可以帮助你在算法上磨练技能：
- en: Mode of learning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习方式
- en: The task at hand
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前任务
- en: The format or form of the data
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的格式或形式
- en: Available resources
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用资源
- en: Also we'll discuss what to do when it all goes wrong, and finally we'll discuss
    combining multiple models together.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将讨论当一切出错时应该做什么，最后我们将讨论将多个模型组合在一起。
- en: Mode of learning
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习方式
- en: 'The first decision point to visit when choosing an ML algorithm is the mode
    of the learning process: supervised, unsupervised, or reinforcement learning.
    These modes have very little overlap; in general an algorithm is either supervised
    or unsupervised but not both. This narrows your choices down by roughly half,
    and fortunately it is very easy to tell which mode of learning applies to your
    problem.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 选择机器学习算法时，首先要考虑学习过程的模式：监督学习、无监督学习或强化学习。这些模式之间几乎没有重叠；一般来说，一个算法要么是监督学习，要么是无监督学习，但不会两者兼具。这大约将你的选择缩小了一半，幸运的是，判断哪种学习模式适用于你的问题非常容易。
- en: The difference between supervised and unsupervised learning is marked by whether
    or not you need labeled training examples to teach the algorithm. If all you have
    is data points, and not labels or categories to associate them with, then you
    are only able to perform unsupervised learning. You must therefore choose one
    of the unsupervised learning algorithms, such as k-means clustering, regressions,
    **Principal Component Analysis** (**PCA**), or singular value decomposition.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习之间的区别在于你是否需要标记的训练示例来教授算法。如果你只有数据点，而没有与之关联的标签或类别，那么你只能执行无监督学习。因此，你必须选择一个无监督学习算法，例如k-means聚类、回归、**主成分分析**（**PCA**）或奇异值分解。
- en: Another telltale difference between supervised and unsupervised learning is
    whether or not there is a way to judge semantic accuracy. If the concept of judging
    accuracy doesn't make sense in your application (because you don't have labeled
    training or reference data), then you are facing an unsupervised learning problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习之间的另一个明显区别是是否存在判断语义准确性的方法。如果你的应用中判断准确性的概念没有意义（因为你没有标记的训练数据或参考数据），那么你面临的是一个无监督学习问题。
- en: In some cases, however, you will not have training data but the problem is best
    solved by a supervised learning problem. It's important to recognize the difference
    between having training data and *needing *training data. When you need training
    data, you are likely looking at a supervised learning problem. If you need training
    data but don't have it, you'll have to figure out some way to get the training
    data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，你可能没有训练数据，但问题最好通过监督学习来解决。认识到拥有训练数据和需要训练数据之间的区别很重要。当你需要训练数据时，你很可能面对的是一个监督学习问题。如果你需要训练数据但没有，你必须想出某种方法来获取训练数据。
- en: The simplest way to generate training data for a problem is to generate it yourself.
    In an image classification task you can simply label a few hundred images by hand
    to generate your training data. This is time consuming but will work for small
    training sets.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为问题生成训练数据的最简单方法是自行生成。在图像分类任务中，你可以手动标记几百张图片来生成你的训练数据。这很耗时，但对于小规模训练集来说可行。
- en: A more scalable approach is to use a service like Amazon Mechanical Turk, through
    which you pay human workers $0.05-$0.10 to label each image. The Mechanical Turk
    approach has become very popular with ML researchers and data scientists, as it
    is a fast and scalable way to generate a large volume of training data at a reasonable
    cost. Generating labels for 5,000 images might cost $250 and take a day or two
    on Mechanical Turk. If you think $250 is expensive, consider the cost of the time
    it would take for you to label those 5,000 images personally.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更可扩展的方法是使用像Amazon Mechanical Turk这样的服务，通过这个服务，你可以支付给人工工作者每张图片0.05-0.10美元的标签费用。Mechanical
    Turk方法在机器学习研究人员和数据科学家中变得非常流行，因为它是一种快速且可扩展的方式，以合理的成本生成大量训练数据。在Mechanical Turk上为5,000张图片生成标签可能需要250美元，并且可能需要一两天的时间。如果你认为250美元很贵，考虑一下如果你亲自标记这5,000张图片需要花费多少时间。
- en: There are more clever ways to generate training data, such as shifting the responsibility
    to your application's users. Years ago, when Facebook first introduced the ability
    to tag people in photos, they required the photo's uploader to draw a box around
    each subject's face and tag them. After billions of photo uploads, Facebook has
    a huge training set that not just identifies facial forms but also specific people
    in photos. Nowadays, there is no need to draw boxes around peoples' faces when
    tagging photos, and typically Facebook is able to figure out who each subject
    in the photo is automatically. We, the users, provided them with this massive
    training set.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有更多巧妙的方法来生成训练数据，例如将责任转移到你的应用程序的用户身上。多年前，当Facebook首次引入在照片中标记人的功能时，他们要求照片的上传者围绕每个主题的脸部画一个框并标记他们。经过数十亿张照片的上传，Facebook拥有一个巨大的训练集，不仅能够识别面部形状，还能识别照片中的特定人物。如今，在标记照片时不再需要围绕人们的脸部画框，通常Facebook能够自动识别照片中的每个主题。我们，作为用户，为他们提供了这个庞大的训练集。
- en: Supervised learning is made apparent by the act of training an algorithm on
    prelabeled data, with the goal of algorithm learning from the labels, and by being
    able to extend that knowledge and apply it to new, unseen examples. If you find
    yourself in a situation where you have a lot of data points and some, but not
    all, have the correct answers or labels available, you likely want a supervised
    learning algorithm. If you have a million emails, 5,000 of which were manually
    filtered as spam or not spam, and if the goal is to extend that knowledge to the
    other 995,000 messages, you're looking for a supervised learning algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通过在预标注数据上训练算法来显现，其目标是算法从标签中学习，并且能够将这种知识扩展并应用于新的、未见过的例子。如果你发现自己处于一个有很多数据点，但其中只有一些，而不是全部都有正确答案或标签的情况，你可能需要一个监督学习算法。如果你有百万封电子邮件，其中5,000封被手动过滤为垃圾邮件或非垃圾邮件，并且如果目标是扩展这种知识到其他995,000条消息，那么你正在寻找一个监督学习算法。
- en: If you have a situation where you need to judge the semantic accuracy of the
    algorithm, you must also use a supervised learning algorithm. An unsupervised
    algorithm has no source of truth; those algorithms will cluster or smooth or extrapolate
    data, but without a canonical reference, there is no way to judge the algorithm's
    accuracy. A supervised learning algorithm, on the other hand, can be judged in
    terms of semantic accuracy as the prelabeled training data serves as the source
    of truth.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要判断算法的语义准确性，你也必须使用监督学习算法。无监督算法没有真相来源；这些算法将聚类、平滑或外推数据，但没有权威的参考，无法判断算法的准确性。另一方面，监督学习算法可以通过预标注的训练数据作为真相来源来判断其语义准确性。
- en: While we haven't covered reinforcement learning algorithms in this book, they
    are marked by situations where the algorithm must affect its environment in an
    attempt to optimize behavior. There is a degree of separation between the algorithm's
    output and the results of the action, since the environment itself is a factor.
    An example of reinforcement learning is teaching an AI to play a video game by
    scanning the screen and using mouse and keyboard controls. An AI playing *Super
    Mario Bros.* can only interact with the environment by hitting combinations of
    up, down, left, right, A, and B on the control pad. The algorithm's output takes
    an action in the environment, and the environment will either reward or punish
    those actions. The algorithm therefore tries to maximize its reward, for instance
    by collecting coins, making progress through the level, defeating enemies, and
    not falling into bottomless pits.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这本书中没有涵盖强化学习算法，但它们的特点是算法必须通过影响其环境来尝试优化行为。算法的输出与行动结果之间存在一定的距离，因为环境本身就是一个因素。强化学习的一个例子是教AI通过扫描屏幕并使用鼠标和键盘控制来玩电子游戏。玩《超级马里奥兄弟》的AI只能通过在控制板上按上、下、左、右、A和B的组合来与环境互动。算法的输出在环境中采取行动，环境将奖励或惩罚这些行动。因此，算法试图最大化其奖励，例如通过收集金币、通过关卡、击败敌人以及不落入无底洞。
- en: Reinforcement learning algorithms are a major topic in applied robotics, control
    system design, simulation-based optimization, hardware-in-the-loop simulation,
    and many other fields that blend the physical world with the algorithmic world.
    Reinforcement learning is most effective when the system you're studying—the environment—is
    a sophisticated black box that cannot be directly modeled. Reinforcement learning
    is used, for instance, to optimize control strategies for systems that will be
    used in arbitrary environments, such as a robot that must autonomously navigate
    unknown terrain.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法是应用机器人学、控制系统设计、基于模拟的优化、硬件在环模拟以及许多其他领域的重大主题，这些领域将物理世界与算法世界相结合。当您研究的系统——环境——是一个复杂的黑盒，无法直接建模时，强化学习最为有效。例如，强化学习被用于优化将在任意环境中使用的系统的控制策略，例如必须自主导航未知地形的机器人。
- en: Finally, there are tasks that require only the optimization of a system whose
    model is either known or directly observable. These are only tangentially considered
    ML problems, and are more appropriately called **optimization problems**. If you
    must choose the best driving route from Point A to Point B based on current traffic
    conditions, you can use a genetic algorithm, for instance. If you must optimize
    a small number of parameters that configure a different model, you might try a
    grid search. If you must determine the boundary conditions of a complex system,
    Monte Carlo methods might be able to help. If you must find the global optimum
    of a continuous system, then stochastic gradient descent might serve your purpose.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有一些任务只需要优化一个模型已知或可以直接观察的系统。这些只是与机器学习问题有间接关系，更合适地称为**优化问题**。如果您必须根据当前交通状况从A点选择最佳驾驶路线到B点，例如，您可以使用遗传算法。如果您必须优化配置不同模型的一小部分参数，您可能尝试网格搜索。如果您必须确定复杂系统的边界条件，蒙特卡洛方法可能有所帮助。如果您必须找到连续系统的全局最优解，那么随机梯度下降可能适合您的目的。
- en: Optimization algorithms are often used to solve problems with ML algorithms.
    Indeed, the backpropagation algorithm used to train ANNs uses gradient descent
    as its optimizer. The parameters given to k-means or other unsupervised algorithms
    can be tuned automatically through grid search in order to minimize variance.
    We have not discussed optimization algorithms in-depth in this book, but you should
    be aware of them and their use cases.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 优化算法通常用于解决机器学习算法的问题。确实，用于训练人工神经网络的反向传播算法使用梯度下降作为其优化器。k-means或其他无监督算法的参数可以通过网格搜索自动调整，以最小化方差。我们在这本书中没有深入讨论优化算法，但您应该了解它们及其用例。
- en: 'When choosing an algorithm for your application, start with the simplest question:
    Do I need supervised or unsupervised learning? Determine whether you have or can
    generate training data; if you cannot, you are forced to use unsupervised algorithms.
    Ask yourself whether you need to judge the accuracy of the algorithm''s output
    (supervised learning), or whether you are simply exploring data (unsupervised).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当为您的应用程序选择算法时，从最简单的问题开始：我需要监督学习还是无监督学习？确定您是否拥有或可以生成训练数据；如果您不能，您被迫使用无监督算法。问问自己是否需要判断算法输出的准确性（监督学习），或者您是否只是在探索数据（无监督）。
- en: After you've identified the mode of learning, which will cut your available
    choices roughly in half, you can further hone in on the algorithm you need by
    thinking about the specific task at hand, or the goal of your research.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在您确定了学习模式，这将大致将您的选择减半之后，您可以通过考虑具体任务或研究目标来进一步专注于您需要的算法。
- en: The task at hand
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当前任务
- en: The most effective way to partition the world of ML algorithms is to consider
    the task at hand, or the desired results and purpose of the algorithm. If you
    can identify the goal of your problem—that is, whether you need to predict continuous
    values based on inputs, categorize data, classify text, reduce dimensionality,
    and so on—you'll be able to reduce your choices to only a handful of algorithms.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最有效地划分机器学习算法的世界的方法是考虑当前任务，或者算法的预期结果和目的。如果您能确定您问题的目标——也就是说，您是否需要根据输入预测连续值，对数据进行分类，对文本进行分类，降低维度等——您就能将您的选择减少到只有几个算法。
- en: For example, in cases where you need to predict a continuous output value—such
    as a prediction for server load at a future date—you will likely need a regression
    algorithm. There are only a handful of regression algorithms to choose from, and
    the other decision points in this guide will help to reduce those options further.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在你需要预测一个连续输出值的情况下——例如预测未来某天的服务器负载——你可能会需要一个回归算法。可供选择的回归算法只有少数几个，本指南中的其他决策点将有助于进一步减少这些选项。
- en: In cases where you need to inspect data and identify data points that look similar
    to one another, a clustering algorithm would be the most appropriate. The specific
    clustering algorithm you choose will depend on the other decision points, such
    as the format or form of the data, the linearity or nonlinearity of the relationships,
    and the resources available to you (time, processing power, memory, and so on).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在你需要检查数据并识别彼此相似的数据点的情况下，聚类算法将是最合适的。你选择的特定聚类算法将取决于其他决策点，例如数据的格式或形式、关系的线性或非线性，以及你拥有的资源（时间、处理能力、内存等）。
- en: If the purpose of your algorithm is to categorize a data point with one of a
    dozen possible labels, you must choose from one of a handful of classification
    algorithms. Again, selecting the correct algorithm from the family of classification
    algorithms available to you will depend on the form of the data, your requirements
    for accuracy and any resource limitations imposed upon you.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的算法目的是将数据点分类为一打可能的标签之一，你必须从几个分类算法中选择一个。再次强调，从你可用的分类算法家族中选择正确的算法将取决于数据的形式、你对准确度的要求以及施加在你身上的任何资源限制。
- en: One common issue novice researchers face at this stage is a lack of clarity
    of the actual goal of the project versus the capabilities of individual algorithms.
    Sometimes, the business goal of a problem is abstract and only partially defined.
    It is often the case in those situations that the business goal is only achievable
    through the use of several individual algorithms. A student of ML may have difficulty
    identifying the concrete technical steps that must be composed in order to achieve
    the goal.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 初学者在这个阶段面临的一个常见问题是项目实际目标与单个算法能力之间的不明确性。有时，问题的业务目标是抽象的，只部分定义。在这些情况下，业务目标通常只能通过使用几个单独的算法来实现。机器学习的学生可能难以确定为了实现目标必须组合的具体技术步骤。
- en: An illustrative example is the business goal of writing an application that
    analyzes an image and returns a natural language description of the image's contents.
    For instance, when uploading a picture of a path through a park, the goal might
    be to return the text *a park bench and trash pail on a path with trees in the
    background*. It would be easy to fixate on the singular business goal of the project
    and assume that a single business goal maps to a single algorithm.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个说明性的例子是编写一个分析图像并返回图像内容自然语言描述的应用程序的业务目标。例如，当上传一张公园小径的照片时，目标可能是返回文本“一条小径上有一个公园长椅和垃圾桶，背景有树木”。人们很容易专注于项目的单一业务目标，并假设一个业务目标对应一个算法。
- en: However, this example requires at least two or three ML algorithms. First, a
    **Convolutional Neural Network** (**CNN**) must be able to identify objects in
    an image. Another algorithm must then be able to determine the spatial relationships
    between the objects. Finally, an NLP or ML algorithm must be able to take the
    output of the first two algorithms and compose a natural language representation
    from that information.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个例子至少需要两个或三个机器学习算法。首先，一个**卷积神经网络**（**CNN**）必须能够识别图像中的对象。然后，另一个算法必须能够确定对象之间的空间关系。最后，一个自然语言处理或机器学习算法必须能够从前两个算法的输出中提取信息，并从该信息中构建自然语言表示。
- en: The ability to understand a business goal and translate it into concrete technical
    steps takes time and experience to develop. You must be able to parse the business
    goal and work backwards in order to decompose it into individual subtasks. Once
    you've identified the subtasks, determining which algorithm best fits each subtask
    becomes a straightforward exercise in this decision-making process. We'll discuss
    the topic of composing algorithms shortly, but for now the important take-away
    is that some business goals will require multiple ML algorithms.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 能够理解业务目标并将其转化为具体的技术步骤需要时间和经验来培养。你必须能够解析业务目标，并反向工作以将其分解为单个子任务。一旦确定了子任务，确定哪个算法最适合每个子任务就成为了这个决策过程中的一个简单练习。我们很快就会讨论算法组合的话题，但到目前为止，重要的启示是，某些业务目标可能需要多个机器学习算法。
- en: In some cases, the task at hand will narrow your choices down to just a single
    algorithm. Object detection in images, for instance, is best achieved by a CNN.
    There are, of course, many different specialized subtypes of CNNs that can perform
    object detection (RCNN, Fast RCNN, Mask RCNN, and so on), but in this case we
    are able to narrow the playing field down to just CNNs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，手头的任务将你的选择缩小到只有一个算法。例如，图像中的目标检测最好通过卷积神经网络（CNN）来实现。当然，有许多不同的专门子类型的CNN可以执行目标检测（如RCNN、Fast
    RCNN、Mask RCNN等），但在这个案例中，我们能够将竞争范围缩小到仅CNN。
- en: In other cases, the task at hand can be achieved with several or many algorithms,
    in which case you must use additional decision points in order to choose the best
    one for your application. Sentiment analysis, for instance, can be achieved with
    many algorithms. Naive Bayes classifiers, maximum entropy models, random forests,
    and ANNs (particularly RNNs) can all solve the sentiment analysis problem.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，手头的任务可以通过几个或许多算法来完成，在这种情况下，你必须使用额外的决策点来选择最适合你应用的最佳算法。例如，情感分析可以通过许多算法来实现。朴素贝叶斯分类器、最大熵模型、随机森林和人工神经网络（尤其是循环神经网络）都可以解决情感分析问题。
- en: You may also be required to compose multiple algorithms in order to achieve
    the best accuracy for your sentiment analyzer. Therefore, the decision of which
    approach to use will depend not only on the task at hand, but also the form and
    format of the data used by the other algorithms in your composition. Not every
    algorithm is compatible for use in composition with all other algorithms, so the
    form and format decision point is effectively recursive, and you will need to
    apply it to each of the subtasks that you have identified in pursuit of your business
    goal.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还需要组合多个算法以实现情感分析器最佳精度。因此，使用哪种方法的决策不仅取决于手头的任务，还取决于你组合中使用的其他算法的数据的形式和格式。并非每个算法都适用于与其他所有算法组合使用，因此形式和格式决策点实际上是递归的，你需要将其应用于你为追求业务目标而确定的每个子任务。
- en: Format, form, input, and output
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 格式、形式、输入和输出
- en: What I've been describing as the format and form of data encapsulates several
    concepts. Most superficially, the *format* of the data relates to the specific
    data types (for example, integers, continuous numbers/floats, text, and discrete
    categories) of both the input and output of the application. The *form* of the
    data encapsulates the relationships between the data structures and the overall
    shape of the problem or solution space. These factors can help you select the
    appropriate algorithm even when the task at hand has given you several choices
    for an algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我所描述的数据的格式和形式包含几个概念。最表面地，数据的*格式*与输入和输出的具体数据类型（例如，整数、连续数字/浮点数、文本和离散类别）有关。数据的*形式*封装了数据结构之间的关系以及问题或解决方案空间的整体形状。这些因素可以帮助你在手头的任务提供了多个算法选择的情况下选择合适的算法。
- en: When dealing with text (the format), for instance, you must consider how the
    text is treated in terms of its relationship to the problem space and the task
    at hand; I call this the form of the data. When filtering spam it is generally
    not necessary to map the relationships between individual words. When analyzing
    text for sentiment it may indeed be necessary to map some relationships between
    words (for example, to deal with negations or other language modifiers), and this
    will require an additional level of dimensionality. When parsing text in order
    to build a knowledge graph you will need to map the relationships between not
    only the individual words but their conceptual meanings meticulously, requiring
    an even higher level of dimensionality.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理文本（格式）时，例如，你必须考虑文本在问题空间和当前任务中的处理方式；我称之为数据的形式。在过滤垃圾邮件时，通常没有必要映射单个单词之间的关系。在分析文本以进行情感分析时，确实可能需要映射一些单词之间的关系（例如，处理否定或其他语言修饰语），这将需要额外的维度。在解析文本以构建知识图谱时，你需要细致地映射不仅单个单词之间的关系，还有它们的概念意义之间的关系，这需要更高的维度。
- en: In these examples, the format of the data is the same—all three examples deal
    with text—but the form is different. The shape of the problem space is different.
    The spam filter has a simpler problem space with lower dimensionality and linearly
    separable relationships; each word is treated independently. The sentiment analyzer
    has a different form, requiring some additional dimensionality in the problem
    space in order to encode the relationships between some words, but not necessarily
    all relationships between all words. The knowledge graph problem requires a highly
    dimensional space into which the complex semantic and spacial relationships between
    words must be mapped.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些例子中，数据格式是相同的——所有三个例子都处理文本，但形式不同。问题空间的结构不同。垃圾邮件过滤器有一个更简单的问题空间，具有较低维度和线性可分的关系；每个单词都是独立处理的。情感分析器具有不同的形式，需要问题空间中的一些额外维度来编码某些单词之间的关系，但并不一定需要所有单词之间所有关系。知识图谱问题需要一个高度维度的空间，将单词之间复杂的语义和空间关系映射进去。
- en: 'In these text analysis cases, you can often ask yourself a series of simple
    questions that helps you narrow in on the correct algorithm: can each word be
    treated independently? Do we need to consider words being modified by other words
    (*like* versus *not like*, or *good* versus *very good*)? Do we need to maintain
    relationships between words separated by large distances (for example, a Wikipedia
    article that introduces a subject in the first paragraph but continues to refer
    to the subject many paragraphs later)?'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些文本分析案例中，你常常可以问自己一系列简单的问题，这些问题有助于你缩小到正确的算法：每个单词是否可以独立处理？我们需要考虑单词是否被其他单词修改（例如*不像*与*不像*，或*好*与*非常好*）吗？我们需要维护相隔很远的单词之间的关系（例如，一篇在第一段介绍主题但在许多段落之后继续提及该主题的维基百科文章）？
- en: Each relationship you keep track of adds a level of dimensionality to the problem
    space, so you must choose an algorithm that can work effectively in the dimensionality
    of the problem. Using a low-dimensionality algorithm such as naive Bayes on a
    high-dimensional problem such as a knowledge graph problem would not yield good
    results; this is akin to a person who lives their whole life in three dimensions
    trying to visualize the ten-dimensional space of superstring theory in physics.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你跟踪的每一项关系都会给问题空间增加一个维度，因此你必须选择一个能够在问题空间维度上有效工作的算法。在处理像知识图谱问题这样的高维问题时使用低维度的算法，例如朴素贝叶斯，是不会得到好结果的；这就像一个人一生都在三维空间中生活，却试图可视化物理中的十维超弦理论空间。
- en: 'Conversely, a highly dimensional algorithm such as a **Long Short-Term Memory**
    (**LSTM**) RNN can solve low-dimensional problems such as spam filtering, but
    it comes with a cost: the time and resources required to train a highly dimensional
    algorithm. They are incongruous with the difficulty of the problem. A Bayesian
    classifier can be trained on millions of documents in dozens of seconds, but an
    LSTM RNN might require hours of training for the same task and be slower to evaluate
    a data point by an order of magnitude. Even then, there is no guarantee that the
    LSTM RNN will outperform the Bayesian classifier in terms of accuracy.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，一个高度维度的算法，如**长短期记忆**（**LSTM**）RNN，可以解决低维问题，如垃圾邮件过滤，但它有一个代价：训练高度维度算法所需的时间和资源。它们与问题的难度不相称。贝叶斯分类器可以在几十秒内训练数百万份文档，但LSTM
    RNN可能需要数小时来训练同样的任务，并且评估数据点的速度慢一个数量级。即使如此，也不能保证LSTM RNN在准确性方面优于贝叶斯分类器。
- en: You must also consider the form and dimensionality when working with numerical
    data. Compared to statistical analysis, time-series analysis requires an additional
    dimension in order to capture the ordinality of the data in addition to the value
    of the data. This is analogous to the difference between a bag-of-words text algorithm
    such as Naive Bayes versus one that preserves sequential ordering of text such
    as an LSTM RNN.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你还必须考虑形式和维度性，当处理数值数据时。与统计分析相比，时间序列分析需要额外的维度来捕捉数据的顺序性，除了数据的值。这类似于词袋模型文本算法（如朴素贝叶斯）与保留文本顺序的算法（如LSTM
    RNN）之间的区别。
- en: Finally, the structural format of the data might be a consideration, though
    formats can often be successfully converted into more amenable formats. In [Chapter
    10](0f60907a-63b3-43ce-b4dd-13d9755b09d5.xhtml), *Natural Language Processing
    in Practice*, we discussed the Word2vec word-embedding algorithm which converts
    text into numerical vectors so that they can be used as the inputs to a neural
    network (which requires numerical inputs). Format conversion actually makes our
    decision-making process more difficult, as it allows us to choose from a wider
    array of algorithms. Using Word2vec means that we can use text as an input to
    algorithms that don't normally accept text, therefore giving us many more options
    to choose from.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据的结构格式可能也是一个需要考虑的因素，尽管格式通常可以成功转换为更易于处理的格式。在[第10章](0f60907a-63b3-43ce-b4dd-13d9755b09d5.xhtml)，《实践中的自然语言处理》中，我们讨论了Word2vec词嵌入算法，该算法将文本转换为数值向量，以便它们可以作为神经网络（需要数值输入）的输入。格式转换实际上使我们的决策过程更加困难，因为它允许我们从更广泛的算法中选择。使用Word2vec意味着我们可以将文本作为输入提供给通常不接受文本的算法，因此给我们提供了更多的选择。
- en: 'Another common format conversion is the quantization or bucketing of continuous
    numerical values. For instance, a continuous numerical value ranging from 0–10
    could be quantized into three buckets: small, medium, and large. This conversion
    allows us to use our continuous-value data in algorithms that only deal with discrete
    values or categories.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的格式转换是对连续数值的量化或分桶。例如，一个从0到10的连续数值可以量化为三个桶：小、中、大。这种转换允许我们在只处理离散值或类别的算法中使用我们的连续值数据。
- en: You should also consider the form and format of the output that you require
    from an algorithm. In a classification task the nominal output of the classifier
    will be a discrete label that describes the input. But not all classifiers are
    created equally. A decision tree classifier will yield one label as its output,
    whereas a Bayesian classifier will yield the probabilities of all possible labels
    as its output. In both cases the nominal output is a probable label, but the Bayesian
    classifier also returns the confidence of its guess along with a probability distribution
    over all possible guesses. In some tasks, the only thing you need from the algorithm
    is a single label; in other cases, the probability distribution is useful or even
    required.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该考虑从算法中获取的输出形式和格式。在分类任务中，分类器的名义输出将是一个离散标签，用于描述输入。但并非所有分类器都是同等创建的。决策树分类器将输出一个标签作为其输出，而贝叶斯分类器将输出所有可能标签的概率作为其输出。在这两种情况下，名义输出是一个可能的标签，但贝叶斯分类器还会返回其猜测的置信度以及所有可能猜测的概率分布。在某些任务中，你从算法中需要的可能只是一个单一标签；在其他情况下，概率分布可能是有用的，甚至可能是必需的。
- en: The form of the output of an algorithm is closely related to the mathematical
    mechanism of the model. This means that, even without a deep understanding of
    the algorithm itself, you can evaluate the mathematical properties of a model
    by looking at the form of its output. If a classifier returns probabilities as
    part of its output, it is likely a probabilistic classifier that can be used for
    tasks where you suspect a nondeterministic approach would be more effective than
    a deterministic one.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 算法输出的形式与模型的数学机制密切相关。这意味着即使没有对算法本身的深入了解，你也可以通过观察其输出的形式来评估模型的数学属性。如果一个分类器将其输出作为概率的一部分，那么它很可能是一个概率分类器，可以用于你怀疑非确定性方法比确定性方法更有效的任务。
- en: Similarly, if an algorithm returns semantically ordered output (as opposed to
    unordered output), that's a clue that the algorithm itself models and remembers
    some form of ordering. Even if your application doesn't directly require ordered
    output, you might still choose this algorithm because you recognize that the form
    of your data contains information embedded in the ordinality of the data. If you
    know, on the other hand, that the ordinality of your data contains no relevant
    information, then an algorithm which returns ordered output (and therefore models
    ordinality as a dimension) may be overkill.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果一个算法返回语义排序的输出（与无序输出相对），那么这表明该算法本身模型化和记住了一些形式的排序。即使你的应用程序不需要直接排序的输出，你也可能选择这个算法，因为你认识到你的数据形式包含了嵌入在数据序数中的信息。另一方面，如果你知道你的数据序数中不包含任何相关信息，那么返回排序输出（因此将序数作为维度进行建模）的算法可能就是过度设计。
- en: If you still have a few algorithms to choose from at this point, the last step
    to honing in on the best algorithm will be to consider the resources available
    to you and balance them against your requirements for accuracy and speed.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果此时你还有几个算法可供选择，那么最后一步确定最佳算法将是要考虑你所能利用的资源，并将它们与你对准确性和速度的要求进行权衡。
- en: Available resources
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可用资源
- en: It is often the case that there is no clear winner discernible from an array
    of algorithm options. In a sentiment analysis problem, for instance, there are
    several possible approaches and it is not often clear which to take. You can choose
    from a Naive Bayes classifier with embedded negations, a Naive Bayes classifier
    using bigrams, an LSTM RNN, a maximum entropy model, and several other techniques.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，从一系列算法选项中很难找出一个明显的胜者。例如，在情感分析问题中，有几种可能的方法，而且通常并不清楚应该选择哪一种。你可以选择带有嵌入否定词的朴素贝叶斯分类器、使用二元组的朴素贝叶斯分类器、LSTM
    RNN、最大熵模型以及几种其他技术。
- en: If the format and form decision point doesn't help you here—for instance, if
    you have no requirement for a probabilistic classifier—you can make your decision
    based on your available resources and performance targets. A Bayesian classifier
    is lightweight with quick training times, very fast evaluation times, a small
    memory footprint and comparatively small storage and CPU requirements.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果格式和形式决策点在这里没有帮助你——例如，如果你没有对概率分类器的需求——你可以根据你可用的资源和性能目标来做出决定。贝叶斯分类器轻量级，训练时间快，评估时间非常快，内存占用小，存储和CPU需求相对较小。
- en: An LSTM RNN, on the other hand, is a sophisticated model that takes a long time
    to train, a moderate amount of evaluation time, significant CPU/GPU requirements,
    especially during training, and steeper memory and storage requirements than the
    Bayesian classifier.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，LSTM RNN是一个复杂的模型，训练时间较长，评估时间适中，对CPU/GPU的要求显著，尤其是在训练期间，并且比贝叶斯分类器有更高的内存和存储需求。
- en: If no other factor gives you clear guidance on how to choose an algorithm, you
    can make your decision based on the resources available (CPU, GPU, memory, storage,
    or time) to you or your application's user. In this case, there is almost always
    a trade-off; more sophisticated models are typically more accurate than simple
    ones. This is not always true, of course, as naive Bayes classifiers consistently
    outperform other approaches to spam filtering; but this is because of the form
    and dimensionality of the spam detection problem space.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有其他因素为你提供如何选择算法的明确指导，你可以根据你或你的应用程序用户可用的资源（CPU、GPU、内存、存储或时间）来做出决定。在这种情况下，几乎总是存在权衡；更复杂的模型通常比简单的模型更准确。当然，这并不总是正确的，因为朴素贝叶斯分类器在垃圾邮件过滤方面始终优于其他方法；但这是因为垃圾邮件检测问题空间的形态和维度。
- en: In some cases, the limiting factor for your application will be training or
    evaluation time. An application that requires evaluations in 1 ms or less may
    not be an appropriate use case for an ANN, whereas an application that tolerates
    50 ms evaluations is much more flexible.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，你的应用的限制因素将是训练或评估时间。如果一个应用需要1毫秒或更少的评估，可能不适合使用人工神经网络（ANN），而一个可以容忍50毫秒评估的应用则更加灵活。
- en: In other cases, the limiting factor or the resource available is the required
    accuracy of the algorithm. If you consider incorrect predictions to be a form
    of resource consumption, you may be able to determine a lower bound on the required
    accuracy of an algorithm. The world of ML is not too different from the world
    of high-performance sports, in that the difference between the gold medal and
    the bronze may only be a matter of a fraction of a second.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，限制因素或可用的资源是算法所需的精度。如果你认为错误的预测是一种资源消耗，你可能能够确定算法所需精度的下限。机器学习的世界并不太不同于高性能体育的世界，因为在金牌和铜牌之间的差距可能只是几秒钟的时间。
- en: 'The same applies to ML algorithms: a state-of-the-art algorithm for a particular
    problem might have an accuracy of 94%, while a more commonplace algorithm might
    yield 90% accuracy. If the cost of an incorrect prediction is sufficiently high,
    that difference of four percentage points might be the deciding factor in which
    algorithm you choose and how much time and energy you invest in the problem. If,
    on the other hand, the cost of an incorrect prediction is low, the more common
    algorithm might be your best choice based on the resources, time, and effort required
    to implement it.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样适用于机器学习算法：对于特定问题，最先进的算法可能具有94%的准确率，而更常见的算法可能只有90%的准确率。如果错误预测的成本足够高，那么这四个百分点的差异可能就是你在选择算法以及投入多少时间和精力解决问题时的决定性因素。另一方面，如果错误预测的成本很低，那么更常见的算法可能基于资源、时间和努力来实现，可能是最佳选择。
- en: If you carefully consider these four decision points—the mode of learning, the
    task at hand, the format and form of the data, and the resources available to
    you—you will often find that the best algorithm to choose stands out clearly.
    This will not always be the case; sometimes you will be forced to make a judgment
    call based on the algorithms and processes you're most comfortable with.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细考虑这四个决策点——学习模式、当前任务、数据的格式和形式，以及你拥有的资源——你通常会发现最佳算法的选择非常明显。这并不总是如此；有时你将不得不基于你最熟悉的算法和流程做出判断。
- en: Sometimes, after choosing an algorithm, you will find that your results are
    unacceptable. It can be tempting to throw out your algorithm immediately and choose
    a new one, but use caution here as it's often difficult to tell the difference
    between a bad choice of algorithm and a bad configuration of an algorithm. You
    must therefore be prepared to debug your system when it all goes wrong.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，在选择了一个算法之后，你会发现你的结果无法接受。立即放弃你的算法并选择一个新的算法可能会很有诱惑力，但在这里要小心，因为通常很难区分算法选择不当和算法配置不当。因此，你必须准备好在一切出错时调试你的系统。
- en: When it goes wrong
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当出错时
- en: There is a wide range of possible undesirable outcomes in ML. These can range
    from models that simply don't work to models that do work but use an unnecessary
    amount of resources in the process. Negative outcomes can be caused by many factors,
    such as the selection of an inappropriate algorithm, poor feature engineering,
    improper training techniques, insufficient preprocessing, or misinterpretation
    of results.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，可能的不理想结果范围很广。这些可能包括模型根本不起作用，或者模型虽然有效但在这个过程中使用了不必要的资源。负面结果可能由许多因素引起，例如选择不适当的算法、特征工程不良、不当的训练技术、预处理不足或结果解释错误。
- en: In the best-case scenario—that is, the best-case scenario of a negative outcome—the
    problem will make itself apparent in the early stages of your implementation.
    You may find during the training and validation stage that your ANN never achieves
    an accuracy greater than 50%. In some cases, an ANN will quickly stabilize at
    a value like 25% accuracy after only a few training epochs and never improve.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在最佳情况下——即负面结果的最好情况——问题将在你实施早期阶段自行显现。你可能会在训练和验证阶段发现你的ANN从未达到超过50%的准确率。在某些情况下，ANN在经过几个训练周期后可能会迅速稳定在25%的准确率，并且不再提高。
- en: Problems that make themselves obvious during training in this manner are the
    easiest to debug. In general, these are indications that you've selected the wrong
    algorithm for your problem. In the case of ANNs, which are rarely the *wrong *choice
    because they are so flexible, this type of plateau can indicate that you've designed
    the wrong network topology (too few layers or too few hidden neurons) or selected
    the wrong activation functions for your layers (for example, using tanh when sigmoid
    is more appropriate). In this case, you have made a reasonable decision to use
    an ANN, but have configured it incorrectly.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方式下训练过程中显现出来的问题是最容易调试的。一般来说，这些都是你选择了错误算法的迹象。在[第五章](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml)《分类算法》中，我向你介绍了随机森林分类器，我们发现它在我们示例问题中的准确率低得无法接受。使用随机森林的决定仅仅是错误的决策吗？或者这是一个合理的决策，但由于参数选择不当而受阻？在这种情况下，答案是**都不是**。我对使用随机森林以及参数选择都很有信心，所以我用不同的编程语言将相同的数据和参数通过一个随机森林库运行，得到了更符合我预期的结果。这指向了第三个可能的原因：我选择的随机森林算法库的具体实现可能存在问题。
- en: 'Sometimes the problem is more difficult to debug, particularly when you have
    not written the algorithm from first principles. In [Chapter 5](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml),
    *Classification Algorithms*, I introduced you to the random forest classifier
    and we found that its accuracy was unacceptably low for our example problem. Was
    the decision to use a random forest simply a poor decision? Or was it a reasonable
    decision that was hindered by the incorrect selection of parameters? In that case,
    the answer was **neither**. I was confident of my choice of using a random forest
    as well as my parameter selection, so I ran the same data and parameters through
    a random forest library in a different programming language and got results more
    in line with my expectations. This pointed to a third likely possibility: there
    must be something wrong with the specific implementation of the random forest
    algorithm in the library that I chose to demonstrate.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候问题更难调试，尤其是当你没有从第一原理编写算法时。在[第五章](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml)《分类算法》中，我向你介绍了随机森林分类器，我们发现它在我们示例问题中的准确率低得无法接受。使用随机森林的决定仅仅是错误的决策吗？或者这是一个合理的决策，但由于参数选择不当而受阻？在这种情况下，答案是**都不是**。我对使用随机森林以及参数选择都很有信心，所以我用不同的编程语言将相同的数据和参数通过一个随机森林库运行，得到了更符合我预期的结果。这指向了第三个可能的原因：我选择的随机森林算法库的具体实现可能存在问题。
- en: Unfortunately, without the confidence that experience brings, it would be easy
    to assume that random forest was simply a poor choice of algorithm for that problem.
    This is why I encourage both practice and play, theory and experimentation. Without
    a thorough understanding of the concepts that underlie random forests I may have
    been tricked into believing that the algorithm itself was to blame, and without
    a tendency to experiment I may never have confirmed that the algorithm and parameters
    were indeed appropriate.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，如果没有经验带来的信心，很容易假设随机森林只是那个问题的算法选择不当。这就是为什么我鼓励实践和玩耍，理论和实验。如果没有对随机森林背后概念的彻底理解，我可能被误导，认为算法本身是问题所在，而且如果没有实验的倾向，我可能永远无法确认算法和参数确实合适。
- en: When things go wrong, my advice is to return to first principles. Go back to
    the very beginning of your engineering design process, and consider each step
    in turn. Is the selection of the algorithm appropriate? Is the form and format
    of the data appropriate? Is the algorithm able to resolve the dimensionality of
    the problem space sufficiently? Have I trained the algorithm appropriately? Ask
    yourself these questions until you've identified the one that you have the least
    confidence in, and then start exploring and experimenting.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当事情出错时，我的建议是回归到第一原理。回到你的工程设计过程的最初阶段，依次考虑每一步。算法的选择是否合适？数据的形式和格式是否合适？算法能否足够地解决问题空间的维度？我是否适当地训练了算法？问自己这些问题，直到你确定你最没有信心的一项，然后开始探索和实验。
- en: From my perspective, the worst-case scenario for ML is an algorithm that *fails
    silently*. These are cases where an algorithm achieves its training and validation
    successfully, is deployed into a real application, but generates poor results
    from real-world data. The algorithm has not been able to generalize its knowledge
    and has only memorized the training data well enough to pass validation. The silent
    failure occurs because the algorithm lulls you into a false sense of security
    by showing good accuracy during validation. You then deploy the algorithm into
    production, trusting its results, only to find a month or a year later that the
    algorithm has been grossly underperforming and making poor decisions affecting
    real people or processes that now need to be corrected.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从我的角度来看，机器学习的最坏情况是一个“无声失败”的算法。这些情况是指一个算法在训练和验证中成功，被部署到实际应用中，但使用现实世界数据产生了较差的结果。该算法未能推广其知识，只是足够好地记住了训练数据以通过验证。这种无声的失败发生是因为算法在验证期间显示出良好的准确性，让你产生了虚假的安全感。然后你将算法部署到生产环境中，信任其结果，但几个月或一年后发现算法表现极差，做出了影响真实人或过程的错误决策，现在需要纠正。
- en: For that reason, you must always monitor the performance of your algorithms
    under real-world workloads. You should periodically spot-check the algorithm's
    work to make sure that its real-world accuracy is comparable to what you've observed
    during training. If an algorithm shows 85% accuracy during training, and a production
    spot-check of 20 data points yields 15 correct answers (75%), that algorithm is
    probably working as expected. However, if you find that only 50% of real-world
    evaluations are correct, you should expand your audit of the algorithm and potentially
    retrain it based on an updated training set drawn from your more realistic data
    points.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你必须始终监控你的算法在实际工作负载下的性能。你应该定期抽查算法的工作，以确保其在现实世界中的准确性与你训练期间观察到的相当。如果一个算法在训练期间达到85%的准确性，而在对20个数据点的生产抽查中产生了15个正确答案（75%），那么这个算法可能正在按预期工作。然而，如果你发现只有50%的现实世界评估是正确的，你应该扩大对算法的审计，并可能基于从更真实的数据点中抽取的更新后的训练集重新训练它。
- en: These silent failures are often caused by over-training or poor training. Even
    if you have followed best practices for training and split your prelabeled data
    set into separate training and validation sets, it is still possible to overtrain
    and undergeneralize. In some cases the source data itself can be blamed. If your
    entire training and validation set was generated from survey results of college
    students, for instance, the model may not be able to accurately evaluate survey
    results from senior citizens. In this case, even though you have validated your
    model on an independent data set, the training and validation data itself is not
    an appropriate random sampling of real-world conditions. In real-world usage you
    will see a lower accuracy than the validation results, as the source data you
    used to train the model has been compromised by selection bias.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些无声的失败通常是由过度训练或训练不当引起的。即使你遵循了训练的最佳实践，并将预标记的数据集分成单独的训练和验证集，仍然可能过度训练和泛化不足。在某些情况下，源数据本身可能存在问题。例如，如果你的整个训练和验证集都是根据大学生的调查结果生成的，那么模型可能无法准确评估老年人的调查结果。在这种情况下，即使你在独立数据集上验证了你的模型，训练和验证数据本身也不是现实世界条件的适当随机抽样。在实际使用中，你将看到比验证结果更低的准确性，因为用于训练模型的数据源已被选择偏差所损害。
- en: A similar thing can happen with other types of classification tasks. A sentiment
    analysis algorithm trained on movie reviews may not be able to generalize to restaurant
    reviews; the jargon and tone—the formof the data—may be different between those
    two data sources.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的情况也可能发生在其他类型的分类任务中。一个在电影评论上训练的情感分析算法可能无法推广到餐厅评论；这两个数据源之间的术语和语气——数据的形式——可能不同。
- en: If your model is underperforming and you are truly lost as to what to do next,
    turn to experimentation and exploration. Test out a different algorithm with the
    same training set and compare results. Try generating a new training set, either
    one more broad or more narrow. Experiment with different tokenization techniques,
    different preprocessing techniques, and potentially even different implementations
    of the same algorithm. Search the web to see how other researchers approach similar
    problems, and most importantly, never give up. Frustration is part of the learning
    process.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型表现不佳，你真正不知道下一步该做什么，转向实验和探索。使用相同的训练集测试不同的算法并比较结果。尝试生成一个新的训练集，要么更广泛，要么更窄。尝试不同的标记化技术，不同的预处理技术，甚至可能尝试同一算法的不同实现。在网上搜索其他研究人员如何处理类似问题，最重要的是，永远不要放弃。挫折是学习过程的一部分。
- en: Combining models
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型组合
- en: 'Sometimes, in order to achieve a singular business goal, you''ll need to combine
    multiple algorithms and models and use them in concert to solve a single problem.
    There are two broad approaches to achieving this: combining models in series and
    combining them in parallel.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，为了实现单一的商业目标，你需要结合多个算法和模型，并将它们协同使用来解决单个问题。实现这一目标有两种主要方法：串联组合模型和并行组合模型。
- en: In a series combination of models, the outputs of the first model become the
    inputs of the second. A very simple example of this is the Word2vec word-embedding
    algorithm used before a classifier ANN. The Word2vec algorithm is itself an ANN
    whose outputs are used as the inputs to another ANN. In this case, Word2vec and
    the classifier are trained separately but evaluated together, in series.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型串联组合中，第一个模型的输出成为第二个模型的输入。一个简单的例子是在分类器ANN之前使用的Word2vec词嵌入算法。Word2vec算法本身就是一个ANN，其输出被用作另一个ANN的输入。在这种情况下，Word2vec和分类器是分别训练但一起评估的，串联进行。
- en: You can also consider a CNN to be a serial combination of models; the operation
    of each of the layers (convolution, max pooling, and fully connected) each has
    a different purpose and is essentially a separate model whose output provides
    the input to the next layer. In this case, however, the entire network is both
    evaluated and trained as a single unit.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以将CNN视为模型的串联组合；每一层（卷积、最大池化和全连接）的操作都有不同的目的，本质上是一个独立的模型，其输出为下一层提供输入。然而，在这种情况下，整个网络既被评估也被作为一个单一单元进行训练。
- en: Models run in parallel are often called **ensembles**, with the random forest
    being a straightforward example. In a random forest many individual decision trees
    are run in parallel and their outputs combined. More generally, however, there
    is no requirement for models run in parallel to be of the same type of algorithm.
    When analyzing sentiment, for instance, you can run both a bigram Naive Bayes
    classifier and an LSTM RNN in parallel, and use a weighted average of their outputs
    in order to produce more accurate results than either would return individually.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 并行运行的模型通常被称为**集成**，随机森林是一个简单的例子。在随机森林中，许多单独的决策树并行运行，并将它们的输出组合。更普遍地说，并行运行的模型不需要是相同类型的算法。例如，在分析情感时，你可以并行运行一个二元朴素贝叶斯分类器和一个LSTM
    RNN，并使用它们输出的加权平均来产生比单独运行更准确的结果。
- en: In some cases, you can combine models in order to better handle heterogeneous
    data. Let's imagine a business goal where a user should be classified into one
    of ten psychometric categories based on both their written content and also a
    number of other features derived from their profile. Perhaps the goal is to analyze
    a user on Twitter in order to determine which marketing vertical to place them
    in: *fashionista*, *weekend warrior*, *sports junkie*, and so on. The data available
    to you is the text content of their tweet history, their list of friends and content
    interactions, and a number of derived metrics such as average post frequency,
    average Flesch-Kincaid reading ease, friends-to-followers ratio, and so on.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，你可以组合模型以更好地处理异构数据。让我们想象一个商业目标，即根据用户的书面内容和从其个人资料中衍生出的许多其他特征，将用户分类到十个心理测量类别之一。也许目标是分析Twitter上的用户，以确定将他们放置在哪个营销垂直领域：*时尚达人*，*周末战士*，*运动狂热者*等等。你可以使用的数据包括他们推文历史的文本内容，他们的朋友列表和内容互动，以及一些衍生指标，如平均发帖频率，平均Flesch-Kincaid阅读难度，关注者与朋友的比例等等。
- en: This problem is a classification task, but it is a business goal that requires
    multiple technical steps to achieve. Because the input data is not homogeneous,
    we must split the problem up into parts and solve each separately. We then combine
    the parts to compose an accurate and performant ML system.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题是一个分类任务，但它是一个需要通过多个技术步骤才能实现的企业目标。因为输入数据不均匀，我们必须将问题分解成几个部分，并分别解决。然后我们将这些部分组合起来，以形成一个准确且高效的机器学习系统。
- en: First, we can take text content of the user's tweets and pass that through a
    naive Bayes classifier in order to determine which of the 10 categories the content
    best fits. The classifier will return a probability distribution like *5% fashionista,
    60% sports junkie, and 25% weekend warrior*. This classifier alone is probably
    not sufficient to solve the problem; weekend warriors and sports junkies tend
    to write about similar topics, and the Bayesian classifier cannot tell the difference
    between the two because there's so much overlap.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以获取用户的推文文本内容，并通过朴素贝叶斯分类器来决定内容最适合的 10 个类别之一。该分类器将返回一个概率分布，例如 *5% fashionista，60%
    sports junkie，和 25% weekend warrior*。这个分类器单独可能不足以解决问题；周末战士和运动狂热者倾向于写类似的话题，贝叶斯分类器无法区分两者，因为它们有很多重叠。
- en: Fortunately, we can combine the text classification with other signals, such
    as how often the user posts images to Twitter, how often they tweet on weekends
    versus weekdays, and so on. An algorithm like random forest, which can deal with
    heterogeneous input data, would be useful here.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以将文本分类与其他信号结合起来，例如用户在推特上发布图片的频率，他们在周末和周中发推文的频率等等。像随机森林这样的算法，可以处理异构输入数据，在这里会很有用。
- en: The approach we can take is to use the 10 probabilities generated by the Bayesian
    classifier, combine them with another 10 features derived directly from the user's
    profile data, and feed the combined list of 20 features to the random forest classifier.
    The random forest will learn when to trust the Bayesian classifier's output and
    when to lean more heavily on other signals. In cases where the Bayesian classifier
    has difficulty discerning between sports junkies and weekend warriors, for instance,
    the random forest may be able to draw distinctions between the two based on the
    additional context.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采取的方法是使用贝叶斯分类器生成的 10 个概率，将它们与从用户个人资料数据直接导出的另外 10 个特征结合起来，然后将这 20 个特征的组合列表输入到随机森林分类器中。随机森林将学会何时信任贝叶斯分类器的输出，何时更依赖其他信号。例如，当贝叶斯分类器难以区分运动狂热者和周末战士时，随机森林可能会根据额外的上下文在这两者之间做出区分。
- en: Furthermore, the random forest will be able to learn when to trust the Bayesian
    probabilities and when not to. A random forest might learn that the Bayesian classifier
    is often correct when it judges *fashionista* with a 90% probability. It might
    similarly learn that the Bayesian classifier is unreliable when judging *weekend
    warrior* even at high probabilities, and that for a significant proportion of
    the time a weekend warrior might be mistaken for a sports junkie.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随机森林将能够学习何时信任贝叶斯概率，何时不信任。随机森林可能会学会，当贝叶斯分类器以 90% 的概率判断 *fashionista* 时，它通常是正确的。它可能会以类似的方式学会，当贝叶斯分类器在高概率下判断
    *weekend warrior* 时是不可靠的，并且对于相当一部分时间，周末战士可能会被误认为是运动狂热者。
- en: 'From an intuitive perspective, the random forest is a good algorithm to choose
    for this use case. Because it is based on decision trees, it is able to create
    decision points based on the values of specific attributes. A random forest might
    generate a logical structure like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观的角度来看，随机森林是适用于此用例的好算法。因为它基于决策树，能够根据特定属性的值创建决策点。随机森林可能会生成如下逻辑结构：
- en: If *bayes_fashionista_probability* > 85%, return *fashionista*
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *bayes_fashionista_probability* 大于 85%，则返回 *fashionista*
- en: If *bayes_weekend_warrior_probability* > 99%, return *weekend warrior*
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *bayes_weekend_warrior_probability* 大于 99%，则返回 *weekend warrior*
- en: 'If *bayes_weekend_warrior_probability* < 99%, continue to:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *bayes_weekend_warrior_probability* 小于 99%，则继续：
- en: If *twitter_weekend_post_frequency* > 70%, return *weekend warrior*
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *twitter_weekend_post_frequency* 大于 70%，则返回 *weekend warrior*
- en: Else, if *bayes_sports_junkie_probability* > 60%, return *sports junkie*
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，如果 *bayes_sports_junkie_probability* 大于 60%，则返回 *sports junkie*
- en: In this simplified example, the random forest has learned to trust the Bayesian
    classifier's judgment for the *fashionista* category. However, the forest will
    only trust the Bayesian classifier's judgement of weekend warriors if the probability
    is very high. If the Bayesian classifier is less than certain about the weekend
    warrior classification, then the random forest can turn to the user's frequency
    of tweets on the weekend as a separate signal used to discriminate between weekend
    warriors and sports junkies.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简化的例子中，随机森林已经学会了信任贝叶斯分类器对**时尚达人**类别的判断。然而，只有当概率非常高时，森林才会信任贝叶斯分类器对周末战士的判断。如果贝叶斯分类器对周末战士的分类不太确定，那么随机森林可以转向用户周末发推文的频率作为一个单独的信号，用于区分周末战士和运动狂热者。
- en: When designed thoughtfully, composed models like this one can be very powerful
    tools capable of handling many situations. This technique allows you to decompose
    a business goal into multiple technical goals, choose the best algorithm for each
    type of data or classification, and combine the results into one coherent and
    confident response.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当精心设计时，像这样的组合模型可以是非常强大的工具，能够处理许多情况。这项技术允许你将业务目标分解成多个技术目标，为每种类型的数据或分类选择最佳算法，并将结果组合成一个连贯且自信的响应。
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Most of this book has focused on the implementation of ML algorithms used to
    solve specific problems. However, the implementation of an algorithm is only one
    part of the software-engineering design process. An engineer must also be skilled
    in choosing the right algorithm or system for her problem and be able to debug
    issues as they arise.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的大部分内容都集中在实现用于解决特定问题的机器学习算法上。然而，算法的实现只是软件开发设计过程的一部分。工程师还必须擅长选择适合她问题的正确算法或系统，并且能够处理出现的问题。
- en: In this chapter, you learned a simple four-point decision-making process that
    can help you choose the best algorithm or algorithms for a specific use case.
    Using the process of elimination, you can progressively reduce your options by
    disqualifying algorithms based on each of those decision points. Most obviously,
    you should not use an unsupervised algorithm when you're facing a supervised learning
    problem. You can further eliminate options by considering the specific task at
    hand or business goal, considering the format and form of the input and output
    data or the problem space, and doing a cost-benefit analysis with regards to the
    resources available to you.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了一个简单的四点决策过程，可以帮助你为特定用例选择最佳算法或算法。通过排除法，你可以通过根据每个决策点排除算法来逐步减少你的选择。最明显的是，当你面对监督学习问题时，你不应该使用无监督算法。你可以通过考虑手头的具体任务或业务目标，考虑输入和输出数据的格式和形式或问题空间，以及对你可用的资源进行成本效益分析来进一步排除选项。
- en: We also discussed some problems that can arise when using ML models in the real
    world, such as the insidious issue of silent failures caused by poor training
    practices, or the more obvious failures caused by inappropriate algorithm selection
    or network topology.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了在现实世界中使用机器学习模型时可能出现的某些问题，例如由于训练实践不当而导致的隐蔽的静默故障问题，或者由于算法选择不当或网络拓扑不合适而导致的更明显的故障。
- en: Finally, we discussed the idea of composing models in either series or parallel,
    in order to leverage the particular strengths of algorithms, especially when presented
    with heterogeneous data. I showed an example of a random forest classifier that
    uses both direct signals and the outputs of a separate Bayesian classifier as
    its inputs; this approach helps to disambiguate confusing signals coming from
    the Bayesian classifier, which on its own may not be able to resolve overlapping
    categories accurately.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了将模型以串联或并行方式组合的想法，以便利用算法的特定优势，尤其是在面对异构数据时。我展示了一个随机森林分类器的例子，它使用直接信号和另一个贝叶斯分类器的输出作为其输入；这种方法有助于消除贝叶斯分类器产生的混淆信号，因为贝叶斯分类器本身可能无法准确解决重叠类别。
- en: There is much more I wish I could teach you. This book has simply been an overview,
    a whirlwind introduction to the central concepts and algorithms of ML. Each one
    of the algorithms I've shown you is its own field of research that goes much deeper
    than what can be taught in just 10 or 20 pages.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我还有很多东西想教给你。这本书仅仅是一个概述，是对机器学习核心概念和算法的快速介绍。我所展示的每一个算法都是一个研究领域，其深度远远超过10或20页所能教授的内容。
- en: I do not expect this book to solve all your ML problems, and neither should
    you. I hope, however, that this book has given you a solid foundation of understanding
    on top of which you can build your future education. In a field like ML, both
    esoteric and full of jargon, the biggest challenge is often knowing where to start.
    Hopefully the information in these pages has given you enough understanding and
    clarity to be able to navigate the wider world of ML on your own.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我不期望这本书能解决你所有的机器学习问题，你也不应该这样期望。然而，我希望这本书为你提供了一个坚实的理解基础，你可以在此基础上构建未来的教育。在像机器学习这样的领域，既神秘又充满术语，最大的挑战往往是知道从哪里开始。希望这些页面中的信息已经给你足够的理解和清晰度，能够独自导航更广泛的机器学习世界。
- en: As you finish reading these final pages, I do not expect you to be fluent in
    the language of ML yet, but hopefully you are now conversational. While you may
    not yet be able to design exotic ANN topologies on your own, you should at least
    be comfortable with the core concepts, be able to communicate with other researchers,
    and find your own way to resources for continued in-depth learning. You can also
    solve many types of problems that you may not have been able to previously, and
    if that is the case then I have achieved my goal.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读完这些最后的页面时，我不期望你在这个机器学习语言的熟练度上已经达到流利，但希望你现在至少能够进行对话。虽然你可能还不能独自设计复杂的ANN拓扑结构，但你至少应该对核心概念感到舒适，能够与其他研究人员交流，并找到自己继续深入学习资源的途径。你也可以解决许多以前可能无法解决的问题，如果那样的话，我就达到了我的目标。
- en: 'I have one final request of you: if you do continue your ML education, particularly
    in the JavaScript ecosystem, please contribute back to the community. As you have
    seen, there are many high-quality JavaScript ML libraries and tools available
    today, but there are also major gaps in the ecosystem. Some algorithms and techniques
    simply do not exist in the JavaScript world yet, and I would encourage you to
    seek out opportunities to fill in these gaps as best as you can, whether by contributing
    to open source software or writing educational materials for others to use.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个最后的请求：如果你继续你的机器学习教育，尤其是在JavaScript生态系统内，请回馈社区。正如你所看到的，今天有很多高质量的JavaScript机器学习库和工具，但生态系统中也存在很大的空白。一些算法和技术在JavaScript世界中尚不存在，我鼓励你寻找机会，尽可能填补这些空白，无论是通过为开源软件做出贡献，还是为他人编写教育材料。
- en: Thank you for taking the time to read this humble introduction to machine learning
    in JavaScript—I hope it has served you well.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你抽出时间阅读这本关于JavaScript机器学习的谦逊导论——我希望它对你有所帮助。
