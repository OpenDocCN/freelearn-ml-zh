- en: Choosing the Best Algorithm for Your Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three distinct phases in the software-engineering process: conception,
    implementation, and deployment. This book has primarily focused on the implementation
    phase of the process, which is when a software engineer develops the core functionality
    (that is, a **machine learning** (**ML**) algorithm) and features of the project.
    In the last chapter, we discussed matters concerning the deployment phase. Our
    learning is nearly complete.'
  prefs: []
  type: TYPE_NORMAL
- en: In this final chapter, we'll turn to the conception phase in order to round
    out our understanding of the full ML development process. Specifically, we'll
    discuss how to choose the best algorithm for a given problem. The ML ecosystem
    is evolving, intimidating, and full of jargon unfamiliar even to experienced software
    developers. I often see students of ML get stuck at the beginning of the process,
    not knowing where to start in a vast and unfamiliar landscape. What they don't
    know yet is that once you get past the initial hurdle of choosing an algorithm
    and decrypting the jargon, the rest of the journey is much easier.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter is to provide a compass, a simple guide one can use
    to find a way around the landscape. It's not always easy to select the right algorithm,
    but sometimes it is. The first sections of this chapter will teach you four simple
    decision points—essentially four multiple-choice questions—that you can use to
    focus in on the most suitable algorithms for your project. Most of the time, you'll
    end up with only one or two algorithms to choose from after going through this
    process.
  prefs: []
  type: TYPE_NORMAL
- en: We'll then continue our education by discussing other topics related to planning
    an ML system. We'll discuss the telltale signs that you've chosen the wrong algorithm
    so that you can identify mistakes early on. You'll also learn how to tell the
    difference between using the wrong algorithm versus a bad implementation of one.
  prefs: []
  type: TYPE_NORMAL
- en: I'll also show you an example of combining two different ML models, so that
    you can compose larger systems out of individual models that are best suited to
    their own tasks. This approach can yield great results if designed carefully.
  prefs: []
  type: TYPE_NORMAL
- en: I called this chapter a compass—not a map—for a reason. It is not a comprehensive
    guide that covers every ML algorithm known to computer scientists. As with a compass,
    you must also use your guile and skills to find your way. Use this chapter to
    find your own project's starting point, and then follow up with your own research.
    While the 20 or so algorithms and techniques we've discussed in this book give
    you a pretty wide view of the landscape, they're only a fraction of the ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: As we come to the beginning of the end of this book, I'd like to give you one
    final piece of advice. To become an expert at something requires a consistent
    dedication to *both *practice and play. If you want to become a world-class pianist,
    you must spend countless hours doing meticulous rote practice with a metronome,
    practicing fingering exercises and learning challenging *études*.
  prefs: []
  type: TYPE_NORMAL
- en: But you also have to *play*, which is where exploration happens and creativity
    is developed. After thirty minutes of running drills, a pianist might spend thirty
    minutes improvising jazz and experimenting with melody and counterpoint, learning
    the *je ne sais quoi* or the emotive essence of the scales and patterns in the
    music. That playful exploration, the experimentation involved in creativity, develops
    the intuitive sense of music in a way that rote practice does not. Rote practice—meticulous
    work and study—in turn develops the mechanical sense and skill that play cannot.
    Practice and play elevate each other in a virtuous cycle. The skilled are able
    to explore farther and deeper than the unskilled, and the excitement of what lies
    even deeper still is what motivates the practice that develops the skill. Time
    and patience, practice and play, motivation and discipline are the only things
    you need to go from being a novice to an expert.
  prefs: []
  type: TYPE_NORMAL
- en: ML is the opposite of jazz piano, but the path to expertise is the same. The
    rote practice of ML—the equivalent of practicing scales—is building and implementing
    algorithms. I particularly recommend writing algorithms from scratch as practice;
    that's the only true way to understand what's really going on inside. Don't just
    write an algorithm from scratch once to prove to yourself that you can. Write
    the algorithm several times, in different environments, different programming
    languages, different architectures, with different datasets, and keep doing that
    until you can write nearly the whole thing off the top of your head. I'm fairly
    certain I can write a Naive Bayes classifier blindfolded in any of three programming
    languages, just as you would be able to after you've written dozens of them.
  prefs: []
  type: TYPE_NORMAL
- en: The play of ML lies in experimentation. This chapter is about choosing the best
    algorithm for your application, but it isn't rule of law. If you never experiment
    you'll never develop a rich intuition for the algorithms or data. Experiment with
    other approaches, parameters, or variations on algorithms and learn from the experimentation.
    You'll be surprised at how often an experiment can end up successful, but more
    importantly than that, experimentation should be part of your practice and education.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s kick the chapter off by discussing the four major decision points you
    can use to hone your skills on an algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Mode of learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The task at hand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The format or form of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also we'll discuss what to do when it all goes wrong, and finally we'll discuss
    combining multiple models together.
  prefs: []
  type: TYPE_NORMAL
- en: Mode of learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first decision point to visit when choosing an ML algorithm is the mode
    of the learning process: supervised, unsupervised, or reinforcement learning.
    These modes have very little overlap; in general an algorithm is either supervised
    or unsupervised but not both. This narrows your choices down by roughly half,
    and fortunately it is very easy to tell which mode of learning applies to your
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: The difference between supervised and unsupervised learning is marked by whether
    or not you need labeled training examples to teach the algorithm. If all you have
    is data points, and not labels or categories to associate them with, then you
    are only able to perform unsupervised learning. You must therefore choose one
    of the unsupervised learning algorithms, such as k-means clustering, regressions,
    **Principal Component Analysis** (**PCA**), or singular value decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: Another telltale difference between supervised and unsupervised learning is
    whether or not there is a way to judge semantic accuracy. If the concept of judging
    accuracy doesn't make sense in your application (because you don't have labeled
    training or reference data), then you are facing an unsupervised learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, however, you will not have training data but the problem is best
    solved by a supervised learning problem. It's important to recognize the difference
    between having training data and *needing *training data. When you need training
    data, you are likely looking at a supervised learning problem. If you need training
    data but don't have it, you'll have to figure out some way to get the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to generate training data for a problem is to generate it yourself.
    In an image classification task you can simply label a few hundred images by hand
    to generate your training data. This is time consuming but will work for small
    training sets.
  prefs: []
  type: TYPE_NORMAL
- en: A more scalable approach is to use a service like Amazon Mechanical Turk, through
    which you pay human workers $0.05-$0.10 to label each image. The Mechanical Turk
    approach has become very popular with ML researchers and data scientists, as it
    is a fast and scalable way to generate a large volume of training data at a reasonable
    cost. Generating labels for 5,000 images might cost $250 and take a day or two
    on Mechanical Turk. If you think $250 is expensive, consider the cost of the time
    it would take for you to label those 5,000 images personally.
  prefs: []
  type: TYPE_NORMAL
- en: There are more clever ways to generate training data, such as shifting the responsibility
    to your application's users. Years ago, when Facebook first introduced the ability
    to tag people in photos, they required the photo's uploader to draw a box around
    each subject's face and tag them. After billions of photo uploads, Facebook has
    a huge training set that not just identifies facial forms but also specific people
    in photos. Nowadays, there is no need to draw boxes around peoples' faces when
    tagging photos, and typically Facebook is able to figure out who each subject
    in the photo is automatically. We, the users, provided them with this massive
    training set.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning is made apparent by the act of training an algorithm on
    prelabeled data, with the goal of algorithm learning from the labels, and by being
    able to extend that knowledge and apply it to new, unseen examples. If you find
    yourself in a situation where you have a lot of data points and some, but not
    all, have the correct answers or labels available, you likely want a supervised
    learning algorithm. If you have a million emails, 5,000 of which were manually
    filtered as spam or not spam, and if the goal is to extend that knowledge to the
    other 995,000 messages, you're looking for a supervised learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a situation where you need to judge the semantic accuracy of the
    algorithm, you must also use a supervised learning algorithm. An unsupervised
    algorithm has no source of truth; those algorithms will cluster or smooth or extrapolate
    data, but without a canonical reference, there is no way to judge the algorithm's
    accuracy. A supervised learning algorithm, on the other hand, can be judged in
    terms of semantic accuracy as the prelabeled training data serves as the source
    of truth.
  prefs: []
  type: TYPE_NORMAL
- en: While we haven't covered reinforcement learning algorithms in this book, they
    are marked by situations where the algorithm must affect its environment in an
    attempt to optimize behavior. There is a degree of separation between the algorithm's
    output and the results of the action, since the environment itself is a factor.
    An example of reinforcement learning is teaching an AI to play a video game by
    scanning the screen and using mouse and keyboard controls. An AI playing *Super
    Mario Bros.* can only interact with the environment by hitting combinations of
    up, down, left, right, A, and B on the control pad. The algorithm's output takes
    an action in the environment, and the environment will either reward or punish
    those actions. The algorithm therefore tries to maximize its reward, for instance
    by collecting coins, making progress through the level, defeating enemies, and
    not falling into bottomless pits.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning algorithms are a major topic in applied robotics, control
    system design, simulation-based optimization, hardware-in-the-loop simulation,
    and many other fields that blend the physical world with the algorithmic world.
    Reinforcement learning is most effective when the system you're studying—the environment—is
    a sophisticated black box that cannot be directly modeled. Reinforcement learning
    is used, for instance, to optimize control strategies for systems that will be
    used in arbitrary environments, such as a robot that must autonomously navigate
    unknown terrain.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there are tasks that require only the optimization of a system whose
    model is either known or directly observable. These are only tangentially considered
    ML problems, and are more appropriately called **optimization problems**. If you
    must choose the best driving route from Point A to Point B based on current traffic
    conditions, you can use a genetic algorithm, for instance. If you must optimize
    a small number of parameters that configure a different model, you might try a
    grid search. If you must determine the boundary conditions of a complex system,
    Monte Carlo methods might be able to help. If you must find the global optimum
    of a continuous system, then stochastic gradient descent might serve your purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization algorithms are often used to solve problems with ML algorithms.
    Indeed, the backpropagation algorithm used to train ANNs uses gradient descent
    as its optimizer. The parameters given to k-means or other unsupervised algorithms
    can be tuned automatically through grid search in order to minimize variance.
    We have not discussed optimization algorithms in-depth in this book, but you should
    be aware of them and their use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'When choosing an algorithm for your application, start with the simplest question:
    Do I need supervised or unsupervised learning? Determine whether you have or can
    generate training data; if you cannot, you are forced to use unsupervised algorithms.
    Ask yourself whether you need to judge the accuracy of the algorithm''s output
    (supervised learning), or whether you are simply exploring data (unsupervised).'
  prefs: []
  type: TYPE_NORMAL
- en: After you've identified the mode of learning, which will cut your available
    choices roughly in half, you can further hone in on the algorithm you need by
    thinking about the specific task at hand, or the goal of your research.
  prefs: []
  type: TYPE_NORMAL
- en: The task at hand
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most effective way to partition the world of ML algorithms is to consider
    the task at hand, or the desired results and purpose of the algorithm. If you
    can identify the goal of your problem—that is, whether you need to predict continuous
    values based on inputs, categorize data, classify text, reduce dimensionality,
    and so on—you'll be able to reduce your choices to only a handful of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in cases where you need to predict a continuous output value—such
    as a prediction for server load at a future date—you will likely need a regression
    algorithm. There are only a handful of regression algorithms to choose from, and
    the other decision points in this guide will help to reduce those options further.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where you need to inspect data and identify data points that look similar
    to one another, a clustering algorithm would be the most appropriate. The specific
    clustering algorithm you choose will depend on the other decision points, such
    as the format or form of the data, the linearity or nonlinearity of the relationships,
    and the resources available to you (time, processing power, memory, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: If the purpose of your algorithm is to categorize a data point with one of a
    dozen possible labels, you must choose from one of a handful of classification
    algorithms. Again, selecting the correct algorithm from the family of classification
    algorithms available to you will depend on the form of the data, your requirements
    for accuracy and any resource limitations imposed upon you.
  prefs: []
  type: TYPE_NORMAL
- en: One common issue novice researchers face at this stage is a lack of clarity
    of the actual goal of the project versus the capabilities of individual algorithms.
    Sometimes, the business goal of a problem is abstract and only partially defined.
    It is often the case in those situations that the business goal is only achievable
    through the use of several individual algorithms. A student of ML may have difficulty
    identifying the concrete technical steps that must be composed in order to achieve
    the goal.
  prefs: []
  type: TYPE_NORMAL
- en: An illustrative example is the business goal of writing an application that
    analyzes an image and returns a natural language description of the image's contents.
    For instance, when uploading a picture of a path through a park, the goal might
    be to return the text *a park bench and trash pail on a path with trees in the
    background*. It would be easy to fixate on the singular business goal of the project
    and assume that a single business goal maps to a single algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: However, this example requires at least two or three ML algorithms. First, a
    **Convolutional Neural Network** (**CNN**) must be able to identify objects in
    an image. Another algorithm must then be able to determine the spatial relationships
    between the objects. Finally, an NLP or ML algorithm must be able to take the
    output of the first two algorithms and compose a natural language representation
    from that information.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to understand a business goal and translate it into concrete technical
    steps takes time and experience to develop. You must be able to parse the business
    goal and work backwards in order to decompose it into individual subtasks. Once
    you've identified the subtasks, determining which algorithm best fits each subtask
    becomes a straightforward exercise in this decision-making process. We'll discuss
    the topic of composing algorithms shortly, but for now the important take-away
    is that some business goals will require multiple ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the task at hand will narrow your choices down to just a single
    algorithm. Object detection in images, for instance, is best achieved by a CNN.
    There are, of course, many different specialized subtypes of CNNs that can perform
    object detection (RCNN, Fast RCNN, Mask RCNN, and so on), but in this case we
    are able to narrow the playing field down to just CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, the task at hand can be achieved with several or many algorithms,
    in which case you must use additional decision points in order to choose the best
    one for your application. Sentiment analysis, for instance, can be achieved with
    many algorithms. Naive Bayes classifiers, maximum entropy models, random forests,
    and ANNs (particularly RNNs) can all solve the sentiment analysis problem.
  prefs: []
  type: TYPE_NORMAL
- en: You may also be required to compose multiple algorithms in order to achieve
    the best accuracy for your sentiment analyzer. Therefore, the decision of which
    approach to use will depend not only on the task at hand, but also the form and
    format of the data used by the other algorithms in your composition. Not every
    algorithm is compatible for use in composition with all other algorithms, so the
    form and format decision point is effectively recursive, and you will need to
    apply it to each of the subtasks that you have identified in pursuit of your business
    goal.
  prefs: []
  type: TYPE_NORMAL
- en: Format, form, input, and output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What I've been describing as the format and form of data encapsulates several
    concepts. Most superficially, the *format* of the data relates to the specific
    data types (for example, integers, continuous numbers/floats, text, and discrete
    categories) of both the input and output of the application. The *form* of the
    data encapsulates the relationships between the data structures and the overall
    shape of the problem or solution space. These factors can help you select the
    appropriate algorithm even when the task at hand has given you several choices
    for an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with text (the format), for instance, you must consider how the
    text is treated in terms of its relationship to the problem space and the task
    at hand; I call this the form of the data. When filtering spam it is generally
    not necessary to map the relationships between individual words. When analyzing
    text for sentiment it may indeed be necessary to map some relationships between
    words (for example, to deal with negations or other language modifiers), and this
    will require an additional level of dimensionality. When parsing text in order
    to build a knowledge graph you will need to map the relationships between not
    only the individual words but their conceptual meanings meticulously, requiring
    an even higher level of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: In these examples, the format of the data is the same—all three examples deal
    with text—but the form is different. The shape of the problem space is different.
    The spam filter has a simpler problem space with lower dimensionality and linearly
    separable relationships; each word is treated independently. The sentiment analyzer
    has a different form, requiring some additional dimensionality in the problem
    space in order to encode the relationships between some words, but not necessarily
    all relationships between all words. The knowledge graph problem requires a highly
    dimensional space into which the complex semantic and spacial relationships between
    words must be mapped.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these text analysis cases, you can often ask yourself a series of simple
    questions that helps you narrow in on the correct algorithm: can each word be
    treated independently? Do we need to consider words being modified by other words
    (*like* versus *not like*, or *good* versus *very good*)? Do we need to maintain
    relationships between words separated by large distances (for example, a Wikipedia
    article that introduces a subject in the first paragraph but continues to refer
    to the subject many paragraphs later)?'
  prefs: []
  type: TYPE_NORMAL
- en: Each relationship you keep track of adds a level of dimensionality to the problem
    space, so you must choose an algorithm that can work effectively in the dimensionality
    of the problem. Using a low-dimensionality algorithm such as naive Bayes on a
    high-dimensional problem such as a knowledge graph problem would not yield good
    results; this is akin to a person who lives their whole life in three dimensions
    trying to visualize the ten-dimensional space of superstring theory in physics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversely, a highly dimensional algorithm such as a **Long Short-Term Memory**
    (**LSTM**) RNN can solve low-dimensional problems such as spam filtering, but
    it comes with a cost: the time and resources required to train a highly dimensional
    algorithm. They are incongruous with the difficulty of the problem. A Bayesian
    classifier can be trained on millions of documents in dozens of seconds, but an
    LSTM RNN might require hours of training for the same task and be slower to evaluate
    a data point by an order of magnitude. Even then, there is no guarantee that the
    LSTM RNN will outperform the Bayesian classifier in terms of accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: You must also consider the form and dimensionality when working with numerical
    data. Compared to statistical analysis, time-series analysis requires an additional
    dimension in order to capture the ordinality of the data in addition to the value
    of the data. This is analogous to the difference between a bag-of-words text algorithm
    such as Naive Bayes versus one that preserves sequential ordering of text such
    as an LSTM RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the structural format of the data might be a consideration, though
    formats can often be successfully converted into more amenable formats. In [Chapter
    10](0f60907a-63b3-43ce-b4dd-13d9755b09d5.xhtml), *Natural Language Processing
    in Practice*, we discussed the Word2vec word-embedding algorithm which converts
    text into numerical vectors so that they can be used as the inputs to a neural
    network (which requires numerical inputs). Format conversion actually makes our
    decision-making process more difficult, as it allows us to choose from a wider
    array of algorithms. Using Word2vec means that we can use text as an input to
    algorithms that don't normally accept text, therefore giving us many more options
    to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another common format conversion is the quantization or bucketing of continuous
    numerical values. For instance, a continuous numerical value ranging from 0–10
    could be quantized into three buckets: small, medium, and large. This conversion
    allows us to use our continuous-value data in algorithms that only deal with discrete
    values or categories.'
  prefs: []
  type: TYPE_NORMAL
- en: You should also consider the form and format of the output that you require
    from an algorithm. In a classification task the nominal output of the classifier
    will be a discrete label that describes the input. But not all classifiers are
    created equally. A decision tree classifier will yield one label as its output,
    whereas a Bayesian classifier will yield the probabilities of all possible labels
    as its output. In both cases the nominal output is a probable label, but the Bayesian
    classifier also returns the confidence of its guess along with a probability distribution
    over all possible guesses. In some tasks, the only thing you need from the algorithm
    is a single label; in other cases, the probability distribution is useful or even
    required.
  prefs: []
  type: TYPE_NORMAL
- en: The form of the output of an algorithm is closely related to the mathematical
    mechanism of the model. This means that, even without a deep understanding of
    the algorithm itself, you can evaluate the mathematical properties of a model
    by looking at the form of its output. If a classifier returns probabilities as
    part of its output, it is likely a probabilistic classifier that can be used for
    tasks where you suspect a nondeterministic approach would be more effective than
    a deterministic one.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if an algorithm returns semantically ordered output (as opposed to
    unordered output), that's a clue that the algorithm itself models and remembers
    some form of ordering. Even if your application doesn't directly require ordered
    output, you might still choose this algorithm because you recognize that the form
    of your data contains information embedded in the ordinality of the data. If you
    know, on the other hand, that the ordinality of your data contains no relevant
    information, then an algorithm which returns ordered output (and therefore models
    ordinality as a dimension) may be overkill.
  prefs: []
  type: TYPE_NORMAL
- en: If you still have a few algorithms to choose from at this point, the last step
    to honing in on the best algorithm will be to consider the resources available
    to you and balance them against your requirements for accuracy and speed.
  prefs: []
  type: TYPE_NORMAL
- en: Available resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is often the case that there is no clear winner discernible from an array
    of algorithm options. In a sentiment analysis problem, for instance, there are
    several possible approaches and it is not often clear which to take. You can choose
    from a Naive Bayes classifier with embedded negations, a Naive Bayes classifier
    using bigrams, an LSTM RNN, a maximum entropy model, and several other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: If the format and form decision point doesn't help you here—for instance, if
    you have no requirement for a probabilistic classifier—you can make your decision
    based on your available resources and performance targets. A Bayesian classifier
    is lightweight with quick training times, very fast evaluation times, a small
    memory footprint and comparatively small storage and CPU requirements.
  prefs: []
  type: TYPE_NORMAL
- en: An LSTM RNN, on the other hand, is a sophisticated model that takes a long time
    to train, a moderate amount of evaluation time, significant CPU/GPU requirements,
    especially during training, and steeper memory and storage requirements than the
    Bayesian classifier.
  prefs: []
  type: TYPE_NORMAL
- en: If no other factor gives you clear guidance on how to choose an algorithm, you
    can make your decision based on the resources available (CPU, GPU, memory, storage,
    or time) to you or your application's user. In this case, there is almost always
    a trade-off; more sophisticated models are typically more accurate than simple
    ones. This is not always true, of course, as naive Bayes classifiers consistently
    outperform other approaches to spam filtering; but this is because of the form
    and dimensionality of the spam detection problem space.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the limiting factor for your application will be training or
    evaluation time. An application that requires evaluations in 1 ms or less may
    not be an appropriate use case for an ANN, whereas an application that tolerates
    50 ms evaluations is much more flexible.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, the limiting factor or the resource available is the required
    accuracy of the algorithm. If you consider incorrect predictions to be a form
    of resource consumption, you may be able to determine a lower bound on the required
    accuracy of an algorithm. The world of ML is not too different from the world
    of high-performance sports, in that the difference between the gold medal and
    the bronze may only be a matter of a fraction of a second.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same applies to ML algorithms: a state-of-the-art algorithm for a particular
    problem might have an accuracy of 94%, while a more commonplace algorithm might
    yield 90% accuracy. If the cost of an incorrect prediction is sufficiently high,
    that difference of four percentage points might be the deciding factor in which
    algorithm you choose and how much time and energy you invest in the problem. If,
    on the other hand, the cost of an incorrect prediction is low, the more common
    algorithm might be your best choice based on the resources, time, and effort required
    to implement it.'
  prefs: []
  type: TYPE_NORMAL
- en: If you carefully consider these four decision points—the mode of learning, the
    task at hand, the format and form of the data, and the resources available to
    you—you will often find that the best algorithm to choose stands out clearly.
    This will not always be the case; sometimes you will be forced to make a judgment
    call based on the algorithms and processes you're most comfortable with.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, after choosing an algorithm, you will find that your results are
    unacceptable. It can be tempting to throw out your algorithm immediately and choose
    a new one, but use caution here as it's often difficult to tell the difference
    between a bad choice of algorithm and a bad configuration of an algorithm. You
    must therefore be prepared to debug your system when it all goes wrong.
  prefs: []
  type: TYPE_NORMAL
- en: When it goes wrong
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a wide range of possible undesirable outcomes in ML. These can range
    from models that simply don't work to models that do work but use an unnecessary
    amount of resources in the process. Negative outcomes can be caused by many factors,
    such as the selection of an inappropriate algorithm, poor feature engineering,
    improper training techniques, insufficient preprocessing, or misinterpretation
    of results.
  prefs: []
  type: TYPE_NORMAL
- en: In the best-case scenario—that is, the best-case scenario of a negative outcome—the
    problem will make itself apparent in the early stages of your implementation.
    You may find during the training and validation stage that your ANN never achieves
    an accuracy greater than 50%. In some cases, an ANN will quickly stabilize at
    a value like 25% accuracy after only a few training epochs and never improve.
  prefs: []
  type: TYPE_NORMAL
- en: Problems that make themselves obvious during training in this manner are the
    easiest to debug. In general, these are indications that you've selected the wrong
    algorithm for your problem. In the case of ANNs, which are rarely the *wrong *choice
    because they are so flexible, this type of plateau can indicate that you've designed
    the wrong network topology (too few layers or too few hidden neurons) or selected
    the wrong activation functions for your layers (for example, using tanh when sigmoid
    is more appropriate). In this case, you have made a reasonable decision to use
    an ANN, but have configured it incorrectly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes the problem is more difficult to debug, particularly when you have
    not written the algorithm from first principles. In [Chapter 5](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml),
    *Classification Algorithms*, I introduced you to the random forest classifier
    and we found that its accuracy was unacceptably low for our example problem. Was
    the decision to use a random forest simply a poor decision? Or was it a reasonable
    decision that was hindered by the incorrect selection of parameters? In that case,
    the answer was **neither**. I was confident of my choice of using a random forest
    as well as my parameter selection, so I ran the same data and parameters through
    a random forest library in a different programming language and got results more
    in line with my expectations. This pointed to a third likely possibility: there
    must be something wrong with the specific implementation of the random forest
    algorithm in the library that I chose to demonstrate.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, without the confidence that experience brings, it would be easy
    to assume that random forest was simply a poor choice of algorithm for that problem.
    This is why I encourage both practice and play, theory and experimentation. Without
    a thorough understanding of the concepts that underlie random forests I may have
    been tricked into believing that the algorithm itself was to blame, and without
    a tendency to experiment I may never have confirmed that the algorithm and parameters
    were indeed appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: When things go wrong, my advice is to return to first principles. Go back to
    the very beginning of your engineering design process, and consider each step
    in turn. Is the selection of the algorithm appropriate? Is the form and format
    of the data appropriate? Is the algorithm able to resolve the dimensionality of
    the problem space sufficiently? Have I trained the algorithm appropriately? Ask
    yourself these questions until you've identified the one that you have the least
    confidence in, and then start exploring and experimenting.
  prefs: []
  type: TYPE_NORMAL
- en: From my perspective, the worst-case scenario for ML is an algorithm that *fails
    silently*. These are cases where an algorithm achieves its training and validation
    successfully, is deployed into a real application, but generates poor results
    from real-world data. The algorithm has not been able to generalize its knowledge
    and has only memorized the training data well enough to pass validation. The silent
    failure occurs because the algorithm lulls you into a false sense of security
    by showing good accuracy during validation. You then deploy the algorithm into
    production, trusting its results, only to find a month or a year later that the
    algorithm has been grossly underperforming and making poor decisions affecting
    real people or processes that now need to be corrected.
  prefs: []
  type: TYPE_NORMAL
- en: For that reason, you must always monitor the performance of your algorithms
    under real-world workloads. You should periodically spot-check the algorithm's
    work to make sure that its real-world accuracy is comparable to what you've observed
    during training. If an algorithm shows 85% accuracy during training, and a production
    spot-check of 20 data points yields 15 correct answers (75%), that algorithm is
    probably working as expected. However, if you find that only 50% of real-world
    evaluations are correct, you should expand your audit of the algorithm and potentially
    retrain it based on an updated training set drawn from your more realistic data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: These silent failures are often caused by over-training or poor training. Even
    if you have followed best practices for training and split your prelabeled data
    set into separate training and validation sets, it is still possible to overtrain
    and undergeneralize. In some cases the source data itself can be blamed. If your
    entire training and validation set was generated from survey results of college
    students, for instance, the model may not be able to accurately evaluate survey
    results from senior citizens. In this case, even though you have validated your
    model on an independent data set, the training and validation data itself is not
    an appropriate random sampling of real-world conditions. In real-world usage you
    will see a lower accuracy than the validation results, as the source data you
    used to train the model has been compromised by selection bias.
  prefs: []
  type: TYPE_NORMAL
- en: A similar thing can happen with other types of classification tasks. A sentiment
    analysis algorithm trained on movie reviews may not be able to generalize to restaurant
    reviews; the jargon and tone—the formof the data—may be different between those
    two data sources.
  prefs: []
  type: TYPE_NORMAL
- en: If your model is underperforming and you are truly lost as to what to do next,
    turn to experimentation and exploration. Test out a different algorithm with the
    same training set and compare results. Try generating a new training set, either
    one more broad or more narrow. Experiment with different tokenization techniques,
    different preprocessing techniques, and potentially even different implementations
    of the same algorithm. Search the web to see how other researchers approach similar
    problems, and most importantly, never give up. Frustration is part of the learning
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Combining models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, in order to achieve a singular business goal, you''ll need to combine
    multiple algorithms and models and use them in concert to solve a single problem.
    There are two broad approaches to achieving this: combining models in series and
    combining them in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: In a series combination of models, the outputs of the first model become the
    inputs of the second. A very simple example of this is the Word2vec word-embedding
    algorithm used before a classifier ANN. The Word2vec algorithm is itself an ANN
    whose outputs are used as the inputs to another ANN. In this case, Word2vec and
    the classifier are trained separately but evaluated together, in series.
  prefs: []
  type: TYPE_NORMAL
- en: You can also consider a CNN to be a serial combination of models; the operation
    of each of the layers (convolution, max pooling, and fully connected) each has
    a different purpose and is essentially a separate model whose output provides
    the input to the next layer. In this case, however, the entire network is both
    evaluated and trained as a single unit.
  prefs: []
  type: TYPE_NORMAL
- en: Models run in parallel are often called **ensembles**, with the random forest
    being a straightforward example. In a random forest many individual decision trees
    are run in parallel and their outputs combined. More generally, however, there
    is no requirement for models run in parallel to be of the same type of algorithm.
    When analyzing sentiment, for instance, you can run both a bigram Naive Bayes
    classifier and an LSTM RNN in parallel, and use a weighted average of their outputs
    in order to produce more accurate results than either would return individually.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, you can combine models in order to better handle heterogeneous
    data. Let's imagine a business goal where a user should be classified into one
    of ten psychometric categories based on both their written content and also a
    number of other features derived from their profile. Perhaps the goal is to analyze
    a user on Twitter in order to determine which marketing vertical to place them
    in: *fashionista*, *weekend warrior*, *sports junkie*, and so on. The data available
    to you is the text content of their tweet history, their list of friends and content
    interactions, and a number of derived metrics such as average post frequency,
    average Flesch-Kincaid reading ease, friends-to-followers ratio, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This problem is a classification task, but it is a business goal that requires
    multiple technical steps to achieve. Because the input data is not homogeneous,
    we must split the problem up into parts and solve each separately. We then combine
    the parts to compose an accurate and performant ML system.
  prefs: []
  type: TYPE_NORMAL
- en: First, we can take text content of the user's tweets and pass that through a
    naive Bayes classifier in order to determine which of the 10 categories the content
    best fits. The classifier will return a probability distribution like *5% fashionista,
    60% sports junkie, and 25% weekend warrior*. This classifier alone is probably
    not sufficient to solve the problem; weekend warriors and sports junkies tend
    to write about similar topics, and the Bayesian classifier cannot tell the difference
    between the two because there's so much overlap.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we can combine the text classification with other signals, such
    as how often the user posts images to Twitter, how often they tweet on weekends
    versus weekdays, and so on. An algorithm like random forest, which can deal with
    heterogeneous input data, would be useful here.
  prefs: []
  type: TYPE_NORMAL
- en: The approach we can take is to use the 10 probabilities generated by the Bayesian
    classifier, combine them with another 10 features derived directly from the user's
    profile data, and feed the combined list of 20 features to the random forest classifier.
    The random forest will learn when to trust the Bayesian classifier's output and
    when to lean more heavily on other signals. In cases where the Bayesian classifier
    has difficulty discerning between sports junkies and weekend warriors, for instance,
    the random forest may be able to draw distinctions between the two based on the
    additional context.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the random forest will be able to learn when to trust the Bayesian
    probabilities and when not to. A random forest might learn that the Bayesian classifier
    is often correct when it judges *fashionista* with a 90% probability. It might
    similarly learn that the Bayesian classifier is unreliable when judging *weekend
    warrior* even at high probabilities, and that for a significant proportion of
    the time a weekend warrior might be mistaken for a sports junkie.
  prefs: []
  type: TYPE_NORMAL
- en: 'From an intuitive perspective, the random forest is a good algorithm to choose
    for this use case. Because it is based on decision trees, it is able to create
    decision points based on the values of specific attributes. A random forest might
    generate a logical structure like this:'
  prefs: []
  type: TYPE_NORMAL
- en: If *bayes_fashionista_probability* > 85%, return *fashionista*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *bayes_weekend_warrior_probability* > 99%, return *weekend warrior*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If *bayes_weekend_warrior_probability* < 99%, continue to:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *twitter_weekend_post_frequency* > 70%, return *weekend warrior*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Else, if *bayes_sports_junkie_probability* > 60%, return *sports junkie*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In this simplified example, the random forest has learned to trust the Bayesian
    classifier's judgment for the *fashionista* category. However, the forest will
    only trust the Bayesian classifier's judgement of weekend warriors if the probability
    is very high. If the Bayesian classifier is less than certain about the weekend
    warrior classification, then the random forest can turn to the user's frequency
    of tweets on the weekend as a separate signal used to discriminate between weekend
    warriors and sports junkies.
  prefs: []
  type: TYPE_NORMAL
- en: When designed thoughtfully, composed models like this one can be very powerful
    tools capable of handling many situations. This technique allows you to decompose
    a business goal into multiple technical goals, choose the best algorithm for each
    type of data or classification, and combine the results into one coherent and
    confident response.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of this book has focused on the implementation of ML algorithms used to
    solve specific problems. However, the implementation of an algorithm is only one
    part of the software-engineering design process. An engineer must also be skilled
    in choosing the right algorithm or system for her problem and be able to debug
    issues as they arise.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned a simple four-point decision-making process that
    can help you choose the best algorithm or algorithms for a specific use case.
    Using the process of elimination, you can progressively reduce your options by
    disqualifying algorithms based on each of those decision points. Most obviously,
    you should not use an unsupervised algorithm when you're facing a supervised learning
    problem. You can further eliminate options by considering the specific task at
    hand or business goal, considering the format and form of the input and output
    data or the problem space, and doing a cost-benefit analysis with regards to the
    resources available to you.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed some problems that can arise when using ML models in the real
    world, such as the insidious issue of silent failures caused by poor training
    practices, or the more obvious failures caused by inappropriate algorithm selection
    or network topology.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed the idea of composing models in either series or parallel,
    in order to leverage the particular strengths of algorithms, especially when presented
    with heterogeneous data. I showed an example of a random forest classifier that
    uses both direct signals and the outputs of a separate Bayesian classifier as
    its inputs; this approach helps to disambiguate confusing signals coming from
    the Bayesian classifier, which on its own may not be able to resolve overlapping
    categories accurately.
  prefs: []
  type: TYPE_NORMAL
- en: There is much more I wish I could teach you. This book has simply been an overview,
    a whirlwind introduction to the central concepts and algorithms of ML. Each one
    of the algorithms I've shown you is its own field of research that goes much deeper
    than what can be taught in just 10 or 20 pages.
  prefs: []
  type: TYPE_NORMAL
- en: I do not expect this book to solve all your ML problems, and neither should
    you. I hope, however, that this book has given you a solid foundation of understanding
    on top of which you can build your future education. In a field like ML, both
    esoteric and full of jargon, the biggest challenge is often knowing where to start.
    Hopefully the information in these pages has given you enough understanding and
    clarity to be able to navigate the wider world of ML on your own.
  prefs: []
  type: TYPE_NORMAL
- en: As you finish reading these final pages, I do not expect you to be fluent in
    the language of ML yet, but hopefully you are now conversational. While you may
    not yet be able to design exotic ANN topologies on your own, you should at least
    be comfortable with the core concepts, be able to communicate with other researchers,
    and find your own way to resources for continued in-depth learning. You can also
    solve many types of problems that you may not have been able to previously, and
    if that is the case then I have achieved my goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have one final request of you: if you do continue your ML education, particularly
    in the JavaScript ecosystem, please contribute back to the community. As you have
    seen, there are many high-quality JavaScript ML libraries and tools available
    today, but there are also major gaps in the ecosystem. Some algorithms and techniques
    simply do not exist in the JavaScript world yet, and I would encourage you to
    seek out opportunities to fill in these gaps as best as you can, whether by contributing
    to open source software or writing educational materials for others to use.'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for taking the time to read this humble introduction to machine learning
    in JavaScript—I hope it has served you well.
  prefs: []
  type: TYPE_NORMAL
