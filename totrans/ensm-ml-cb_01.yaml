- en: Get Closer to Your Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Data manipulation with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing, visualizing, and treating missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we will cover various ensemble techniques and will learn how to
    ensemble multiple machine learning algorithms to enhance a model's performance.
    We will use pandas, NumPy, scikit-learn, and Matplotlib, all of which were built
    for working with Python, as we will do throughout the book. By now, you should be
    well aware of data manipulation and exploration.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will recap how to read and manipulate data in Python, how
    to analyze and treat missing values, and how to explore data to gain deeper insights. We
    will use various Python packages, such as `numpy` and `pandas`, for data manipulation
    and exploration, and `seaborn` packages for data visualization. We will continue
    to use some or all of these libraries in the later chapters of this book as well.
    We will also use the Anaconda distribution for our Python coding. If you have
    not installed Anaconda, you need to download it from [https://www.anaconda.com/download](https://www.anaconda.com/download/#macos).
    At the time of writing this book, the latest version of Anaconda is 5.2, and comes
    with both Python 3.6 and Python 2.7\. We suggest you download Anaconda for Python
    3.6\. We will also use the `HousePrices` dataset, which is available on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Data manipulation with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In real life, it is often hard to get a complete and clean dataset formatted
    exactly as we need it. The data we receive often cannot be directly used in statistical
    or machine learning algorithms. We need to manipulate the raw data so that the
    processed data can be used for further analysis and modelling purposes. To begin
    with, we need to import the required packages, such as `pandas`, and read our
    dataset into Python.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the `os` package in the operating system's dependent functionality,
    and the `pandas` package for data manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a look at the data definitions to understand our variables.
    In the following code, we list the data definition for a few variables. The dataset
    and the complete data definitions are available on GitHub. Here is an abridged
    version of the data description file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then import the `os` and `pandas` packages and set our working directory
    according to our requirements, as seen in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to download the dataset from GitHub and copy it to your working
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s perform some data manipulation steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will read the data in `HousePrices.csv` from our current working
    directory and create our first DataFrame for manipulation. We name the DataFrame `housepricesdata`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now take a look at our DataFrame and see how it looks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You might not be able to see all the rows; Jupyter will truncate some of the
    variables. In order to view all of the rows and columns for any output in Jupyter,
    execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`# Setting options to display all rows and columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '`pd.options.display.max_rows = None`'
  prefs: []
  type: TYPE_NORMAL
- en: '`pd.options.display.max_columns = None`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the dimensions of the DataFrame with `shape`. `shape` is an attribute
    of the `pandas` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding command, we can see the number of rows and columns, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the DataFrame has `1460` observations and `81` columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the datatypes of the variables in the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code block, we can see the datatypes of each variable in the
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We're now all ready to start with our data manipulation, which we can do in
    many different ways. In this section, we'll look at a few ways in which we can
    manipulate and prepare our data for the purpose of analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by summarizing our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `describe()` function will show the statistics for the numerical variables
    only:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the output in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae71b07f-4ae3-49f4-824c-63a95f2fabd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will remove the `id` column, as this will not be necessary for our analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now look at the distribution of some of the object type variables, that
    is, the categorical variables. In the following example, we are going to look
    at `LotShape` and `LandContour`. We can study the other categorical variables
    of the dataset in the same way as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We will now see how to perform a conversion between datatypes. What we notice
    is that the data definition of variables such as `MSSubClass`, `OverallQual`,
    and `OverallCond` are all categorical variables. After importing the dataset,
    however, they appear as integers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prior to typecasting any variable, ensure that there are no missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we''ll convert the variables to a categorical datatype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the count of observations for each category of houses, as shown
    in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are many variables that might not be very useful by themselves, but transforming
    them gives us a lot of interesting insights. Let's create some new, meaningful
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: '`YearBuilt` and `YearRemodAdd` represent the original construction date and
    the remodel date respectively. However, if they can be converted into age, these
    variables will tell us how old the buildings are and how many years it has been
    since they were remodeled. To do this, we create two new variables, `BuildingAge`
    and `RemodelAge`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add the two variables to our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We notice that `building_age` and `remodelled_age` are now added to the DataFrame,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a775483-e117-482a-9a06-ca71ada35476.png)'
  prefs: []
  type: TYPE_IMG
- en: Variables that contain label data need to be converted into a numerical form
    for machine learning algorithms to use. To get around this, we will perform encoding
    that will transform the labels into numerical forms so that the algorithms can
    use them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to identify the variables that need encoding, which include `Street`,
    `LotShape`, and `LandContour`. We will perform one-hot encoding, which is a representation
    of categorical variables as binary vectors. We will use the `pandas` package in
    Python to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the one-hot encoded variables that have been created in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/394c838b-dc90-47d9-9e03-9676cca5c3c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Add the one-hot encoded variables to our DataFrame, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the output that we get after adding the one-hot encoded variables
    to the DataFrame in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62732d2d-f5c0-4382-8ab2-7a6820ab2193.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s remove the original variables since we have already created our
    one-hot encoded variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `pandas` module is a part of the Python standard library – it is one of
    the key modules for data manipulation. We have also used other packages, such
    as `os` and `datetime`. After we set our working directory and read the CSV file
    into Python as a `pandas` DataFrame, we moved on to looking at a few data manipulation
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1* to *Step 5* in the preceding section showed us how to read the data
    from a CSV file in Python using `pandas`, and also how to use functions such as `dtypes`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `pandas` package also provides methods for reading data from various file
    types. For example, `pandas.read_excel()` reads an Excel table into a `pandas`
    DataFrame; `pandas.read_json()` converts a JSON string into a`pandas` object;
    and `pandas.read_parquet()` loads a parquet object from a file path and returns
    the `pandas` DataFrame. More information on this can be found at [https://bit.ly/2yBqtvd](https://bit.ly/2yBqtvd).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also read HDF5 format files in Python using the `h5py` package. The `h5py` package
    is a Python interface to the HDF5 binary data format. HDF® supports n-dimensional
    datasets, and each element in the dataset may itself be a complex object. There
    is no limit on the number or size of data objects in the collection. More info
    can be found at [https://www.hdfgroup.org/](https://www.hdfgroup.org/). A sample
    code block looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We look at the datatypes of the variables, and use `describe()` to see the summary
    statistics for the numerical variables. We need to note that `describe()` works
    only for numerical variables and is intelligent enough to ignore non-numerical
    variables. In *Step 6*, we saw how to look at the count of each level for categorical
    variables such as `LotShape` and `LandContour`. We can use the same code to take
    a look at the distribution of other categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 7*, we took a look at the distribution of the `LotShape` and `LandContour` variables
    using `pd.crosstab()`.
  prefs: []
  type: TYPE_NORMAL
- en: One common requirement in a crosstab is to include subtotals for the rows and
    the columns. We can display subtotals using the `margins` keyword. We pass `margins=True` to
    the `pd.crosstab()` function. We can also give a name to subtotal columns using
    the `margins_name` keyword. The default value for `margins_name` is `All`.
  prefs: []
  type: TYPE_NORMAL
- en: We then moved on to learning how to convert datatypes. We had a few variables
    that were actually categorical, but appeared to be numerical in the dataset. This
    is often the case in a real-life scenario, hence we need to learn how to typecast
    our variables. *Step 8* showed us how to convert a numerical variable, such as `MSSubClass`,
    into a categorical type. In *Step 8*, we converted a few variables into a categorical
    datatype. We then created a crosstab to visualize the frequencies of each level
    of categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 9*, we created new meaningful variables from existing variables. We
    created the new variables, `BuildingAge` and `RemodelAge`, from `YearBuilt` and
    `YearRemodAdd` respectively, to represent the age of the building and the number
    of years that have passed since the buildings were remodeled. This method of creating
    new variables can provide better insights into our analysis and modeling. This
    process of creating new features is called **feature engineering**. In *Step 10*,
    we added the new variables to our DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: From there, we moved on to encoding our categorical variables. We needed to
    encode our categorical variables because they have named descriptions. Many machine
    learning algorithms cannot operate on labelled data because they require all input
    and output variables to be numeric. In *Step 12*, we encoded them with one-hot
    encoding. In *Step 11*, we learned how to use the `get_dummies()` function, which
    is a part of the `pandas` package, to create the one-hot encoded variables. In
    *Step 12*, we added the `one-hot_encoded_variables` to our DataFrame. And finally,
    in *Step 13*, we removed the original variables that are now one-hot encoded.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The types of data manipulation required depend on your business requirements.
    In this first recipe, we saw a few ways to carry out data manipulation, but there
    is no limit to what you can do and how you can manipulate data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have also seen how to convert a numerical variable into a categorical variable.
    We can do this kind of typecasting in many ways. For example, we can convert a
    categorical variable into a numerical variable, if required, with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You can only convert the `GarageYrBlt` variable if it does not contain any missing
    values. The preceding code will throw an error, since `GarageYrBlt` contains missing
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have looked at how we can use one-hot encoding to convert categorical variables
    to numerical variables, and why we do this. In addition to one-hot encoding, we
    can perform other kinds of encoding, such as label encoding, frequency encoding,
    and so on. An example code for label encoding is given in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pandas guide to type conversion functions ([https://bit.ly/2MzFwiG](https://bit.ly/2MzFwiG))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pandas guide to one-hot encoding using `get_dummies()` ([https://bit.ly/2N1xjTZ](https://bit.ly/2N1xjTZ))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scikit-learn guide to one-hot encoding ([https://bit.ly/2wrNNLz](https://bit.ly/2wrNNLz))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scikit-learn guide to label encoding ([https://bit.ly/2pDddVb](https://bit.ly/2pDddVb))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing, visualizing, and treating missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Missing values are caused by incomplete data. It is important to handle missing
    values effectively, as they can lead to inaccurate inferences and conclusions. In
    this section, we will look at how to analyze, visualize, and treat missing values.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by analyzing variables with missing values. Set the options in
    pandas to view all rows and columns, as shown in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the following syntax, we can see which variables have missing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9acdec10-9936-422f-9b64-e31b4d337ac2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You might also like to see the missing values in terms of percentages. To see
    the count and percentage of missing values, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It will show you the missing values in both absolute and percentage terms,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56aadd15-3792-4bd6-af6b-8494ad2e1288.png)'
  prefs: []
  type: TYPE_IMG
- en: We notice that variables such as **Alley**, **PoolQC**, **Fence**, and **MiscFeature**
    have **80%** to **90%** of their values missing. **FireplaceQu** has **47.26%**
    of its values missing. A few other variables, such as **LotFrontage**, **MasVnrType**,
    **MasVnrArea**, **BsmtQual**, **BsmtCond**, and a few more Garage-related variables
    have missing values as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there is a catch. Let''s look at the `Alley` variable again. It shows us
    that it has **93.76%** missing values. Now take another look at the data description
    that we looked at in the preceding section. The variable description for `Alley`
    shows that it has three levels: *gravel*, *paved*, and *no access*. In the original
    dataset, `''No Access''` is codified as `NA`. When `NA` is read in Python, it
    is treated as **NaN**, which means that a value is missing, so we need to be careful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will replace the missing values for `Alley` with a valid value, such
    as `''No Access''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s visualize the missing values and try to see how can we treat them. The following
    code generates a chart that showcases the spread of missing values. Here we use
    the `seaborn` library to plot the charts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The color of the map is generated with linearly increasing brightness by the
    `cubehelix_palette()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fce12232-d35b-4808-a677-f6aa0a48a877.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding plot, it is easier to read the spread of the missing values.
    The white marks on the chart indicate missing values. Notice that `Alley` no longer
    reports any missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '`LotFrontage` is a continuous variable and has **17.74%** of its values missing.
    Replace the missing values in this variable with its median as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s view the missing value plot once again to see if the missing values
    from `LotFrontage` have been imputed. Copy and execute the preceding code. The
    missing value plot will look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3fee1f9b-cda4-48f0-acaf-6dddba9faf97.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see in the preceding plot that there are no more missing values
    for `Alley` or `LotFrontage`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have figured out from the data description that several variables have values
    that are codified as `NA`. Because this is read in Python as missing values, we
    replace all of these with their actual values, which we get to see in the data
    description shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the missing value plot after having treated the preceding variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e0321ab2-6f0f-4f91-9157-699fdae69fed.png)'
  prefs: []
  type: TYPE_IMG
- en: We notice from the preceding plot that there are no more missing values for
    the variables that we have just treated. However, we are left with a few missing
    values in `MasVnrType`, `MasVnrArea`, and `Electrical`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to look at the distribution of `MasVnrType` by `MasVnrArea` with
    a crosstab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that when `MasVnrArea` is zero, we have `MasVnrType`
    as `None` in the majority of cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a65a465b-030d-4368-8a76-e1c0e6b36702.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will then impute the missing values in `MasVnrType` with `None` and `MasVnrArea`
    with zero. This is done with the commands shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We are still left with one missing value in the `Electrical` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the observation where `Electrical` has a missing value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9dbf6447-f753-49e3-8e0a-7a674d0c92d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We see that `MSSubClass` is `80` when `Electrical` is null. Let''s see the
    distribution of the `Electrical` type by `MSSubClass`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'From the following output, we can see that when `MSSubClass` is `80`, the majority of
    cases of the `Electrical` type are `SBrkr`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba143935-687a-420c-955c-06f40a4920d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Go ahead and impute the missing value in the `Electrical` variable with `SBrKr` by
    executing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, let''s take a look at our missing value plot for a final time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we get can be seen in the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce0bfa9e-4173-4621-9497-9242509f5889.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the plot has changed and now shows no missing values in our DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1 *and *Step **2*, we looked at the variables with missing values in
    absolute and percentage terms. We noticed that the `Alley` variable had more than
    **93%** of its values missing. However, from the data description, we figured
    out that the `Alley` variable had a **No Access to Alley **value, which is codified
    as `NA` in the dataset. When this value was read in Python, all instances of `NA`
    were treated as missing values. In *Step 3*, we replaced the `NA` in `Alley` with
    `No Access`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we used `%matplotlib inline` in *Step 2*. This is a magic function
    that renders the plot in the notebook itself.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we used the `seaborn` library to plot the missing value chart.
    In this chart, we identified the variables that had missing values. The missing
    values were denoted in white, while the presence of data was denoted in color.
    We noticed from the chart that `Alley` had no more missing values.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we used `cubehelix_palette()` from the `seaborn` library,  which produces
    a color map with linearly decreasing (or increasing) brightness. The `seaborn`
    library also provides us with options including `light_palette()` and `dark_palette()`.
    `light_palette()` gives a sequential palette that blends from light to color,
    while `dark_palette()` produces a sequential palette that blends from dark to
    color.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we noticed that one of the numerical variables, `LotFrontage`,
    had more than **17%** of its values missing. We decided to impute the missing
    values with the median of this variable. We revisited the missing value chart
    in *Step 6* to see whether the variables were left with any missing values. We
    noticed that `Alley` and `LotFrontage` showed no white marks, indicating that
    neither of the two variables had any further missing values.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 7*, we identified a handful of variables that had data codified with
    `NA`. This caused the same problem we encountered previously, as Python treated
    them as missing values. We replaced all such codified values with actual information.
  prefs: []
  type: TYPE_NORMAL
- en: We then revisited the missing value chart in *Step 8*. We saw that almost all
    the variables then had no missing values, except for `MasVnrType`, `MasVnrArea`,
    and `Electrical`.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 9* and *10*, we filled in the missing values for the `MasVnrType` and
    `MasVnrArea` variables. We noticed that `MasVnrType` is `None` whenever `MasVnrArea`
    is `0.0`, except for some rare occasions. So, we imputed the `MasVnrType` variable
    with `None`, and `MasVnrArea` with `0.0` wherever those two variables had missing
    values. We were then only left with one variable with missing values, `Electrical`.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 11*, we looked at what type of house was missing the `Electrical` value. We
    noticed that `MSSubClass` denoted the dwelling type and, for the missing `Electrical `value,
    the `MSSubClass` was `80`, which meant it was split or multi-level. In *Step 12*,
    we checked the distribution of `Electrical` by the dwelling type, which was `MSSubClass`.
    We noticed that when `MSSubClass` equals `80`, the majority of the values of `Electrical`
    are `SBrkr`, which stands for standard circuit breakers and Romex. For this reason,
    we decided to impute the missing value in `Electrical` with `SBrkr`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in *Step 14*, we again revisited the missing value chart and saw that
    there were no more missing values in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the preceding plots and missing value charts, it was easy to figure out
    the count, percentage, and spread of missing values in the datasets. We noticed
    that many variables had missing values for the same observations. However, after
    consulting the data description, we saw that most of the missing values were actually
    not missing, but since they were codified as `NA`, pandas treated them as missing
    values.
  prefs: []
  type: TYPE_NORMAL
- en: It is very important for data analysts to understand data descriptions and treat
    the missing values appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, missing data is categorized into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing completely at random** (**MCAR**): MCAR denotes that the missing
    values have nothing to do with the object being studied. In other words, data
    is MCAR when the probability of missing data on a variable is not related to other
    measured variables or to the values themselves. An example of this could be, for
    instance, the age of certain respondents to a survey not being recorded, purely
    by chance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing at random** (**MAR**): The name MAR is a little misleading here because
    the absence of values is not random in this case. Data is MAR if its absence is
    related to other observed variables, but not to the underlying values of the data
    itself. For example, when we collect data from customers, rich customers are less
    likely to disclose their income than their other counterparts, resulting in MAR
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing not at random** (**MNAR**): Data is MNAR, also known as **non-ignorable** if
    it can''t be classified as MCAR nor MAR. For example, perhaps some consumers don''t
    want to share their age when it is above 40 because they would like to hide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are various strategies that can be applied to impute the missing values,
    as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Source the missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leave out incomplete observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace missing data with an estimate, such as a mean or a median
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimate the missing data from other variables in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scikit-learn module for imputation ([https://bit.ly/2MzFwiG](https://bit.ly/2MzFwiG))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple imputation by chained equations using the `StatsModels` library in
    Python ([https://bit.ly/2PYLuYy](https://bit.ly/2PYLuYy))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature imputation algorithms using fancyimpute ([https://bit.ly/2MJKfOY](https://bit.ly/2MJKfOY))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will continue from where we left off in the previous section, on analyzing
    and treating missing values. Data scientists spend the majority of their time
    doing data preparation and exploration, not model building and optimization. Now
    that our dataset has no missing values, we can proceed with our exploratory data
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first section on data manipulation, we saw the summary statistics for
    our datasets. However, we have not looked at this since imputing the missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s now look at the data and its basic statistics using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: With the preceding code, we can see the summary statistics of the variables
    in the earlier section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now let''s see how many columns there are by datatype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows us how many variables there are for each datatype.
    We can see that we have 3 float-type variables, 33 integer-type variables, 45
    object-type variables, and 4 unsigned integers that hold the one-hot encoded values
    for the `LotShape` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90ebfe78-e84f-406b-9455-c774e6817f53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s create two variables to hold the names of the numerical and categorical
    variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us the amount of numerical and categorical variables there are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2029e24-dcf1-403a-b0f3-11f5a2444dfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now use the `numerical_features` variable that we previously created
    to see the distributions of numerical variables. We will use the `seaborn` library
    to plot our charts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the `melt()` method from pandas to reshape our DataFrame. You may want
    to view the reshaped data after using the `melt()` method to understand how the DataFrame
    is arranged.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows us the univariate distribution of the observations
    of numerical variables using distribution plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7c5098f-9d3f-4ddd-b430-a60b5a8883c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we use the `categorical_features` variable to plot the distribution of
    house prices by each categorical variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In our dataset, we see that various attributes are present that can drive house
    prices. We can try to see the relationship between the attributes and the `SalesPrice`
    variable, which indicates the prices of the houses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the distribution of the house sale prices by each categorical variable
    in the following plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f69cc0a0-e586-4c9f-8e8a-74fd2816c47e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now take a look at the correlation matrix for all numerical variables
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e63dc46-aceb-4074-b6c0-5c5ec3261313.png)'
  prefs: []
  type: TYPE_IMG
- en: It might be tough to view the correlations displayed in the preceding format.
    You might want to take a look at the correlations graphically.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also view the correlation matrix plot for the numerical variables. In
    order to do this, we use the `numerical_features` variable that we created in
    *Step 3* to hold the names of all the numerical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we used `select_dtypes(include=[np.number])` to create
    the `df_numeric_features` variable. However, in *Step 3*, we used `dtypes[housepricesdata.dtypes
    != "object"].index`. Note that `select_dtypes()` returns a `pandas.DataFrame`,
    whereas `dtypes[].index` returns a `pandas.Index` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now visualize the correlation plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43b35176-1948-4674-89e9-07bfac1934cd.png)'
  prefs: []
  type: TYPE_IMG
- en: '`cmap` is a Matplotlib color map object.There are various categories of color
    map, including sequential, diverging, and qualitative. Among the sequential colors,
    you may choose to set your `cmap` parameter to `BuPu` or `YlGn`. For qualitative
    colors, you can set it to values such as `Set3`, `Pastel2`, and so on. More information
    on color options can be found at [https://matplotlib.org/tutorials/colors/colormaps.html](https://matplotlib.org/tutorials/colors/colormaps.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may also want to evaluate the correlation of your numerical variables with
    `SalePrice` to see how these numerical variables are related to the prices of
    the houses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows us the correlation plots. Here, we plot the
    correlation between each of the numerical variables and `SalePrice`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a52456a-94c6-48ac-a721-8808c27fbaff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you want to evaluate the correlation of your numerical variables with the
    sale prices of the houses numerically, you can use the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You can view the correlation output sorted in a descending manner in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed9ccd39-d221-4fd8-b335-5dd28ab8cee9.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we started by reading and describing our data. This step provided
    us with summary statistics for our dataset. We looked at the number of variables
    for each datatype in *Step 2*.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, we created two variables, namely, `numerical_features` and `categorical_features`,
    to hold the names of numerical and categorical variables respectively. We used
    these two variables in the steps when we worked with numerical and categorical
    features separately.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step *4 and *Step *5, we used the `seaborn` library to plot our charts.
    We also introduced the `melt()` function from pandas, which can be used to reshape
    our DataFrame and feed it to the `FacetGrid()` function of the `seaborn` library.
    Here, we showed how you can paint the distribution plots for all the numerical variables
    in one single go. We also showed you how to use the same `FacetGrid()` function
    to plot the distribution of `SalesPrice` by each categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: We generated the correlation matrix in *Step 6* using the `corr()` function
    of the DataFrame object. However, we noticed that with too many variables, the
    display does not make it easy for you to identify the correlations. In *Step 7*,
    we plotted the correlation matrix heatmap by using the `heatmap()` function from
    the `seaborn` library.
  prefs: []
  type: TYPE_NORMAL
- en: The `corr()` function computes the pairwise correlation of variables, excluding
    the missing values. The `pearson` method is used as the default for computing
    the correlation. You can also use the `kendall` or `spearman` methods, depending
    on your requirements. More information can be found at [https://bit.ly/2CdXr8n](https://bit.ly/2CdXr8n).
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 8*, we saw how the numerical variables correlated with the sale prices
    of houses using a scatter plot matrix. We generated the scatter plot matrix using
    the `regplot()` function from the `seaborn` library. Note that we used a parameter, `fit_reg=False`,
    to remove the regression line from the scatter plots.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 9*, we repeated *Step 8* to see the relationship of the numerical variables
    with the sale prices of the houses in a numerical format, instead of scatter plots.
    We also sorted the output in descending order by passing a `[::-1]` argument to
    the `corr()` function.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen a few ways to explore data, both statistically and visually. There
    are quite a few libraries in Python that you can use to visualize your data. One
    of the most widely used of these is `ggplot`. Before we look at a few commands,
    let's learn how `ggplot` works.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are seven layers of grammatical elements in `ggplot`, out of which, first
    three layers are mandatory:'
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aesthetics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theme
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will often start by providing a dataset to `ggplot()`. Then, you provide
    an aesthetic mapping with the `aes()` function to map the variables to the *x *and
    *y* axes. With `aes()`, you can also set the color, size, shape, and position of
    the charts. You then add the type of geometric shape you want with functions such
    as `geom_point()` or `geom_histogram()`. You can also add various options, such
    as plotting statistical summaries, faceting, visual themes, and coordinate systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is an extension to what we have used already in this chapter,
    so we will directly delve into the `ggplot` code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33685075-4116-479c-9e26-f86291e74187.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, in order to view the density plot for the numerical variables, we
    can execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot shows us the univariate density plot for each of our numerical variables.
    The `geom_density()` computes and draws a kernel density estimate, which is a
    smoothed version of the histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77c12994-1627-466f-a8f9-e17432430292.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The guide to the `seaborn` library ([https://bit.ly/2iU2aRU](https://bit.ly/2iU2aRU))
  prefs: []
  type: TYPE_NORMAL
