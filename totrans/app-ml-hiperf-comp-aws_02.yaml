- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Data Management and Transfer
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管理和传输
- en: In [*Chapter 1*](B18493_01.xhtml#_idTextAnchor015), *High-Performance Computing
    Fundamentals*, we introduced the concepts of HPC applications, why we need HPC,
    and its use cases across different industries. Before we begin developing HPC
    applications, we need to migrate the required data into the cloud. In this chapter,
    we will uncover some of the challenges in managing and transferring data to the
    cloud and the ways to mitigate them. We will dive deeper into the services in
    **AWS online and offline data transfer services** using which you can securely
    transfer data to the AWS cloud, while maintaining data integrity and consistency.
    We will cover different data transfer scenarios and provide guidance on how to
    select the right service for each one.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B18493_01.xhtml#_idTextAnchor015)《高性能计算基础》中，我们介绍了HPC应用的概念、为什么我们需要HPC以及它在不同行业的应用案例。在我们开始开发HPC应用之前，我们需要将所需数据迁移到云端。在本章中，我们将揭示管理和传输数据到云端的一些挑战以及缓解这些挑战的方法。我们将深入了解**AWS在线和离线数据传输服务**，使用这些服务您可以安全地将数据传输到AWS云端，同时保持数据完整性和一致性。我们将涵盖不同的数据传输场景，并提供如何为每个场景选择正确服务的指导。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Importance of data management
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管理的重要性
- en: Challenges of moving data into the cloud
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据迁移到云端的挑战
- en: How to securely transfer large amounts of data into the cloud
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何安全地将大量数据传输到云端
- en: AWS online data transfer services
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS在线数据传输服务
- en: AWS offline data transfer services
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS离线数据传输服务
- en: These topics will help you understand how you can transfer **Gigabytes** (**GB**),
    **Terabytes** (**TB**), or **Petabytes** (**PB**) of data onto the cloud with
    minimal disruption, cost, and time involved.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '这些主题将帮助您了解如何以最小的干扰、成本和时间将**千兆字节**（**GB**）、**兆字节**（**TB**）或**拍字节**（**PB**）的数据传输到云端。 '
- en: Let’s get started with data management and its role in HPC applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始了解数据管理及其在HPC应用中的作用。
- en: Importance of data management
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管理的重要性
- en: '**Data management** is the process of effectively capturing, storing, and collating
    data created by different applications in your company to make sure it’s accurate,
    consistent, and available when needed. It includes developing policies and procedures
    for managing your end-to-end data life cycle. The following are some of the elements
    of the data life cycle specific to HPC applications, due to which it’s important
    to have data management policies in place:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据管理**是有效捕捉、存储和整理公司内不同应用程序创建的数据的过程，以确保在需要时数据准确、一致且可用。它包括制定管理您端到端数据生命周期的政策和程序。以下是一些特定于高性能计算（HPC）应用的数据生命周期元素，因此建立数据管理政策至关重要：'
- en: Cleaning and transforming raw data to perform detailed faultless analysis.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗和转换原始数据以执行详细的无误分析。
- en: Designing and building data pipelines to automatically transfer data from one
    system to another.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和构建数据管道以自动将数据从一个系统传输到另一个系统。
- en: '**Extracting, Transforming, and Loading** (**ETL**) data into appropriate data
    storage systems such as databases, data warehouses, and object storage or filesystems
    from disparate data sources.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取、转换和加载**（**ETL**）数据到适当的数据存储系统，例如数据库、数据仓库、对象存储或文件系统，这些系统来自不同的数据源。'
- en: Building data catalogs for storing metadata to make it easier to find and track
    the data lineage.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建数据目录以存储元数据，使其更容易找到和跟踪数据血缘。
- en: Following policies and procedures as outlined by your data governance model.
    This also involves conforming to the compliance requirements of the federal and
    regional authorities of the country where data is being captured and stored. For
    example, if you are a healthcare organization in California, United States, you
    would need to follow both federal and state data privacy laws, including the **Health
    Insurance Portability and Accountability Act** (**HIPAA**) and California’s health
    data privacy law, the **Confidentiality of Medical Information Act** (**CMIA**). Additionally,
    you would also need to follow the **California Consumer Privacy Act** (**CCPA**),
    which came into effect starting January 1, 2020, as it relates to healthcare data.
    If you are in Europe, you would have to follow the data guidelines governed by
    the European Union’s **General Data Protection Regulation** (**GDPR**).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵循您数据治理模型中概述的政策和程序。这也包括遵守数据被捕获和存储的国家联邦和地区当局的合规要求。例如，如果您是美国加利福尼亚州的一家医疗保健组织，您将需要遵守联邦和州的数据隐私法，包括**健康保险可携带性和问责制法案**（**HIPAA**）和加利福尼亚州的健康数据隐私法，即**医疗信息保密法案**（**CMIA**）。此外，您还需要遵守**加利福尼亚消费者隐私法案**（**CCPA**），该法案自2020年1月1日起生效，与医疗数据相关。如果您在欧洲，您将必须遵守欧盟的**通用数据保护条例**（**GDPR**）规定的数据指南。
- en: Protecting your data from unauthorized access, while at rest or in transit.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据静止或传输过程中保护您的数据不受未经授权的访问。
- en: Now that we have understood the significance of data management in HPC applications,
    let’s see some of the challenges of transferring large amounts of data into the
    cloud.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了数据管理在HPC应用程序中的重要性，让我们看看将大量数据传输到云中的挑战。
- en: Challenges of moving data into the cloud
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据迁移到云中的挑战
- en: 'In order to start building HPC applications on the cloud, you need to have
    data on the cloud, and also think about the various elements of your data life
    cycle in order to be able to manage data effectively. One way is to write custom
    code for transferring data, which will be time-consuming and might involve the
    following challenges:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始在云上构建高性能计算（HPC）应用程序，您需要在云上拥有数据，并且还需要考虑您数据生命周期的各个方面，以便能够有效地管理数据。一种方法是编写用于传输数据的自定义代码，这将耗时且可能涉及以下挑战：
- en: Preserving the permissions and metadata of files.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留文件的权限和元数据。
- en: Making sure that data transfer does not impact other existing applications in
    terms of performance, availability, and scalability, especially in the case of
    online data transfer (transferring data over the network).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据传输不会影响其他现有应用程序的性能、可用性和可伸缩性，尤其是在在线数据传输（通过网络传输数据）的情况下。
- en: Scheduling data transfer for non-business hours to ensure other applications
    are not impeded.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非工作时间安排数据传输，以确保不会阻碍其他应用程序。
- en: In terms of structured data, you might have to think about schema conversion
    and database migration.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在结构化数据方面，您可能需要考虑模式转换和数据库迁移。
- en: Maintaining data integrity and validating the transfer.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护数据完整性和验证传输。
- en: Monitoring the status of the data transfer, having the ability to look up the
    history of previous transfers, and having a retry mechanism in place to ensure
    successful transfers.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控数据传输的状态，能够查找先前传输的历史记录，并实施重试机制以确保传输成功。
- en: Making sure there are no duplicates – once the data has been transferred, the
    system should not trigger the transfer again.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保没有重复项——一旦数据已传输，系统不应再次触发传输。
- en: Protecting data during the transfer, which will include encrypting data both
    in transit and at rest.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在传输过程中保护数据，这包括在传输中和静止时加密数据。
- en: Ensuring data arrives intact and is not corrupted. You would need a mechanism
    to check that the data arriving at the destination matches the data read from
    the source to validate data consistency.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据完整到达且未被损坏。您需要一个机制来检查到达目的地的数据与从源读取的数据匹配，以验证数据一致性。
- en: Last but not least, you would have to manage, version-control, and optimize
    your data-copying scripts.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，您将不得不管理、版本控制和优化您的数据复制脚本。
- en: The data transfer and migration services offered by AWS can assist you in securely
    transferring data to the cloud without you having to write and manage code, helping
    you overcome these aforementioned challenges. In order to select the right service
    based on your business requirement, you first need to build a data transfer strategy.
    We will discuss the AWS data transfer services in a subsequent section of this
    chapter. Let’s first understand the items that you need to consider while building
    your strategy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供的数据传输和迁移服务可以帮助你在不编写和管理代码的情况下安全地将数据传输到云端，帮助你克服上述挑战。为了根据你的业务需求选择正确的服务，你首先需要构建一个数据传输策略。我们将在本章的下一节讨论AWS数据传输服务。让我们首先了解在构建你的策略时需要考虑的项目。
- en: 'In a nutshell, your data transfer strategy needs to take the following into
    account in order to move data with minimal disruption, time, and cost:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，你的数据传输策略需要考虑以下因素，以便以最小的干扰、时间和成本来迁移数据：
- en: What kind of data do you need for developing your HPC application – for example,
    structured data, unstructured data (such as images and PDF documents), or a combination
    of both?
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在开发HPC应用时需要哪种类型的数据——例如，结构化数据、非结构化数据（如图像和PDF文档），或者两者的组合？
- en: For unstructured data, which filesystem do you use for storing your files currently?
    Is it on **Network Attached Storage** (**NAS**) or **Storage Area Network** (**SAN**)?
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于非结构化数据，你目前使用哪种文件系统来存储文件？是在**网络附加存储**（**NAS**）还是**存储区域网络**（**SAN**）上？
- en: How much purchased storage is available right now, and for how long will it
    last based on the rate of growth of your data, before you plan to buy more storage?
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前购买了多少存储空间可用，根据你数据增长的速度，在计划购买更多存储之前，这些存储空间还能使用多久？
- en: For structured data, which database do you use?
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于结构化数据，你使用哪种数据库？
- en: Are you tied up in database licenses? If yes, when are they due for renewal,
    and what are the costs of the licenses?
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否受限于数据库许可证？如果是，它们何时到期，许可证的成本是多少？
- en: What is the volume of data that you need to transfer to the cloud?
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要将多少数据传输到云端？
- en: What are the other applications that are using this data?
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些数据的其他应用有哪些？
- en: Do these applications require local access to data? Will there be any performance
    impact on the existing applications if the data is moved to the cloud?
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些应用是否需要本地访问数据？如果数据迁移到云端，现有的应用会有任何性能影响吗？
- en: What is your network bandwidth? Is it good enough to transfer data over the
    network?
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的网络带宽是多少？是否足够用于网络数据传输？
- en: How quickly do you need to move your data to the cloud?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要多快地将数据迁移到云端？
- en: Based on the answers to these questions, you can create your data strategy and
    select appropriate AWS services that will help you to transfer data with ease
    and mitigate the challenges mentioned in the preceding list. To understand it
    better, let’s move to the next topic and see how to securely transfer large amounts
    of data into the cloud with a simple example.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些问题的答案，你可以制定你的数据策略，并选择合适的AWS服务，帮助你轻松地迁移数据并减轻前面提到的挑战。为了更好地理解，让我们转到下一个主题，看看如何通过一个简单的示例来安全地将大量数据传输到云端。
- en: How to securely transfer large amounts of data into the cloud
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何安全地将大量数据传输到云端
- en: 'To understand this topic, let’s start with a simple example where you want
    to build and train a computer vision deep learning model to detect product defects
    in your manufacturing production line. You have cameras installed on each production
    line, which capture hundreds of images each day. Each image can be up to 5 MB
    in size, and you have about 1 TB of data, which is currently stored on-premises
    in a NAS filesystem that you want to use to train your machine learning model.
    You have about 1 Gbps of network bandwidth and need to start training your model
    in 2-4 weeks. There is no impact on other applications if the data is moved to
    the cloud and no structured data is needed for building the computer vision model.
    Let’s rearrange this information into the following structure, which will become
    part of your data strategy document:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个主题，让我们从一个简单的例子开始，您想要构建和训练一个计算机视觉深度学习模型来检测您的制造生产线上的产品缺陷。您在每个生产线上安装了摄像头，每天可以捕获数百张图片。每张图片的大小可以达到5
    MB，您大约有1 TB的数据，目前存储在本地NAS文件系统中，您希望使用这些数据来训练您的机器学习模型。您大约有1 Gbps的网络带宽，需要在2-4周内开始训练模型。如果数据移动到云中，不会对其他应用程序产生影响，并且构建计算机视觉模型不需要结构化数据。让我们将以下信息重新组织成以下结构，这将成为您的数据策略文档的一部分：
- en: '**Objective**: To transfer 1 TB of image data to the cloud, where the file
    size can be up to 5 MB. Need to automate the data transfer to copy about 10 GB
    of images every night to the cloud. Additionally, need to preserve the metadata
    and file permissions while copying data to the cloud.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**: 将1 TB的图像数据传输到云中，其中文件大小可以达到5 MB。需要自动化数据传输，每晚复制大约10 GB的图片到云中。此外，在复制数据到云中时需要保留元数据和文件权限。'
- en: '**Timeline**: 2-4 weeks'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间表**: 2-4周'
- en: '**Data type**: Unstructured data – JPG or PNG format image files'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据类型**: 非结构化数据 – JPG或PNG格式的图像文件'
- en: '**Dependency**: None'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖项**: 无'
- en: '**Impact on existing applications**: None'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对现有应用程序的影响**: 无'
- en: '**Network bandwidth**: 1 Gbps'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络带宽**: 1 Gbps'
- en: '**Existing storage type**: Network attached storage'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现有存储类型**: 网络附加存储'
- en: '**Purpose of data transfer**: To perform distributed training on a computer
    vision deep learning model using multiple GPUs'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据传输目的**: 使用多个GPU在计算机视觉深度学习模型上执行分布式训练'
- en: '**Data destination**: Amazon S3, which is secure, durable, and the most cost-effective
    object storage on AWS for storing large amounts of data'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据目的地**: 亚马逊S3，它是AWS上存储大量数据的最高效、最安全且最具成本效益的对象存储'
- en: '**Sensitive data**: None, but data should not be available for public access'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**敏感数据**: 无，但数据不应可供公众访问'
- en: '**Local data access**: Not required'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地数据访问**: 不需要'
- en: Since you have 5 TB of data with a maximum file size of 5 MB to transfer securely
    to Amazon S3, you can use the AWS DataSync service. It is an AWS online data transfer
    service to migrate data securely using a **Virtual Private Cloud** (**VPC**) endpoint
    to avoid your data going through the open internet. We will discuss all the AWS
    data transfer services in detail in the later sections of this chapter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您有5 TB的数据，最大文件大小为5 MB，需要安全地传输到亚马逊S3，您可以使用AWS DataSync服务。这是一个AWS在线数据传输服务，通过使用**虚拟专用云**（**VPC**）端点迁移数据，以避免您的数据通过公开互联网。我们将在本章后面的部分详细讨论所有AWS数据传输服务。
- en: 'The following architecture visually depicts how the transfer will take place:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的架构图直观地展示了数据传输的过程：
- en: '![Figure 2.1 – Data transfer using AWS DataSync with a VPC endpoint](img/B18493_02_001.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 使用AWS DataSync和VPC端点进行数据传输](img/B18493_02_001.jpg)'
- en: Figure 2.1 – Data transfer using AWS DataSync with a VPC endpoint
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 使用AWS DataSync和VPC端点进行数据传输
- en: The AWS DataSync agent transfers the data between your local storage, NAS in
    this case, and AWS. You deploy the agent in a **Virtual Machine** (**VM**) in
    your on-premises network, where your data source resides. With this approach,
    you can minimize the network overhead while transferring data using the **Network
    File System** (**NFS**) and **Server Message Block** (**SMB**) protocols.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DataSync代理在您的本地存储（在这种情况下为NAS）和AWS之间传输数据。您在本地网络中的虚拟机（**VM**）上部署代理，其中您的数据源位于。采用这种方法，您可以在使用**网络文件系统**（**NFS**）和**服务器消息块**（**SMB**）协议传输数据时最小化网络开销。
- en: Let’s take a deeper look into AWS DataSync in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节更深入地了解AWS DataSync。
- en: AWS online data transfer services
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS在线数据传输服务
- en: 'Online data transfer services are out-of-the-box solutions built by AWS for
    transferring data between on-premises systems and the AWS cloud via the internet.
    They include the following services:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在线数据传输服务是 AWS 为通过互联网在本地系统与 AWS 云之间传输数据而构建的即用型解决方案。它们包括以下服务：
- en: AWS DataSync
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS DataSync
- en: AWS Transfer Family
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Transfer Family
- en: Amazon S3 Transfer Acceleration
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon S3 转加速
- en: Amazon Kinesis
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Kinesis
- en: AWS Snowcone
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Snowcone
- en: Let’s look at each of these services in detail to understand the scenarios in
    which we can use the relevant services.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解一下这些服务，以了解我们可以使用相关服务的场景。
- en: AWS DataSync
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS DataSync
- en: 'AWS DataSync helps you overcome the challenges of transferring data from on-premises
    to AWS storage services and between AWS storage services in a fast and secure
    fashion. It also enables you to automate or schedule the data transfer to optimize
    your use of network bandwidth, which might be shared with other applications.
    You can monitor the data transfer task, add data integrity checks to make sure
    that the data transfer was successful, and validate that data was not corrupted
    during the transfer, while preserving the file permissions and associated metadata.
    DataSync offers integration with multiple filesystems and enables you to transfer
    data between the following resources:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DataSync 帮助您以快速、安全的方式克服从本地到 AWS 存储服务以及 AWS 存储服务之间传输数据的挑战。它还使您能够自动化或安排数据传输以优化网络带宽的使用，这可能与其他应用程序共享。您可以监控数据传输任务，添加数据完整性检查以确保数据传输成功，并验证数据在传输过程中未被损坏，同时保留文件权限和相关元数据。DataSync
    提供与多个文件系统的集成，并允许您在以下资源之间传输数据：
- en: 'On-premises file servers and object storage:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地文件服务器和对象存储：
- en: NFS file servers
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFS 文件服务器
- en: SMB file servers
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMB 文件服务器
- en: '**Hadoop Distributed File System** (**HDFS**)'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop 分布式文件系统** (**HDFS**)'
- en: Self-managed object storage
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自管理对象存储
- en: 'AWS storage services:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 存储服务：
- en: '**Snow Family Devices**'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Snow Family Devices**'
- en: '**Amazon Simple Storage Service** (**Amazon S3**) buckets'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Simple Storage Service** (**Amazon S3**) 存储桶'
- en: '**Amazon Elastic File System** (**EFS**)'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Elastic File System** (**EFS**)'
- en: '**Amazon FSx for Windows File Server**'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon FSx for Windows File Server**'
- en: '**Amazon FSx for Lustre** filesystems'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon FSx for Lustre** 文件系统'
- en: Important note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We will discuss AWS storage services in detail in [*Chapter 4*](B18493_04.xhtml#_idTextAnchor074),
    *Data Storage*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [*第 4 章*](B18493_04.xhtml#_idTextAnchor074)，*数据存储* 中详细讨论 AWS 存储服务。
- en: Use cases
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用例
- en: 'As discussed, AWS DataSync is used for transferring data to the cloud over
    the network. Let’s now see some of the specific use cases for which you can use
    DataSync:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，AWS DataSync 用于通过网络传输数据到云。现在让我们看看一些可以使用 DataSync 的具体用例：
- en: Hybrid cloud workloads, where data is generated by on-premises applications
    and needs to be moved to and from the AWS cloud for processing. This can include
    HPC applications in healthcare, manufacturing, life sciences, big data analytics
    in financial services, and research purposes.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合云工作负载，其中数据由本地应用程序生成，需要将数据从本地移动到 AWS 云进行处理，以及从 AWS 云返回本地。这可能包括医疗保健、制造、生命科学中的高性能计算应用、金融服务中的大数据分析以及研究目的。
- en: Migrate data rapidly over the network into AWS storage services such as Amazon
    S3, where you need to make sure that data arrives securely and completely. DataSync
    has encryption and data integrity during transfer enabled by default. You can
    also choose to enable additional data verification checks to compare the source
    and destination data.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据快速通过网络迁移到 AWS 存储服务，如 Amazon S3，您需要确保数据安全且完整地到达。DataSync 默认启用了传输过程中的加密和数据完整性。您还可以选择启用额外的数据验证检查，以比较源数据和目标数据。
- en: Data archiving, where you want to archive the infrequently accessed data (cold
    data) directly into durable and long-term storage in the AWS cloud, such as **Amazon
    S3 Glacier** or **S3 Glacier Deep Archive**. This helps you to free up your on-premises
    storage capacity and reduce costs.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据归档，您希望将不常访问的数据（冷数据）直接存入 AWS 云中持久和长期存储，例如 **Amazon S3 Glacier** 或 **S3 Glacier
    Deep Archive**。这有助于您释放本地存储容量并降低成本。
- en: Scheduling a data transfer job to automatically start on a recurring basis at
    a particular time of the day to optimize network bandwidth usage, which might
    be shared with other applications. For example, in the life sciences domain, you
    may want to upload genomic data generated by on-premises applications for processing
    and training machine learning models on a daily basis. You can both schedule data
    transfer tasks and monitor them as required using DataSync.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workings of AWS DataSync
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use an architecture diagram to show how DataSync can transfer data between
    on-premises self-managed storage systems to AWS storage services and between AWS
    storage resources.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: We will start with on-premises storage to AWS storage services.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Data transfer from on-premises to AWS storage services
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The architecture in *Figure 2.2* depicts the data transfer from on-premises
    to AWS storage resources:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Data transfer from on-premises to AWS storage services using
    AWS DataSync](img/B18493_02_002.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Data transfer from on-premises to AWS storage services using AWS
    DataSync
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The **DataSync agent** is a VM that reads from and writes the data to the on-premises
    storage. You can configure and activate your agent using the DataSync console
    or API. This process associates your agent with your AWS account. Once the agent
    is activated, you can create the data transfer task from the console or API to
    kick start the data transfer. DataSync encrypts and performs a data integrity
    check during transfer to make sure that data is transferred securely. You can
    enable additional checks as well to verify the data copied to the destination
    is the same as that read at the source. Additionally, you can also monitor your
    data transfer task. The time that DataSync takes to transfer depends on your network
    bandwidth, the amount of data, and the network traffic. However, a single data
    transfer task is capable of utilizing a 10-Gbps network link.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Data transfer between AWS storage resources
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s take a deeper look to understand the data transfer between AWS storage
    resources using AWS DataSync.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture in *Figure 2.3* depicts the data transfer between AWS storage
    resources using DataSync in the same AWS account. The same architecture applies
    for data transfers within the same region as well as cross-region:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Data transfer between AWS storage resources using AWS DataSync](img/B18493_02_003.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Data transfer between AWS storage resources using AWS DataSync
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the architecture, DataSync does not use the agent for transferring
    data between AWS resources in the same account. However, if you want to transfer
    data between different AWS accounts, then you need to set up and activate the
    DataSync Amazon EC2 agent in an AWS Region.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: In summary, you can use AWS DataSync for online data transfer from on-premises
    to AWS storage services, and between AWS storage resources. AWS DataSync transfers
    data quickly, safely, and in a cost-effective manner while ensuring data integrity
    and consistency, without the need to write and manage data-copy scripts.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，您可以使用AWS DataSync进行从本地到AWS存储服务的在线数据传输，以及AWS存储资源之间的数据传输。AWS DataSync快速、安全且经济高效地传输数据，同时确保数据完整性和一致性，无需编写和管理数据复制脚本。
- en: Now, let’s move on to another AWS data transfer service, AWS Transfer Family,
    which is used for scaling your recurring business-to-business file transfers to
    Amazon S3 and Amazon EFS.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续介绍另一个AWS数据传输服务——AWS Transfer Family，它用于扩展您定期进行的商业对商业文件传输到Amazon S3和Amazon
    EFS。
- en: AWS Transfer Family
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Transfer Family
- en: File transfer protocols such as **File Transfer Protocol** (**FTP**), **Secure
    File Transfer Protocol** (**SFTP**), and **File Transfer Protocol Secure** (**FTPS**)
    are commonly used in business-to-business data exchange workflows across different
    industries including financial services, healthcare, manufacturing, and retail.
    AWS Transfer Family helps in scaling and migrating these file workflows to the
    AWS cloud. It uses the FTP, SFTP, and FTPS protocols for data transfer. It enables
    you to transfer files to and from Amazon EFS and Amazon S3\.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 文件传输协议，如**文件传输协议**（**FTP**）、**安全文件传输协议**（**SFTP**）和**安全文件传输协议**（**FTPS**），在包括金融服务、医疗保健、制造业和零售在内的不同行业的商业对商业数据交换工作流程中普遍使用。AWS
    Transfer Family有助于扩展和迁移这些文件工作流程到AWS云。它使用FTP、SFTP和FTPS协议进行数据传输。它使您能够将文件传输到和从Amazon
    EFS和Amazon S3。
- en: Use cases
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用例
- en: 'As discussed, AWS Transfer Family uses protocols such as FTP, SFTP, and FTPS
    for data exchange workflows in business-to-business contexts. So, let’s understand
    some of the common use cases for transferring data to and from Amazon S3 and Amazon
    EFS using AWS Transfer Family:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如讨论的那样，AWS Transfer Family使用FTP、SFTP和FTPS等协议，在商业对商业环境中进行数据交换工作流程。那么，让我们了解一些使用AWS
    Transfer Family将数据传输到和从Amazon S3和Amazon EFS的常见用例：
- en: Securely transfer files internally within your organization or with third-party
    vendors. Some industries, including financial services, life sciences, and healthcare,
    have to make sure to have a secure file transfer workflow in place due to the
    sensitive nature of the data and to comply with regulations such as **Payment
    Card Industry Data Security Standard** (**PCI DSS**), HIPPA, or GDPR, depending
    on their location.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的组织内部或与第三方供应商之间安全地传输文件。一些行业，包括金融服务、生命科学和医疗保健，由于数据的敏感性和遵守如**支付卡行业数据安全标准**（**PCI
    DSS**）、HIPAA或GDPR等法规的必要性，必须确保有安全的文件传输工作流程。
- en: To distribute subscription-based content to your customers. For example, BluTV
    is a famous subscription-based video-on-demand service in Turkey, which is available
    globally and caters to Turkish and Arabic speaking viewers. Previously, they self-managed
    their SFTP setup on the cloud, and ran into a lot of problems such as managing
    open source projects for mounting S3 to Amazon EC2 and scaling issues when additional
    resources were required. After moving their setup to the fully managed AWS Transfer
    Family for SFTP, they no longer have to monitor their file transfers, manage open
    source projects, or pay for unused resources.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了向您的客户分发基于订阅的内容。例如，BluTV是土耳其一家著名的基于订阅的视频点播服务，它在全球范围内可用，并为土耳其语和阿拉伯语观众提供服务。之前，他们自行管理云上的SFTP设置，遇到了很多问题，例如管理用于将S3挂载到Amazon
    EC2的开源项目，以及需要额外资源时的扩展问题。在将他们的设置迁移到完全管理的AWS Transfer Family SFTP之后，他们不再需要监控文件传输，管理开源项目，或为未使用的资源付费。
- en: For building a central repository of data (also known as a data lake) on AWS
    for storing both structured and unstructured data coming from disparate data sources
    and third parties such as vendors or partners. For example, FINRA, a government-authorized
    non-profit organization that oversees US stockbrokers, has a data lake on Amazon
    S3 as its central source of data. FINRA uses the AWS Transfer Family for SFTP
    service to alleviate operational overheads while maintaining a connection to their
    existing authentication systems for external users to avoid any disruption while
    migrating their SFTP services to AWS.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS 上建立一个数据仓库（也称为数据湖），用于存储来自不同数据源和第三方（如供应商或合作伙伴）的结构化和非结构化数据。例如，FINRA，一个由美国政府授权的非营利组织，负责监管美国股票经纪人，它在
    Amazon S3 上有一个数据湖作为其数据的主要来源。FINRA 使用 AWS Transfer Family 的 SFTP 服务来减轻运营负担，同时保持与现有身份验证系统连接，以便在将
    SFTP 服务迁移到 AWS 的过程中避免任何中断。
- en: Now that we have gone over some of the use cases for AWS Transfer Family, let’s
    see how it works.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了一些 AWS Transfer Family 的用例，让我们看看它是如何工作的。
- en: Workings of AWS Transfer Family
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Transfer Family 的工作原理
- en: 'The architecture in *Figure 2.4* shows how files are transferred using AWS
    Transfer Family from on-premises file servers to Amazon S3 or Amazon EFS, which
    can then be used for downstream file processing workflows such as content distribution,
    machine learning, and data analysis:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.4* 中的架构展示了如何使用 AWS Transfer Family 将文件从本地文件服务器传输到 Amazon S3 或 Amazon EFS，然后可以用于下游文件处理工作流程，例如内容分发、机器学习和数据分析：'
- en: '![Figure 2.4 – File transfer workflow using AWS Transfer Family](img/B18493_02_004.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – 使用 AWS Transfer Family 的文件传输工作流程](img/B18493_02_004.jpg)'
- en: Figure 2.4 – File transfer workflow using AWS Transfer Family
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 使用 AWS Transfer Family 的文件传输工作流程
- en: You can configure any standard file transfer protocol client such as WinSCP,
    FileZilla, or OpenSSH to initially transfer to Amazon S3 or EFS using AWS Transfer
    Family. It will first authenticate the user based on the identity provider type
    that you have configured, and once the user is authenticated, it will initiate
    the file transfer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以配置任何标准文件传输协议客户端，例如 WinSCP、FileZilla 或 OpenSSH，使用 AWS Transfer Family 首先传输到
    Amazon S3 或 EFS。它将首先根据您配置的身份提供者类型对用户进行身份验证，一旦用户通过身份验证，它将启动文件传输。
- en: So far, we have seen how we can transfer data using AWS DataSync and AWS Transfer
    Family over the network and understood their use cases and how these services
    work to transfer data securely while reducing the operational burden in a cost-effective
    manner. Let’s now see how we can accelerate the data transfer to S3 using Amazon
    S3 Transfer Acceleration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何使用 AWS DataSync 和 AWS Transfer Family 通过网络传输数据，并了解了它们的用例以及这些服务如何以经济高效的方式在安全传输数据的同时减少运营负担。现在让我们看看如何使用
    Amazon S3 传输加速来加速数据传输到 S3。
- en: Amazon S3 Transfer Acceleration
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon S3 传输加速
- en: '**Amazon S3 Transfer Acceleration** (**S3TA**) is a feature in Amazon S3 buckets
    that lets you speed up your data transfer to a S3 bucket over long distances,
    regardless of internet traffic and without the need for any special clients or
    proprietary network protocols. You can speed up transfers to and from Amazon S3
    by 50-500% using the transfer acceleration feature.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon S3 传输加速**（**S3TA**）是 Amazon S3 存储桶中的一个功能，允许您在长距离上加速数据传输到 S3 存储桶，无论互联网流量如何，无需任何特殊客户端或专有网络协议。您可以使用传输加速功能将到和从
    Amazon S3 的传输速度提高 50-500%。'
- en: 'Some of the use cases include the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一些用例包括以下内容：
- en: Time-sensitive transfers of large files, such as lab imagery or media, from
    distributed locations to your data lake built on Amazon S3 (centralized data repository).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从分布式位置到基于 Amazon S3 构建的数据湖（集中式数据存储库）的大文件，如实验室图像或媒体，进行时间敏感的传输。
- en: Web or mobile applications with a file upload or download feature where users
    are geographically distributed and are far from the destination S3 bucket. S3TA
    can accelerate this long-distance transfer of files and helps you provide a better
    user experience.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有文件上传或下载功能的 Web 或移动应用程序，其中用户地理位置分散，且距离目标 S3 存储桶较远。S3TA 可以加速这种远程文件传输，并帮助您提供更好的用户体验。
- en: It uses Amazon CloudFront’s globally distributed edge locations, AWS backbone
    networks, and network protocol optimizations to route traffic, which speeds up
    the transfer, reduces the internet traffic variability, and helps in logically
    shortening the distance to S3 for remote applications.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用Amazon CloudFront的全球分布式边缘位置、AWS骨干网络和网络协议优化来路由流量，这加快了传输速度，减少了互联网流量的可变性，并有助于在逻辑上缩短远程应用程序到S3的距离。
- en: Important note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: There is an additional charge to use Amazon S3 Transfer Acceleration.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Amazon S3传输加速会有额外费用。
- en: We have discussed using online data transfer services such as AWS DataSync,
    AWS Transfer Family, and S3TA for moving data from on-premises storage to AWS
    storage resources over the network. There might be scenarios where you want to
    transfer streaming data in real time to the AWS cloud, for example, telemetry
    data from IoT sensors, video for online streaming applications, and so on. For
    this, we will go deeper into Amazon Kinesis, which is a fully managed streaming
    service built by AWS.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了使用在线数据传输服务，如AWS DataSync、AWS Transfer Family和S3TA，通过网络将数据从本地存储传输到AWS存储资源。可能存在一些场景，您希望将实时流数据传输到AWS云，例如，来自物联网传感器的遥测数据、用于在线流媒体应用的视频等。为此，我们将更深入地介绍Amazon
    Kinesis，这是一个由AWS构建的完全管理的流服务。
- en: Amazon Kinesis
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Kinesis
- en: '**Amazon Kinesis** is a **fully managed** service used to collect, process,
    and analyze streaming data in **real time at any scale**. Streaming data can include
    ingesting application logs, audio, video, website clickstreams, or IoT sensor
    data for deep learning, machine learning, analytics, and other applications. It
    allows you to perform data analysis as the data arrives in real time, instead
    of waiting for all the data to be transferred before processing.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Kinesis**是一个**完全管理**的服务，用于在任何规模上实时收集、处理和分析流数据。流数据可以包括应用程序日志、音频、视频、网站点击流或物联网传感器数据，用于深度学习、机器学习、分析和其他应用。它允许您在数据到达时实时执行数据分析，而不是等待所有数据传输完成后再进行处理。'
- en: 'Amazon Kinesis includes the following services:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Kinesis包括以下服务：
- en: '**Kinesis Video Streams**: This is used when you have to securely stream video
    from connected devices to AWS for applications such as processing, analytics,
    or machine learning to drive insights in real time. It has a built-in autoscaling
    mechanism to provision the infrastructure required for ingesting video streams
    coming from millions of devices. It automatically encrypts the data at rest as
    well as in transit. It uses Amazon S3 as its underlying storage, allowing you
    to store and retrieve data reliably. This helps you to develop real-time computer
    vision applications by integrating it with other fully managed AWS services or
    using popular open source machine learning frameworks on AWS. *Figure 2.5* shows
    how Kinesis video streams can be used to collect, process, and store video streams
    coming from media devices for machine learning, analytics, and playback with integration
    with other media applications:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kinesis视频流**：当您需要从连接的设备安全地将视频流传输到AWS进行应用，如处理、分析或机器学习以实时驱动洞察时使用。它具有内置的自动扩展机制，用于提供从数百万个设备接收视频流所需的基础设施。它自动加密静态数据和传输中的数据。它使用Amazon
    S3作为其底层存储，允许您可靠地存储和检索数据。这有助于您通过将其与其他完全管理的AWS服务集成或使用AWS上流行的开源机器学习框架来开发实时计算机视觉应用。*图2.5*展示了如何使用Kinesis视频流收集、处理和存储来自媒体设备的视频流，用于机器学习、分析和回放，并与其他媒体应用集成：'
- en: '![Figure 2.5 – Capture, process, and store video streams for machine learning,
    analytics, and playback](img/B18493_02_005.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – 捕获、处理和存储用于机器学习、分析和回放的视频流](img/B18493_02_005.jpg)'
- en: Figure 2.5 – Capture, process, and store video streams for machine learning,
    analytics, and playback
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 捕获、处理和存储用于机器学习、分析和回放的视频流
- en: '**Kinesis Data Streams**: This is a fully managed serverless service used for
    securely streaming data at any scale in a cost-effective manner. You can stream
    gigabytes of data per second by adjusting your capacity or use it in on-demand
    mode for automatic scaling and provisioning the underlying infrastructure based
    on the capacity required by your application. It has built-in integration with
    other AWS services, and you only pay for what you use. *Figure 2.6* shows how
    Kinesis Data Streams can be used to ingest, process, and store streaming data
    generated by different sources to other AWS services to gain insights in real
    time:'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kinesis Data Streams**：这是一个全托管的无服务器服务，用于以经济高效的方式在任何规模上安全地传输数据。您可以通过调整您的容量每秒传输数吉字节的数据，或者以按需模式使用它来自动扩展和配置底层基础设施，以满足应用程序所需的容量。它具有与其他
    AWS 服务内置的集成，并且您只需为使用的部分付费。*图 2.6* 展示了如何使用 Kinesis Data Streams 从不同的来源摄取、处理和存储生成的流数据，以实时获得洞察：'
- en: '![Figure 2.6 – Capture data from different sources into Amazon Kinesis Data
    Streams](img/B18493_02_006.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 将来自不同来源的数据捕获到 Amazon Kinesis Data Streams](img/B18493_02_006.jpg)'
- en: Figure 2.6 – Capture data from different sources into Amazon Kinesis Data Streams
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 将来自不同来源的数据捕获到 Amazon Kinesis Data Streams
- en: '**Kinesis Data Firehose**: This is used to securely stream data into data lakes
    built on Amazon S3, or data warehouses such as Amazon Redshift, into the required
    formats for further processing or analysis without the need to build data processing
    pipelines. Let’s look at some of the benefits of using Kinesis Data Firehose:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kinesis Data Firehose**：用于将数据安全地流式传输到基于 Amazon S3 构建的数据湖或数据仓库，如 Amazon Redshift，以进行进一步处理或分析，无需构建数据处理管道。让我们看看使用
    Kinesis Data Firehose 的一些好处：'
- en: It enables you to create your delivery stream easily by extracting, transforming,
    and loading streaming data securely at scale, without the need to manage the underlying
    infrastructure.
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使您能够通过安全地提取、转换和加载流数据来轻松创建交付流，无需管理底层基础设施。
- en: It has built-in autoscaling to provision the resources required by your streaming
    application, without any continuous management.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有内置的自扩展功能，可以提供流应用程序所需的资源，无需任何持续管理。
- en: It enables you to transform raw streaming data using either built-in or custom
    transformations. It supports converting data into different formats such as Apache
    Parquet and can dynamically partition data without building any custom processing
    logic.
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使您能够使用内置或自定义转换来转换原始流数据。它支持将数据转换为不同的格式，如 Apache Parquet，并且可以动态分区数据，而无需构建任何自定义处理逻辑。
- en: You can enhance your data streams using machine learning models within Kinesis
    Firehose to analyze and perform inference as the data travels to the destination.
    It provides enhanced network security by monitoring and creating alerts in real
    time when there is a potential threat using **Security Information and Event Management**
    (**SIEM**) tools.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 Kinesis Firehose 中的机器学习模型来增强您的数据流，在数据传输到目的地时进行分析和推理。它通过实时监控和创建警报来提供增强的网络安全性，当使用
    **安全信息和事件管理**（**SIEM**）工具检测到潜在威胁时。
- en: You can connect with 30+ AWS services and streaming destinations, which are
    fully integrated with Kinesis Data Firehose.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以连接到 30 多个 AWS 服务和流式传输目的地，这些服务与 Kinesis Data Firehose 完全集成。
- en: '*Figure 2.7* shows how Amazon Kinesis Data Firehose can be used for ETL use
    cases without having to write long lines of code or managing your own infrastructure
    at scale:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.7* 展示了如何使用 Amazon Kinesis Data Firehose 进行 ETL 用例，而无需编写长行代码或管理大规模的自身基础设施：'
- en: '![Figure 2.7 – ETL using Amazon Kinesis Data Firehose](img/B18493_02_007.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – 使用 Amazon Kinesis Data Firehose 进行 ETL](img/B18493_02_007.jpg)'
- en: Figure 2.7 – ETL using Amazon Kinesis Data Firehose
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 使用 Amazon Kinesis Data Firehose 进行 ETL
- en: '**Kinesis Data Analytics**: This is used to process data streams in real time
    with serverless and fully managed **Apache Flink** or SQL, from data sources such
    as Amazon S3, Amazon Kinesis Data Streams, and **Amazon Managed Apache Kafka**
    (**MSK**) (used to ingest and process streaming data). It can also be used to
    trigger real-time actions such as anomaly detection from long-running stateful
    computations based on past data trends. It has a built-in autoscaling mechanism
    to match the volume and throughput of your input data stream. You only pay for
    what you use; there is no minimum fee or set-up cost associated with it. It helps
    you to understand your data in real time, for example, by building a leaderboard
    for your gaming application, analyzing sensor data, log analytics, web clickstream
    analytics, building a streaming ETL application, or continuously generating metrics
    for understanding data trends.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kinesis Data Analytics**：此功能用于实时处理数据流，使用无服务器和完全管理的**Apache Flink**或SQL，从数据源如Amazon
    S3、Amazon Kinesis Data Streams和**Amazon Managed Apache Kafka**（**MSK**）（用于摄取和处理流数据）。它还可以用于从基于过去数据趋势的长时间运行的有状态计算中触发实时操作，如异常检测。它具有内置的自适应扩展机制，以匹配您的输入数据流的体积和吞吐量。您只需为使用的部分付费；与之相关的没有最低费用或设置成本。它可以帮助您实时了解数据，例如，通过为您的游戏应用程序构建排行榜，分析传感器数据，日志分析，Web点击流分析，构建流式ETL应用程序，或持续生成指标以了解数据趋势。'
- en: '*Figure 2.8* shows how a typical Kinesis Data Analytics application works.
    It has three main components:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.8*展示了典型的Kinesis Data Analytics应用程序的工作方式。它有三个主要组件：'
- en: An input data streaming source on which to perform real-time analytics
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于执行实时分析的数据流输入源
- en: '**Amazon Kinesis Data Analytics Studio Notebook** to analyze streaming data
    using SQL queries and Python or Scala programs'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Kinesis Data Analytics Studio Notebook**，用于使用SQL查询和Python或Scala程序分析流数据'
- en: 'Finally, it stores processed results on a destination service or application,
    such as Amazon Redshift or Amazon DynamoDB (NoSQL database) and Amazon Kinesis
    Data Streams:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，它将处理后的结果存储在目标服务或应用程序上，例如Amazon Redshift或Amazon DynamoDB（NoSQL数据库）和Amazon
    Kinesis Data Streams：
- en: '![Figure 2.8 – Real-time processing of streaming data using Amazon Kinesis
    Data Analytics](img/B18493_02_008.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图2.8 – 使用Amazon Kinesis Data Analytics实时处理流数据](img/B18493_02_008.jpg)'
- en: Figure 2.8 – Real-time processing of streaming data using Amazon Kinesis Data
    Analytics
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – 使用Amazon Kinesis Data Analytics实时处理流数据
- en: In this section, we discussed how to transfer and process streaming data to
    AWS Storage, using Amazon Kinesis. There are some use cases, such as edge computing
    and edge storage, for which you can use **AWS Snowcone**, a portable, rugged,
    and secure device for edge computing, storage, and data transfer. Next, let’s
    see how we can transfer data online from AWS Snowcone to AWS.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了如何使用Amazon Kinesis将流数据传输和传输到AWS存储。有一些用例，如边缘计算和边缘存储，您可以使用**AWS Snowcone**，这是一种用于边缘计算、存储和数据传输的便携、坚固且安全的设备。接下来，让我们看看如何从AWS
    Snowcone在线传输数据到AWS。
- en: AWS Snowcone
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Snowcone
- en: 'AWS Snowcone is a small, rugged, and portable device, used for running edge
    computing workloads, edge storage, and data transfer. The device weighs about
    4.5 lbs (2.1 kg) and has multiple layers of security and encryption. It has 8
    TB of storage, while the AWS Snowcone **Solid State Drive** (**SSD**) version
    provides 14 TB. Some of the common use cases for Snowcone are as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Snowcone是一种小型、坚固且便携的设备，用于运行边缘计算工作负载、边缘存储和数据传输。该设备重约4.5磅（2.1千克），具有多层安全和加密。它有8
    TB的存储空间，而AWS Snowcone的**固态硬盘**（**SSD**）版本提供14 TB。Snowcone的一些常见用例如下：
- en: Healthcare IoT, for transferring critical and sensitive data from emergency
    medical vehicles to hospitals for processing data faster and reducing the response
    time to serve the patients in a better way. You can then transfer the data securely
    to the AWS cloud.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗物联网，用于从紧急医疗车辆向医院传输关键和敏感数据，以加快数据处理速度并减少响应时间，更好地服务患者。然后，您可以安全地将数据传输到AWS云。
- en: Industrial IoT, for capturing sensor or machine data, as it can withstand extreme
    temperatures, vibrations, and humidity found on factory floors where traditional
    edge devices might not work.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工业物联网，用于捕获传感器或机器数据，因为它可以承受工厂地板上发现的极端温度、振动和湿度，而传统的边缘设备可能无法工作。
- en: Capturing and storing sensor data from autonomous vehicles and drones.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从自动驾驶汽车和无人机捕获和存储传感器数据。
- en: You can transfer terabytes of data from various AWS Snowcone devices over the
    network using AWS DataSync, as discussed in the *AWS DataSync* section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 AWS DataSync，如 *AWS DataSync* 部分所述，通过网络从各种 AWS Snowcone 设备传输千兆字节的数据。
- en: AWS online data transfer services are helpful when you have to transfer up to
    terabytes of data over the network to AWS. The time taken to transfer data is
    dependent on your available network bandwidth and internet traffic. When you have
    to transfer data from remote locations, or when your network bandwidth is heavily
    used by existing applications, you would need an alternative mechanism to transfer
    data offline. Let’s discuss the AWS offline data transfer services in the next
    section.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要通过网络将高达千兆字节的数据传输到 AWS 时，AWS 在线数据传输服务非常有用。数据传输所需的时间取决于您的可用网络带宽和互联网流量。当您需要从偏远地点传输数据，或者当您的网络带宽被现有应用程序大量使用时，您就需要一个替代机制来离线传输数据。让我们在下一节中讨论
    AWS 离线数据传输服务。
- en: AWS offline data transfer services
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS 离线数据传输服务
- en: For transferring up to petabytes of data via offline methods, in a secure and
    cost-effective fashion, you can use **AWS Snow Family devices**. Sometimes, your
    applications may require enhanced performance at the edge, where you want to process
    and analyze your data close to the source in order to deliver real-time meaningful
    insights. This would mean having AWS-managed hardware and software services beyond
    the AWS cloud. AWS Snow Family can help you to run operations outside of your
    data center, as well as in remote locations with limited network connectivity.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以安全且经济高效的方式通过离线方法传输高达千兆字节的数据，您可以使用 **AWS Snow Family 设备**。有时，您的应用程序可能需要在边缘进行增强性能，您希望在数据源附近处理和分析数据，以便提供实时有意义的见解。这意味着您需要
    AWS 云之外的 AWS 管理的硬件和软件服务。AWS Snow Family 可以帮助您在数据中心外以及网络连接有限的偏远地点运行操作。
- en: 'It consists of the following devices:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 它包括以下设备：
- en: '**AWS Snowcone**: In the *AWS online data transfer services* section, we introduced
    and discussed how Snowcone can be used for collecting and storing data at the
    edge and then transferring it to the AWS cloud using AWS DataSync. In cases of
    limited network bandwidth, you can also use it for offline data transfer by sending
    the device to an AWS facility. It includes an E Ink shipping label, which also
    aids in tracking.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Snowcone**：在 *AWS 在线数据传输服务* 部分中，我们介绍了 Snowcone 的使用方法，并讨论了如何使用 AWS DataSync
    在边缘收集和存储数据，然后将其传输到 AWS 云。在网络带宽有限的情况下，您还可以通过将设备发送到 AWS 设施来进行离线数据传输。它包括一个电子墨水运输标签，这也有助于跟踪。'
- en: '**AWS Snowball**: This is used for migrating data and performing edge computing.
    It comes with two options:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Snowball**：此设备用于数据迁移和边缘计算。它提供两种选择：'
- en: Snowball Edge Compute Optimized has 42 TB of block or Amazon S3-compatible storage
    with 52 vCPUs and an optional GPU for edge computing use cases such as machine
    learning, video analysis, and big data processing in environments with intermittent
    network connectivity, such as industrial and transportation use cases, or extremely
    remote locations found in defense or military applications.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snowball Edge 计算优化版拥有 42 TB 的块存储或与 Amazon S3 兼容的存储，配备 52 个 vCPU 和可选的 GPU，用于边缘计算用例，如机器学习、视频分析和在具有间歇性网络连接的环境中的大数据处理，例如工业和交通用例，或在国防或军事应用中发现的极端偏远地区。
- en: Snowball Edge Storage Optimized has 80 TB of usable block or Amazon S3-compatible
    object storage, with 40 vCPUs for performing computing at the edge. It is primarily
    used for either local storage or large-scale offline data transfers to the AWS
    cloud.
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snowball Edge 存储优化版拥有 80 TB 的可用块存储或与 Amazon S3 兼容的对象存储，配备 40 个 vCPU 以在边缘进行计算。它主要用于本地存储或大规模离线数据传输到
    AWS 云。
- en: AWS Snowmobile is used to migrate up to 100 PB of data in a 45-foot-long shipping
    container, which is tamper resistant, waterproof, and temperature controlled with
    multiple layers of logical and physical security. It is ideal for use cases where
    you have to transfer exabytes or hundreds of petabytes of data, which might occur
    due to data center shutdowns. You need to order it from the AWS Snow Family console,
    and it arrives at your site as a network-attached data store that connects to
    your local network to perform high-speed data transfers. Once the data is moved
    to the device, it is driven back to the AWS facility where the data is then uploaded
    to the specified Amazon S3 bucket. To ensure data security in transit and the
    successful delivery of data to the AWS facility, it comes with fire suppression,
    encryption, dedicated security personnel, GPS tracking, alarm monitoring, 24/7
    video surveillance, and an escort security vehicle.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand the various offline data transfer options offered by
    AWS, let’s understand the process for ordering the device.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Process for ordering a device from AWS Snow Family
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To order AWS Snowmobile, you need to contact AWS sales support. For Snowcone
    or Snowball devices, you can follow these steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to the AWS Console and type `AWS Snow Family` in the search bar. Click
    on **AWS Snow Family**, which will take you to the AWS Snow Family console as
    shown in *Figure 2.9*:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.9 – AWS Snow Family \uFEFFconsole](img/B18493_02_009.jpg)"
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – AWS Snow Family console
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the orange button reading **Order an AWS Snow Family device**, which
    opens another screen, as shown in *Figure 2.10*. This will provide you with steps
    to create a job for ordering the device:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.10 – AWS Snow Family – Create new job](img/B18493_02_010.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – AWS Snow Family – Create new job
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Next** to go to **Step 2**, **Get started with import to S3 job**,
    as shown in *Figure 2.11*:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Getting started with import to S3 job](img/B18493_02_011.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Getting started with import to S3 job
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the instructions carefully, check the acknowledgment checkbox, and click
    **Next** to go to **Step 3**, **Choose your shipping preferences**, as shown in
    *Figure 2.12*:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Choose your shipping preferences](img/B18493_02_012.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Choose your shipping preferences
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Fill in your shipping details, select your preferred shipping speed, and click
    **Next** to go to **Step 4**, **Choose your job details**, as shown in *Figure
    2.13*. Make sure to enter a valid address as it will give you an error message
    if your address is incorrect:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Choose your job details](img/B18493_02_013.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Choose your job details
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: On this screen, you can select your Snow device, power supply, wireless options
    for Snowcone, S3 bucket, compute using EC2 instances, and the option to install
    the AWS IoT Greengrass validated AMI.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the S3 bucket will appear as directories on your device, and
    the data in those directories will be transferred back to S3\. If you have selected
    the AWS IoT Greengrass AMI to run IoT workloads on the device, you also need to
    select the **Remote device management** option to open and manage the device remotely
    with OpsHub or Snowball Client.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: All the options as mentioned in *step 4* are not shown in *Figure 2.13* – *Choose
    your job details*, but will be present on your console screen.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have filled out the job details, click on the **Next** button to
    go to **Step 5**, **Choose your security preferences**, as shown in *Figure 2.14*:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Choose your security preferences](img/B18493_02_014.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Choose your security preferences
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the permission and encryption settings for your job on this screen,
    which will help you to protect your data while in transit, and then click on **Next**
    to go to **Step 6**, **Choose your notification preferences**, as shown in *Figure
    2.15*:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Choose your notification preferences](img/B18493_02_015.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – Choose your notification preferences
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'To receive email notifications of your job status changes, you can choose either
    an existing **Simple Notification Service** (**SNS**) topic or create a new SNS
    topic. Click on **Next** to go to **Step 7**, **Review and create your job**,
    as shown in *Figure 2.16*:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Review and create your job](img/B18493_02_016.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Review and create your job
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: You can review all the details that you have entered from **Step 1** through
    **Step 7** and then click on the **Create job** button.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the job is created, it will take you to the **Jobs** screen, as shown
    in *Figure 2.17*, where you can see details of your job including the status:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.17 – Snow \uFEFFFamily jobs](img/B18493_02_017.jpg)"
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – Snow Family jobs
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: On the **Actions** drop-down menu, you also have options to cancel a job, edit
    a job name, and clone a job.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about AWS Snow Family devices to transfer data offline
    based on our application requirements, network connectivity, available bandwidth,
    and the location of our data sources. We also discussed how we can use these devices
    not only for transferring data but also for edge computing.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most frequently asked questions on this topic is, how do we calculate
    the time taken to move data to the cloud based on the network speed and available
    bandwidth? For this, AWS provides a simple formula based on the best-case scenario,
    which is given as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18493_02_F01.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: For example, if we have a network connection of 1.544 Mbps, and we want to move
    1 TB of data into and out of the AWS cloud, then theoretically the minimum time
    that it would take to transfer over your network connection at 80% network utilization
    is 82 days.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this formula only gives a high-level estimate; the actual time
    taken might differ based on the variability of network traffic and available bandwidth.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now take a brief look at all the topics that we have covered in this chapter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about various aspects of data management, including
    data governance and compliance with the legal requirements of federal and regional
    authorities of the country where the data resides. We also discussed that in order
    to build HPC applications on the cloud, we need to have data on the cloud, and
    looked at the challenges of transferring this data to the cloud. In order to mitigate
    these challenges, we can use the managed AWS data transfer services, and in order
    to select which service to use for your application, we then discussed the elements
    of building a data strategy.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: We then took an example of how we can transfer petabyte-scale data to the cloud
    in order to understand the concepts involved in a data transfer strategy. Finally,
    we did a deep dive on various AWS data transfer services for both online and offline
    data transfer based on your network bandwidth, connectivity, type of application,
    speed of data transfer, and location of your data source.Now that we understand
    the mechanisms for transferring data to the cloud, the challenges involved, and
    how to mitigate them, in the next chapter, we will focus on understanding the
    various compute options provided by AWS for running HPC applications, and how
    to optimize these based on the application requirements.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some additional resources for this chapter:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '*Migrate Petabyte Scale Data*: [https://aws.amazon.com/getting-started/projects/migrate-petabyte-scale-data/](https://aws.amazon.com/getting-started/projects/migrate-petabyte-scale-data/)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introduction to HPC on AWS*: [https://d1.awsstatic.com/whitepapers/Intro_to_HPC_on_AWS.pdf](https://d1.awsstatic.com/whitepapers/Intro_to_HPC_on_AWS.pdf)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data management versus data governance*: [https://www.tableau.com/learn/articles/data-management-vs-data-governance](https://www.tableau.com/learn/articles/data-management-vs-data-governance)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is DataSync?*: [https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html](https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S3 Transfer Acceleration*: [https://aws.amazon.com/s3/transfer-acceleration/](https://aws.amazon.com/s3/transfer-acceleration/)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS Transfer Family Customers*: [https://aws.amazon.com/aws-transfer-family/customers/](https://aws.amazon.com/aws-transfer-family/customers/)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kinesis Data Streams*: [https://aws.amazon.com/kinesis/data-streams/](https://aws.amazon.com/kinesis/data-streams/)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kinesis Video Streams*: [https://aws.amazon.com/kinesis/video-streams/](https://aws.amazon.com/kinesis/video-streams/)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kinesis Data Analytics*: [https://aws.amazon.com/kinesis/data-analytics/](https://aws.amazon.com/kinesis/data-analytics/)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS Snowcone*: [https://aws.amazon.com/snowcone](https://aws.amazon.com/snowcone)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS Snowcone*: [https://aws.amazon.com/snowcone](https://aws.amazon.com/snowcone)'
- en: '*AWS Snow Family*: [https://aws.amazon.com/snow/](https://aws.amazon.com/snow/)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS Snow Family*: [https://aws.amazon.com/snow/](https://aws.amazon.com/snow/)'
- en: '*Cloud Data Migration*: [https://aws.amazon.com/cloud-data-migration/](https://aws.amazon.com/cloud-data-migration/)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云数据迁移*: [https://aws.amazon.com/cloud-data-migration/](https://aws.amazon.com/cloud-data-migration/)'
