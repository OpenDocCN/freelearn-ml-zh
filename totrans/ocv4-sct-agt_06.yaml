- en: Controlling a Phone App with Your Suave Gestures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"You''ve got all the moves."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Lani Hall, Never Say Never Again (1983)'
  prefs: []
  type: TYPE_NORMAL
- en: He raises an eyebrow; he lowers his chin; he twists the corners of his mouth;
    he folds one arm into the crook of the other as he points his pistol at the ceiling.
    It all looks very impressive, but is he simply wasting time while trying to remember
    people's names?
  prefs: []
  type: TYPE_NORMAL
- en: Agent 007 has a few old friends with normal names, such as Bill Tanner and Felix
    Leiter. Almost every other name is a number, a single letter, a mash-up of multiple
    languages, or a blindingly obvious double entendre. After a few vodka martinis
    and tranquilizer darts, any man would start to wonder whether his memory for names
    was playing tricks on him.
  prefs: []
  type: TYPE_NORMAL
- en: To put such doubts to rest, we will develop an Android app that determines a
    person's name based on a series of yes/no questions. To allow a secret agent to
    use it discretely, the app will rely on gesture controls and audio output, which
    can go to a Bluetooth headset so that others cannot hear.
  prefs: []
  type: TYPE_NORMAL
- en: The app's logic is like the game Twenty Questions. First, the app asks a question
    by playing an audio clip. Then, the user responds with a nod or a shake of the
    head. Each question is more specific than the last, until the app is ready to
    guess a name or give up. Recognizing the two possible head gestures—a nod or a
    shake—is our computer vision task for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, this chapter covers the following programming topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Android Studio and the Android SDK to build an Android app in Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using OpenCV's Android camera functions to capture, process, and display images
    from the Android device's camera
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking head gestures using OpenCV's functions for face detection, feature
    detection, and optical flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The app's codename is `Goldgesture`.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter’s project has the following software dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Android Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCV Android pack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setup instructions are covered in [Chapter 1](e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml),
    *Preparing for the Mission*. Refer to the setup instructions for any version requirements.
    Instructions for building and running Android projects are covered in the current
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The completed project for this chapter can be found in the book's GitHub repository, [https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition](https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition),
    in the `Chapter004` folder. If you want to open the completed project, just launch
    Android Studio, select Open an existing Android Studio project, and then select
    the `Chapter004/Goldgesture` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the Goldgesture app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Goldgesture` is a GUI app built with the Android SDK and OpenCV''s Java bindings
    for Android. It has just a single view, seen in the screenshot on the next page.
    The app has the following flow of execution:'
  prefs: []
  type: TYPE_NORMAL
- en: Constantly display a live video feed from the front-facing (self-portrait) camera.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform human face detection using OpenCV's `CascadeClassifier` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When a human face is detected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a blue rectangle around the face.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Detect features of the face (points that should be easy to track in subsequent
    frames despite movement) using OpenCV's `goodFeaturesToTrack` function. Draw green
    circles around these features.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: As the face moves, track the features in every frame using OpenCV's `calcOpticalFlowPyrLK`
    function. This function can continuously track the features even though `CascadeClassifier`
    is unlikely to continuously detect a face.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the features' center point moves up and down by a certain amount and a
    certain number of times, deem that a nod has occurred.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the features' center point moves left and right by a certain amount and
    a certain number of times, deem that a shake of the head has occurred.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Play a sequence of audio clips. At each juncture, choose the next clip depending
    (in part) on whether a nod or shake of the head has occurred.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reset the tracking if its reliability deteriorates to a certain extent or if
    the user''s head appears to be nodding and shaking at the same time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9eba51d6-30cc-4e7e-a933-3a732aacd28f.png)'
  prefs: []
  type: TYPE_IMG
- en: The face-detection functionality in `Goldgesture` should already be familiar
    from the Angora Blue project in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml)*,
    Training a Smart Alarm to Recognize the Villain and His Cat*. However, feature
    tracking, and specifically optical flow, is a new topic for us. Let's talk about
    the concepts a little before proceeding to set up our project.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding optical flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Optical flow** is the pattern of apparent motion between two consecutive
    frames of video. We select feature points in the first frame and try to determine
    where those features have gone in the second frame. This search is subject to
    a few caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: We make no attempt to distinguish between camera motion and subject motion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We assume that a feature's color or brightness remains similar between frames.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We assume that neighboring pixels have similar motions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenCV''s `calcOpticalFlowPyrLK` function implements the Lucas-Kanade method
    of computing optical flow. Lucas-Kanade relies on a *3 x 3* neighborhood (that
    is, 9 pixels) around each feature. Taking each feature''s neighborhood from the
    first frame, we try to find the best matching neighborhood in the second frame,
    based on least squares error. OpenCV''s implementation of Lucas-Kanade uses an
    image pyramid, meaning it performs the search at various scales. Thus, it supports
    both large and small motions (`PyrLK` in the function name stands for *pyramidal
    Lucas-Kanade*). The following diagram is a visualization of a pyramid—a progression
    from low-resolution (or low-magnification) images to high-resolution (or high-magnification)
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e38cd409-3f9b-4e21-b0fd-28516c6f15be.png)'
  prefs: []
  type: TYPE_IMG
- en: For more details on optical flow and the Lucas-Kanade method, see the official
    OpenCV documentation at [http://docs.opencv.org/master/d7/d8b/tutorial_py_lucas_kanade.html](http://docs.opencv.org/master/d7/d8b/tutorial_py_lucas_kanade.html).
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV offers implementations of other optical flow algorithms as well. For
    example, the `calcOpticalFlowSF` function implements the SimpleFlow algorithm,
    which makes optimizations for high-resolution video by assuming that smooth (uniform)
    image regions move in unison. The `calcOpticalFlowFarneback` function implements
    Gunnar Farneback's algorithm, which posits that a neighborhood remains identifiable,
    even during motion, by the coefficients of a polynomial relationship among its
    pixel values. Both of these algorithms are forms of *dense* optical flow, meaning
    that they analyze every pixel in the image instead of just selected (*sparse*)
    features. More of OpenCV's optical flow functions are documented at [https://docs.opencv.org/master/dc/d6b/group__video__track.html](https://docs.opencv.org/master/dc/d6b/group__video__track.html)
    and [https://docs.opencv.org/master/d2/d84/group__optflow.html](https://docs.opencv.org/master/d2/d84/group__optflow.html).
  prefs: []
  type: TYPE_NORMAL
- en: Of the several options, why choose `calcOpticalFlowPyrLK`? *You see, it is a
    pyramid,* as Imhotep said to the Pharaoh Djoser, *and it has open spaces inside
    it.* A pyramidal, sparse technique is a good way for us to cheaply and robustly
    track a few features in a face, which may change scale as it moves nearer to or
    farther from the camera.
  prefs: []
  type: TYPE_NORMAL
- en: For our purposes, it is useful to select features inside a detected object,
    specifically a detected face. We choose an inner portion of the face (to avoid
    background regions) and then use an OpenCV function called `goodFeaturesToTrack`,
    which selects features based on the algorithm described in Jianbo Shi and Carlo
    Tomasi's paper, "Good Features to Track", *Proc. of IEEE Conf. on Computer Vision
    and Pattern Recognition*, pp. 593-600, June 1994.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, the **Good Features to Track** (**GFTT**) algorithm (also
    known as the **Shi-Tomasi algorithm**) takes into account the requirements of
    tracking algorithms and tracking use cases, and attempts to select features that
    work well with these algorithms and use cases. As described in detail in the paper,
    good features to track must have a stable appearance with respect to small changes
    in the camera's perspective. Examples of poor features to track are reflections
    (such as sunlight on a car's hood) and lines that cross at different depths (such
    as a tree's branches), since these features move quickly as the viewer or camera
    moves. The effects of a change in perspective can be simulated (albeit imperfectly)
    by warping a given image and moving its contents linearly. Based on such a simulation,
    the most stable features can be selected.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV offers implementations of several feature-detection algorithms, besides
    Good Features to Track. For references to information about these other algorithms,
    please refer to [Appendix B](01685b22-2dcc-4d5b-ac19-0b8a15e0e3b1.xhtml),* Learning
    More about Feature Detection in OpenCV*.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the project in Android Studio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a refresher on setting up Android Studio and the OpenCV Android pack, refer
    to the *Setting up Android Studio and OpenCV* section in [Chapter 1](e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml),
    *Preparing for the Mission*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will organize all the source code and resources for our Android app in an
    Android Studio project, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open Android Studio and select File | New | New Project... from the menu. The
    Create New Project window should appear, and it should show the **Choose your
    project** form. Select Empty Activity, as shown in the following screenshot, and
    click Next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/efe6e451-a22b-40a6-9dcf-34c017da0e1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Create New Project window should show the Configure your project form. We
    want to specify that our app name is `Goldgesture`, its package name is `com.nummist.goldgesture`,
    it is a Java project, and its minimum Android SDK version is API level 21, which
    is Android 5.0\. You may choose any new folder as the project''s location. Fill
    out the form as shown in the following screenshot, and click Finish:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aabdd9d9-f12d-467e-bb92-15548b922d21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By default, Android Studio creates a main class, called MainActivity. Let''s
    rename this to give it a more descriptive name, CameraActivity. Right-click on
    `app/src/main/java/com.nummist.goldgesture/MainActivity` (in the Project pane)
    and select Refactor | Rename... from the context menu. The Rename dialog should
    appear. Fill it out as shown in the following screenshot, and click Refactor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4288f5da-a3f8-4c83-8314-72ff2b958849.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s rename the XML file that defines the GUI layout associated with the
    `main` class. Right-click on `app/src/main/res/layout/activity_main.xml` (in the
    Project pane) and select Refactor | Rename... from the context menu. The Rename
    dialog should appear again. Fill it out as shown in the following screenshot,
    and click Refactor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2ef64a54-3085-439e-a2fc-f6f7989c3cb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since our app will depend on OpenCV, we need to import the OpenCV library module
    that we obtained as part of the OpenCV Android pack in [Chapter 1](e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml)*,
    Preparing for the Mission*. From Android Studio''s menu, select File | New | New
    Module.... The Create New Module dialog should appear, and it should show the
    New Module form. Select Import Gradle Project, as shown in the following screenshot,
    and click Next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/700b1b37-8bae-46d9-b0d1-06ff77964796.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A file picker dialog should appear. Select the `sdk` subfolder of the OpenCV
    Android pack, as shown in the following screenshot, and confirm the choice by
    clicking the Open or OK button (whose name varies depending on the operating system):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bcf30f4e-c994-4972-b99b-67638aba9efc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Create New Module dialog should show the Import Module from Source form.
    Enter `:OpenCV` in the Module name field, as shown in the following screenshot,
    and click Finish:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fb464e3a-333e-4357-9cf3-cba7092b9e15.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, Android Studio might prompt you to perform updates and accept
    license agreements so that you have all of OpenCV's dependencies. If you are prompted,
    agree.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to specify that the `Goldgesture` app module depends on the OpenCV
    library module. From Android Studio''s menus, select File | Project Structure....
    The Project Structure dialog should appear. Under Modules, select app. Then, select
    the Dependencies tab, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7a3f0eff-604e-496c-86ce-684cc3665d45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hit the + button to add a dependency. A menu should appear. Select Module dependency.
    The Choose Modules dialog should appear. Select :OpenCV, as shown in the following
    screenshot, and click OK:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fd64d877-3ee5-491c-8201-5bee23d6207d.png)'
  prefs: []
  type: TYPE_IMG
- en: The OpenCV library is now linked into `Goldgesture`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting a cascade file and audio files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like parts of [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml), *Training
    a Smart Alarm to Recognize the Villain and His Cat,* *Angora Blue project*, `Goldgesture`
    performs human face detection and requires one of the cascade files that comes
    with OpenCV. Also, `Goldgesture` uses audio clips. The cascade file and audio
    clips are located in the book's GitHub repository in the `Chapter004/Goldgesture/app/src/main/res/raw` subfolder.
    If you are recreating the project from scratch, you should copy these files to
    your own `app/src/main/res/raw` folder. This folder is a standard location for
    files that we want bundled with the Android app in raw (unmodified) form. By default,
    this folder does not exist in new Android Studio projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create it in Android Studio, right-click on the `app/src/main/res` folder
    (in the Project pane) and select New | Android Resource Directory from the context
    menu. The New Resource Directory window should appear. Fill it out as shown in
    the following screenshot, and click OK:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fe45012-dc95-4f81-b826-c525c91ae4f9.png)'
  prefs: []
  type: TYPE_IMG
- en: After you create the `app/src/main/res/raw` folder, you can drag and drop files
    into it in Android Studio.
  prefs: []
  type: TYPE_NORMAL
- en: 'The audio clips are generated using the Vicki voice of the standard text-to-speech
    synthesizer on Mac. For example, one of the clips is created by running the following
    command in Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Speech synthesis is hours of fun for the whole family.
  prefs: []
  type: TYPE_NORMAL
- en: The Mac speech synthesizer pronounces `007` as double-O seven. This is an anomaly.
    For example, *008* is pronounced as *zero, zero, eight*.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the app's requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`AndroidManifest.xml` (the Android Manifest) is the place where an app announces
    information that the system, Google Play, and other apps might need to know. For
    example, `Goldgesture` requires a front-facing camera and permission to use it
    (a license to shoot, one might say). `Goldgesture` also expects to run in landscape
    mode, regardless of the physical orientation of the phone, because OpenCV''s camera
    preview always uses the camera''s landscape dimensions (OpenCV''s Android documentation
    does not indicate whether this behavior is intended. Perhaps future versions will
    provide better support for portrait orientation). To specify these requirements,
    edit `app/src/main/AndroidManifest.xml` to match the following sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When you open `AndroidManifest.xml` in Android Studio, you might see two tabs,
    one labeled Text and another labeled Merged Manifest. Select the Text tab, which
    allows us to directly edit the source code of `AndroidManifest.xml` (by contrast,
    the Merged Manifest tab is not directly editable, and it shows a combination of
    settings from `AndroidManifest.xml` and the project properties).
  prefs: []
  type: TYPE_NORMAL
- en: Now, our app can use a camera and will remain in landscape mode. Also, if we
    publish it on Google Play, it will only be available to devices with a front-facing
    camera.
  prefs: []
  type: TYPE_NORMAL
- en: Laying out a camera preview as the main view
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Android, like many systems, enables the programmer to specify GUI layouts in
    XML files. Our Java code can load an entire view, or pieces of it, from these
    XML files.
  prefs: []
  type: TYPE_NORMAL
- en: '`Goldgesture` has a simple layout that contains only a camera preview, on which
    we draw some additional graphics using OpenCV. The camera preview is represented
    by an OpenCV class called `JavaCameraView`. Let''s edit `app/src/main/res/layout/activity_camera.xml`
    to fill the layout with a `JavaCameraView`, using the front-facing camera, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, OpenCV also provides a class called `JavaCamera2View`. Both `JavaCameraView`
    and `JavaCamera2View` are implementations of an interface called `CameraBridgeViewBase`.
    The difference is that `JavaCamera2View` builds atop a more recent version of
    Android's camera APIs, but currently it yields a lower frame rate on many devices.
    The performance of `JavaCamera2View` could improve in future versions of OpenCV
    or on future Android devices, so you might want to run your own performance tests
    on the particular Android devices you are targeting.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking back-and-forth gestures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Several common gestures consist of a repetitive, back-and-forth movement. Consider
    the following examples of this type of gesture:'
  prefs: []
  type: TYPE_NORMAL
- en: Nodding (yes or I'm listening)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shaking one's head (no or dismay)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Waving (a greeting)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shaking hands (a greeting)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shaking one's fist (a threat or a protest)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wagging a finger (scolding)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wiggling a finger or fingers (beckoning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapping one's foot against the ground (impatience)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapping four fingers against a table (impatience)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapping two fingers against a table (Thanks for the green tea)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pacing (anxiety)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jumping up and down (excitement, joy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To help us recognize such gestures, let's write a class, `BackAndForthGesture`,
    which keeps track of the number of times that a value (such as an *x* coordinate
    or *y* coordinate) has oscillated between a low threshold and a high threshold.
    A certain number of oscillations can be considered a complete gesture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file, `app/src/main/java/com/nummist/goldgesture/BackAndForthGesture.java`.
    To do this in Android Studio, right-click on the `app/src/main/java/com.nummist.goldgesture` folder
    (in the Project pane) and select New | Java Class from the context menu. The Create
    New Class window should appear. Fill it out as shown in the following screenshot,
    and click OK:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82d88afa-76ae-48f5-9b0d-2df8a34a379e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As member variables, `BackAndForthGesture` will store the minimum distance
    or threshold that defines a back or forth motion, an initial position, the latest
    delta from this position, and the number of back movements and forth movements.
    Here is the first part of the class''s code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The back-and-forth count (or number of oscillations) is the lesser of the back
    count and the forth count. Let''s implement this rule in the following getter
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor takes one argument, the minimum distance or threshold of movement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To begin tracking movement, we call a `start` method with an initial position
    as an argument. This method records the initial position and resets the delta
    and counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We are considering position as a one-dimensional value because a head nodding
    (up and down) or shaking (left and right) is a linear gesture. For an upright
    head, only one of the image's two dimensions is relevant to a nod or shake gesture.
  prefs: []
  type: TYPE_NORMAL
- en: 'To continue tracking movement, we call an `update` method with the new position
    as an argument. This method recalculates the delta and if a threshold has just
    been passed, the back count or forth count is incremented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If we consider the gesture complete, or for some other reason we believe the
    counts to be invalid, we call a `resetCounts` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that `BackAndForthGesture` contains no computer vision functionality of
    its own, but the position values we pass to it will be derived from computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Playing audio clips as questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The logic of the question-and-answer sequence is another component that has
    no computer vision functionality. We encapsulate it in a class called `YesNoAudioTree`,
    which is responsible for playing the next audio clip whenever the app's computer
    vision component notifies it of a yes or no answer.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the audio clips are part of the book's GitHub repository, and
    they belong in the project's `app/src/main/res/raw` folder. However, note that
    the audio clips in the repository are by no means an exhaustive set of questions
    and guesses about characters in the Bond franchise. Feel free to add your own
    clips and your own logic to play them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file, `app/src/main/java/com/nummist/goldgesture/YesNoAudioTree.java`.
    Our `YesNoAudioTree` class needs member variables to store a media player and
    a related context, an ID for the most-recently-played audio clip, and information
    gathered from the answers to previous questions. Specifically, the next question
    depends on whether the unknown person is already identified as a member of MI6,
    the CIA, the KGB, or a criminal organization. This information, along with the
    answer to the most recent question, will be enough for us to build a simple tree
    of questions to identify several characters from the Bond franchise. The class''s
    implementation begins as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The class is instantiated with a `Context` object, which is a standard abstraction
    of the app''s Android environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `Context` object is needed to create a media player, as we will see later
    in this section of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the Android SDK's `MediaPlayer` class, see the official
    documentation at [http://developer.android.com/reference/android/media/MediaPlayer.html](http://developer.android.com/reference/android/media/MediaPlayer.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To (re)start from the first question, we call a `start` method. It resets the
    data about the person and plays the first audio clip using a private helper method,
    `play`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To stop any current clip and clean up the audio player (for example, when the
    app pauses or finishes), we call a `stop` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When the user has answered Yes to a question, we call the `takeYesBranch` method.
    It uses nested `switch` statements to pick the next audio clip based on previous
    answers and the most recent question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, when the user has answered No to a question, we call the `takeNoBranch`
    method, which also contains big, nested `switch` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When certain clips finish, we want to automatically advance to another clip
    without requiring a Yes or No from the user. A private helper method, `takeAutoBranch`,
    implements the relevant logic in a `switch` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever we need to play an audio clip, we call the `play` private helper method.
    It creates an instance of `MediaPlayer` using the context and an audio clip''s
    ID, which is given to `play` as an argument. The audio is played and a callback
    is set so that the media player will be cleaned up and `takeAutoBranch` will be
    called when the clip is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have written our supporting classes, we are ready to tackle the
    app's main class, including the computer vision functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing images and tracking faces in an activity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Android app is a state machine in which each state is called an **activity**.
    An activity has a life cycle. For example, it can be created, paused, resumed,
    and finished. During a transition between activities, the paused or finished activity
    can send data to the created or resumed activity. An app can define many activities
    and transition between them in any order. It can even transition between activities
    defined by the Android SDK or by other apps.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Android activities and their life cycles, see the
    official documentation at [http://developer.android.com/guide/components/activities.html](http://developer.android.com/guide/components/activities.html). For
    more information about OpenCV's Android and Java APIs (used throughout our activity
    class), see the official Javadocs at [https://docs.opencv.org/master/javadoc/index.html](https://docs.opencv.org/master/javadoc/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV provides classes and interfaces that can be considered add-ons to an
    activity''s life cycle. Specifically, we can use OpenCV callback methods to handle
    the following events:'
  prefs: []
  type: TYPE_NORMAL
- en: The camera preview starts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The camera preview stops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The camera preview captures a new frame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Goldgesture` uses just one activity, called `CameraActivity`. `CameraActivity`
    uses a `CameraBridgeViewBase` object (more specifically, a `JavaCameraView` object)
    as its camera preview. (Recall that we saw this earlier, in the *Laying out a
    camera preview as the main view* section of this chapter, when we implemented
    `CameraActivity`''s layout in XML.) `CameraActivity` implements an interface called
    `CvCameraViewListener2`, which provides callbacks for this camera preview. (Alternatively,
    an interface called `CvCameraViewListener` can serve this purpose. The difference
    between the two interfaces is that `CvCameraViewListener2` allows us to specify
    a format for the captured image, whereas `CvCameraViewListener` does not.) The
    implementation of our class begins as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For readability and easy editing, we use static final variables to store many
    parameters in our computer vision functions. You might wish to adjust these values
    based on experimentation. First, we have face-detection parameters that should
    be familiar to you from the Angora Blue project in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml)*,* *Training
    a Smart Alarm to R**ecognize the Villain and His Cat*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'For the purpose of selecting features, we do not use the entire detected face.
    Rather, we use an inner portion that is less likely to contain any non-face background.
    Thus, we define a proportion of the face that should be excluded from feature
    selection on each side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For face tracking using optical flow, we define a minimum and maximum number
    of features. If we fail to track at least the minimum number of features, we deem
    that the face has been lost. We also define a minimum feature quality (relative
    to the quality of the best feature found), a minimum pixel distance between features,
    and a maximum acceptable error value when trying to match a new feature to an
    old feature. As we will see later in this section of the chapter, these parameters
    pertain to OpenCV''s `calcOpticalFlowPyrLK` function and its return values. Here
    are the declarations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We also define how much movement (as a proportion of the image size) and how
    many back-and-forth cycles are required before we deem that a nod or shake has
    occurred:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Our member variables include the camera view, the dimensions of captured images,
    and the images at various stages of processing. The images are stored in OpenCV
    `Mat` objects, which are analogous to the NumPy arrays that we saw in the Python
    bindings. OpenCV always captures the images in landscape format, but we reorient
    them to portrait format, which is a more common orientation for a picture of one''s
    own face on a smartphone. Here are the relevant variable declarations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen in the following code and comments, we also declare several member
    variables related to face detection and tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we store instances of the classes that we defined earlier, namely
    `BackAndForthGesture` (in the *Tracking back-and-forth gestures* section of this
    chapter) and `YesNoAudioTree` (in the *Playing audio clips as questions and answers*
    section of this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s implement the standard life cycle callbacks of an Android activity.
    First, when the activity is created, we try to load the OpenCV library (if for
    some reason this fails, we log an error message and exit). If OpenCV loads successfully,
    we specify that we want to keep the screen on even when there is no touch interaction
    (since all interaction is through the camera). Moreover, we need to load the layout
    from the XML file, get a reference to the camera preview, and set this activity
    as the handler for the camera preview''s events. Here is the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we have not yet initialized most of our member variables. Instead,
    we do so once the camera preview has started. When the activity is paused, we
    disable the camera preview, stop the audio, and reset the gesture recognition
    data, as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'When the activity resumes (including the first time it comes to the foreground,
    after being created), we check whether the user has granted permission for the
    app to use the camera. If permission has not yet been granted, we request it.
    (In some circumstances, Android requires us to display a rationale for the permission
    request. We do this through a private helper method called `showRequestPermissionRationale`.)
    If permission has already been granted, we enable the camera view. Here is the
    relevant code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'When the activity is destroyed, we clean things up in the same way as when
    the activity is paused:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `showRequestPermissionRationale` helper method shows a dialog that explains
    why `Goldgesture` needs to use the camera. When the user clicks this dialog''s
    `OK` button, we request permission to use the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We implement a callback to handle the result of the permission request. If
    the user granted permission to use the camera, we enable the camera view. Otherwise,
    we log an error and exit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s turn our attention to the camera callbacks. When the camera preview
    starts (after the OpenCV library is loaded and permission to use the camera is
    obtained), we initialize our remaining member variables. To begin, we store the
    pixel dimensions that the camera is using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we initialize our face-detection variables, mostly through a private
    helper method called `initFaceDetector`. The role of `initFaceDetector` includes
    loading the detector''s cascade file, `app/main/res/raw/lbpcascade_frontalface.xml`.
    A lot of boilerplate code for file handling and error handling is involved in
    this task, so separating it into another function improves readability. We will
    examine the helper function''s implementation later in this section of the chapter,
    but here is the call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As we did in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml)*, Training
    a Smart Alarm to Recognize the Villain and His Cat*, we determine the smaller
    of the two image dimensions and use it in proportional size calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialize matrices relating to the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify colors (in **RGB** (**red, green, and blue**) format, not **BGR**
    (**blue, green, and red**)) for drawing a rectangle around the face and circles
    around the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialize variables relating to nod and shake recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialize and start the audio sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we initialize the image matrices, most of which are transposed to
    be in portrait format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'When the camera view stops, we do not do anything. Here is the empty callback
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'When the camera captures a frame, we do all the real work, the computer vision.
    We start by getting the color image (in **red, green, blue, and alpha** (**RGBA**)
    format, not BGR), convert it to grayscale, and reorient it to portrait format.
    The reorientation from landscape to portrait format is equivalent to rotating
    the image''s *content* 90 degrees *counterclockwise*, or rotating the image''s
    *X and Y **coordinate axes* 90 degrees *clockwise.* To accomplish this, we apply
    a transpose operation followed by a vertical flip. After reorienting the grayscale
    image to portrait format, we equalize it. Thus, the callback''s implementation
    begins as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We get the RGBA image by calling `inputFrame.rgba()` and then we convert it
    to grayscale. Alternatively, we could get the grayscale image directly by calling
    `inputFrame.gray()`. In our case, we want both the RGBA and grayscale images because
    we use the RGBA image for display and the grayscale image for detection and tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we declare a list of features. A standard Java `List` allows for fast
    insertion and removal of elements, whereas an OpenCV `Mat` does not, so we are
    going to need a `List` when we filter out features that did not track well. Here
    is the declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We detect faces—a familiar task from the Angora Blue project in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml)*,
    Training a Smart Alarm to Recognize the Villain and His Cat*. Unlike in OpenCV''s
    Python bindings, the structure to store the face rectangles is provided as an
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If at least one face is detected, we take the first detected face and draw
    a rectangle around it. We are performing face detection on an image in portrait
    orientation, but we are drawing the original image in landscape orientation, so
    some conversion of coordinates is necessary. Note that the origin (the upper-left
    corner) of the portrait image corresponds to the upper-right corner of the landscape
    image.  Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we select features within the inner part of the detected face. We specify
    the region of interest by passing a mask to OpenCV''s `goodFeaturesToTrack` function.
    A mask is an image that is white in the foreground (the inner part of the face)
    and black in the background. The following code finds the region of interest,
    creates the mask, and calls `goodFeaturesToTrack` with all relevant parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we copy the features into several variables: a matrix of initial
    features, a matrix of current features, and a mutable list of features that we
    will filter later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on whether we were already tracking a face, we call a helper function
    to either initialize our data on gestures or update our data on gestures. We also
    record that we are now tracking a face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we might not have detected any face in this frame. Then, we
    update any previously-selected features using OpenCV''s `calcOpticalFlowPyrLK`
    function to give us a matrix of new features, a matrix of error values, and a
    matrix of status values (`0` for an invalid feature, `1` for a valid feature).
    Being invalid typically means that the new feature is estimated to be outside
    the frame and thus it can no longer be tracked by optical flow. We convert the
    new features to a list and filter out the ones that are invalid or have a high
    error, as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If too few features remain after filtering, we deem that the face is no longer
    tracked and we discard all features. Otherwise, we put the accepted features back
    in the matrix of current features and we update our data on gestures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We draw green circles around the current features. Again, we must convert coordinates
    from portrait format back to landscape format in order to draw on the original
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the frame, the current equalized gray image and current features
    become the previous equalized gray image and previous features. Rather than copying
    these matrices, we swap references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We horizontally flip the preview image to make it look like a mirror. Then,
    we return it so that OpenCV can display it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We have mentioned several helper functions, which we will examine now. When
    we start analyzing face motion, we find the geometric mean of the features and
    use the mean''s *x* and *y* coordinates, respectively, as the starting coordinates
    for shake and nod gestures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Recall that our `BackAndForthGesture` class uses one-dimensional positions.
    For an upright head, only the *x* coordinate is relevant to a shake gesture and
    only the *y* coordinate is relevant to a nod gesture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, as we continue to analyze face motion, we find the features'' new
    geometric mean and use the mean''s coordinates to update the shake and nod data.
    Based on the number of back-and-forth shaking or nodding motions, we may take
    a yes branch or a no branch in the question-and-answer tree. Alternatively, we
    may decide that the user''s current gesture is ambiguous (both a yes and a no),
    in which case we reset the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We always reset the nod gesture data and the shake gesture data at the same
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Our helper method for initializing the face detector is very similar to the
    method found in an official OpenCV sample project that performs face detection
    on Android. We copy the cascade''s raw data from the app bundle to a new file
    that is more accessible. Then, we initialize a `CascadeClassifier` object using
    this file''s path. If an error is encountered at any point, we log it and close
    the app. Here is the method''s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s all the code! We are ready to test. Make sure your Android device has
    its sound turned on. Plug the device into a USB port and press the run button
    (the play icon in green). The first time you run the project, you might see the
    Select Deployment Target window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9dfbc1fd-fc7d-49bd-a22c-2b229ea9e69b.png)'
  prefs: []
  type: TYPE_IMG
- en: If you see this window, select your Android device and hit the OK button.
  prefs: []
  type: TYPE_NORMAL
- en: 'Soon, you should see the app''s camera preview appear on your device. Nod or
    shake your head knowingly as the questions are asked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bc6829a-8706-47d7-b9c3-efce3bd336ad.png)'
  prefs: []
  type: TYPE_IMG
- en: You should see a blue rectangle around your face, and green dots that remain
    (more or less) anchored to some features of your face as you move. Refer to the
    previous screenshot as an example.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the gesture detection results for your particular camera and environment,
    you may want to experiment with adjusting the parameters that we defined as constants
    in the code. Moreover, try to keep the camera still. Camera motion will interfere
    with our gesture detection algorithm because we rely on optical flow, which does
    not differentiate between camera motion and subject motion.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Silence is golden—or perhaps gestures are. At least, gestures can fill an awkward
    silence and control an app that whispers reminders in your earphones.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we built our first Android app with OpenCV's Java bindings.
    We also learned to use optical flow to track the movement of an object after detection.
    Thus, we are able to recognize a gesture, such as a head moving up and down in
    a nod.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, our project deals with motion in three dimensions. We will
    build a system that estimates changes in distance in order to alert a driver when
    the car is being followed.
  prefs: []
  type: TYPE_NORMAL
