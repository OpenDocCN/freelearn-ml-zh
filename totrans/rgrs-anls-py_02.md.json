["```py\nIn: import numpy as np\n vector = np.array([1,2,3,4,5])\n row_vector = vector.reshape((5,1))\n column_vector = vector.reshape((1,5))\n single_feature_matrix = vector.reshape((1,5))\n\n```", "```py\nIn:  multiple_feature_matrix = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\n\n```", "```py\nIn: multiple_feature_matrix = \\\nnp.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\n\n```", "```py\nIn: y = np.array([1,2,3,4,5]).reshape((5,1))\n\n```", "```py\nIn: from sklearn.datasets import fetch_california_housing\n from sklearn.datasets import load_boston\n boston = load_boston()\n california = fetch_california_housing()\n\n```", "```py\nboston and california variables available for analysis.\n```", "```py\nIn: import numpy as np\n import pandas as pd\n import matplotlib.pyplot as plt\n import matplotlib as mpl\n\n```", "```py\nIn: %matplotlib inline\n # If you are using IPython, this will make the images available in the Notebook\n\n```", "```py\nIn: dataset = pd.DataFrame(boston.data, columns=boston.feature_names)\n dataset['target'] = boston.target\n\n```", "```py\nIn: import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.mlab as mlab\nimport math\nx = np.linspace(-4,4,100)\nfor mean, variance in [(0,0.7),(0,1),(1,1.5),(-2,0.5)]:\n plt.plot(x,mlab.normpdf(x,mean,variance))\nplt.show()\n\n```", "```py\nIn: mean_expected_value = dataset['target'].mean()\n\n```", "```py\nIn: np.mean(dataset['target'])\n\n```", "```py\nIn: Squared_errors = pd.Series(mean_expected_value -\\\n dataset['target'])**2\n SSE = np.sum(Squared_errors)\n print ('Sum of Squared Errors (SSE): %01.f' % SSE)\n\n```", "```py\nIn: density_plot = Squared_errors.plot('hist')\n\n```", "```py\nIn: def standardize(x):\n return (x-np.mean(x))/np.std(x)\n\n```", "```py\nIn: \ndef covariance(variable_1, variable_2, bias=0):\n observations = float(len(variable_1))\n return np.sum((variable_1 - np.mean(variable_1)) * \\\n (variable_2 - np.mean(variable_2)))/(observations-min(bias,1))\n\n def standardize(variable):\n return (variable - np.mean(variable)) / np.std(variable)\n\n def correlation(var1,var2,bias=0):\n return covariance(standardize(var1), standardize(var2),bias)\n\n from scipy.stats.stats import pearsonr\n print ('Our correlation estimation: %0.5f' % (correlation(dataset['RM'], dataset['target'])))\n print ('Correlation from Scipy pearsonr estimation: %0.5f' % pearsonr(dataset['RM'], dataset['target'])[0])\n\nOut: Our correlation estimation: 0.69536\n Correlation from Scipy pearsonr estimation: 0.69536\n\n```", "```py\nIn: x_range = [dataset['RM'].min(),dataset['RM'].max()]\n y_range = [dataset['target'].min(),dataset['target'].max()]\n scatter_plot = dataset.plot(kind='scatter', x='RM', y='target',\\xlim=x_range, ylim=y_range)\n meanY = scatter_plot.plot(x_range, [dataset['target'].mean(),\\  dataset['target'].mean()], '--' , color='red', linewidth=1)\n meanX = scatter_plot.plot([dataset['RM'].mean(),\\dataset['RM'].mean()], y_range, '--', color='red', linewidth=1)\n\n```", "```py\nIn: import statsmodels.api as sm\n import statsmodels.formula.api as smf\n\n```", "```py\nIn: y = dataset['target']\n X = dataset['RM']\n X = sm.add_constant(X)\n\n```", "```py\nIn: X.head()\n\n```", "```py\nIn: linear_regression = sm.OLS(y,X)\n\n```", "```py\nIn: fitted_model = linear_regression.fit()\n\n```", "```py\nIn: linear_regression = smf.ols(formula='target ~ RM', data=dataset)\n fitted_model = linear_regression.fit()\n\n```", "```py\nIn: linear_regression = sm.OLS(y,X)\n fitted_model = linear_regression.fit()\n fitted_model.summary()\n\n```", "```py\nIn: print (fitted_model.params)\n betas = np.array(fitted_model.params)\n fitted_values = fitted_model.predict(X)\n\n```", "```py\nIn: mean_sum_squared_errors = np.sum((dataset['target']-\\dataset['target'].mean())**2)\n regr_sum_squared_errors = np.sum((dataset['target']-\\fitted_values)**2)\n (mean_sum_squared_errors-\\regr_sum_squared_errors) / mean_sum_squared_errors\n\nOut: 0.48352545599133412\n\n```", "```py\nIn: (pearsonr(dataset['RM'], dataset['target'])[0])**2\n\nOut: 0.4835254559913339\n\n```", "```py\nIn: 9.1021*4.55-34.6706\n\nOut: 6.743955\n\n```", "```py\nIn: 9.1021*5.55-34.6706\n\nOut: 15.846055\n\n```", "```py\nIn: (np.min(dataset['RM']),np.max(dataset['RM']))\nOut: (3.5609999999999999, 8.7799999999999994)\n\n```", "```py\nIn: residuals = dataset['target']-fitted_values\n normalized_residuals = standardize(residuals)\n\nIn: residual_scatter_plot = plt.plot(dataset['RM'], normalized_residuals,'bp')\nmean_residual = plt.plot([int(x_range[0]),round(x_range[1],0)], [0,0], '-', color='red', linewidth=2)\nupper_bound = plt.plot([int(x_range[0]),round(x_range[1],0)], [3,3], '--', color='red', linewidth=1)\nlower_bound = plt.plot([int(x_range[0]),round(x_range[1],0)], [-3,-3], '--', color='red', linewidth=1)\nplt.grid()\n\n```", "```py\nIn: RM = 5\n Xp = np.array([1,RM])\n print (\"Our model predicts if RM = %01.f the answer value \\is %0.1f\" % (RM, fitted_model.predict(Xp)))\n\nOut:  Our model predicts if RM = 5 the answer value is 10.8\n\n```", "```py\nIn: x_range = [dataset['RM'].min(),dataset['RM'].max()]\n y_range = [dataset['target'].min(),dataset['target'].max()]\n scatter_plot = dataset.plot(kind='scatter', x='RM', y='target',\\xlim=x_range, ylim=y_range)\n meanY = scatter_plot.plot(x_range,\\[dataset['target'].mean(),dataset['target'].mean()], '--',\\color='red', linewidth=1)\n meanX =scatter_plot.plot([dataset['RM'].mean(),\\dataset['RM'].mean()], y_range, '--', color='red', linewidth=1)\n regression_line = scatter_plot.plot(dataset['RM'], fitted_values,\\'-', color='orange', linewidth=1)\n\n```", "```py\nx and *y* averages.\n```", "```py\nIn: predictions_by_dot_product = np.dot(X,betas)\n print (\"Using the prediction method: %s\" % fitted_values[:10])\n print (\"Using betas and a dot product: %s\" % \npredictions_by_dot_product[:10])\n\nOut: Using the prediction method: [ 25.17574577  23.77402099  30.72803225  29.02593787  30.38215211\n 23.85593997  20.05125842  21.50759586  16.5833549   19.97844155]\nUsing betas and a dot product: [ 25.17574577  23.77402099  30.72803225  29.02593787  30.38215211\n 23.85593997  20.05125842  21.50759586  16.5833549   19.97844155]\n\n```", "```py\nIn: from sklearn import linear_model\n linear_regression = \\linear_model.LinearRegression(normalize=False,\\fit_intercept=True)\n\n```", "```py\nIn: observations = len(dataset)\nX = dataset['RM'].values.reshape((observations,1)) \n# X should be always a matrix, never a vector\ny = dataset['target'].values # y can be a vector\n\n```", "```py\nIn: linear_regression.fit(X,y)\n\n```", "```py\nIn: print (linear_regression.coef_)\n print (linear_regression.intercept_)\n\nOut: [ 9.10210898]\n -34.6706207764\n\n```", "```py\nIn: print (linear_regression.predict(X)[:10])\n\nOut: [ 25.17574577  23.77402099  30.72803225  29.02593787  30.38215211\n 23.85593997  20.05125842  21.50759586  16.5833549   19.97844155]\n\n```", "```py\nIn: Xp = np.column_stack((X,np.ones(observations)))\n v_coef = list(linear_regression.coef_) +\\[linear_regression.intercept_]\n\n```", "```py\nIn: np.dot(Xp,v_coef)[:10]\n\nOut: array([ 25.17574577,  23.77402099,  30.72803225,  29.02593787,\n 30.38215211,  23.85593997,  20.05125842,  21.50759586,\n 16.5833549 ,  19.97844155])\n\n```", "```py\nIn: from sklearn.datasets import make_regression\n HX, Hy = make_regression(n_samples=10000000, n_features=1,\\n_targets=1, random_state=101)\n\n```", "```py\nIn: %%time\n sk_linear_regression = linear_model.LinearRegression(\\normalize=False,fit_intercept=True)\n sk_linear_regression.fit(HX,Hy)\n\nOut: Wall time: 647 ms\n\n```", "```py\nIn: %%time\n sm_linear_regression = sm.OLS(Hy,sm.add_constant(HX))\n sm_linear_regression.fit()\n\nOut: Wall time: 2.13 s\n\n```", "```py\nIn: import numpy as np\n x = np.array([9.5, 8.5, 8.0, 7.0, 6.0])\n\n```", "```py\nIn: def squared_cost(v,e):\n return np.sum((v-e)**2)\n\n```", "```py\nIn: from scipy.optimize import fmin\n xopt = fmin(squared_cost, x0=0, xtol=1e-8, args=(x,))\n\nOut: Optimization terminated successfully.\n Current function value: 7.300000\n Iterations: 44\n Function evaluations: 88\n\n```", "```py\nIn: print ('The result of optimization is %0.1f' % (xopt[0]))\n print ('The mean is %0.1f' % (np.mean(x)))\n\nOut: The result of optimization is 78.0\n The mean is 78.0\n\n```", "```py\nIn: def absolute_cost(v,e):\n return np.sum(np.abs(v-e))\n\nIn: xopt = fmin(absolute_cost, x0=0, xtol=1e-8, args=(x,))\n\nOut: Optimization terminated successfully.\n Current function value: 5.000000\n Iterations: 44\n Function evaluations: 88\n\nIn: print ('The result of optimization is %0.1f' % (xopt[0]))\n print ('The median is %0.1f' % (np.median(x)))\n\nOut: The result of optimization is 8.0\n The median is 8.0\n\n```", "```py\nIn: observations = len(dataset)\n X  = dataset['RM'].values.reshape((observations,1)) # X should be always a matrix, never a vector\n Xb = np.column_stack((X,np.ones(observations))) # We add the bias\n y  = dataset['target'].values # y can be a vector\n\n def matrix_inverse(X,y, pseudo=False):\n if pseudo:\n return np.dot(np.linalg.pinv(np.dot(X.T,X)),np.dot(X.T,y))\n else:\n return np.dot(np.linalg.inv(np.dot(X.T, X)),np.dot(X.T,y))\n\n def normal_equations(X,y):\n return np.linalg.solve(np.dot(X.T,X), np.dot(X.T,y))\n\n print (matrix_inverse(Xb, y))\n print (matrix_inverse(Xb, y, pseudo=True))\n print (normal_equations(Xb, y))\n\nOut:\n [  9.10210898 -34.67062078]\n [  9.10210898 -34.67062078]\n [  9.10210898 -34.67062078]\n\n```", "```py\nIn: observations = len(dataset)\n X  = dataset['RM'].values.reshape((observations,1))\n # X should be always a matrix, never a vector\n X = np.column_stack((X,np.ones(observations))) # We add the bias\n y  = dataset['target'].values # y can be a vector\n\n```", "```py\nIn: import random\n\n def random_w( p ):\n return np.array([np.random.normal() for j in range(p)])\n\n def hypothesis(X,w):\n return np.dot(X,w)\n\n def loss(X,w,y):\n return hypothesis(X,w) - y\n\n def squared_loss(X,w,y):\n return loss(X,w,y)**2\n\n def gradient(X,w,y):\n gradients = list()\n n = float(len( y ))\n for j in range(len(w)):\n gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)\n return gradients\n\n def update(X,w,y, alpha=0.01):\n return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\n\n def optimize(X,y, alpha=0.01, eta = 10**-12, iterations = 1000):\n w = random_w(X.shape[1])\n path = list()\n for k in range(iterations):\n SSL = np.sum(squared_loss(X,w,y))\n new_w = update(X,w,y, alpha=alpha)\n new_SSL = np.sum(squared_loss(X,new_w,y))\n w = new_w\n if k>=5 and (new_SSL - SSL <= eta and \\new_SSL - SSL >= -eta):\n path.append(new_SSL)\n return w, path\n if k % (iterations / 20) == 0:\n path.append(new_SSL)\n return w, path\n\n```", "```py\nIN: alpha = 0.048\n w, path = optimize(X,y,alpha, eta = 10**-12, iterations = 25000)\n print (\"These are our final coefficients: %s\" % w)\n print (\"Obtained walking on this path of squared loss %s\" % path)\n\nOut: These are our final coefficients: [9.1021032698295059,\\-34.670584445862119]\n Obtained walking on this path of squared loss [369171.02494038735,   23714.645148620271, 22452.194702610999, 22154.055704515144,   22083.647505550518, 22067.019977742671, 22063.093237887566,   22062.165903044533, 22061.946904602359, 22061.895186155631,   22061.882972380481, 22061.880087987909, 22061.879406812728,   22061.879245947097, 22061.879207957238, 22061.879198985589,   22061.879196866852, 22061.879196366495, 22061.879196248334,   22061.879196220427, 22061.879196220034]\n\n```"]