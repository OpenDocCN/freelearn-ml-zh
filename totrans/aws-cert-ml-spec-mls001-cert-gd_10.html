<html><head></head><body>
		<div id="_idContainer136">
			<h1 id="_idParaDest-132"><em class="italic"><a id="_idTextAnchor136"/>Chapter 7</em>: Applying Machine Learning Algorithms</h1>
			<p>In the previous chapter, we studied AWS services for data processing, including Glue, Athena, and Kinesis! It is now time to move on to the modeling phase and study machine learning algorithms. I am sure that, during the earlier chapters, you have realized that building machine learning models requires a lot of knowledge about AWS services, data engineering, data exploratory, data architecture, and much more. This time, we will go deeper into the algorithms that we have been talking about so far and many others.</p>
			<p>Having a good sense of the different types of algorithms and machine learning approaches will put you in a very good position to make decisions during your projects. Of course, this type of knowledge is also crucial to the AWS machine learning specialty exam.</p>
			<p>Bear in mind that there are thousands of algorithms out there and, by the way, you can even propose your own algorithm for a particular problem. Furthermore, we will cover the most relevant ones and, hopefully, the ones that you will probably face in the exam.</p>
			<p>The main topics of this chapter are as follows:</p>
			<ul>
				<li>Storing the training data</li>
				<li>A word about ensemble models</li>
				<li>Regression models</li>
				<li>Classification models</li>
				<li>Forecasting models</li>
				<li>Object2Vec</li>
				<li>Clustering</li>
				<li>Anomaly detection</li>
				<li>Dimensionality reduction</li>
				<li>IP Insights</li>
				<li>Natural language processing</li>
				<li>Reinforcement learning</li>
			</ul>
			<p>Alright, let's do it!</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor137"/>Introducing this chapter</h1>
			<p>During this chapter, we will talk about several algorithms, modeling concepts, and learning strategies. We think all these topics will be beneficial for you during the exam and your data scientist career.</p>
			<p>We have structured this chapter in a way so that it covers not only the necessary topics of the exam but also gives you a good sense of the most important learning strategies out there. For example, the exam will check your knowledge regarding the basic concepts of K-means; however, we will cover it on a much deeper level, since this is an important topic for your career as a data scientist.</p>
			<p>We will follow this approach, looking deeper into the logic of the algorithm, for some types of models that we feel every data scientist should master. So, keep that in mind: sometimes, we might go deeper than expected in the exam, but that will be extremely important for you.</p>
			<p>Many times, during this chapter, we will use the term <strong class="bold">built-in algorithms</strong>. We will use this term when we want to refer to the list of algorithms that's implemented by AWS on their SageMaker SDK.</p>
			<p>Let me give you a concrete example: you can use scikit-learn's KNN algorithm (if you don't remember what scikit-learn is, refresh your memory by going back to <a href="B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Fundamentals</em>) to create a classification model and deploy it to SageMaker. However, AWS also offers its own implementation of the KNN algorithm on their SDK, which is optimized to run in the AWS environment. Here, KNN is an example of a built-in algorithm.</p>
			<p>As you can see, the possibilities on AWS are endless, because you can either take advantage of built-in algorithms or bring in your own algorithm to create models on SageMaker. Finally, just to make this very clear, here is an example of how to import a built-in algorithm from the AWS SDK:</p>
			<p class="source-code">import sagemaker</p>
			<p class="source-code">knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, "knn"),</p>
			<p class="source-code">        get_execution_role(),</p>
			<p class="source-code">        train_instance_count=1,</p>
			<p class="source-code">        train_instance_type='ml.m5.2xlarge',</p>
			<p class="source-code">        output_path=output_path,</p>
			<p class="source-code">        sagemaker_session=sagemaker.Session())</p>
			<p class="source-code">knn.set_hyperparameters(**hyperparams)</p>
			<p>You will learn how to create models on SageMaker in <a href="B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173"><em class="italic">Chapter 9</em></a>, <em class="italic">Amazon SageMaker Modeling</em>. For now, just understand that AWS has its own set of libraries where those built-in algorithms are implemented.</p>
			<p>To train and evaluate a model, you need training and testing data. After instantiating your estimator, you should then feed it with those datasets. I don't want to spoil <a href="B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173"><em class="italic">Chapter 9</em></a>, <em class="italic">Amazon SageMaker Modeling</em>, but you should know the concepts of <strong class="bold">data channels</strong> in advance.</p>
			<p>Data channels are<a id="_idIndexMarker535"/> configurations related to input data that you can pass to SageMaker when you're creating a training job. You should set these configurations just to inform SageMaker of how your input data is formatted.</p>
			<p>In <a href="B16735_09_Final_VK_ePub.xhtml#_idTextAnchor173"><em class="italic">Chapter 9</em></a>, <em class="italic">Amazon SageMaker Modeling</em>, you will learn how to create training jobs and how to set data channels. As of now, you should know that, while configuring data channels, you can set <strong class="bold">content types</strong> (<strong class="bold">ContentType</strong>) and an <strong class="bold">input mode</strong> (<strong class="bold">TrainingInputMode</strong>). Let's take a closer look at how and where the training data should be stored so that it can be integrated properly with AWS's built-in algorithms.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor138"/>Storing the training data</h1>
			<p>First <a id="_idIndexMarker536"/>of all, you can use multiple AWS services to prepare data for machine learning, such as EMR, Redshift, Glue, and so on. After preprocessing the training data, you should store it in S3, in a format expected by the algorithm you are using. The following table shows the list of acceptable data formats per algorithm:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B16735_07_001.jpg" alt="Figure 7.1 – Data formats that are acceptable per AWS algorithm&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Data formats that are acceptable per AWS algorithm</p>
			<p>As we can see, many algorithms accept text<strong class="source-inline">/.csv</strong> format. Keep in mind that you should follow these rules if you want to use that format:</p>
			<ul>
				<li>Your CSV file <em class="italic">can't</em> have a header record.</li>
				<li>For supervised learning, the target variable must be in the first column.</li>
				<li>While configuring the training pipeline, set the input data channel as <strong class="source-inline">content_type</strong> equal to <strong class="source-inline">text/csv</strong>.</li>
				<li>For unsupervised learning, set the <strong class="source-inline">label_size</strong> within the <strong class="bold">content_type</strong> to <strong class="source-inline">'content_type=text/csv;label_size=0'</strong>.</li>
			</ul>
			<p>Although<a id="_idIndexMarker537"/> text/.csv format is fine for many use cases, most of the time, AWS's built-in algorithms work better <a id="_idIndexMarker538"/>with <strong class="bold">recordIO-protobuf</strong>. This is an optimized data format that's used to train AWS's built-in algorithms, where SageMaker converts each observation in the dataset into a binary representation that's a set of 4-byte floats.</p>
			<p>RecordIO-protobuf accepts two types of input mode: <strong class="bold">pipe mode</strong> and <strong class="bold">file mode</strong>. In pipe mode, the data will be streamed directly from S3, which helps optimize storage. In file mode, the data is copied from S3 to the training instance's store volume.</p>
			<p>We are almost ready! Now, let's have a quick look at some modeling definitions that will help you understand some more advanced algorithms.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor139"/>A word about ensemble models</h1>
			<p>Before we start <a id="_idIndexMarker539"/>diving into the algorithms, there is an important modeling concept that you should be <a id="_idIndexMarker540"/>aware of, known as <strong class="bold">ensemble</strong>. The term ensemble is used to describe methods that use multiple algorithms to create a model. </p>
			<p>For example, instead of creating just one model to predict fraudulent transactions, you could create multiple models that do the same thing and, using a vote sort of system, select the predicted outcome. The following table illustrates this simple example:</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B16735_07_002.jpg" alt="Figure 7.2 – An example of a voting system on ensemble methods&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – An example of a voting system on ensemble methods</p>
			<p>The same approach works for <a id="_idIndexMarker541"/>regression problems, where, instead of voting, we could average the results of each model and use that as the final outcome.</p>
			<p>Voting and averaging are just two examples of ensemble approaches. Other powerful techniques <a id="_idIndexMarker542"/>include <strong class="bold">blending</strong> and <strong class="bold">stacking</strong>, where you can create multiple models and use the outcome of each <a id="_idIndexMarker543"/>model as features for a main model. Looking back at the preceding table, columns "Model A," "Model B," and "Model C" would be used as features to predict the final outcome.</p>
			<p>It turns out that many machine learning algorithms use ensemble methods while training, in an embedded way. These algorithms can be classified into two main categories:</p>
			<ul>
				<li><strong class="bold">Bootstrapping Aggregation</strong> or <strong class="bold">Bagging</strong>: With this approach, several models are trained on top<a id="_idIndexMarker544"/> of different samples of the da<a id="_idTextAnchor140"/>ta. Then, predictions <a id="_idIndexMarker545"/>are made through the voting or averaging system. The<a id="_idIndexMarker546"/> main algorithm from this category is known as <strong class="bold">Random Forest</strong>.</li>
				<li><strong class="bold">Boosting</strong>: With this approach, several <a id="_idIndexMarker547"/>models are trained on top of different samples of the data. Then, one model tries to correct the error of the next model by penalizing incorrect predictions. The<a id="_idIndexMarker548"/> main algorithms from this category are <a id="_idIndexMarker549"/>known as <strong class="bold">Stochastic Gradient Boosting</strong> and <strong class="bold">AdaBoost</strong>.</li>
			</ul>
			<p>Now that you know what ensemble models are, let's move on and study some machine learning algorithms that are likely to be present in your exam. Not all of them use ensemble approaches, but I trust it is going to be easier for you to recognize that.</p>
			<p>We will split the next few sections up based on AWS algorithm categories:</p>
			<ul>
				<li>Supervised learning</li>
				<li>Unsupervised learning</li>
				<li>Textual analysis</li>
				<li>Image processing</li>
			</ul>
			<p>Finally, we will provide an overview of reinforcement learning in AWS.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor141"/>Supervised learning</h1>
			<p>AWS provides <a id="_idIndexMarker550"/>supervised learning algorithms for general purposes (regression and classification tasks) and for more specific purposes (forecasting and vectorization). The list of built-in algorithms that can be found in these sub-categories is as follows:</p>
			<ul>
				<li>Linear learner algorithm</li>
				<li>Factorization machines algorithm</li>
				<li>XGBoost algorithm</li>
				<li>K-Nearest Neighbor algorithm</li>
				<li>Object2Vec algorithm</li>
				<li>DeepAR Forecasting algorithm</li>
			</ul>
			<p>Let's start with regression models and the linear learner algorithm.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor142"/>Working with regression models</h2>
			<p>Okay; I <a id="_idIndexMarker551"/>know that real <a id="_idIndexMarker552"/>problems usually aren't linear nor simple. However, looking into <strong class="bold">linear regression</strong> models is a<a id="_idIndexMarker553"/> nice way to figure out what's going on inside <strong class="bold">regression models</strong> in general (yes, regression models can be linear and non-linear). This is mandatory knowledge for every data scientist and can help you solve real challenges as well. We'll take a closer look at this in the following subsections.</p>
			<h3>Introducing regression algorithms</h3>
			<p>Linear regression <a id="_idIndexMarker554"/>models aim to predict a numeric value (Y) according to one or more variables (X). Mathematically, we can define such a relationship as Y = f(X), where Y is known <a id="_idIndexMarker555"/>as the <strong class="bold">dependent variable</strong> and X is known as <a id="_idIndexMarker556"/>the <strong class="bold">independent variable</strong>.</p>
			<p>With regression models, the component that we want to predict (Y) is always a continuous number; for example, the price of houses or the number of transactions. We saw that in <a href="B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Fundamentals</em>, <em class="italic">Table 2</em>, when we were choosing the right type of supervised learning algorithm, given the target variable. Please, feel free to go back and review it.</p>
			<p><em class="italic">When we use just one variable to predict Y</em>, we refer to this<a id="_idIndexMarker557"/> problem as <strong class="bold">simple linear regression</strong>. On the other hand, when we use <em class="italic">more than one variable to predict Y</em>, we say that we <a id="_idIndexMarker558"/>have a <strong class="bold">multiple linear regression</strong> problem.</p>
			<p>There is also another class of regression models, known as <strong class="bold">non-linear regression</strong>. However, let's put that aside for a moment and <a id="_idIndexMarker559"/>understand what simple linear regression means.</p>
			<p>Regression models belong to the supervised side of machine learning (the other side is non-supervised) because algorithms try to predict values according to existing correlations between independent and dependent variables.</p>
			<p>But what does "f" mean in Y=f(X)? "f" is the regression function responsible for predicting Y based on X. In other words, this is the function that we want to figure out! When we start talking about simple linear regression, pay attention to the next three questions and answers:</p>
			<ol>
				<li value="1">What is the shape of "f" in linear regression? <p>Linear, sure!</p></li>
				<li>How can we represent a linear relationship? <p>Using a <em class="italic">straight </em>line (you will understand why in a few minutes).</p></li>
				<li>So, what's the function that defines a line? <p>ax + b (just check any <em class="italic">mathematics</em> book).</p></li>
			</ol>
			<p>That's it! Linear regression models are given by <strong class="bold">y = ax + b</strong>. Once we are trying to predict Y given X, we just need to find out the values of "a" and "b". We can adopt the same logic to figure out what's going on inside other kinds of regression.</p>
			<p>And believe me, finding out the values of "a" and "b" isn't the only thing we're going to do. It's nice to know that "a" is also known as <a id="_idIndexMarker560"/>the <strong class="bold">alpha coefficient</strong>, or <strong class="bold">slope</strong>, and<a id="_idIndexMarker561"/> represents the line's inclination, while "b" is also known as <a id="_idIndexMarker562"/>the <strong class="bold">beta coefficient</strong>, or <strong class="bold">y-intercept</strong>, and represents the place where the line crosses the <a id="_idIndexMarker563"/>y-axis (into a two-dimensional plan consisting of x and y). You will see these two terms in the next subsection.</p>
			<p>It's also nice to know that<a id="_idIndexMarker564"/> there is an error associated with every predictor that we don't have control. Let's name it "e" and formally define simple linear regression as <strong class="bold">y = ax + b + e</strong>. Mathematically, this error is expressed by the difference between the prediction and the real value.</p>
			<p>Alright, let's find alpha and beta and give this section a happy ending!</p>
			<h3>Least square method</h3>
			<p>There are <a id="_idIndexMarker565"/>different ways to find the slope and y-intercept of a line, but the most used method is known as the <strong class="bold">least square method</strong>. The principle behind this method is simple: we have to find the <em class="italic">best line that reduces the sum of squared error</em>. </p>
			<p>In the following graph, we can see a Cartesian plane with multiple points and lines in it. "Line a" represents the best fit for this data – in other words, that would be the best linear regression function for those points. But how do I know that? It's simple: if we compute the error associated with each point (like the one we've zoomed in on in the following graph), we will realize that "line a" contains the least sum of square errors:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B16735_07_003.jpg" alt="Figure 7.3 – Visualizing the principle of the least square method&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Visualizing the principle of the least square method</p>
			<p>It is worth <a id="_idIndexMarker566"/>understanding linear regression from scratch, not only for the certification exam but mainly for your career as a data scientist. To provide you with a complete example, we have developed a spreadsheet containing all the calculations that we are going to see, step by step! We encourage you to jump on this support material and perform some simulations. Let's see this in action.</p>
			<h3>Creating a linear regression model from scratch</h3>
			<p>We are going to use a very <a id="_idIndexMarker567"/>simple dataset, with only two variables:</p>
			<ul>
				<li><em class="italic">X</em>: Represents the person's number of years of work experience</li>
				<li><em class="italic">Y</em>: Represents the person's average salary </li>
			</ul>
			<p>We want to understand the relationship between X and Y and, if possible, predict the salary (Y) based on years of experience (X). As I mentioned previously, often, real problems have far more independent variables and are not necessarily linear. However, I am sure this example will give you the baseline knowledge to master more complex algorithms.</p>
			<p>To find out what the alpha and beta coefficients are (or slope and y-intercept, if you prefer), we need to find some statistics related to the dataset, so let's take a look at the data and the auxiliary statistics shown here:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B16735_07_004.jpg" alt=" Figure 7.4 – Dataset to predict average salary based on amount of work experience&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 7.4 – Dataset to predict average salary based on amount of work experience</p>
			<p>As we can see, there <a id="_idIndexMarker568"/>is an almost perfect linear relationship between X and Y. As the amount of work experience increases, so does the salary. In addition to X and Y, we need to compute the following statistics: the number of records, the mean of X, the mean of Y, the covariance of X and Y, the variance of X, and the variance of Y. The following formulas provide a mathematical representation of variance and covariance (respectively), where <em class="italic">x bar</em>, <em class="italic">y bar</em>, and <em class="italic">n</em> represent the mean of X, the mean of Y, and the number of records, respectively:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/image3.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/image4.jpg" alt=""/>
				</div>
			</div>
			<p>If you want to check the calculation details of the formulas for each of those auxiliary statistics in <em class="italic">Table 7.2</em>, please refer to the support material provided along with this book. There, you will find those formulas already implemented for you.</p>
			<p>These statistics are important because they will be used to compute our alpha and beta coefficients. The following image explains how we are going to compute both coefficients, along with the<a id="_idIndexMarker569"/> correlation <a id="_idIndexMarker570"/>coefficients <strong class="bold">R</strong> and <strong class="bold">R squared</strong>. These last two metrics will give us an <a id="_idIndexMarker571"/>idea about the quality of the model, where the closer the model is to 1, the better the model is:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B16735_07_005.jpg" alt="Figure 7.5 – Equations to calculate coefficients for simple linear regression&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Equations to calculate coefficients for simple linear regression</p>
			<p>After applying these formulas, we will come up with the results shown here:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B16735_07_006.jpg" alt="Figure 7.6 – Finding regression coefficients&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Finding regression coefficients</p>
			<p>The preceding table already contains all the information that we need to make predictions on top of the new data. If we replace the coefficients in the original equation, <strong class="bold">y = ax + b + e</strong>, we will find the regression formula to be as follows:</p>
			<p>Y = 1021,212 * X + 53,3</p>
			<p>From this point on, to make predictions, all we have to do is replace X with the number of years of experience. As a result, we will find Y, which is the projected salary. We can see the model fit in the<a id="_idIndexMarker572"/> following graph and some model predictions in <em class="italic">Figure 7.8</em>:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B16735_07_007.jpg" alt="Figure 7.7 – Fitting data in the regression equation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 7.7 – Fitting data in the regression equation</p>
			<p>We see the prediction values here:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B16735_07_008.jpg" alt="Figure 7.8 – Model predictions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – Model predictions</p>
			<p>While you are analyzing regression models, you should be able to understand whether your model is of a good quality or not. We talked about many modeling issues (such as overfitting) in <a href="B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Fundamentals</em>, and you already know that you always have to check model performance.</p>
			<p>A good approach to <a id="_idIndexMarker573"/>regression models is performing what is called <strong class="bold">residual analysis</strong>. This is where we plot the errors of the model in a scatter plot and check if<a id="_idIndexMarker574"/> they are randomly distributed (as expected) or not. If the errors are <em class="italic">not</em> randomly distributed, this means that your model was unable to generalize the data. The following graph shows a residual analysis of our example:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B16735_07_009.jpg" alt="Figure 7.9 – Residual analysis&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – Residual analysis</p>
			<p>The takeaway here is that the errors are randomly distributed. Such evidence, along with a high R squared rating, can be used as arguments to support the use of this model.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In <a href="B16735_08_Final_VK_ePub.xhtml#_idTextAnchor162"><em class="italic">Chapter 8</em></a>, <em class="italic">Evaluating and Optimizing Models</em>, we will learn about evaluation metrics. For instance, we will learn that each type of model may have its own set of evaluation metrics. Regression models <a id="_idIndexMarker575"/>are commonly evaluated with <strong class="bold">Mean Square Error</strong> (<strong class="bold">MSE</strong>) and <strong class="bold">Root Mean Square Error</strong> (<strong class="bold">RMSE</strong>). In other words, apart from R, R squared, and<a id="_idIndexMarker576"/> residual analysis, ideally, you will execute your model on test sets to extract other performance metrics. You can even use a cross-validation system to check model performance, as we learned in <a href="B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Fundamentals</em>.</p>
			<p>Very often, when the model residuals <em class="italic">do</em> present a pattern and are <em class="italic">not</em> randomly distributed, it's because the existing relationship in the data is not linear, but non-linear, so another modeling technique must be applied. Now, let's take a look at how we can interpret a model's results.</p>
			<h3>Interpreting regression models</h3>
			<p>It is also good to know how<a id="_idIndexMarker577"/> to interpret a linear regression model. Sometimes, we use linear regression not necessarily to create a predictive model but to do a regression analysis, where we can understand the relationship between the independent and dependent variables.</p>
			<p>Looking back at our regression equation (Y = 1021,212 * X + 53,3), we can see our two terms: alpha or slope (1021.2) and beta or y-intercept (53.3). We can interpret this model as follows: <em class="italic">for each additional year of working experience, you will increase your salary by $1,021.3 dollars</em>. Also, note that when "years of experience" is equal to zero, the expected salary is going to be $53.3 dollars (this is the point where our straight line crosses the y-axis).</p>
			<p>From a generic perspective, your regression analysis should answer the following question: for each extra unit that's added to the independent variable (slope), what is the average change in the dependent variable? Please take notes and make sure you know how to interpret simple linear regression models – this is a very important thing to do as one of your daily activities as a data scientist! Let's move on and take a look at some final considerations about linear regression.</p>
			<h3>Checking R squared adjusted</h3>
			<p>At this point, I <a id="_idIndexMarker578"/>hope you have a much better idea of regression models! There is just another very important topic that you should be aware of, regardless of whether it will come up in the exam or not, which is the parsimony aspect of your model.</p>
			<p>We have talked about parsimony already, in <a href="B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Fundamentals</em>. This is your ability to prioritize simple models over complex models. Looking into regression models, you might have to use more than one feature to predict your outcome. This is also known as a multiple regression model.</p>
			<p>When that's the case, the R and R squared coefficients tend to reward more complex models that have more features. In other words, if you keep adding new features to a multiple regression model, you will come up with higher R and R squared coefficients. That's why you <em class="italic">can't</em> anchor your decisions <em class="italic">only</em> based on those two metrics.</p>
			<p>Another <a id="_idIndexMarker579"/>additional metric that you could use (apart from R, R squared, MSE, and RMSE) is known as <strong class="bold">R squared adjusted</strong>. This metric is penalized when we add extra features to the model that do not bring any real gain. In the following table, we have illustrated a hypothetical example just to show you when you are starting to lose parsimony:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B16735_07_010.jpg" alt="Figure 7.10 – Comparing R squared and R squared adjusted&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – Comparing R squared and R squared adjusted</p>
			<p>Here, we can conclude that maintaining three variables in the model is better than maintaining four or five variables. Adding four or five variables to that model will increase R squared (as expected), but decrease R squared adjusted.</p>
			<p>Alright; at this point, you should have a very good understanding of regression models. Now, let's check what AWS offers in terms of built-in algorithms for this class of models. That is going to be important for your exam, so let's take a look.</p>
			<h3>Regression modeling on AWS</h3>
			<p>AWS has <a id="_idIndexMarker580"/>a built-in<a id="_idIndexMarker581"/> algorithm known as <strong class="bold">linear learner</strong>, where we<a id="_idIndexMarker582"/> can implement linear regression models. The built-in linear learner<a id="_idIndexMarker583"/> uses <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>) to train the model.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We will learn more about SGD when we talk about neural networks. For now, we can look at SGD as an alternative to the popular least square error method that we just dissected.</p>
			<p>The linear<a id="_idIndexMarker584"/> learn built-in<a id="_idIndexMarker585"/> algorithm provides a hyperparameter that can apply normalization to the data, prior to the training process. The name of this hyperparameter is <strong class="source-inline">normalize_data</strong>. This is very helpful since linear models are sensitive to the scale of the data and usually take advantage of data normalization.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We talked about data normalization in <a href="B16735_03_Final_VK_ePub.xhtml#_idTextAnchor059"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Preparation and Transformation</em>. Please review that chapter if you need to.</p>
			<p>Some other important hyperparameters of the linear learner algorithm are <strong class="bold">L1</strong> and <strong class="bold">wd</strong>, which <a id="_idIndexMarker586"/>play the<a id="_idIndexMarker587"/> roles of <strong class="bold">L1 regularization</strong> and <strong class="bold">L2 regularization</strong>, respectively. </p>
			<p>L1 and L2 regularization help the linear learner (or any other regression algorithm implementation) avoid overfitting. Conventionally, we call regression models that implement L1 regularization <strong class="bold">Lasso Regression</strong> models, while for <a id="_idIndexMarker588"/>regression models with L2 regularization, we<a id="_idIndexMarker589"/> call them <strong class="bold">Ridge Regression</strong> models.</p>
			<p>Although it might sound complex, it is not! Actually, the regression model equation is still the same; that is, <strong class="bold">y = ax + b + e</strong>. The change is in the loss function, which is used to find the coefficients that best minimize the error. If you look back at <em class="italic">Figure 7.3</em>, you will see that we have defined the error function as <strong class="bold">e = (ŷ - y)2</strong>, where <strong class="bold">ŷ</strong> is the regression function value and <strong class="bold">y</strong> is the real value.</p>
			<p>L1 and L2 regularization add a penalty term to the loss function, as shown in the following formulas (note that we are replacing ŷ with ax + b):</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/image10.jpg" alt=""/>
				</div>
			</div>
			<p>The λ (lambda) parameter must be greater than 0 and manually tuned. A very high lambda value may result in an underfitting issue, while a very low lambda may not result in expressive changes in the final results (if your model is overfitted, it will stay overfitted). </p>
			<p>In practical <a id="_idIndexMarker590"/>terms, the main difference between L1 and L2 regularization is that L1 will shrink<a id="_idIndexMarker591"/> the less important coefficients to zero, which will force the feature to be dropped (acting as a feature selector). In other words, if your model is overfitting due to it having a high number of features, L1 regularization should help you solve this problem.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">During your exam, remember the basis of L1 and L2 regularization, especially the key difference between them, where L1 works well as a feature selector.</p>
			<p>Last but not least, many built-in algorithms can serve multiple modeling purposes. The linear learner algorithm can be used for regression, binary classification, and multi-classification. Make sure you remember this during your exam (it is <em class="italic">not only</em> about regression models).</p>
			<p>Still going in that direction, AWS<a id="_idIndexMarker592"/> has other built-in algorithms that work for regression and classification problems; that is, <strong class="bold">Factorization Machines</strong>, <strong class="bold">K-Nearest Neighbor</strong> (<strong class="bold">KNN</strong>) and the <strong class="bold">XGBoost</strong> algorithm. Since these algorithms can also be used for<a id="_idIndexMarker593"/> classification purposes, we'll cover them in the<a id="_idIndexMarker594"/> section about classification algorithms.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You just got a very important tip to remember during the exam: linear learner, Factorization Machines, K-Nearest Neighbor, and XGBoost are suitable for both regression and classification problems. These algorithms are often known as algorithms for general purposes.</p>
			<p>With that, we have reached the end of this section on regression models. I hope you have enjoyed it; remember to check out our support material before you take your exam. By the way, you can use that reference material when you're working on your daily activities! Now, let's move on to another classical example of a machine learning problem: classification models.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor143"/>Working with classification models</h2>
			<p>You have been learning what<a id="_idIndexMarker595"/> classification models are throughout this book. However, now, we are going to discuss some algorithms that are suitable for classification problems. Keep<a id="_idIndexMarker596"/> in mind that there are hundreds of classification algorithms out there, but since we are preparing for the AWS machine learning specialty exam, we will cover the ones that have been pre-built by AWS.</p>
			<p>We already know what the linear learner does (and we know that it is suitable for both regression and classification tasks), so let's take a look at the other built-in algorithms for general purposes (which includes classification tasks).</p>
			<p>We will start with <strong class="bold">Factorization Machines</strong>. Factorization machines are<a id="_idIndexMarker597"/> considered an extension of the linear learner, optimized to find the relationship between features within high-dimensional sparse datasets.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">A very traditional use case for<a id="_idIndexMarker598"/> Factorization machines is <em class="italic">recommendation systems</em>, where we usually have a high level of sparsity in the data. During the exam, if you are faced with a general-purpose problem (either a regression or binary classification task) where the underlying datasets are sparse, then Factorization Machines are probably the best answer from an algorithm perspective.</p>
			<p>When we use Factorization Machines in a regression model, the <strong class="bold">Root Mean Square Error</strong> (<strong class="bold">RMSE</strong>) will be used to evaluate <a id="_idIndexMarker599"/>the model. On the other hand, in binary classification mode, the algorithm will use Log Loss, Accuracy, and F1 score to evaluate results. We will have a deeper discussion about evaluation metrics in <a href="B16735_08_Final_VK_ePub.xhtml#_idTextAnchor162"><em class="italic">Chapter 8</em></a>, <em class="italic">Evaluating and Optimizing Models</em>.</p>
			<p>You should be <a id="_idIndexMarker600"/>aware that Factorization Machines only accept input data in <strong class="bold">recordIO-protobuf</strong> format. This is <a id="_idIndexMarker601"/>because of the data sparsity issue, in which recordIO-protobuf is supposed to do a better job on data processing than text/.csv format.</p>
			<p>The next built-in algorithm suitable for classification problems is known as K-Nearest Neighbors, or KNN for short. As the name suggests, this algorithm will try to find the <em class="italic">K</em> closest points to the input data and return either of the following predictions:</p>
			<ul>
				<li>The most repeated class of the k closest points, if it's a classification task</li>
				<li>The average value of the label of the k closest points, if it's a regression task</li>
			</ul>
			<p>We say that KNN is an <strong class="bold">index-based algorithm</strong> because it computes distances between points, assigns indexes<a id="_idIndexMarker602"/> for these points, and then stores the sorted distances and their indexes. With that type of data structure, KNN can easily select the top K closest points to make the final prediction.</p>
			<p>Note that K is a hyperparameter of KNN and should be optimized during the modeling process.</p>
			<p>The other AWS built-in algorithm available for general purposes, including classification, is known as <strong class="bold">eXtreme Gradient Boosting</strong>, or <strong class="bold">XGBoost</strong> for short. This is an ensemble, decision<a id="_idIndexMarker603"/> tree-based model.</p>
			<p>XGBoost uses a set of <strong class="bold">weaker</strong> models (decision trees) to predict the target variable, which can be a regression task, binary class, or multi-class. This is a very popular algorithm and has been used in machine learning competitions by the top performers.</p>
			<p>XGBoost uses a boosting learning strategy when one model tries to correct the error of the prior model. It carries the name "gradient" because it uses the gradient descent algorithm to minimize the loss when adding new trees.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The term weaker is used in this context to describe very simple decision trees.</p>
			<p>Although XGBoost <a id="_idIndexMarker604"/>is much more robust than a single decision tree, it is important to go into the exam with a clear understanding of what decision trees are and their main configurations. By the way, they are the base model of many ensemble algorithms, such as AdaBoost, Random Forest, Gradient Boost, and XGBoost.</p>
			<p>Decision trees are rule-based algorithms that organize decisions in the form of a tree, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B16735_07_011.jpg" alt="Figure 7.11 – Example of what a decision tree model looks like&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.11 – Example of what a decision tree model looks like</p>
			<p>They are formed by a root node (at the very top of the tree), intermediary or decision nodes (in the middle of the tree), and leaf nodes (bottom nodes with no splits). The depth of the tree is given by the difference between the root node and the very last leaf node. For example, in the preceding diagram, the depth of the tree is 3. </p>
			<p>The depth of the tree is one of the most important hyperparameters of this type of model and it is often known <a id="_idIndexMarker605"/>as the <strong class="bold">max depth</strong>. In other words, max depth controls the maximum depth that a decision tree can reach.</p>
			<p>Another very important hyperparameter of decision tree models is known as the minimum number of samples/observations in the leaf nodes. It is also used to control the growth of the tree.</p>
			<p>Decision trees have many other <a id="_idIndexMarker606"/>types of hyperparameters, but these two are especially important for controlling how the model overfits. Decision trees with a high depth or very small number of observations in the leaf nodes are likely to face issues during extrapolation/prediction.</p>
			<p>The reason for this is simple: decision trees use data from the leaf nodes to make predictions, based on the proportion (for classification tasks) or average value (for regression tasks) of each observation/target variable that belongs to that node. Thus, the node should have enough data to make good predictions outside the training set.</p>
			<p>In case you face the term <strong class="bold">CART</strong> during the exam, you should know that it stands for <strong class="bold">Classification and Regression Trees</strong>, since <a id="_idIndexMarker607"/>decision trees can be used for classification and regression tasks.</p>
			<p>To select the best variables to split the data in the tree, the model will choose the ones that maximize the separation of the target variables across the nodes. This task can be performed by different <a id="_idIndexMarker608"/>methods, such <a id="_idIndexMarker609"/>as <strong class="bold">Gini</strong> and <strong class="bold">Information Gain</strong>.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor144"/>Forecasting models</h2>
			<p><strong class="bold">Time series</strong>, or <strong class="bold">TS</strong> for short, refers<a id="_idIndexMarker610"/> to data points that are<a id="_idIndexMarker611"/> collected on <a id="_idIndexMarker612"/>a regular basis with an ordered dependency. Time series have a measure, a fact, and a time unit, as shown in the following image:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B16735_07_012.jpg" alt="Figure 7.12 – Time series statement&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.12 – Time series statement</p>
			<p>Additionally, time series can be classified as <strong class="bold">univariate</strong> or <strong class="bold">multivariate</strong>. Univariate time series <a id="_idIndexMarker613"/>have just one variable connected across a period of time, while a multivariate time series<a id="_idIndexMarker614"/> have two or more variables connected across a period of time. The following graph shows the univariate time series we showed in the preceding image: </p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B16735_07_013.jpg" alt="Figure 7.13 – Time series example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13 – Time series example</p>
			<p>Time series can be<a id="_idIndexMarker615"/> decomposed as follows:</p>
			<ul>
				<li><strong class="bold">Observed</strong> or <strong class="bold">level</strong>: The<a id="_idIndexMarker616"/> average values of the series</li>
				<li><strong class="bold">Trend</strong>: Increasing, decreasing pattern (sometimes, there is no trend)</li>
				<li><strong class="bold">Seasonality</strong>: Regular peaks at specific periods of time (sometimes, there is no seasonality)</li>
				<li><strong class="bold">Noise</strong>: Something we cannot explain</li>
			</ul>
			<p>Sometimes, we can also find isolated peaks in the series that cannot be captured in a forecasting model. In such cases, we might want to consider those peaks as outliers. The following is a decomposition of the time series shown in the preceding graph:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B16735_07_014.jpg" alt="Figure 7.14 – Time series decomposition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14 – Time series decomposition</p>
			<p>It's also worth<a id="_idIndexMarker617"/> highlighting that we can use <strong class="bold">additive</strong> or <strong class="bold">multiplicative</strong> approaches to decompose time series. Additive models suggest that your time series <em class="italic">adds</em> each component to explain the target variable; that is, <strong class="bold">y(t) = level + trend + seasonality + noise</strong>. </p>
			<p>Multiplicative models, on the other hand, suggest that your time series <em class="italic">multiplies</em> each component to explain the target variable; that is, <strong class="bold">y(t) = level * trend * seasonality * noise</strong>.</p>
			<p>In the next section, we will have a closer look at time series components.</p>
			<h3>Checking the stationarity of time series</h3>
			<p>Decomposing time series<a id="_idIndexMarker618"/> and understanding how their components <a id="_idIndexMarker619"/>interact with additive and multiplicative models is a great achievement! However, the more we learn, the more we question ourselves. Maybe you have realized that time series without trend and seasonality are easier to predict than the ones with all those components!</p>
			<p>That is naturally right. If you don't have to understand trend and seasonality, and if you don't have control over the noise, all you have to do is explore the observed values and find their regression relationship.</p>
			<p>We refer to time series with constant<a id="_idIndexMarker620"/> mean and variance across a period of time as <strong class="bold">stationary</strong>. In general, time series <em class="italic">with</em> trend and seasonality are <em class="italic">not</em> stationary. It is possible to apply data transformations to the series to transform it into a stationary time series so that the modeling task tends to be easier. This type of transformation is known <a id="_idIndexMarker621"/>as <strong class="bold">differentiation</strong>.</p>
			<p>While you are exploring a time series, you can check stationarity by applying hypothesis tests, such <a id="_idIndexMarker622"/>as <strong class="bold">Dickey-Fuller</strong>, <strong class="bold">KPSS</strong>, and <strong class="bold">Philips-Perron</strong>, just to mention a few. If you find it non-stationary, then<a id="_idIndexMarker623"/> you can apply differentiation to make it a stationary <a id="_idIndexMarker624"/>time series. Some algorithms already have that capability embedded.</p>
			<h3>Exploring, exploring, and exploring</h3>
			<p>At this point, I am sure I don't have to remind you that exploration tasks happen all the time in data science. Nothing is different here. While you are building time series models, you might want to have a look at the data and check if it is suitable for this type of modeling.</p>
			<p><strong class="bold">Autocorrelation plots</strong> are <a id="_idIndexMarker625"/>one of the tools that you can use for time series analysis. Autocorrelation plots allow you to check the correlations between lags in the time series. The following graph shows an example of this type of visualization:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B16735_07_015.jpg" alt="Figure 7.15 – Autocorrelation plot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.15 – Autocorrelation plot</p>
			<p>Remember, if you're playing with univariate time series, your time series just has one variable, so finding autocorrelation across the lags of your unique variable is crucial to understanding whether you can build a good model or not.</p>
			<p>And yes, it turns out that, sometimes, it might happen that you don't have a time series in front of you. Furthermore, no matter your efforts, you will not be able to model this data as a time series. This type of data is often known <a id="_idIndexMarker626"/>as <strong class="bold">white noise</strong>.</p>
			<p>Another type of series that we cannot predict is<a id="_idIndexMarker627"/> known as <strong class="bold">random walk</strong>. Random walks are random by nature, but they have a dependency on the previous time step. For example, the next point of a random walk could be a random number between 0 and 1, and also the last point of the series.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Be careful if you come across those terms in the exam and remember to relate them to randomness in time series.</p>
			<p>With that, we have covered the main theory about time series modeling. You should also be aware that the most popular algorithms out there for working with time series are known<a id="_idIndexMarker628"/> as <strong class="bold">Auto-Regressive Integrated Moving Average</strong> (<strong class="bold">ARIMA</strong>) and <strong class="bold">Exponential Smoothing</strong> (<strong class="bold">ETS</strong>). We will<a id="_idIndexMarker629"/> not look at the details of these two models. Instead, we will see what AWS can offer us in terms of time series modeling.</p>
			<h3>Understanding DeepAR</h3>
			<p>The <strong class="bold">DeepAR</strong> forecasting<a id="_idIndexMarker630"/> algorithm is a built-in SageMaker algorithm that's used to forecast a one-dimensional<a id="_idIndexMarker631"/> time series using a <strong class="bold">Recurrent Neural Network</strong> (<strong class="bold">RNN</strong>). </p>
			<p>Traditional time series algorithms, such as ARIMA and ETS, are designed to fit one model per time series. For example, if you want to forecast sales per region, you might have to create one model per region, since each region might have its own sales behaviors. DeepAR, on the other hand, allows you to operate more than one time series in a single model, which seems to be a huge advantage for more complex use cases.</p>
			<p>The input data for <a id="_idIndexMarker632"/>DeepAR, as expected, is one <em class="italic">or more</em> time series. Each of these time series can be associated with the following:</p>
			<ul>
				<li>A vector of static (time-independent) categorical features, controlled by the <strong class="source-inline">cat</strong> field</li>
				<li>a vector of dynamic (time-dependent) time series, controlled by <strong class="source-inline">dynamic_feat</strong><p class="callout-heading">Important note</p><p class="callout">Note that the ability to train and make predictions on top of multiple time series is strictly related to the vector of static categorical features. While defining the time series that DeepAR will train on, you can set categorical variables to specify which group each time series belongs to. </p></li>
			</ul>
			<p>Two of the main hyperparameters of DeepAR are <strong class="source-inline">context_length</strong>, which is used to control how far in the past the model can see during the training process, and <strong class="source-inline">prediction_length</strong>, which is used to control how far in the future the model will output predictions.</p>
			<p>DeepAR can also handle missing values, which, in this case, refers to existing gaps in the time series. A very interesting functionality of DeepAR is its ability to create derived features from time series. These derived features, which are created from basic time frequencies, help the algorithm learn time-dependent patterns. The following table shows all the derived features created by DeepAR, according to each type of time series that it is trained on:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B16735_07_016.jpg" alt="Figure 7.16 – DeepAR derived features per frequency of time series&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.16 – DeepAR derived features per frequency of time series</p>
			<p>We have now completed this section about forecasting models. Next, we will have a look at the last algorithm regarding supervised learning; that is, the <strong class="bold">Object2Vec</strong> algorithm.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor145"/>Object2Vec</h2>
			<p>Object2Vec<a id="_idIndexMarker633"/> is a built-in <a id="_idIndexMarker634"/>SageMaker algorithm that generalizes the well-known <strong class="bold">word2vec</strong> algorithm. Object2Vec is used to create <strong class="bold">embedding spaces</strong> for high multidimensional objects. These embedding spaces are, per the definition, compressed representatiosn of the original object and can be used for multiple purposes, such as feature engineering or object comparison:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B16735_07_017.jpg" alt="Figure 7.17 – A visual example of an embedding space&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.17 – A visual example of an embedding space</p>
			<p>The preceding diagram illustrates what we mean by an embedding space. The first and the last layers of the neural network model just map the input data with itself (represented by the same vector size).</p>
			<p>As we move on to the internal layers of the model, the data is compressed more and more until it hits the layer in the middle of this architecture, known as the embedding layer. On that particular layer, we have a smaller vector, which aims to be an accurate and compressed representation of the high-dimensional original vector from the first layer.</p>
			<p>With this, we just completed our first section about machine learning algorithms in AWS. Coming up next, we will have a look at some unsupervised algorithms.</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor146"/>Unsupervised learning</h1>
			<p>AWS provides several unsupervised learning<a id="_idIndexMarker635"/> algorithms for the following tasks:</p>
			<ul>
				<li>Clustering:</li>
				<li>K-means algorithm</li>
				<li>Dimension reduction:</li>
				<li><strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>)</li>
				<li>Pattern recognition:</li>
				<li>IP Insights</li>
				<li>Anomaly detection:</li>
				<li><strong class="bold">Random Cut Forest Algorithm</strong> (<strong class="bold">RCF</strong>)</li>
			</ul>
			<p>Let's start by talking about clustering and how the most popular clustering algorithm works: K-means.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor147"/>Clustering</h2>
			<p>Clustering <a id="_idIndexMarker636"/>algorithms are<a id="_idIndexMarker637"/> very popular in data science. Basically, they aim to identify groups in a given dataset. Technically, we call these findings or groups <strong class="bold">clusters</strong>. Clustering algorithms belong to the field of non-supervised learning, which means that they don't need a label or response variable to be trained. </p>
			<p>This is just fantastic because labeled data used to be scarce. However, it comes with some limitations. The main one is that clustering algorithms provide clusters for you, but not the meaning of each cluster. Thus, someone, as a subject matter expert, has to analyze the properties of each cluster to define their meanings.</p>
			<p>There are many <a id="_idIndexMarker638"/>types of clustering approaches, such as hierarchical clustering and partitional clustering. Inside<a id="_idIndexMarker639"/> each approach, we will find several algorithms. However, <strong class="bold">K-Means</strong> is probably the most popular clustering algorithm and you are likely to<a id="_idIndexMarker640"/> come across it in your exam, so let's take a closer look at it.</p>
			<p>When we are playing with K-means, somehow, we have to specify the number of clusters that we want to create. Then, we have to allocate the data points across each cluster so that each data point will belong to a single cluster. This is exactly what we expect as a result at the end of the clustering process!</p>
			<p>You, as a user, have to specify the number of clusters you want to create and pass this number to K-means. Then, the algorithm will randomly initiate the central point of each cluster, which is also known <a id="_idIndexMarker641"/>as <strong class="bold">centroid</strong> initialization.</p>
			<p>Once we have the centroids of each cluster, all we need to do is assign a cluster to each data point. To do that, we have to use a proximity or distance metric! Let's adopt the term distance metric.</p>
			<p>The <strong class="bold">distance metric</strong> is<a id="_idIndexMarker642"/> responsible for calculating the distance between data points and centroids. The data point will belong to the closer cluster centroid, according to the distance metric!</p>
			<p>The most well-known and used distance metric is<a id="_idIndexMarker643"/> called <strong class="bold">Euclidean distance</strong> and the math behind it is really simple: imagine that the points of your dataset are composed of two dimensions, X and Y. So, we could consider points a and b as follows:</p>
			<ul>
				<li>a (X=1, Y=1) </li>
				<li>b (X=2, Y=5)</li>
			</ul>
			<p>The Euclidean distance between points a and b is given by the following formula, where X1 and Y1 refer to the values of point a and X2 and Y2 refer to the values of point b: </p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/image17.jpg" alt=""/>
				</div>
			</div>
			<p>The same function can be generalized by the following equation:<img src="image/image18.png" alt=""/></p>
			<p>Once we have completed this process and assigned a cluster with each data point, we have to recalculate the <a id="_idIndexMarker644"/>cluster centroids. This <a id="_idIndexMarker645"/>process can be done by different <a id="_idIndexMarker646"/>methods, such as <strong class="bold">single link</strong>, <strong class="bold">average link</strong>, and <strong class="bold">complete link</strong>.</p>
			<p>Due to this centroid refreshment, we will have to keep checking the closest cluster for each data point and keep refreshing the centroids. We have to reexecute steps 1 and 2 <strong class="bold">iteratively</strong>, until the cluster centroids converge or the maximum number of allowed iterations is reached.</p>
			<p>Alright; let's recap the components that compose the K-means method:</p>
			<ul>
				<li>Centroid initialization, cluster assignment, centroid refreshment, and then redo the last two steps until it converges.</li>
				<li>The algorithm itself:</li>
				<li>A distance metric to assign data points to each cluster:</li>
				<li>We have selected Euclidian distance here.</li>
				<li>And a linkage method to recalculate the cluster centroids:</li>
				<li>For the sake of our demonstration, we'll select the average linkage.</li>
			</ul>
			<p>With these definitions, we are ready to walk through our real example, step by step. As with our regression models, some support material is also available for your reference.</p>
			<h3>Computing K-means step by step</h3>
			<p>In this example, we will simulate<a id="_idIndexMarker647"/> K-means in a very small dataset, with only two columns (x and y) and six data points (A, B, C, D, E, F), as defined in the following table:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B16735_07_018.jpg" alt="Figure 7.18 – Input data for K-means&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.18 – Input data for K-means</p>
			<p>In the preceding table, we created three clusters with the following centroids: (1,1), (2,2), (5,5). The number of clusters (3) was defined <em class="italic">a priori</em> and the centroid for each cluster was randomly defined. The following graph shows the stage of the algorithm we are at right now:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B16735_07_019.jpg" alt="Figure 7.19 – Plotting the K-means results before completing the first iteration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.19 – Plotting the K-means results before completing the first iteration</p>
			<p>Here, you can't<a id="_idIndexMarker648"/> see points A, B, and C since they overlap with cluster centroids, but don't worry – they will appear soon. What we have to do now is compute the distance of each data point to each cluster centroid. Then, we need to choose the cluster that is the closest to each point:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B16735_07_020.jpg" alt="Figure 7.20 – Processing iteration 1&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.20 – Processing iteration 1</p>
			<p>In the preceding table, we have the following elements:</p>
			<ul>
				<li>Each row represents a data point.</li>
				<li>The first six columns represent the centroid axis (x and y) of each cluster.</li>
				<li>The next three columns represent the distance of each data point to each cluster centroid.</li>
				<li>The last column represents the clusters that are the closest to each data point.</li>
			</ul>
			<p>Looking at data point A (first row), we can see that it was assigned to cluster 1 because the distance from data point A to cluster 1 is 0 (remember when I told you they were overlapping?). The same calculation happens to all other data points to define a cluster for each data point.</p>
			<p>Before we move <a id="_idIndexMarker649"/>on, you might want to see how we computed those distances between the clusters and the data points. As we stated previously, we have used the Euclidian distance, so let's see that in action. For demonstration purposes, let's check out how we came up with the distance between data point A and cluster 3 (the first row in <em class="italic">Figure 7.20</em>, column <strong class="source-inline">distance-c3</strong>, value 5,7).</p>
			<p>First of all, we applied the same equation from the following formula:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/image171.jpg" alt=""/>
				</div>
			</div>
			<p>Here, we have the following:</p>
			<ul>
				<li>X1 = X of data point A = 1</li>
				<li>Y1 = Y of data point A = 1</li>
				<li>X2 = X of cluster 3 = 5</li>
				<li>Y2 = Y of cluster 3 = 5</li>
			</ul>
			<p>Applying the formula step by step, we will come up with the following results: </p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/image22.jpg" alt="Figure 7.21 – Computing the Euclidian distance step by step&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.21 – Computing the Euclidian distance step by step</p>
			<p>That is just fantastic, isn't it? We have almost completed the first iteration of K-means. In the very last step of iteration 1, we have to refresh the cluster centroids. Remember: initially, we randomly defined those centroids, but now, we have just assigned some data points to each cluster, which means we should be able to identify where the central point of the cluster is.</p>
			<p>In this example, we<a id="_idIndexMarker650"/> have defined that we are going to use the <strong class="bold">average linkage</strong> method to <a id="_idIndexMarker651"/>refresh the cluster centroids. This is a very simple step, and the results are present in the following table:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B16735_07_022.jpg" alt="Figure 7.22 – K-means results after iteration 1&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.22 – K-means results after iteration 1</p>
			<p>The preceding table shows the same data points that we are dealing with (by the way, they will never change), and the centroids of clusters 1, 2, and 3. Those centroids are quite different from what they were initially, as shown in <em class="italic">Figure 7.18</em>. This is because they were refreshed using average linkage! The method got the average value of all the x and y values of the data points of each cluster. For example, let's see how we came up with (1.5, 3.5) as centroids of cluster 2.</p>
			<p>If you look at <em class="italic">Figure 7.20</em>, you will see that cluster 2 only has two data points assigned to it: B and E. These are the second and fifth rows in that image. If we take the average values of the x-axis of each point, then we'll have <strong class="bold">(2 + 1) / 2 = 1.5</strong> and <strong class="bold">(2 + 5) / 2 = 3.5</strong>.</p>
			<p>With that, we are done with iteration 1 of K-means and we can view the results:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B16735_07_023.jpg" alt="Figure 7.23 – Plotting the K-means results after the first iteration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.23 – Plotting the K-means results after the first iteration</p>
			<p>Now, we can see<a id="_idIndexMarker652"/> almost all the data points, except for data point A because it is still overlapping with the centroid of cluster 1. Moving on, we have to redo the following steps:</p>
			<ul>
				<li>Recalculate the distance between each data point and each cluster centroid and reassign clusters, if needed</li>
				<li>Recalculate the cluster centroids</li>
			</ul>
			<p>We do those two tasks many times until the cluster centroids converge and they don't change anymore <em class="italic">or</em> we reach the maximum number of allowed iterations, which can be set as a hyperparameter of K-means. For demonstration purposes, after four iterations, our clusters will look as follows:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B16735_07_024.jpg" alt="Figure 7.24 – Plotting the K-means results after the fourth iteration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.24 – Plotting the K-means results after the fourth iteration</p>
			<p>On the fourth iteration, our cluster centroids look pretty consistent, and we can clearly see that we can group our six data points according to their proximity. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In this example, we have only set two dimensions for each data point (dimension x and y). In real use cases, we can see far more dimensions, and that's why clustering algorithms play a very important role in identifying groups in the data in a more automated fashion.</p>
			<p>I hope you have <a id="_idIndexMarker653"/>enjoyed how to compute K-means from scratch! I am sure this knowledge will be beneficial for your exam and for your career as a data scientist. By the way, I have told you many times that data scientists must be skeptical and curious, so you might be wondering why we defined three clusters in this example and not two or four. You may also be wondering how we measure the quality of the clusters.</p>
			<p>You didn't think I wouldn't explain this to you, did you? In the next section, we will clarify those points together.</p>
			<h3>Defining the number of clusters and measuring cluster quality</h3>
			<p>Although K-means is a great algorithm for finding patterns in your data, it will not provide the meaning of each cluster, nor the number of clusters you have to create to maximize cluster quality.</p>
			<p>In clustering, cluster quality<a id="_idIndexMarker654"/> means that we want to create groups with a high homogeneity among the elements of the same cluster, and a high heterogeneity among the elements of different clusters. In other words, the elements of the same clusters should be close/similar, whereas the elements of different clusters should be well separated.</p>
			<p>One way to compute the cluster's homogeneity <a id="_idIndexMarker655"/>is by using a metric known as <strong class="bold">Sum of Square Errors</strong>, or <strong class="bold">SSE</strong> for short. This metric will compute the sum of squared differences between each data point and its cluster centroid. For example, when all the data points are located at the same point where the cluster centroid is, then SSE will be 0. In other words, we want to minimize SSE. The following equation formally defines SSE:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/image26.jpg" alt=""/>
				</div>
			</div>
			<p>Now that we know how to check cluster quality, it is easier to understand how to define the number of appropriated clusters for a given dataset. All we have to do is find several clusters that minimize SSE. A very popular method that works <a id="_idIndexMarker656"/>around that logic is known as the <strong class="bold">Elbow method</strong>.</p>
			<p>The Elbow method proposes executing the clustering algorithm many times. In each execution, we will test a different number of clusters, <em class="italic">k</em>. After each execution, we compute the SSE related to that <em class="italic">k</em> number of cluster clusters. Finally, we can plot these results and select the number of <em class="italic">k</em> where the SSE stops to drastically decrease.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Adding more clusters will naturally decrease the SSE. In the Elbow method, we want to find the point where that change becomes smoother.</p>
			<p>In the previous example, we decided to create three clusters. The following graph shows the Elbow analysis that supports this decision:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B16735_07_025.jpg" alt="Figure 7.25 – The Elbow method&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.25 – The Elbow method</p>
			<p>We can conclude that adding more than three or four clusters will add unnecessary complexity to the clustering process.</p>
			<p>Of course, you<a id="_idIndexMarker657"/> should always consider the business background while defining the number of clusters. For example, if you are creating a customer segmentation model and your company has prepared the commercial team and business processes to support four segments of customers, considering the preceding graph, there is no harm in setting four clusters instead of three.</p>
			<p>Finally, you should know that AWS has implemented K-means as part of their list of built-in algorithms. In other words, you don't have to use external libraries or bring your own algorithm to play with K-means on AWS.</p>
			<h3>Conclusion</h3>
			<p>That was a really good accomplishment: you just mastered the basics of clustering algorithms and you should now be able to drive your own projects and research about this topic! For the exam, remember that clustering belongs to the unsupervised field of machine learning, so there is no need to have labeled data.</p>
			<p>Also, make sure that you know how the most popular algorithm of this field works; that is, K-means. Although clustering algorithms do not provide the meaning of each group, they are very powerful for finding patterns in the data, either to model a particular problem or just to explore the data.</p>
			<p>Coming up next, we'll keep studying unsupervised algorithms and see how AWS has built one of the most powerful algorithms out there for anomaly detectio<a id="_idTextAnchor148"/>n, known as <strong class="bold">Random Cut Forest </strong>(<strong class="bold">RCF</strong>).</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor149"/>Anomaly detection</h2>
			<p>Finding <a id="_idIndexMarker658"/>anomalies in data is very<a id="_idIndexMarker659"/> common in modeling and data exploratory analysis. Sometimes, you might want to find anomalies in the data just to remove them before fitting a regression model, while other times, you might want to create a model that identifies anomalies as an end goal, for example, in fraud detection systems.</p>
			<p>Again, we can use many different methods to find anomalies in the data. With some creativity, the possibilities are endless. However, there is a particular algorithm that works around this problem that you should definitely be aware of for your exam: <strong class="bold">Random Cut Forest</strong> (<strong class="bold">RCF</strong>).</p>
			<p>RCF is an <a id="_idIndexMarker660"/>unsupervised decision tree-based algorithm that creates multiple decision trees (forests) using random subsamples of the training data. Technically, it randomizes the data and then creates samples according to the number of trees. Finally, these samples are distributed across each tree.</p>
			<p>These sets of trees are used to assign an anomaly score tp the data points. This anomaly score is defined as the expected change in the complexity of the tree as a result of adding that point to the tree.</p>
			<p>The most important hyperparameters of RCF are <strong class="source-inline">num_trees</strong> and <strong class="source-inline">num_samples_per_tree</strong>, which are the number of trees in the forest and the number of samples per tree, respectively.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor150"/>Dimensionality reduction</h2>
			<p>Another <a id="_idIndexMarker661"/>unsupervised <a id="_idIndexMarker662"/>algorithm that's implemented by AWS in their list of built-in algorithms is known <a id="_idIndexMarker663"/>as <strong class="bold">Principal Component Analysis</strong>, or <strong class="bold">PCA</strong> for short. PCA is a technique that's used to reduce the number of variables/dimensions in a given dataset.</p>
			<p>The main idea behind <a id="_idIndexMarker664"/>PCA is plotting the data points to another set of coordinates, known as <strong class="bold">principal components</strong> (<strong class="bold">PC</strong>), which aims to explain the most variance in the data. By definition, the first component will capture more variance than the second component, then the second component will capture more variance than the third one, and so on.</p>
			<p>You can set up as many <a id="_idIndexMarker665"/>principal components as you need as long as it <a id="_idIndexMarker666"/>does not surpass the number of variables in your dataset. The following graph shows how these principal components are drawn:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B16735_07_026.jpg" alt="Figure 7.26 – Finding principal components in PCA&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.26 – Finding principal components in PCA</p>
			<p>As we mentioned previously, the first principal component will be drawn in such a way that it will capture most of the variance in the data. That's why it passes close to the majority of the data points in the preceding graph.</p>
			<p>Then, the second principal component will be perpendicular to the first one, so that it will be the second component that explains the variance in the data. If you want to create more components (consequentially, capturing more variance), you just have to follow the same rule of adding perpendicular<a id="_idIndexMarker667"/> components. <strong class="bold">Eigenvectors</strong> and <strong class="bold">eigenvalues</strong> are the <a id="_idIndexMarker668"/>linear algebra concepts associated with PCA that compute the principal components.</p>
			<p>So, what's the story with dimension reduction here? In case it is not clear yet, these principal components can be used to replace your original variables. For example, let's say you have 10 variables in your dataset and you want to reduce this dataset to three variables that best represent the others. A potential solution for that <a id="_idIndexMarker669"/>would be applying PCA and extracting the first three principal components!</p>
			<p>Do these <a id="_idIndexMarker670"/>three components explain 100% of your dataset? Probably not, but ideally, they will explain most of the variance. Adding more principal components will explain more variance, but at the cost of you adding extra dimensions.</p>
			<h3>Using AWS's built-in algorithm for PCA</h3>
			<p>In AWS, PCA works in two different modes:</p>
			<ul>
				<li><strong class="bold">Regular</strong>: For datasets<a id="_idIndexMarker671"/> with a moderate number of observations and features</li>
				<li><strong class="bold">Randomized</strong>: For datasets with a large number of observations and features</li>
			</ul>
			<p>The difference is that, in randomized mode, it is used an approximation algorithm.</p>
			<p>Of course, the main hyperparameter of PCA is the number of components that you want to extract, known as <strong class="source-inline">num_components</strong>.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor151"/>IP Insights</h2>
			<p>IP Insights is an <a id="_idIndexMarker672"/>unsupervised algorithm that's used for pattern recognition. Essentially, it learns the <a id="_idIndexMarker673"/>usage pattern of IPv4 addresses.</p>
			<p>The modus operandi of this algorithm is very intuitive: it is trained on top of pairs of events in the format of entity and IPv4 address so that it can understand the pattern of each entity that it was trained on.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For instance, we can understand "entity" as user IDs or account numbers.</p>
			<p>Then, to make predictions, it receives a pair of events with the same data structure (entity, IPv4 address) and returns an anomaly score for that particular IP address regarding the input entity.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This anomaly score that's returned by IP Insight infers how anomalous the pattern of the event is.</p>
			<p>We might <a id="_idIndexMarker674"/>come across many applications with IP Insights. For example, you can create an IP Insights model that was trained on top of your application login events (this is your entity). You should be able to expose this model through an API endpoint to make predictions in real time.</p>
			<p>Then, during the <a id="_idIndexMarker675"/>authentication process of your application, you could call your endpoint and pass the IP address that is trying to log in. If you got a high score (meaning this pattern of logging in looks anomalous), you can request extra information before authorizing access (even if the password was right).</p>
			<p>This is just one of the many applications of IP Insights you could think about. Next, we will discuss textual analysis.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor152"/>Textual analysis</h1>
			<p>Modern <a id="_idIndexMarker676"/>applications<a id="_idIndexMarker677"/> use <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) for<a id="_idIndexMarker678"/> several <a id="_idIndexMarker679"/>purposes, such as <strong class="bold">text translation</strong>, <strong class="bold">document classifications</strong>, <strong class="bold">web search</strong>, <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>), and<a id="_idIndexMarker680"/> many <a id="_idIndexMarker681"/>others.</p>
			<p>AWS offers a suite of algorithms for most NLP use cases. In the next few subsections, we will have a look at these built-in algorithms for textual analysis.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor153"/>Blazing Text algorithm</h2>
			<p>Blazing<a id="_idIndexMarker682"/> Text does <a id="_idIndexMarker683"/>two different types of tasks: text classification, which is a supervised learning approach that <a id="_idIndexMarker684"/>extends the <strong class="bold">fastText</strong> text classifier, and <strong class="bold">word2vec</strong>, which is an<a id="_idIndexMarker685"/> unsupervised learning algorithm.</p>
			<p>The Blazing Text's implementations of these two algorithms are optimized to run on large datasets. For example, you can train a model on top of billions of words in a few minutes.</p>
			<p>This scalability aspect of Blazing Text is possible due to the following:</p>
			<ul>
				<li>Its ability to use multi-core CPUs and a single GPU to accelerate text classification</li>
				<li>Its ability to use multi-core CPUs or GPUs, with custom CUDA kernels for GPU acceleration, when playing with the word2vec algorithm</li>
			</ul>
			<p>The word2vec option <a id="_idIndexMarker686"/>supports a <strong class="bold">batch_skipgram</strong> mode, which allows Blazing Text to do distributed training across multiple CPUs.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The distributed training that's performed by Blazing Text uses a mini-batching approach to <a id="_idIndexMarker687"/>convert <strong class="bold">level-1 BLAS</strong> operations into <strong class="bold">level-3 BLAS</strong> operations. If you see<a id="_idIndexMarker688"/> these terms during your exam, you should know that they are related to Blazing Text in terms of word2vec.</p>
			<p>Still in word2vec<a id="_idIndexMarker689"/> mode, Blazing Text supports<a id="_idIndexMarker690"/> both the <strong class="bold">skip-gram</strong> and <strong class="bold">continuous bag of words</strong> (<strong class="bold">CBOW</strong>) architectures.</p>
			<p>Last but not least, note the following configurations of Blazing Text, since they are likely to be present in your exam:</p>
			<ul>
				<li>In word2vec mode, only the train channel is available.</li>
				<li>Blazing Text expects a single text file with space-separated tokens. Each line of the file must contain a single sentence. This means you usually have to pre-process your corpus of data before using Blazing Text.</li>
			</ul>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor154"/>Sequence-to-sequence algorithm</h2>
			<p>This is a supervised algorithm<a id="_idIndexMarker691"/> that transforms an input sequence<a id="_idIndexMarker692"/> into an output sequence. This sequence can be a text sentence or even an audio recording.</p>
			<p>The most common use cases for sequence-to-sequence are machine translation, text summarization, and speech-to-text. Anything that you think is a sequence-to-sequence problem can be approached by this algorithm.</p>
			<p>Technically, AWS SageMaker's Seq2Seq <a id="_idIndexMarker693"/>uses two types of neural networks to create models: a <strong class="bold">Recurrent Neural Network</strong> (<strong class="bold">RNN</strong>) and a <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>) with <a id="_idIndexMarker694"/>an attention mechanism.</p>
			<p><strong class="bold">Latent Dirichlet Allocation</strong>, or <strong class="bold">LDA</strong> for short, is used for topic modeling. Topic modeling is a textual analysis<a id="_idIndexMarker695"/> technique where you can extract a set of topics from a corpus of text data. LDA learns these topics based on the probability distribution of the words in the corpus of text.</p>
			<p>Since this is an unsupervised algorithm, there is no need to set a target variable. Also, the number of topics must be specified up-front and you will have to analyze each topic to find their domain meaning.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor155"/>Neural Topic Model (NTM) algorithm</h2>
			<p>Just like the<a id="_idIndexMarker696"/> LDA algorithm, the <strong class="bold">Neural Topic Model</strong> (<strong class="bold">NTM</strong>) also aims to<a id="_idIndexMarker697"/> extract topics from a corpus of data. However, the difference between LDA and NTM is their learning logic. While LDA learns from probability distributions of the words in the documents, NTM is built on top of neural networks.</p>
			<p>The NTM network architecture has a bottleneck layer, which creates an embedding representation of the documents. This bottleneck layer contains all the necessary information to predict document composition, and its coefficients can be considered topics.</p>
			<p>With that, we have completed this section on textual analysis. In the next section, we will learn about image processing algorithms.</p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor156"/>Image processing</h1>
			<p>Image processing<a id="_idIndexMarker698"/> is a very popular topic in machine learning. The idea is pretty self-explanatory: creating models that can analyze images and make inferences on top of them. By inference, you can understand this as detecting objects in an image, classifying images, and so on.</p>
			<p>AWS offers a set of built-in algorithms we can use to train image processing models. In the next few sections, we will have a look at those algorithms.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor157"/>Image classification algorithm</h2>
			<p>As the name suggests, the image classification algorithm<a id="_idIndexMarker699"/> is used to classify images using supervised learning. In other words, it needs a label within each image. It supports multi-label classification.</p>
			<p>The way it operates is simple: during training, it receives an image and its associated labels. During inference, it receives an image and returns all the predicted labels. The image classification algorithm uses a CNN<a id="_idIndexMarker700"/> (<strong class="bold">ResNet</strong>) for training. It can either train the model from scratch or take advantage of transfer learning to pre-load the first few layers of the neural network.</p>
			<p>According to AWS's documentation, the <strong class="source-inline">.jpg</strong> and <strong class="source-inline">.png</strong> file formats are supported, but the recommended<a id="_idIndexMarker701"/> format is <strong class="bold">MXNet RecordIO</strong>.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor158"/>Semantic segmentation algorithm</h2>
			<p>The semantic segmentation algorithm<a id="_idIndexMarker702"/> provides a pixel-level capability for creating computer vision applications. It tags each pixel of the image with a class, which is an important feature for complex applications such as self-driving and medical image diagnostics.</p>
			<p>In terms of its implementation, the semantic segmentation<a id="_idIndexMarker703"/> algorithm uses the <strong class="bold">MXNet Gluon framework</strong> and the <strong class="bold">Gluon CV toolkit</strong>. You<a id="_idIndexMarker704"/> can choose any of the following algorithms to train a model:</p>
			<ul>
				<li><strong class="bold">Fully convolutional network</strong> (<strong class="bold">FCN</strong>)</li>
				<li><strong class="bold">Pyramid scene parsing</strong> (<strong class="bold">PSP</strong>) </li>
				<li>DeepLabV3</li>
			</ul>
			<p>All these options work as <a id="_idIndexMarker705"/>an <strong class="bold">encoder-decoder</strong> neural network architecture. The <a id="_idIndexMarker706"/>output of the network is known as a <strong class="bold">segmentation mask</strong>.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor159"/>Object detection algorithm</h2>
			<p>Just<a id="_idIndexMarker707"/> like the image classification algorithm, the main goal of the object detection algorithm is also self-explanatory: it detects and classifies objects in images. It uses a supervised approach to train a deep neural network.</p>
			<p>During the inference process, this algorithm returns the identified objects and a score of confidence regarding the prediction. The object detection algorithm uses a <strong class="bold">Single Shot MultiBox Detector</strong> (<strong class="bold">SSD</strong>) and<a id="_idIndexMarker708"/> supports <a id="_idIndexMarker709"/>two types of network <a id="_idIndexMarker710"/>architecture: <strong class="bold">VGG</strong> and <strong class="bold">ResNet</strong>.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor160"/>Summary</h1>
			<p>That was such a journey! Let's take a moment to highlight what we have just learned. We broke this chapter into four main sections: supervised learning, unsupervised learning, textual analysis, and image processing. Everything that we have learned fits those subfields of machine learning.</p>
			<p>The list of supervised learning algorithms that we have studied includes the following:</p>
			<ul>
				<li>Linear learner algorithm</li>
				<li>Factorization machines algorithm</li>
				<li>XGBoost algorithm</li>
				<li>K-Nearest Neighbors algorithm</li>
				<li>Object2Vec algorithm</li>
				<li>DeepAR forecasting algorithm</li>
			</ul>
			<p>Remember that you can use linear learner, factorization machines, XGBoost, and KNN for multiple purposes, including to solve regression and classification problems. Linear learner is probably the simplest algorithm out of these four; factorization machines extend linear learner and are good for sparse datasets, XGBoost uses an ensemble method based on decision trees, and KNN is an index-based algorithm.</p>
			<p>The other two algorithms, Object2Vec and DeepAR, are used for specific purposes. Object2Vec is used to create vector representations of the data, while DeepAR is used to create forecast models.</p>
			<p>The list of unsupervised learning algorithms that we have studied includes the following:</p>
			<ul>
				<li>K-means algorithm</li>
				<li><strong class="bold">Principal</strong> <strong class="bold">Component</strong> <strong class="bold">Analysis</strong> (<strong class="bold">PCA</strong>)</li>
				<li>IP Insights</li>
				<li><strong class="bold">Random</strong> <strong class="bold">Cut</strong> <strong class="bold">Forest</strong> (<strong class="bold">RCF</strong>) algorithm</li>
			</ul>
			<p>K-means is a very popular algorithm that's used for clustering. PCA is used for dimensionality reduction, IP Insights is used for pattern recognition, and RCF is used for anomaly detection.</p>
			<p>We then looked at regression models and K-means in more detail. We did this because, as a data scientist, we think you should at least master these two very popular algorithms so that you can go deeper into other algorithms by yourself. </p>
			<p>Then, we moved on to the second half of this chapter, where we talked about textual analysis and the following algorithms:</p>
			<ul>
				<li>Blazing Text algorithm</li>
				<li>Sequence-to-sequence algorithm</li>
				<li><strong class="bold">Latent</strong> <strong class="bold">Dirichlet</strong> <strong class="bold">Allocation</strong> (<strong class="bold">LDA</strong>) algorithm</li>
				<li><strong class="bold">Neural</strong> <strong class="bold">Topic</strong> <strong class="bold">Model</strong> (<strong class="bold">NTM</strong>) algorithm</li>
			</ul>
			<p>Finally, we talked about image processing and looked at the following:</p>
			<ul>
				<li>Image classification algorithm</li>
				<li>Semantic segmentation algorithm</li>
				<li>Object detection algorithm</li>
			</ul>
			<p>Since this is a very important chapter regarding the AWS machine learning specialty exam, we encourage you to jump onto the AWS website and search for machine learning algorithms. There, you will find the most recent information about the algorithms that we have just covered.</p>
			<p>That brings us to the end of this quick refresher and the end of this chapter. In the next chapter, we will have a look at the existing mechanisms provided by AWS that we can use to optimize these algorithms.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor161"/>Questions</h1>
			<ol>
				<li value="1">You are working as a lead data scientist for a retail company. Your team is building a regression model and using the linear learner built-in algorithm to predict the optimal price of a particular product. The model is clearly overfitting to the training data and you suspect that this is due to the excessive number of variables being used. Which of the following approaches would best suit a solution that addresses your suspicion?<p>a) Implementing a cross-validation process to reduce overfitting during the training process.</p><p>b) Applying L1 regularization and changing the <strong class="source-inline">wd</strong> hyperparameter of the linear learner algorithm.</p><p>c) Applying L2 regularization and changing the <strong class="source-inline">wd</strong> hyperparameter of the linear learner algorithm.</p><p>d) Applying L1 and L2 regularization.</p><p class="callout-heading">Answers</p><p class="callout">C, This question prompts about to the problem of overfitting due an excessive number of features being used. L2 regularization, which is available in linear learner through the <strong class="source-inline">wd</strong> hyperparameter, will work as a feature selector. Some less important features will be penalized by receiving very low weights, which will, in practical terms, eliminate the variable.</p></li>
				<li>RecordIO-protobuf is an optimized data format that's used to train AWS, built-in algorithms, where SageMaker converts each observation in the dataset into a binary representation as a set of 4-byte floats. RecordIO-protobuf can operate in two modes: pipe and file mode. What is the difference between them?<p>a) Pipe mode accepts encryption at rest, while file mode does not.</p><p>b) In pipe mode, the data will be streamed directly from S3, which helps optimize storage. In file mode, the data is copied from S3 to the training instance's store volume.</p><p>c) In pipe mode, the data is copied from S3 to the training instance store volume. In file mode, the data will be streamed directly from S3, which helps optimize storage.</p><p>d) In pipe mode, the data will be streamed directly from S3, which helps optimize storage. In file mode, the data is copied from S3 to another temporary S3 bucket.</p><p class="callout-heading">Answers</p><p class="callout">B, Remember that RecordIO-protobuf has a pipe mode, which allows us to stream data directly from S3.</p></li>
				<li>You are the cloud administrator of your company. You have done great work creating and managing user access and you have fine-grained control of daily activities in the cloud. However, you want to add an extra layer of security by identifying accounts that are attempting to create cloud resources from an unusual IP address. What would be the fastest solution to address this use case (choose all the correct answers)?<p>a) Create an IP Insights model to identity anomalous accesses.</p><p>b) Create a clustering model to identify anomalies in the application connections.</p><p>c) Integrate your IP Insights with existing rules from Amazon Guard Duty.</p><p>d) Integrate your anomaly detection model with existing rules from Amazon Guard Duty.</p><p class="callout-heading">Answers</p><p class="callout">A,C, Remember: You can always come up with different approaches to solve problems. However, taking advantage of SageMaker's built-in algorithms is usually the fastest way to do things.</p></li>
				<li>You are working as a data scientist for a large company. One of your internal clients has requested that you improve a regression model that they have implemented in production. You have added a few features to the model and now you want to know if the model's performance has improved due to this change. Which of the following options best describes the evaluation metrics that you should use to evaluate your change?<p>a) Check if the R squared of the new model is better than the R squared of the current model in production.</p><p>b) Check if the R squared adjusted of the new model is better than the R squared of the current model in production.</p><p>c) Check if the R squared and the RMSE of the new model are better than the R squared of the current model in production.</p><p>d) Check if the R squared adjusted RMSE of the new model is better than the R squared of the current model in production.</p><p class="callout-heading">Answers</p><p class="callout">D, In this case, you have been exposed to a particular behavior of regression model evaluation, where, by adding new features, you will always increase R squared. You should use R squared adjusted to understand if the new features are adding value to the model or not. Additionally, RMSE will give you a business perspective of the model's performance. Although option b is correct, option d best describes the optimal decision you should make.</p></li>
				<li>Which of the following algorithms is optimized to work with sparse data?<p>a) Factorization machines</p><p>b) XGBoost</p><p>c) Linear Learner</p><p>d) KNN</p><p class="callout-heading">Answers</p><p class="callout">A, Factorization machines is a general-purpose algorithm that is optimized for sparse data.</p></li>
				<li>Which of the following algorithms uses an ensemble method based on decision trees during the training process?<p>a) Factorization machines</p><p>b) XGBoost</p><p>c) Linear learner</p><p>d) KNN</p><p class="callout-heading">Answers</p><p class="callout">B, XGBoost is a very popular algorithm that uses an ensemble of decision trees to train the model. XGBoost uses a boosting approach, where decision trees try to correct the error of the prior model.</p></li>
				<li>Which of the following options is considered an index-based algorithm?<p>a) Factorization machines</p><p>b) XGBoost</p><p>c) Linear learner</p><p>d) KNN</p><p class="callout-heading">Answers</p><p class="callout">D, We say that KNN is an index-based algorithm because it has to compute distances between points, assign indexes for these points, and then store the sorted distances and their indexes. With that type of data structure, KNN can easily select the top K closest points to make the final prediction. </p></li>
				<li>You are a data scientist in a big retail company that wants to predict their sales per region on a monthly basis. You have done some exploratory work and discovered that the sales pattern per region is different. Your team has decided to approach this project as a time series model, and now, you have to select the best approach to create a solution. Which of the following options would potentially give you a good solution with the least effort?<p>a) Approach this problem using the ARIMA algorithm. Since each region might have different sales behavior, you would have to create one independent model per region.</p><p>b) Approach this problem with the RNN algorithm. Since neural networks are robust, you could create one single model to predict sales in any region.</p><p>c) Develop a DeepAR model and set the region, associated with each time series, as a vector of static categorical features. You can use the <strong class="bold">cat</strong> field to set up this option.</p><p>d) Develop a DeepAR model and set the region, associated with each time series, as a vector of static categorical features. You can use the <strong class="bold">dynamic_feat</strong> field to set this option.</p><p class="callout-heading">Answers</p><p class="callout">C, Options a and c are potentially right. However, the problem states that we want the solution with the least effort. In this case, setting up a DeepAR model and separating the time series by region would be the expected solution (option c). We can set up that type of configuration by passing a vector of static categorical features into the cat field of the DeepAR class model.</p></li>
				<li>You are working on a dataset that contains nine numerical variables. You want to create a scatter plot to see if those variables could be potentially grouped on clusters of high similarity. How could you achieve this goal?<p>a) Execute the K-means algorithm.</p><p>b) Compute the two <strong class="bold">principal components</strong> (<strong class="bold">PCs</strong>) using PCA. Then, plot PC1 and PC2 in the scatter plot.</p><p>c) Execute the KNN algorithm.</p><p>d) Compute the three <strong class="bold">principal components</strong> (<strong class="bold">PCs</strong>) using PCA. Then, plot PC1, PC2, and PC3 in the scatter plot.</p><p class="callout-heading">Answers</p><p class="callout">B, Using K-means or KNN will not solve this question. You have to apply PCA to reduce the number of features and then plot the results in a scatter plot. Since scatter plots only accept two variables, option b is the right one.</p></li>
				<li>How should you preprocess your data in order to train a Blazing Text model on top of 100 text files? <p>a) You should create a text file with space-separated tokens. Each line of the file must contain a single sentence. If you have multiple files for training, you should concatenate all of them into a single one.</p><p>b) You should create a text file with space-separated tokens. Each line of the file must contain a single sentence. If you have multiple files for training, you should apply the same transformation to each one.</p><p>c) You should create a text file with comma-separated tokens. Each line of the file must contain a single sentence. If you have multiple files for training, you should concatenate all of them into a single one.</p><p>d) You should create a text file with comma-separated tokens. Each line of the file must contain a single sentence. If you have multiple files for training, you should apply the same transformation to each one.</p><p class="callout-heading">Answers</p><p class="callout">Option a is the right one. You should provide just a single file to Blazing Text with space-separated tokens, where each line of the file must contain a single sentence.</p></li>
			</ol>
		</div>
	</body></html>