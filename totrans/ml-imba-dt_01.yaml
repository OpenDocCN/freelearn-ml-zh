- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Data Imbalance in Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning algorithms have helped solve real-world problems as diverse
    as disease prediction and online shopping. However, many problems we would like
    to address with machine learning involve imbalanced datasets. In this chapter,
    we will discuss and define imbalanced datasets, explaining how they differ from
    other types of datasets. The ubiquity of imbalanced data will be demonstrated
    with examples of common problems and scenarios. We will also go through the basics
    of machine learning and cover the essentials, such as loss functions, regularization,
    and feature engineering. We will also learn about common evaluation metrics, particularly
    those that can be very helpful for imbalanced datasets. We will then introduce
    the `imbalanced-learn` library.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will learn about the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to imbalanced datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning 101
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of datasets and splits
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common evaluation metrics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and considerations when dealing with imbalanced data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When can we have an imbalance in datasets?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why can imbalanced data be a challenge?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to not worry about data imbalance
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the `imbalanced-learn` library
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General rules to follow
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will utilize common libraries such as `numpy` and `scikit-learn`
    and introduce the `imbalanced-learn` library. The code and notebooks for this
    chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter01](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter01).
    You can fire up the GitHub notebook using Google Colab by clicking on the **Open
    in Colab** icon at the top of this chapter’s notebook or by launching it from
    [https://colab.research.google.com](https://colab.research.google.com) using the
    GitHub URL of the notebook.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to imbalanced datasets
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning algorithms learn from collections of examples that we call
    **datasets**. These datasets contain multiple data samples or points, which we
    may refer to as examples, samples, or instances interchangeably throughout this
    book.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'A dataset can be said to have a balanced distribution when all the target classes
    have a similar number of examples, as shown in *Figure 1**.1*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_01_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Balanced distribution with an almost equal number of examples for
    each class
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Imbalanced datasets or skewed datasets are those that have some target classes
    (also called labels) that outnumber the rest of the classes (*Figure 1**.2*).
    Though this generally applies to classification problems (for example, fraud detection)
    in machine learning, they inevitably occur in regression problems (for example,
    house price prediction) too:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_01_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – An imbalanced dataset with five classes and a varying number of
    samples
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'We label the class with more instances as the “majority” or “negative” class
    and the one with fewer instances as the “minority” or “positive” class. Most of
    the time, our main interest lies in the minority class, which is why we often
    refer to the minority class as the “positive” class and to the majority class
    as the “negative” class:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将具有更多实例的类别标记为“多数”或“负”类，将具有较少实例的类别标记为“少数”或“正”类。大多数时候，我们的主要兴趣在于少数类，这就是为什么我们经常将少数类称为“正”类，将多数类称为“负”类：
- en: '![](img/B17259_01_03.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_01_03.jpg)'
- en: Figure 1.3 – A visual guide to common terminology used in imbalanced classification
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 不平衡分类中常用术语的视觉指南
- en: This can be scaled to more than two classes, and such classification problems
    are called multi-class classification. In the first half of this book, we will
    focus our attention only on binary class classification to keep the material easier
    to grasp. It’s relatively easy to extend the concepts to multi-class classification.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以扩展到超过两个类别，这样的分类问题称为多类分类。在这本书的前半部分，我们将只关注二元分类，以使材料更容易理解。将概念扩展到多类分类相对容易。
- en: 'Let’s look at a few examples of imbalanced datasets:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看几个不平衡数据集的例子：
- en: '**Fraud detection** is where fraudulent transactions need to be detected out
    of several transactions. This problem is often encountered and widely used in
    finance, healthcare, and e-commerce industries.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈检测**是指从多笔交易中检测出欺诈交易。这个问题在金融、医疗和电子商务行业中经常遇到并被广泛使用。'
- en: '**Network intrusion detection** using machine learning involves analyzing large
    volumes of network traffic data to detect and prevent instances of unauthorized
    access and misuse of computer systems.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习的**网络入侵检测**涉及分析大量网络流量数据，以检测和防止未经授权的访问和计算机系统的滥用。
- en: '**Cancer detection**. Cancer is not rare, but we still may want to use machine
    learning to analyze medical data to identify potential cases of cancer earlier
    and improve treatment outcomes.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**癌症检测**。癌症并不罕见，但我们仍然可能想使用机器学习来分析医疗数据，以更早地识别潜在的癌症病例并改善治疗效果。'
- en: In this book, we would like to focus on the class imbalance problem in general
    and look at various solutions where we see that class imbalance is affecting the
    performance of our model. A typical problem is that models perform quite poorly
    on the minority classes for which the model has seen a very low number of examples
    during model training.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们希望专注于一般类别不平衡问题，并查看各种解决方案，在这些解决方案中，我们看到类别不平衡正在影响我们模型的性能。一个典型的问题是在模型训练期间模型看到的少数类示例非常少，模型在这些少数类上的表现相当差。
- en: Machine learning 101
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习101
- en: 'Let’s do a quick overview of machine learning and its related fields:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速概述一下机器学习及其相关领域：
- en: '**Artificial intelligence** is the superset of all intelligence-related problems.
    Classical machine learning encompasses problems that can be solved by training
    traditional classical models (such as decision trees or logistic regression) and
    predicting the target values. They typically work on tabular data, require extensive
    feature engineering (manual development of features), and are less effective on
    text and image data. Deep learning tends to do far better on image, text, speech,
    and video data, wherein, typically, no manual feature engineering is needed, and
    various layers in the neural network automatically do feature engineering for
    us.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工智能**是所有与智能相关问题的超集。经典机器学习包括可以通过训练传统经典模型（如决策树或逻辑回归）并预测目标值来解决的问题。它们通常在表格数据上工作，需要大量的特征工程（手动开发特征），并且在文本和图像数据上的效果较差。深度学习在图像、文本、语音和视频数据上往往表现得更好，在这些情况下，通常不需要手动特征工程，神经网络中的各个层自动为我们进行特征工程。'
- en: In **supervised learning**, we have both inputs and outputs (labels) in the
    dataset, and the model learns to predict the output during the training. Each
    input can be represented as a list of features. The output or labels can be a
    finite set of classes (classification), a real number (regression), or something
    more complex. A classic example of supervised learning in classification is the
    Iris flowers classification. In this case, the dataset includes features such
    as petal length, petal width, sepal length, and sepal width, and the labels are
    the species of the Iris flowers (setosa, versicolor, or virginica). A model can
    be trained on this dataset and then be used to classify new, unseen Iris flowers
    as one of these species.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**监督学习**中，数据集既有输入也有输出（标签），模型在训练期间学习预测输出。每个输入可以表示为一个特征列表。输出或标签可以是有限类别的集合（分类）、实数（回归）或更复杂的东西。分类中的监督学习的一个经典例子是鸢尾花分类。在这种情况下，数据集包括花瓣长度、花瓣宽度、萼片长度和萼片宽度等特征，标签是鸢尾花的种类（setosa、versicolor或virginica）。可以在该数据集上训练一个模型，然后使用该模型将新的、未见过的鸢尾花分类为这些种类之一。
- en: In **unsupervised learning**, models either don’t have access to the labels
    or don’t use the labels and then try to make some predictions – for example, clustering
    the examples in the dataset into different groups.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**无监督学习**中，模型要么无法访问标签，要么不使用标签，然后尝试做出一些预测——例如，将数据集中的示例聚类到不同的组中。
- en: In **reinforcement learning**, the model tries to learn by making mistakes and
    optimizing a goal or profit variable. An example would be training a model to
    play chess and adjusting its strategy based on feedback received through rewards
    and penalties.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**强化学习**中，模型通过犯错误和优化目标或利润变量来尝试学习。一个例子是训练一个模型来下棋，并根据通过奖励和惩罚获得的反馈调整其策略。
- en: 'In supervised learning (which is the focus of this book), there are two main
    types of problems: classification and regression. Classification problems involve
    categorizing data into predefined classes or labels, such as “fraud” or “non-fraud”
    and “spam” or “non-spam.” On the other hand, regression problems aim to predict
    a continuous variable, such as the price of a house.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中（本书的重点），主要有两种类型的问题：分类和回归。分类问题涉及将数据分类到预定义的类别或标签中，例如“欺诈”或“非欺诈”以及“垃圾邮件”或“非垃圾邮件”。另一方面，回归问题旨在预测一个连续变量，例如房屋的价格。
- en: While data imbalance can also affect regression problems, this book will concentrate
    solely on classification problems. This focus is due to several factors, such
    as the limited scope of this book and the well-established techniques available
    for classification. In some cases, you might even be able to reframe a regression
    problem as a classification problem, making the methods discussed in this book
    still relevant.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据不平衡也可能影响回归问题，但本书将仅专注于分类问题。这种关注是由于几个因素，例如本书的范围有限以及分类中可用的技术已经建立。在某些情况下，甚至可以将回归问题重新构造成分类问题，使得本书中讨论的方法仍然相关。
- en: 'When it comes to various kinds of models that are popular for classification
    problems, we have quite a few categories of classical supervised machine learning
    models:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到用于分类问题的各种流行模型时，我们有相当多的经典监督机器学习模型类别：
- en: '**Logistic regression**: This is a supervised machine learning algorithm that’s
    used for binary classification problems. It predicts the probability of a binary
    target variable based on a set of predictor variables (features) by fitting a
    logistic function to the data, which outputs a value between 0 and 1.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归**：这是一种用于二元分类问题的监督机器学习算法。它通过拟合逻辑函数到数据，根据一组预测变量（特征）预测二元目标变量的概率，该函数输出介于0和1之间的值。'
- en: '**Support Vector Machines** (**SVMs**): These are supervised machine learning
    algorithms that are mainly used for classification and can be extended to regression
    problems. SVMs classify data by finding the optimal hyperplane that maximally
    separates the different classes in the input data, thus making it a powerful tool
    for binary and multiclass classification tasks.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVMs**）：这些是主要用于分类的监督机器学习算法，也可以扩展到回归问题。SVMs通过找到最优的超平面来最大化分离输入数据中的不同类别，从而成为二元和多类别分类任务的强大工具。'
- en: '**K-Nearest Neighbors** (**KNN**): This is a supervised machine learning algorithm
    that’s used for classification and regression analysis. It predicts the target
    variable based on the *k*-nearest neighbors in the training dataset. The value
    of *k* determines the number of neighbors to consider when making a prediction,
    and it can be tuned to optimize the model’s performance.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree models**: These are a type of supervised machine learning algorithm
    that’s used for classification and regression analysis. They recursively split
    the data into smaller subsets based on the most important features to create a
    decision tree that predicts the target variable based on the input features.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble models**: These combine multiple individual models to improve predictive
    accuracy and reduce overfitting (explained later in this chapter). Ensemble techniques
    include bagging (for example, random forest), boosting (for example, XGBoost),
    and stacking. They are commonly used for classification as well as regression
    analysis.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural networks**: These models are inspired by the human brain, consist
    of multiple layers with numerous neurons in each, and are capable of learning
    complex functions. We will discuss these in more detail in [*Chapter 6*](B17259_06.xhtml#_idTextAnchor185),
    *Data Imbalance in* *Deep Learning*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 1**.4* displays the decision boundaries of various classifiers we have
    reviewed so far. It shows that logistic regression has a linear decision boundary,
    while tree-based models such as decision trees, random forests, and XGBoost work
    by dividing examples into axis-parallel rectangles to form their decision boundary.
    SVM, on the other hand, transforms the data to a different space so that it can
    plot its non-linear decision boundary. Neural networks have a non-linear decision
    boundary:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_01_04.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – The decision boundaries of popular machine learning algorithms
    on an imbalanced dataset
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll delve into the principles underlying the process of model training.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: What happens during model training?
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the training phase of a machine learning model, we provide a dataset consisting
    of examples, each with input features and a corresponding label, to the model.
    Let X represent the list of features used for training, and y be the list of labels
    in the training dataset. The goal of the model is to learn a function, f, such
    that f(X) ≈ y.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The model has adjustable parameters, denoted as θ, which are fine-tuned during
    the training process. The error function, commonly referred to as the **loss function**,
    is defined as L(f(X; θ), y). This error function needs to be minimized by a learning
    algorithm, which finds the optimal setting of these parameters, θ.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'In classification problems, our typical loss functions are cross-entropy loss
    (also called the log loss):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: CrossEntropyLoss(p) = {− log(p) if y = 1  − log(1 − p) otherwise
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Here, p is the predicted probability from the model when y = 1.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: When the model’s prediction closely agrees with the target label, the loss function
    will approach zero. However, when the prediction deviates significantly from the
    target, the loss can become arbitrarily large, indicating a poor model fit.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'As training progresses, the training loss keeps going down (*Figure 1**.5*):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_01_05.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Rate of change of the loss function as training progresses
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the concept of the fit of a model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: A model is said to **underfit** if it is too simple and can’t capture the data’s
    complexity. It performs poorly on both training and new data.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model is of **right fit** if it accurately captures data patterns without
    learning noise. It performs well on both training and new data.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An **overfit** model is too complex and learns noise along with data patterns.
    It performs well on training data but poorly on new data (*Figure 1**.6*):'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_01_06.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Underfit, right fit, and overfit models for classification task
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s briefly try to learn about two important concepts in machine learning:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularization** is a set of techniques that are used to prevent the overfitting
    of a model to the training data. One type of regularization (namely L1 or L2)
    adds a penalty term to the loss function, which encourages the model to have smaller
    weights and reduces its complexity. This helps prevent the model from fitting
    too closely to the training data and generalizes better to unseen data.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering** is the process of selecting and transforming the input
    features of a model to improve its performance. Feature engineering involves selecting
    the most relevant features for the problem, transforming them to make them more
    informative, and creating new features from the existing ones. Good feature engineering
    can make a huge difference in the performance of a model and can often be more
    important than the choice of algorithm or hyperparameters.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of dataset and splits
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typically, we train our model on the training set and test the model on an independent
    unseen dataset called the test set. We do this to do a fair evaluation of the
    model. If we don’t do this and train the model on the full dataset and evaluate
    the model on the same dataset, we don’t know how good the model would do on unseen
    data, plus the model will likely be overfitted.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'We may encounter three kinds of datasets in machine learning:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set**: A dataset on which the model is trained.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set**: A dataset used for tuning the hyperparameters of the model.
    A validation set is often referred to as a development set.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation set or test set**: A dataset used for evaluating the performance
    of the model.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When working with small example datasets, it’s common to allocate 80% of the
    data for the training set, 10% for the validation set, and 10% for the test set.
    However, the specific ratio between training and test sets is not as important
    as ensuring that the test set is large enough to provide statistically meaningful
    evaluation results. In the context of big data, a split of 98%, 1%, and 1% for
    training, validation, and test sets, respectively, could be appropriate.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Often, people don’t have a dedicated validation set for hyperparameter tuning
    and refer to the test set as an evaluation set. This can happen when the hyperparameter
    tuning is not performed as a part of the regular training cycle and is a one-off
    activity.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Cross-validation** can be a confusing term to guess its meaning. Breaking
    it down: cross + validation, so it’s some sort of validation on an extended (cross)
    something. *Something* here is the test set for us.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what cross-validation is:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation is a technique that’s used to estimate how accurately a model
    will perform in practice
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is typically used to detect overfitting – that is, failing to generalize
    patterns in data, particularly when the amount of data may be limited
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at the different types of cross-validation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '**Holdout**: In the holdout method, we randomly assign data points to two sets,
    usually called the training set and the test set, respectively. We then train
    (build a model) on the *training set* and test (evaluate its performance) on the
    *test set*.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**k-fold**: This works as follows:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We randomly shuffle the data.
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We divide all the data into *k* parts, also known as folds. We train the model
    on *k*-1 folds and evaluate it on the remaining fold. We record the performance
    of this model using our chosen model evaluation metric, then discard this model.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We repeat this process *k* times, each time holding out a different subset for
    testing. We take an average of the evaluation metric values (for example, accuracy)
    from all the previous models. This average represents the overall performance
    measure of the model.
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**k-fold cross-validation** is mainly used when you have limited data points,
    say 100 points. Using 5 or 10 folds is the most common when doing cross-validation.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the common evaluation metrics in machine learning, with a special
    focus on the ones relevant to problems with imbalanced data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Common evaluation metrics
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several machine learning and deep learning metrics are used for evaluating the
    performance of classification models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some of the helpful metrics that can help evaluate the performance
    of our model on the test set.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given a model that tries to classify an example as belonging to the positive
    or negative class, there are four possibilities:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive (TP)**: This occurs when the model correctly predicts a sample
    as part of the positive class, which is its actual classification'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative (FN)**: This happens when the model incorrectly classifies
    a sample from the positive class as belonging to the negative class'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative (TN)**: This refers to instances where the model correctly
    identifies a sample as part of the negative class, which is its actual classification'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive (FP)**: This occurs when the model incorrectly predicts a
    sample from the negative class as belonging to the positive class'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Table 1.1* shows in what ways the model can get “confused” when making predictions,
    aptly called the **confusion matrix**. The confusion matrix forms the basis of
    many common metrics in machine learning:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predicted Positive** | **Predicted Negative** |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
- en: '| **Actually Positive** | True Positive (TP) | False Negative (FN) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| **Actually Negative** | False Positive (FP) | True Negative (TN) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: Table 1.1 – Confusion matrix
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some of the most common metrics in machine learning:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive Rate** (**TPR**) measures the proportion of actual positive
    examples correctly classified by the model:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TPR = Positives classified correctly  ______________ Total positives  =  TP _ TP
    + FN
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**False Positive Rate** (**FPR**) measures the proportion of actual negative
    examples that are incorrectly identified as positives by the model:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FPR = Negatives classified incorrectly  _______________  Total negatives  =
     FP _ FP + TN
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sklearn` library as `sklearn.metrics.accuracy_score`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` library under the name `sklearn.metrics.precision_score`. Precision
    =  TP _ TP + FP.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` library under the name `sklearn.metrics.recall_score`. Recall =  TP _ TP
    + FN.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Table 1.2* summarizes the differences between precision and recall:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| Definition | Precision is a measure of trustworthiness | Recall is a measure
    of completeness |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Question to ask | When the model says something is positive, how often is
    it right? | Out of all the positive instances, how many did the model correctly
    identify? |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| Example (using an email filter) | Precision measures how many of the emails
    the model flags as spam are actually spam, as a percentage of all the flagged
    emails | Recall measures how many of the actual spam emails the model catches,
    as a percentage of all the spam emails in the dataset |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| Formula | Precision =  TP _ TP + FP  | Recall =  TP _ TP + FN  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: Table 1.2 – Precision versus recall
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Why can accuracy be a bad metric for imbalanced datasets?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume we have an imbalanced dataset with 1,000 examples, with 100 labels
    belonging to class 1 (the minority class) and 900 belonging to class 0 (the majority
    class).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have a model that always predicts 0 for all examples. The model’s
    accuracy for the minority class is   900 + 0 _ (900 + 0+ 100 + 0 ) = 90%.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_01_07.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – A comic showing accuracy may not always be the right metric
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the **precision-recall trade-off** in machine learning. Usually,
    precision and recall are inversely correlated – that is, when recall increases,
    precision most often decreases. Why? Note that recall =  TP _ TP + FN and for
    recall to increase, FN should decrease. This means the model needs to classify
    more items as positive. However, if the model classifies more items as positive,
    some of these will likely be incorrect classifications, leading to an increase
    in the number of **false positives** (**FPs**). As the number of FPs increases,
    precision, defined as  TP _ TP + FP, will decrease. With similar logic, you can
    argue that when recall decreases, precision often increases.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s try to understand some of the precision and recall-based metrics
    that can help measure the performance of models trained on imbalanced data:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn` library as `sklearn.metrics.f1_score`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` library as `sklearn.metrics.fbeta_score`.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` library as `sklearn.metrics.balanced_accuracy_score`.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specificity (SPE)**: Specificity is a measure of the model’s ability to correctly
    identify the negative samples. In binary classification, it is calculated as the
    ratio of true negative predictions to the total number of negative samples. High
    specificity indicates that the model is good at identifying the negative class,
    while low specificity indicates that the model is biased toward the positive class.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn.metrics.precision_recall_fscore_support` and `imblearn.metrics.classification_report_imbalanced`
    APIs.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imbalanced-learn`, `geometric_mean_score()` is defined by the geometric mean
    of “accuracy on positive class examples” (recall or sensitivity or TPR) and “accuracy
    on negative class examples” (specificity or TNR). So, even if one class is heavily
    outnumbered by the other class, the metric will still be representative of the
    model’s overall performance.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imblearn.metrics.classification_report_imbalanced`.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Table 1.3* shows the associated metrics and their formulas as an extension
    of the confusion matrix:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predicted Positive** | **Predicted Negative** |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| **Actually** **Positive** | True positive (TP) | False negative (FN) | *Recall
    = Sensitivity = True positive* *rate (TPR) =*  TP _ TP + FN |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| **Actually** **Negative** | False positive (FP) | True negative (TN) | Specificity
    =  TN _ TN + FP  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '|  | Precision = TP/(TP+FP)FPR = FP/(FP+TN) |  | Accuracy =  TP + TN ______________  TP
    + TN + FP + FN F1 − score =  2 * Precision * Recall  _______________  Precision
    + Recall  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: Table 1.3 – Confusion matrix with various metrics and their definitions
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: ROC
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Receiver Operating Characteristics**, commonly known as **ROC** curves, are
    plots that display the **TPR** on the *y*-axis against the **FPR** on the *x*-axis
    for various threshold values:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve essentially represents the proportion of correctly predicted positive
    instances on the *y*-axis, contrasted with the proportion of incorrectly predicted
    negative instances on the *x*-axis.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In classification tasks, a threshold is a cut-off value that’s used to determine
    the class of an example. For instance, if a model classifies an example as “positive,”
    a threshold of 0.5 might be set to decide whether the instance should be labeled
    as belonging to the “positive” or “negative” class. The ROC curve can be used
    to identify the optimal threshold for a model. This topic will be discussed in
    detail in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To create the ROC curve, we calculate the TPR and FPR for many various threshold
    values of the model’s predicted probabilities. For each threshold, the corresponding
    TPR value is plotted on the *y*-axis, and the FPR value is plotted on the *x*-axis,
    creating a single point. By connecting these points, we generate the ROC curve
    (*Figure 1**.8*):'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_01_08.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – The ROC curve as a plot of TPR versus FPR (the dotted line shows
    a model with no skill)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Some properties of the ROC curve are as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Area Under Curve** (**AUC**) of a ROC curve (also called **AUC-ROC**)
    serves a specific purpose: it provides a single numerical value that represents
    the model’s performance across all possible classification thresholds:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUC-ROC represents the degree of separability of the classes. This means that
    the higher the AUC-ROC, the more the model can distinguish between the classes
    and predict a positive class example as positive and a negative class example
    as negative. A poor model with an AUC near 0 essentially predicts a positive class
    as a negative class and vice versa.
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The AUC-ROC of a random classifier is 0.5 and is the diagonal joining the points
    (0,0) and (1,0) on the ROC curve.
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The AUC-ROC has a probabilistic interpretation: an AUC of 0.9 indicates a 90%
    likelihood that the model will assign a higher score to a randomly chosen positive
    class example than to a negative class example. That is, AUC-ROC can be depicted
    as follows:'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: P(score(x+ ) > score(x− ))
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Here, 𝑥+ denotes the positive (minority) class, and 𝑥− denotes the negative
    (majority) class.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of evaluating model performance, it’s crucial to use a test
    set that reflects the distribution of the data the model will encounter in real-world
    scenarios. This is particularly relevant when considering metrics such as the
    ROC curve, which remains consistent regardless of changes in class imbalance within
    the test data. Whether we have 1:1, 1:10, or 1:100 as the minority_class: majority_class
    distribution in the test set, the ROC curve remains the same [2]. The reason for
    this is that both of these rates are independent of the class distribution in
    the test data because they are calculated only based on the correctly and incorrectly
    classified instances of each class, not the total number of instances of each
    class. This is not to be confused with the change in imbalance in the training
    data, which can adversely impact the model’s performance and would be reflected
    in the ROC curve.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s look at some of the problems in using ROC for imbalanced datasets:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: ROC does not distinguish between the various classes – that is, it does not
    emphasize one class more over the other. This can be a problem for imbalanced
    datasets where, often, the minority class is more important to detect than the
    majority class. Because of this, it may not reflect the minority class well. For
    example, we may want better recall over precision.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While ROC curves can be useful for comparing the performance of models across
    a full range of FPRs, they may not be as relevant for specific applications that
    require a very low FPR, such as fraud detection in financial transactions or banking
    applications. The reason the FPR needs to be very low is that such applications
    usually require limited manual intervention. The number of transactions that can
    be manually checked may be as low as 1% or even 0.1% of all the data, which means
    the FPR can’t be higher than 0.001\. In these cases, anything to the right of
    an FPR equal to 0.001 on the ROC curve becomes irrelevant [3]. To further understand
    this point, let’s consider an example:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s say that for a test set, we have a total of 10,000 examples and only 100
    examples of the positive class, making up 1% of the examples. So, any FPR higher
    than 1% - that is, 0.01 – is going to raise too many alerts to be handled manually
    by investigators.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance on the far left-hand side of the ROC curve becomes crucial in
    most real-world problems, which are often dominated by a large number of negative
    instances. As a result, most of the ROC curve becomes irrelevant for applications
    that need to maintain a very low FPR.
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision-Recall curve
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to ROC curves, **Precision-Recall** (**PR**) curves plot a pair of
    metrics for different threshold values. But unlike ROC curves, which plot TPR
    and FPR, PR curves plot precision and recall. To demonstrate the difference between
    the two curves, let’s say we compare the performance of two models – Model 1 and
    Model 2 – on a particular handcrafted imbalanced dataset:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 1**.9 (a)*, the ROC curves for both models appear to be close to
    the top-left corner (point (0, 1)), which might lead you to conclude that both
    models are performing well. However, this can be misleading, especially in the
    context of imbalanced datasets.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we turn our attention to the PR curves in *Figure 1**.9 (b)*, a different
    story unfolds. Model 2 comes closer to the ideal top-right corner (point (1, 1))
    of the plot, indicating that its performance is much better than Model 1 when
    precision and recall are considered.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PR curve reveals that Model 2 has an advantage over Model 1.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This discrepancy between the ROC and PR curves also underscores the importance
    of using multiple metrics for model evaluation, particularly when dealing with
    imbalanced data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_01_09.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – The PR curve can show obvious differences between models compared
    to the ROC curve
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to understand these observations in detail. While the ROC curve shows
    very little difference between the performance of the two models, the PR curve
    shows a much bigger gap. The reason for this is that the ROC curve uses FPR, which
    is FP/(FP+TN). Usually, TN is really high for an imbalanced dataset, and hence
    even if FP changes by a decent amount, FPR’s overall value is overshadowed by
    TN. Hence, ROC doesn’t change by a whole lot.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The conclusion of which classifier is superior can change with the distribution
    of classes in the test set. In the case of skewed datasets, the PR curve can more
    clearly show that the model did not work well compared to the ROC curve, as shown
    in the preceding figure.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The `sklearn` is `sklearn.metrics.average_precision_score`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Relation between the ROC curve and PR curve
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary distinction between the ROC curve and the PR curve lies in the fact
    that while ROC assesses how well the model can “calculate” both positive and negative
    classes, PR solely focuses on the positive class. Therefore, when dealing with
    a balanced dataset scenario and you are concerned with both the positive and negative
    classes, ROC AUC works exceptionally well. In contrast, when dealing with an imbalanced
    situation, PR AUC is more suitable. However, it’s important to keep in mind that
    PR AUC only evaluates the model’s ability to “calculate” the positive class. *Because
    PR curves are more sensitive to the positive (minority) class, we will be using
    PR curves throughout the first half of* *this book.*
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: We can reimagine the PR curve with precision on the *x*-axis and TPR, also known
    as recall, on the *y*-axis. The key difference between the two curves is that
    while the ROC curve uses FPR, the PR curve uses precision.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, FPR tends to be very low when dealing with imbalanced
    datasets. This aspect of having low FPR values is crucial in certain applications
    such as fraud detection, where the capacity for manual investigations is inherently
    limited. Consequently, this perspective can alter the perceived performance of
    classifiers. As shown in *Figure 1**.9*, it’s also possible that the performances
    of the two models seem reversed when compared using average precision (0.69 versus
    0.90) instead of AUC-ROC (0.97 and 0.95).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s summarize this:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The AUC-ROC is the area under the curve plotted with TPR on the *y*-axis and
    FPR on the *x*-axis.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AUC-PR is the area under the curve plotted with precision on the *y*-axis
    and recall on the *x*-axis.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As TPR equals recall, the two plots only differ in what recall is compared
    to – either precision or FPR. Additionally, the plots are rotated by 90 degrees
    relative to each other:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **AUC-ROC** | **AUC-PR** |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| General formula | AUC(TPR, FPR) | AUC(Precision, Recall) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| Expanded formula | AUC(  TP _ TP + FN ,  FP _ FP + TN ) | AUC(  TP _ TP +
    FP ,  TP _ TP + FN ) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| Equivalence | AUC(Recall, FPR) | AUC(Precision, Recall) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: Table 1.4 – Comparing the ROC and PR curves
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we’ll explore the circumstances that lead to imbalances
    in datasets, the challenges these imbalances can pose, and the situations where
    data imbalance might not be a concern.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and considerations when dealing with imbalanced data
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In certain instances, directly using data for machine learning without worrying
    about data imbalance can yield usable results suitable for a given business scenario.
    Yet, there are situations where a more dedicated effort is needed to manage the
    effects of imbalanced data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Broad statements claiming that you must always or never adjust for imbalanced
    classes tend to be misleading. The truth is that the need to address class imbalance
    is contingent on the specific characteristics of the data, the problem at hand,
    and the definition of an acceptable solution. Therefore, the approach to dealing
    with class imbalance should be tailored according to these factors.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: When can we have an imbalance in datasets?
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll explore various situations and causes leading to an
    imbalance in datasets, such as rare event occurrences or skewed data collection
    processes:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '**Inherent in the problem**: Sometimes, the task we need to solve involves
    detecting outliers in datasets – for example, patients with a certain disease
    or fraud cases in a set of transactions. In such cases, the dataset is inherently
    imbalanced because the target events are rare to begin with.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High cost of data collection while bootstrapping a machine learning solution**:
    The cost of collecting data might be too high for certain classes. For example,
    collecting data on COVID-19 patients incurs high costs due to the need for specialized
    medical tests, protective equipment, and the ethical and logistical challenges
    of obtaining informed consent in a high-stress healthcare environment.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noisy labels for certain classes**: This may happen when a lot of noise is
    introduced into the labels of the dataset for certain classes during data collection.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labeling errors**: Errors in labeling can also contribute to data imbalance.
    For example, if some samples are mistakenly labeled as negative when they are
    positive, this can result in an imbalance in the dataset. Additionally, if a class
    is already inherently rare, human annotators might be biased and overlook the
    few examples of that rare class that do exist.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling bias**: Data collection methods can sometimes introduce bias in
    the dataset. For example, if a survey is conducted in a specific geographical
    area or among a specific group of people, the resulting dataset may not be representative
    of the entire population.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data cleaning**: During the data cleaning or filtering process, some classes
    or samples may be removed due to incomplete or missing data. This can result in
    an imbalance in the remaining dataset.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why can imbalanced data be a challenge?
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s delve into the difficulties posed by imbalanced data on model predictions
    and their impact on model performance:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '**Failure of metrics such as accuracy**: As we discussed previously, conventional
    metrics such as accuracy can be misleading in the context of imbalanced data (a
    99% imbalanced dataset would still achieve 99% accuracy). Threshold-invariant
    metrics such as the PR curve or ROC curve attempt to expose the performance of
    the model over a wide range of thresholds. The real challenge lies in the disproportionate
    influence of the “true negative” cell in the confusion matrix. Metrics that focus
    less on “true negatives,” such as precision, recall, or F1 score, are more appropriate
    for evaluating model performance. It’s important to note that these metrics have
    a hidden hyperparameter – the classification threshold – that should not be ignored
    but optimized for real-world applications (refer to [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151),
    *Cost-Sensitive Learning*, to learn more about threshold tuning).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imbalanced data can be a challenge for a model’s loss function**: This may
    happen because the loss function is typically designed to minimize the errors
    between the predicted outputs and the true labels of the training data. When the
    data is imbalanced, there are more instances of one class than another, and the
    model may become biased toward the majority class. We will discuss solutions to
    this issue in more detail in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151),
    *Cost-Sensitive Learning*, and [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep* *Learning Techniques*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Different misclassification costs for different classes**: Often, it may
    be more expensive to misclassify positive examples than to misclassify negative
    examples. We may have false positives that are more expensive than false negatives.
    For example, usually, the cost of misclassifying a patient with cancer as healthy
    (false negative) will be much higher than misclassifying a healthy patient as
    having cancer (false positive). Why? Because it’s much cheaper to go through some
    extra tests to revalidate the test results in the second case instead of detecting
    it much later in the first case. This is called the cost of misclassification,
    which could be different for the majority and minority classes, making things
    complicated for imbalanced datasets. We will discuss more about this in [*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constraints on computational resources**: In sectors such as finance, healthcare,
    and retail, handling big data is a common challenge. Training on these large datasets
    is not only time-consuming but also costly due to the computational power needed.
    In such scenarios, downsampling or undersampling the majority class becomes essential,
    as will be discussed in [*Chapter 3*](B17259_03.xhtml#_idTextAnchor079), *Undersampling
    Methods*. Additionally, acquiring more samples for the minority class can further
    increase dataset size and computational costs. Memory limitations may also restrict
    the amount of data that can be processed.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not enough variation in the minority class examples to sufficiently represent
    its distribution**: Often, an absolute number of samples of the minority class
    is not as big of a problem as the **variation** in the samples of the minority
    class. The dataset might look large, but there might not be many variations or
    varieties in the samples that adequately represent the distribution of minority
    classes. This can lead to the model not being able to learn the classification
    boundary properly, which would lead to poor performance of the model (*Figure
    1**.10*). This can often happen in computer vision problems, such as object detection,
    where we may have very few samples of certain classes. In such cases, data augmentation
    techniques (discussed in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level
    Deep Learning Methods*) can help significantly:'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_01_10.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – Change in decision boundary with a different distribution of minority
    class examples – the crosses denote the majority class, and the circles denote
    the minority class
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '**Poor performance of uncalibrated models**: Imbalanced data can be a challenge
    for uncalibrated models. Uncalibrated models are models that do not output well-calibrated
    probabilities, which means that the predicted probabilities may not reflect the
    true likelihood of the predicted classes:'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When dealing with imbalanced data, uncalibrated models can be particularly susceptible
    to producing biased predictions toward the majority class as they may not be able
    to effectively differentiate between the minority and majority classes. This can
    lead to poor performance in the minority class, where the model may produce overly
    confident predictions or predictions that are too conservative.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, an uncalibrated model that is trained on imbalanced data may incorrectly
    classify instances that belong to the minority class as majority class examples,
    often with high confidence. This is because the model may not have learned to
    adjust its predictions based on the imbalance in the data and may not have a good
    understanding of the minority class examples.
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To address this challenge, it is important to use well-calibrated models [4]
    that can output probabilities that reflect the true likelihood of the predicted
    classes. This can be achieved through techniques such as Platt scaling or isotonic
    regression, which can calibrate the predicted probabilities of an uncalibrated
    model to produce more accurate and reliable probabilities. Model calibration will
    be discussed in detail in [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279), *Model
    Calibration*.
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poor performance of models because of non-adjusted thresholds**: It’s important
    to use intelligent thresholding when making predictions using models trained on
    imbalanced datasets. Simply predicting 1 when the model probability is over 0.5
    may not always be the best approach. Instead, we should consider other thresholds
    that may be more effective. This can be achieved by examining the PR curve of
    the model rather than relying solely on its success rate with a default probability
    threshold of 0.5\. Threshold adjustment can be quite important, even for models
    trained on naturally or artificially balanced datasets. We will discuss threshold
    adjustment in detail in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive
    Learning*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s try to see when we shouldn’t do anything about data imbalance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: When to not worry about data imbalance
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Class imbalance may not always negatively impact performance, and using imbalance-specific
    methods can sometimes worsen results [5]. Therefore, it’s crucial to accurately
    assess whether a task is genuinely affected by class imbalance before applying
    any specialized techniques. One such strategy can be as simple as setting up a
    baseline model without worrying about class imbalance and observing the model’s
    performance on various classes using various performance metrics.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore scenarios where data imbalance may not be a concern and no corrective
    measures may be needed:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '**When the imbalance is small**: If the imbalance in the dataset is relatively
    small, with the ratio of the minority class to the majority class being only slightly
    skewed (say 4:5 or 2:3), the impact on the model’s performance may be minimal.
    In such cases, the model may still perform reasonably well without requiring any
    special techniques to handle the imbalance.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When the goal is to predict the majority class**: In some cases, the focus
    may be on predicting the majority class accurately, and the minority class may
    not be of particular interest. For example, in online ad placement, the focus
    can be on targeting users (majority class) likely to click on ads to maximize
    click-through rates and immediate revenue, while less attention is given to users
    (minority class) who may find ads annoying.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When the cost of misclassification is nearly equal for both classes**: In
    some applications, the cost of misclassifying a positive class example is not
    high (that is, false negative). An example is classifying emails as spam or non-spam.
    It’s totally fine to miss a spam email once in a while and misclassify it as non-spam.
    In such cases, the impact of misclassification on the performance metrics may
    be negligible, and the imbalance may not be a concern.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When the dataset is sufficiently large**: Even if the ratio of minority to
    majority class samples is very low, such as 1:100, and if the dataset is sufficiently
    large, with a large number of samples in both the minority and majority classes,
    the impact of data imbalance on the model’s performance may be reduced. With a
    larger dataset, the model may be able to learn the patterns in the minority class
    more effectively. However, it would still be advisable to compare the baseline
    model’s performance with the performance of models that take the data imbalance
    into account. For example, compare a baseline model to models with threshold adjustment,
    oversampling, and undersampling ([*Chapter 2*](B17259_02.xhtml#_idTextAnchor042),
    *Oversampling Methods*, and [*Chapter 3*](B17259_03.xhtml#_idTextAnchor079), *Undersampling
    Methods*), and algorithm-based techniques such as cost-sensitive learning ([*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will become familiar with a library that can be very
    useful when dealing with imbalanced data. We will train a model on an imbalanced
    toy dataset and look at some metrics to evaluate the performance of the trained
    model.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the imbalanced-learn library
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`imbalanced-learn` (imported as `imblearn`) is a Python package that offers
    several techniques to deal with data imbalance. In the first half of this book,
    we will rely heavily on this library. Let’s install the `imbalanced-learn` library:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can use `imbalanced-learn` to create a synthetic dataset for our analysis:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s analyze the generated dataset:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here’s the output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/B17259_01_11.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – 2 class dataset with two features
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s split this dataset into training and test sets:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here’s the output:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note the usage of `stratify` in the `train_test_split` API of `sklearn`. Specifying
    `stratify=y` ensures we maintain the same ratio of majority and minority classes
    in both the training set and the test set. Let’s understand stratification in
    more detail.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '**Stratified sampling** is a way to split the dataset into various subgroups
    (called “strata”) based on certain characteristics they share. It can be highly
    valuable when dealing with imbalanced datasets because it ensures that the train
    and test datasets have the same proportions of class labels as the original dataset.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: In an imbalanced dataset, the minority class constitutes a small fraction of
    the total data. If we perform a simple random split without any stratification,
    there’s a risk that the minority class may not be adequately represented in the
    training set or could be entirely left out from the test set, which may lead to
    poor performance and unreliable evaluation metrics.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: With stratified sampling, the proportion of each class in the overall dataset
    is preserved in both training and test sets, ensuring representative sampling
    and a better chance for the model to learn from the minority class. This leads
    to a more robust model and a more reliable evaluation of the model’s performance.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn APIs for stratification
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The `scikit-learn` APIs, such as `RepeatedStratifiedKFold` and `StratifiedKFold`,
    employ the concept of stratification to evaluate model performance through cross-validation,
    especially when working with imbalanced datasets.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s train a logistic regression model on training data:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s get the report metrics from the `sklearn` library:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This outputs the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s get the report metrics from `imblearn`:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This outputs a lot more columns:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_01_12.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – Output of the classification report from imbalanced-learn
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Do you notice the extra metrics here compared to the API of `sklearn`? We got
    three additional metrics: `spe` for specificity, `geo` for geometric mean, and
    `iba` for index balanced accuracy.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: The `imblearn.metrics` module has several such functions that can be helpful
    for imbalanced datasets. Apart from `classification_report_imbalanced()`, it offers
    APIs such as `sensitivity_specificity_support()`, `geometric_mean_score()`, `sensitivity_score()`,
    and `specificity_score()`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: General rules to follow
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Usually, the first step in any machine learning pipeline should be to split
    the data into train/test/validation sets. We should avoid applying any techniques
    to handle the imbalance until after the data has been split. We should begin by
    splitting the data into training, testing, and validation sets and then proceed
    with any necessary adjustments to the training data. Applying techniques such
    as oversampling (see [*Chapter 2*](B17259_02.xhtml#_idTextAnchor042), *Oversampling
    Methods*) before splitting the data can result in data leakage, overfitting, and
    over-optimism [6].
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: We should ensure that the validation data closely resembles the test data. Both
    validation data and test data should represent real-world scenarios on which the
    model will be used for prediction. Avoid applying any sampling techniques or modifications
    to the validation set. The only requirement is to include a sufficient number
    of samples from all classes.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Let’s switch to discussing a bit about using unsupervised learning algorithms.
    **Anomaly detection** or **outlier detection** is a class of problems that can
    be used for dealing with imbalanced data problems. Anomalies or outliers are data
    points that deviate significantly from the rest of the data. These anomalies often
    correspond to the minority class in an imbalanced dataset, making unsupervised
    methods potentially useful.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The term that’s often used for these kinds of problems is **one-class classification**.
    This technique is particularly beneficial when the positive (minority) cases are
    sparse or when gathering them before the training is not feasible. The model is
    trained exclusively on what is considered the “normal” or majority class. It then
    classifies new instances as “normal” or “anomalous,” effectively identifying what
    could be the minority class. This can be especially useful for binary imbalanced
    classification problems, where the majority class is deemed “normal,” and the
    minority class is considered an anomaly.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it does have a drawback: outliers or positive cases during training
    are discarded [7], which could lead to the potential loss of valuable information.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: In summary, while unsupervised methods such as one-class classification offer
    an alternative for managing class imbalance, our discussion in this book will
    remain centered on supervised learning algorithms. Nevertheless, we recommend
    that you explore and experiment with such solutions when you find them appropriate.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s summarize what we’ve learned so far. Imbalanced data is a common problem
    in machine learning, where there are significantly more instances of one class
    than another. Imbalanced datasets can arise from various situations, including
    rare event occurrences, high data collection costs, noisy labels, labeling errors,
    sampling bias, and data cleaning. This can be a challenge for machine learning
    models as they may be biased toward the majority class.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Several techniques can be used to deal with imbalanced data, such as oversampling,
    undersampling, and cost-sensitive learning. The best technique to use depends
    on the specific problem and the data.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, data imbalance may not be a concern. When the dataset is sufficiently
    large, the impact of data imbalance on the model’s performance may be reduced.
    However, it is still advisable to compare the baseline model’s performance with
    the performance of models that have been built using techniques that address data
    imbalance, such as threshold adjustment, data-based techniques (oversampling and
    undersampling), and algorithm-based techniques.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Traditional performance metrics such as accuracy can fail in imbalanced datasets.
    Some more useful metrics for imbalanced datasets are the ROC curve, the PR curve,
    precision, recall, and F1 score. While ROC curves are suitable for balanced datasets,
    PR curves are more suitable for imbalanced datasets when one class is more important
    than the other.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The `imbalanced-learn` library is a Python package that offers several techniques
    to deal with data imbalance.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: There are some general rules to follow, such as splitting the data into train/test/validation
    sets before applying any techniques to handle the imbalance in the data, ensuring
    that the validation data closely resembles the test data and that test data represents
    the data on which the model will make final predictions, and avoiding applying
    any sampling techniques or modifications to the validation set and test set.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: One-class classification or anomaly detection is another technique that can
    be used for dealing with unsupervised imbalanced data problems. In this book,
    we will focus our discussion on supervised learning algorithms only.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at one of the common ways to handle the data
    imbalance problem in datasets by applying oversampling techniques.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does the choice of loss function when training a model affect the performance
    of the model on imbalanced datasets?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you explain why the PR curve is more informative than the ROC curve when
    dealing with highly skewed datasets?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the potential issues with using accuracy as a metric for model
    performance on imbalanced datasets?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the concept of “class imbalance” affect the process of feature engineering
    in machine learning?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the context of imbalanced datasets, how does the choice of “k” in k-fold
    cross-validation affect the performance of the model? How would you fix the issue?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the distribution of classes in the test data affect the PR curve, and
    why? What about the ROC curve?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the implications of having a high AUC-ROC but a low AUC-PR in the context
    of an imbalanced dataset?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the concept of “sampling bias” contribute to the challenge of imbalanced
    datasets in machine learning?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the concept of “labeling errors” contribute to the challenge of imbalanced
    datasets in machine learning?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the real-world scenarios where dealing with imbalanced datasets
    is inherently part of the problem?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`fetch_dataset` API and then compute the values of MCC, accuracy, precision,
    recall, and F1 score. See if the MCC value can be a useful metric for this dataset.'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'V. García, R. A. Mollineda, and J. S. Sánchez, *Index of Balanced Accuracy:
    A Performance Measure for Skewed Class Distributions*, in Pattern Recognition
    and Image Analysis, vol. 5524, H. Araujo, A. M. Mendonça, A. J. Pinho, and M.
    I. Torres, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2009, pp. 441–448\.
    Accessed: Mar. 18, 2023\. [Online]. Available at [http://link.springer.com/10.1007/978-3-642-02172-5_57](http://link.springer.com/10.1007/978-3-642-02172-5_57).'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'T. Fawcett, *An introduction to ROC analysis*, Pattern Recognition Letters,
    vol. 27, no. 8, pp. 861–874, Jun. 2006, doi: 10.1016/j.patrec.2005.10.010.'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y.-A. Le Borgne, W. Siblini, B. Lebichot, and G. Bontempi, *Reproducible Machine
    Learning for Credit Card Fraud Detection - Practical Handbook*. Université Libre
    de Bruxelles, 2022\. [Online]. Available at [https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook](https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook).
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'W. Siblini, J. Fréry, L. He-Guelton, F. Oblé, and Y.-Q. Wang, *Master your
    Metrics with Calibration*, vol. 12080, 2020, pp. 457–469\. doi: 10.1007/978-3-030-44584-3_36.'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou, *Exploratory Undersampling for Class-Imbalance
    Learning*, IEEE Trans. Syst., Man, Cybern. B, vol. 39, no. 2, pp. 539–550, Apr.
    2009, doi: 10.1109/TSMCB.2008.2007853.'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. S. Santos, J. P. Soares, P. H. Abreu, H. Araujo, and J. Santos, *Cross-Validation
    for Imbalanced Datasets: Avoiding Overoptimistic and Overfitting Approaches [Research
    Frontier]*, IEEE Comput. Intell. Mag., vol. 13, no. 4, pp. 59–76, Nov. 2018, doi:
    10.1109/MCI.2018.2866730.'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Fernández, S. García, M. Galar, R. Prati, B. Krawczyk, and F. Herrera,
    *Learning from Imbalanced Data Sets*. Springer International Publishing, 2018
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
