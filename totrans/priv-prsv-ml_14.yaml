- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Preserving Privacy in Large Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在大型语言模型中保护隐私
- en: '**Large language models** (**LLMs**) have emerged as a transformative technology
    in the field of artificial intelligence (AI), enabling advanced **natural language
    processing** (**NLP**) tasks and generative capabilities. These models, such as
    OpenAI’s GPT-3.5 and Meta’s Llama 2 have shown remarkable proficiency in generating
    human-like text and demonstrating a deep understanding of language patterns. In
    this chapter, you will learn about closed source and open source LLMs at a high
    level, privacy issues with these LLMs, and **state-of-the-art** (**SOTA**) research
    in privacy-preserving technologies for LLMs.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型**（**LLMs**）已成为人工智能（AI）领域的一项变革性技术，它使高级自然语言处理（NLP）任务和生成能力成为可能。这些模型，如OpenAI的GPT-3.5和Meta的Llama
    2，在生成类似人类的文本和展示对语言模式深入理解方面表现出卓越的技能。在本章中，你将从高层次上了解封闭源和开源LLMs，以及这些LLMs的隐私问题，以及LLMs隐私保护技术的**最先进**（**SOTA**）研究。'
- en: 'We will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主要主题：
- en: Key concepts/terms used in LLMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs中使用的关键概念/术语
- en: 'Prompt engineering: Sentence translation using ChatGPT (closed source LLM)
    as well as using open source LLMs'
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程：使用ChatGPT（封闭源LLM）以及开源LLM进行句子翻译
- en: Comparison of open source LLMs and closed source LLMs
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源LLM和封闭源LLM的比较
- en: AI standards and terminology of attacks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI标准和攻击术语
- en: '**National Institute of Standards and Technology** (**NIST**) Trustworthy and
    Responsible AI'
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**国家标准与技术研究院**（**NIST**）的可信和负责任的AI'
- en: '**Open Worldwide Application Security Project** (**OWASP**) Top 10 LLMs'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开放式全球应用安全项目**（**OWASP**）的Top 10 LLMs'
- en: Privacy attacks on LLMs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对LLMs的隐私攻击
- en: Real-world incidents of privacy leaks in LLMs
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs中隐私泄露的真实世界事件
- en: Membership inference attacks against generative models
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对生成模型进行的成员推理攻击
- en: Extracting training data from LLMs
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从LLMs中提取训练数据
- en: Prompt injection attacks
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示注入攻击
- en: Privacy-preserving technologies for LLMs
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs的隐私保护技术
- en: Text attacks on **machine learning** (**ML**) and **generative** **AI** (**GenAI**)
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对机器学习（ML）和生成式AI（GenAI）的文本攻击
- en: Training LLMs using differential privacy with private transformer
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用私有transformer进行差分隐私训练LLMs
- en: SOTA research on privacy-preserving LLMs
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私保护LLMs的SOTA研究
- en: Key concepts/terms used in LLMs
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs中使用的关键概念/术语
- en: LLMs are a complex field of NLP, and there are several terms associated with
    them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs是自然语言处理（NLP）的一个复杂领域，与之相关的术语有几个。
- en: 'Some key terms and concepts used in the context of LLMs are the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs的背景下使用的一些关键术语和概念如下：
- en: '**Transformer architecture**: The foundational architecture for most LLMs,
    known for its self-attention mechanism, which allows the model to weigh the importance
    of different words in a sentence.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer架构**：大多数LLMs的基础架构，以其自注意力机制而闻名，这使得模型能够在句子中权衡不同单词的重要性。'
- en: '**Pre-training**: The initial phase in which the LLM is trained on a massive
    corpus of text data from the internet to learn language patterns and context.
    This pre-trained model is often referred to as the “base model.”'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练**：LLM在从互联网上的大量文本数据语料库中训练的初始阶段，以学习语言模式和上下文。这个预训练模型通常被称为“基础模型”。'
- en: '**Fine-tuning**: The subsequent phase where the pre-trained model is adapted
    to perform specific NLP tasks, such as text classification, translation, summarization,
    or question answering. Fine-tuning helps the model specialize in these tasks.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：随后的阶段，预训练模型被调整以执行特定的NLP任务，如文本分类、翻译、摘要或问答。微调有助于模型在这些任务上专业化。'
- en: '**Parameters**: These are the trainable components of the LLM, represented
    by numerical values. The number of parameters is a key factor in determining the
    size and capability of an LLM.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数**：这些是LLM的可训练组件，由数值表示。参数的数量是确定LLM大小和能力的关键因素。'
- en: '**Attention mechanism**: A core component of the transformer architecture,
    it enables the model to focus on different parts of the input sequence when processing
    it, improving contextual understanding.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意力机制**：变压器架构的核心组件，它使模型在处理输入序列时能够关注不同的部分，从而提高上下文理解。'
- en: '**Self-attention**: A specific type of attention where the model assigns weights
    to different words in a sentence based on their relevance to each other, allowing
    it to capture dependencies between words. Most transformers are built based on
    the research paper from Google, *Attention Is All You* *Need* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自注意力**: 一种特定的注意力机制，模型根据句子中不同单词之间的相关性为它们分配权重，使其能够捕捉单词之间的依赖关系。大多数transformer都是基于谷歌的研究论文《Attention
    Is All You Need》构建的（[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))。'
- en: '**Embeddings**: Word embeddings or token embeddings are vector representations
    of words or tokens in a continuous space. These embeddings capture semantic relationships
    between words.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入**: 词嵌入或token嵌入是单词或token在连续空间中的向量表示。这些嵌入捕捉了单词之间的语义关系。'
- en: '**Contextual embeddings**: Unlike static word embeddings, these embeddings
    change based on the context of the sentence, allowing LLMs to understand the meaning
    of words in different contexts. Positional embeddings and rotary position embeddings
    come under the category of contextual embeddings.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文嵌入**: 与静态词嵌入不同，这些嵌入根据句子的上下文而变化，使LLM能够理解不同上下文中单词的含义。位置嵌入和旋转位置嵌入属于上下文嵌入的范畴。'
- en: '**Tokenization**: The process of breaking down text into individual tokens
    (words or subwords) for input into the model. LLMs use tokenizers to perform this
    task.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**: 将文本分解成单个token（单词或子词）以输入到模型中的过程。LLM使用分词器来完成这项任务。'
- en: '**Decoding**: The process of converting model-generated representations (usually
    logits or token IDs) into human-readable text. Decoding is necessary to produce
    the final output.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码**: 将模型生成的表示（通常是logits或token ID）转换为人类可读文本的过程。解码是生成最终输出的必要步骤。'
- en: '**Transfer learning (TL)**: The concept of transferring knowledge gained from
    one task or domain to another. LLMs often benefit from TL, as they are pre-trained
    on a broad range of text before fine-tuning for specific tasks.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迁移学习（TL）**: 将从一个任务或领域获得的知识转移到另一个任务或领域上的概念。LLM通常从TL中受益，因为它们在微调特定任务之前已经在广泛的文本上进行了预训练。'
- en: '**Prompt engineering**: The process of designing input prompts or instructions
    that guide the LLM to generate the desired output. Crafting effective prompts
    is crucial in controlling the model’s behavior:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示工程**: 设计输入提示或指令的过程，以引导LLM生成所需的输出。制作有效的提示对于控制模型的行为至关重要：'
- en: '![Figure 10.1 – Simple prompt flow](img/B16573_10_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 简单提示流程](img/B16573_10_01.jpg)'
- en: Figure 10.1 – Simple prompt flow
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 简单提示流程
- en: '**Zero-shot learning**: A type of TL where a model is asked to perform a task
    for which it was not explicitly fine-tuned. LLMs are capable of zero-shot learning
    to some extent.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零样本学习**: 一种迁移学习类型，其中模型被要求执行它没有明确微调的任务。LLM在一定程度上具有零样本学习能力。'
- en: '**Few-shot learning**: Like zero-shot learning, but the model is provided with
    a limited number of examples for a new task during fine-tuning.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少样本学习**: 与零样本学习类似，但在微调新任务时，模型提供了有限数量的示例。'
- en: '**Chain of Thought (CoT)**: CoT prompting is a technique that guides LLMs to
    follow a reasoning process when dealing with hard problems. This is done by showing
    the model a few examples where the step-by-step reasoning is clearly laid out.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**思维链（CoT）**: CoT提示是一种技术，指导LLM在处理难题时遵循推理过程。这是通过向模型展示一些逐步推理清晰展示的例子来实现的。'
- en: '**Tree of Thoughts (ToT)**: ToT prompting breaks a problem into a sequence
    of smaller steps—or *thoughts*—that are solved individually. This approach does
    not constrain the model to output these steps all at once. Rather, each thought
    is generated or solved independently and passed to the next step for solving the
    problem.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**思维树（ToT）**: ToT提示将问题分解为一系列较小的步骤——或称为“思维”——这些步骤分别解决。这种方法并不限制模型一次性输出所有这些步骤。相反，每个思维都是独立生成或解决的，然后传递给下一个步骤以解决问题。'
- en: '**Graph of Thoughts (GoT)**: It conceptualizes the data generated by an LLM
    as a graph, where each node symbolizes a unit of information, commonly known as
    “LLM thoughts.” The connections between these nodes represent the dependencies
    or associations among distinct units of thought.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**思维图（GoT）**: 将LLM生成的数据概念化为一个图，其中每个节点代表一个信息单元，通常称为“LLM思维”。这些节点之间的连接代表了不同思维单元之间的依赖关系或关联。'
- en: Prompt example using ChatGPT (closed source LLM)
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ChatGPT（闭源LLM）的提示示例
- en: Let’s try an example using ChatGPT ([https://chat.openai.com/](https://chat.openai.com/))
    and ask questions to translate a sentence from English to German.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来试试ChatGPT ([https://chat.openai.com/](https://chat.openai.com/))，并提问将一句话从英语翻译成德语。
- en: 'In this case, the question is called a prompt, and the response from ChatGPT
    (LLM) is called a completion/result:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，问题被称为提示，ChatGPT（LLM）的响应被称为完成/结果：
- en: '![Figure 10.2 – Simple prompt request and completion](img/B16573_10_02.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – 简单的提示请求和完成](img/B16573_10_02.jpg)'
- en: Figure 10.2 – Simple prompt request and completion
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 简单的提示请求和完成
- en: Prompt example using open source LLMs
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用开源LLM的提示示例
- en: Let’s try an example using open source LLMs programmatically using Python.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用Python程序化地使用开源LLM的例子。
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Ensure that you have the Hugging Face Transformers library installed and that
    you have access to the `"google/flan-t5-large"` model for this code to run successfully.
    Follow the next steps to implement this example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你已经安装了Hugging Face Transformers库，并且你有权限使用`"google/flan-t5-large"`模型，以便此代码能够成功运行。按照以下步骤实现此示例：
- en: 'Install the library using the following command:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装库：
- en: '[PRE1]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Additionally, you need to download the model using **transformers.AutoModel.from_pretrained("google/flan-t5-large")**
    if you haven’t already.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，如果你还没有下载模型，你需要使用**transformers.AutoModel.from_pretrained("google/flan-t5-large")**来下载模型。
- en: Import the necessary classes from the Transformers library, namely **T5Tokenizer**
    and **T5ForConditionalGeneration**.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Transformers库中导入必要的类，即**T5Tokenizer**和**T5ForConditionalGeneration**。
- en: Initialize the T5 tokenizer and model with the pre-trained **"google/flan-t5-large"**
    model. This model is designed for translation tasks.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的**"google/flan-t5-large"**模型初始化T5分词器和模型。此模型是为翻译任务设计的。
- en: 'Define the input text you want to translate from English to German, which is
    **"translate English to German: How old** **are you?"**.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '定义你想要从英语翻译成德语的输入文本，即**"translate English to German: How old are you?"**。'
- en: Tokenize the input text using the tokenizer, and convert it into PyTorch tensors.
    This step prepares the text for input to the model.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分词器对输入文本进行分词，并将其转换为PyTorch张量。这一步为将文本输入到模型中做准备。
- en: Generate the translation using the T5 model by passing the tokenized input to
    the model’s **generate** method. The translation output is stored in the **outputs**
    variable.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将分词后的输入传递给模型的**generate**方法，使用T5模型生成翻译。翻译输出存储在**outputs**变量中。
- en: Decode the generated output using the tokenizer’s **decode** method, and print
    the translated text, which will be the German translation of the input text.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分词器的**decode**方法解码生成的输出，并打印翻译后的文本，这将是对输入文本的德语翻译。
- en: '*Source code: Ex_LLM_Opensource.ipynb*The following is a detailed source code
    of the example:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*源代码：Ex_LLM_Opensource.ipynb*以下是一个示例的详细源代码：'
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Figure 10.3 – T5 model weights, tokenizer and config downloads](img/B16573_10_03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – T5模型权重、分词器和配置下载](img/B16573_10_03.jpg)'
- en: Figure 10.3 – T5 model weights, tokenizer and config downloads
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – T5模型权重、分词器和配置下载
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This results in the following output
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Comparison of open source LLMs and closed source LLMs
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源LLM和闭源LLM的比较
- en: Open source and closed source LLMs represent two different approaches to the
    development and availability of LLMs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 开源和闭源LLM代表了LLM开发和可用性的两种不同方法。
- en: Open source LLMs
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开源LLM
- en: 'Let’s look at some of the attributes of open source LLMs:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些开源LLM的属性：
- en: '**Accessibility**: Open source LLMs are publicly accessible, and their architecture
    and parameters can be examined, modified, and shared by the community. This transparency
    fosters collaboration and innovation.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可访问性**：开源LLM是公开可访问的，它们的架构和参数可以被社区检查、修改和共享。这种透明度促进了合作和创新。'
- en: '**Community contributions**: They often benefit from contributions and enhancements
    from a diverse community of researchers and developers, leading to rapid improvements
    and addressing potential biases.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社区贡献**：它们经常从来自不同研究者和开发者的贡献和改进中受益，从而实现快速改进并解决潜在的偏差。'
- en: '**Customization**: Users have the freedom to fine-tune and adapt open source
    LLMs for specific tasks, languages, or domains, making them highly flexible and
    versatile.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制**：用户有自由对开源LLM进行微调和适应特定任务、语言或领域，使它们高度灵活和多功能。'
- en: '**Cost-efficiency**: Typically, open source LLMs are free to use, which can
    be particularly advantageous for researchers, start-ups, and developers.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：通常，开源大型语言模型是免费使用的，这对于研究人员、初创公司和开发者来说特别有利。'
- en: '**Hardware infrastructure**: Open source models need to be hosted on GPUs for
    inferencing, and associated costs need to be owned.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件基础设施**：开源模型需要托管在GPU上进行推理，并且相关的成本需要自行承担。'
- en: '**Security**: Open source LLMs may have security vulnerabilities or the underlying
    software versions, so addressing these security vulnerabilities (**Common Vulnerabilities
    and Exposures**, or **CVEs**) needs to be managed on its own.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全**：开源大型语言模型可能存在安全漏洞或底层软件版本问题，因此需要单独管理这些安全漏洞（通用漏洞和暴露，或CVE）。'
- en: '*Examples*: Google’s FLAN-T5, Meta’s Llama models, GPT-3, and Hugging Face
    transformers are open source and widely accessible.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例*：Google的FLAN-T5、Meta的Llama模型、GPT-3以及Hugging Face的转换器都是开源的，并且广泛可访问。'
- en: Closed source LLMs
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 闭源大型语言模型
- en: 'Now, let’s turn our attention to the attributes of closed source LLMs:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向闭源大型语言模型的属性：
- en: '**Proprietary**: Closed source LLMs are developed and owned by organizations
    or companies, and their architecture and parameters are not publicly disclosed'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专有**：闭源大型语言模型由组织或公司开发和拥有，其架构和参数不公开披露。'
- en: '**Control**: Developers of closed source LLMs retain control over their models,
    algorithms, and **intellectual property** (**IP**), allowing them to protect their
    innovations'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制**：闭源大型语言模型的开发者保留对其模型、算法和**知识产权**（IP）的控制权，使他们能够保护其创新。'
- en: '**Limited customization**: Users of closed source LLMs may have limited options
    for fine-tuning or adapting the model to specific needs, as the source code is
    not openly available'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的定制**：闭源大型语言模型的用户可能对微调或调整模型以满足特定需求的选择有限，因为源代码不可公开获取。'
- en: '**Costs**: Closed source LLMs often come with licensing fees or usage costs,
    which can be a significant factor for some users or organizations'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：闭源大型语言模型通常附带许可费或使用费，这可能对某些用户或组织是一个重要因素。'
- en: '**Hardware infrastructure**: Closed source models are deployed in GPUs by the
    vendors, and they provide the APIs to access either through REST or gRPC, so infra
    costs are owned by the providers (in the case of GPT-4 or GPT-3.x, OpenAI and
    Microsoft will own the hosted versions)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件基础设施**：闭源模型由供应商在GPU上部署，并提供通过REST或gRPC访问的API，因此基础设施成本由供应商承担（在GPT-4或GPT-3.x的情况下，OpenAI和微软将拥有托管版本）。'
- en: '**Security**: Closed source LLMs may have security vulnerabilities in the underlying
    software versions, so LLM providers will address these security vulnerabilities
    (CVEs), and it is a black box for the users who make use of these.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全**：闭源大型语言模型可能在底层软件版本中存在安全漏洞，因此大型语言模型提供商将解决这些安全漏洞（CVE），而对于使用这些模型的用户来说，这是一个黑盒。'
- en: '*Examples*: Commercial language models such as GPT-3.5 or GPT-4 models and
    proprietary models used by tech companies may be closed source.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例*：商业语言模型如GPT-3.5或GPT-4模型以及科技公司使用的专有模型可能是闭源的。'
- en: The choice between open source and closed source LLMs depends on factors such
    as budget, data privacy concerns, customization needs, and the level of control
    required.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 开源和闭源大型语言模型之间的选择取决于预算、数据隐私担忧、定制需求以及所需的控制水平。
- en: Open source LLMs offer accessibility, collaboration, and cost savings but may
    require more technical expertise for customization. Closed source LLMs provide
    IP protection and may come with specialized support and features, but at the cost
    of limited transparency and potential licensing fees.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 开源大型语言模型提供可访问性、协作和成本节约，但可能需要更多的技术专业知识进行定制。闭源大型语言模型提供知识产权保护，可能附带专业支持和功能，但代价是有限的透明度和可能的许可费。
- en: Organizations and developers should carefully consider their specific requirements
    when choosing between these two approaches.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 组织和开发者在选择这两种方法之间应仔细考虑其具体需求。
- en: AI standards and terminology of attacks
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能标准和攻击术语
- en: In the following section, we will go through some AI standards and terminology
    of attacks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨一些人工智能标准和攻击术语。
- en: NIST
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NIST
- en: '*NIST Trustworthy and Responsible AI* released a paper on taxonomy and terminologies
    used in AI with respect to attacks and mitigations. It covers both predictive
    AI (traditional ML) and GenAI.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*NIST可信和负责任的人工智能*发布了一篇关于攻击和缓解中使用的分类和术语的论文。它涵盖了预测性人工智能（传统机器学习）和生成人工智能。'
- en: '![Figure 10.4 – Taxonomy of attacks on Generative AI systems](img/B16573_10_04.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – 对生成式AI系统攻击的分类](img/B16573_10_04.jpg)'
- en: Figure 10.4 – Taxonomy of attacks on Generative AI systems
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 对生成式AI系统攻击的分类
- en: '*Image source: “*Adversarial Machine Learning: A Taxonomy and Terminology of
    Attacks and Mitigations *” paper from* *NIST.* [https://doi.org/10.6028/NIST.AI.100-2e2023](https://doi.org/10.6028/NIST.AI.100-2e2023)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源：“*对抗性机器学习：攻击和缓解的分类与术语*”论文，来自*NIST.* [https://doi.org/10.6028/NIST.AI.100-2e2023](https://doi.org/10.6028/NIST.AI.100-2e2023)'
- en: OWASP Top 10 for LLM applications
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OWASP Top 10 for LLM applications
- en: The *OWASP Top 10 for Large Language Model Applications* project aims to educate
    developers, designers, architects, managers, and organizations about the potential
    security risks when deploying and managing LLMs. The OWASP Top 10 for LLM applications
    are as follows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*OWASP Top 10 for Large Language Model Applications* 项目旨在教育开发者、设计师、架构师、经理和组织了解在部署和管理LLMs时可能存在的安全风险。LLM应用的OWASP
    Top 10如下。'
- en: '*LLM01:* *Prompt Injection*'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM01：* *提示注入*'
- en: '*LLM02: Insecure* *Output Handling*'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM02：不安全的* *输出处理*'
- en: '*LLM03: Training* *Data Poisoning*'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM03：训练* *数据中毒*'
- en: '*LLM04: Model Denial* *of Service*'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM04：模型拒绝* *服务*'
- en: '*LLM05: Supply* *Chain Vulnerabilities*'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM05：供应链漏洞*'
- en: '*LLM06: Sensitive* *Information Disclosure*'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM06：敏感* *信息泄露*'
- en: '*LLM07: Insecure* *Plugin Design*'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM07：不安全的* *插件设计*'
- en: '*LLM08:* *Excessive Agency*'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM08：过度的* *代理权*'
- en: '*LLM09: Overreliance*'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM09：过度依赖*'
- en: '*LLM10:* *Model Theft*'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM10：模型盗窃*'
- en: The detailed vulnerabilities and how to detect each vulnerability and possible
    solutions are documented at [https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 详细的安全漏洞、如何检测每个漏洞以及可能的解决方案已在[https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf)中记录。
- en: In the next section, we will cover in more detail privacy attacks on LLMs/GenAI.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更详细地介绍LLMs/GenAI的隐私攻击。
- en: Privacy attacks on LLMs
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs的隐私攻击
- en: In recent years, LLMs have revolutionized **natural language understanding**
    (**NLU**) and **natural language generation** (**NLG**), powering a wide range
    of applications from chatbots and virtual assistants to content recommendation
    systems and language translation services. However, the rapid advancement of these
    models has raised significant concerns about privacy and security. LLM applications
    have the potential to expose sensitive data, proprietary algorithms, or other
    confidential information through their output. This could lead to unauthorized
    access to sensitive data, IP, privacy infringements, and other security violations.
    As LLMs become increasingly prevalent in our digital landscape, there is a growing
    need for effective strategies to protect sensitive information and uphold user
    privacy.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，LLMs已经彻底改变了**自然语言理解**（NLU）和**自然语言生成**（NLG），为从聊天机器人、虚拟助手到内容推荐系统和语言翻译服务等各种应用提供了动力。然而，这些模型快速发展的同时也引发了关于隐私和安全的重大担忧。LLM应用有可能通过其输出暴露敏感数据、专有算法或其他机密信息。这可能导致对敏感数据、知识产权、隐私侵犯和其他安全违规行为的未授权访问。随着LLMs在我们数字景观中的日益普及，迫切需要有效的策略来保护敏感信息和维护用户隐私。
- en: As discussed in the earlier chapters, ML models are susceptible to privacy attacks,
    and there’s no exception for GenAI models (LLMs) either.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，机器学习模型容易受到隐私攻击，生成式AI模型（LLMs）也不例外。
- en: 'The following two recent articles provide details of privacy issues in enterprises
    with respect to GenAI:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两篇最近的文章提供了关于企业中GenAI隐私问题的详细信息：
- en: '**Cyberhaven’s survey**: As per the article from Cyberhaven ([https://www.cyberhaven.com/blog/4-2-of-workers-have-pasted-company-data-into-chatgpt/](https://www.cyberhaven.com/blog/4-2-of-workers-have-pasted-company-data-into-chatgpt/)),
    the potential risks of data leaks when employees paste company data into chatbots
    such as OpenAI’s GPT-3\. The company conducted a survey of 2,000 workers in the
    US and the UK and found that 4.2% of them had pasted company data into chatbots.
    While chatbots such as GPT-3 are designed to forget information after the conversation
    ends, the risk lies in the fact that these chatbots could potentially remember
    and replicate sensitive information during the conversation. The article also
    mentions that if a hacker gains control of the chatbot during the conversation,
    they could access sensitive data. The article emphasizes the need for companies
    to have clear policies about what data can be shared with chatbots and to educate
    employees about potential risks. It also suggests that companies should implement
    **data loss prevention** (**DLP**) solutions to automatically block sensitive
    data from being shared with chatbots. It concludes by stating that while AI chatbots
    have many benefits, companies need to be aware of potential security and privacy
    risks and take appropriate measures to protect sensitive data.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**Cyberhaven的调查**：根据Cyberhaven的文章([https://www.cyberhaven.com/blog/4-2-of-workers-have-pasted-company-data-into-ChatGPT/](https://www.cyberhaven.com/blog/4-2-of-workers-have-pasted-company-data-into-ChatGPT/))，当员工将公司数据粘贴到聊天机器人（如OpenAI的GPT-3）时，数据泄露的潜在风险。该公司对美国和英国的2,000名员工进行了调查，发现其中4.2%的人将公司数据粘贴到了聊天机器人中。尽管像GPT-3这样的聊天机器人在对话结束后会忘记信息，但风险在于这些聊天机器人在对话过程中可能会记住并复制敏感信息。文章还提到，如果黑客在对话期间控制了聊天机器人，他们可以访问敏感数据。文章强调，公司需要制定明确的数据共享政策，并教育员工关于潜在风险。它还建议公司实施**数据丢失预防**（**DLP**）解决方案，以自动阻止敏感数据与聊天机器人共享。文章最后指出，尽管AI聊天机器人有许多好处，但公司需要意识到潜在的安全和隐私风险，并采取适当的措施来保护敏感数据。'
- en: '**Samsung’s IP leak**: As per the article at [https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/](https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/),
    Samsung employees unintentionally disclosed confidential information while using
    ChatGPT for work-related tasks, highlighting potential privacy and security risks.
    Samsung’s semiconductor division permitted engineers to employ ChatGPT for source
    code checks and other duties. However, *The Economist* in Korea reported three
    separate incidents where sensitive data was inadvertently exposed to ChatGPT.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**三星的知识产权泄露**：根据[https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-ChatGPT-after-april-internal-data-leak/](https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-ChatGPT-after-april-internal-data-leak/)上的文章，三星员工在用ChatGPT处理工作相关任务时无意中泄露了机密信息，突显了潜在的隐私和安全风险。三星半导体部门允许工程师使用ChatGPT进行源代码检查和其他任务。然而，韩国的《经济学人》报道了三起敏感数据意外暴露给ChatGPT的事件。'
- en: In one incident, an employee copied confidential source code into a chat to
    identify errors. Another employee shared code and requested optimization. A third
    employee shared a meeting recording for transcription into presentation notes.
    This data is now accessible to ChatGPT.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个事件中，一名员工将机密源代码复制到聊天中，以识别错误。另一名员工分享了代码并请求优化。第三名员工分享了会议录音，以便转录成演示文稿笔记。这些数据现在可以通过ChatGPT访问。
- en: Samsung has responded promptly by limiting ChatGPT’s upload capacity to 1,024
    bytes per user and initiating investigations into those responsible for the data
    breaches. Moreover, Samsung is considering developing an in-house AI chatbot to
    bolster data security and privacy going forward. However, it’s improbable that
    Samsung can retrieve the leaked data due to ChatGPT’s data policy, which employs
    data for model training unless users explicitly opt out. The ChatGPT usage guide
    explicitly warns against sharing sensitive information during conversations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 三星公司迅速作出反应，通过限制每个用户上传容量为1,024字节并启动对数据泄露负责人员的调查来应对ChatGPT的问题。此外，三星正在考虑开发内部AI聊天机器人，以加强未来的数据安全和隐私保护。然而，由于ChatGPT的数据政策，除非用户明确退出，否则将用于模型训练，因此三星不太可能恢复泄露的数据。ChatGPT使用指南明确警告在对话中不要分享敏感信息。
- en: These incidents illustrate real-world scenarios that privacy experts have long
    been wary of, such as sharing confidential legal or medical documents for text
    analysis or summarization, which could be utilized to refine the model. Privacy
    experts caution that this may potentially contravene **General Data Protection
    Regulation** (**GDPR**) compliance, resulting in regulatory consequences.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这些事件说明了隐私专家长期以来一直警惕的现实世界场景，例如为了文本分析或摘要而共享机密法律或医疗文件，这些文件可能被用来改进模型。隐私专家警告，这可能会违反**通用数据保护条例**（**GDPR**）的合规性，从而导致监管后果。
- en: Membership inference attacks against generative models
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对生成模型的成员身份推断攻击
- en: 'We learned about membership inference attacks on ML models in [*Chapter 4*](B16573_04.xhtml#_idTextAnchor079).
    GenAI models also are susceptible to membership inference attacks along a similar
    line:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第4章*](B16573_04.xhtml#_idTextAnchor079)中学习了关于机器学习模型上的成员身份推断攻击。生成AI模型也容易受到类似成员身份推断攻击的影响：
- en: Generative models aim to estimate the fundamental distribution of a dataset,
    enabling the creation of lifelike samples based on that distribution.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型旨在估计数据集的基本分布，从而可以根据该分布创建逼真的样本。
- en: When presented with a data point, the adversary discerns whether it was utilized
    in training the model.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当面对一个数据点时，攻击者会判断它是否被用于训练模型。
- en: These attacks are based on both white-box and black-box access to the target
    model, against several SOTA generative models
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些攻击基于对目标模型的白盒和黑盒访问，针对多个SOTA生成模型
- en: Let’s go through an example.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来了解。
- en: 'This example provides a basic membership inference attack against a generative
    model using PyTorch. The attack aims to determine if a specific data point was
    part of the generative model’s training dataset. It includes the following components:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例提供了一个使用PyTorch对生成模型进行基本成员身份推断攻击的例子。攻击旨在确定特定数据点是否是生成模型训练数据集的一部分。它包括以下组件：
- en: 'Sample GenAI model using a **variational** **autoencoder** (**VAE**):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**变分****自动编码器**（**VAE**）的样本生成AI模型：
- en: '**VAE**: A simple VAE is used as the generative model. The VAE is capable of
    encoding and decoding binary data points.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VAE**：使用一个简单的VAE作为生成模型。VAE能够编码和解码二进制数据点。'
- en: '**Adversary model**: An adversary model is implemented as a two-layer'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**攻击模型**：攻击模型被实现为两层'
- en: '**feedforward neural network** (**FNN**): This model is trained to predict
    whether a given data point was a member of the training dataset.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈神经网络**（**FNN**）：此模型经过训练，用于预测给定数据点是否是训练数据集的成员。'
- en: '**Synthetic data**: Synthetic binary data is generated for demonstration purposes.
    In practice, you would replace this with your actual dataset.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合成数据**：为了演示目的，生成了合成二进制数据。在实际应用中，您应将其替换为您的实际数据集。'
- en: '**Training process**: The VAE and the adversary model are trained independently.
    The VAE learns to encode and decode data, while the adversary model learns to
    predict membership.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练过程**：VAE和攻击模型独立训练。VAE学习编码和解码数据，而攻击模型学习预测成员身份。'
- en: '**Membership inference attack**: The membership inference attack function takes
    a target data point, encodes it using the VAE, and then uses the adversary model
    to predict whether the target data point is a member or non-member of the training
    dataset.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成员身份推断攻击**：成员身份推断攻击函数接受一个目标数据点，使用VAE对其进行编码，然后使用攻击模型预测目标数据点是否是训练数据集的成员或非成员。'
- en: 'Source code components:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码组件：
- en: '**SampleGenModel class**: Defines the architecture of the VAE'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SampleGenModel类**：定义VAE的架构'
- en: '**Adversary class**: Defines the architecture of the adversary model'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**攻击类**：定义攻击模型的架构'
- en: '**Data generation**: Generates synthetic binary data for training and testing'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据生成**：生成用于训练和测试的合成二进制数据'
- en: '**Training**: Training loops for the VAE-based **SampleGenModel** class and
    the adversary model'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**：基于VAE的**SampleGenModel**类和攻击模型的训练循环'
- en: '**Membership inference attack**: The function for conducting the membership
    inference attack'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成员身份推断攻击**：进行成员身份推断攻击的函数'
- en: '**Main execution**: Initializes the VAE and the adversary model and performs
    an attack on a target data point'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主要执行**：初始化VAE和攻击模型，并对目标数据点进行攻击'
- en: '*Source* *code: MemberShipInference_LLM.ipynb*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*源代码*：MemberShipInference_LLM.ipynb'
- en: '[PRE5]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This results in the following output:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Membership inference attacks are more complex in practice, and this code serves
    as a basic demonstration. Implement privacy and security measures when deploying
    generative models to protect against such attacks. We will cover in detail how
    to protect GenAI models in a privacy-preserving manner in the next section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 成员身份推断攻击在实践中更为复杂，此代码仅作为基本演示。在部署生成模型时，请实施隐私和安全措施以防止此类攻击。我们将在下一节详细介绍如何以隐私保护的方式保护GenAI模型。
- en: Extracting training data attack from generative models
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从生成模型中提取训练数据攻击
- en: Extracting training data from LLMs can be a challenging task because the training
    data is not typically available directly from the model. Instead, LLMs are pre-trained
    on vast datasets from the internet. If we have a specific LLM in mind and want
    to extract training data related to it, we may need access to the original data
    sources used for pre-training, which may not be publicly available.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从LLM（大型语言模型）中提取训练数据可能是一项具有挑战性的任务，因为训练数据通常无法直接从模型中获取。相反，LLM是在互联网上的大量数据集上预训练的。如果我们有特定的LLM并希望提取与其相关的训练数据，我们可能需要访问用于预训练的原始数据源，这些数据源可能并非公开可用。
- en: 'Here’s a sample Python code snippet that demonstrates how we can extract text
    data from a pre-trained Hugging Face Transformers model, such as GPT-2\. Keep
    in mind that this code is for illustrative purposes and won’t retrieve the actual
    training data but rather generates text samples from the model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个Python代码片段示例，展示了我们如何从预训练的Hugging Face Transformers模型（如GPT-2）中提取文本数据。请注意，此代码仅用于说明目的，不会检索实际训练数据，而是从模型中生成文本样本：
- en: 'In this code, we do the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们执行以下操作：
- en: We load a pre-trained GPT-2 model and tokenizer from the Hugging Face Transformers
    library. You can choose other models based on your requirements.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从Hugging Face Transformers库中加载了一个预训练的GPT-2模型和分词器。您可以根据需求选择其他模型。
- en: We define a prompt, which serves as the starting point for generating text.
    You can change the prompt to suit your needs.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义一个提示，它作为生成文本的起点。您可以根据需要更改提示。
- en: We specify the number of text samples (**num_samples**) to generate from the
    model.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们指定从模型中生成文本样本的数量（**num_samples**）。
- en: Inside the loop, we encode the prompt using the tokenizer and generate text
    sequences using the model. We decode the output to obtain human-readable text.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在循环内部，我们使用分词器对提示进行编码，并使用模型生成文本序列。我们将输出解码以获得可读性强的文本。
- en: Please note that the generated text is not actual training data used for the
    model but rather synthetic text produced by the model based on the provided prompt.
    To access the actual training data used to train LLMs, you would need access to
    the original data sources, which are typically large and diverse web corpora.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成的文本不是用于模型的实际训练数据，而是基于提供的提示由模型产生的合成文本。要访问用于训练LLM的实际训练数据，您需要访问原始数据源，这些数据源通常是大型且多样化的网络语料库。
- en: '*Source* *code: Training_Data_Extraction_Gen_AI.ipynb*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*源代码*：`Training_Data_Extraction_Gen_AI.ipynb`'
- en: '[PRE7]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 10.5 – GPT2 model weights, tokenizer and config downloads](img/B16573_10_05.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图10.5 – GPT2模型权重、分词器和配置下载](img/B16573_10_05.jpg)'
- en: Figure 10.5 – GPT2 model weights, tokenizer and config downloads
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – GPT2模型权重、分词器和配置下载
- en: 'Researchers from Google, Apple, OpenAI, Harvard, UC Berkeley, Northeastern
    University and Stanford demonstrated an attack on GPT-2, a language model trained
    on scrapes of the public internet, and were able to extract hundreds of verbatim
    text sequences from the model’s training data. These extracted examples included
    (public) **personally identifiable information** or **PII** (names, phone numbers,
    and email addresses): [https://arxiv.org/pdf/2012.07805.pdf](https://arxiv.org/pdf/2012.07805.pdf).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 来自谷歌、苹果、OpenAI、哈佛大学、加州大学伯克利分校、东北大学和斯坦福大学的研究人员演示了对GPT-2的攻击，GPT-2是一个基于公共互联网抓取数据的语言模型，他们能够从模型的训练数据中提取出数百个逐字逐句的文本序列。这些提取的示例包括（公开的）**个人可识别信息**或**PII**（姓名、电话号码和电子邮件地址）：[https://arxiv.org/pdf/2012.07805.pdf](https://arxiv.org/pdf/2012.07805.pdf)。
- en: Prompt injection attacks
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示注入攻击
- en: A prompt injection attack, also known as data or command injection, is a type
    of security vulnerability that happens when an attacker can influence the prompts
    or commands sent to a data processing system such as an LLM. These attacks potentially
    allow attackers to manipulate the actions of the system or extract sensitive or
    private data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入攻击，也称为数据或命令注入，是一种安全漏洞，当攻击者可以影响发送到数据处理系统（如LLM）的提示或命令时发生。这些攻击可能允许攻击者操纵系统的行为或提取敏感或私人数据。
- en: In the context of an LLM, prompt injection attacks could involve an attacker
    providing a crafted input designed to trick the model into providing information
    it has been trained on, which could potentially include sensitive or confidential
    information if the training data was not properly anonymized or scrubbed. Moreover,
    an attacker could inject malicious prompts to make the model produce outputs that
    inflict harm, such as generating offensive, defamatory, or illegal content. This
    could be used for spear phishing, spreading disinformation, defaming individuals
    or entities, and many other nefarious purposes.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的背景下，提示注入攻击可能涉及攻击者提供一个精心设计的输入，以欺骗模型提供其训练过的信息，如果训练数据没有正确匿名化或清理，这可能包括敏感或机密信息。此外，攻击者可以注入恶意提示，使模型产生有害的输出，例如生成攻击性、诽谤性或非法内容。这可以用于针对性钓鱼、散布虚假信息、诽谤个人或实体，以及许多其他恶意目的。
- en: 'LangChain ([https://www.langchain.com/](https://www.langchain.com/)) is one
    of the open source frameworks that provides tools to build LLM applications. In
    August 2023, the NVIDIA AI Red Team identified three vulnerabilities in LangChain
    through prompt injection; they are listed as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain ([https://www.langchain.com/](https://www.langchain.com/)) 是提供构建LLM应用程序工具的开源框架之一。2023年8月，NVIDIA
    AI Red Team在LangChain中发现了三个通过提示注入的漏洞；它们如下列出：
- en: '*CVE-2023-29374*: In LangChain through 0.0.131, the **LLMMathChain** chain
    allows prompt injection attacks that can execute arbitrary code via the Python
    **exec** method'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CVE-2023-29374*: 在LangChain 0.0.131版本中，**LLMMathChain**链允许通过Python **exec**方法执行任意代码的提示注入攻击。'
- en: '*CVE-2023-32786*: In Langchain through 0.0.155, prompt injection allows an
    attacker to force the service to retrieve data from an arbitrary URL, essentially
    providing **server-side request forgery** (**SSRF**) and potentially injecting
    content into downstream tasks'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CVE-2023-32786*: 在Langchain 0.0.155版本中，提示注入允许攻击者强制服务从任意URL检索数据，本质上提供了**服务器端请求伪造**（**SSRF**）并可能将内容注入到下游任务中。'
- en: '*CVE-2023-36189*: SQL injection vulnerability in LangChain before v0.0.247
    allows a remote attacker to obtain sensitive information via the **SQLDatabaseChain**
    component'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CVE-2023-36189*: 在LangChain v0.0.247之前的SQL注入漏洞允许远程攻击者通过**SQLDatabaseChain**组件获取敏感信息。'
- en: Currently, the extent to which LLMs are vulnerable to these attacks isn’t fully
    understood. It’s also worth mentioning that these models are designed not to directly
    recall any specifics about their training data, including documents or sources
    they were trained on, and they generally don’t have the ability to access or retrieve
    personal data unless they’ve been explicitly programmed to do so, or they’ve been
    trained on data that contains sensitive personal information. Nonetheless, it’s
    always crucial to approach the use of LLMs, or any AI system, with robust security
    measures and an understanding of potential risks.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，LLM对这些攻击的受影响程度尚未完全了解。还值得一提的是，这些模型的设计目的是不直接回忆任何关于其训练数据的详细信息，包括它们训练过的文档或来源，并且它们通常没有访问或检索个人数据的能力，除非它们被明确编程这样做，或者它们是在包含敏感个人信息的训练数据上训练的。尽管如此，始终以强大的安全措施和对潜在风险的了解来使用LLM或任何AI系统，这一点至关重要。
- en: '*Example: PromptInjection.ipynb*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例: PromptInjection.ipynb*'
- en: '[PRE8]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`This results in the` `following output.`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`这导致以下输出。`'
- en: '[PRE9]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Privacy-preserving technologies for LLMs
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的隐私保护技术
- en: Differential privacy is one of the privacy-preserving technologies that can
    be used for LLMs as well.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私是可用于LLM的隐私保护技术之一。
- en: Text attacks on ML models and LLMs
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对ML模型和LLM的文本攻击
- en: TextAttack stands as a Python framework designed for conducting adversarial
    attacks, adversarial training, and data augmentation within the field of NLP.
    This versatile tool streamlines the process of exploring NLP model robustness,
    offering a seamless, rapid, and user-friendly experience. Furthermore, it proves
    invaluable for NLP model training, adversarial training, and data augmentation
    purposes. TextAttack offers various components tailored for typical NLP tasks,
    including sentence encoding, grammar checking, and word replacement, which can
    also be utilized independently.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: TextAttack是一个Python框架，旨在在NLP领域进行对抗攻击、对抗训练和数据增强。这个多功能的工具简化了探索NLP模型鲁棒性的过程，提供了一个无缝、快速且用户友好的体验。此外，它在NLP模型训练、对抗训练和数据增强方面非常有价值。TextAttack提供了针对典型NLP任务的各个组件，包括句子编码、语法检查和词替换，这些也可以独立使用。
- en: 'Instructions on how to install the TextAttack package can be found at this
    GitHub URL: [https://github.com/QData/TextAttack](https://github.com/QData/TextAttack).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如何安装TextAttack包的说明可以在以下GitHub URL找到：[https://github.com/QData/TextAttack](https://github.com/QData/TextAttack)。
- en: 'Install TextAttack framework using `pip install` in the following way:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下方式通过`pip install`安装TextAttack框架：
- en: '[PRE10]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: TextAttack provides various recipes to attack on NLP modules. The following
    example utilizes various libraries and components to perform adversarial attacks
    on NLP models using the TextAttack framework.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: TextAttack提供了各种配方来攻击NLP模块。以下示例利用各种库和组件，使用TextAttack框架对NLP模型进行对抗攻击。
- en: 'Here are the high-level steps in the implementation of this example:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实现此示例的高级步骤：
- en: '**Importing libraries**: Import the necessary libraries, including **transformers**
    from Hugging Face, **torch** for PyTorch, **math**, **textattack**, and **random**.'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入库**: 导入必要的库，包括来自Hugging Face的**transformers**，PyTorch的**torch**，**math**，**textattack**和**random**。'
- en: '**Environment setup**: It sets the **CUDA_VISIBLE_DEVICES** environment variable
    to an empty string, essentially disabling GPU usage. It specifies the device to
    be used as **"cpu"** for PyTorch operations.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**环境设置**: 将**CUDA_VISIBLE_DEVICES**环境变量设置为空字符串，实际上禁用了GPU的使用。它指定用于PyTorch操作的设备为**"cpu"**。'
- en: '**Model definition**: Defines a custom PyTorch model called **Model**. This
    model uses **Bidirectional Encoder Representations from Transformers** (**BERT**)
    for NLP tasks. The model loads the pre-trained **''bert-base-uncased''** BERT
    model from Hugging Face’s Transformers library. It includes a dropout layer and
    a linear layer for classification.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型定义**: 定义一个名为**Model**的自定义PyTorch模型。该模型使用**Bidirectional Encoder Representations
    from Transformers**（**BERT**）进行NLP任务。模型从Hugging Face的Transformers库中加载预训练的**''bert-base-uncased''**
    BERT模型。它包括一个dropout层和一个线性层用于分类。'
- en: 'Initialization of model and tokenization:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型和分词的初始化：
- en: '**Model initialization**: An instance of the **Model** class is created and
    moved to the CPU for evaluation. The model is set to evaluation mode using **model.eval()**.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型初始化**: 创建**Model**类的一个实例，并将其移动到CPU上进行评估。使用**model.eval()**将模型设置为评估模式。'
- en: '**Tokenizer initialization**: Initializes a BERT tokenizer (**BertTokenizer**)
    for tokenizing text.'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词器初始化**: 初始化一个BERT分词器（**BertTokenizer**）用于对文本进行分词。'
- en: '**Custom model wrapper**: Defines a custom model wrapper class called **CustomWrapper**
    that wraps the PyTorch model. This wrapper allows the model to be used with the
    **TextAttack** library.'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自定义模型包装器**: 定义一个名为**CustomWrapper**的自定义模型包装器类，该类包装了PyTorch模型。这个包装器允许模型与**TextAttack**库一起使用。'
- en: 'Utilizes the TextAttack library to build an attack using the **TextFoolerJin2019**
    recipe. The **CustomWrapper** instance is passed to the attack:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TextAttack库，通过**TextFoolerJin2019**配方构建攻击。将**CustomWrapper**实例传递给攻击：
- en: '**Dataset**: Defines a list called **dataset**, containing text samples and
    corresponding labels. These samples are examples of performing adversarial attacks.'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**: 定义一个名为**dataset**的列表，包含文本样本和相应的标签。这些样本是执行对抗攻击的示例。'
- en: '**Attack execution**: Creates an **Attacker** instance, specifying the attack,
    dataset, and other attack parameters. Finally, the **attack_dataset()** method
    is called on the attacker to perform adversarial attacks on the dataset.'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**攻击执行**: 创建一个**Attacker**实例，指定攻击、数据集和其他攻击参数。最后，在攻击者上调用**attack_dataset()**方法对数据集进行对抗攻击。'
- en: Overall, this code sets up a PyTorch model, initializes an attack using the
    TextAttack library, and then applies this attack to a dataset of text samples
    for the purpose of evaluating the robustness of the NLP model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，此代码设置了一个 PyTorch 模型，使用 TextAttack 库初始化攻击，然后将此攻击应用于文本样本数据集，以评估 NLP 模型的鲁棒性。
- en: '*Source* *code: Privacy_attacks_LLMs.ipynb*'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*源代码*：Privacy_attacks_LLMs.ipynb'
- en: '[PRE11]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This results in the following output:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE12]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In a similar way, the GPT-2 model also can be explored for NLP attacks (for
    the complete source code, refer to the GitHub repo at [https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter10/Privacy_attacks_LLMs.ipynb](https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter10/Privacy_attacks_LLMs.ipynb)):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，GPT-2 模型也可以用于 NLP 攻击（对于完整的源代码，请参阅 GitHub 仓库 [https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter10/Privacy_attacks_LLMs.ipynb](https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter10/Privacy_attacks_LLMs.ipynb)）：
- en: '[PRE13]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`attacker.attack_dataset()`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`attacker.attack_dataset()`'
- en: 'This results in the following output:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE14]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Private transformers – training LLMs using differential privacy
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 私有变压器 – 使用差分隐私训练 LLMs
- en: The complete source code for this section can be found at [https://github.com/lxuechen/private-transformers](https://github.com/lxuechen/private-transformers).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的完整源代码可在 [https://github.com/lxuechen/private-transformers](https://github.com/lxuechen/private-transformers)
    找到。
- en: Xuechen Li, Florian Tramer, Percy Liang, Tatsunori Hashimoto et al. provided
    private transformers to train LLMs using differential privacy.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Xuechen Li, Florian Tramer, Percy Liang, Tatsunori Hashimoto 等人提供了使用差分隐私训练 LLMs
    的私有变压器。
- en: 'They modified the Opacus framework, integrated it with Hugging Face’s `transformers`
    library, and provided a **privacy engine** to train the LLMs in a privacy-preserving
    manner. Using this code base, they successfully fine-tuned exceptionally large
    pre-trained models, achieving some of the most impressive differentially private
    NLP results to date. In fact, certain models have exhibited performance comparable
    to robust non-private baseline approaches. This provides compelling empirical
    support for the notion that highly effective differentially private NLP models
    can be constructed even with relatively modest datasets. Furthermore, support
    for the ghost-clipping technique enables the private training of large transformers
    with significantly reduced memory requirements. In many instances, the memory
    footprint is nearly as lightweight as non-private training, with only a modest
    increase in runtime overhead. Private transformers currently support the following
    LLMs only:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 他们修改了 Opacus 框架，将其与 Hugging Face 的 `transformers` 库集成，并提供了一个 **隐私引擎** 以隐私保护的方式训练
    LLMs。使用此代码库，他们成功地对异常大的预训练模型进行了微调，实现了迄今为止一些最令人印象深刻的差分隐私 NLP 结果。事实上，某些模型的表现与鲁棒的私有基线方法相当。这为高度有效的差分隐私
    NLP 模型甚至可以构建于相对较小的数据集上提供了令人信服的经验支持。此外，对 ghost-clipping 技术的支持使得可以以显著降低的内存需求私密地训练大型变压器。在许多情况下，内存占用几乎与私有训练一样轻量，仅略微增加了运行时开销。目前，私有变压器仅支持以下
    LLMs：
- en: '**OpenAIGPTLMHeadModel**'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAIGPTLMHeadModel**'
- en: '**OpenAIGPTDoubleHeadsModel**'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAIGPTDoubleHeadsModel**'
- en: '**GPT2LMHeadModel**'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT2LMHeadModel**'
- en: '**GPT2DoubleHeadsModel**'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT2DoubleHeadsModel**'
- en: '**BertForSequenceClassification**'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BertForSequenceClassification**'
- en: '**RobertaForSequenceClassification**'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RobertaForSequenceClassification**'
- en: '**AlbertForSequenceClassification**'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlbertForSequenceClassification**'
- en: '**BartForConditionalGeneration**'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BartForConditionalGeneration**'
- en: '**T5ForConditionalGeneration**'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**T5ForConditionalGeneration**'
- en: '**OPTForCausalLM**'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OPTForCausalLM**'
- en: '**ViTForImageClassification**'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ViTForImageClassification**'
- en: '**DeiTForImageClassification**'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeiTForImageClassification**'
- en: '**BeitForImageClassification**'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BeitForImageClassification**'
- en: 'Privately training Hugging Face transformers simply consists of four steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 私密训练 Hugging Face 变压器简单分为四个步骤：
- en: Create your favorite transformer model and optimizer; attach this optimizer
    to a **PrivacyEngine** instance.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建您喜欢的变压器模型和优化器；将此优化器附加到 **PrivacyEngine** 实例。
- en: Compute a per-example loss (1-D tensor) for a mini-batch of data.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为数据的小批量计算每个示例的损失（1-D 张量）。
- en: Pass the loss to **optimizer.step** or **optimizer.virtual_step** as a keyword
    argument.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将损失传递给 **optimizer.step** 或 **optimizer.virtual_step** 作为关键字参数。
- en: Repeat from *step 2*.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *步骤 2* 重复。
- en: '*Example*'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例*'
- en: The code shown next is designed for training a language model with privacy-preserving
    features. It utilizes the Hugging Face Transformers library and PyTorch. Next
    are the detailed steps to implement.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码是为了训练具有隐私保护功能的语言模型而设计的。它利用了Hugging Face Transformers库和PyTorch。以下是实现这些详细步骤的步骤。
- en: 'The following things are covered in the steps outlined next:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步骤中涵盖了以下内容：
- en: Libraries and imports
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库和导入
- en: Dataset class
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dataset类
- en: Loading data from a text file
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本文件加载数据
- en: Forward step
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向步骤
- en: Training function
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练函数
- en: Running the training
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行训练
- en: 'Importing the necessary libraries and modules. These include the following:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库和模块。以下是一些例子：
- en: '**tqdm**: A library for displaying progress bars during training.'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tqdm**: 一个在训练期间显示进度条的库。'
- en: '**transformers**: A library for working with transformer-based models.'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**transformers**: 用于处理基于transformer的模型的库。'
- en: '**torch**: The PyTorch library for **deep** **learning** (**DL**).'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**torch**: 用于**深度学习**（**DL**）的PyTorch库。'
- en: '**GPT2Tokenizer** and **GPT2LMHeadModel** from **transformers**: These classes
    provide access to the GPT-2 model and tokenizer.'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**transformers**中的**GPT2Tokenizer**和**GPT2LMHeadModel**: 这些类提供了访问GPT-2模型和分词器的接口。'
- en: '**PrivacyEngine** from **private_transformers**: A custom privacy engine for
    training the model with privacy constraints.'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**private_transformers**中的**PrivacyEngine**: 用于在隐私约束下训练模型的自定义隐私引擎。'
- en: '**Dataset class**: A custom **Dataset** class is defined to handle the training
    data. This class has the following methods:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Dataset类**: 定义了一个自定义的**Dataset**类来处理训练数据。该类具有以下方法：'
- en: '**__init__(self, texts, labels, eos_token)**: Initializes the dataset with
    texts, labels, and an **end-of-sequence** (**EOS**) token (**eos_token**).'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**__init__(self, texts, labels, eos_token)**: 使用文本、标签和一个**序列结束**（**EOS**）标记（**eos_token**）初始化数据集。'
- en: '**__len__(self)**: Returns the length of the dataset.'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**__len__(self)**: 返回数据集的长度。'
- en: '**__getitem__(self, index)**: Retrieves a specific text and its corresponding
    label at the given index.'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**__getitem__(self, index)**: 在指定索引处检索特定的文本及其对应的标签。'
- en: '**Loading data from a text file**: The **get_data_from_txt(path)** function
    is used to load text data and labels from a text file. Each line in the file contains
    a label followed by a text. This function reads the file, extracts the labels
    and texts, and returns them as lists.'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从文本文件加载数据**: 使用**get_data_from_txt(path)**函数从文本文件中加载数据和标签。文件中的每一行包含一个标签后跟一个文本。此函数读取文件，提取标签和文本，并将它们作为列表返回。'
- en: '**Forward step**: The **forward_step(correct_texts, wrong_texts, tokenizer,
    model, mismatch_loss, mismatch_weight)** function performs a forward step during
    training. It takes a list of correct and incorrect texts, a tokenizer, the model,
    and parameters for mismatch loss and mismatch weight. It tokenizes the texts,
    calculates the language modeling loss, and applies mismatch loss if specified.
    The result is a loss tensor.'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**前向步骤**: **forward_step(correct_texts, wrong_texts, tokenizer, model, mismatch_loss,
    mismatch_weight)** 函数在训练期间执行前向步骤。它接受正确和错误文本的列表、分词器、模型以及不匹配损失和不匹配权重的参数。它对文本进行分词，计算语言模型损失，并在指定的情况下应用不匹配损失。结果是损失张量。'
- en: '**Training function**: The **train_llm(args_model_out, return_results, train_data,
    train_loader)** function trains the language model. It initializes the GPT-2 model,
    tokenizer, optimizer, and privacy engine. A privacy budget (epsilon) value of
    **0.5** is used in this example, but it can be changed to the desired privacy
    budget. It then iterates over training epochs, processing data in batches and
    calculating losses. The model is saved at the end of each epoch.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练函数**: **train_llm(args_model_out, return_results, train_data, train_loader)**
    函数用于训练语言模型。它初始化GPT-2模型、分词器、优化器和隐私引擎。本例中使用了一个隐私预算（epsilon）值为**0.5**，但可以更改为所需的隐私预算。然后它遍历训练的各个epoch，批量处理数据并计算损失。在每个epoch结束时保存模型。'
- en: '**Running the training**: At the end of the code, a sample dataset is loaded
    from a text file, and the training process is initiated using the **train_llm()**
    function. The function takes parameters such as the output path for saving the
    model, whether to return results, the training data, and the data loader.'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**运行训练**: 在代码的末尾，使用**train_llm()**函数从一个文本文件中加载样本数据集，并开始训练过程。该函数接受诸如保存模型的输出路径、是否返回结果、训练数据和数据加载器等参数。'
- en: 'All the preceding six steps are implemented in the following code snippet:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的前六个步骤在以下代码片段中实现：
- en: '*Source* *code: Privacy_Transformer.ipynb*'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '*源代码: Privacy_Transformer.ipynb*'
- en: '[PRE15]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Figure 10.6 Training Loss and Model Parameters](img/B16573_10_06.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图10.6 训练损失和模型参数](img/B16573_10_06.jpg)'
- en: Figure 10.6 Training Loss and Model Parameters
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 训练损失和模型参数
- en: STOA – Privacy-preserving technologies for LLMs
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: STOA – LLM的隐私保护技术
- en: The following section provides high-level SOTA research work on privacy-preserving
    technologies for LLMs. This is not an exhaustive list but details current trends
    in the research.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分提供了关于LLM隐私保护技术的SOTA研究工作的高级概述。这不是一个详尽的列表，但详细介绍了当前的研究趋势。
- en: 'Prompts – Privacy: “Flocks of Stochastic Parrots: Differentially Private Prompt
    Learning for Large Language Models” *Research article*: [https://arxiv.org/pdf/2305.15594.pdf](https://arxiv.org/pdf/2305.15594.pdf)'
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示 – 隐私：“随机鹦鹉群：大型语言模型差分隐私提示学习” *研究论文*：[https://arxiv.org/pdf/2305.15594.pdf](https://arxiv.org/pdf/2305.15594.pdf)
- en: 'Large Language Models (LLMs) are adept at understanding contextual information;
    however, concerns arise regarding the privacy implications associated with the
    data contained within prompts. This study validates these concerns by demonstrating
    a simple yet highly effective membership inference attack on the data used for
    LLM prompts. To address this vulnerability, one option is to move away from prompting
    and instead focus on fine-tuning LLMs using established algorithms for private
    gradient descent. However, this approach sacrifices the practicality and efficiency
    provided by the prompting method. Therefore, the authors propose a new solution:
    private prompt learning. They first show the feasibility of obtaining soft prompts
    privately through gradient descent on downstream data. However, the challenge
    lies in handling discrete prompts. To overcome this, a process is devised where
    an ensemble of LLMs is engaged with various prompts, similar to a group of diverse
    parrots. A noisy vote among these LLMs privately transfers the collective knowledge
    of the ensemble into a single public prompt. Their results demonstrate that LLMs
    prompted using their private algorithms closely approach the performance of their
    non-private counterparts. For example, when using GPT-3 as the base model, they
    achieve a downstream accuracy of 92.7% on the sst2 dataset with (ε = 0.147, δ
    = 10-6)-differential privacy, compared to 95.2% for the non-private baseline.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）擅长理解上下文信息；然而，关于提示中包含的数据的隐私影响引起了担忧。这项研究通过展示对LLM提示所使用数据的简单但非常有效的成员身份推断攻击来验证这些担忧。为了解决这种漏洞，一个选择是放弃提示，转而专注于使用私有无梯度下降算法对LLM进行微调。然而，这种方法牺牲了提示方法提供的实用性和效率。因此，作者提出了一个新的解决方案：隐私提示学习。他们首先展示了通过下游数据的梯度下降获得软提示的可行性。然而，挑战在于处理离散提示。为了克服这一点，设计了一个过程，其中一组LLM与各种提示互动，类似于一群多样化的鹦鹉。这些LLM之间的噪声投票将整个集体的知识私下转移到单个公共提示中。他们的结果表明，使用他们的私有算法提示的LLM在性能上接近其非隐私的对应物。例如，当使用GPT-3作为基础模型时，他们在sst2数据集上实现了92.7%的下游准确率，具有（ε
    = 0.147，δ = 10^-6）差分隐私，而基线非隐私的准确率为95.2%。
- en: 'Prompts – Privacy: LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing
    Friendly Transformers'
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示 – 隐私：LLM可以理解加密提示：面向隐私计算友好的Transformer
- en: '*Research* *article*: [https://arxiv.org/abs/2305.18396](https://arxiv.org/abs/2305.18396)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*研究论文*：[https://arxiv.org/abs/2305.18396](https://arxiv.org/abs/2305.18396)'
- en: In this study, scholars illustrated that replacing computationally and communication-intensive
    functions within the transformer framework with privacy-computing-compatible approximations
    markedly reduced the expenses linked to private inference, with only slight impacts
    on model effectiveness. Contrasting with the state-of-the-art Iron framework (*NeurIPS
    2022*), their model inference process tailored for privacy-computing demonstrated
    a fivefold increase in computational speed and an 80% decrease in communication
    overhead, while preserving almost identical accuracy levels.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，学者们说明了通过在transformer框架内用隐私计算兼容的近似值替换计算和通信密集型函数，显著降低了与隐私推理相关的成本，同时对模型的有效性影响很小。与最先进的Iron框架（*NeurIPS
    2022*）相比，他们为隐私计算量身定制的模型推理过程在计算速度上提高了五倍，在通信开销上减少了80%，同时保持了几乎相同的准确率水平。
- en: Differentially private attention computation
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 差分隐私注意力计算
- en: '*Research article*: [https://arxiv.org/abs/2305.04701](https://arxiv.org/abs/2305.04701)'
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '*研究论文*：[https://arxiv.org/abs/2305.04701](https://arxiv.org/abs/2305.04701)'
- en: The attention mechanism plays a crucial role in LLMs, enabling them to selectively
    focus on various segments of input text. Computing the attention matrix is a well-recognized
    and substantial task in the LLM computation process. Consequently, determining
    how to offer verifiable privacy guarantees for the computation of the attention
    matrix is a significant research avenue. One natural mathematical concept for
    quantifying privacy, as found in theoretical computer science graduate textbooks,
    is differential privacy.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在大型语言模型（LLMs）中起着至关重要的作用，使它们能够选择性地关注输入文本的各个部分。计算注意力矩阵是LLM计算过程中的一个公认且重要的任务。因此，确定如何为注意力矩阵的计算提供可验证的隐私保证是一个重要的研究方向。在理论计算机科学研究生教科书中发现的一个自然数学概念，用于量化隐私，是差分隐私。
- en: In this study, inspired by the work of Vyas, Kakade, and Barak (2023), researchers
    present a provable outcome that demonstrates how to differentially privately approximate
    the attention matrix. From a technical perspective, the results draw upon pioneering
    research in the realm of differential privacy as established by Alabi, Kothari,
    Tankala, Venkat, and Zhang (2022).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，受Vyas、Kakade和Barak（2023）的工作启发，研究人员提出了一种可证明的结果，展示了如何差分隐私地近似注意力矩阵。从技术角度来看，这些结果借鉴了Alabi、Kothari、Tankala、Venkat和Zhang（2022）在差分隐私领域的开创性研究。
- en: Differentially private decoding in LLMs
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLMs中的差分隐私解码
- en: 'Research article: [https://arxiv.org/abs/2205.13621](https://arxiv.org/abs/2205.13621)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 研究论文：[https://arxiv.org/abs/2205.13621](https://arxiv.org/abs/2205.13621)
- en: Researchers presented a straightforward, easily interpretable, and computationally
    efficient perturbation technique designed for implementation during the decoding
    phase of a pre-trained model. This perturbation mechanism is model-agnostic and
    compatible with any LLM. Their work includes a theoretical analysis demonstrating
    the differential privacy properties of the proposed mechanism, along with experimental
    results illustrating the trade-off between privacy and utility.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员提出了一种简单、易于解释且计算效率高的扰动技术，旨在在预训练模型的解码阶段实施。这种扰动机制是模型无关的，并且与任何LLM兼容。他们的工作包括对所提出机制差分隐私属性的理论分析，以及说明隐私与效用之间权衡的实验结果。
- en: Differentially private model compression
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 差分隐私模型压缩
- en: '*Research* *article*: [https://arxiv.org/abs/2206.01838](https://arxiv.org/abs/2206.01838)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*研究* *论文*：[https://arxiv.org/abs/2206.01838](https://arxiv.org/abs/2206.01838)'
- en: Large pre-trained LLMs have demonstrated the ability to undergo fine-tuning
    on private data, achieving performance levels comparable to non-private models
    across numerous downstream NLP) tasks while ensuring differential privacy. However,
    these models, comprising hundreds of millions of parameters, often incur prohibitively
    high inference costs. Therefore, in practical applications, LLMs are frequently
    subjected to compression before deployment. Researchers embark on the exploration
    of differentially private model compression and propose frameworks capable of
    achieving 50% sparsity levels while retaining nearly full performance. Their study
    includes practical demonstrations of standard **General Language Understanding
    Evaluation** (**GLUE**) benchmarks using BERT models, thus establishing benchmarks
    for future research in this field.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 大型预训练LLMs已经证明了在私有数据上进行微调的能力，在众多下游自然语言处理（NLP）任务中实现了与无隐私模型相当的性能水平，同时确保了差分隐私。然而，这些包含数亿个参数的模型通常会产生过高的推理成本。因此，在实际应用中，LLMs在部署前通常需要进行压缩。研究人员开始探索差分隐私模型压缩，并提出能够实现50%稀疏度水平同时保留几乎全部性能的框架。他们的研究包括使用BERT模型进行标准**通用语言理解评估（GLUE**）基准的实践演示，从而为该领域的未来研究建立了基准。
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In conclusion, this chapter has provided an in-depth exploration into the world
    of Language Models (LLMs) and the critical considerations surrounding their use,
    particularly focusing on privacy and security aspects. We have covered key concepts
    such as prompt engineering and compared open-source versus closed-source LLMs.
    Additionally, we delved into AI standards and terminology of attacks, highlighting
    NIST’s guidelines and the OWASP Top 10 LLMs vulnerabilities.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本章深入探讨了语言模型（LLMs）的世界及其使用过程中的关键考虑因素，特别是隐私和安全方面。我们涵盖了诸如提示工程等关键概念，并比较了开源与闭源LLMs。此外，我们还探讨了AI标准和攻击术语，强调了NIST的指南和OWASP
    Top 10 LLMs漏洞。
- en: Furthermore, we discussed various privacy attacks on LLMs, including real-world
    incidents of privacy leaks, membership inference attacks, and prompt injection
    attacks. These examples underscore the importance of robust privacy-preserving
    technologies in LLMs. We examined techniques like training LLMs using Differential
    Privacy with Private Transformer to mitigate privacy risks while maintaining model
    performance.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们讨论了针对LLMs的各种隐私攻击，包括现实中的隐私泄露事件、成员推断攻击和提示注入攻击。这些例子强调了在LLMs中采用稳健的隐私保护技术的重要性。我们探讨了使用带有私有Transformer的差分隐私来训练LLMs的技术，以减轻隐私风险同时保持模型性能。
- en: Overall, this chapter aims to empower readers with the knowledge and tools necessary
    to navigate the complexities of LLMs while safeguarding user privacy and ensuring
    responsible AI deployment. As the field continues to evolve, it becomes increasingly
    crucial to stay informed and proactive in addressing privacy concerns in LLMs.
    By understanding the nuances of prompt engineering, AI standards, privacy attacks,
    and privacy-preserving technologies, stakeholders can make informed decisions
    to promote trustworthy and responsible use of LLMs in various applications.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，本章旨在赋予读者必要的知识和工具，以便在导航LLMs的复杂性时保护用户隐私并确保负责任的AI部署。随着该领域的持续发展，保持对LLMs中隐私问题的了解和积极应对变得越来越重要。通过理解提示工程、AI标准、隐私攻击和隐私保护技术的细微差别，利益相关者可以做出明智的决定，以促进LLMs在各种应用中的可信和负责任的使用。
