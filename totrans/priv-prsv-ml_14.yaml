- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preserving Privacy in Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Large language models** (**LLMs**) have emerged as a transformative technology
    in the field of artificial intelligence (AI), enabling advanced **natural language
    processing** (**NLP**) tasks and generative capabilities. These models, such as
    OpenAI’s GPT-3.5 and Meta’s Llama 2 have shown remarkable proficiency in generating
    human-like text and demonstrating a deep understanding of language patterns. In
    this chapter, you will learn about closed source and open source LLMs at a high
    level, privacy issues with these LLMs, and **state-of-the-art** (**SOTA**) research
    in privacy-preserving technologies for LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts/terms used in LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt engineering: Sentence translation using ChatGPT (closed source LLM)
    as well as using open source LLMs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison of open source LLMs and closed source LLMs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: AI standards and terminology of attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**National Institute of Standards and Technology** (**NIST**) Trustworthy and
    Responsible AI'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open Worldwide Application Security Project** (**OWASP**) Top 10 LLMs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy attacks on LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world incidents of privacy leaks in LLMs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Membership inference attacks against generative models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting training data from LLMs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt injection attacks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy-preserving technologies for LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text attacks on **machine learning** (**ML**) and **generative** **AI** (**GenAI**)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Training LLMs using differential privacy with private transformer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SOTA research on privacy-preserving LLMs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Key concepts/terms used in LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are a complex field of NLP, and there are several terms associated with
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key terms and concepts used in the context of LLMs are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformer architecture**: The foundational architecture for most LLMs,
    known for its self-attention mechanism, which allows the model to weigh the importance
    of different words in a sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-training**: The initial phase in which the LLM is trained on a massive
    corpus of text data from the internet to learn language patterns and context.
    This pre-trained model is often referred to as the “base model.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning**: The subsequent phase where the pre-trained model is adapted
    to perform specific NLP tasks, such as text classification, translation, summarization,
    or question answering. Fine-tuning helps the model specialize in these tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameters**: These are the trainable components of the LLM, represented
    by numerical values. The number of parameters is a key factor in determining the
    size and capability of an LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attention mechanism**: A core component of the transformer architecture,
    it enables the model to focus on different parts of the input sequence when processing
    it, improving contextual understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-attention**: A specific type of attention where the model assigns weights
    to different words in a sentence based on their relevance to each other, allowing
    it to capture dependencies between words. Most transformers are built based on
    the research paper from Google, *Attention Is All You* *Need* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embeddings**: Word embeddings or token embeddings are vector representations
    of words or tokens in a continuous space. These embeddings capture semantic relationships
    between words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual embeddings**: Unlike static word embeddings, these embeddings
    change based on the context of the sentence, allowing LLMs to understand the meaning
    of words in different contexts. Positional embeddings and rotary position embeddings
    come under the category of contextual embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenization**: The process of breaking down text into individual tokens
    (words or subwords) for input into the model. LLMs use tokenizers to perform this
    task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoding**: The process of converting model-generated representations (usually
    logits or token IDs) into human-readable text. Decoding is necessary to produce
    the final output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transfer learning (TL)**: The concept of transferring knowledge gained from
    one task or domain to another. LLMs often benefit from TL, as they are pre-trained
    on a broad range of text before fine-tuning for specific tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt engineering**: The process of designing input prompts or instructions
    that guide the LLM to generate the desired output. Crafting effective prompts
    is crucial in controlling the model’s behavior:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Simple prompt flow](img/B16573_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Simple prompt flow
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero-shot learning**: A type of TL where a model is asked to perform a task
    for which it was not explicitly fine-tuned. LLMs are capable of zero-shot learning
    to some extent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Few-shot learning**: Like zero-shot learning, but the model is provided with
    a limited number of examples for a new task during fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chain of Thought (CoT)**: CoT prompting is a technique that guides LLMs to
    follow a reasoning process when dealing with hard problems. This is done by showing
    the model a few examples where the step-by-step reasoning is clearly laid out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree of Thoughts (ToT)**: ToT prompting breaks a problem into a sequence
    of smaller steps—or *thoughts*—that are solved individually. This approach does
    not constrain the model to output these steps all at once. Rather, each thought
    is generated or solved independently and passed to the next step for solving the
    problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph of Thoughts (GoT)**: It conceptualizes the data generated by an LLM
    as a graph, where each node symbolizes a unit of information, commonly known as
    “LLM thoughts.” The connections between these nodes represent the dependencies
    or associations among distinct units of thought.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt example using ChatGPT (closed source LLM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s try an example using ChatGPT ([https://chat.openai.com/](https://chat.openai.com/))
    and ask questions to translate a sentence from English to German.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the question is called a prompt, and the response from ChatGPT
    (LLM) is called a completion/result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Simple prompt request and completion](img/B16573_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Simple prompt request and completion
  prefs: []
  type: TYPE_NORMAL
- en: Prompt example using open source LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s try an example using open source LLMs programmatically using Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure that you have the Hugging Face Transformers library installed and that
    you have access to the `"google/flan-t5-large"` model for this code to run successfully.
    Follow the next steps to implement this example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the library using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Additionally, you need to download the model using **transformers.AutoModel.from_pretrained("google/flan-t5-large")**
    if you haven’t already.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the necessary classes from the Transformers library, namely **T5Tokenizer**
    and **T5ForConditionalGeneration**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the T5 tokenizer and model with the pre-trained **"google/flan-t5-large"**
    model. This model is designed for translation tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the input text you want to translate from English to German, which is
    **"translate English to German: How old** **are you?"**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize the input text using the tokenizer, and convert it into PyTorch tensors.
    This step prepares the text for input to the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the translation using the T5 model by passing the tokenized input to
    the model’s **generate** method. The translation output is stored in the **outputs**
    variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decode the generated output using the tokenizer’s **decode** method, and print
    the translated text, which will be the German translation of the input text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Source code: Ex_LLM_Opensource.ipynb*The following is a detailed source code
    of the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 10.3 – T5 model weights, tokenizer and config downloads](img/B16573_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – T5 model weights, tokenizer and config downloads
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This results in the following output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Comparison of open source LLMs and closed source LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open source and closed source LLMs represent two different approaches to the
    development and availability of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Open source LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s look at some of the attributes of open source LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accessibility**: Open source LLMs are publicly accessible, and their architecture
    and parameters can be examined, modified, and shared by the community. This transparency
    fosters collaboration and innovation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Community contributions**: They often benefit from contributions and enhancements
    from a diverse community of researchers and developers, leading to rapid improvements
    and addressing potential biases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization**: Users have the freedom to fine-tune and adapt open source
    LLMs for specific tasks, languages, or domains, making them highly flexible and
    versatile.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-efficiency**: Typically, open source LLMs are free to use, which can
    be particularly advantageous for researchers, start-ups, and developers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware infrastructure**: Open source models need to be hosted on GPUs for
    inferencing, and associated costs need to be owned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Open source LLMs may have security vulnerabilities or the underlying
    software versions, so addressing these security vulnerabilities (**Common Vulnerabilities
    and Exposures**, or **CVEs**) needs to be managed on its own.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Examples*: Google’s FLAN-T5, Meta’s Llama models, GPT-3, and Hugging Face
    transformers are open source and widely accessible.'
  prefs: []
  type: TYPE_NORMAL
- en: Closed source LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let’s turn our attention to the attributes of closed source LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Proprietary**: Closed source LLMs are developed and owned by organizations
    or companies, and their architecture and parameters are not publicly disclosed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Control**: Developers of closed source LLMs retain control over their models,
    algorithms, and **intellectual property** (**IP**), allowing them to protect their
    innovations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited customization**: Users of closed source LLMs may have limited options
    for fine-tuning or adapting the model to specific needs, as the source code is
    not openly available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Costs**: Closed source LLMs often come with licensing fees or usage costs,
    which can be a significant factor for some users or organizations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware infrastructure**: Closed source models are deployed in GPUs by the
    vendors, and they provide the APIs to access either through REST or gRPC, so infra
    costs are owned by the providers (in the case of GPT-4 or GPT-3.x, OpenAI and
    Microsoft will own the hosted versions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Closed source LLMs may have security vulnerabilities in the underlying
    software versions, so LLM providers will address these security vulnerabilities
    (CVEs), and it is a black box for the users who make use of these.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Examples*: Commercial language models such as GPT-3.5 or GPT-4 models and
    proprietary models used by tech companies may be closed source.'
  prefs: []
  type: TYPE_NORMAL
- en: The choice between open source and closed source LLMs depends on factors such
    as budget, data privacy concerns, customization needs, and the level of control
    required.
  prefs: []
  type: TYPE_NORMAL
- en: Open source LLMs offer accessibility, collaboration, and cost savings but may
    require more technical expertise for customization. Closed source LLMs provide
    IP protection and may come with specialized support and features, but at the cost
    of limited transparency and potential licensing fees.
  prefs: []
  type: TYPE_NORMAL
- en: Organizations and developers should carefully consider their specific requirements
    when choosing between these two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: AI standards and terminology of attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following section, we will go through some AI standards and terminology
    of attacks.
  prefs: []
  type: TYPE_NORMAL
- en: NIST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*NIST Trustworthy and Responsible AI* released a paper on taxonomy and terminologies
    used in AI with respect to attacks and mitigations. It covers both predictive
    AI (traditional ML) and GenAI.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Taxonomy of attacks on Generative AI systems](img/B16573_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Taxonomy of attacks on Generative AI systems
  prefs: []
  type: TYPE_NORMAL
- en: '*Image source: “*Adversarial Machine Learning: A Taxonomy and Terminology of
    Attacks and Mitigations *” paper from* *NIST.* [https://doi.org/10.6028/NIST.AI.100-2e2023](https://doi.org/10.6028/NIST.AI.100-2e2023)'
  prefs: []
  type: TYPE_NORMAL
- en: OWASP Top 10 for LLM applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *OWASP Top 10 for Large Language Model Applications* project aims to educate
    developers, designers, architects, managers, and organizations about the potential
    security risks when deploying and managing LLMs. The OWASP Top 10 for LLM applications
    are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '*LLM01:* *Prompt Injection*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM02: Insecure* *Output Handling*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM03: Training* *Data Poisoning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM04: Model Denial* *of Service*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM05: Supply* *Chain Vulnerabilities*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM06: Sensitive* *Information Disclosure*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM07: Insecure* *Plugin Design*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM08:* *Excessive Agency*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM09: Overreliance*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM10:* *Model Theft*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The detailed vulnerabilities and how to detect each vulnerability and possible
    solutions are documented at [https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover in more detail privacy attacks on LLMs/GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy attacks on LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, LLMs have revolutionized **natural language understanding**
    (**NLU**) and **natural language generation** (**NLG**), powering a wide range
    of applications from chatbots and virtual assistants to content recommendation
    systems and language translation services. However, the rapid advancement of these
    models has raised significant concerns about privacy and security. LLM applications
    have the potential to expose sensitive data, proprietary algorithms, or other
    confidential information through their output. This could lead to unauthorized
    access to sensitive data, IP, privacy infringements, and other security violations.
    As LLMs become increasingly prevalent in our digital landscape, there is a growing
    need for effective strategies to protect sensitive information and uphold user
    privacy.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the earlier chapters, ML models are susceptible to privacy attacks,
    and there’s no exception for GenAI models (LLMs) either.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following two recent articles provide details of privacy issues in enterprises
    with respect to GenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cyberhaven’s survey**: As per the article from Cyberhaven ([https://www.cyberhaven.com/blog/4-2-of-workers-have-pasted-company-data-into-chatgpt/](https://www.cyberhaven.com/blog/4-2-of-workers-have-pasted-company-data-into-chatgpt/)),
    the potential risks of data leaks when employees paste company data into chatbots
    such as OpenAI’s GPT-3\. The company conducted a survey of 2,000 workers in the
    US and the UK and found that 4.2% of them had pasted company data into chatbots.
    While chatbots such as GPT-3 are designed to forget information after the conversation
    ends, the risk lies in the fact that these chatbots could potentially remember
    and replicate sensitive information during the conversation. The article also
    mentions that if a hacker gains control of the chatbot during the conversation,
    they could access sensitive data. The article emphasizes the need for companies
    to have clear policies about what data can be shared with chatbots and to educate
    employees about potential risks. It also suggests that companies should implement
    **data loss prevention** (**DLP**) solutions to automatically block sensitive
    data from being shared with chatbots. It concludes by stating that while AI chatbots
    have many benefits, companies need to be aware of potential security and privacy
    risks and take appropriate measures to protect sensitive data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Samsung’s IP leak**: As per the article at [https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/](https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/),
    Samsung employees unintentionally disclosed confidential information while using
    ChatGPT for work-related tasks, highlighting potential privacy and security risks.
    Samsung’s semiconductor division permitted engineers to employ ChatGPT for source
    code checks and other duties. However, *The Economist* in Korea reported three
    separate incidents where sensitive data was inadvertently exposed to ChatGPT.'
  prefs: []
  type: TYPE_NORMAL
- en: In one incident, an employee copied confidential source code into a chat to
    identify errors. Another employee shared code and requested optimization. A third
    employee shared a meeting recording for transcription into presentation notes.
    This data is now accessible to ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Samsung has responded promptly by limiting ChatGPT’s upload capacity to 1,024
    bytes per user and initiating investigations into those responsible for the data
    breaches. Moreover, Samsung is considering developing an in-house AI chatbot to
    bolster data security and privacy going forward. However, it’s improbable that
    Samsung can retrieve the leaked data due to ChatGPT’s data policy, which employs
    data for model training unless users explicitly opt out. The ChatGPT usage guide
    explicitly warns against sharing sensitive information during conversations.
  prefs: []
  type: TYPE_NORMAL
- en: These incidents illustrate real-world scenarios that privacy experts have long
    been wary of, such as sharing confidential legal or medical documents for text
    analysis or summarization, which could be utilized to refine the model. Privacy
    experts caution that this may potentially contravene **General Data Protection
    Regulation** (**GDPR**) compliance, resulting in regulatory consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Membership inference attacks against generative models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We learned about membership inference attacks on ML models in [*Chapter 4*](B16573_04.xhtml#_idTextAnchor079).
    GenAI models also are susceptible to membership inference attacks along a similar
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: Generative models aim to estimate the fundamental distribution of a dataset,
    enabling the creation of lifelike samples based on that distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When presented with a data point, the adversary discerns whether it was utilized
    in training the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These attacks are based on both white-box and black-box access to the target
    model, against several SOTA generative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go through an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example provides a basic membership inference attack against a generative
    model using PyTorch. The attack aims to determine if a specific data point was
    part of the generative model’s training dataset. It includes the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample GenAI model using a **variational** **autoencoder** (**VAE**):'
  prefs: []
  type: TYPE_NORMAL
- en: '**VAE**: A simple VAE is used as the generative model. The VAE is capable of
    encoding and decoding binary data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversary model**: An adversary model is implemented as a two-layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**feedforward neural network** (**FNN**): This model is trained to predict
    whether a given data point was a member of the training dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthetic data**: Synthetic binary data is generated for demonstration purposes.
    In practice, you would replace this with your actual dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training process**: The VAE and the adversary model are trained independently.
    The VAE learns to encode and decode data, while the adversary model learns to
    predict membership.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Membership inference attack**: The membership inference attack function takes
    a target data point, encodes it using the VAE, and then uses the adversary model
    to predict whether the target data point is a member or non-member of the training
    dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Source code components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SampleGenModel class**: Defines the architecture of the VAE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversary class**: Defines the architecture of the adversary model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data generation**: Generates synthetic binary data for training and testing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training**: Training loops for the VAE-based **SampleGenModel** class and
    the adversary model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Membership inference attack**: The function for conducting the membership
    inference attack'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Main execution**: Initializes the VAE and the adversary model and performs
    an attack on a target data point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Source* *code: MemberShipInference_LLM.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Membership inference attacks are more complex in practice, and this code serves
    as a basic demonstration. Implement privacy and security measures when deploying
    generative models to protect against such attacks. We will cover in detail how
    to protect GenAI models in a privacy-preserving manner in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting training data attack from generative models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Extracting training data from LLMs can be a challenging task because the training
    data is not typically available directly from the model. Instead, LLMs are pre-trained
    on vast datasets from the internet. If we have a specific LLM in mind and want
    to extract training data related to it, we may need access to the original data
    sources used for pre-training, which may not be publicly available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a sample Python code snippet that demonstrates how we can extract text
    data from a pre-trained Hugging Face Transformers model, such as GPT-2\. Keep
    in mind that this code is for illustrative purposes and won’t retrieve the actual
    training data but rather generates text samples from the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this code, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We load a pre-trained GPT-2 model and tokenizer from the Hugging Face Transformers
    library. You can choose other models based on your requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We define a prompt, which serves as the starting point for generating text.
    You can change the prompt to suit your needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We specify the number of text samples (**num_samples**) to generate from the
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inside the loop, we encode the prompt using the tokenizer and generate text
    sequences using the model. We decode the output to obtain human-readable text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that the generated text is not actual training data used for the
    model but rather synthetic text produced by the model based on the provided prompt.
    To access the actual training data used to train LLMs, you would need access to
    the original data sources, which are typically large and diverse web corpora.
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: Training_Data_Extraction_Gen_AI.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 10.5 – GPT2 model weights, tokenizer and config downloads](img/B16573_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – GPT2 model weights, tokenizer and config downloads
  prefs: []
  type: TYPE_NORMAL
- en: 'Researchers from Google, Apple, OpenAI, Harvard, UC Berkeley, Northeastern
    University and Stanford demonstrated an attack on GPT-2, a language model trained
    on scrapes of the public internet, and were able to extract hundreds of verbatim
    text sequences from the model’s training data. These extracted examples included
    (public) **personally identifiable information** or **PII** (names, phone numbers,
    and email addresses): [https://arxiv.org/pdf/2012.07805.pdf](https://arxiv.org/pdf/2012.07805.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt injection attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A prompt injection attack, also known as data or command injection, is a type
    of security vulnerability that happens when an attacker can influence the prompts
    or commands sent to a data processing system such as an LLM. These attacks potentially
    allow attackers to manipulate the actions of the system or extract sensitive or
    private data.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of an LLM, prompt injection attacks could involve an attacker
    providing a crafted input designed to trick the model into providing information
    it has been trained on, which could potentially include sensitive or confidential
    information if the training data was not properly anonymized or scrubbed. Moreover,
    an attacker could inject malicious prompts to make the model produce outputs that
    inflict harm, such as generating offensive, defamatory, or illegal content. This
    could be used for spear phishing, spreading disinformation, defaming individuals
    or entities, and many other nefarious purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain ([https://www.langchain.com/](https://www.langchain.com/)) is one
    of the open source frameworks that provides tools to build LLM applications. In
    August 2023, the NVIDIA AI Red Team identified three vulnerabilities in LangChain
    through prompt injection; they are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*CVE-2023-29374*: In LangChain through 0.0.131, the **LLMMathChain** chain
    allows prompt injection attacks that can execute arbitrary code via the Python
    **exec** method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CVE-2023-32786*: In Langchain through 0.0.155, prompt injection allows an
    attacker to force the service to retrieve data from an arbitrary URL, essentially
    providing **server-side request forgery** (**SSRF**) and potentially injecting
    content into downstream tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CVE-2023-36189*: SQL injection vulnerability in LangChain before v0.0.247
    allows a remote attacker to obtain sensitive information via the **SQLDatabaseChain**
    component'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currently, the extent to which LLMs are vulnerable to these attacks isn’t fully
    understood. It’s also worth mentioning that these models are designed not to directly
    recall any specifics about their training data, including documents or sources
    they were trained on, and they generally don’t have the ability to access or retrieve
    personal data unless they’ve been explicitly programmed to do so, or they’ve been
    trained on data that contains sensitive personal information. Nonetheless, it’s
    always crucial to approach the use of LLMs, or any AI system, with robust security
    measures and an understanding of potential risks.
  prefs: []
  type: TYPE_NORMAL
- en: '*Example: PromptInjection.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`This results in the` `following output.`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Privacy-preserving technologies for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Differential privacy is one of the privacy-preserving technologies that can
    be used for LLMs as well.
  prefs: []
  type: TYPE_NORMAL
- en: Text attacks on ML models and LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TextAttack stands as a Python framework designed for conducting adversarial
    attacks, adversarial training, and data augmentation within the field of NLP.
    This versatile tool streamlines the process of exploring NLP model robustness,
    offering a seamless, rapid, and user-friendly experience. Furthermore, it proves
    invaluable for NLP model training, adversarial training, and data augmentation
    purposes. TextAttack offers various components tailored for typical NLP tasks,
    including sentence encoding, grammar checking, and word replacement, which can
    also be utilized independently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instructions on how to install the TextAttack package can be found at this
    GitHub URL: [https://github.com/QData/TextAttack](https://github.com/QData/TextAttack).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install TextAttack framework using `pip install` in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: TextAttack provides various recipes to attack on NLP modules. The following
    example utilizes various libraries and components to perform adversarial attacks
    on NLP models using the TextAttack framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the high-level steps in the implementation of this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing libraries**: Import the necessary libraries, including **transformers**
    from Hugging Face, **torch** for PyTorch, **math**, **textattack**, and **random**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Environment setup**: It sets the **CUDA_VISIBLE_DEVICES** environment variable
    to an empty string, essentially disabling GPU usage. It specifies the device to
    be used as **"cpu"** for PyTorch operations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model definition**: Defines a custom PyTorch model called **Model**. This
    model uses **Bidirectional Encoder Representations from Transformers** (**BERT**)
    for NLP tasks. The model loads the pre-trained **''bert-base-uncased''** BERT
    model from Hugging Face’s Transformers library. It includes a dropout layer and
    a linear layer for classification.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialization of model and tokenization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model initialization**: An instance of the **Model** class is created and
    moved to the CPU for evaluation. The model is set to evaluation mode using **model.eval()**.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenizer initialization**: Initializes a BERT tokenizer (**BertTokenizer**)
    for tokenizing text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom model wrapper**: Defines a custom model wrapper class called **CustomWrapper**
    that wraps the PyTorch model. This wrapper allows the model to be used with the
    **TextAttack** library.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Utilizes the TextAttack library to build an attack using the **TextFoolerJin2019**
    recipe. The **CustomWrapper** instance is passed to the attack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dataset**: Defines a list called **dataset**, containing text samples and
    corresponding labels. These samples are examples of performing adversarial attacks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attack execution**: Creates an **Attacker** instance, specifying the attack,
    dataset, and other attack parameters. Finally, the **attack_dataset()** method
    is called on the attacker to perform adversarial attacks on the dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, this code sets up a PyTorch model, initializes an attack using the
    TextAttack library, and then applies this attack to a dataset of text samples
    for the purpose of evaluating the robustness of the NLP model.
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: Privacy_attacks_LLMs.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In a similar way, the GPT-2 model also can be explored for NLP attacks (for
    the complete source code, refer to the GitHub repo at [https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter10/Privacy_attacks_LLMs.ipynb](https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter10/Privacy_attacks_LLMs.ipynb)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`attacker.attack_dataset()`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Private transformers – training LLMs using differential privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The complete source code for this section can be found at [https://github.com/lxuechen/private-transformers](https://github.com/lxuechen/private-transformers).
  prefs: []
  type: TYPE_NORMAL
- en: Xuechen Li, Florian Tramer, Percy Liang, Tatsunori Hashimoto et al. provided
    private transformers to train LLMs using differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: 'They modified the Opacus framework, integrated it with Hugging Face’s `transformers`
    library, and provided a **privacy engine** to train the LLMs in a privacy-preserving
    manner. Using this code base, they successfully fine-tuned exceptionally large
    pre-trained models, achieving some of the most impressive differentially private
    NLP results to date. In fact, certain models have exhibited performance comparable
    to robust non-private baseline approaches. This provides compelling empirical
    support for the notion that highly effective differentially private NLP models
    can be constructed even with relatively modest datasets. Furthermore, support
    for the ghost-clipping technique enables the private training of large transformers
    with significantly reduced memory requirements. In many instances, the memory
    footprint is nearly as lightweight as non-private training, with only a modest
    increase in runtime overhead. Private transformers currently support the following
    LLMs only:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAIGPTLMHeadModel**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAIGPTDoubleHeadsModel**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT2LMHeadModel**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT2DoubleHeadsModel**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BertForSequenceClassification**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RobertaForSequenceClassification**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AlbertForSequenceClassification**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BartForConditionalGeneration**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**T5ForConditionalGeneration**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OPTForCausalLM**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ViTForImageClassification**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeiTForImageClassification**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BeitForImageClassification**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Privately training Hugging Face transformers simply consists of four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create your favorite transformer model and optimizer; attach this optimizer
    to a **PrivacyEngine** instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute a per-example loss (1-D tensor) for a mini-batch of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the loss to **optimizer.step** or **optimizer.virtual_step** as a keyword
    argument.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from *step 2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Example*'
  prefs: []
  type: TYPE_NORMAL
- en: The code shown next is designed for training a language model with privacy-preserving
    features. It utilizes the Hugging Face Transformers library and PyTorch. Next
    are the detailed steps to implement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following things are covered in the steps outlined next:'
  prefs: []
  type: TYPE_NORMAL
- en: Libraries and imports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading data from a text file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Importing the necessary libraries and modules. These include the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**tqdm**: A library for displaying progress bars during training.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**transformers**: A library for working with transformer-based models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**torch**: The PyTorch library for **deep** **learning** (**DL**).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT2Tokenizer** and **GPT2LMHeadModel** from **transformers**: These classes
    provide access to the GPT-2 model and tokenizer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PrivacyEngine** from **private_transformers**: A custom privacy engine for
    training the model with privacy constraints.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset class**: A custom **Dataset** class is defined to handle the training
    data. This class has the following methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**__init__(self, texts, labels, eos_token)**: Initializes the dataset with
    texts, labels, and an **end-of-sequence** (**EOS**) token (**eos_token**).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**__len__(self)**: Returns the length of the dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**__getitem__(self, index)**: Retrieves a specific text and its corresponding
    label at the given index.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loading data from a text file**: The **get_data_from_txt(path)** function
    is used to load text data and labels from a text file. Each line in the file contains
    a label followed by a text. This function reads the file, extracts the labels
    and texts, and returns them as lists.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Forward step**: The **forward_step(correct_texts, wrong_texts, tokenizer,
    model, mismatch_loss, mismatch_weight)** function performs a forward step during
    training. It takes a list of correct and incorrect texts, a tokenizer, the model,
    and parameters for mismatch loss and mismatch weight. It tokenizes the texts,
    calculates the language modeling loss, and applies mismatch loss if specified.
    The result is a loss tensor.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training function**: The **train_llm(args_model_out, return_results, train_data,
    train_loader)** function trains the language model. It initializes the GPT-2 model,
    tokenizer, optimizer, and privacy engine. A privacy budget (epsilon) value of
    **0.5** is used in this example, but it can be changed to the desired privacy
    budget. It then iterates over training epochs, processing data in batches and
    calculating losses. The model is saved at the end of each epoch.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Running the training**: At the end of the code, a sample dataset is loaded
    from a text file, and the training process is initiated using the **train_llm()**
    function. The function takes parameters such as the output path for saving the
    model, whether to return results, the training data, and the data loader.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All the preceding six steps are implemented in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: Privacy_Transformer.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 10.6 Training Loss and Model Parameters](img/B16573_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 Training Loss and Model Parameters
  prefs: []
  type: TYPE_NORMAL
- en: STOA – Privacy-preserving technologies for LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following section provides high-level SOTA research work on privacy-preserving
    technologies for LLMs. This is not an exhaustive list but details current trends
    in the research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompts – Privacy: “Flocks of Stochastic Parrots: Differentially Private Prompt
    Learning for Large Language Models” *Research article*: [https://arxiv.org/pdf/2305.15594.pdf](https://arxiv.org/pdf/2305.15594.pdf)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) are adept at understanding contextual information;
    however, concerns arise regarding the privacy implications associated with the
    data contained within prompts. This study validates these concerns by demonstrating
    a simple yet highly effective membership inference attack on the data used for
    LLM prompts. To address this vulnerability, one option is to move away from prompting
    and instead focus on fine-tuning LLMs using established algorithms for private
    gradient descent. However, this approach sacrifices the practicality and efficiency
    provided by the prompting method. Therefore, the authors propose a new solution:
    private prompt learning. They first show the feasibility of obtaining soft prompts
    privately through gradient descent on downstream data. However, the challenge
    lies in handling discrete prompts. To overcome this, a process is devised where
    an ensemble of LLMs is engaged with various prompts, similar to a group of diverse
    parrots. A noisy vote among these LLMs privately transfers the collective knowledge
    of the ensemble into a single public prompt. Their results demonstrate that LLMs
    prompted using their private algorithms closely approach the performance of their
    non-private counterparts. For example, when using GPT-3 as the base model, they
    achieve a downstream accuracy of 92.7% on the sst2 dataset with (ε = 0.147, δ
    = 10-6)-differential privacy, compared to 95.2% for the non-private baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompts – Privacy: LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing
    Friendly Transformers'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Research* *article*: [https://arxiv.org/abs/2305.18396](https://arxiv.org/abs/2305.18396)'
  prefs: []
  type: TYPE_NORMAL
- en: In this study, scholars illustrated that replacing computationally and communication-intensive
    functions within the transformer framework with privacy-computing-compatible approximations
    markedly reduced the expenses linked to private inference, with only slight impacts
    on model effectiveness. Contrasting with the state-of-the-art Iron framework (*NeurIPS
    2022*), their model inference process tailored for privacy-computing demonstrated
    a fivefold increase in computational speed and an 80% decrease in communication
    overhead, while preserving almost identical accuracy levels.
  prefs: []
  type: TYPE_NORMAL
- en: Differentially private attention computation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Research article*: [https://arxiv.org/abs/2305.04701](https://arxiv.org/abs/2305.04701)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The attention mechanism plays a crucial role in LLMs, enabling them to selectively
    focus on various segments of input text. Computing the attention matrix is a well-recognized
    and substantial task in the LLM computation process. Consequently, determining
    how to offer verifiable privacy guarantees for the computation of the attention
    matrix is a significant research avenue. One natural mathematical concept for
    quantifying privacy, as found in theoretical computer science graduate textbooks,
    is differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: In this study, inspired by the work of Vyas, Kakade, and Barak (2023), researchers
    present a provable outcome that demonstrates how to differentially privately approximate
    the attention matrix. From a technical perspective, the results draw upon pioneering
    research in the realm of differential privacy as established by Alabi, Kothari,
    Tankala, Venkat, and Zhang (2022).
  prefs: []
  type: TYPE_NORMAL
- en: Differentially private decoding in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Research article: [https://arxiv.org/abs/2205.13621](https://arxiv.org/abs/2205.13621)'
  prefs: []
  type: TYPE_NORMAL
- en: Researchers presented a straightforward, easily interpretable, and computationally
    efficient perturbation technique designed for implementation during the decoding
    phase of a pre-trained model. This perturbation mechanism is model-agnostic and
    compatible with any LLM. Their work includes a theoretical analysis demonstrating
    the differential privacy properties of the proposed mechanism, along with experimental
    results illustrating the trade-off between privacy and utility.
  prefs: []
  type: TYPE_NORMAL
- en: Differentially private model compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Research* *article*: [https://arxiv.org/abs/2206.01838](https://arxiv.org/abs/2206.01838)'
  prefs: []
  type: TYPE_NORMAL
- en: Large pre-trained LLMs have demonstrated the ability to undergo fine-tuning
    on private data, achieving performance levels comparable to non-private models
    across numerous downstream NLP) tasks while ensuring differential privacy. However,
    these models, comprising hundreds of millions of parameters, often incur prohibitively
    high inference costs. Therefore, in practical applications, LLMs are frequently
    subjected to compression before deployment. Researchers embark on the exploration
    of differentially private model compression and propose frameworks capable of
    achieving 50% sparsity levels while retaining nearly full performance. Their study
    includes practical demonstrations of standard **General Language Understanding
    Evaluation** (**GLUE**) benchmarks using BERT models, thus establishing benchmarks
    for future research in this field.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, this chapter has provided an in-depth exploration into the world
    of Language Models (LLMs) and the critical considerations surrounding their use,
    particularly focusing on privacy and security aspects. We have covered key concepts
    such as prompt engineering and compared open-source versus closed-source LLMs.
    Additionally, we delved into AI standards and terminology of attacks, highlighting
    NIST’s guidelines and the OWASP Top 10 LLMs vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we discussed various privacy attacks on LLMs, including real-world
    incidents of privacy leaks, membership inference attacks, and prompt injection
    attacks. These examples underscore the importance of robust privacy-preserving
    technologies in LLMs. We examined techniques like training LLMs using Differential
    Privacy with Private Transformer to mitigate privacy risks while maintaining model
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this chapter aims to empower readers with the knowledge and tools necessary
    to navigate the complexities of LLMs while safeguarding user privacy and ensuring
    responsible AI deployment. As the field continues to evolve, it becomes increasingly
    crucial to stay informed and proactive in addressing privacy concerns in LLMs.
    By understanding the nuances of prompt engineering, AI standards, privacy attacks,
    and privacy-preserving technologies, stakeholders can make informed decisions
    to promote trustworthy and responsible use of LLMs in various applications.
  prefs: []
  type: TYPE_NORMAL
