- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hyperparameters and Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced the concepts of hyperparameters and hyperparameter optimization
    (or tuning) in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035). In this chapter,
    we will dive into these concepts in more detail, and we will use Google Cloud
    products such as Vertex AI Vizier to define and run hyperparameter tuning jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Following our established pattern, we will begin by covering some prerequisites
    that are required for the hands-on activities in this chapter. Then, we cover
    some important basic concepts that relate to the content covered in this chapter,
    and finally, we perform hands-on activities that teach you how to apply those
    concepts in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites and basic concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are hyperparameters?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hands-on: performing hyperparameter tuning in Vertex AI'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by reviewing the prerequisites for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The steps in this section need to be completed before we can perform the primary
    activities in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the Artifact Registry API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re going to create Docker images in order to run our custom code in conjunction
    with the Google Cloud Vertex AI Vizier service. The Google Cloud Artifact Registry
    is a fully managed artifact repository that we can use for storing our container
    images. It can be seen as the next generation of the **Google Cloud Container
    Registry** (**GCR**) that can be used to store artifacts such as Java JAR files,
    Node.js modules, Python wheels, Go modules, Maven artifacts, and npm packages
    (in addition to Docker images, which were already supported in GCR).
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable the Artifact Registry API, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to **Google Cloud services menu** → **APIs
    & Services** → **Library**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search for `Artifact Registry` in the search box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the API in the list of results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the page that displays information about the API, click **Enable**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, let’s set up the required permissions for the steps in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AI/ML service account
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 6*](B18143_06.xhtml#_idTextAnchor187), we created a service account
    for using Google Cloud’s data processing services. In this chapter, we will create
    a service account that will be used in our hyperparameter tuning job for administering
    resources in Google Cloud Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to create the required service account:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to **Google Cloud services menu** → **IAM
    & Admin** → **Service accounts**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create** **service account**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the service account name, enter `ai-ml-sa`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Create** **and Continue**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the section titled **Grant this service account access to project**, add
    the roles shown in *Figure 8**.1*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.1: AI/ML service account permissions](img/B18143_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: AI/ML service account permissions'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Done**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our service account is now ready to be used later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the prerequisites, let’s discuss some concepts that we
    need to understand before performing the hands-on activities in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section describes concepts that underpin the practical activities we will
    cover in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation metrics used in this chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve already discussed the topic of model evaluation metrics in previous chapters.
    We first introduced the concept in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    where we briefly discussed metrics such as the **mean squared error** (**MSE**)
    for regression use cases and accuracy for classification use cases. In [*Chapter
    5*](B18143_05.xhtml#_idTextAnchor168), we used functions in scikit-learn to calculate
    some of these metrics for the models we created, and we suggested looking up additional
    metrics as a supplemental learning activity at the end of that chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will train models for a classification use case, and we
    will introduce some additional metrics to evaluate our models. The main metric
    we will use is something called **AUC ROC**, which stands for **area under the
    receiver operating characteristic curve**. That sounds like a lot, but don’t worry,
    we will explain this metric in more detail in this section. In order to do so,
    we need to first introduce some concepts and simpler metrics that are used in
    calculating the AUC ROC.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in a binary classification use case, the model needs to predict one
    of two possible outcomes for each data point in the dataset, true or false, also
    referred to as **positive** or **negative**. They are usually represented by 1
    and 0.
  prefs: []
  type: TYPE_NORMAL
- en: Models are rarely perfect, so they will sometimes make mistakes. Let’s take
    a look at the possible outcomes of a binary classification model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: True positives, false positives, true negatives, and false negatives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A binary classification model’s predictions generally have four possible outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: When our model predicts something to be true (or positive) and it really is
    true (or positive), we call this a **true** **positive** (**TP**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When our model predicts something to be true (or positive) but really it is
    false (or negative), we call this a **false** **positive** (**FP**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When our model predicts something to be false (or negative) and it really is
    false (or negative), we call this a **true** **negative** (**TN**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When our model predicts something to be false (or negative) but it is really
    true (or positive), we call this a **false** **negative** (**FN**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at how these outcomes are related to each other in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding concepts can be represented visually in something called a confusion
    matrix, which is demonstrated in *Table 8.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predicted negative** | **Predicted positive** |'
  prefs: []
  type: TYPE_TB
- en: '| Actual negative | TN | FP |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual positive** | FN | TP |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.1: Confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: We can use the preceding concepts to calculate metrics that measure how well
    our models are performing when trying to accurately identify positive or negative
    data points in our dataset. We define these metrics next.
  prefs: []
  type: TYPE_NORMAL
- en: True positive rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **true positive rate** (**TPR**) represents a count of all of the data points
    in our dataset that our model correctly predicted as positive compared against
    all of the data points in the dataset that are positive, including any data points
    that our model erroneously predicted to be negative (i.e., our model said they
    were negative, even though they were positive, which means it was a false negative).
  prefs: []
  type: TYPE_NORMAL
- en: The formula to calculate TPR is TPR = TP / (TP + FN)
  prefs: []
  type: TYPE_NORMAL
- en: TPR is also referred to as **recall** or **sensitivity**.
  prefs: []
  type: TYPE_NORMAL
- en: False positive rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **false positive rate** (**FPR**) is the ratio of false positives (the number
    of negative instances incorrectly predicted as positive by our model) to the sum
    of false positives and true negatives (the number of negative instances correctly
    predicted by the model). In other words, it’s the proportion of actual negatives
    that are incorrectly identified as positive.
  prefs: []
  type: TYPE_NORMAL
- en: The formula to calculate FPR is FPR = FP / (FP + TN)
  prefs: []
  type: TYPE_NORMAL
- en: TPR is also referred to as **fall out**.
  prefs: []
  type: TYPE_NORMAL
- en: True negative rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **true negative rate** (**TNR**) is described very similarly to how we described
    the TPR, just with positive and negative switched around. That is, the TNR represents
    a count of all of the data points in our dataset that our model correctly predicted
    as negative compared against all of the data points in the dataset that are negative,
    including any data points that our model erroneously predicted to be positive
    (i.e., our model said they were positive, even though they were negative, which
    means it was a false positive).
  prefs: []
  type: TYPE_NORMAL
- en: The formula to calculate TNR is TNR = TN / (TN + FP)
  prefs: []
  type: TYPE_NORMAL
- en: TPR is also referred to as **specificity**.
  prefs: []
  type: TYPE_NORMAL
- en: False negative rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **false negative rate** (**FNR**) is described very similarly to how we
    described the FPR, just with positive and negative switched around. It is the
    ratio of wrongly predicted negative observations to the actual positives. In other
    words, it’s the proportion of actual positives that are incorrectly identified
    as negative.
  prefs: []
  type: TYPE_NORMAL
- en: The formula to calculate FNR is FNR = FN / (FN + TP)
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Precision is the ratio of correctly predicted positive observations to the total
    predicted positives. In other words, out of all the instances the model predicted
    as positive, how many were actually positive?
  prefs: []
  type: TYPE_NORMAL
- en: The formula to calculate FNR is P = TP / (TP + FP)
  prefs: []
  type: TYPE_NORMAL
- en: When I first started learning all of this stuff, I wondered why there were so
    many different metrics for measuring slightly different aspects of binary classification
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are at least a couple of reasons for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical basis**: These metrics are natural outcomes from statistical
    analysis in binary classification use cases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trial and error**: Each metric may be more important than the others, based
    on the desired outcomes of the use case'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the following example for the second point. If you’re trying to predict
    credit card fraud, you may want to maximize the sensitivity of your model, which
    would reduce the number of false negatives as much as possible, even if that ends
    up causing more false positives. In other words, it’s better to accidentally flag
    a transaction as fraudulent even if it’s not fraudulent than to accidentally allow
    a fraudulent transaction to occur.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you’re creating a spam filter, you would probably prefer
    allowing a few spam emails to accidentally reach your inbox (false negatives)
    than having valid emails flagged as spam (false positives).
  prefs: []
  type: TYPE_NORMAL
- en: The preceding metrics are often too simple to use independently, and you will
    therefore usually want to find a more complex combination of metrics that provides
    a more balanced outcome. Even in the credit card fraud use case, too many false
    positives would be disruptive and frustrating for credit card customers. The balance
    depends on the threshold you specify (between 0 and 1) for determining whether
    something is positive or negative. For example, a low threshold results in more
    positives, while a higher threshold results in more negatives. This brings us
    to more advanced metrics such as F1 score and AUC ROC, which we describe next.
  prefs: []
  type: TYPE_NORMAL
- en: F1 score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'F1 score is defined as the **harmonic mean** of precision and recall, which
    is calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: F1 = 2 * (precision * recall) / (precision + recall)
  prefs: []
  type: TYPE_NORMAL
- en: The F1 score is particularly useful when you care more about the positive class
    and you want to balance precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: AUC ROC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand AUC ROC, let’s first break down its name. The **receiver operating
    characteristic** (**ROC**) is a pretty fancy name for a curve that is generated
    by plotting the TPR against the FPR at various classification threshold settings,
    as depicted in *Figure 8**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: AUC ROC](img/B18143_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: AUC ROC'
  prefs: []
  type: TYPE_NORMAL
- en: The **area under the curve** (**AUC**) is a measure of the entire two-dimensional
    area underneath the ROC curve from (0, 0) to (1, 1), as represented by the blue
    area in *Figure 8**.2*. The AUC provides an aggregate measure of performance across
    all possible classification thresholds, and the objective is to maximize the area
    under the curve, so in the best possible scenario, the curve would stretch up
    into the top-left corner, filling up the entire graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at how to interpret the AUC ROC score values in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: An AUC ROC score of 1.0 means that the model is able to perfectly distinguish
    between all the positive and the negative data points correctly, in which case
    it has no false negatives and no false positives (i.e., no mistakes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AUC ROC score of 0.5 means that the model is not able to accurately distinguish
    between positive and negative data points and performs no better than random guessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AUC ROC score of less than 0.5 means that the model is performing worse than
    random guessing, predicting negatives as positives and positives as negatives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding these metrics is important because they are generally what our
    ML algorithms are trying to optimize. In the next section, we will discuss hyperparameters
    and hyperparameter tuning and we will see that these objective metrics form the
    fundamental goal of our tuning jobs.
  prefs: []
  type: TYPE_NORMAL
- en: What are hyperparameters?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035), hyperparameters
    are parameters that define aspects of how our model training jobs run. They are
    not the parameters in the dataset from which our models learn but rather external
    configuration options related to how the model training process is executed. They
    influence how the resulting models perform and they represent higher-level properties
    of the model, such as its complexity or how quickly it should learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are examples of hyperparameters that we’ve already discussed
    in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: In our [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035) discussion of hyperparameters,
    we covered examples such as learning rate and the number of epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168), we configured the number
    of clusters as a hyperparameter for our K-means algorithm and we configured hyperparameters
    for our tree-based models, such as the maximum depth of our trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We talked about regularization in [*Chapter 7*](B18143_07.xhtml#_idTextAnchor215),
    and regularization parameters are another example of hyperparameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many more types of hyperparameters for different kinds of algorithms,
    and we will encounter more as we progress through this book.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we know what kinds of hyperparameters to use and what their values should
    be? Hyperparameters can be chosen based on domain knowledge, experience, or trial
    and error, but to most efficiently choose the best hyperparameters, we can use
    a process called hyperparameter optimization, or hyperparameter tuning, which
    is a systematic process that can be implemented via different mechanisms that
    we will discuss next. Ultimately, the goal of hyperparameter optimization is to
    tune the hyperparameters of a model to achieve the best performance as measured
    by running it against a validation set, which is a subset of our source dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Methods for optimizing hyperparameter values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035), we described hyperparameter
    tuning mechanisms such as grid search, random search, and Bayesian optimization,
    summarized here as a quick refresher:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Grid search**: This is an exhaustive search of the entire hyperparameter
    space (i.e., it tries out every possible combination of all hyperparameter values).
    This is usually impractical and unnecessarily computationally expensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random search**: The random search approach uses a subsampling technique
    in which hyperparameter values are selected at random for each training job experiment.
    This will not result in all possible values of every hyperparameter being tested,
    but it can often be quite an efficient method for finding an effective set of
    hyperparameter values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian optimization**: This uses an optimization algorithm, and it is something
    that is provided as a managed service in Google Cloud Vertex AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some additional hyperparameter tuning mechanisms that exist
    in the industry:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient-based optimization**: This method uses Gradient Descent, which we’ve
    already covered in depth earlier in this book. These methods are often used when
    training neural networks. We provide a separate section later in this book that
    describes how to train neural networks in detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evolutionary algorithms**: These are **population-based** optimization algorithms
    loosely modeled on the process of evolutionary natural selection. The term “population-based”
    refers to the practice of building a pool (or population) of potential candidates.
    In this context, each candidate in the population represents a different set of
    hyperparameters, and candidates are evaluated based on their validation performance.
    The best-performing ones are then selected to produce “offspring” for the next
    generation. These algorithms are also more likely to be used for advanced use
    cases such as neural networks, where the hyperparameter search space can be large
    and complex, and it can be expensive to evaluate the performance of individual
    solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated machine learning** (**AutoML**) **systems**: We discussed the process
    of AutoML in previous chapters. It can be used to automate the entire ML lifecycle,
    including hyperparameter tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In any case, the general tuning process works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split your source dataset into three subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training dataset**: Used to train the model'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Validation dataset**: Used to evaluate each combination of hyperparameters
    during the tuning process'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test dataset**: Used to test the final model'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select which type of machine learning model we want to create (e.g., linear
    regression, decision tree, neural network). This determines which specific hyperparameters
    can be tuned.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set an initial range or grid of hyperparameters and values. This can be based
    on domain knowledge or research or we could just start with a random broad range
    and refine it over time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a method for searching through the model’s hyperparameter space (e.g.,
    random search, Bayesian optimization).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each combination of hyperparameters, fit the model to the training data
    and evaluate its performance by testing it against the validation data and measuring
    the appropriate objective metrics for the chosen type of model (e.g., MSE for
    regression, AUC ROC for binary classification).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all combinations have been evaluated, choose the combination of hyperparameter
    values that resulted in the best model performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a final model using those hyperparameters and test the resulting model
    using the `test` dataset to confirm the model’s ability to generalize to unseen
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that finding the best set of hyperparameters and values could require iterating
    through the outlined steps hundreds or even thousands of times, which would be
    extremely time-consuming or potentially impossible to perform manually. This is
    why hyperparameter tuning jobs, which automate the steps, are often required.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered many of the important theoretical concepts related to
    hyperparameter tuning, it’s time for us to shift our focus to the practical implementation
    of these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hands-on: performing hyperparameter tuning in Vertex AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Considering that Google Cloud Vertex AI provides tools that make it easy for
    us to implement every step in the data science project lifecycle, this gives us
    the perfect environment to put our knowledge into practice and start implementing
    hyperparameter tuning jobs. In fact, as we mentioned previously, Vertex AI provides
    a tool called Vizier that is specialized for the purpose of automating hyperparameter
    tuning jobs, which we will dive into in more detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Vizier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vertex AI Vizier is a service in Google Cloud that automates the hyperparameter
    tuning process that we outlined in the previous section of this chapter. In this
    section, we discuss some terminology used by the Vertex AI Vizier service and
    we describe some details on how it works. Then, we will actually use it in our
    hands-on activities to implement hyperparameter tuning jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Vizier terminology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Cloud uses some terminology that is specific to the Vertex AI Vizier
    service. We will briefly describe some important terms here and relate them back
    to the generic concepts we covered earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Studies, study configurations, and trials
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Vertex AI Vizier, a **study** represents the overall objective we are trying
    to achieve and all of the steps and other details involved in working towards
    that objective. For example, if we look at the general tuning process steps we
    outlined in the *Methods for optimizing hyperparameter values* section of this
    chapter, a study encapsulates all of those steps. A **study configuration** is
    the actual configuration object that contains all of the details of our study,
    such as the objective metric that we want the study to optimize, what parameters
    to test, and what kind of parameter search method to use.
  prefs: []
  type: TYPE_NORMAL
- en: A **trial** is an individual experiment in our study, or a single iteration
    in the tuning process (i.e., a single training and evaluation job that uses a
    specific set of hyperparameter values). A study will run many trials when working
    toward our specified objective.
  prefs: []
  type: TYPE_NORMAL
- en: After you create a study, Vertex AI Vizier will start running trials on its
    own. In each test, a different set of hyperparameters will be used. Vertex AI
    Vizier will keep track of the results of each run and use this knowledge to choose
    the best set of hyperparameters (it will automatically stop running trials when
    it has found the best set of hyperparameters). Vizier will also summarize all
    of the trials and rank them according to which ones performed best with regard
    to the objective metric. Then, we can train our ML model with the hyperparameters
    from the top-ranking trial.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve gotten the terminology covered, let’s dive into the hands-on
    activities!
  prefs: []
  type: TYPE_NORMAL
- en: Use case and dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will develop an XGBoost model to detect credit card fraud
    using the `Credit Card Fraud Detection` dataset available on Kaggle ([https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’re going to use Jupyter Notebook for the hands-on activities in this chapter,
    and we’re going to customize the contents of the notebook, so we will use a `user-managed`
    notebook instance. We can use the same Vertex AI Workbench user-managed notebook
    instance that we created in [*Chapter 7*](B18143_07.xhtml#_idTextAnchor215). Please
    open JupyterLab on that notebook instance. In the directory explorer on the left
    side of the screen, navigate to the `Chapter-08` directory and open the `vizier-hpo.ipynb`
    notebook. You can choose `Python (Local)` as the kernel. Again, you can run each
    cell in the notebook by selecting the cell and pressing *Shift* + *Enter* on your
    keyboard. In addition to the relevant code, the notebook contains markdown text
    that describes what the code is doing.
  prefs: []
  type: TYPE_NORMAL
- en: How our hyperparameter tuning job works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using Vertex AI Vizier for hyperparameter tuning involves several steps that
    we implement in the model. Let’s take a look at the salient steps in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a training application, which consists of a Python script that
    trains our model with the given hyperparameters. This script must also track and
    report the performance of the model when testing it on the validation set so that
    Vertex AI Vizier can use those performance metrics to determine the best hyperparameters.
    For this reason, we use the `cloudml-hypertune` Python library in our code to
    periodically report the hyperparameter tuning metric back to Vertex AI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we create a configuration object for the hyperparameter tuning job, which
    specifies the hyperparameters to tune and the range of their possible values to
    try, as well as the objective metric we want to optimize (in our case, we’re using
    AUC ROC, referred to simply as `auc` in the code). One important thing to note
    at this point is that the more hyperparameters we include, the more combinations
    of trials will need to be run. This could result in additional time and computing
    resources (and therefore cost) being needed for our tuning job. For this reason,
    it is best to use domain knowledge wherever possible to determine which hyperparameters
    we want the tuning job to focus on. We can also use the `maxTrials` variable in
    the configuration for the hyperparameter tuning job to control the number of trials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understandably, it’s not always possible to use domain knowledge to narrow the
    parameter search space, and we often will need to find a trade-off between the
    quality of our hyperparameter tuning job outputs and the time and costs required
    to run them. For example, running the tuning job for a very long time may get
    us as close as possible to finding the perfect set of hyperparameter values, but
    running it for a shorter time may get us results that are just good enough, depending
    on the needs of our use case.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The final stage in our hyperparameter tuning implementation is to use the Vertex
    AI Vizier client library to submit the hyperparameter tuning job to Vertex AI,
    which then runs our training application with different sets of hyperparameter
    values and finds the best ones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the results of our hyperparameter tuning job
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Of course, we’re not just running hyperparameter tuning jobs for fun (although
    it is also fun)! When our tuning job finds the best set of hyperparameters, we
    will want to access and review them, and usually, we will want to then use them
    to train the final version of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the results via the Google Cloud console
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When we run the hyperparameter tuning job in the notebook, the output from
    our code will display a link that will enable us to view the status of the tuning
    job in the Google Cloud console. The most important thing to view at that link
    is the list of trials performed by our tuning job, which will look similar to
    *Figure 8**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: Hyperparameter tuning trials](img/B18143_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Hyperparameter tuning trials'
  prefs: []
  type: TYPE_NORMAL
- en: In the list of our hyperparameter tuning trials in the Google Cloud console,
    we can see the AUC metric for each trial, as well as the related trial ID (depicted
    within the box on the left in *Figure 8**.3*), and we can also see the hyperparameter
    values that were used for each trial (depicted within the box on the right in
    *Figure 8**.3*). We can click the arrow symbol in the header of the `auc` column
    to sort that column by ascending or descending AUC score. In our case, we want
    to sort it in descending order because we want the maximum score to appear at
    the top. This then tells us which trial had the hyperparameters that resulted
    in the best-performing model. In *Figure 8**.3*, you may notice that at least
    the top five trials all have the same AUC score. This is common because there
    may be multiple different combinations of hyperparameter values that can result
    in the same metric score. You can use the arrow in the bottom-right of the screen
    to peruse through the pages of additional trials, and you will see other trials
    that resulted in lower AUC scores.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the results programmatically
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While it’s useful and interesting to view the results of our hyperparameter
    tuning jobs in the Google Cloud console, we likely won’t want to have to manually
    copy and paste them into the final training job to create our resulting model.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we can access all of those details programmatically via the Vertex
    API, and we can use the Vertex client library to do that from our development
    environment. In our notebook, after the tuning job finishes, we can continue with
    the additional activities in that notebook, which will show you how to access
    and use the best set of hyperparameter values produced by our tuning job. We then
    use those hyperparameter values to train a new model in our notebook, and then
    we finally test that model against the test dataset and calculate and display
    the resulting final AUC score. Note that when I ran this, I got a ROC-AUC score
    of 0.9188, which is pretty good!
  prefs: []
  type: TYPE_NORMAL
- en: Great job, you have now learned quite a lot about the topic of hyperparameter
    tuning and you should be ready to start applying what you’ve learned to other
    types of ML problems. Let’s summarize what we’ve learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we dived deeper into the important concept of objective metrics
    in machine learning. We covered, in detail, many of the most popular metrics that
    are used for evaluating binary classification models, such as precision, recall,
    F1 score, and ROC AUC. We then moved on to discuss hyperparameter optimization,
    including some of the important theoretical information in this area, such as
    the different types of methods that can be used to search for the optimal set
    of hyperparameters and associated values. This also provided some insight into
    why it can be very difficult or even impossible to efficiently perform hyperparameter
    tuning manually due to the large number of trials that can be required.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we dived into the Google Cloud Vertex AI Vizier service, which can be
    used to automate the hyperparameter tuning process for us. We then performed hands-on
    activities in Jupyter Notebook on Vertex AI, and we used Vizier to automatically
    find the best set of hyperparameters for training a credit card fraud detection
    model using XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we used the outputs of our hyperparameter tuning job to train a final
    version of our model, and we then evaluated that model against our test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, begin exploring beyond the simpler machine-learning algorithms
    such as linear regression and decision trees, and delve into the realm of Artificial
    Neural Networks (ANNs). Let's move on and discover this fascinating category of
    concepts and technologies.
  prefs: []
  type: TYPE_NORMAL
