["```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n    from sklearn.svm import LinearSVC\n    from scipy.stats import uniform\n    from sklearn.impute import SimpleImputer\n    from sklearn.pipeline import make_pipeline\n    from sklearn.compose import ColumnTransformer\n    from sklearn.feature_selection import RFECV\n    from sklearn.inspection import DecisionBoundaryDisplay\n    from sklearn.model_selection import cross_validate, \\\n      RandomizedSearchCV, RepeatedStratifiedKFold\n    import sklearn.metrics as skmet\n    import seaborn as sns\n    import os\n    import sys\n    sys.path.append(os.getcwd() + \"/helperfunctions\")\n    from preprocfunc import OutlierTrans\n    ```", "```py\nnbagames = pd.read_csv(\"data/nbagames2017plus.csv\", parse_dates=['GAME_DATE'])\nnbagames = \\\n  nbagames.loc[nbagames.WL_HOME.isin(['W','L'])]\nnbagames.shape\n(4568, 149)\nnbagames['WL_HOME'] = \\\n  np.where(nbagames.WL_HOME=='L',0,1).astype('int')\nnbagames.WL_HOME.value_counts(dropna=False)\n1    2586\n0    1982\nName: WL_HOME, dtype: int64\n```", "```py\n    num_cols = ['FG_PCT_HOME','FTA_HOME','FG3_PCT_HOME',\n      'FTM_HOME','FT_PCT_HOME','OREB_HOME','DREB_HOME',\n      'REB_HOME','AST_HOME','STL_HOME','BLK_HOME',\n      'TOV_HOME','FG_PCT_AWAY','FTA_AWAY','FG3_PCT_AWAY',\n      'FT_PCT_AWAY','OREB_AWAY','DREB_AWAY','REB_AWAY',\n      'AST_AWAY','STL_AWAY','BLK_AWAY','TOV_AWAY']\n    cat_cols = ['SEASON']\n    ```", "```py\n    nbagames[['WL_HOME'] + num_cols].agg(['count','min','median','max']).T\n                      count     min     median  max\n    WL_HOME           4,568     0.00    1.00    1.00\n    FG_PCT_HOME       4,568     0.27    0.47    0.65\n    FTA_HOME          4,568     1.00    22.00   64.00\n    FG3_PCT_HOME      4,568     0.06    0.36    0.84\n    FTM_HOME          4,568     1.00    17.00   44.00\n    FT_PCT_HOME       4,568     0.14    0.78    1.00\n    OREB_HOME         4,568     1.00    10.00   25.00\n    DREB_HOME         4,568     18.00   35.00   55.00\n    REB_HOME          4,568     22.00   45.00   70.00\n    AST_HOME          4,568     10.00   24.00   50.00\n    .........\n    FT_PCT_AWAY       4,568     0.26    0.78    1.00\n    OREB_AWAY         4,568     0.00    10.00   26.00\n    DREB_AWAY         4,568     18.00   34.00   56.00\n    REB_AWAY          4,568     22.00   44.00   71.00\n    AST_AWAY          4,568     9.00    24.00   46.00\n    STL_AWAY          4,568     0.00    8.00    19.00\n    BLK_AWAY          4,568     0.00    5.00    15.00\n    TOV_AWAY          4,568     3.00    14.00   30.00\n    ```", "```py\n    corrmatrix = nbagames[['WL_HOME'] + \\\n      num_cols].corr(method=\"pearson\")\n    sns.heatmap(corrmatrix, \n      xticklabels=corrmatrix.columns,\n      yticklabels=corrmatrix.columns, cmap=\"coolwarm\")\n    plt.title('Heat Map of Correlation Matrix')\n    plt.tight_layout()\n    plt.show()\n    ```", "```py\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nbagames[num_cols + cat_cols],\\\n      nbagames[['WL_HOME']], test_size=0.2, random_state=0)\n    ```", "```py\n    ohe = OneHotEncoder(drop='first', sparse=False)\n    cattrans = make_pipeline(ohe)\n    standtrans = make_pipeline(OutlierTrans(2),\n      SimpleImputer(strategy=\"median\"), StandardScaler())\n    coltrans = ColumnTransformer(\n      transformers=[\n        (\"cat\", cattrans, cat_cols),\n        (\"stand\", standtrans, num_cols)\n      ]\n    )\n    ```", "```py\npipe0 = make_pipeline(OutlierTrans(2),\n  SimpleImputer(strategy=\"median\"), StandardScaler())\nX_train_enc = pipe0.\\\n  fit_transform(X_train[['FG_PCT_HOME','DREB_HOME']])\ndef dispbound(model, X, xvarnames, y, title):\n  dispfit = model.fit(X,y)\n  disp = DecisionBoundaryDisplay.from_estimator(\n    dispfit, X, response_method=\"predict\",\n    xlabel=xvarnames[0], ylabel=xvarnames[1],\n    alpha=0.5,\n  )\n  scatter = disp.ax_.scatter(X[:,0], X[:,1],\n    c=y, edgecolor=\"k\")\n\n  disp.ax_.set_title(title)\n  legend1 = disp.ax_.legend(*scatter.legend_elements(),\n    loc=\"lower left\", title=\"Home Win\")\n  disp.ax_.add_artist(legend1)\ndispbound(LinearSVC(max_iter=1000000,loss='hinge'),\n  X_train_enc, ['FG_PCT_HOME','DREB_HOME'],\n  y_train.values.ravel(),\n  'Linear SVC Decision Boundary')\n```", "```py\n    svc = LinearSVC(max_iter=1000000, loss='hinge',\n       random_state=0)\n    rfecv = RFECV(estimator=svc, cv=5)\n    pipe1 = make_pipeline(coltrans, rfecv, svc)\n    pipe1.fit(X_train, y_train.values.ravel())\n    ```", "```py\n    new_cat_cols = \\\n      pipe1.named_steps['columntransformer'].\\\n      named_transformers_['cat'].\\\n      named_steps['onehotencoder'].\\\n      get_feature_names(cat_cols)\n    new_cols = np.concatenate((new_cat_cols, np.array(num_cols)))\n    sel_cols = new_cols[pipe1['rfecv'].get_support()]\n    np.set_printoptions(linewidth=55)\n    sel_cols\n    array(['SEASON_2018', 'SEASON_2019', 'SEASON_2020',\n           'FG_PCT_HOME', 'FTA_HOME', 'FG3_PCT_HOME',\n           'FTM_HOME', 'FT_PCT_HOME', 'OREB_HOME',\n           'DREB_HOME', 'REB_HOME', 'AST_HOME',\n           'TOV_HOME', 'FG_PCT_AWAY', 'FTA_AWAY',\n           'FG3_PCT_AWAY', 'FT_PCT_AWAY', 'OREB_AWAY',\n           'DREB_AWAY', 'REB_AWAY', 'AST_AWAY',\n           'BLK_AWAY', 'TOV_AWAY'], dtype=object)\n    ```", "```py\n    pd.Series(pipe1['linearsvc'].\\\n      coef_[0], index=sel_cols).\\\n      sort_values(ascending=False)\n    FG_PCT_HOME     2.21\n    TOV_AWAY        1.20\n    REB_HOME        1.19\n    FTM_HOME        0.95\n    FG3_PCT_HOME    0.94\n    FT_PCT_HOME     0.31\n    AST_HOME        0.25\n    OREB_HOME       0.18\n    DREB_AWAY       0.11\n    SEASON_2018     0.10\n    FTA_HOME       -0.05\n    BLK_AWAY       -0.07\n    SEASON_2019    -0.11\n    SEASON_2020    -0.19\n    AST_AWAY       -0.44\n    OREB_AWAY      -0.47\n    DREB_HOME      -0.49\n    FT_PCT_AWAY    -0.53\n    REB_AWAY       -0.63\n    FG3_PCT_AWAY   -0.80\n    FTA_AWAY       -0.81\n    TOV_HOME       -1.19\n    FG_PCT_AWAY    -1.91\n    dtype: float64\n    ```", "```py\n    pred = pipe1.predict(X_test)\n    print(\"accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f\"  %\n      (skmet.accuracy_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred, pos_label=0),\n      skmet.precision_score(y_test.values.ravel(), pred)))\n    accuracy: 0.93, sensitivity: 0.95, specificity: 0.92, precision: 0.93\n    ```", "```py\n    kf = RepeatedStratifiedKFold(n_splits=7,n_repeats=10,\\\n      random_state=0)\n    scores = cross_validate(pipe1, X_train, \\\n      y_train.values.ravel(), \\\n      scoring=['accuracy','precision','recall','f1'], \\\n      cv=kf, n_jobs=-1)\n    print(\"accuracy: %.2f, precision: %.2f, sensitivity: %.2f, f1: %.2f\"  %\n      (np.mean(scores['test_accuracy']),\\\n      np.mean(scores['test_precision']),\\\n      np.mean(scores['test_recall']),\\\n      np.mean(scores['test_f1'])))\n    accuracy: 0.93, precision: 0.93, sensitivity: 0.95, f1: 0.94\n    ```", "```py\n    svc_params = {\n     'linearsvc__C': uniform(loc=0, scale=100)\n    }\n    rs = RandomizedSearchCV(pipe1, svc_params, cv=10, \n      scoring='accuracy', n_iter=20, random_state=0)\n    rs.fit(X_train, y_train.values.ravel())\n    rs.best_params_\n    {'linearsvc__C': 54.88135039273247}\n    rs.best_score_\n    0.9315809566584325\n    ```", "```py\n    results = \\\n      pd.DataFrame(rs.cv_results_['mean_test_score'], \\\n        columns=['meanscore']).\\\n      join(pd.DataFrame(rs.cv_results_['params'])).\\\n      sort_values(['meanscore'], ascending=False)\n    results\n                  meanscore        linearsvc__C\n    0             0.93             54.88\n    8             0.93             96.37\n    18            0.93             77.82\n    17            0.93             83.26\n    13            0.93             92.56\n    12            0.93             56.80\n    11            0.93             52.89\n    1             0.93             71.52\n    10            0.93             79.17\n    7             0.93             89.18\n    6             0.93             43.76\n    5             0.93             64.59\n    3             0.93             54.49\n    2             0.93             60.28\n    19            0.93             87.00\n    9             0.93             38.34\n    4             0.93             42.37\n    14            0.93             7.10\n    15            0.93             8.71\n    16            0.93             2.02\n    ```", "```py\n    pred = rs.predict(X_test)\n    print(\"accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f\"  %\n      (skmet.accuracy_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred, pos_label=0),\n      skmet.precision_score(y_test.values.ravel(), pred)))\n    accuracy: 0.93, sensitivity: 0.95, specificity: 0.92, precision: 0.93\n    ```", "```py\n    cm = skmet.confusion_matrix(y_test, pred)\n    cmplot = \\\n      skmet.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Loss', 'Won'])\n    cmplot.plot()\n    cmplot.ax_.set(title='Home Team Win Confusion Matrix', \n      xlabel='Predicted Value', ylabel='Actual Value')\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.pipeline import make_pipeline\n    from sklearn.svm import SVC\n    from sklearn.linear_model import LogisticRegression\n    from scipy.stats import uniform\n    from sklearn.feature_selection import RFECV\n    from sklearn.impute import SimpleImputer\n    from scipy.stats import randint\n    from sklearn.model_selection import RandomizedSearchCV\n    import sklearn.metrics as skmet\n    import os\n    import sys\n    sys.path.append(os.getcwd() + \"/helperfunctions\")\n    from preprocfunc import OutlierTrans\n    ```", "```py\nimport nbagames as ng\nfrom displayfunc import dispbound\n```", "```py\n    X_train = ng.X_train\n    X_test = ng.X_test\n    y_train = ng.y_train\n    y_test = ng.y_test\n    ```", "```py\n    pipe0 = make_pipeline(OutlierTrans(2),\n      SimpleImputer(strategy=\"median\"),\n      StandardScaler())\n    X_train_enc = \\\n      pipe0.fit_transform(X_train[['FG_PCT_HOME',\n       'DREB_HOME']])\n    dispbound(SVC(kernel='rbf', gamma=30, C=1),\n      X_train_enc,['FG_PCT_HOME','DREB_HOME'],\n      y_train.values.ravel(),\n      \"SVC with rbf kernel-gamma=30, C=1\")\n    ```", "```py\n    dispbound(SVC(kernel='poly', degree=7),\n      X_train_enc, ['FG_PCT_HOME','DREB_HOME'],\n      y_train.values.ravel(),\n      \"SVC with polynomial kernel - degree=7\")\n    ```", "```py\n    rfecv = RFECV(estimator=LogisticRegression())\n    svc = SVC()\n    pipe1 = make_pipeline(ng.coltrans, rfecv, svc)\n    ```", "```py\n    svc_params = [\n      {\n        'svc__kernel': ['rbf'],\n        'svc__C': uniform(loc=0, scale=20),\n        'svc__gamma': uniform(loc=0, scale=100)\n      },\n      {\n        'svc__kernel': ['poly'],\n        'svc__degree': randint(1, 5),\n        'svc__C': uniform(loc=0, scale=20),\n        'svc__gamma': uniform(loc=0, scale=100)\n      },\n      {\n        'svc__kernel': ['linear','sigmoid'],\n        'svc__C': uniform(loc=0, scale=20)\n      }\n    ]\n    ```", "```py\n    rs = RandomizedSearchCV(pipe1, svc_params, cv=5, \n      scoring='accuracy', n_iter=10, n_jobs=-1,\n      verbose=5, random_state=0)\n    rs.fit(X_train, y_train.values.ravel())\n    rs.best_params_\n    {'svc__C': 1.1342595463488636, 'svc__kernel': 'linear'}\n    rs.best_score_\n    0.9299405955437289\n    ```", "```py\nresults = \\\n  pd.DataFrame(rs.cv_results_['mean_test_score'], \\\n    columns=['meanscore']).\\\n  join(pd.json_normalize(rs.cv_results_['params'])).\\\n  sort_values(['meanscore'], ascending=False)\nresults\n            C          gamma     kernel     degree\nmeanscore                              \n0.93        1.13       NaN       linear     NaN\n0.89        1.42       64.82     poly       3.00\n0.89        9.55       NaN       sigmoid    NaN\n0.89        11.36      NaN       sigmoid    NaN\n0.89        2.87       75.86     poly       5.00\n0.64        12.47      43.76     poly       4.00\n0.64        15.61      72.06     poly       4.00\n0.57        11.86      84.43     rbf        NaN\n0.57        16.65      77.82     rbf        NaN\n0.57        19.57      79.92     rbf        NaN\n```", "```py\n    rs.best_estimator_['svc'].\\\n      support_vectors_.shape\n    (625, 18)\n    ```", "```py\n    pred = rs.predict(X_test)\n    print(\"accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f\"  %\n      (skmet.accuracy_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred, \n        pos_label=0),\n      skmet.precision_score(y_test.values.ravel(), pred)))\n    accuracy: 0.93, sensitivity: 0.94, specificity: 0.91, precision: 0.93\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n    from sklearn.pipeline import make_pipeline\n    from sklearn.svm import SVC\n    from scipy.stats import uniform\n    from sklearn.impute import SimpleImputer\n    from sklearn.compose import ColumnTransformer\n    from sklearn.model_selection import RandomizedSearchCV\n    import sklearn.metrics as skmet\n    import os\n    import sys\n    sys.path.append(os.getcwd() + \"/helperfunctions\")\n    from preprocfunc import OutlierTrans\n    ```", "```py\n    machinefailuretype = pd.read_csv(\"data/machinefailuretype.csv\")\n    machinefailuretype.info()\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 10000 entries, 0 to 9999\n    Data columns (total 10 columns):\n     #   Column                 Non-Null Count     Dtype\n    ---  ------                 --------------     -----  \n     0   udi                    10000 non-null     int64\n     1   product                10000 non-null     object \n     2   machinetype            10000 non-null     object \n     3   airtemp                10000 non-null     float64\n     4   processtemperature     10000 non-null     float64\n     5   rotationalspeed        10000 non-null     int64\n     6   torque                 10000 non-null     float64\n     7   toolwear               10000 non-null     int64\n     8   fail                   10000 non-null     int64\n     9   failtype               10000 non-null     object \n    dtypes: float64(3), int64(4), object(3)\n    memory usage: 781.4+ KB\n    ```", "```py\n    machinefailuretype.head()\n       udi product machinetype airtemp processtemperature\\\n    0  1   M14860       M         298        309 \n    1  2   L47181       L         298        309 \n    2  3   L47182       L         298        308 \n    3  4   L47183       L         298        309 \n    4  5   L47184       L         298        309 \n       rotationalspeed  torque  toolwear  fail  failtype  \n    0        1551         43        0       0   No Failure  \n    1        1408         46        3       0   No Failure  \n    2        1498         49        5       0   No Failure  \n    3        1433         40        7       0   No Failure  \n    4        1408         40        9       0   No Failure\n    ```", "```py\n    machinefailuretype.failtype.\\\n      value_counts(dropna=False).sort_index()\n    Heat Dissipation Failure     112\n    No Failure                   9652\n    Overstrain Failure           78\n    Power Failure                95\n    Random Failures              18\n    Tool Wear Failure            45\n    Name: failtype, dtype: int64\n    ```", "```py\n    def setcode(typetext):\n      if (typetext==\"No Failure\"):\n        typecode = 1\n      elif (typetext==\"Heat Dissipation Failure\"):\n        typecode = 2\n      elif (typetext==\"Power Failure\"):\n        typecode = 3\n      elif (typetext==\"Overstrain Failure\"):\n        typecode = 4\n      else:\n        typecode = 5\n      return typecode\n    machinefailuretype[\"failtypecode\"] = \\\n      machinefailuretype.apply(lambda x: setcode(x.failtype), axis=1)\n    ```", "```py\n    num_cols = ['airtemp','processtemperature',\n      'rotationalspeed','torque','toolwear']\n    cat_cols = ['machinetype']\n    machinefailuretype[num_cols].agg(['min','median','max']).T\n                           min        median     max\n    airtemp                295.30     300.10     304.50\n    processtemperature     305.70     310.10     313.80\n    rotationalspeed        1,168.00   1,503.00   2,886.00\n    torque                 3.80       40.10      76.60\n    toolwear               0.00       108.00     253.00\n    ```", "```py\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(machinefailuretype[num_cols + cat_cols],\\\n      machinefailuretype[['failtypecode']],\\\n      stratify=machinefailuretype[['failtypecode']], \\\n      test_size=0.2, random_state=0)\n    ```", "```py\n    ohe = OneHotEncoder(drop='first', sparse=False)\n    cattrans = make_pipeline(ohe)\n    standtrans = make_pipeline(OutlierTrans(3),\n      SimpleImputer(strategy=\"median\"),\n      MinMaxScaler())\n    coltrans = ColumnTransformer(\n      transformers=[\n        (\"cat\", cattrans, cat_cols),\n        (\"stand\", standtrans, num_cols),\n      ]\n    )\n    ```", "```py\n    svc = SVC(class_weight='balanced', probability=True)\n    pipe1 = make_pipeline(coltrans, svc)\n    ```", "```py\n    svc_params = [\n      {\n        'svc__kernel': ['rbf'],\n        'svc__C': uniform(loc=0, scale=20),\n        'svc__gamma': uniform(loc=0, scale=100),\n        'svc__decision_function_shape': ['ovr','ovo']\n      },\n      {\n        'svc__kernel': ['poly'],\n        'svc__degree': np.arange(0,6),\n        'svc__C': uniform(loc=0, scale=20),\n        'svc__gamma': uniform(loc=0, scale=100),\n        'svc__decision_function_shape': ['ovr','ovo']\n      },\n      {\n        'svc__kernel': ['linear','sigmoid'],\n        'svc__C': uniform(loc=0, scale=20),\n        'svc__decision_function_shape': ['ovr','ovo']\n      }\n    ]\n    ```", "```py\n    rs = RandomizedSearchCV(pipe1, svc_params, cv=7, scoring=\"roc_auc_ovr\", n_iter=10)\n    rs.fit(X_train, y_train.values.ravel())\n    rs.best_params_\n    {'svc__C': 5.609789456747942,\n     'svc__decision_function_shape': 'ovo',\n     'svc__gamma': 27.73459801111866,\n     'svc__kernel': 'rbf'}\n    rs.best_score_\n    0.9187636814475847\n    ```", "```py\n    results = \\\n      pd.DataFrame(rs.cv_results_['mean_test_score'], \\\n        columns=['meanscore']).\\\n      join(pd.json_normalize(rs.cv_results_['params'])).\\\n      sort_values(['meanscore'], ascending=False)\n    results\n    meanscore  svc__C svc__decision_function_shape  svc__gamma svc__kernel\n    7     0.92     5.61     ovo     27.73     rbf\n    5     0.91     9.43     ovr     NaN       linear\n    3     0.91     5.40     ovr     NaN       linear\n    0     0.90     19.84    ovr     28.70     rbf\n    8     0.87     5.34     ovo     93.87     rbf\n    6     0.86     8.05     ovr     80.57     rbf\n    9     0.86     4.41     ovo     66.66     rbf\n    1     0.86     3.21     ovr     85.35     rbf\n    4     0.85     0.01     ovo     38.24     rbf\n    2     0.66     7.61     ovr     NaN       sigmoid\n    ```", "```py\n    pred = rs.predict(X_test)\n    cm = skmet.confusion_matrix(y_test, pred)\n    cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm,\n       display_labels=['None', 'Heat','Power','Overstrain','Other'])\n    cmplot.plot()\n    cmplot.ax_.set(title='Machine Failure Type Confusion Matrix', \n      xlabel='Predicted Value', ylabel='Actual Value')\n    ```", "```py\n    print(skmet.classification_report(y_test, pred,\n      target_names=['None', 'Heat','Power', 'Overstrain', 'Other']))\n                  precision    recall  f1-score   support\n            None       0.99      0.97      0.98      1930\n            Heat       0.50      0.91      0.65        22\n           Power       0.60      0.47      0.53        19\n      Overstrain       0.65      0.81      0.72        16\n           Other       0.06      0.15      0.09        13\n        accuracy                           0.96      2000\n       macro avg       0.56      0.66      0.59      2000\n    weighted avg       0.97      0.96      0.96      2000\n    ```"]