<html><head></head><body>
<div id="_idContainer091">
<h1 class="chapter-number" id="_idParaDest-163"><a id="_idTextAnchor167"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-164"><a id="_idTextAnchor168"/><span class="koboSpan" id="kobo.2.1">MATLAB for Image Processing and Computer Vision</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Computer vision is a discipline that explores methods for processing, analyzing, and comprehending visual data. </span><span class="koboSpan" id="kobo.3.2">In the realm of image content analysis, a multitude of computer vision algorithms is employed to develop insights into the objects depicted in the image. </span><span class="koboSpan" id="kobo.3.3">Encompassing diverse facets of image analysis, computer vision addresses tasks such as object recognition, shape analysis, pose estimation, 3D modeling, visual search, and more. </span><span class="koboSpan" id="kobo.3.4">While humans excel at identifying and recognizing objects in their surroundings, the objective of computer vision is to faithfully replicate the capabilities of the </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">human visual system</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">HVS</span></strong><span class="koboSpan" id="kobo.7.1">) using </span><a id="_idIndexMarker892"/><span class="koboSpan" id="kobo.8.1">computational methods. </span><span class="koboSpan" id="kobo.8.2">In this chapter, we will understand the basic concepts of computer vision and how to implement a model for object recognition </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">using MATLAB.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.12.1">Introducing image processing and </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">computer vision</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">Exploring MATLAB tools for </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">computer vision</span></span></li>
<li><span class="koboSpan" id="kobo.16.1">Building a MATLAB model for </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">object recognition</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Training and fine-tuning pretrained deep learning models </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">in MATLAB</span></span></li>
<li><span class="koboSpan" id="kobo.20.1">Interpreting and explaining machine </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">learning models</span></span></li>
</ul>
<h1 id="_idParaDest-165"><a id="_idTextAnchor169"/><span class="koboSpan" id="kobo.22.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.23.1">In this chapter, we will introduce basic machine learning concepts. </span><span class="koboSpan" id="kobo.23.2">To understand these topics, a basic knowledge of algebra and mathematical modeling is needed. </span><span class="koboSpan" id="kobo.23.3">You will also require a working knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">of MATLAB.</span></span></p>
<p><span class="koboSpan" id="kobo.25.1">To work with the MATLAB code in this chapter, you’ll need the files available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">at </span></span><a href="https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition"><span class="No-Break"><span class="koboSpan" id="kobo.27.1">https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.28.1">.</span></span></p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor170"/><span class="koboSpan" id="kobo.29.1">Introducing image processing and computer vision</span></h1>
<p><span class="koboSpan" id="kobo.30.1">Through the five </span><a id="_idIndexMarker893"/><span class="koboSpan" id="kobo.31.1">senses, humans</span><a id="_idIndexMarker894"/><span class="koboSpan" id="kobo.32.1"> gather information from the external world and process it, making decisions to carry out the actions that shape their daily lives. </span><span class="koboSpan" id="kobo.32.2">One of the most intriguing challenges in computer science is replicating this sequence of events, identifying and harnessing new sources of information. </span><span class="koboSpan" id="kobo.32.3">The ability to acquire and interpret information by simulating the human sensory system is called machine perception, and it is fundamental in the field of </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">artificial intelligence.</span></span></p>
<p><span class="koboSpan" id="kobo.34.1">Being able to interpret and acquire information from the external world is made possible through techniques such as encoding and information processing. </span><span class="koboSpan" id="kobo.34.2">Through digital image encoding techniques, it is possible to represent what humans can perceive in the form of bits. </span><span class="koboSpan" id="kobo.34.3">Depending on the methodologies used, it is possible to select the quantity and quality of the information to be represented. </span><span class="koboSpan" id="kobo.34.4">Through processing methods, on the other hand, it is possible to interpret the information contained in images and attempt to replicate the mechanisms of human decision-making. </span><span class="koboSpan" id="kobo.34.5">One such human ability is the capacity to recognize, through sight, what types of objects are present within a scene, thereby identifying the distinctive characteristics of each object. </span><span class="koboSpan" id="kobo.34.6">The highest level of information that can be extracted from images is done through identifying and recognizing individual objects within a scene. </span><span class="koboSpan" id="kobo.34.7">This information can be used to categorize and group images based on the objects </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">they contain.</span></span></p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor171"/><span class="koboSpan" id="kobo.36.1">Understanding image processing</span></h2>
<p><span class="koboSpan" id="kobo.37.1">For humans, a fundamental</span><a id="_idIndexMarker895"/><span class="koboSpan" id="kobo.38.1"> sense is vision. </span><span class="koboSpan" id="kobo.38.2">Through digital images, it is possible to represent what the HVS captures in an instant numerically. </span><span class="koboSpan" id="kobo.38.3">An image is a 2D representation of visual perception; it is perceived in the form of electromagnetic waves that enter the eye and impact the retina. </span><span class="koboSpan" id="kobo.38.4">The elements that make up the retina capture information, such as luminance and spectral characteristics. </span><span class="koboSpan" id="kobo.38.5">These are transformed into nerve signals to be sent, through the optic nerve, to the brain structures responsible for </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">visual interpretation.</span></span></p>
<p><span class="koboSpan" id="kobo.40.1">In the digital domain, images are commonly represented as an ordered set of points and pixels, arranged in rows and columns. </span><span class="koboSpan" id="kobo.40.2">This mode of representation is called raster or bitmap. </span><span class="koboSpan" id="kobo.40.3">Essentially, it involves 2D sampling of a continuous signal in </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">two dimensions:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<span class="koboSpan" id="kobo.42.1"><img alt="Figure 8.1 – Image representation as a sequence of pixels. Each pixel has a value from 0 (black) to 255 (white)" src="image/B21156_08_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.43.1">Figure 8.1 – Image representation as a sequence of pixels. </span><span class="koboSpan" id="kobo.43.2">Each pixel has a value from 0 (black) to 255 (white)</span></p>
<p><span class="koboSpan" id="kobo.44.1">The simplest technique for representation is the use of grayscale. </span><span class="koboSpan" id="kobo.44.2">In this type of representation, pixels contain the amount of luminance. </span><span class="koboSpan" id="kobo.44.3">Luminance is a fundamental quantity in the visual field and represents the amount of light that reaches the observer’s eye. </span><span class="koboSpan" id="kobo.44.4">Pixel values range from absence (black) to the maximum level of light (white), while intermediate states are perceived as varying shades </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">of gray.</span></span></p>
<p><span class="koboSpan" id="kobo.46.1">In computer graphics, the pixel is the smallest conventional unit of a digital image’s surface. </span><span class="koboSpan" id="kobo.46.2">The more pixels an image has, the more information it contains, and consequently, our ability to notice details within it increases. </span><span class="koboSpan" id="kobo.46.3">The amount of information can be indicated by measuring resolution, either </span><a id="_idIndexMarker896"/><span class="koboSpan" id="kobo.47.1">in absolute terms (pixels) or concerning physical </span><a id="_idIndexMarker897"/><span class="koboSpan" id="kobo.48.1">measurements (</span><strong class="bold"><span class="koboSpan" id="kobo.49.1">dots per </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.50.1">inch</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.51.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.52.1">dpi</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">)).</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">When deciding to change the resolution of an image, two situations </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">can arise:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.56.1">Constant pixel size</span></strong><span class="koboSpan" id="kobo.57.1">: The number of pixels that make up the image is reduced, resulting in the image shrinking </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">in size</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.59.1">Increase in pixel size</span></strong><span class="koboSpan" id="kobo.60.1">: The pixels (dpi) are reduced in size, while the overall dimensions of the image remain constant, causing the individual pixel size </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">to increase</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.62.1">An important aspect of digital images is how to represent the information contained in their pixels. </span><span class="koboSpan" id="kobo.62.2">In the case of grayscale, it represents luminance, which needs to be quantized by representing it in a finite number of bits. </span><span class="koboSpan" id="kobo.62.3">The greater the number of bits, the lower the quantization noise. </span><span class="koboSpan" id="kobo.62.4">Using </span><em class="italic"><span class="koboSpan" id="kobo.63.1">b</span></em><span class="koboSpan" id="kobo.64.1"> bits, there are 2</span><span class="superscript"><span class="koboSpan" id="kobo.65.1">b</span></span><span class="koboSpan" id="kobo.66.1"> possible values. </span><span class="koboSpan" id="kobo.66.2">Typically, 8 bits is the most common value, allowing for a total of 256 levels of luminance to be represented. </span><span class="koboSpan" id="kobo.66.3">It has been demonstrated that 8 bits provide an acceptable representation of grayscale gradients in most applications. </span><span class="koboSpan" id="kobo.66.4">This quantity effectively adapts to the HVS’s ability to distinguish different luminance levels in </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">the image.</span></span></p>
<p><span class="koboSpan" id="kobo.68.1">This holds for grayscale images that contain only luminance information for each pixel. </span><span class="koboSpan" id="kobo.68.2">Introducing color increases the complexity of representation because it requires the use of a model to represent it. </span><span class="koboSpan" id="kobo.68.3">This model must be able to capture the chromatic information significant to the HVS and translate it into numbers. </span><span class="koboSpan" id="kobo.68.4">The goal is to obtain a vector of numbers that “summarizes” the frequencies contained in the electromagnetic wave for </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">each pixel.</span></span></p>
<p><span class="koboSpan" id="kobo.70.1">The most commonly used model in the digital domain is undoubtedly RGB. </span><span class="koboSpan" id="kobo.70.2">It is based on the combination of three chromatic components with different intensities: red, green, </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">and blue.</span></span></p>
<p><span class="koboSpan" id="kobo.72.1">These components roughly correspond to the three types of cones in the human retina. </span><span class="koboSpan" id="kobo.72.2">Therefore, it is not necessary to represent all the color information that exists in the real world, but only the information to which the HVS is sensitive. </span><span class="koboSpan" id="kobo.72.3">The information that’s carried by this electromagnetic wave corresponds to the light that hits the organs inside the retina; hence, the color corresponds to the spectrum of the electromagnetic signal. </span><span class="koboSpan" id="kobo.72.4">Therefore, to represent the signal, only the three components related to the red, green, and blue colors, known as primary colors, which coincide with the luminance in three different frequency bands, are required. </span><span class="koboSpan" id="kobo.72.5">The RGB model represents how much energy is present in the spectral bands corresponding to the primary colors and provides this information in the form of three distinct values </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">or components.</span></span></p>
<p><span class="koboSpan" id="kobo.74.1">Once you understand how it’s possible to represent a digital image numerically, it’s necessary to know</span><a id="_idIndexMarker898"/><span class="koboSpan" id="kobo.75.1"> how to process it to facilitate its representation and extract relevant information. </span><span class="koboSpan" id="kobo.75.2">Image processing techniques can leverage digital transformation algorithms that modify the pixels of the original image, resulting in a new one. </span><span class="koboSpan" id="kobo.75.3">However, they also include techniques that extract numerical or tabular values from the image, representing a particular feature of it. </span><span class="koboSpan" id="kobo.75.4">Depending on their complexity, these techniques can be placed into </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">different categories.</span></span></p>
<p><span class="koboSpan" id="kobo.77.1">The simplest processing methods are those that pertain to the transformation of </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">individual pixels:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.79.1">Grayscale conversion</span></strong><span class="koboSpan" id="kobo.80.1">: This is a</span><a id="_idIndexMarker899"/><span class="koboSpan" id="kobo.81.1"> type of transformation that allows you to transition from the RGB model to grayscale. </span><span class="koboSpan" id="kobo.81.2">There is a linear relationship between luminance and the three chromatic components of the RGB model. </span><span class="koboSpan" id="kobo.81.3">This conversion allows us to have simpler pixels to manage and process in subsequent operations. </span><span class="koboSpan" id="kobo.81.4">Each pixel will have only one value corresponding to luminance rather than three values, one for each chromatic component (R, G, B) (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.82.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.83.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">):</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer080">
<span class="koboSpan" id="kobo.85.1"><img alt="Figure 8.2 – Grayscale conversion of the Flavian ﻿amphitheatre" src="image/B21156_08_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.86.1">Figure 8.2 – Grayscale conversion of the Flavian amphitheatre</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.87.1">Thresholding</span></strong><span class="koboSpan" id="kobo.88.1">: This is a very useful transformation in the image segmentation phase, where the goal is to isolate an object of interest from the background. </span><span class="koboSpan" id="kobo.88.2">The idea is to set pixels above a certain threshold value equal to the maximum luminance intensity value, while pixels below the threshold are set to the minimum </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">intensity value.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.90.1">Aliasing</span></strong><span class="koboSpan" id="kobo.91.1">: This is an effect that makes two different signals that are indistinguishable during sampling. </span><span class="koboSpan" id="kobo.91.2">Aliasing occurs when sampling or interpolation produces a lower resolution in the image, distorting the output compared to the </span><a id="_idIndexMarker900"/><span class="koboSpan" id="kobo.92.1">original signal. </span><span class="koboSpan" id="kobo.92.2">Anti-aliasing filters can be used to correct this problem. </span><span class="koboSpan" id="kobo.92.3">In the case of a digital image, aliasing manifests as a moiré effect or a </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">wavy pattern.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.94.1">Then, there are the </span><a id="_idIndexMarker901"/><span class="koboSpan" id="kobo.95.1">direct comparison and feature extraction methods. </span><span class="koboSpan" id="kobo.95.2">The first group of techniques is used to compare two images pixel by pixel, obtaining a value that measures the discrepancy between them. </span><span class="koboSpan" id="kobo.95.3">The second, on the other hand, allows for creating a summary of the initial image and using a smaller dataset that still adequately describes the original set. </span><span class="koboSpan" id="kobo.95.4">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">see something:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.97.1">Direct comparison methods</span></strong><span class="koboSpan" id="kobo.98.1">: One of the most interesting pieces of information that can be derived from images is their degree of similarity. </span><span class="koboSpan" id="kobo.98.2">The human brain can process the information contained in visual perceptions that arrive through the retina. </span><span class="koboSpan" id="kobo.98.3">There are dedicated neurons for interpreting visual information (shape, color, motion, space, lines, and so on). </span><span class="koboSpan" id="kobo.98.4">Recognition, for example, of faces and objects, occurs only after this information is extracted and memory is accessed. </span><span class="koboSpan" id="kobo.98.5">One type of recognition methodology is the direct approach. </span><span class="koboSpan" id="kobo.98.6">This method of comparing images is also called brute force as it involves comparing each pixel in the two images. </span><span class="koboSpan" id="kobo.98.7">These methodologies apply algebraic formulas and calculate a discrepancy index that indicates the degree of similarity between two images. </span><span class="koboSpan" id="kobo.98.8">If we have two images called </span><em class="italic"><span class="koboSpan" id="kobo.99.1">image1</span></em><span class="koboSpan" id="kobo.100.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.101.1">image2</span></em><span class="koboSpan" id="kobo.102.1">, we can calculate the discrepancy index using the </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">following formula:</span></span><p class="list-inset"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.104.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.105.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.106.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.107.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.108.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.109.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.110.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.111.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.112.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.113.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.114.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.115.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.116.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.117.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.118.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.119.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.120.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.121.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.122.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.123.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.124.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.125.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.126.1">m</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.127.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.128.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.129.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.130.1">m</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.131.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.132.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.133.1">b</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.134.1">s</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.135.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.136.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.137.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.138.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.139.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.140.1">e</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.141.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.142.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.143.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.144.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.145.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.146.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.147.1">e</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.148.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.149.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.150.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.151.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.152.1">/</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.153.1">s</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.154.1">u</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.155.1">m</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.156.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.157.1">s</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.158.1">u</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.159.1">m</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.160.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.161.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.162.1">m</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.163.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.164.1">g</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.165.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.166.1">1</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.167.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.168.1">)</span></span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.169.1">Mean squared error</span></strong><span class="koboSpan" id="kobo.170.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.171.1">MSE</span></strong><span class="koboSpan" id="kobo.172.1">): This is a value that indicates, in an absolute sense, how</span><a id="_idIndexMarker902"/><span class="koboSpan" id="kobo.173.1"> similar two images are. </span><span class="koboSpan" id="kobo.173.2">This index compares the images pixel by pixel and represents the average discrepancy of these values. </span><span class="koboSpan" id="kobo.173.3">The closer the MSE value is to 0, the greater the similarity between the </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">analyzed images.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.175.1">Structural similarity index measure</span></strong><span class="koboSpan" id="kobo.176.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.177.1">SSIM</span></strong><span class="koboSpan" id="kobo.178.1">): Another method of direct comparison is </span><a id="_idIndexMarker903"/><span class="koboSpan" id="kobo.179.1">SSIM. </span><span class="koboSpan" id="kobo.179.2">The difference compared to MSE is that the latter makes estimates using absolute errors. </span><span class="koboSpan" id="kobo.179.3">SSIM, on the other hand, is a perception-based model that considers image degradation as a change in the perception of structural information. </span><span class="koboSpan" id="kobo.179.4">Instead of performing a pixel-by-pixel comparison, the algorithm divides</span><a id="_idIndexMarker904"/><span class="koboSpan" id="kobo.180.1"> the image into grids of N x N pixels. </span><span class="koboSpan" id="kobo.180.2">Within each grid, an average value of the pixels is calculated, allowing for relationships between neighboring pixels to be considered rather than the absolute value of a </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">single pixel.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.182.1">Direct comparison methodologies come with a significant cost associated with the amount of data to process as they consider all the information contained in the image. </span><span class="koboSpan" id="kobo.182.2">Through some techniques, it’s possible to reduce dimensionality. </span><span class="koboSpan" id="kobo.182.3">When there’s an excessive amount of data to process and the possibility of redundancy, a transformation can be applied, adopting a reduced representation of the data. </span><span class="koboSpan" id="kobo.182.4">This reduced representation is nothing but a set of features. </span><span class="koboSpan" id="kobo.182.5">The process that transforms the input data into the feature set is</span><a id="_idIndexMarker905"/><span class="koboSpan" id="kobo.183.1"> called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.184.1">feature extraction</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.186.1">The chosen characteristics encompass pertinent details from the input data, enabling the intended task to be accomplished by utilizing this condensed representation rather than the entirety of the original data. </span><span class="koboSpan" id="kobo.186.2">This method minimizes the cost and resources needed for an accurate depiction of a vast dataset. </span><span class="koboSpan" id="kobo.186.3">When tackling intricate data analysis, a primary hurdle involves diminishing the number of variables in play. </span><span class="koboSpan" id="kobo.186.4">Analyzing a large number of variables usually translates into high memory usage and computational power requirements. </span><span class="koboSpan" id="kobo.186.5">Moreover, when applying classification algorithms, there is a </span><a id="_idIndexMarker906"/><span class="koboSpan" id="kobo.187.1">risk of </span><strong class="bold"><span class="koboSpan" id="kobo.188.1">overfitting</span></strong><span class="koboSpan" id="kobo.189.1">. </span><span class="koboSpan" id="kobo.189.2">In this case, the model adapts too closely to the dataset that was used for learning, failing to generalize and thus losing effectiveness. </span><span class="koboSpan" id="kobo.189.3">Feature extraction is a broad term that’s used to describe methods of creating variable combinations aimed at addressing these issues while maintaining sufficient accuracy in depicting the data. </span><span class="koboSpan" id="kobo.189.4">Now</span><a id="_idIndexMarker907"/><span class="koboSpan" id="kobo.190.1"> that we’ve introduced the most widespread image processing methodologies, we can focus on how to extract knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">from images.</span></span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor172"/><span class="koboSpan" id="kobo.192.1">Explaining computer vision</span></h2>
<p><span class="koboSpan" id="kobo.193.1">Computer vision is </span><a id="_idIndexMarker908"/><span class="koboSpan" id="kobo.194.1">an interdisciplinary field of computer science and artificial intelligence that deals with the development of algorithms, models, and computer systems capable of interpreting, understanding, and analyzing visual information from images or videos. </span><span class="koboSpan" id="kobo.194.2">The main goal of computer vision is to replicate some of the capabilities of the HVS, allowing computers to perceive and understand the world around them through </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">visual data.</span></span></p>
<p><span class="koboSpan" id="kobo.196.1">This field of study focuses on a wide range of tasks, including object recognition, motion detection, pattern recognition, information extraction from images, image segmentation, 3D reconstruction, and much more. </span><span class="koboSpan" id="kobo.196.2">Computer vision has applications in a wide range of industries, including medicine, automotive, security, manufacturing, retail, entertainment, and robotics, to name </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">a few.</span></span></p>
<p><span class="koboSpan" id="kobo.198.1">Recent developments in computer vision have been driven by the use of deep neural networks, in</span><a id="_idIndexMarker909"/><span class="koboSpan" id="kobo.199.1"> particular </span><strong class="bold"><span class="koboSpan" id="kobo.200.1">convolutional neural networks</span></strong><span class="koboSpan" id="kobo.201.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.202.1">CNNs</span></strong><span class="koboSpan" id="kobo.203.1">), which have delivered exceptional results in many visual recognition tasks. </span><span class="koboSpan" id="kobo.203.2">These advances are fundamentally changing the way machines can interact with the visual world, opening up new possibilities in areas such as autonomous driving, medical diagnosis, augmented reality, and </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">much more.</span></span></p>
<p><span class="koboSpan" id="kobo.205.1">This task is the most complex in terms of abstraction. </span><span class="koboSpan" id="kobo.205.2">The most common use of this category is for object recognition. </span><span class="koboSpan" id="kobo.205.3">Conceptually, the process can be divided into two steps. </span><span class="koboSpan" id="kobo.205.4">The first step involves defining an object of interest according to a model using feature extraction techniques. </span><span class="koboSpan" id="kobo.205.5">In the second step, the object is searched for within an image. </span><span class="koboSpan" id="kobo.205.6">These types of transformations require the use of machine learning and data mining algorithms that, through a dataset, allow a model to be constructed for the object to be searched for. </span><span class="koboSpan" id="kobo.205.7">Subsequently, it can be determined whether there are pixels within the image that match </span><a id="_idIndexMarker910"/><span class="koboSpan" id="kobo.206.1">the previously created model. </span><span class="koboSpan" id="kobo.206.2">There are different approaches </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">to this:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.208.1">Shape matching</span></strong><span class="koboSpan" id="kobo.209.1">: This is an </span><a id="_idIndexMarker911"/><span class="koboSpan" id="kobo.210.1">approach that involves searching for a silhouette. </span><span class="koboSpan" id="kobo.210.2">This method allows you to measure the similarity between shapes and identify correspondences between points that belong to the contour of the object being searched for. </span><span class="koboSpan" id="kobo.210.3">The basic idea is to select </span><em class="italic"><span class="koboSpan" id="kobo.211.1">n</span></em><span class="koboSpan" id="kobo.212.1"> points on the contour of the silhouette. </span><span class="koboSpan" id="kobo.212.2">For each point, the n - 1 vectors connecting it to all the others are considered. </span><span class="koboSpan" id="kobo.212.3">A set of all these vectors forms a complex descriptor of the silhouette localized at that point. </span><span class="koboSpan" id="kobo.212.4">This set of vectors is obtained through a shape extraction process, which is part of feature extraction. </span><span class="koboSpan" id="kobo.212.5">The idea is to obtain a descriptor using these vectors and use it to identify a similar shape within other images and </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">perform classification:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer081">
<span class="koboSpan" id="kobo.214.1"><img alt="Figure 8.3 – Shape matching" src="image/B21156_08_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.215.1">Figure 8.3 – Shape matching</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.216.1">Pattern matching</span></strong><span class="koboSpan" id="kobo.217.1">: This is a technique that involves identifying a specific sequence or regularity of data (referred to as a pattern) within a large dataset. </span><span class="koboSpan" id="kobo.217.2">The field of pattern recognition involves automatically searching for regularities within data using computer algorithms and patterns to perform actions such as classifying data into different categories. </span><span class="koboSpan" id="kobo.217.3">In digital images, the identification process involves preparing a pattern and corresponding to a set of pixels that describes a specific object of interest or a part of it. </span><span class="koboSpan" id="kobo.217.4">Then, a pixel classification process is performed within the image to determine whether a group of pixels comparable to the pattern being searched for exists </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">or not.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.219.1">Feature-based object recognition</span></strong><span class="koboSpan" id="kobo.220.1">: Through this technique, it is possible to create descriptors that represent the typical characteristics of an object. </span><span class="koboSpan" id="kobo.220.2">Each object has unique features that describe it. </span><span class="koboSpan" id="kobo.220.3">If we can identify all these features within a dataset, we can assume that the object is present. </span><span class="koboSpan" id="kobo.220.4">For example, a human face can be modeled based on specific anatomical features such as the placement of eyes, nostrils, angles formed by the lips, and so on. </span><span class="koboSpan" id="kobo.220.5">The combination of these anatomical elements and the vectors that connect them forms a patch model. </span><span class="koboSpan" id="kobo.220.6">This </span><a id="_idIndexMarker912"/><span class="koboSpan" id="kobo.221.1">model represents the ordered set of elements that are sufficient to accurately describe </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">an object.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.223.1">It’s interesting to understand how object recognition technology within images has evolved and the impact it has had on society. </span><span class="koboSpan" id="kobo.223.2">One of the most fascinating cases that leverages image processing techniques is facial recognition. </span><span class="koboSpan" id="kobo.223.3">Until not long ago, this technology was commonly seen as something straight out of science fiction. </span><span class="koboSpan" id="kobo.223.4">However, in the last decade, it has </span><a id="_idIndexMarker913"/><span class="koboSpan" id="kobo.224.1">not only become a reality but also become </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">widely used.</span></span></p>
<p><span class="koboSpan" id="kobo.226.1">Now that we’ve analyzed the basic concepts connected to image processing and computer vision, let’s analyze the necessary tools to address these problems </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">in MATLAB.</span></span></p>
<h1 id="_idParaDest-169"><a id="_idTextAnchor173"/><span class="koboSpan" id="kobo.228.1">Exploring MATLAB tools for computer vision</span></h1>
<p><span class="koboSpan" id="kobo.229.1">Computer vision </span><a id="_idIndexMarker914"/><span class="koboSpan" id="kobo.230.1">encompasses the development </span><a id="_idIndexMarker915"/><span class="koboSpan" id="kobo.231.1">of algorithms, techniques, and systems that allow computers to acquire, process, analyze, and make decisions based on visual data from images and videos. </span><span class="koboSpan" id="kobo.231.2">The primary goal of computer vision is to enable machines to perform tasks that typically require human visual perception </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">and comprehension.</span></span></p>
<p><span class="koboSpan" id="kobo.233.1">Computer vision can automate various tasks that would be time-consuming or even impossible for humans to perform consistently and at scale. </span><span class="koboSpan" id="kobo.233.2">This includes tasks such as object detection, image classification, and tracking. </span><span class="koboSpan" id="kobo.233.3">When trained and configured properly, computer vision algorithms can achieve high levels of accuracy in tasks such as image recognition and segmentation. </span><span class="koboSpan" id="kobo.233.4">They don’t suffer from fatigue or distraction, leading to consistent results. </span><span class="koboSpan" id="kobo.233.5">These algorithms can process images and videos in real time or near real time, making them suitable for applications that require rapid analysis </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">and decision-making.</span></span></p>
<p><span class="koboSpan" id="kobo.235.1">Computer vision can also be applied to a wide range of industries and applications, from healthcare and automotive to agriculture and manufacturing. </span><span class="koboSpan" id="kobo.235.2">It can adapt to various domains with appropriate training. </span><span class="koboSpan" id="kobo.235.3">These systems can scale easily to handle large volumes of data and images, making them suitable for big data applications. </span><span class="koboSpan" id="kobo.235.4">In applications such as medical imaging, computer vision provides a non-invasive way to diagnose and monitor conditions without the need for </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">invasive procedures.</span></span></p>
<p><span class="koboSpan" id="kobo.237.1">In contrast, these</span><a id="_idIndexMarker916"/><span class="koboSpan" id="kobo.238.1"> algorithms heavily rely on large </span><a id="_idIndexMarker917"/><span class="koboSpan" id="kobo.239.1">datasets for training. </span><span class="koboSpan" id="kobo.239.2">Insufficient or biased training data can lead to poor performance and inaccurate results. </span><span class="koboSpan" id="kobo.239.3">Developing and fine-tuning computer vision models can be complex and time-consuming. </span><span class="koboSpan" id="kobo.239.4">It often requires expertise in machine learning, deep learning, and image processing. </span><span class="koboSpan" id="kobo.239.5">Deep learning models used in computer vision can be computationally intensive and require powerful hardware, such as GPUs, for training and inference. </span><span class="koboSpan" id="kobo.239.6">Deep learning models, especially CNNs, are often perceived as black boxes, posing challenges in interpreting the rationale behind their </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">specific decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.241.1">Computer vision systems may struggle to perform well under adverse conditions, such as poor lighting, occlusions, or variations in camera angles. </span><span class="koboSpan" id="kobo.241.2">The use of computer vision in surveillance and facial recognition has raised concerns about privacy and potential misuse. </span><span class="koboSpan" id="kobo.241.3">These algorithms may adopt biases embedded in their training data, resulting in unjust or discriminatory outcomes. </span><span class="koboSpan" id="kobo.241.4">Ensuring fairness and mitigating bias remains a persistent challenge. </span><span class="koboSpan" id="kobo.241.5">While computer vision can identify and classify objects, it often lacks a deep understanding of the context in which those objects appear, which can limit its usefulness in </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">some scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.243.1">Overall, computer vision is a powerful and rapidly evolving field with the potential to transform many industries. </span><span class="koboSpan" id="kobo.243.2">However, it’s essential to be aware of its limitations and challenges and to apply it responsibly and ethically, addressing issues related to data quality, privacy, </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">and bias.</span></span></p>
<p><span class="koboSpan" id="kobo.245.1">MATLAB provides a variety of tools and functions for computer vision tasks. </span><span class="koboSpan" id="kobo.245.2">These tools can be found in </span><a id="_idIndexMarker918"/><span class="koboSpan" id="kobo.246.1">Computer </span><a id="_idIndexMarker919"/><span class="koboSpan" id="kobo.247.1">Vision Toolbox, Image Processing Toolbox, and other related toolboxes. </span><span class="koboSpan" id="kobo.247.2">Here are some of the key MATLAB tools and functionalities for </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">computer vision:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.249.1">Computer Vision Toolbox</span></strong><span class="koboSpan" id="kobo.250.1">: This toolbox is specifically designed for computer vision tasks. </span><span class="koboSpan" id="kobo.250.2">It includes a wide range of functions and algorithms for image processing, feature extraction, object detection and recognition, 3D vision, camera calibration, </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">and more.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.252.1">Image Processing Toolbox</span></strong><span class="koboSpan" id="kobo.253.1">: While not exclusively for computer vision, this toolbox is often used in conjunction with Computer Vision Toolbox. </span><span class="koboSpan" id="kobo.253.2">It provides fundamental image processing functions such as filtering, morphological operations, and </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">image enhancement.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.255.1">Camera calibration</span></strong><span class="koboSpan" id="kobo.256.1">: MATLAB offers tools for camera calibration, which is essential for mapping 2D image points to 3D world coordinates. </span><span class="koboSpan" id="kobo.256.2">This is crucial for tasks such as 3D reconstruction and </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">object tracking.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.258.1">Object detection and recognition</span></strong><span class="koboSpan" id="kobo.259.1">: MATLAB provides functions and pretrained models for object detection and recognition. </span><span class="koboSpan" id="kobo.259.2">You can use popular deep learning models such as YOLO, SSD, and Faster R-CNN for </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">these tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.261.1">Feature extraction</span></strong><span class="koboSpan" id="kobo.262.1">: MATLAB </span><a id="_idIndexMarker920"/><span class="koboSpan" id="kobo.263.1">supports</span><a id="_idIndexMarker921"/><span class="koboSpan" id="kobo.264.1"> feature extraction techniques such as </span><strong class="bold"><span class="koboSpan" id="kobo.265.1">Scale-Invariant Feature Transform</span></strong><span class="koboSpan" id="kobo.266.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.267.1">SIFT</span></strong><span class="koboSpan" id="kobo.268.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.269.1">Speeded-Up Robust Features</span></strong><span class="koboSpan" id="kobo.270.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.271.1">SURF</span></strong><span class="koboSpan" id="kobo.272.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.273.1">Histogram of Oriented Gradients</span></strong><span class="koboSpan" id="kobo.274.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.275.1">HOG</span></strong><span class="koboSpan" id="kobo.276.1">) for object</span><a id="_idIndexMarker922"/><span class="koboSpan" id="kobo.277.1"> detection </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">and matching.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.279.1">Stereo vision</span></strong><span class="koboSpan" id="kobo.280.1">: MATLAB supports stereo vision techniques for depth estimation and 3D reconstruction from stereo </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">camera setups.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.282.1">Motion analysis</span></strong><span class="koboSpan" id="kobo.283.1">: You can perform motion analysis tasks such as optical flow estimation, tracking, and motion segmentation using </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">MATLAB functions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.285.1">Machine learning and deep learning</span></strong><span class="koboSpan" id="kobo.286.1">: MATLAB integrates with various machine learning and deep learning frameworks, making it suitable for training custom models for computer </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">vision tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.288.1">Semantic segmentation</span></strong><span class="koboSpan" id="kobo.289.1">: MATLAB includes tools for semantic segmentation, which is the process of labeling each pixel in an image with the class it </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">belongs to.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.291.1">Point cloud processing</span></strong><span class="koboSpan" id="kobo.292.1">: For 3D point cloud data, MATLAB provides tools for visualization, manipulation, </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">and analysis.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.294.1">Apps</span></strong><span class="koboSpan" id="kobo.295.1">: MATLAB offers interactive apps for tasks such as image labeling, camera calibration, and object training, which simplify the </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">development workflow.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.297.1">Parallel computing</span></strong><span class="koboSpan" id="kobo.298.1">: MATLAB supports parallel computing, allowing you to speed up computationally intensive computer vision tasks by leveraging multiple CPU cores </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">or GPUs.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.300.1">These tools and functions make MATLAB a powerful environment for developing and prototyping computer </span><a id="_idIndexMarker923"/><span class="koboSpan" id="kobo.301.1">vision </span><a id="_idIndexMarker924"/><span class="koboSpan" id="kobo.302.1">applications, whether you’re working on image analysis, object detection, 3D reconstruction, or any other related tasks. </span><span class="koboSpan" id="kobo.302.2">Now, let’s learn how to recognize an object using MATLAB </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">and CNN.</span></span></p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor174"/><span class="koboSpan" id="kobo.304.1">Building a MATLAB model for object recognition</span></h1>
<p><span class="koboSpan" id="kobo.305.1">An enduring</span><a id="_idIndexMarker925"/><span class="koboSpan" id="kobo.306.1"> challenge within the realm of </span><a id="_idIndexMarker926"/><span class="koboSpan" id="kobo.307.1">computer vision involves ascertaining the presence of specific objects (object recognition) or activities within an image. </span><span class="koboSpan" id="kobo.307.2">For objects under predefined conditions, such as the identification of specific geometric shapes such as polyhedral or the recognition of faces and handwritten characters, this problem can be tackled effectively and without significant hurdles. </span><span class="koboSpan" id="kobo.307.3">However, the complexity escalates when dealing with arbitrary objects in </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">unrestricted scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.309.1">Object recognition entails the capacity to detect a particular object within a series of images or videos. </span><span class="koboSpan" id="kobo.309.2">Human beings possess the remarkable ability to identify various objects in images effortlessly, even when the objects’ appearances may vary. </span><span class="koboSpan" id="kobo.309.3">Moreover, objects can be recognized even when they are partially obscured from view. </span><span class="koboSpan" id="kobo.309.4">However, this remains a formidable challenge for </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">computer vision.</span></span></p>
<p><span class="koboSpan" id="kobo.311.1">Each object within an image exhibits a multitude of intriguing characteristics that can be extracted to construct a comprehensive description of the object. </span><span class="koboSpan" id="kobo.311.2">This description serves to identify the object when seeking it within a test image containing multiple objects. </span><span class="koboSpan" id="kobo.311.3">Crucially, the set of characteristics that are extracted from the reference image must be resilient to variations in image scale, disturbances, lighting conditions, and geometric distortions</span><a id="_idIndexMarker927"/><span class="koboSpan" id="kobo.312.1"> to</span><a id="_idIndexMarker928"/><span class="koboSpan" id="kobo.313.1"> ensure dependable recognition. </span><span class="koboSpan" id="kobo.313.2">CNNs excel in this endeavor, offering algorithms with exceptional object </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">recognition performance.</span></span></p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.315.1">Introducing handwriting recognition (HWR)</span></h2>
<p><span class="koboSpan" id="kobo.316.1">HWR encompasses</span><a id="_idIndexMarker929"/><span class="koboSpan" id="kobo.317.1"> the computer’s capability to receive and comprehend handwritten input, transforming it into readable text. </span><span class="koboSpan" id="kobo.317.2">This input can originate from various sources, including paper documents, photographs, and touchscreens. </span><span class="koboSpan" id="kobo.317.3">Detection of written text can be accomplished through optical scanning, which involves </span><strong class="bold"><span class="koboSpan" id="kobo.318.1">optical character recognition</span></strong><span class="koboSpan" id="kobo.319.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.320.1">OCR</span></strong><span class="koboSpan" id="kobo.321.1">), or through intelligent word </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">recognition </span></span><span class="No-Break"><a id="_idIndexMarker930"/></span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">techniques.</span></span></p>
<p><span class="koboSpan" id="kobo.324.1">We have long been acutely aware of the challenge of automating HWR to facilitate smoother interactions between humans and machines. </span><span class="koboSpan" id="kobo.324.2">In recent years, this challenge has witnessed intriguing advancements and increasingly efficient solutions, primarily fueled by substantial economic interest and the growing computational capabilities of modern computers. </span><span class="koboSpan" id="kobo.324.3">Notably, certain countries, such as Japan and various other Asian nations, have made significant investments in research and financial resources to pioneer cutting-edge </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">OCR technologies.</span></span></p>
<p><span class="koboSpan" id="kobo.326.1">The rationale behind the enthusiasm of these countries in this research domain is quite evident. </span><span class="koboSpan" id="kobo.326.2">Their goal is to develop devices capable of interpreting the intricate ideograms that characterize their respective cultures, thereby enhancing the ease of interaction with machines. </span><span class="koboSpan" id="kobo.326.3">Given that there are currently no input devices, such as keyboards, capable of representing thousands of characters, the focus is on acquiring this information directly from handwritten scripts through </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">digitized scanning.</span></span></p>
<p><span class="koboSpan" id="kobo.328.1">Nevertheless, even in Western countries, substantial attention has been dedicated to the field of optical HWR. </span><span class="koboSpan" id="kobo.328.2">There exist numerous applications that stand to benefit from automated text interpretation. </span><span class="koboSpan" id="kobo.328.3">Consider, for instance, the automatic parsing of preprinted templates or the recognition of addresses and postal codes on envelopes, which are just a few instances where OCR technology </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">proves invaluable.</span></span></p>
<p><span class="koboSpan" id="kobo.330.1">HWR is accomplished using diverse techniques that typically involve OCR. </span><span class="koboSpan" id="kobo.330.2">Nevertheless, a comprehensive script recognition system goes beyond OCR and encompasses tasks such as formatting, accurate character segmentation, and identifying the most </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">probable words.</span></span></p>
<p><span class="koboSpan" id="kobo.332.1">To better understand how HWR can be approached using machine learning methodologies, we will use a very popular dataset that’s largely used in the community to address this type of topic. </span><span class="koboSpan" id="kobo.332.2">This is the </span><strong class="bold"><span class="koboSpan" id="kobo.333.1">Modified National Institute of Standards and Technology</span></strong><span class="koboSpan" id="kobo.334.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.335.1">MNIST</span></strong><span class="koboSpan" id="kobo.336.1">), a large</span><a id="_idIndexMarker931"/><span class="koboSpan" id="kobo.337.1"> database of handwritten digits. </span><span class="koboSpan" id="kobo.337.2">This dataset consists of 70,000 data examples, which is a subset of a larger dataset maintained by NIST. </span><span class="koboSpan" id="kobo.337.3">These examples represent digits and are in a format of 28 x 28-pixel resolution, organized as a matrix with 70,000 rows and 785 columns. </span><span class="koboSpan" id="kobo.337.4">In each row, there are 784 columns corresponding to pixel values from the 28 x 28 matrix, and one column containing the actual</span><a id="_idIndexMarker932"/><span class="koboSpan" id="kobo.338.1"> digit label. </span><span class="koboSpan" id="kobo.338.2">These digits have been size-normalized and positioned at the center of a </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">fixed-size image.</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">The digit images within the MNIST dataset were originally chosen and processed by Chris Burges and Corinna Cortes, who employed bounding box normalization and centering techniques. </span><span class="koboSpan" id="kobo.340.2">Yann LeCun’s version of the dataset, on the other hand, utilizes centering based on the center of mass within a </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">larger window.</span></span></p>
<p><span class="koboSpan" id="kobo.342.1">This dataset is already available in the MATLAB environment, in a short version with only 10,000 images evenly distributed over the 10 digits (0-9). </span><span class="koboSpan" id="kobo.342.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">get started:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.344.1">First, we will import the dataset into the MATLAB workspace. </span><span class="koboSpan" id="kobo.344.2">The dataset is available in the standard MATLAB installation with Deep Learning Toolbox. </span><span class="koboSpan" id="kobo.344.3">Under our MATLAB installation folder, we will find the </span><strong class="source-inline"><span class="koboSpan" id="kobo.345.1">toolbox\nnet\nndemos\nndatasets\DigitDataset</span></strong><span class="koboSpan" id="kobo.346.1"> path. </span><span class="koboSpan" id="kobo.346.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.347.1">DigitDataset</span></strong><span class="koboSpan" id="kobo.348.1"> folder contains 10 subfolders, each of which contains 1,000 images of a single digit; each folder is named after the digit it contains. </span><span class="koboSpan" id="kobo.348.2">To find the path in an automated way, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.350.1">
FolderPath = fullfile(toolboxdir('nnet'),'nndemos','nndatasets','DigitDataset');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.351.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.352.1">fullfile()</span></strong><span class="koboSpan" id="kobo.353.1"> function constructs a complete file path by combining the provided folder names and filenames. </span><span class="koboSpan" id="kobo.353.2">We have used the sequence of the folder, starting from the toolbox directory. </span><span class="koboSpan" id="kobo.353.3">The output can be in the form of a character array, a string array, or a cell array of character vectors. </span><span class="koboSpan" id="kobo.353.4">If any of the input arguments are in the form of a string array, the output will also be a string array. </span><span class="koboSpan" id="kobo.353.5">Conversely, if any of the input arguments are in the form of a cell array containing character vectors, the output will be a cell array of character vectors. </span><span class="koboSpan" id="kobo.353.6">In all other cases, it will </span><a id="_idIndexMarker933"/><span class="koboSpan" id="kobo.354.1">be a character array. </span><span class="koboSpan" id="kobo.354.2">Now that we have the path, we can extract the </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">image dataset:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.356.1">DigitData = imageDatastore(FolderPath, 'IncludeSubfolders',true,'LabelSource','foldernames');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.357.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.358.1">imageDatastore()</span></strong><span class="koboSpan" id="kobo.359.1"> function generates a data store by incorporating the dataset’s path for a specified collection of image data. </span><span class="koboSpan" id="kobo.359.2">We passed </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">three arguments:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">FolderPath</span></strong><span class="koboSpan" id="kobo.362.1">: The path of the folder containing </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">the images</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.364.1">IncludeSubfolders</span></strong><span class="koboSpan" id="kobo.365.1">: The possibility to include all the subfolders in the </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">main folder</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.367.1">LabelSource</span></strong><span class="koboSpan" id="kobo.368.1">: We used the folder names to label </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">the data</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.370.1">An </span><strong class="source-inline"><span class="koboSpan" id="kobo.371.1">ImageDatastore</span></strong><span class="koboSpan" id="kobo.372.1"> object will be created with some properties. </span><span class="koboSpan" id="kobo.372.2">These properties delineate the characteristics of the data and provide instructions on how to retrieve data from the data store. </span><span class="koboSpan" id="kobo.372.3">When creating the data store object, you have the option to set these properties using name-value pairs as arguments. </span><span class="koboSpan" id="kobo.372.4">If you wish to inspect or adjust a property after the object’s creation, you can do so using the </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">dot notation.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.374.1">Let’s look at some of </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">these properties:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.376.1">Files</span></strong><span class="koboSpan" id="kobo.377.1">: This is a cell array that stores the file paths of all the images in the data store. </span><span class="koboSpan" id="kobo.377.2">You can access this property to get a list of </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">file paths.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.379.1">Labels</span></strong><span class="koboSpan" id="kobo.380.1">: This is an array or cell array that associates labels or categories with each image in the data store. </span><span class="koboSpan" id="kobo.380.2">This property is often used for image </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">classification tasks.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.382.1">ReadSize</span></strong><span class="koboSpan" id="kobo.383.1">: This specifies the number of images to read at once during data iteration. </span><span class="koboSpan" id="kobo.383.2">This can impact memory usage </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">and performance.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.385.1">Now, we can</span><a id="_idIndexMarker934"/><span class="koboSpan" id="kobo.386.1"> display a selection of images that have been loaded via a random </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">selection process:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.388.1">figure;
RandId = randperm(10000,9);
for i = 1:9
    subplot(3,3,i);
    imshow(DigitData.Files{RandId(i)});
end</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.389.1">Here, we have used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.390.1">randperm()</span></strong><span class="koboSpan" id="kobo.391.1"> function, which generates a row vector comprising nine distinct random integers chosen from a range of 1 to 10,000. </span><span class="koboSpan" id="kobo.391.2">Each number that was generated was used as an index to identify an image file path stored in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">Data.Files</span></strong><span class="koboSpan" id="kobo.393.1"> property. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.394.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.395.1">.4</span></em><span class="koboSpan" id="kobo.396.1"> shows a collage of nine images extracted from </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">the database:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer082">
<span class="koboSpan" id="kobo.398.1"><img alt="Figure 8.4 – The MNIST dataset" src="image/B21156_08_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.399.1">Figure 8.4 – The MNIST dataset</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.400.1">Now, we can check the distribution of the images over </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">the classes:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.402.1">
ClassDist = countEachLabel(DigitData)</span></pre> <p class="list-inset"><span class="koboSpan" id="kobo.403.1">The following</span><a id="_idIndexMarker935"/><span class="koboSpan" id="kobo.404.1"> results </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">are returned:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.406.1">
ClassDist =
  10×2 table
    Label    Count
    _____    _____
      0      1000
      1      1000
      2      1000
      3      1000
      4      1000
      5      1000
      6      1000
      7      1000
      8      1000
      9      1000</span></pre> <p class="list-inset"><span class="koboSpan" id="kobo.407.1">With this, we have proof that the images are evenly distributed across the 10 </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">digits (0-9).</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.409.1">Before we start building the machine learning algorithm, it’s imperative to partition the available data into two distinct subsets. </span><span class="koboSpan" id="kobo.409.2">The first subset will serve as the training data, while the second will be earmarked for algorithm validation. </span><span class="koboSpan" id="kobo.409.3">Data partitioning plays a pivotal role in machine learning and data analysis as it involves segregating a dataset into multiple subsets for training, validation, and model testing. </span><span class="koboSpan" id="kobo.409.4">In our case, we have 10,000 samples, each containing 1,000 images for a specific digit. </span><span class="koboSpan" id="kobo.409.5">Our chosen approach is to split the data into a 70% portion for training and a 30% portion for validation. </span><span class="koboSpan" id="kobo.409.6">This split rate is used because</span><a id="_idIndexMarker936"/><span class="koboSpan" id="kobo.410.1"> most of the data must be used for training the network. </span><span class="koboSpan" id="kobo.410.2">Accordingly, we will employ 7,000 samples for training and reserve the remainder </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">for validation:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.412.1">
SplitRate = 0.7;
[TrainDat,ValDat] = splitEachLabel(DigitData,SplitRate,'randomize');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.413.1">To achieve this, we utilized the </span><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">splitEachLabel()</span></strong><span class="koboSpan" id="kobo.415.1"> function. </span><span class="koboSpan" id="kobo.415.2">This function effectively separates the image files within the </span><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">Data</span></strong><span class="koboSpan" id="kobo.417.1"> dataset into two separate data stores: </span><strong class="source-inline"><span class="koboSpan" id="kobo.418.1">TrainDat</span></strong><span class="koboSpan" id="kobo.419.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.420.1">ValDat</span></strong><span class="koboSpan" id="kobo.421.1">. </span><span class="koboSpan" id="kobo.421.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.422.1">TrainDat</span></strong><span class="koboSpan" id="kobo.423.1"> data store consists of an initial portion of each category determined by </span><strong class="source-inline"><span class="koboSpan" id="kobo.424.1">SplitRate</span></strong><span class="koboSpan" id="kobo.425.1">, while the </span><strong class="source-inline"><span class="koboSpan" id="kobo.426.1">ValDat</span></strong><span class="koboSpan" id="kobo.427.1"> data store contains the remaining images from each class. </span><strong class="source-inline"><span class="koboSpan" id="kobo.428.1">SplitRate</span></strong><span class="koboSpan" id="kobo.429.1"> can take the form of a fractional value ranging from 0 to 1, indicating the proportion of images allocated to </span><strong class="source-inline"><span class="koboSpan" id="kobo.430.1">TrainDat</span></strong><span class="koboSpan" id="kobo.431.1">, or it can be an integer denoting the exact count of images assigned to </span><strong class="source-inline"><span class="koboSpan" id="kobo.432.1">TrainDat</span></strong><span class="koboSpan" id="kobo.433.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">each class.</span></span></p></li> <li><span class="koboSpan" id="kobo.435.1">Let’s begin constructing our convolutional network. </span><span class="koboSpan" id="kobo.435.2">As expected, a CNN consists of a sequence of interconnected layers. </span><span class="koboSpan" id="kobo.435.3">To commence, you’ll need to employ a layer so that you can import your </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">input data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.437.1">
layers = [
    imageInputLayer([28, 28, 1])</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.438.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.439.1">layers</span></strong><span class="koboSpan" id="kobo.440.1"> variable is an array that contains the list of layers for our CNN, defining the architecture of the neural network used for deep learning. </span><span class="koboSpan" id="kobo.440.2">To initiate this architecture, we begin with </span><strong class="source-inline"><span class="koboSpan" id="kobo.441.1">imageInputLayer</span></strong><span class="koboSpan" id="kobo.442.1">. </span><span class="koboSpan" id="kobo.442.2">This layer functions as an input for images, accepting 2D image data into the neural network and performing data normalization. </span><span class="koboSpan" id="kobo.442.3">Additionally, this layer specifies the unmodifiable </span><strong class="source-inline"><span class="koboSpan" id="kobo.443.1">InputSize</span></strong><span class="koboSpan" id="kobo.444.1"> attribute. </span><span class="koboSpan" id="kobo.444.2">This attribute contains the height, width, and number of channels, respectively. </span><span class="koboSpan" id="kobo.444.3">In this case, we are working with grayscale images with a height of 28 and a width of 28. </span><span class="koboSpan" id="kobo.444.4">Following the input layer, we establish the initial block of three </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">consecutive layers:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.446.1">    convolution2dLayer(3,8,'Padding','same')
    batchNormalizationLayer
    reluLayer</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.447.1">In this code, we </span><a id="_idIndexMarker937"/><span class="koboSpan" id="kobo.448.1">have </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">the following:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.450.1">convolution2dLayer</span></strong><span class="koboSpan" id="kobo.451.1">: This layer employs 2D convolutional filters that slide across 2D input data. </span><span class="koboSpan" id="kobo.451.2">The following parameters </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">are passed:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.453.1">3</span></strong><span class="koboSpan" id="kobo.454.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">Filter size.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.456.1">8</span></strong><span class="koboSpan" id="kobo.457.1">: Number </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">of filters.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.459.1">Padding</span></strong><span class="koboSpan" id="kobo.460.1">: This is a method that involves augmenting the input image or feature map with additional rows and columns of pixels before conducting convolutional operations. </span><span class="koboSpan" id="kobo.460.2">Its primary purpose is to regulate the spatial dimensions of the resulting feature maps following the </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">convolution process.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.462.1">same</span></strong><span class="koboSpan" id="kobo.463.1">: This specifies that you want the output feature map to have the same spatial dimensions as the input image. </span><span class="koboSpan" id="kobo.463.2">This means that the necessary padding will be added automatically to </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">achieve this.</span></span></li></ul></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">batchNormalizationLayer</span></strong><span class="koboSpan" id="kobo.466.1">: This layer facilitates batch normalization, standardizing a mini-batch of data independently across all observations for </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">each channel.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">reluLayer</span></strong><span class="koboSpan" id="kobo.469.1">: The ReLU layer performs a thresholding operation on each input element, setting any value less than 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">to 0.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.471.1">Following this initial block, we proceed by applying a pooling layer, as </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">outlined here:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.473.1">    maxPooling2dLayer(2,'Stride',2)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.474.1">Downsampling is accomplished via a 2D max pooling layer, which divides the input into rectangular pooling regions and subsequently identifies the maximum value within each of </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">these regions.</span></span></p></li> <li><span class="koboSpan" id="kobo.476.1">Next, we will introduce a second set of layers, akin to the first set, with </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">adjusted parameters:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.478.1">
    convolution2dLayer(1,16,'Padding','same')
    batchNormalizationLayer
    reluLayer</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.479.1">A new pooling </span><a id="_idIndexMarker938"/><span class="No-Break"><span class="koboSpan" id="kobo.480.1">layer follows:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.481.1">    maxPooling2dLayer(2,'Stride',2)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.482.1">Then, we add a third block of layers, to finish </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">the architecture:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.484.1">    convolution2dLayer(3,32,'Padding','same')
    batchNormalizationLayer
    reluLayer</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.485.1">After the three blocks, we add a </span><strong class="bold"><span class="koboSpan" id="kobo.486.1">fully connected</span></strong><span class="koboSpan" id="kobo.487.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.488.1">FC</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">)</span></span><span class="No-Break"><span class="koboSpan" id="kobo.490.1"> layer:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.491.1">    fullyConnectedLayer(10)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.492.1">In an FC layer, the input undergoes multiplication with a weight matrix and is subsequently adjusted by a bias vector. </span><span class="koboSpan" id="kobo.492.2">The “parameter” in this context specifies the desired output size, which, in our case, is 10 since we are classifying </span><strong class="source-inline"><span class="koboSpan" id="kobo.493.1">10</span></strong><span class="koboSpan" id="kobo.494.1"> distinct digits. </span><span class="koboSpan" id="kobo.494.2">This layer type mirrors the layer configuration commonly found in a</span><a id="_idIndexMarker939"/><span class="koboSpan" id="kobo.495.1"> traditional </span><strong class="bold"><span class="koboSpan" id="kobo.496.1">artificial neural network</span></strong><span class="koboSpan" id="kobo.497.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.498.1">ANN</span></strong><span class="koboSpan" id="kobo.499.1">) employing</span><a id="_idIndexMarker940"/><span class="koboSpan" id="kobo.500.1"> an FC architecture. </span><span class="koboSpan" id="kobo.500.2">Within an FC layer, every neuron establishes direct connections with all neurons from the preceding layer, engaging directly with their respective </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">activation values.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.502.1">Following this, we require a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">softmax</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.504.1"> layer:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.505.1">softmaxLayer</span></pre><p class="list-inset"><strong class="source-inline"><span class="koboSpan" id="kobo.506.1">softmaxLayer</span></strong><span class="koboSpan" id="kobo.507.1"> is a specialized layer that’s employed within neural networks, designed specifically to implement the </span><strong class="source-inline"><span class="koboSpan" id="kobo.508.1">softmax</span></strong><span class="koboSpan" id="kobo.509.1"> function on its input. </span><span class="koboSpan" id="kobo.509.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">softmax</span></strong><span class="koboSpan" id="kobo.511.1"> function finds extensive use in classification tasks as it transforms raw scores or logits into a probability distribution spanning multiple classes. </span><span class="koboSpan" id="kobo.511.2">Typically, this layer </span><a id="_idIndexMarker941"/><span class="koboSpan" id="kobo.512.1">serves as the concluding component in a neural network designed for multi-class classification. </span><span class="koboSpan" id="kobo.512.2">It transforms the network’s output values into probabilities that collectively sum to </span><strong class="source-inline"><span class="koboSpan" id="kobo.513.1">1</span></strong><span class="koboSpan" id="kobo.514.1">, simplifying the interpretation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">model’s predictions.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.516.1">To complete the network, a classification layer </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">was added:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.518.1">  classificationLayer];</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.519.1">The classification layer calculates cross-entropy loss for both regular and weighted classification tasks that pertain to distinct and mutually exclusive classes. </span><span class="koboSpan" id="kobo.519.2">It automatically deduces the number of classes by inspecting the dimensions of the output originating from the </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">preceding layer.</span></span></p></li> <li><span class="koboSpan" id="kobo.521.1">Before training the CNN, we need to configure </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">the settings:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.523.1">
options = trainingOptions('sgdm', ...
</span><span class="koboSpan" id="kobo.523.2">    'InitialLearnRate',0.01, ...
</span><span class="koboSpan" id="kobo.523.3">    'MaxEpochs',50, ...
</span><span class="koboSpan" id="kobo.523.4">    'Shuffle','every-epoch', ...
</span><span class="koboSpan" id="kobo.523.5">    'ValidationData',ValDat, ...
</span><span class="koboSpan" id="kobo.523.6">    'ValidationFrequency',30, ...
</span><span class="koboSpan" id="kobo.523.7">    'Verbose',false, ...
</span><span class="koboSpan" id="kobo.523.8">    'Plots','training-progress');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.524.1">To learn more about the various training options, please read </span><a href="B21156_06.xhtml#_idTextAnchor124"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.525.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.526.1">, </span><em class="italic"><span class="koboSpan" id="kobo.527.1">Deep Learning and Convolutional </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.528.1">Neural Networks</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">.</span></span></p></li> <li><span class="koboSpan" id="kobo.530.1">It’s time to train </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">the network:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.532.1">
CNNnet = trainNetwork(TrainDat,layers,options);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.533.1">The following plot was printed on the screen (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.534.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.535.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">):</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer083">
<span class="koboSpan" id="kobo.537.1"><img alt="Figure 8.5 – Training process of the CNN for handwritten digit recognition" src="image/B21156_08_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.538.1">Figure 8.5 – Training process of the CNN for handwritten digit recognition</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.539.1">This plot will</span><a id="_idIndexMarker942"/><span class="koboSpan" id="kobo.540.1"> undergo continuous updates as the training progresses, enabling us to monitor how the algorithm adapts the weights to achieve convergence. </span><span class="koboSpan" id="kobo.540.2">We can see the results with an accuracy of 84.1%, which indicates a </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">good result.</span></span></p>
<p><span class="koboSpan" id="kobo.542.1">It is natural to ask whether it is possible to improve the performance of the handwritten digit recognition model. </span><span class="koboSpan" id="kobo.542.2">In the next section, we will see how to improve the accuracy of the model using </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">transfer learning.</span></span></p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.544.1">Training and fine-tuning pretrained deep learning models in MATLAB</span></h1>
<p><span class="koboSpan" id="kobo.545.1">Transfer learning</span><a id="_idIndexMarker943"/><span class="koboSpan" id="kobo.546.1"> is a </span><a id="_idIndexMarker944"/><span class="koboSpan" id="kobo.547.1">machine learning approach wherein a model created for a particular task is repurposed as the initial foundation for a model addressing a second task. </span><span class="koboSpan" id="kobo.547.2">This technique entails leveraging knowledge acquired from one problem and applying it to a distinct yet related problem. </span><span class="koboSpan" id="kobo.547.3">Transfer learning is particularly useful in deep learning and neural networks, where pretrained models can be fine-tuned or used as feature extractors for </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">new tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.549.1">In pretrained models, you start with a pretrained model that has been trained on a large dataset for a specific task, such as image classification, natural language processing, or speech recognition. </span><span class="koboSpan" id="kobo.549.2">These pretrained models are often complex neural networks with many layers. </span><span class="koboSpan" id="kobo.549.3">In many cases, you can use the layers of the pretrained model as feature extractors. </span><span class="koboSpan" id="kobo.549.4">You remove the final classification layer(s) and use the activations from the earlier layers as features for your new task. </span><span class="koboSpan" id="kobo.549.5">This is especially common in computer vision tasks. </span><span class="koboSpan" id="kobo.549.6">Optionally, you can fine-tune the pretrained model on your specific task by training it further with your own dataset. </span><span class="koboSpan" id="kobo.549.7">This involves updating the weights of some or all layers while keeping the knowledge you’ve learned from the </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">original task.</span></span></p>
<p><span class="koboSpan" id="kobo.551.1">Transfer learning can significantly reduce the amount of data and time required to train a model for a new task, especially when you have a limited dataset. </span><span class="koboSpan" id="kobo.551.2">Pretrained models have already learned useful features from large and diverse datasets, which can be valuable for related tasks. </span><span class="koboSpan" id="kobo.551.3">It can help improve model performance when you have limited computational resources or limited </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">labeled data.</span></span></p>
<p><span class="koboSpan" id="kobo.553.1">Transfer learning is commonly used in various fields, including computer vision, natural language processing, and audio processing, and has been a key technique in advancing the state of the art in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">learning applications.</span></span></p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.555.1">Introducing the ResNet pretrained network</span></h2>
<p><span class="koboSpan" id="kobo.556.1">ResNet stands for </span><a id="_idIndexMarker945"/><span class="koboSpan" id="kobo.557.1">Residual Network, a profound CNN architecture introduced by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their 2015 paper titled </span><em class="italic"><span class="koboSpan" id="kobo.558.1">Deep Residual Learning for Image Recognition</span></em><span class="koboSpan" id="kobo.559.1">. </span><span class="koboSpan" id="kobo.559.2">This groundbreaking architecture has had a substantial impact on the domains of computer vision and </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">deep learning.</span></span></p>
<p><span class="koboSpan" id="kobo.561.1">The key innovation in ResNet is the use of residual blocks. </span><span class="koboSpan" id="kobo.561.2">In traditional deep neural networks, as the network becomes deeper, it becomes increasingly difficult to train. </span><span class="koboSpan" id="kobo.561.3">This is because of the vanishing gradient problem, where gradients become extremely small as they are propagated back through the network during training. </span><span class="koboSpan" id="kobo.561.4">As a result, deep networks tend to suffer from degradation in performance as their </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">depth increases.</span></span></p>
<p><span class="koboSpan" id="kobo.563.1">ResNet addresses this problem by introducing residual blocks, which contain skip or shortcut connections that allow the gradient to flow more easily through the network. </span><span class="koboSpan" id="kobo.563.2">These shortcut connections bypass one or more layers, making it easier to train very deep networks. </span><span class="koboSpan" id="kobo.563.3">The skip connections essentially learn the residual (the difference) between the output</span><a id="_idIndexMarker946"/><span class="koboSpan" id="kobo.564.1"> and input of a layer, hence the name “</span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">residual network.”</span></span></p>
<p><span class="koboSpan" id="kobo.566.1">We can summarize the following key features </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">of ResNet:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.568.1">Deep architecture</span></strong><span class="koboSpan" id="kobo.569.1">: ResNet can</span><a id="_idIndexMarker947"/><span class="koboSpan" id="kobo.570.1"> be very deep, with hundreds or even thousands of layers, thanks to the use of </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">residual blocks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.572.1">Skip connections</span></strong><span class="koboSpan" id="kobo.573.1">: The skip connections allow gradients to propagate effectively, mitigating the vanishing </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">gradient problem.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.575.1">High accuracy</span></strong><span class="koboSpan" id="kobo.576.1">: ResNet achieved state-of-the-art performance on various image classification tasks, including the ImageNet Large Scale Visual </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">Recognition Challenge.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.578.1">Transfer learning</span></strong><span class="koboSpan" id="kobo.579.1">: Pretrained ResNet models are widely used as feature extractors or starting points for various computer vision tasks through </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">transfer learning.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.581.1">Architectural variations</span></strong><span class="koboSpan" id="kobo.582.1">: There are several ResNet architectures with different depths, such as ResNet-18, ResNet-34, ResNet-50, and </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">deeper variants.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.584.1">ResNet has become a foundational architecture in deep learning, and its principles of skip connections and residual learning have influenced the design of many subsequent neural network architectures. </span><span class="koboSpan" id="kobo.584.2">It has been applied not only to image classification but also to various other computer vision tasks, including object detection, semantic segmentation, </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">and</span></span><span class="No-Break"><a id="_idIndexMarker948"/></span><span class="No-Break"><span class="koboSpan" id="kobo.586.1"> more.</span></span></p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.587.1">The MATLAB Deep Network Designer app</span></h2>
<p><span class="koboSpan" id="kobo.588.1">The MATLAB </span><a id="_idIndexMarker949"/><span class="koboSpan" id="kobo.589.1">Deep Network Designer app is a </span><strong class="bold"><span class="koboSpan" id="kobo.590.1">graphical user interface</span></strong><span class="koboSpan" id="kobo.591.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.592.1">GUI</span></strong><span class="koboSpan" id="kobo.593.1">) tool provided by MATLAB for designing, training, and analyzing </span><a id="_idIndexMarker950"/><span class="koboSpan" id="kobo.594.1">deep neural networks. </span><span class="koboSpan" id="kobo.594.2">It’s part of the MATLAB Deep Learning Toolbox, which offers a comprehensive set of tools and functions for working with ANN and deep learning. </span><span class="koboSpan" id="kobo.594.3">You can visually design neural network architectures by adding layers, connecting them, and specifying their properties and parameters. </span><span class="koboSpan" id="kobo.594.4">This allows you to create custom network </span><span class="No-Break"><span class="koboSpan" id="kobo.595.1">architectures easily.</span></span></p>
<p><span class="koboSpan" id="kobo.596.1">The app provides a library of predefined layers that you can drag and drop into your network design. </span><span class="koboSpan" id="kobo.596.2">These layers include common types such as convolutional layers, FC layers, and more. </span><span class="koboSpan" id="kobo.596.3">The Deep Network Designer app simplifies the process of designing and training deep neural networks, making it more accessible for users who may not be familiar with deep learning concepts and programming. </span><span class="koboSpan" id="kobo.596.4">It’s a valuable tool for researchers, engineers, and data scientists working on machine learning and deep learning projects using MATLAB. </span><span class="koboSpan" id="kobo.596.5">Let’s take a </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">closer look:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.598.1">To open the Deep Network Designer app, simply click on the </span><strong class="bold"><span class="koboSpan" id="kobo.599.1">Deep Learning</span></strong><span class="koboSpan" id="kobo.600.1"> section of the </span><strong class="bold"><span class="koboSpan" id="kobo.601.1">Apps</span></strong><span class="koboSpan" id="kobo.602.1"> tab at the top of the MATLAB interface. </span><span class="koboSpan" id="kobo.602.2">In the </span><strong class="bold"><span class="koboSpan" id="kobo.603.1">Deep Learning</span></strong><span class="koboSpan" id="kobo.604.1"> section, you will find the </span><strong class="bold"><span class="koboSpan" id="kobo.605.1">Deep Network Designer</span></strong><span class="koboSpan" id="kobo.606.1"> icon. </span><span class="koboSpan" id="kobo.606.2">Click on this icon to open the </span><strong class="bold"><span class="koboSpan" id="kobo.607.1">Deep Network Designer</span></strong><span class="koboSpan" id="kobo.608.1"> app. </span><span class="koboSpan" id="kobo.608.2">Alternatively, you can open the Deep Network Designer app by entering the following command in the MATLAB </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">command window:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.610.1">
deepNetworkDesigner</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.611.1">This command will launch the app, allowing you to create, design, and train deep neural networks using a GUI. </span><span class="koboSpan" id="kobo.611.2">You need to have the MATLAB Deep Learning Toolbox installed to use the Deep Network Designer app. </span><span class="koboSpan" id="kobo.611.3">If it’s not already installed, you may need to install it separately from your </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">MATLAB installation.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.613.1">The following window will open (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.614.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.615.1">.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">):</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer084">
<span class="koboSpan" id="kobo.617.1"><img alt="Figure 8.6 – Deep Network Designer Start Page" src="image/B21156_08_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.618.1">Figure 8.6 – Deep Network Designer Start Page</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.619.1">If you don’t find </span><a id="_idIndexMarker951"/><span class="koboSpan" id="kobo.620.1">the network, you can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">following command:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.622.1">
deepNetworkDesigner(resnet18)</span></pre> <p class="list-inset"><span class="koboSpan" id="kobo.623.1">In the image pretrained network, we will find ResNet-18. </span><span class="koboSpan" id="kobo.623.2">ResNet-18 consists of 18 layers, which include convolutional layers, residual blocks, and FC layers. </span><span class="koboSpan" id="kobo.623.3">It’s considered relatively shallow compared to deeper variants such as ResNet-50 or ResNet-101. </span><span class="koboSpan" id="kobo.623.4">Like all ResNet architectures, ResNet-18 employs residual blocks. </span><span class="koboSpan" id="kobo.623.5">These blocks contain skip connections (or shortcut connections) that allow gradients to flow more effectively during training, addressing the vanishing gradient problem. </span><span class="koboSpan" id="kobo.623.6">ResNet-18 has been widely adopted in the deep learning community due to its balance between model complexity and performance. </span><span class="koboSpan" id="kobo.623.7">It’s often used in tasks such as image classification, object detection, and feature extraction in various computer vision applications. </span><span class="koboSpan" id="kobo.623.8">Its architectural principles, such as residual learning, have influenced the design of many subsequent neural network architectures. </span><span class="koboSpan" id="kobo.623.9">To import ResNet-18, it is necessary to install the </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">relative toolbox.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.625.1">After importing it, we can import the dataset that we’ll train the network on. </span><span class="koboSpan" id="kobo.625.2">To do that, we can move to the </span><strong class="bold"><span class="koboSpan" id="kobo.626.1">Data</span></strong><span class="koboSpan" id="kobo.627.1"> tab of the app. </span><span class="koboSpan" id="kobo.627.2">Upon clicking the </span><strong class="bold"><span class="koboSpan" id="kobo.628.1">Import Data</span></strong><span class="koboSpan" id="kobo.629.1"> icon, the following window will open (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.630.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.631.1">.7</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">):</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer085">
<span class="koboSpan" id="kobo.633.1"><img alt="Figure 8.7 – The Import Image Data window" src="image/B21156_08_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.634.1">Figure 8.7 – The Import Image Data window</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.635.1">Under </span><strong class="bold"><span class="koboSpan" id="kobo.636.1">Data source</span></strong><span class="koboSpan" id="kobo.637.1">, we </span><a id="_idIndexMarker952"/><span class="koboSpan" id="kobo.638.1">can select </span><strong class="bold"><span class="koboSpan" id="kobo.639.1">ImageDatastore in workspace</span></strong><span class="koboSpan" id="kobo.640.1"> to select the data that’s already been imported into the MATLAB workspace, as indicated in the previous section. </span><span class="koboSpan" id="kobo.640.2">The MNIST dataset will be imported, as shown in the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.641.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.642.1">.8</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<span class="koboSpan" id="kobo.644.1"><img alt="Figure 8.8 – The MNIST dataset imported into the Deep Network Designer app" src="image/B21156_08_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.645.1">Figure 8.8 – The MNIST dataset imported into the Deep Network Designer app</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.646.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.647.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.648.1">.8</span></em><span class="koboSpan" id="kobo.649.1">, we can see that a good distribution of the images is regularly present in the 10 classes, with 700 images for each class. </span><span class="koboSpan" id="kobo.649.2">We will use 70% of the data for training</span><a id="_idIndexMarker953"/><span class="koboSpan" id="kobo.650.1"> and the rest </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">for validation.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.652.1">At this point, we can set the pretrained network (ResNet-18) by moving to the </span><strong class="bold"><span class="koboSpan" id="kobo.653.1">Designer</span></strong><span class="koboSpan" id="kobo.654.1"> tab. </span><span class="koboSpan" id="kobo.654.2">We’ll use ResNet-18 for another type of image (an RGB image whose size is 227 x 227 x 3). </span><span class="koboSpan" id="kobo.654.3">To do this, we have to change the first layer, which defines the size of the input data. </span><span class="koboSpan" id="kobo.654.4">Click on the first layer (</span><strong class="bold"><span class="koboSpan" id="kobo.655.1">ImageInput</span></strong><span class="koboSpan" id="kobo.656.1">) and then click on the </span><strong class="bold"><span class="koboSpan" id="kobo.657.1">Canc</span></strong><span class="koboSpan" id="kobo.658.1"> button to remove </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">this layer.</span></span></li>
<li><span class="koboSpan" id="kobo.660.1">After that, we can click on the </span><strong class="bold"><span class="koboSpan" id="kobo.661.1">ImageInputLayer</span></strong><span class="koboSpan" id="kobo.662.1"> icon in </span><strong class="bold"><span class="koboSpan" id="kobo.663.1">Layer Library</span></strong><span class="koboSpan" id="kobo.664.1"> to the left of the tab, at which point we have to connect this layer with the </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">next layer.</span></span></li>
<li><span class="koboSpan" id="kobo.666.1">After that, we have to change the </span><strong class="bold"><span class="koboSpan" id="kobo.667.1">InputSize</span></strong><span class="koboSpan" id="kobo.668.1"> property by clicking on the layer and modifying it to </span><strong class="source-inline"><span class="koboSpan" id="kobo.669.1">28,28,1</span></strong><span class="koboSpan" id="kobo.670.1"> in the </span><strong class="bold"><span class="koboSpan" id="kobo.671.1">Properties</span></strong><span class="koboSpan" id="kobo.672.1"> window to the right of the </span><strong class="bold"><span class="koboSpan" id="kobo.673.1">Designer</span></strong><span class="koboSpan" id="kobo.674.1"> tab (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.675.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.676.1">.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">).</span></span></li>
<li><span class="koboSpan" id="kobo.678.1">After that, we have to change the first convolutional layer. </span><span class="koboSpan" id="kobo.678.2">First, we must remove that and then drop a </span><strong class="screen-inline"><span class="koboSpan" id="kobo.679.1">Convolution2Dlayer</span></strong><span class="koboSpan" id="kobo.680.1"> and connect it to another layer. </span><span class="koboSpan" id="kobo.680.2">Then, we have to set </span><strong class="bold"><span class="koboSpan" id="kobo.681.1">FilterSize</span></strong><span class="koboSpan" id="kobo.682.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.683.1">3,3</span></strong><span class="koboSpan" id="kobo.684.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.685.1">NumFilters</span></strong><span class="koboSpan" id="kobo.686.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.687.1">64</span></strong><span class="koboSpan" id="kobo.688.1"> in the </span><strong class="bold"><span class="koboSpan" id="kobo.689.1">Properties</span></strong><span class="koboSpan" id="kobo.690.1"> window to </span><a id="_idIndexMarker954"/><span class="koboSpan" id="kobo.691.1">the right of the </span><strong class="bold"><span class="koboSpan" id="kobo.692.1">Designer</span></strong><span class="koboSpan" id="kobo.693.1"> tab (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.694.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.695.1">.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">):</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer087">
<span class="koboSpan" id="kobo.697.1"><img alt="Figure 8.9 – Modifying the first two layers of ResNet-18 to adapt it to the new input data" src="image/B21156_08_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.698.1">Figure 8.9 – Modifying the first two layers of ResNet-18 to adapt it to the new input data</span></p>
<ol>
<li value="7"><span class="koboSpan" id="kobo.699.1">Now, we have to move on to the final part of ResNet-18, which involves setting the classification option. </span><span class="koboSpan" id="kobo.699.2">To do that, we have to replace the FC layer and the classification layer, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.700.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.701.1">.10</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer088">
<span class="koboSpan" id="kobo.703.1"><img alt="Figure 8.10 – Modifying the final layers of ResNet-18 to adapt it to the new classification" src="image/B21156_08_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.704.1">Figure 8.10 – Modifying the final layers of ResNet-18 to adapt it to the new classification</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.705.1">To check that the layer</span><a id="_idIndexMarker955"/><span class="koboSpan" id="kobo.706.1"> has been modified correctly, we can test it by clicking on the </span><strong class="bold"><span class="koboSpan" id="kobo.707.1">Analyze</span></strong><span class="koboSpan" id="kobo.708.1"> icon at the top of the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.709.1">Designer</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.710.1"> tab.</span></span></p>
<ol>
<li value="8"><span class="koboSpan" id="kobo.711.1">Now that we are ready to train the model, we can move to the </span><strong class="bold"><span class="koboSpan" id="kobo.712.1">Training</span></strong><span class="koboSpan" id="kobo.713.1"> tab of the Deep Network Designer app. </span><span class="koboSpan" id="kobo.713.2">We can check the training option by clicking on the </span><strong class="bold"><span class="koboSpan" id="kobo.714.1">Training option</span></strong><span class="koboSpan" id="kobo.715.1"> icon at the top of the </span><strong class="bold"><span class="koboSpan" id="kobo.716.1">Training</span></strong><span class="koboSpan" id="kobo.717.1"> tab. </span><span class="koboSpan" id="kobo.717.2">After that, we can push the </span><strong class="bold"><span class="koboSpan" id="kobo.718.1">Train</span></strong><span class="koboSpan" id="kobo.719.1"> button at the top of the tab. </span><span class="koboSpan" id="kobo.719.2">The training process will start; we will check its progress in the </span><strong class="bold"><span class="koboSpan" id="kobo.720.1">Training Progress</span></strong><span class="koboSpan" id="kobo.721.1"> window (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.722.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.723.1">.11</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">):</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer089">
<span class="koboSpan" id="kobo.725.1"><img alt="Figure 8.11 – The Training Progress window" src="image/B21156_08_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.726.1">Figure 8.11 – The Training Progress window</span></p>
<p><span class="koboSpan" id="kobo.727.1">At the end of the training process, we will be able to verify the performance of the model by reading the accuracy value that was obtained in the validation procedure. </span><span class="koboSpan" id="kobo.727.2">As we can see, we obtained an accuracy </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">of 90.27%.</span></span></p>
<p><span class="koboSpan" id="kobo.729.1">Now, let’s try to </span><a id="_idIndexMarker956"/><span class="koboSpan" id="kobo.730.1">collect some useful information on how to interpret the results that have been obtained from a model based on </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">machine learning.</span></span></p>
<h1 id="_idParaDest-175"><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.732.1">Interpreting and explaining machine learning models</span></h1>
<p><span class="koboSpan" id="kobo.733.1">Interpreting and </span><a id="_idIndexMarker957"/><span class="koboSpan" id="kobo.734.1">explaining machine learning models is essential for understanding their predictions and making them more transparent and trustworthy, especially in applications where interpretability is critical. </span><span class="koboSpan" id="kobo.734.2">This is an ongoing process that requires collaboration between data scientists, domain experts, and stakeholders. </span><span class="koboSpan" id="kobo.734.3">The choice of interpretation techniques depends on the model type, problem domain, and level of transparency required for the application. </span><span class="koboSpan" id="kobo.734.4">It’s important to strike a balance between model complexity and interpretability, depending on the specific </span><span class="No-Break"><span class="koboSpan" id="kobo.735.1">use case.</span></span></p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor180"/><span class="koboSpan" id="kobo.736.1">Understanding saliency maps</span></h2>
<p><span class="koboSpan" id="kobo.737.1">Saliency maps </span><a id="_idIndexMarker958"/><span class="koboSpan" id="kobo.738.1">are a visualization technique that’s</span><a id="_idIndexMarker959"/><span class="koboSpan" id="kobo.739.1"> used in computer vision and deep learning to understand and interpret neural network predictions, particularly in image classification and object recognition tasks. </span><span class="koboSpan" id="kobo.739.2">Saliency maps help identify which regions of an input image or feature map are most relevant to a model’s prediction. </span><span class="koboSpan" id="kobo.739.3">They are especially useful for gaining insights into why a neural network is making a </span><span class="No-Break"><span class="koboSpan" id="kobo.740.1">particular decision.</span></span></p>
<p><span class="koboSpan" id="kobo.741.1">Saliency maps are generated using gradient-based methods, typically backpropagation. </span><span class="koboSpan" id="kobo.741.2">The idea is to compute the gradients of the model’s output concerning the input image’s pixels. </span><span class="koboSpan" id="kobo.741.3">By calculating these gradients, you can identify which pixels in the input image have the most significant impact on the model’s prediction. </span><span class="koboSpan" id="kobo.741.4">In other words, saliency maps highlight the regions that the model pays attention to when making a decision. </span><span class="koboSpan" id="kobo.741.5">Saliency maps are usually visualized as heatmaps overlaid on the original input image. </span><span class="koboSpan" id="kobo.741.6">In a heatmap, the intensity of color corresponds to the importance of each pixel. </span><span class="koboSpan" id="kobo.741.7">High-intensity areas indicate regions that strongly influence the </span><span class="No-Break"><span class="koboSpan" id="kobo.742.1">model’s output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<span class="koboSpan" id="kobo.743.1"><img alt="Figure 8.12 – A saliency map as a heatmap" src="image/B21156_08_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.744.1">Figure 8.12 – A saliency map as a heatmap</span></p>
<p><span class="koboSpan" id="kobo.745.1">Saliency maps provide interpretability to neural network predictions. </span><span class="koboSpan" id="kobo.745.2">By examining the saliency map, you can see which parts of the image resemble specific features and contribute to the decision. </span><span class="koboSpan" id="kobo.745.3">These maps can be used for model debugging and improvement. </span><span class="koboSpan" id="kobo.745.4">If the model’s predictions appear incorrect, examining the saliency map can reveal whether the model is focusing on the right or wrong features. </span><span class="koboSpan" id="kobo.745.5">There are variations of saliency maps, such as class-specific saliency maps (highlighting features specific to a particular class) and gradient-based approaches such as guided backpropagation and SmoothGrad, which improve the interpretability of </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">saliency maps.</span></span></p>
<p><span class="koboSpan" id="kobo.747.1">It’s important to note that saliency maps provide insights into a model’s behavior but do not necessarily explain why a neural network made a particular decision in human-understandable terms. </span><span class="koboSpan" id="kobo.747.2">They are just one tool in the interpretability toolbox and are often used in</span><a id="_idIndexMarker960"/><span class="koboSpan" id="kobo.748.1"> conjunction with other techniques for a</span><a id="_idIndexMarker961"/><span class="koboSpan" id="kobo.749.1"> more comprehensive understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">model decisions.</span></span></p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.751.1">Understanding feature importance scores</span></h2>
<p><span class="koboSpan" id="kobo.752.1">Feature importance</span><a id="_idIndexMarker962"/><span class="koboSpan" id="kobo.753.1"> scores </span><a id="_idIndexMarker963"/><span class="koboSpan" id="kobo.754.1">are a set of metrics or values that indicate the relative importance of different input features (also known as variables or attributes) in a machine learning model’s prediction. </span><span class="koboSpan" id="kobo.754.2">These scores help data scientists and analysts understand which features have the most significant influence on the model’s output. </span><span class="koboSpan" id="kobo.754.3">Feature importance scores are particularly valuable for feature selection, model interpretation, </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">and debugging.</span></span></p>
<p><span class="koboSpan" id="kobo.756.1">There are some common methods and techniques for calculating feature </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">importance scores:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.758.1">Gini importance</span></strong><span class="koboSpan" id="kobo.759.1">: In decision </span><a id="_idIndexMarker964"/><span class="koboSpan" id="kobo.760.1">trees and random forests, Gini importance measures how often a feature is used for splitting data across the tree nodes. </span><span class="koboSpan" id="kobo.760.2">Higher values indicate more </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">important features.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.762.1">Mean decrease in impurity</span></strong><span class="koboSpan" id="kobo.763.1">: Similar to Gini importance, this metric calculates how much the impurity (or impurity reduction) decreases when a particular feature is used </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">for splitting.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.765.1">Coefficient magnitude</span></strong><span class="koboSpan" id="kobo.766.1">: In linear models, the magnitude (absolute value) of the coefficients represents the feature’s importance. </span><span class="koboSpan" id="kobo.766.2">Larger coefficients indicate </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">greater importance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.768.1">Permutation feature importance</span></strong><span class="koboSpan" id="kobo.769.1">: This method involves randomly permuting the values of a single feature while keeping other features constant and measuring how much the model’s performance decreases. </span><span class="koboSpan" id="kobo.769.2">A significant drop in performance indicates an </span><span class="No-Break"><span class="koboSpan" id="kobo.770.1">important feature.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.771.1">Recursive feature elimination</span></strong><span class="koboSpan" id="kobo.772.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.773.1">RFE</span></strong><span class="koboSpan" id="kobo.774.1">): RFE is an iterative method that starts with all features </span><a id="_idIndexMarker965"/><span class="koboSpan" id="kobo.775.1">and gradually removes the least important ones based on a model’s performance. </span><span class="koboSpan" id="kobo.775.2">The order in which features are removed indicates </span><span class="No-Break"><span class="koboSpan" id="kobo.776.1">their importance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.777.1">SHapley Additive exPlanations</span></strong><span class="koboSpan" id="kobo.778.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.779.1">SHAP</span></strong><span class="koboSpan" id="kobo.780.1">): SHAP values provide a unified measure of </span><a id="_idIndexMarker966"/><span class="koboSpan" id="kobo.781.1">feature importance by considering all possible feature combinations. </span><span class="koboSpan" id="kobo.781.2">They </span><a id="_idIndexMarker967"/><span class="koboSpan" id="kobo.782.1">can be applied to various models, including deep </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">learning models.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.784.1">The choice of feature importance calculation method depends on the machine learning algorithm used, the dataset, and the problem at hand. </span><span class="koboSpan" id="kobo.784.2">Different algorithms may provide different rankings of feature importance, so it’s essential to consider multiple methods and use domain</span><a id="_idIndexMarker968"/><span class="koboSpan" id="kobo.785.1"> knowledge to interpret the results effectively. </span><span class="koboSpan" id="kobo.785.2">Feature importance scores help identify relevant features, reduce dimensionality, and improve </span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">model interpretability.</span></span></p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.787.1">Discovering gradient-based attribution methods</span></h2>
<p><span class="koboSpan" id="kobo.788.1">Gradient-based </span><a id="_idIndexMarker969"/><span class="koboSpan" id="kobo.789.1">attribution</span><a id="_idIndexMarker970"/><span class="koboSpan" id="kobo.790.1"> methods, also known as gradient-based attribution techniques, are approaches that are used to understand and attribute the contributions of individual features or input elements to the output of a machine learning model. </span><span class="koboSpan" id="kobo.790.2">These methods rely on gradients, which represent the sensitivity of the model’s output to changes in input features. </span><span class="koboSpan" id="kobo.790.3">Here are some gradient-based attribution methods that are commonly used in </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">machine learning:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.792.1">Gradient saliency</span></strong><span class="koboSpan" id="kobo.793.1">: Saliency maps emphasize the most pertinent areas in an input image that influence a model’s prediction. </span><span class="koboSpan" id="kobo.793.2">These maps are created by calculating the gradient of the model’s output concerning the input image pixels. </span><span class="koboSpan" id="kobo.793.3">Regions with high gradients correspond to areas of </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">significant relevance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.795.1">Integrated gradients</span></strong><span class="koboSpan" id="kobo.796.1">: Integrated gradients assigns attribution scores to each input feature by computing the cumulative integral of the gradient concerning the input along a path from a reference input (usually all zeros) to the actual input. </span><span class="koboSpan" id="kobo.796.2">This method provides a more comprehensive understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.797.1">feature importance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.798.1">Guided backpropagation</span></strong><span class="koboSpan" id="kobo.799.1">: Guided backpropagation is a modified backpropagation algorithm that retains only positive gradients during backpropagation. </span><span class="koboSpan" id="kobo.799.2">This helps highlight the positive contributions of input features to predictions and suppresses </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">negative contributions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.801.1">SmoothGrad</span></strong><span class="koboSpan" id="kobo.802.1">: SmoothGrad reduces noise in saliency maps by averaging gradients across multiple perturbed versions of the input and then visualizing the smoothed </span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">gradient values.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.804.1">Layer-wise relevance propagation</span></strong><span class="koboSpan" id="kobo.805.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.806.1">LRP</span></strong><span class="koboSpan" id="kobo.807.1">): LRP is an attribution method that assigns</span><a id="_idIndexMarker971"/><span class="koboSpan" id="kobo.808.1"> relevance scores to each neuron in the network’s hidden layers and propagates them backward to the input features. </span><span class="koboSpan" id="kobo.808.2">It provides fine-grained feature </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">relevance information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.810.1">Deconvolutional networks</span></strong><span class="koboSpan" id="kobo.811.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.812.1">DeconvNets</span></strong><span class="koboSpan" id="kobo.813.1">): DeconvNets are designed to reverse the</span><a id="_idIndexMarker972"/><span class="koboSpan" id="kobo.814.1"> effects of convolutional layers in a neural network. </span><span class="koboSpan" id="kobo.814.2">They help visualize feature maps at different layers of the network to understand what each </span><span class="No-Break"><span class="koboSpan" id="kobo.815.1">layer learns.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.816.1">Gradient Class Activation Mapping</span></strong><span class="koboSpan" id="kobo.817.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.818.1">GradientCAM</span></strong><span class="koboSpan" id="kobo.819.1">): GradientCAM combines gradient </span><a id="_idIndexMarker973"/><span class="koboSpan" id="kobo.820.1">information with class activation mapping techniques to highlight regions in an input image that are important for a specific </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">class prediction.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.822.1">Gradient-based attribution methods are valuable for model interpretability and debugging. </span><span class="koboSpan" id="kobo.822.2">They help identify which features or parts of the input are influential in driving the model’s</span><a id="_idIndexMarker974"/><span class="koboSpan" id="kobo.823.1"> decisions. </span><span class="koboSpan" id="kobo.823.2">Choosing </span><a id="_idIndexMarker975"/><span class="koboSpan" id="kobo.824.1">the right attribution method depends on the model architecture, dataset, and specific goals of interpretation. </span><span class="koboSpan" id="kobo.824.2">These methods provide insights into model behavior and can help build trust in </span><span class="No-Break"><span class="koboSpan" id="kobo.825.1">AI systems.</span></span></p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.826.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.827.1">In this chapter, we understood the basic concepts surrounding computer vision and how to implement a model for object recognition using MATLAB. </span><span class="koboSpan" id="kobo.827.2">We started by introducing image processing and computer vision. </span><span class="koboSpan" id="kobo.827.3">We learned how tools are available to process images and how computer vision is used for object recognition, motion detection, and pattern recognition. </span><span class="koboSpan" id="kobo.827.4">Then, we explored MATLAB tools for computer vision, and how the capabilities and functions provided by MATLAB create a robust environment for the development and prototyping of computer vision applications. </span><span class="koboSpan" id="kobo.827.5">Whether your focus is on tasks such as image analysis, object detection, 3D reconstruction, or any related application, MATLAB offers the necessary tools and features to support your </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">work effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.829.1">After that, we learned how to build a MATLAB model for object recognition by using a CNN and the MNIST dataset. </span><span class="koboSpan" id="kobo.829.2">We understood how to import image data into a MATLAB workspace and how to use images to train a CNN. </span><span class="koboSpan" id="kobo.829.3">Then, we learned how to use pretrained deep learning models in MATLAB to improve the performance of the object recognition model. </span><span class="koboSpan" id="kobo.829.4">Finally, we introduced some tools for interpreting and explaining deep </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.831.1">In the next chapter, we will delve into fundamental concepts surrounding sequential data and explore the process of constructing a model to capture patterns within time series or any general sequence. </span><span class="koboSpan" id="kobo.831.2">We will learn the basic concepts of time series data, how to extract statistics from sequential data, and how to implement a model to predict the stock market data. </span><span class="koboSpan" id="kobo.831.3">Finally, we will understand oversampling, undersampling, and </span><span class="No-Break"><span class="koboSpan" id="kobo.832.1">cost-sensitive learning.</span></span></p>
</div>
</body></html>