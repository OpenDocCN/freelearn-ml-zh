- en: Chapter 2. Machine learning and Large-scale datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen a dramatic change in the way data has been handled in the recent
    past with the advent of big data. The field of Machine learning has seen the need
    to include scaling up strategies to handle the new age data requirements. This
    actually means that some of the traditional Machine learning implementations will
    not all be relevant in the context of big data now. Infrastructure and tuning
    requirements are now the challenges with the need to store and process large scale
    data complimented by the data format complexities.
  prefs: []
  type: TYPE_NORMAL
- en: With the evolution of hardware architectures, accessibility of cheaper hardware
    with distributed architectures and new programming paradigms for simplified parallel
    processing options, which can now be applied to many learning algorithms, we see
    a rising interest in scaling up the Machine learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics listed next are covered in-depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to big data and typical challenges of large-scale Machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The motivation behind scaling up and scaling out Machine learning, and an overview
    of parallel and distributed processing for huge datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of Concurrent Algorithm design, Big O notations, and task decomposition
    techniques for achieving parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advent of cloud frameworks to provide cloud clustering, distributed data
    storage, fault tolerance, and high availability coupled with effective utilization
    of computational resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frameworks and platform options for implementing large-scale Machine learning
    (Parallel Processing Frameworks such as MapReduce in **Massive Parallel Processing**
    (**MPP**), MRI, platforms as GPU, FPGA, and Multicore)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data and the context of large-scale Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I have covered some of the core aspects of big data in my previous Packt book
    titled *Getting Started with Greenplum for Big Data Analytics*. In this section,
    we will quickly recap some of the core aspects of big data and its impact in the
    field of Machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of large-scale is a scale of terabytes, petabytes, exabytes,
    or higher. This is typically the volume that cannot be handled by traditional
    database engines. The following chart lists the orders of magnitude that represents
    data volumes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Multiples of bytes |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **SI decimal prefixes** | **Binary Usage** |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **Name(Symbol)** | **Value** |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Kilobyte (KB) | 103 | 210 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Megabyte (MB) | 106 | 220 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Gigabyte (GB) | 109 | 230 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Terabyte (TB) | 1012 | 240 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Petabyte (PB) | 1015 | 250 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Exabyte (EB) | 1018 | 260 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Zettabyte (ZB) | 1021 | 270 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Yottabyte (YB) | 1024 | 280 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Data formats that are referred to in this context are distinct; they are generated
    and consumed, and need not be structured (for example, DBMS and relational data
    stores). Now, there are new sources of data; this data can be generated by social
    networking sites, equipment, and more. This can be streaming data that is heterogeneous
    in nature (for example, videos, emails, tweets, and so on). Again, none of the
    traditional data marts / data stores and data mining applications support these
    formats today.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, all the large-scale processing always happened in batches, but
    we are now seeing the need to support *real-time* processing capabilities. The
    new **Lambda Architectures** (**LA**) address the need to support both batch and
    real-time data ingestion and processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, the response time windows are shrinking and this adds to the challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s recap the four key characteristics of big data. All of these need special
    tools, frameworks, infrastructure, and capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Higher volumes (to the degree of petabytes )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for availability/accessibility of data (more real-time)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diversified data formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The increase in unlabeled data, and thus the **Noise**![Big data and the context
    of large-scale Machine learning](img/B03980_02_01.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional versus Structural – A methodological mismatch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We could never have imagined even five years ago that Relational Databases or
    non-relational databases like object databases will become only a single kind
    of database technology, and not the database technology in itself. Internet-scale
    data processing has changed the way we process data.
  prefs: []
  type: TYPE_NORMAL
- en: The new generation architectures, such as Facebook, Wikipedia, Salesforce, and
    more, are founded on principles and paradigms, which are radically different from
    the well-established theoretical foundations on which the current data management
    technologies are developed.
  prefs: []
  type: TYPE_NORMAL
- en: Commoditizing information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Apple App Store, SaaS, Ubiquitous Computing, Mobility, Cloud-Based Multi-Tenant
    architectures have unleashed, in business terms, an ability to commoditize information
    delivery. This model changes almost all the architecture decision making—as we
    now need to think in terms of what is the "units of information" that can be offered
    and billed as services, instead of thinking in terms of the **Total Cost of Ownership**
    (**TCO**) of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical limitations of RDBMS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As Michael Stonebreaker, the influential database theorist, has been writing
    in recent times, at the heart of the Internet-Scale Architectures is a new theoretical
    model of data processing and management. The theories of database management are
    now more than three decades old, and they were designed for mainframe-type computing
    environments and unreliable electronic components. Nature and the capabilities
    of systems and applications have since evolved significantly. With reliability
    becoming a quality attribute of the underlying environment, systems are composed
    of parallel processing cores, and the nature of data creation and usage has undergone
    tremendous change. In order to conceptualize solutions for these new environments,
    we need to approach the designing of solution architectures from a computing perspective
    and not only from an engineering perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Six major forces that are driving the data revolution today are:'
  prefs: []
  type: TYPE_NORMAL
- en: Massive Parallel Processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commoditized Information Delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ubiquitous Computing and Mobile Devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-RDBMS and Semantic Databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Community Computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop** and **MapReduce** have unleashed massive parallel processing of
    data on a colossal scale, and have made the complex computing algorithms in a
    programmatic platform. This has changed analytics and Business Intelligence forever.
    Similarly, the web services and API-driven architectures have made information
    delivery commoditized on an enormous scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Today, it is possible to build very large systems in such a way that each subsystem
    or component is a complete platform in itself, hosted and managed by a different
    entity altogether.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dijkstra once made an insightful remark that:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Computer Science is no more about computers than astronomy is about telescopes"*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: He would perhaps be a happy man today, as computing has liberated itself from
    the clutches of a personal computer, also known as workstations and servers. Most
    of our information consumption today is from the devices that we hardly call computers.
    Mobile devices, wearable devices, and information everywhere are changing the
    way data is created, assembled, consumed, and analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: As the limitations of the traditional databases have been exposed, in recent
    years, many special purpose databases have emerged—in-memory, columnar, graph-DB,
    and semantic stores are all now commercially available.
  prefs: []
  type: TYPE_NORMAL
- en: The previously mentioned innovations have changed the traditional data architecture
    completely. Especially, the semantic computing, ontology-driven modelling of information
    has turned data design over its head. Philosophically, data architecture is going
    through an factual underpinning. In the traditional data models, we first design
    the "data model"—a fixed, design time understanding of the world and its future.
    A data model fixes the meaning of data forever into a fixed structure. A table
    is nothing but a category, a set of something. As a result, data has to mean if
    we understand the set/category to which it belongs. For example, if we design
    an automobile processing system into some categories, such as four-wheelers, two-wheelers,
    commercial vehicles, and so on, then this division itself has a relevant meaning
    embedded into it. The data that is stored in each of these categories does not
    reveal the purpose of the design that is embedded in the way the categories are
    designed. For example, another system might view the world of automobiles regarding
    of its drivetrain—electric, petroleum powered, nuclear powered, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This categorization itself reveals the purpose of the system in some manner,
    which is impossible to obtain the attributes of any single record. Semantic and
    Metadata-Driven architectures can turn such a data model over its head. In a metadata
    model, it is the object that exists first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the core characteristics of how data is stored and managed in an RDBMS-based
    storage system are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Data is stored in a table that is typically characterized by rows and columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tables are linked using relationships between data attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is known for efficiency and flexibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This supports normalization techniques that reduce data duplication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand:'
  prefs: []
  type: TYPE_NORMAL
- en: The metadata driven / NoSQL / Semantic data architectures are free from relationships
    that tie down the purpose of the usage of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The focus is more on accommodating constant changes in business requirements
    that results in least changes in the software system being built
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for large datasets with distributed storage techniques, with lowered
    storage costs is of great importance in the metadata driven / NoSQL /semantic
    data architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling-up versus Scaling-out storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the advent of big data, there is now a need to scale data storage equipment
    to be able to store the petabyte-scale data. There are two ways of scaling storage
    equipment:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling-up (vertical scalability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling-out (horizontal scalability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling up** or vertical scalability is about adding more resources to the
    existing system that in turn increases the ability to hold more data. Here, resources
    can mean RAM, computation power, hard drive, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling out** or horizontal scalability is about adding new components to
    the system. This requires the data to be stored and distributed, and there are
    tasks that can be parallelized. This usually adds complexity to the system, and
    most of the time requires a redesign of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: All the big data technologies work on and support the scaling out of the infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling-up versus Scaling-out storage](img/B03980_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '| Scaling up (Vertical Scalability) | Scaling out (Horizontal Scalability)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Lesser and high capacity server | More and moderate, or low capacity server
    |'
  prefs: []
  type: TYPE_TB
- en: '| There could be a threshold beyond which an infrastructure can cease to scale
    vertically | There is no limit, the infrastructure can be scaled on a need basis
    without any impact on the design |'
  prefs: []
  type: TYPE_TB
- en: '| Can accommodate larger VMs | Runs with lower VMs and can be affected by failure
    in the host |'
  prefs: []
  type: TYPE_TB
- en: '| Shared everything data architecture | Shared nothing data architecture |'
  prefs: []
  type: TYPE_TB
- en: '| Higher TCO | Relatively lower and variable costs |'
  prefs: []
  type: TYPE_TB
- en: '| Lower network equipment | Needs relatively larger number of equipments (routers,
    switches, and more…) |'
  prefs: []
  type: TYPE_TB
- en: Distributed and parallel computing strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though distributed and parallel processing have been around for several years
    now, but with the advent of usability priorities needed for cost-effective solutions,
    these strategies have become critical for the Machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram depicts Flynn's taxonomy for computing. The categorization
    is done based on the number of data streams versus the number of instruction streams.
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed and parallel computing strategies](img/B03980_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Single Instruction Single Data** (**SISD**): This is a case of a single processor
    with no parallelism in data or instruction. A single instruction is executed on
    a single data in a sequential manner, for example, a uniprocessor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple Instruction Single Data** (**MISD**): Here, multiple instructions
    operate on a single data stream; a typical example can be fault tolerance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single Instruction Multiple Data** (**SIMD**): This is a case of natural
    parallelism; a single instruction triggers operation on multiple data streams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple Instructions Multiple Data** (**MIMD**): This is a case where multiple
    independent instructions operate on multiple and independent data streams. Since
    the data streams are multiple, the memory can either be shared or distributed.
    Distributed processing can be categorized here. The previous figure depicts MIMD
    and a variation in a "distributed" context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram explains parallel processor architectures and categorization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed and parallel computing strategies](img/B03980_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the critical requirements of parallel/distributed processing systems
    is High Availability and fault tolerance. There are several programming paradigms
    to implement parallelism. The following list details the important ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Master/Workers Model**: Master model is the driver where the work is
    held and then disseminated to the workers. Pivotal Greenplum Database and HD (Pivotal''s
    Hadoop''s distribution) modules implement this pattern.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Producer/Consumer Model**: Here, there is no owner who triggers the work.
    Producer generates work items and consumer subscribes and executes asynchronously.
    The **Enterprise Service Bus** (**ESB**) based data integration systems implement
    this pattern.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Distributed and parallel computing strategies](img/B03980_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In theory, there are two types of parallelization; one is data parallelization,
    the other one is execution or task parallelization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data parallelization**: It deals with running the same computations with
    multiple inputs in parallel. In the Machine learning world, this is a case where
    we consider running the same algorithm across different data samples without really
    worrying about how the data samples are distributed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execution or Task parallelization**: Unlike data parallelization, this is
    about breaking the functionality into multiple pieces and running them in a parallel
    manner. These pieces of work may work on the same dataset, but this is possible
    only for the tasks that can be parallelized and have no dependencies between the
    sub tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task parallelization can be fine grained or coarse grained.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many distributed platform options to bring efficiency and scale to
    Machine learning algorithms and can process large datasets. Some of the options
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Field-Programmable Gate Arrays** (**FPGAs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graphics Processing Units** (**GPUs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-Performance Computing** (**HPC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multicore and multi-processor parallel systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Infrastructures for virtual-large scale clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides the multiple platform options available, there are other highly adopted
    frameworks available that have out-of-box APIs for building Machine learning algorithms.
    The choice of this framework depends on the choice of hardware in particular.
  prefs: []
  type: TYPE_NORMAL
- en: It is important that we take an option that can take maximum advantage of the
    existing architecture, and suits the choice of learning algorithm and the data
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning: Scalability and Performance'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two important ways in which Machine learning algorithms can be scaled:'
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed systems with parallel processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to concurrently execute a given learning algorithm as separate
    chunks of work and consolidate the results. This sounds like a fairly simple way
    of parallelizing and being able to scale and perform well on a bigger dataset.
    This comes with an assumption that the datasets are discrete and there isn't any
    dependency between these distributed sets of data.
  prefs: []
  type: TYPE_NORMAL
- en: By the virtue of the proliferation of data sources, we now have access to large
    sets that are already distributed, and this brings in a need for the ability to
    have the learning algorithms running in a distributed mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are now a variety of options for distributed and parallel framework for
    Machine learning. Let''s look at some key differentiating factors between these
    platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: The degree of granularity in parallelization is a critical aspect. Support for
    fine-grained versus coarse-grained parallelization is what it refers to. A lower
    degree of granularity defines a fine-grained task parallelization, while a higher
    level of granularity defines coarse-grained task parallelization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The degree to which algorithm customization is supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for mixing a variety of programming paradigms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ease with which datasets can be scaled-out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The degree to which batch and real-time processing is supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a problem context, the choice of the platform and programming framework
    should be guided by the previous criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are some key metrics to measure the computational performance of
    parallel algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance** is the ratio of solution time for the sequential algorithms
    versus parallel process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency** or **Throughput** measures the ratio of performance across multiple
    processors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability** is the percentage improvement in efficiency with the growing
    number of processors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section covers some key characteristics of the Machine learning problem
    that motivate scaling-up the Machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Too many data points or instances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now see that in most of the Machine learning problems, there is an abundance
    of datasets and in many cases, all these data points are relevant in model building
    and refining. These data points can potentially run into terabyte scale with all
    their relevance.
  prefs: []
  type: TYPE_NORMAL
- en: This brings in a need to support distributed storage and a bandwidth to process
    these data points in the cluster. High-capacity storage systems with the ability
    to run parallel programming language paradigms like MapReduce and LINQ are used
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Too many attributes or features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The datasets that form an input to a building model can come with too many features,
    attributes, or dimensions. In this case, the Machine learning algorithms group
    the dependent or more relevant attributes and run the algorithms in iteration.
    These kind of datasets can be seen in case of Text mining and **Natural language
    processing** (**NLP**), where the number of features can run into multiples of
    millions. In this case, parallelizing the computation across features can get
    us to solve the problem effectively by the way of eliminating irrelevant features.
    Random forest and Decision trees are some of the examples. Also, some specific
    feature selection techniques such as the regularization methods will be covered
    in the chapters to come.
  prefs: []
  type: TYPE_NORMAL
- en: Shrinking response time windows – need for real-time responses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are certain Machine learning requirements such as speech recognition that
    will demand a real-time response from the systems. In these applications, response
    time from a Machine learning implementation is critical, and the response itself
    will become irrelevant otherwise. Parallelization can bring in this efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Latency and performance of the model are more important a problem to deal with
    than the throughput. There are many use cases where this latency in inference
    can invalidate the model itself, as the response becomes obsolete.
  prefs: []
  type: TYPE_NORMAL
- en: For these kinds of problems, highly parallelized hardware architectures such
    as GPUs or FPGAs will be very effective.
  prefs: []
  type: TYPE_NORMAL
- en: Highly complex algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a case where the algorithm of choice itself is complex, for example,
    a computational intensive function or any non-linear models. Let's take an example
    of a text or image content; it is inherently non-linear in nature. This complexity
    can easily be addressed using distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways we can solve these problems and one way is to prioritize
    features and still target for higher accuracies. However this will remove the
    automation part in the learning. There always needs to be a step that engineers
    the features before running the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The cases where there is more data complexity, there is a computational complexity.
    Unless the platform is scaled, there is no way to get the learning process run
    faster.
  prefs: []
  type: TYPE_NORMAL
- en: Multicore and GPU systems are apt for this kind of requirement. They bring in
    both; storage scale and computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Feed forward, iterative prediction cycles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are some unique use cases in the Machine learning space that do not stop
    at one level of execution of the algorithm. The algorithm runs iteratively and
    sequentially where the output from an iteration feeds into another iteration.
    This is critical for the outcome of the model. There can also be a need to consolidate
    the inferences across all the iterations that are run sequentially. This can make
    the model execution process quite complex. We can deal with inference process
    as a one-shot process, which will bring up the computational costs, or there can
    be stages of parallelization of individual tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some real-world examples are:'
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model selection process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, we will need to run multiple models in parameters on the same
    training and test sets with the different priority of features, and compare the
    accuracy to choose an appropriate model for the given problem domain. These trials
    can run in parallel as there will not be any dependencies between these models.
    The complexity increases when we will have to tune the parameters of learning
    algorithms and evaluate across multiple executions to infer from the learning.
  prefs: []
  type: TYPE_NORMAL
- en: The very fact that there is no dependency between the executions makes it highly
    parallelizable and requires no intercommunication. One of the examples of this
    use case is statistical significance testing. The usefulness of the parallel platforms
    is obvious for these tasks, as they can be easily performed concurrently without
    the need to parallelize actual learning and inference algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Potential issues in large-scale Machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now look at some potential issues encountered in the large-scale Machine
    learning implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallel execution**: Managing the accuracy of the parallel execution requires
    special care and a different design paradigm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing** and **managing skews**: With data and execution now distributed
    and running parallel, it is very imperative to manage the data and compute skews.
    No single node will need to take relatively more data storage or computations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**: With a variety of hardware, effective monitoring and automatic
    recovery systems need to be placed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: A foolproof failover and recovery system is a mandate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto scaling**: The scaling out and scaling up process is automatic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Job scheduling**: *Batch* jobs will need to be scheduled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workflow Management**: Choreography and Orchestration process to coordinate
    and monitor work execution among the nodes of the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms and Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now look at some basics of algorithms in general, the time complexity;
    and the order of magnitude measurements, before we start talking about building
    concurrency in executing algorithms, then explore the approaches to parallelizing
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'An algorithm can be defined as a sequence of steps that takes an input to produce
    the desired output. They are agnostic technology representations; let''s look
    at a sorting algorithm example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following algorithm is an insertion-sort algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For measuring the time and space complexity of algorithms, one of the elements
    is the input size. The time complexity is a measure of how "fast enough" the algorithm
    is for the defined needs; more importantly, how the algorithm would react when
    the volume of the data is increased.
  prefs: []
  type: TYPE_NORMAL
- en: 'Frequency count is one of the key measures for an algorithm. It is a prediction
    of how many times each instruction of the algorithm will run for an execution.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Instruction | Code | Frequency count (FC) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '| n+1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '| N |'
  prefs: []
  type: TYPE_TB
- en: '| 3 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '| N |'
  prefs: []
  type: TYPE_TB
- en: '| 4 |   | 3n +1 |'
  prefs: []
  type: TYPE_TB
- en: The FC measure is relatively meaningless unless it considers the relative performance
    to volume. There is another measure called "order of magnitude" that is an estimate
    of performance versus data volume. The *Big-O* is a measure of the rate at which
    the algorithm performance degrades as the function of the amount of data that
    it requires to process.
  prefs: []
  type: TYPE_NORMAL
- en: For example, *O(n)* represents linear performance degradation and *O(n2)* represents
    quadratic performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Developing concurrent algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step in developing a parallel algorithm is to decompose the problem
    into tasks that can be executed concurrently. A given problem may be decomposed
    into tasks in many different ways. Tasks may be of same or different sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: Task dependency graph is a directed graph with nodes corresponding to tasks
    and edges indicating that the result of one task is required for processing the
    next task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: This is the database query processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following execution of the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'on the following database:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing concurrent algorithms](img/B03980_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There can be fine-grained and coarse-grained task decomposition. The degree
    of concurrency increases as the decomposition becomes finer.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many decomposition techniques and there is no single best way of
    doing it. Following are some techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Recursive decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speculative decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decomposition results in several tasks and some characteristics of these tasks
    critically affect the performance of the parallel algorithms. Some of these features
    are task interactions (inter-task communication), the size of data that each task
    handles, and the task size. Some important aspects that need to be kept in mind
    while designing parallel execution algorithms include decoupling tasks in such
    a way that there is minimal interaction and handling granularity trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Technology and implementation options for scaling-up Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore some parallel programming techniques and distributed
    platform options that Machine learning implementations can adopt. The Hadoop platform
    will be introduced in the next chapter, and we will look into some practical examples
    starting from [Chapter 3](ch03.html "Chapter 3. An Introduction to Hadoop's Architecture
    and Ecosystem"), *An Introduction to Hadoop's Architecture and Ecosystem* with
    some real-world examples.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce programming paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MapReduce is a parallel programming paradigm that abstracts the parallelizing
    computing and data complexities in a distributed computing environment. It works
    on the concept of taking the compute function to the data rather than taking the
    data to the compute function.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce is more of a programming framework that comes with many built-in functions
    that the developer need not worry about building, and can alleviate many implementation
    complexities like data partitioning, scheduling, managing exceptions, and intersystem
    communications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a typical composition of the MapReduce function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MapReduce programming paradigm](img/B03980_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MapReduce was originally designed and adopted by Google as a programming model
    for processing large data sets on a cluster with parallel processing over distributed
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: The MapReduce paradigm now has become an industry standard and many platforms
    are internally built on this paradigm and support MapReduce implementation. For
    example, Hadoop is an open source implementation that can be run either in-house
    or on cloud computing services such as, **Amazon EC2** with elastic MapReduce.
  prefs: []
  type: TYPE_NORMAL
- en: This has, at the core, the `Map()` and `Reduce()` functions that are capable
    of running in parallel across the nodes in the cluster. The `Map()` function works
    on the distributed data and runs the required functionality in parallel, and the
    `Reduce()` function runs a summary operation of the data.
  prefs: []
  type: TYPE_NORMAL
- en: High Performance Computing (HPC) with Message Passing Interface (MPI)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MPI is designed to provide access to advanced parallel hardware, and is meant
    to work with heterogeneous networks and clusters. It is an impressive specification
    and provides a portable way to implement the parallel programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Message passing is a process of data transfer and synchronization between the
    sender and the receiver. The following figure demonstrates the message passing
    between sender and receiver:'
  prefs: []
  type: TYPE_NORMAL
- en: '![High Performance Computing (HPC) with Message Passing Interface (MPI)](img/B03980_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The processes can be grouped; the message sharing between the sender and the
    receiver needs to happen in the same context. Communicator thus is a combination
    of a group and the context. The data in a message is sent or received as triples.
  prefs: []
  type: TYPE_NORMAL
- en: MPI can be used to achieve portability and can improve performance through parallel
    processing. It can support unique data structures, and libraries can be built
    for reuse. MPI does not support liberal fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Language Integrated Queries (LINQ) framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The LINQ framework is a general-purpose system for large-scale data and parallel
    computing. Similar to the MapReduce paradigm, it comes with a high level of abstraction
    that comes with base implementations, and helps developers reduce the development
    complexities of the parallel and distributed execution.
  prefs: []
  type: TYPE_NORMAL
- en: With the Machine learning functions moving out of general data handling and
    operating on diverse data types including documents, images, and graphs, the need
    for generic implementation paradigms is increasing. This framework pertains to
    the .NET languages only.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating datasets with LINQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LINQ is shipped with a set of functions that operate on collections of .NET
    objects. These collections are modified by the LINQ functions that contain the
    .NET datatypes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Manipulating datasets with LINQ](img/B03980_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Graphics Processing Unit (GPU)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPUs are electronic circuits designed to handle the memory requirements and
    rapidly create images in the frame buffers for visual display.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs have been consistently supporting growing computational capabilities. They
    were initially meant to handle image processing and rendering, but the advanced
    GPUs are now positioned as self-contained, general purpose computing platforms.
  prefs: []
  type: TYPE_NORMAL
- en: While CPUs are designed to perform well on heterogeneous workloads, GPUs are
    built for tasks that are meant to ensure the availability of massive datasets
    and run in a parallel manner.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphics Processing Unit (GPU)](img/B03980_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: GPUs are mainly used in deep learning and training neural networks that can
    potentially need larger training datasets, lesser computational power, and storage
    space optimization. They are being employed in solving both classification and
    prediction problems in the cloud. Most of the social media companies have been
    in the list of early adopters for GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With GPUs, pre-recorded speech or multimedia content can be transcribed much
    more quickly. Compared to a CPU implementation we are able to perform recognition
    up to 33x faster.
  prefs: []
  type: TYPE_NORMAL
- en: Field Programmable Gate Array (FPGA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FPGAs are emerging in many areas of HPC. FPGAs can be used in the context of
    massive parallel processing. In this section, we will look at understanding some
    of the architecture and implementation aspects of FPGA.
  prefs: []
  type: TYPE_NORMAL
- en: FPGAs are known to provide high performance. They support different parallel
    computation applications. They have an on-chip memory to facilitate easy memory
    access to the processor. Above all, the memory is coupled to the algorithm logic
    and this means that we will not need any additional high-speed memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Field Programmable Gate Array (FPGA)](img/B03980_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FPGA contains an enormous number of **Configurable Logical Blocks** (**CLB**);
    each of these CLBs are connected using programmable interfaces that pass signals
    among them. The I/O blocks are the connections points for CLBs to the outside
    world.
  prefs: []
  type: TYPE_NORMAL
- en: FPGAs offer a variety of paradigms that help speed up computations in a hardware
    and software design. FPGAs are cost effective and the hardware resources are used
    in an optimal way. IBM Netezza leverages FPGA architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Multicore or multiprocessor systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multiprocessor systems usually have multiple CPUs that need not necessarily
    be on the same chip. The new age multiprocessors are on the same physical board,
    and the communication happens via high-speed connection interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: '![Multicore or multiprocessor systems](img/B03980_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multicore processors represent a family of processors that may contain many
    CPUs on one chip (such as two, four, and eight. In case of multicore systems,
    the efficiency of the multi-threading implementation is determined by how well-parallel
    the code is written).
  prefs: []
  type: TYPE_NORMAL
- en: Further to all the hardware and infrastructure advancements, we have just seen
    that the cloud frameworks for Machine learning are picking up considerable traction
    based on their ability to scale Machine learning processes at an optimal cost.
  prefs: []
  type: TYPE_NORMAL
- en: With the emergence of cloud computing, infrastructure service providers, such
    as Amazon Web Services, offer access to virtually unlimited computing power on
    demand that can be paid for, based on the usage.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we have explored the qualifiers of large datasets, their common
    characteristics, the problems of repetition, and the reasons for the hyper-growth
    in volumes; in fact, the big data context.
  prefs: []
  type: TYPE_NORMAL
- en: The need for applying conventional Machine learning algorithms to large datasets
    has given rise to new challenges for Machine learning practitioners. Traditional
    Machine learning libraries do not quite support, processing huge datasets. Parallelization
    using modern parallel computing frameworks, such as MapReduce, have gained popularity
    and adoption; this has resulted in the birth of new libraries that are built over
    these frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: The concentration was on methods that are suitable for massive data, and have
    potential for the parallel implementation. The landscape of Machine learning applications
    has changed dramatically in the last decade. Throwing more machines doesn't always
    prove to be a solution. There is a need to revisit traditional algorithms and
    models in the way they are being executed as now an another dimension in the study
    of Machine learning techniques is the scalability, parallel execution, load balancing,
    fault tolerance, and dynamic scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: We have also taken a look at the emerging parallelization and distribution architectures
    and frameworks in the context of large datasets, and understood the need for scaling
    up and scaling out Machine learning. Furthermore, we have recapped the internals
    of some parallel and distributed platform techniques for Machine learning such
    as MapReduce, GPUs, FGPA, and more.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how Hadoop is the best platform for large-scale
    Machine learning.
  prefs: []
  type: TYPE_NORMAL
