<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Command Line and SDK</h1>
            </header>

            <article>
                
<p>Using the AWS web interface to manage and run your projects is time-consuming. In this chapter, we move away from the web interface and start running our projects via the command line with the <strong>AWS Command Line Interface</strong> (<strong>AWS CLI</strong>) and the Python SDK with the <kbd>Boto3</kbd> library.</p>
<p>The first step will be to drive a whole project via the AWS CLI, uploading files to S3, creating datasources, models, evaluations, and predictions. As you will see, scripting will greatly facilitate using Amazon ML. We will use these new abilities to expand our Data Science powers by carrying out cross-validation and feature selection.</p>
<p>So far we have split our original dataset into three data chunks: training, validation, and testing. However, we have seen that the model selection can be strongly dependent on the data split. Shuffle the data — a different model might come as being the best one. Cross-validation is a technique that reduces this dependency by averaging the model performance on several data splits. Cross-validation involves creating many datasources for training, validation, and testing, and would be time-consuming using the web interface. The AWS CLI will allow us to quickly spin new datasources and models and carry out cross-validation effectively.</p>
<p>Another important technique in data science is feature elimination. Having a large number of features in your dataset either as the results of intensive feature engineering or because they are present in the original dataset can impact the model's performance. It's possible to significantly improve the model prediction capabilities by selecting and retaining only the best and most meaningful features while rejecting less important ones. There are many feature selection methods. We will implement a simple and efficient one, called recursive feature selection. The AWS Python SDK accessible via the Boto3 library will allow us to build the code wrapping around Amazon ML required for recursive feature selection.</p>
<p>In this chapter, you will learn the following:</p>
<ul>
<li>How to handle a whole project workflow through the AWS command line and the AWS Python SDK:
<ul>
<li>Managing data uploads to S3</li>
<li>Creating and evaluating models</li>
<li>Making and exporting the predictions</li>
</ul>
</li>
<li>How to implement cross-validation with the AWS CLI</li>
<li>How to implement Recursive Feature Selection with AWS the Python SDK</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Getting started and setting up</h1>
            </header>

            <article>
                
<p>Creating a performing predictive model from raw data requires many trials and errors, much back and forth. Creating new features, cleaning up data, and trying out new parameters for the model are needed to ensure the robustness of the model. There is a constant back and forth between the data, the models, and the evaluations. Scripting this workflow either via the AWS CLI or with the <kbd>Boto3</kbd> Python library, will give us the ability to speed up the create, test, select loop.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using the CLI versus SDK</h1>
            </header>

            <article>
                
<p>AWS offers several ways besides the UI to interact with its services, the CLI, APIs, and SDKs in several languages. Though the AWS CLI and SDKs do not include all AWS services. Athena SQL, for instance, being a new service, is not yet included in the AWS CLI module or in any of AWS SDK at the time of writing.</p>
<p>The AWS Command Line Interface or CLI is a command-line shell program that allows you to manage your AWS services from your shell terminal. Once installed and set up with proper permissions, you can write commands to manage your S3 files, AWS EC2 instances, Amazon ML models, and most AWS services.</p>
<p>Generally speaking, a software development kit<em>,</em> or SDK for short, is a set of tools that can be used to develop software applications targeting a specific platform. In short, the SDK is a wrapper around an API. Where an API holds the core interaction methods, the SDK includes debugging support, documentation, and higher-level functions and methods. The API can be seen as the lowest common denominator that AWS supports and the SDK as a higher-level implementation of the API.</p>
<p>AWS SDKs are available in 12 different languages including PHP, Java, Ruby, and .NET. In this chapter, we will use the Python SDK.</p>
<p>Using the AWS CLI or SDK requires setting up our credentials, which we'll do in the following section</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Installing AWS CLI</h1>
            </header>

            <article>
                
<p>In order to set up your CLI credentials, you need your access key ID and your secret access key. You have most probably downloaded and saved them in a previous chapter. If that's not the case, you should simply create new ones from the <strong>IAM</strong> console (<a href="https://console.aws.amazon.com/iam" target="_blank">https://console.aws.amazon.com/iam</a>).</p>
<p>Navigate to Users, select your IAM user name and click on the <span class="packt_screen">Security credentials</span> tab. Choose <span class="packt_screen">Create Access Key</span> and download the CSV file. Store the keys in a secure location. We will need the key in a few minutes to set up AWS CLI. But first, we need to install AWS CLI.</p>
<div class="packt_infobox">
<p><strong>Docker environment</strong> – This tutorial will help you use the AWS CLI within a docker container: <a href="https://blog.flowlog-stats.com/2016/05/03/aws-cli-in-a-docker-container/" target="_blank">https://blog.flowlog-stats.com/2016/05/03/aws-cli-in-a-docker-container/</a>. A docker image for running the AWS CLI is available at <a href="https://hub.docker.com/r/fstab/aws-cli/" target="_blank">https://hub.docker.com/r/fstab/aws-cli/</a>.</p>
</div>
<p>There is no need to rewrite the AWS documentation on how to install the AWS CLI. It is complete and up to date, and available at <a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html" target="_blank">http://docs.aws.amazon.com/cli/latest/userguide/installing.html</a>. In a nutshell, installing the CLI requires you to have Python and <kbd>pip</kbd> already installed.</p>
<p>Then, run the following:</p>
<pre>
<strong>$ pip install --upgrade --user awscli</strong>
</pre>
<p>Add AWS to your <kbd>$PATH</kbd>:</p>
<pre>
<strong>$ export PATH=~/.local/bin:$PATH</strong>
</pre>
<p>Reload the bash configuration file (this is for OSX):</p>
<pre>
<strong>$ source ~/.bash_profile</strong>
</pre>
<p>Check that everything works with the following command:</p>
<pre>
<strong>$ aws --version</strong>
</pre>
<p>You should see something similar to the following output:</p>
<pre>
<strong>$ aws-cli/1.11.47 Python/3.5.2 Darwin/15.6.0 botocore/1.5.10</strong>
</pre>
<p>Once installed, we need to configure the AWS CLI type:</p>
<pre>
<strong>$ aws configure</strong>
</pre>
<p>Now input the access keys you just created:</p>
<pre>
<strong>$ aws configure</strong><br/><strong><br/>AWS Access Key ID [None]: ABCDEF_THISISANEXAMPLE</strong><br/><strong>AWS Secret Access Key [None]: abcdefghijk_THISISANEXAMPLE</strong><br/><strong>Default region name [None]: us-west-2</strong><br/><strong>Default output format [None]: json</strong>
</pre>
<p>Choose the region that is closest to you and the format you prefer (JSON, text, or table). JSON is the default format.</p>
<p>The AWS configure command creates two files: a <kbd>config</kbd> file and a credential file. On OSX, the files are <kbd>~/.aws/config</kbd> and <kbd>~/.aws/credentials</kbd>. You can directly edit these files to change your access or configuration. You will need to create different profiles if you need to access multiple AWS accounts. You can do so via the AWS configure command:</p>
<pre>
<strong>$ aws configure --profile user2</strong>
</pre>
<p>You can also do so directly in the <kbd>config</kbd> and <kbd>credential</kbd> files:</p>
<pre>
<strong>~/.aws/config</strong><br/><br/>[default]<br/>output = json<br/>region = us-east-1<br/><br/>[profile user2]<br/>output = text<br/>region = us-west-2
</pre>
<p>You can edit <kbd>Credential</kbd> file as follows:</p>
<pre>
<strong>~/.aws/credentials</strong><br/><br/>[default]<br/>aws_secret_access_key = ABCDEF_THISISANEXAMPLE<br/>aws_access_key_id = abcdefghijk_THISISANEXAMPLE<br/><br/>[user2]<br/>aws_access_key_id = ABCDEF_ANOTHERKEY<br/>aws_secret_access_key = abcdefghijk_ANOTHERKEY
</pre>
<p>Refer to the AWS CLI setup page for more in-depth information:<br/>
<a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html" target="_blank">http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html</a></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Picking up CLI syntax</h1>
            </header>

            <article>
                
<p>The overall format of any AWS CLI command is as follows:</p>
<pre>
<strong>$ aws &lt;service&gt; [options] &lt;command&gt; &lt;subcommand&gt; [parameters]</strong>
</pre>
<p>Here the terms are stated as:</p>
<ul>
<li><kbd>&lt;service&gt;</kbd>: Is the name of the service you are managing: S3, machine learning, and EC2</li>
<li><kbd>[options]</kbd> : Allows you to set the region, the profile, and the output of the command</li>
<li><kbd>&lt;command&gt; &lt;subcommand&gt;</kbd>: Is the actual command you want to execute</li>
<li> <kbd>[parameters]</kbd> : Are the parameters for these commands</li>
</ul>
<p>A simple example will help you understand the syntax better. To list the content of an S3 bucket named <kbd>aml.packt</kbd><em>,</em> the command is as follows:</p>
<pre>
<strong>$ aws s3 ls aml.packt</strong>
</pre>
<p>Here, <kbd>s3</kbd> is the service, <kbd>ls</kbd> is the command, and <kbd>aml.packt</kbd> is the parameter. The <kbd>aws help</kbd> command will output a list of all available services.<br/>
To get help on a particular service and its commands, write the following:</p>
<pre>
<strong>$ aws &lt;service&gt; help</strong>
</pre>
<p>For instance, <kbd>aws s3 help</kbd> will inform you that the available <kbd>s3</kbd> commands on single objects are ls, mv, and rm for list, move, and remove, and that the basic <kbd>aws s3</kbd> command follows the following format:</p>
<pre>
<strong>$ aws s3 &lt;command&gt; sourceURI destinationURI  [parameters]</strong>
</pre>
<p>Here, <kbd>sourceURI</kbd> or <kbd>destinationURI</kbd> can be a file (or multiple files) on your local machine and a file on S3 or both files on S3. Take the following, for instance:</p>
<pre>
<strong>$ aws s3 cp /tmp/foo/ s3://The_Bucket/ --recursive --exclude "*" --include "*.jpg"</strong>
</pre>
<p>This will copy all (thanks to the parameter — recursive) JPG files (and only <kbd>*.jpg</kbd> files) in the <kbd>/tmp/foo</kbd> folder on your local machine to the S3 bucket named <kbd>The_Bucket</kbd>.</p>
<p>There are many more examples and explanations on the AWS documentation available at<br/>
<a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html" target="_blank">http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html</a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Passing parameters using JSON files</h1>
            </header>

            <article>
                
<p>For some services and commands, the list of parameters can become long and difficult to check and maintain.</p>
<p>For instance, in order to create an Amazon ML model via the CLI, you need to specify at least seven different elements: the Model ID, name, type, the model's parameters, the ID of the training data source, and the recipe name and URI (<kbd>aws machinelearning create-ml-model help</kbd> ).</p>
<p>When possible, we will use the CLI ability to read parameters from a JSON file instead of specifying them in the command line. AWS CLI also offers a way to generate a JSON template, which you can then use with the right parameters. To generate that JSON parameter file model (the JSON skeleton), simply add <kbd>--generate-cli-skeleton</kbd> after the command name. For instance, to generate the JSON skeleton for the create model command of the machine learning service, write the following:</p>
<pre>
<strong>$ aws machinelearning create-ml-model --generate-cli-skeleton</strong>
</pre>
<p>This will give the following output:</p>
<pre>
{<br/>   "MLModelId": "",<br/>   "MLModelName": "",<br/>   "MLModelType": "",<br/>   "Parameters": {<br/>       "KeyName": ""<br/>   },<br/>   "TrainingDataSourceId": "",<br/>   "Recipe": "",<br/>   "RecipeUri": ""<br/>}
</pre>
<p>You can then configure this to your liking.</p>
<p>To have the skeleton command generate a JSON file and not simply output the skeleton in the terminal, add <kbd>&gt; filename.json</kbd>:</p>
<pre>
<strong>$ aws machinelearning create-ml-model --generate-cli-skeleton &gt; filename.json</strong>
</pre>
<p>This will create a <kbd>filename.json</kbd> file with the JSON template. Once all the required parameters are specified, you create the model with the command (assuming the <kbd>filename.json</kbd> is in the current folder):</p>
<pre>
<strong>$ aws machinelearning create-ml-model file://filename.json</strong>
</pre>
<p>Before we dive further into the machine learning workflow via the CLI, we need to introduce the dataset we will be using in this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Introducing the Ames Housing dataset</h1>
            </header>

            <article>
                
<p>In this chapter, we will use the <kbd>Ames Housing</kbd> dataset that was compiled by <em>Dean De Cock</em> for use in data science education. It is a great alternative to the popular but older <kbd>Boston Housing</kbd> dataset. The <kbd>Ames Housing</kbd> dataset is used in the Advanced Regression Techniques challenge on the Kaggle website: <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/</a>. The original version of the dataset is available: <a href="http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls">http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls</a> and in the GitHub repository for this chapter.</p>
<p>The <kbd>Ames Housing</kbd> dataset contains 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa with the goal of predicting the selling price of each home. The dataset has 2930 rows. The high number of variables makes this dataset a good candidate for Feature Selection.</p>
<p>For more information on the genesis of this dataset and an in-depth explanation of the different variables, read the paper by <em>Dean De Cock</em> available in PDF at <a href="https://ww2.amstat.org/publications/jse/v19n3/decock.pdf">https://ww2.amstat.org/publications/jse/v19n3/decock.pdf</a>.</p>
<p>As usual, we will start by splitting the dataset into a train and a validate set and build a model on the train set. Both train and validate sets are available in the GitHub repository as <kbd>ames_housing_training.csv</kbd> and <kbd>ames_housing_validate.csv</kbd>. The entire dataset is in the <kbd>ames_housing.csv</kbd> file.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Splitting the dataset with shell commands</h1>
            </header>

            <article>
                
<p>The command line is an often forgotten but powerful ally to the data scientist. Many very powerful operations on the data can be achieved with the right shell commands and executed blazingly fast. To illustrate this, we will use shell commands to shuffle, split, and create training and validation subsets of the <kbd>Ames Housing</kbd> dataset:</p>
<ol>
<li>First, extract the first line into a separate file, <kbd>ames_housing_header.csv</kbd> and remove it from the original file:</li>
</ol>
<pre>
<strong>        $ head -n 1 ames_housing.csv &gt; ames_housing_header.csv</strong>
</pre>
<ol start="2">
<li>We just tail all the lines after the first one into the same file:</li>
</ol>
<pre>
<strong>        $ tail -n +2 ames_housing.csv &gt; ames_housing_nohead.csv</strong>
</pre>
<ol start="3">
<li>Then randomly sort the rows into a temporary file. (<kbd>gshuf</kbd> is the OSX equivalent of the Linux <strong>shuf shell</strong> command. It can be installed via <kbd>brew install coreutils</kbd>):</li>
</ol>
<pre>
<strong>        $ gshuf ames_housing_nohead.csv -o ames_housing_nohead.csv</strong>
</pre>
<ol start="4">
<li>Extract the first 2,050 rows as the training file and the last 880 rows as the validation file:</li>
</ol>
<pre>
<strong>        $ head -n 2050 ames_housing_nohead.csv &gt; ames_housing_training.csv</strong><br/><strong>    <br/>        $ tail -n 880 ames_housing_nohead.csv &gt; ames_housing_validate.csv</strong>
</pre>
<ol start="5">
<li>Finally, add back the header into both training and validation files:</li>
</ol>
<pre>
<strong>        $ cat ames_housing_header.csv ames_housing_training.csv &gt; tmp.csv <br/>        $ mv tmp.csv ames_housing_training.csv</strong><br/><br/><strong>        $ cat ames_housing_header.csv ames_housing_validate.csv &gt; tmp.csv<br/>        $ mv tmp.csv ames_housing_validate.csv</strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">A simple project using the CLI</h1>
            </header>

            <article>
                
<p>We are now ready to execute a simple Amazon ML workflow using the CLI. This includes the following:</p>
<ul>
<li>Uploading files on S3</li>
<li>Creating a datasource and the recipe</li>
<li>Creating a model</li>
<li>Creating an evaluation</li>
<li>Prediction batch and real time</li>
</ul>
<p>Let's start by uploading the training and validation files to S3. In the following lines, replace the bucket name <kbd>aml.packt</kbd> with your own bucket name.</p>
<p>To upload the files to the S3 location <kbd>s3://aml.packt/data/ch8/</kbd>, run the following command lines:</p>
<pre>
<strong>$ aws s3 cp ./ames_housing_training.csv s3://aml.packt/data/ch8/</strong><br/><strong>upload: ./ames_housing_training.csv to s3://aml.packt/data/ch8/ames_housing_training.csv</strong><br/><br/><strong>$ aws s3 cp ./ames_housing_validate.csv s3://aml.packt/data/ch8/</strong><br/><strong>upload: ./ames_housing_validate.csv to s3://aml.packt/data/ch8/ames_housing_validate.csv</strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An overview of Amazon ML CLI commands</h1>
            </header>

            <article>
                
<p>That's it for the S3 part. Now let's explore the CLI for Amazon's machine learning service.<br/>
All Amazon ML CLI commands are available at <a href="http://docs.aws.amazon.com/cli/latest/reference/machinelearning/">http://docs.aws.amazon.com/cli/latest/reference/machinelearning/</a>. There are 30 commands, which can be grouped by object and action.</p>
<p>You can perform the following:</p>
<ul>
<li><kbd>create</kbd> : creates the object</li>
<li><kbd>describe</kbd>: searches objects given some parameters (location, dates, names, and so on)</li>
<li><kbd>get</kbd>: given an object ID, returns information</li>
<li><kbd>update</kbd>: given an object ID, updates the object</li>
<li><kbd>delete</kbd>: deletes an object</li>
</ul>
<p>These can be performed on the following elements:</p>
<ul>
<li>datasource
<ul>
<li><kbd>create-data-source-from-rds</kbd></li>
<li><kbd>create-data-source-from-redshift</kbd></li>
<li><kbd>create-data-source-from-s3</kbd></li>
<li><kbd>describe-data-sources</kbd></li>
<li><kbd>delete-data-source</kbd></li>
<li><kbd>get-data-source</kbd></li>
<li><kbd>update-data-source</kbd></li>
</ul>
</li>
<li>ml-model
<ul>
<li><kbd>create-ml-model</kbd></li>
<li><kbd>describe-ml-models</kbd></li>
<li><kbd>get-ml-model</kbd></li>
<li><kbd>delete-ml-model</kbd></li>
<li><kbd>update-ml-model</kbd></li>
</ul>
</li>
<li>evaluation
<ul>
<li><kbd>create-evaluation</kbd></li>
<li><kbd>describe-evaluations</kbd></li>
<li><kbd>get-evaluation</kbd></li>
<li><kbd>delete-evaluation</kbd></li>
<li><kbd>update-evaluation</kbd></li>
</ul>
</li>
<li>batch prediction
<ul>
<li><kbd>create-batch-prediction</kbd></li>
<li><kbd>describe-batch-predictions</kbd></li>
<li><kbd>get-batch-prediction</kbd></li>
<li><kbd>delete-batch-prediction</kbd></li>
<li><kbd>update-batch-prediction</kbd></li>
</ul>
</li>
<li>real-time end point
<ul>
<li><kbd>create-realtime-endpoint</kbd></li>
<li><kbd>delete-realtime-endpoint</kbd></li>
<li><kbd>predict</kbd></li>
</ul>
</li>
</ul>
<p>You can also handle tags and set waiting times.</p>
<p>Note that the AWS CLI gives you the ability to create datasources from S3, Redshift, and RDS, while the web interface only allowed datasources from S3 and Redshift.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating the datasource</h1>
            </header>

            <article>
                
<p>We will start by creating the datasource. Let's first see what parameters are needed by generating the following skeleton:</p>
<pre>
<strong>$ aws machinelearning create-data-source-from-s3 --generate-cli-skeleton</strong>
</pre>
<p>This generates the following JSON object:</p>
<pre>
{<br/>   "DataSourceId": "",<br/>   "DataSourceName": "",<br/>   "DataSpec": {<br/>       "DataLocationS3": "",<br/>       "DataRearrangement": "",<br/>       "DataSchema": "",<br/>       "DataSchemaLocationS3": ""<br/>   },<br/>   "ComputeStatistics": true<br/>}
</pre>
<p>The different parameters are mostly self-explanatory and further information can be found on the AWS documentation at <a href="http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-s3.html">http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-s3.html</a>.</p>
<p>A word on the schema: when creating a datasource from the web interface, you have the possibility to use a wizard, to be guided through the creation of the schema. As you may recall, you are guided through several screens where you can specify the type of all the columns, and the existence of a target variable and an index column. The wizard facilitates the process by guessing the type of the variables, thus making available a default schema that you can modify.</p>
<p>There is no default schema available via the AWS CLI. You have to define the entire schema yourself, either in a JSON format in the <kbd>DataSchema</kbd> field or by uploading a schema file to S3 and specifying its location, in the <kbd>DataSchemaLocationS3</kbd> field.</p>
<p>Since our dataset has many variables (79), we cheated and used the wizard to create a default schema that we uploaded to S3. Throughout the rest of the chapter, we will specify the schema location not its JSON definition.</p>
<p>In this example, we will create the following datasource parameter file, <kbd>dsrc_ames_housing_001.json</kbd>:</p>
<pre>
{<br/>   "DataSourceId": "ch8_ames_housing_001",<br/>   "DataSourceName": "[DS] Ames Housing 001",<br/>   "DataSpec": {<br/>       "DataLocationS3": <br/>         "s3://aml.packt/data/ch8/ames_housing_training.csv",<br/>       "DataSchemaLocationS3": <br/>         "s3://aml.packt/data/ch8/ames_housing.csv.schema"        <br/>   },<br/>   "ComputeStatistics": true<br/>}
</pre>
<p>For the validation subset (save to <kbd>dsrc_ames_housing_002.json</kbd>):</p>
<pre>
{<br/>   "DataSourceId": "ch8_ames_housing_002",<br/>   "DataSourceName": "[DS] Ames Housing 002",<br/>   "DataSpec": {<br/>       "DataLocationS3": <br/>         "s3://aml.packt/data/ch8/ames_housing_validate.csv",<br/>       "DataSchemaLocationS3": <br/>         "s3://aml.packt/data/ch8/ames_housing.csv.schema"        <br/>   },<br/>   "ComputeStatistics": true<br/>}
</pre>
<p>Since we have already split our data into a training and a validation set, there's no need to specify the data <kbd>DataRearrangement</kbd> field.</p>
<p>Alternatively, we could also have avoided splitting our dataset and specified the following <kbd>DataRearrangement</kbd> on the original dataset, assuming it had been already shuffled: (save to <kbd>dsrc_ames_housing_003.json</kbd>):</p>
<pre>
{<br/>   "DataSourceId": "ch8_ames_housing_003",<br/>   "DataSourceName": "[DS] Ames Housing training 003",<br/>   "DataSpec": {<br/>       "DataLocationS3": <br/>         "s3://aml.packt/data/ch8/ames_housing_shuffled.csv",<br/>       "DataRearrangement": <br/>         "{"splitting":{"percentBegin":0,"percentEnd":70}}",<br/>       "DataSchemaLocationS3":<br/>         "s3://aml.packt/data/ch8/ames_housing.csv.schema"        <br/>   },<br/>   "ComputeStatistics": true<br/>}
</pre>
<p>For the validation set (save to <kbd>dsrc_ames_housing_004.json</kbd>):</p>
<pre>
{<br/>   "DataSourceId": "ch8_ames_housing_004",<br/>   "DataSourceName": "[DS] Ames Housing validation 004",<br/>   "DataSpec": {<br/>       "DataLocationS3":<br/>         "s3://aml.packt/data/ch8/ames_housing_shuffled.csv",<br/>       "DataRearrangement": <br/>         "{"splitting":{"percentBegin":70,"percentEnd":100}}",<br/>   },<br/>   "ComputeStatistics": true<br/>}
</pre>
<p>Here, the <kbd>ames_housing.csv</kbd> file has previously been shuffled using the <kbd>gshuf</kbd> command line and uploaded to S3:</p>
<pre>
<strong>$ gshuf ames_housing_nohead.csv -o ames_housing_nohead.csv</strong><br/><strong>$ cat ames_housing_header.csv ames_housing_nohead.csv &gt; tmp.csv<br/>$ mv tmp.csv ames_housing_shuffled.csv</strong><br/><strong>$ aws s3 cp ./ames_housing_shuffled.csv s3://aml.packt/data/ch8/</strong>
</pre>
<p>Note that we don't need to create these four datasources; these are just examples of alternative ways to create datasources.</p>
<p>We then create these datasources by running the following:</p>
<pre>
<strong>$ aws machinelearning create-data-source-from-s3 --cli-input-json file://dsrc_ames_housing_001.json</strong>
</pre>
<p>We can check whether the datasource creation is pending:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="21" src="assets/B05028_08_01.png" width="571"/></div>
<p>In return, we get the datasoure ID we had specified:</p>
<pre>
{<br/>   "DataSourceId": "ch8_ames_housing_001"<br/>}
</pre>
<p>We can then obtain information on that datasource with the following:</p>
<pre>
<strong>$ aws machinelearning  get-data-source --data-source-id ch8_ames_housing_001</strong>
</pre>
<p>This returns the following:</p>
<pre>
{<br/>   "Status": "COMPLETED",<br/>   "NumberOfFiles": 1,<br/>   "CreatedByIamUser": "arn:aws:iam::178277xxxxxxx:user/alexperrier",<br/>   "LastUpdatedAt": 1486834110.483,<br/>   "DataLocationS3": "s3://aml.packt/data/ch8/ames_housing_training.csv",<br/>   "ComputeStatistics": true,<br/>   "StartedAt": 1486833867.707,<br/>   "LogUri": "https://eml-prod-emr.s3.amazonaws.com/178277513911-ds-ch8_ames_housing_001/.....",<br/>   "DataSourceId": "ch8_ames_housing_001",<br/>   "CreatedAt": 1486030865.965,<br/>   "ComputeTime": 880000,<br/>   "DataSizeInBytes": 648150,<br/>   "FinishedAt": 1486834110.483,<br/>   "Name": "[DS] Ames Housing 001"<br/>}
</pre>
<p>Note that we have access to the operation log URI, which could be useful to analyze the model training later on.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating the model</h1>
            </header>

            <article>
                
<p>Creating the model with the <kbd>create-ml-model</kbd> command follows the same steps:</p>
<ol>
<li>Generate the skeleton with the following:</li>
</ol>
<pre>
        $ aws machinelearning create-ml-model --generate-cli-skeleton &gt; <br/>        mdl_ames_housing_001.json
</pre>
<ol start="2">
<li>Write the configuration file:</li>
</ol>
<pre>
        {<br/>            "MLModelId": "ch8_ames_housing_001",<br/>            "MLModelName": "[MDL] Ames Housing 001",<br/>            "MLModelType": "REGRESSION",<br/>            "Parameters": {<br/>                "sgd.shuffleType": "auto",<br/>                "sgd.l2RegularizationAmount": "1.0E-06",<br/>                "sgd.maxPasses": "100"<br/>            },<br/>            "TrainingDataSourceId": "ch8_ames_housing_001",<br/>            "RecipeUri": "s3://aml.packt/data/ch8<br/>              /recipe_ames_housing_001.json"<br/>        }
</pre>
<p style="padding-left: 60px">Note the parameters of the algorithm. Here, we used mild L2 regularization and 100 passes.</p>
<ol start="3">
<li>Launch the model creation with the following:</li>
</ol>
<pre>
        $ aws machinelearning create-ml-model --cli-input-json <br/>        file://mdl_ames_housing_001.json
</pre>
<ol start="4">
<li>The model ID is returned:</li>
</ol>
<pre>
        {<br/>            "MLModelId": "ch8_ames_housing_001"<br/>        }
</pre>
<ol start="5">
<li>This <kbd>get-ml-model</kbd> command gives you a status update on the operation as well as the URL to the log.</li>
</ol>
<pre>
        $ aws machinelearning get-ml-model --ml-model-id <br/>        ch8_ames_housing_001
</pre>
<ol start="6">
<li>The <kbd>watch</kbd> command allows you to repeat a shell command every <em>n</em> seconds. To get the status of the model creation every <em>10s</em>, just write the following:</li>
</ol>
<pre>
        $ watch -n 10 aws machinelearning get-ml-model --ml-model-id <br/>        ch8_ames_housing_001
</pre>
<p>The output of the <kbd>get-ml-model</kbd> will be refreshed every 10s until you kill it.</p>
<div class="packt_tip">It is not possible to create the default recipe via the AWS CLI commands. You can always define a blank recipe that would not carry out any transformation on the data. However, the default recipe has been shown to be positively impacting the model performance. To obtain this default recipe, we created it via the web interface, copied it into a file that we uploaded to S3. The resulting file <kbd>recipe_ames_housing_001.json</kbd> is available in our GitHub repository. Its content is quite long as the dataset has 79 variables and is not reproduced here for brevity purposes.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Evaluating our model with create-evaluation</h1>
            </header>

            <article>
                
<p>Our model is now trained and we would like to evaluate it on the evaluation subset. For that, we will use the <kbd>create-evaluation</kbd> CLI command:</p>
<ol>
<li>Generate the skeleton:</li>
</ol>
<pre>
        $ aws machinelearning create-evaluation --generate-cli-skeleton &gt;  <br/>        eval_ames_housing_001.json
</pre>
<ol start="2">
<li>Configure the parameter file:</li>
</ol>
<pre>
        {<br/>            "EvaluationId": "ch8_ames_housing_001",<br/>            "EvaluationName": "[EVL] Ames Housing 001",<br/>            "MLModelId": "ch8_ames_housing_001",<br/>            "EvaluationDataSourceId": "ch8_ames_housing_002"<br/>        }
</pre>
<ol start="3">
<li>Launch the evaluation creation:</li>
</ol>
<pre>
        $ aws machinelearning create-evaluation --cli-input-json <br/>        file://eval_ames_housing_001.json
</pre>
<ol start="4">
<li>Get the evaluation information:</li>
</ol>
<pre>
        $ aws machinelearning get-evaluation --evaluation-id <br/>        ch8_ames_housing_001
</pre>
<ol start="5">
<li>From that output, we get the performance of the model in the form of the RMSE:</li>
</ol>
<pre>
        "PerformanceMetrics": {<br/>            "Properties": {<br/>                 "RegressionRMSE": "29853.250469108018"<br/>            }<br/>        }
</pre>
<p>The value may seem big, but it is relative to the range of the <kbd>salePrice</kbd> variable for the houses, which has a mean of 181300.0 and std of 79886.7. So an RMSE of 29853.2 is a decent score.</p>
<div class="packt_infobox">You don't have to wait for the datasource creation to be completed in order to launch the model training. Amazon ML will simply wait for the parent operation to conclude before launching the dependent one. This makes chaining operations possible.</div>
<p>The next step would be to make batch predictions or create a real-time endpoint. These would follow the exact same steps of model creation and evaluation, and are not presented here.</p>
<p>At this point, we have a trained and evaluated model. We chose a certain set of parameters and carried out a certain preprocessing of the data via the default recipe. We now would like to know whether we can improve on that model and feature set by trying new parameters for the algorithm and doing some creative feature engineering. We will then train our new models and evaluate them on the validation subset. As we've seen before, the problem with that approach is that our evaluation score can be highly dependent on the evaluation subset. Shuffling the data to generate new training and validation sets may result in different model performance and make us choose the wrong model. Even though we have shuffled the data to avoid sequential patterns, there is no way to be sure that our split is truly neutral and that both subsets show similar data distribution. One of the subsets could present anomalies such as outliers, or missing data that the other does not have. To solve this problem, we turn to cross-validation.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">What is cross-validation?</h1>
            </header>

            <article>
                
<p>To lower the dependence on the data distribution in each split, the idea is to run many trials in parallel, each with a different data split, and average the results. This is called cross-validation.</p>
<p>The idea is simply to average the model performance across K trials, where each trial is built on a different split of the original dataset. There are many strategies to split the dataset. The most common one is called <strong>k-fold cross-validation</strong> and consists of splitting the dataset into <strong>K chunks</strong>, and for each trial using <em>K-1</em> chunks aggregated to train the model and the remaining chunk to evaluate it. Another strategy, called <strong>leave-one-out</strong> (<strong>LOO</strong>), comes from taking this idea to its extreme with <em>K</em> as the number of samples. You train your model on all the samples except one and estimate the error on the remaining sample. LOO is obviously more resource intensive.</p>
<p>The strategy we will implement is called <strong>Monte Carlo cross-validation</strong>, where the initial dataset is randomly split into a training and validation set in each trial. The advantage of that method over k-fold cross validation is that the proportion of the training/validation split is not dependent on the number of iterations (<em>K</em>). Its disadvantage is that some samples may never be selected in the validation subset, whereas others may be selected more than once. Validation subsets may overlap.</p>
<p>Let's look at an example with <kbd>k =5</kbd> trials. We will repeat these steps five times to evaluate one model (for instance, L2 mild regularization):</p>
<ol>
<li>Shuffle the Ames Housing dataset.</li>
<li>Split the dataset into training and validation subsets.</li>
<li>Train the model on the training set.</li>
<li>Evaluate the model on the validation set.</li>
</ol>
<p>At this point, we have five measures of the model performance; we average it to get a measure of overall model performance. We repeat the aforementioned five steps to evaluate another model (for instance, L1 medium regularization). Once we have tested all our models, we select the one that gives the best average performance on the trials.</p>
<p>This is why scripting becomes a necessity. To test one model setup, a cross-validation with <em>K trials</em> (K fold or Monte Carlo) requires <kbd>2*K</kbd> datasources, K models, and K evaluations. This will surely be too time-consuming when done via the web interface alone. This is where scripting the whole process becomes extremely useful and much more efficient.</p>
<p>There are many ways to actually create the different subset files for cross-validation. The simplest way might be to use a spreadsheet editor with random sorting, and some cutting and pasting. R and Python libraries, such as the popular <kbd>scikit-learn</kbd> library or the Caret package, have rich methods that can be used out of the box. However, since this chapter is about the AWS command line interface, we will use shell commands to generate the files. We will also write shell scripts to generate the sequence of AWS CLI commands in order to avoid manually editing the same commands for the different data files and models.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Implementing Monte Carlo cross-validation</h1>
            </header>

            <article>
                
<p>We will now implement a Monte Carlo cross-validation strategy with five trials using shell commands and AWS CLI. And we will use this evaluation method to compare two models, one with L2 mild regularization and the second with L1 heavy regularization on the Ames Housing dataset. Cross-validation will allow us to conclude with some level of confidence which model performs better.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Generating the shuffled datasets</h1>
            </header>

            <article>
                
<p>We will use the datasource creation <kbd>DataRearrangement</kbd> field to split the data into a training and a validation subset. So, we only need to create five files of shuffled data in the first place.</p>
<p>The following shell script will create five shuffled versions of the <kbd>Ames Housing</kbd> dataset and upload the files to S3. You can either save that code in a file with the <kbd>.sh</kbd> extension (<kbd>datasets_creation.sh</kbd>) or run it with <kbd>sh ./datasets_creation.sh</kbd>:</p>
<pre>
#!/bin/bash<br/>for k in 1 2 3 4 5 <br/>do<br/>    filename="data/ames_housing_shuffled_$k.csv"<br/>    gshuf data/ames_housing_nohead.csv -o data/ames_housing_nohead.csv<br/>    cat data/ames_housing_header.csv data/ames_housing_nohead.csv &gt; tmp.csv;<br/>    mv tmp.csv $filename<br/>    aws s3 cp ./$filename s3://aml.packt/data/ch8/<br/>done
</pre>
<div class="packt_infobox">
<p>Note that in this chapter, the code is organized around the following folder structure. All the command lines are run from the root folder, for instance, to run a Python script: <kbd>python py/the_script.py</kbd>, to list the data files <kbd>ls data/</kbd> and to run shell scripts: <kbd>sh ./shell/the_script.sh</kbd>.</p>
<p><kbd>.</kbd><br/>
<kbd>├── data</kbd><br/>
<kbd>├── images</kbd><br/>
<kbd>├── py</kbd><br/>
<kbd>└── shell<br/></kbd>All the shell scripts and command are based on bash shell and should probably require adaptation to other shells such as zsh.</p>
</div>
<p>Our datasets have been created and uploaded to S3. The general strategy is now to create templates for each of the parameter JSON files required for the Amazon ML CLI commands: create datasources, models, and evaluations. We will create the template files for the following:</p>
<ul>
<li>Training datasource</li>
<li>Evaluation datasource</li>
<li>L2 model</li>
<li>L1 model</li>
<li>L2 evaluation</li>
<li>L1 evaluation</li>
</ul>
<p>In all these template files, we will index the filenames with <kbd>{k}</kbd> and use the <kbd>sed</kbd> shell command to replace <kbd>{k}</kbd> with the proper index (1 to 5). Once we have the template files, we can use a simple shell script to generate the actual JSON parameter files for the datasources, models, and evaluations. We will end up with the following:</p>
<ul>
<li>10 datasource configuration files (five for training and five for evaluation)</li>
<li>10 model configuration files (five for L2 and five for L1)</li>
<li>10 evaluation configuration files (one for each of the models)</li>
</ul>
<p>In the end, we will obtain five RMSE results for the L2 model and five RMSE results for the L1 model, whose average will tell us which model is the best, which type of regularization should be selected to make sales price predictions on the Ames Housing dataset.</p>
<p>Let's start by writing the configuration files.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Generating the datasources template</h1>
            </header>

            <article>
                
<p>The template for the training files is as follows:</p>
<pre>
{<br/>   "DataSourceId": "CH8_AH_training_00{k}",<br/>   "DataSourceName": "[DS AH] training 00{k}",<br/>   "DataSpec": {<br/>       "DataLocationS3": "s3://aml.packt/data/ch8/shuffled_{k}.csv",<br/>       "DataSchemaLocationS3":"s3://aml.packt/data/ch8<br/>        /ames_housing.csv.schema",<br/>       "DataRearrangement": "{"splitting":<br/>       {"percentBegin":0,"percentEnd":70}}"<br/>   },<br/>   "ComputeStatistics": true<br/>}
</pre>
<p>And the template for for validation datasources is as follows:</p>
<pre>
{<br/>   "DataSourceId": "CH8_AH_evaluate_00{k}",<br/>   "DataSourceName": "[DS AH] evaluate 00{k}",<br/>   "DataSpec": {<br/>       "DataLocationS3": "s3://aml.packt/data/ch8/shuffled_{k}.csv",<br/>       "DataSchemaLocationS3":"s3://aml.packt/data/ch8<br/>       /ames_housing.csv.schema",<br/>       "DataRearrangement": "{"splitting":<br/>       {"percentBegin":70,"percentEnd":100}}"<br/>   },<br/>   "ComputeStatistics": true<br/>}
</pre>
<p>The only different between the training and the validation templates are the names/IDs and the splitting ratio in the <kbd>DataRearrangement</kbd> field. We save these files to <kbd>dsrc_training_template.json</kbd> and <kbd>dsrc_validate_template.json</kbd> respectively.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Generating the models template</h1>
            </header>

            <article>
                
<p>In the case of a model with L2 regularization, the model template is as follows:</p>
<pre>
{<br/>   "MLModelId": "CH8_AH_L2_00{k}",<br/>   "MLModelName": "[MDL AH L2] 00{k}",<br/>   "MLModelType": "REGRESSION",<br/>   "Parameters": {<br/>       "sgd.shuffleType": "auto",<br/>       "sgd.l1RegularizationAmount": "0.0",<br/>       "sgd.l2RegularizationAmount": "1.0E-06",<br/>       "sgd.maxPasses": "100"<br/>   },<br/>   "TrainingDataSourceId": "CH8_AH_training_00{k}",<br/>   "RecipeUri": "s3://aml.packt/data/ch8/recipe_ames_housing_001.json"<br/>}
</pre>
<p>And for a model with L1 regularization, the model template is as follows:</p>
<pre>
{<br/>   "MLModelId": "CH8_AH_L1_00{k}",<br/>   "MLModelName": "[MDL AH L1] 00{k}",<br/>   "MLModelType": "REGRESSION",<br/>   "Parameters": {<br/>       "sgd.shuffleType": "auto",<br/>       "sgd.l1RegularizationAmount": "1.0E-04",<br/>       "sgd.l2RegularizationAmount": "0.0",<br/>       "sgd.maxPasses": "100"<br/>   },<br/>   "TrainingDataSourceId": "CH8_AH_training_00{k}",<br/>   "RecipeUri": "s3://aml.packt/data/ch8/recipe_ames_housing_001.json"<br/>}
</pre>
<p>Note that the same recipe is used for both models. If we wanted to compare the performance of data preprocessing strategies, we could modify the recipes used in both models. The template files are very similar. The only difference is in the model name and ID and in the values for the <kbd>l1RegularizationAmount</kbd> and <kbd>l2RegularizationAmount</kbd>. We save these files to <kbd>mdl_l2_template.json</kbd> and <kbd>mdl_l1_template.json</kbd> respectively<em><em>.<br/></em></em></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Generating the evaluations template</h1>
            </header>

            <article>
                
<p>In the case of a model with L2 regularization, the evaluation template is as follows:</p>
<pre>
{<br/>   "EvaluationId": "CH8_AH_L2_00{k}",<br/>   "EvaluationName": "[EVL AH L2] 00{k}",<br/>   "MLModelId": "CH8_AH_L2_00{k}",<br/>   "EvaluationDataSourceId": "CH8_AH_evaluate_00{k}"<br/>}
</pre>
<p>And for a model with L1 regularization, the evaluation template is as follows:</p>
<pre>
{<br/>   "EvaluationId": "CH8_AH_L1_00{k}",<br/>   "EvaluationName": "[EVL AH L1] 00{k}",<br/>   "MLModelId": "CH8_AH_L1_00{k}",<br/>   "EvaluationDataSourceId": "CH8_AH_evaluate_00{k}"<br/>}
</pre>
<p>Save these files to <kbd>eval_l2_template.json</kbd> and <kbd>eval_l1_template.json</kbd> espectively.<em><br/></em></p>
<p>We will now use these template files to generate the different configuration files for the datasources, models, and evaluations. To keep things separate, all the generated files are in a subfolder <kbd>cfg/</kbd>.</p>
<p>The following shell script generates the actual configuration files that we will feed to the AWS CLI Machine Learning commands. It uses the <kbd>sed</kbd> command to find and replace the instances of <kbd>{k}</kbd> with the numbers 1 to 5. The output is written to the configuration file. Since there will be many configuration files generated, the files are written in a <kbd>/cfg</kbd> subfolder under <kbd>/data</kbd>. The folder structure is now as follows:</p>
<pre>
.<br/>├── data<br/>│   └── cfg<br/>│   └── templates<br/>├── images<br/>├── py<br/>└── shell
</pre>
<pre>
#!/bin/bash<br/><br/>for k in 1 2 3 4 5 <br/>do<br/>    # training datasource<br/>    sed 's/{k}/1/g' data/templates/dsrc_training_template.json &gt; data/cfg<br/>    /dsrc_training_00$k.json<br/><br/>    # evaluation datasource<br/>    sed 's/{k}/1/g' data/templates/dsrc_validate_template.json &gt; data/cfg<br/>    /dsrc_validate_00$k.json<br/><br/>    # L2 model<br/>    sed 's/{k}/1/g' data/templates/mdl_l2_template.json &gt; data/cfg<br/>    /mdl_l2_00$k.json<br/><br/>    # L2 evaluation<br/>    sed 's/{k}/1/g' data/templates/eval_l2_template.json &gt; data/cfg<br/>    /eval_l2_00$k.json<br/><br/>    # L1 model<br/>    sed 's/{k}/1/g' data/templates/mdl_l1_template.json &gt; data/cfg<br/>    /mdl_l1_00$k.json<br/><br/>    # L1 evaluation<br/>    sed 's/{k}/1/g' data/templates/eval_l1_template.json &gt; data/cfg<br/>    /eval_l1_00$k.json<br/><br/>done
</pre>
<p>The last remaining step is to execute the AWS commands that will create the objects in Amazon ML. We also use a shell loop to execute the AWS CLI commands.</p>
<p>Create datasources for training and evaluation:</p>
<pre>
#!/bin/bash<br/>for k in 1 2 3 4 5 <br/>do<br/>    aws machinelearning create-data-source-from-s3 --cli-input-json <br/>    file://data/cfg/dsrc_kfold_training_00$k.json<br/>    aws machinelearning create-data-source-from-s3 --cli-input-json <br/>    file://data/cfg/dsrc_kfold_validate_00$k.json<br/>done
</pre>
<p>Train models with L2 and L1 regularization:</p>
<pre>
#!/bin/bash<br/>for k in 1 2 3 4 5 <br/>    aws machinelearning create-ml-model --cli-input-json file://data<br/>    /cfg/mdl_l2_00$k.json<br/>    aws machinelearning create-ml-model --cli-input-json file://data<br/>    /cfg/mdl_l1_00$k.json<br/>done
</pre>
<p>Evaluate trained models:</p>
<pre>
#!/bin/bash<br/>for k in 1 2 3 4 5 <br/>    aws machinelearning create-evaluation --cli-input-json file://cfg<br/>    /eval_l2_00$k.json<br/>    aws machinelearning create-evaluation --cli-input-json file://cfg<br/>    /eval_l1_00$k.json<br/>done
</pre>
<p>You can check the status of the different jobs with the <kbd>get-data-source</kbd>, <kbd>get-ml-model</kbd> and <kbd>get-evaluation</kbd> CLI commands or on the Amazon ML dashboard. Once all the evaluation is finished, you capture the RMSE for each model by first creating a couple of files to receive the RMSE score and then running the following, final shell loop:</p>
<pre>
#!/bin/bash<br/>for k in 1 2 3 4 5 <br/>    aws machinelearning get-evaluation --evaluation-id CH8_AH_L2_00$k | <br/>    grep RegressionRMSE &gt;&gt; l2_model_rmse.log<br/>    aws machinelearning get-evaluation --evaluation-id CH8_AH_L1_00$k |<br/>    grep RegressionRMSE &gt;&gt; l1_model_rmse.log<br/>done
</pre>
<p>The <kbd>get-evaluation</kbd> command, given the ID of the evaluation, returns a JSON-formatted string that is fed to a grepping command and added to the <kbd>l1/l2_model_rmse.log</kbd> files.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The results</h1>
            </header>

            <article>
                
<p>We end up with the following results for the two models:</p>
<pre>
l1 | 26570.0 | 28880.4 | 27287.8 | 29815.7 | 27822.0]<br/><br/>L2 | 36670.9 | 25804.3 | 28127.2 | 30539.0 | 24740.4
</pre>
<p>On average, L1 gives an RMSE of 28075.2 (std: 1151), while L2 gives an RMSE of 29176.4 (std: 4246.7). Not only is the L1 model better performing, but it is also more robust when it comes to handling data variations since its std is lower.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Conclusion</h1>
            </header>

            <article>
                
<p>Cross-validation may be too time-consuming to implement via shell only. There are many files to create and coordinate. There are simpler ways to implement cross-validation with libraries such as <kbd>scikit-learn</kbd> for Python or Caret for R, where the whole model training and evaluation loop over several training and validation sets only requires a few lines of code. However, we showed that it was possible to implement cross-validation with Amazon ML. Cross-validation is a key component of the data-science workflow. Not being able to do cross validation with Amazon ML would have been a significant flaw in the service. In the end, the AWS CLI for machine learning is a very powerful and useful tool to conduct sequences of trials and compare results across different models, datasets, recipes, and features.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Boto3, the Python SDK</h1>
            </header>

            <article>
                
<p>Another tool to interact with the Amazon ML service outside of the web interface is an SDK. Simply put, an SDK is a wrapper around an API that makes working with the service much simpler and more efficient, as many details of the interactions are taken care of. AWS offers SDKs in the most widespread languages such as PHP, Java, Ruby, .Net, and of course, Python. In this chapter, we will focus on working with the Amazon ML service through the Python SDK. The Python SDK requires the Boto3 module.</p>
<p>Installation of the Boto3 module is done via pip. Refer to the quickstart guide available at <a href="http://boto3.readthedocs.io/en/latest/guide/quickstart.html" target="_blank">http://boto3.readthedocs.io/en/latest/guide/quickstart.html</a> if you need more information and troubleshooting:</p>
<pre>
<strong>pip install boto3</strong>
</pre>
<p>Boto3 is available for most AWS services. The complete list can be found at <a href="http://boto3.readthedocs.io/en/latest/reference/services/index.html" target="_blank">http://boto3.readthedocs.io/en/latest/reference/services/index.html</a>. We will focus on Boto3 for S3 and Amazon ML.</p>
<p>Setting up permissions for SDK access can be done via the <kbd>aws configure</kbd> command that we followed at the beginning of this chapter, or directly by adding your access keys to the <kbd>~/.aws/credentials</kbd> file.</p>
<p>Overall, the <kbd>Boto3</kbd> logic is very similar to the AWS CLI logic and follows similar steps: declaring the service to be used and running commands with the appropriate set of parameters. Let's start with a simple example around S3 with the following Python script, which will list all the buckets in your account:</p>
<pre>
import boto3<br/># Initialize the S3 client<br/>s3 = boto3.resource('s3')<br/># List all the buckets in out account<br/>for bucket in s3.buckets.all():<br/>    print(bucket.name)
</pre>
<p>Uploading a local file to a bucket would be achieved by the following:</p>
<pre>
# load the file    <br/>data = open('data/ames_housing_nohead.csv', 'rb')<br/>s3.Object('aml.packt', 'data/ames_housing_nohead.csv').put(Body=data)
</pre>
<p>The put command returns a JSON string, with an HTTPStatusCode field with a 200 value, indicating that the upload was successful.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Working with the Python SDK for Amazon Machine Learning</h1>
            </header>

            <article>
                
<p>The list of available methods can be found at <a href="http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html">http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html</a> and closely follows the list of available commands for the AWS CLI for the Machine Learning service organized around the main objects: datasource, model, evaluation, batch prediction, and real-time endpoints. For each object, the methods are: create, update, describe, get, and delete.</p>
<p>We will now implement the standard Amazon ML workflow. But first, let's define a naming method for the objects we will create. An important part of the workflow revolves around naming convention for object names and IDs. When working with the CLI, we created the names and IDs on the fly. This time we will use the following function to name our objects:</p>
<pre>
def name_id_generation(prefix, mode, trial):<br/>    Id = '_'.join([prefix, mode, "%02d"%int(trial)])<br/>    name = "[%s] %s %02d"% (prefix, mode, int(trial) )<br/>    return {'Name':name, 'Id':Id}
</pre>
<p>This function takes in two strings and one integer as arguments, a prefix for the type of the object (datasource, model, and so on), a mode to specify training versus validation datasource, and a trial value to easily increment our experiments. The function returns a dictionary.</p>
<p>Let's now define a few variables that we will use later on in the script:</p>
<pre>
# The iteration number of our experiements<br/>trial = 5<br/># The S3 location of schemas and files<br/>data_s3   = 's3://aml.packt/data/ch8/ames_housing_shuffled.csv'<br/>schema_s3 = 's3://aml.packt/data/ch8/ames_housing.csv.schema'<br/>recipe_s3 = 's3://aml.packt/data/ch8/recipe_ames_housing_001.json'<br/><br/># And the parameters for the SGD algrithm<br/>sgd_params = {<br/>  "sgd.shuffleType": "auto",<br/>  "sgd.l1RegularizationAmount": "1.0E-04",<br/>  "sgd.maxPasses": "100"<br/>} 
</pre>
<p>We need to import the following libraries:</p>
<pre>
import boto3<br/>import time<br/>import json
</pre>
<p>Declare that we want to interact with the Machine Learning service:</p>
<pre>
client = boto3.client('machinelearning')
</pre>
<p>We are now all set to create our training and validation datasources with the following:</p>
<pre>
# Create datasource for training<br/>resource = name_id_generation('DS', 'training', trial)<br/>print("Creating datasources for training (%s)"% resource['Name'] )<br/>response = client.create_data_source_from_s3(<br/>  DataSourceId = resource['Id'] ,<br/>  DataSourceName = resource['Name'],<br/>  DataSpec = {<br/>    'DataLocationS3' : data_s3,<br/>    'DataSchemaLocationS3' : schema_s3,<br/>   'DataRearrangement':'{"splitting":{"percentBegin":0,"percentEnd":70}}'<br/>  },<br/>   ComputeStatistics = True<br/>)<br/><br/># Create datasource for validation<br/>resource = name_id_generation('DS', 'validation', trial)<br/>print("Creating datasources for validation (%s)"% resource['Name'] )<br/>response = client.create_data_source_from_s3(<br/>  DataSourceId = resource['Id'] ,<br/>  DataSourceName = resource['Name'],<br/>  DataSpec = {<br/>    'DataLocationS3': data_s3,<br/>    'DataSchemaLocationS3': schema_s3,<br/>    'DataRearrangement':'{"splitting":{"percentBegin":0,"percentEnd":70}}'<br/>  },<br/>  ComputeStatistics = True<br/>)
</pre>
<p>In both cases, we call on the naming function we defined earlier to generate the Name and ID of the datasource and use that dictionary when calling the <kbd>create_data_source_from_s3</kbd><span> Boto3</span> <span>method.</span></p>
<p>We launch the training of the model with the following:</p>
<pre>
# Train model with existing recipe<br/>resource = name_id_generation('MDL', '', trial) <br/>print("Training model (%s) with params:n%s"% <br/>               (resource['Name'], json.dumps(sgd_params, indent=4)) )<br/>response = client.create_ml_model(<br/>  MLModelId = resource['Id'],<br/>  MLModelName = resource['Name'],<br/>  MLModelType = 'REGRESSION',<br/>  Parameters = sgd_params,<br/>  TrainingDataSourceId= name_id_generation('DS', 'training', trial)['Id'],<br/>  RecipeUri = recipe_s3<br/>)
</pre>
<p>And create the evaluation:</p>
<pre>
resource = name_id_generation('EVAL', '', trial) <br/>print("Launching evaluation (%s) "% resource['Name'] )<br/>response = client.create_evaluation(<br/>  EvaluationId = resource['Id'],<br/>  EvaluationName = resource['Name'],<br/>  MLModelId = name_id_generation('MDL', '', trial)['Id'],<br/>  EvaluationDataSourceId = name_id_generation('DS', 'validation', trial)<br/>  ['Id']<br/>)
</pre>
<p>You can now go to the Amazon ML dashboard and verify that you have two datasources, one model, and one evaluation in the <span class="packt_screen">In progress</span> or <span class="packt_screen">Pending</span> status: </p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="115" src="assets/B05028_08_02.png" width="651"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Waiting on operation completion</h1>
            </header>

            <article>
                
<p>All these object, creation operations are, by default, chained by Amazon ML. This means that Amazon ML will wait on the datasources to be ready before launching the model training, and will also wait for the model training to be completed before trying to run the evaluation. However, at this point, we still need to wait for the evaluation to be complete before we can access its results. Similarly, we need to wait for the different objects to have been utilized by the next operation before deleting them.</p>
<p>This is where the waiter methods become useful. Waiters are methods that simply wait for an AWS operation to be completed, to have status <em>Completed.</em> Waiters exists for all AWS operations and services. Amazon ML offers four waiters for models, datasources, evaluations, and batch predictions: </p>
<ul>
<li><kbd>MachineLearning.Waiter.BatchPredictionAvailable</kbd></li>
<li><kbd>MachineLearning.Waiter.DataSourceAvailable</kbd></li>
<li><kbd>MachineLearning.Waiter.EvaluationAvailable</kbd></li>
<li><kbd>MachineLearning.Waiter.MLModelAvailable</kbd></li>
</ul>
<p>A Machine Learning waiter follows the syntax – first, declare the object the waiter has to monitor, for instance an evaluation:</p>
<pre>
waiter = client.get_waiter('evaluation_available')
</pre>
<p>Then call the <kbd>wait</kbd> method on the waiter you just declared:</p>
<pre>
waiter.wait(FilterVariable='Name', EQ='the name of the evaluation')
</pre>
<p>Once the wait method is called, the Python scripts hangs until the operation reaches a status of <kbd>Completed</kbd>. The wait function takes the following:</p>
<ul>
<li>A filter value: <kbd>FilterVariable <span class="o">= <span class="s1">CreatedAt</span></span></kbd><span class="s1">,</span> <span class="s1"><kbd>LastUpdatedAt</kbd><span>,</span></span> <span class="s1"><kbd>Status</kbd><span>,</span></span> <span class="s1"><kbd>Name</kbd><span>,</span></span> <span class="s1"><kbd>IAMUser</kbd><span>,</span></span> <span class="s1"><kbd>MLModelId</kbd><span>,</span></span> <span class="s1"><kbd>DataSourceId</kbd><span>,</span></span> <kbd><span class="s1">DataURI</span></kbd></li>
<li>An operator: EQ, GT, LT, GE, LE, NE</li>
<li>Other parameters that depend on the nature of the object</li>
</ul>
<p>With that parameter structure, you can make your script wait on a specific object completion, or wait on all the objects based on a datasource, a model, or even a user name. If we were to launch several evaluations for different models based on the same validation datasource, we would simply call a waiter for each model as such:<br/></p>
<pre>
waiter.wait(FilterVariable='<span>DataSourceId</span>', EQ='the DatasourceId')
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Wrapping up the Python-based workflow</h1>
            </header>

            <article>
                
<p>Now that we know how to wait for all our evaluation to finish, we still need to get the evaluation result and delete the models and datasources we have created. As in the case of the <kbd>get-evaluation</kbd> AWS CLI command, the Boto3 <kbd>get_evaluation</kbd> method returns a JSON string with the model performance measure, the RMSE in the case of regression. The following script wraps up our trial:</p>
<pre>
t0 = time.time()<br/># declare the waiter and call the wait method on the evaluation<br/>waiter = client.get_waiter('evaluation_available')<br/>print("Waiting on evaluation to finish ")<br/>waiter.wait(FilterVariable='Name', EQ=name_id_generation('EVAL', '', trial)['Name'])<br/>t = time.time() - t0<br/>print("Evaluation has finished after %sm %ss"% (int(t/60), t%60) )<br/># get the evaluation results<br/>response = client.get_evaluation(<br/>  EvaluationId=name_id_generation('EVAL', '', trial)['Id']<br/>)<br/>RMSE =float(response['PerformanceMetrics']['Properties']['RegressionRMSE'])<br/>print("[trial %0.2f] RMSE %0.2f"% (trial, RMSE) )<br/># and delete the resources<br/>print("Deleting datasources and model")<br/>response = client.delete_data_source(<br/>  DataSourceId=name_id_generation('DS', 'training', trial)['Id']<br/>)<br/>response = client.delete_data_source(<br/>  DataSourceId=name_id_generation('DS', 'validation', trial)['Id']<br/>)<br/>response = client.delete_ml_model(<br/>  MLModelId=name_id_generation('MDL', '', trial)['Id']<br/>)
</pre>
<p>Putting all the code blocks together returns the following output:</p>
<pre>
Creating datasources for training ([DS] training 04)<br/>Creating datasources for validation ([DS] validation 04)<br/>Training model ([MDL] 04) with params:<br/>{<br/>  "sgd.shuffleType": "auto",<br/>  "sgd.l1RegularizationAmount": "1.0E-04",<br/>  "sgd.maxPasses": "100"<br/>}<br/>Launching evaluation ([EVAL] 04)<br/>Waiting on evaluation to finish<br/>Evaluation has finished after 11m 43.78s<br/>[trial 4] RMSE 22437.33<br/>Deleting datasources and model
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Implementing recursive feature selection with Boto3</h1>
            </header>

            <article>
                
<p>In many real-world cases, the original dataset could have a very large number of variables. As dimensionality increases, so does the need for a larger sample set. This is called the <em>curse of dimensionality</em>, a classic predictive analytics problem. Simply put, if there is not enough diversity to infer a representative distribution for some variables, the algorithm will be unable to extract relevant information from the said variables. These low-signal variables drag down the algorithm's performance without adding any data fuel by adding useless complexity to the model. One strategy is to reduce the number of variables on which to train the model. However, that implies identifying which features can be dropped without significant loss of information.</p>
<p>There are many techniques to reduce the number of features:</p>
<ul>
<li><strong>Wrapper</strong> techniques: These use rules and criteria to select the best and most impacting features.</li>
<li><strong>Filter</strong> techniques: These use statistical tests to measure the importance of each feature. Measuring the correlation with the target could be a simple way to remove non-significant variables.</li>
<li><strong>Embedded</strong> methods: For certain models, such as random forests, iteratively train the model on subsets of features, it is possible to evaluate the impact of the features that are left out during each iteration and thus infer the importance of each feature. </li>
</ul>
<p>The method most adapted to the Amazon Machine Learning context is the recursive evaluation of each feature's importance, filtering out the least important ones by measuring when performance drops significantly with the discarding of a certain feature. It is a brute force version of Recursive Feature Elimination.</p>
<p>It follows these steps:</p>
<ol>
<li>Build an initial model with all <em>N</em> features.</li>
<li>Then identify and remove the least important features by:
<ul>
<li>Building N subsets, removing a different feature in each subset</li>
<li>Building a model for each subset and evaluating its performance</li>
<li>Identifying the features for which the model's performance was least impacted</li>
</ul>
</li>
</ol>
<ol start="3">
<li>You now have <em>N-1</em> features. Reiterate steps 1 to 3 to identify and remove the next least important feature.</li>
<li>Stop when you notice a significant drop in the model's performance compared to the initial N-feature model.</li>
</ol>
<p>The inverse version of this algorithm starts with <em>N</em> models, each built with just one feature, with a new feature added at each iteration. Choose the new feature as the new feature that generates the best increase in performance. Stop when adding new features does not lead to a significant increase in the model's performance.</p>
<p>In the rest of the chapter, we will show how to implement this feature selection strategy in Python.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Managing schema and recipe</h1>
            </header>

            <article>
                
<p>Removing or adding features to a dataset directly impacts the schema and the recipe. The schema is used when creating the datasources, while the recipe is needed to train the model, as it specifies which data transformation will be performed prior to the model training.</p>
<p>Modifying the schema to remove features from the dataset can be done by simply adding the names of the variable to the <kbd>excludedAttributeNames</kbd> field. We can take the initial schema, and each time we remove a feature from the initial feature list, we add it to the <span>excludedAttributeNames list. The steps are as follows:</span></p>
<ol>
<li>Open the JSON formatted schema into a schema dict</li>
<li>Append the feature name to schema [<kbd>'excludedAttributeNames'</kbd>]</li>
<li>Save the schema to a properly indented JSON file</li>
<li>Upload the file to S3</li>
</ol>
<p>When creating the datasource, we will point to the S3 location of the schema we just updated.</p>
<p>The initial recipe generated by default by Amazon ML for the Ames Housing dataset applies different quantile binning transformations on certain numerical features. The groups section of the recipe is as follows:</p>
<pre>
"groups": {<br/>  "NUMERIC_VARS_QB_50":   "group('LotFrontage','KitchenAbvGr','BsmtFinSF1','GarageCars','1stFlrSF','ScreenPorch','LowQualFinSF','LotArea','OverallCond','2ndFlrSF','GarageArea','EnclosedPorch','HalfBath')",<br/> "NUMERIC_VARS_QB_100": "group('BsmtFinSF2','WoodDeckSF','BsmtHalfBath','MiscVal','GrLivArea','Fireplaces')",<br/> "NUMERIC_VARS_QB_500": "group('OverallQual')",<br/> "NUMERIC_VARS_QB_20": "group('TotalBsmtSF')",<br/> "NUMERIC_VARS_QB_200": "group('MSSubClass','OpenPorchSF','YearRemod/Add','BsmtFullBath','MasVnrArea')",<br/> "NUMERIC_VARS_QB_10": "group('PoolArea','BedroomAbvGr','TotRmsAbvGrd','YearBuilt','MoSold','YrSold','GarageYrBlt','FullBath','BsmtUnfSF','3SsnPorch')"<br/> }
</pre>
<p>Adding or removing variable names from that structure requires a more complex script than just adding an element in a list. Since such a script would not add much educational value to the book, we decided to use a default simple recipe that does not perform any transformation on the dataset. As long as we have a baseline RMSE with all the features available, the recursive feature elimination strategy is still valid. The only difference is that the overall RMSE scores will probably be made higher by not applying any quantile binning to our numerical data. The recipe we use is defined by the following:</p>
<pre>
{<br/> "groups" : {},<br/> "assignments" : { },<br/> "outputs" : [ "ALL_INPUTS" ]<br/>}
</pre>
<p>This is available in our examples at the S3 location <kbd>s3://aml.packt/data/ch8/recipe_ames_housing_default.json</kbd>. Using that recipe to evaluate our baseline model gives a baseline RMSE of <em>61507.35.</em> We will use that baseline RMSE to see whether removing a feature improved (lower) or degraded (higher) the model performance. </p>
<p>The following script is broken into three parts:</p>
<ul>
<li>Initialization and functions</li>
<li>Launching the Amazon ML workflow</li>
<li>Getting the evaluation results and deleting the resources</li>
</ul>
<p>The script is available in our GitHub repo in its entirety. We use the same strategy to have a function to generate the names and IDs. We start with the following script to initialize the variable and declare the function:<br/></p>
<pre>
import pandas as pd<br/>import boto3<br/>import json<br/><br/># Local schema with all the features<br/>original_schema_filename = 'data/ames_housing.csv.schema'<br/># Initialize boto3 objects<br/>s3 = boto3.resource('s3')<br/>client = boto3.client('machinelearning')<br/><br/># load dataset and feature_ names<br/>df = pd.read_csv('data/ames_housing.csv')<br/>original_features = df.columns.difference(['SalePrice', 'Order'])<br/><br/># load original schema with all the features<br/>schema = json.load( open(original_schema_filename) )<br/><br/># SGD parameters: L1 heavy regularization<br/>sgd_parameters = {<br/>  "sgd.shuffleType": "auto",<br/>  "sgd.l1RegularizationAmount": "1.0E-04",<br/>  "sgd.maxPasses": "100"<br/>}<br/><br/># memorize all object Ids for future deletion<br/>baseline_rmse = 61507.35<br/>datasource_ids = []<br/>model_ids = []<br/>evaluation_ids = []<br/>features_rmse = {}<br/><br/>def generate_trial(n):<br/>  n = "X" + str(n).zfill(3)<br/>  return {<br/>   'schema_filename': "rfs_ames_housing_%s.schema"% n, <br/>   'recipe_s3': 's3://aml.packt/RFS/recipe_ames_housing_default.json',<br/>   'data_s3': 's3://aml.packt/RFS/ames_housing_shuffled.csv',<br/>   'datasource_training_id': "rfs_training_%s"% n,<br/>   'datasource_training_name': "[DS RFS] training %s"% n,<br/>   'datasource_validation_id': "rfs_validation_%s"% n,<br/>   'datasource_validation_name': "[DS RFS] validation %s"% n,<br/>   'model_id': "rfs_%s"% n,<br/>   'model_name': "[MDL RFS] %s"% n,<br/>   'evaluation_id': "rfs_%s"% n,<br/>   'evaluation_name': "[EVAL RFS] %s"% n,<br/>  }
</pre>
<p>We now launch the datasource, model, and evaluation creation. The script only looks at the first 10 features, and not the entire set of 79 features, to save on resources. </p>
<div class="packt_tip">You will notice that we added a prefix "<em>X"</em> to the numbering of the Amazon ML objects. We found that sometimes, Amazon ML cannot create an object if the IDs and names have been used on previous objects that have now been deleted. That problem may disappear after some time. In any case, making sure that all new datasources, models, evaluations, and batch predictions have names and IDs that have never been used previously removes any naming issue.</div>
<p>The second part launches the creation of the datasources, models, and evaluations:</p>
<pre>
for k in range(10):<br/> print("="* 10 + " feature: %s"% original_features[k])<br/> trial = generate_trial(k)<br/><br/> # remove feature[k] from schema and upload to S3<br/> schema['excludedAttributeNames'] = [original_features[k]]<br/> with open("data/%s"%trial['schema_filename'], 'w') as fp:<br/> json.dump(schema, fp, indent=4)<br/> s3.Object('aml.packt', "RFS/%s"% trial['schema_filename']).put(Body=open("data/%s"%trial['schema_filename'], 'rb'))<br/><br/> # create datasource<br/> print("Datasource %s"% trial['datasource_training_name'])<br/> datasource_ids.append( trial['datasource_training_id'] )<br/> response = client.create_data_source_from_s3(<br/>   DataSourceId = trial['datasource_training_id'] ,<br/>   DataSourceName= trial['datasource_training_name'] ,<br/>   DataSpec={<br/>     'DataLocationS3': trial['data_s3'],<br/>     'DataRearrangement': '{"splitting":<br/>      {"percentBegin":0,"percentEnd":70}}',<br/>     'DataSchemaLocationS3': "s3://aml.packt/RFS/%s"% <br/>     trial['schema_filename']<br/>   },<br/>   ComputeStatistics=True<br/> )<br/><br/> # Create datasource for validation<br/> print("Datasource %s"% trial['datasource_validation_name'])<br/> datasource_ids.append( trial['datasource_validation_id'] )<br/> response = client.create_data_source_from_s3(<br/> DataSourceId = trial['datasource_validation_id'] ,<br/> DataSourceName= trial['datasource_validation_name'] ,<br/> DataSpec={<br/> 'DataLocationS3': trial['data_s3'],<br/> 'DataRearrangement': '{"splitting":{"percentBegin":70,"percentEnd":100}}',<br/> 'DataSchemaLocationS3': "s3://aml.packt/RFS/%s"% trial['schema_filename']<br/> },<br/> ComputeStatistics=True<br/> )<br/><br/> # Train model with existing recipe<br/> print("Model %s"% trial['model_name'])<br/> model_ids.append(trial['model_id'] )<br/> response = client.create_ml_model(<br/> MLModelId = trial['model_id'],<br/> MLModelName = trial['model_name'],<br/> MLModelType = 'REGRESSION',<br/> Parameters = sgd_parameters,<br/> TrainingDataSourceId = trial['datasource_training_id'] ,<br/> RecipeUri = trial['recipe_s3']<br/> )<br/><br/> print("Evaluation %s"% trial['evaluation_name'])<br/> evaluation_ids.append(trial['evaluation_id'])<br/> response = client.create_evaluation(<br/> EvaluationId = trial['evaluation_id'],<br/> EvaluationName = trial['evaluation_name'],<br/> MLModelId = trial['model_id'],<br/> EvaluationDataSourceId= trial['datasource_validation_id'] <br/> )
</pre>
<p>Finally, the third part waits for the evaluation to be complete, records the RMSE for each removed feature, and deletes the datasources and models (we kept the evaluations to avoid having to rerun the whole script to get the results):</p>
<pre>
for k in range(10):<br/> trial = generate_trial(k)<br/> waiter = client.get_waiter('evaluation_available')<br/> print("Waiting on evaluation %s to finish "% trial['evaluation_name'])<br/> waiter.wait(FilterVariable='Name', EQ=trial['evaluation_name'])<br/> print("Evaluation has finished ")<br/><br/> response = client.get_evaluation( EvaluationId=trial['evaluation_id'] )<br/> features_rmse[original_features[k]] = float(response['PerformanceMetrics']['Properties']['RegressionRMSE'])<br/> print("[%s] RMSE %0.2f"% (original_features[k], float(response['PerformanceMetrics']['Properties']['RegressionRMSE'])) )<br/> # Now delete the resources<br/> print("Deleting datasources and model")<br/> response = client.delete_data_source(<br/> DataSourceId = trial['datasource_training_id']<br/> )<br/> response = client.delete_data_source(<br/> DataSourceId = trial['datasource_validation_id']<br/> )<br/> response = client.delete_ml_model(<br/> MLModelId = trial['model_id']<br/> )<br/><br/>print("removing the feature increased the RMSE by ")<br/>for k,v in features_rmse.items():<br/> print("%s t%0.2f %% "% (k, (baseline_rmse - v)/ baseline_rmse *100.0 ) )
</pre>
<p>In the end, we get the following RMSE variations  for the first 10 features:</p>
<ul>
<li><kbd>1stFlrSF 0.07%</kbd></li>
<li><kbd>2ndFlrSF -18.28%</kbd></li>
<li><kbd>BedroomAbvGr -0.02 %</kbd></li>
<li><kbd>BsmtExposure -0.56 %</kbd></li>
<li><kbd>BsmtFinSF2 -0.50 %</kbd></li>
<li><kbd>BsmtCond 0.00 %</kbd></li>
<li><kbd>BsmtFinSF1 -2.56 %</kbd></li>
<li><kbd>Alley 0.00 %</kbd></li>
<li><kbd>3SsnPorch -4.60 %</kbd></li>
<li><kbd>BldgType -0.00 %</kbd></li>
</ul>
<p>For instance, removing the <span><kbd>2ndFlrSF</kbd> feature increased the RMSE by nearly 20%. This feature is definitely very important to predict salesPrice. Similarly, features <kbd>3SsnPorch</kbd> and <kbd>BsmtFinSF1</kbd> are also important to the model, since removing them increases the RMSE. On the other hand, removing <kbd>1stFlrSF</kbd></span>, <span><kbd>Alley</kbd><em>,</em> <kbd>BedroomAbvGr</kbd><em> </em>or<em> </em><kbd>BldgType</kbd> only modified the RMSE by less than 0.10%. We can probably remove these feature without too much impact on the model performance.</span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we have moved away from the Amazon ML web interface and learned how to work with the service through the AWS CLI and the Python SDK. The commands and methods for both types of interaction are very similar. The functions and commands perform a standard set of operations from creation to deletion of Amazon ML objects: datasources, models, evaluation, and batch predictions. The fact that Amazon ML chains the sequence of dependent object creation allows you to create all the objects at once without having to wait for one upstream to finish (datasource or model) before creating the downstream one (model or evaluation). The waiter methods make it possible to wait for all evaluations to be completed before retrieving the results and making the necessary object deletion. </p>
<p>We showed how scripting Amazon ML allowed us to implement Machine Learning methods such as cross-validation and Recursive Feature Selection, both very useful methods in predictive analytics. Although we end up having to create many datasources and objects to conduct cross-validation and feature selection, the overall costs remain under control.</p>
<p>In the next chapter, we will start using other AWS services to expand the capabilities of Amazon ML. We will look at other datasources beyond S3, such as Redshift and RDS, and how to use Amazon Lambda for machine learning.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>