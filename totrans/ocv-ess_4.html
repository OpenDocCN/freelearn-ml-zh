<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;4.&#xA0;What's in the Image? Segmentation"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04" class="calibre1"/>Chapter 4. What's in the Image? Segmentation</h1></div></div></div><p class="calibre7">Segmentation<a id="id262" class="calibre1"/> is any process that partitions an image into multiple regions or segments. These will typically correspond to meaningful regions or objects, such as face, car, road, sky, grass, and so on. Segmentation is one of the most important stages in a computer vision system. In OpenCV, there is no specific module for segmentation, though a number of ready-to-use methods are available in other modules (most of them in <code class="email">imgproc</code>). In this chapter, we will cover the most important and frequently used methods available in the library. In some cases, additional processing will have to be added to improve the results or obtain seeds (this refers to rough segments that allow an algorithm to perform a complete segmentation). In this chapter we will look at the following major segmentation methods: thresholding, contours and connected components, flood filling, watershed segmentation, and the GrabCut algorithm.</p></div>

<div class="book" title="Chapter&#xA0;4.&#xA0;What's in the Image? Segmentation">
<div class="book" title="Thresholding"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch04lvl1sec29" class="calibre1"/>Thresholding</h1></div></div></div><p class="calibre7">Thresholding is<a id="id263" class="calibre1"/> one of the<a id="id264" class="calibre1"/> simplest yet most useful segmentation operations. We can safely say that you will end up using some sort of thresholding in almost any image-processing application. We consider it a segmentation operation since it partitions an image into two regions, typically, an object and its background. In OpenCV, thresholding is performed with the <a id="id265" class="calibre1"/>function <code class="email">double threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)</code>.</p><p class="calibre7">The first two parameters are the input and output images, respectively. The third input parameter is the threshold chosen. The meaning of <code class="email">maxval</code> is controlled by the type of thresholding we want to perform. The following table shows the operation performed for each type:</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Type</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">dst(x,y)</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">THRESH_BINARY</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">maxval</code> if <code class="literal">src(x,y)</code> is greater than <code class="literal">thresh</code> and <code class="literal">0</code> if otherwise</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">THRESH_BINARY_INV</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">0</code> if <code class="literal">src(x,y)</code> is greater than <code class="literal">thresh</code> and <code class="literal">maxval</code> if otherwise</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">THRESH_TRUNC</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">thresh</code> if <code class="literal">src(x,y)</code> is greater than <code class="literal">thresh</code> and <code class="literal">src(x,y)</code> if otherwise</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">THRESH_TOZERO</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">src(x,y)</code> if <code class="literal">src(x,y)</code> is greater than <code class="literal">thresh</code> and <code class="literal">0</code> if otherwise</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">THRESH_TOZERO_INV</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">0</code> if <code class="literal">src(x,y)</code> is greater than <code class="literal">thresh</code> and <code class="literal">src(x,y)</code> if otherwise</p>
</td></tr></tbody></table></div><p class="calibre7">While in previous<a id="id266" class="calibre1"/> OpenCV books (and the available reference manual) each type of <a id="id267" class="calibre1"/>thresholding is illustrated with the help of 1D signal plots, our experience shows that numbers and gray levels allow you to grasp the concept faster. The following table shows the effect of the different threshold types using a single-line image as an example input:</p><div class="mediaobject"><img src="../images/00021.jpeg" alt="Thresholding" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">The special <a id="id268" class="calibre1"/>value <code class="email">THRESH_OTSU</code> may be combined with the previous values (with the OR operator). In <a id="id269" class="calibre1"/>such cases, the threshold value is automatically estimated by the function (using Otsu's algorithm). This function returns the estimated threshold value.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note13" class="calibre1"/>Note</h3><p class="calibre7">Otsu's method obtains a threshold that best separates the background from the foreground's pixels (in an interclass/intraclass variance ratio sense). See the full explanation<a id="id270" class="calibre1"/> and demos at <a class="calibre1" href="http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html">http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html</a>.</p></div><p class="calibre7">While the function described uses a single threshold for the whole image, adaptive thresholding estimates a different threshold for each pixel. This produces a better result when the input image is less homogeneous (with unevenly illuminated regions, for example). The function to perform adaptive thresholding is as follows:</p><div class="informalexample"><pre class="programlisting">adaptiveThreshold(InputArray src, OutputArray dst, double maxValue, int adaptiveMethod, int thresholdType, int blockSize, double C)</pre></div><p class="calibre7">This<a id="id271" class="calibre1"/> function is <a id="id272" class="calibre1"/>similar to the previous one. The parameter <code class="email">thresholdType</code> must be either <code class="email">THRESH_BINARY</code> or <code class="email">THRESH_BINARY_INV</code>. This function computes a threshold for each pixel by computing a weighted average of pixels in a neighborhood minus a constant (<code class="email">C</code>). When <code class="email">thresholdType</code> is ADAPTIVE_THRESH_MEAN_C, the threshold computed is the mean of the neighborhood (that is, all the elements are weighted equally).When <code class="email">thresholdType</code> is ADAPTIVE_THRESH_GAUSSIAN_C, the pixels in the neighborhood are weighted according to a Gaussian function.</p><p class="calibre7">The following <code class="email">thresholding</code> example shows how to perform thresholding operations on an image:</p><div class="informalexample"><pre class="programlisting">#include "opencv2/opencv.hpp"
#include &lt;iostream&gt;

using namespace std;
using namespace cv;

Mat src, dst, adaptDst;
int threshold_value, block_size, C;

void thresholding( int, void* )
{
<span class="strong"><strong class="calibre8">  threshold( src, dst, threshold_value, 255, THRESH_BINARY );</strong></span>

  imshow( "Thresholding", dst );
}

void adaptThreshAndShow()
{
<span class="strong"><strong class="calibre8">    adaptiveThreshold( src, adaptDst, 255, CV_ADAPTIVE_THRESH_MEAN_C, THRESH_BINARY, block_size, C);</strong></span>
    imshow( "Adaptive Thresholding", adaptDst );
}

void adaptiveThresholding1( int, void* )
{
  static int prev_block_size=block_size;
  if ((block_size%2)==0)    // make sure that block_size is odd
  {
      if (block_size&gt;prev_block_size) block_size++;
      if (block_size&lt;prev_block_size) block_size--;
  }
  if (block_size&lt;=1) block_size=3;  // check block_size min value

  adaptThreshAndShow();
}

void adaptiveThresholding2( int, void* )
{
    adaptThreshAndShow();
}

int main(int argc, char *argv[])
{
    //Read original image and clone it to contain results
    src = imread("left12.jpg", CV_LOAD_IMAGE_GRAYSCALE );
    dst=src.clone();
    adaptDst=src.clone();

    //Create 3 windows
    namedWindow("Source", WINDOW_AUTOSIZE);
    namedWindow("Thresholding", WINDOW_AUTOSIZE);
    namedWindow("Adaptive Thresholding", WINDOW_AUTOSIZE);
    imshow("Source", src);

    //Create trackbars
    threshold_value=127;
    block_size=7;
    C=10;
    createTrackbar( "threshold", "Thresholding", &amp;threshold_value, 255, thresholding );
    createTrackbar( "block_size", "Adaptive Thresholding", &amp;block_size, 25, adaptiveThresholding1 );
    createTrackbar( "C", "Adaptive Thresholding", &amp;C, 255, adaptiveThresholding2 );

    //Perform operations a first time
    thresholding(threshold_value,0);
    adaptiveThresholding1(block_size, 0);
    adaptiveThresholding2(C, 0);

    // Position windows on screen
    moveWindow("Source", 0,0);
    moveWindow("Thresholding", src.cols,0);
    moveWindow("Adaptive Thresholding", 2*src.cols,0);

    cout &lt;&lt; "Press any key to exit...\n";
    waitKey(); // Wait for key press
    return 0;
}</pre></div><p class="calibre7">The<a id="id273" class="calibre1"/> example in the <a id="id274" class="calibre1"/>preceding code creates three windows with the source image, which is loaded in grayscale, and the result of thresholding and adaptive thresholding. Then, it creates three trackbars: one associated to the thresholding result window (to handle the threshold value) and two associated to the adaptive thresholding result window (to handle the block's size and the value of the constant <code class="email">C</code>). Note that since two callback functions are necessary in this case, and we do not want to repeat code, the call to <code class="email">adaptiveThreshold</code> is embedded in the function, <code class="email">adaptThreshAndShow</code>.</p><p class="calibre7">Next, a call is made to the functions that perform the operations using default parameter values. Finally, the <code class="email">moveWindow</code> function from <code class="email">highgui</code> is used to reposition the windows on the screen (otherwise they will be displayed on top of each other, and only the third one will be visible). Also, note that the first six lines in the function <code class="email">adaptiveThresholding1</code> are needed to keep an odd value in the parameter <code class="email">block_size</code>. The following screenshot shows the output of the example:</p><div class="mediaobject"><img src="../images/00022.jpeg" alt="Thresholding" class="calibre9"/><div class="caption"><p class="calibre13">Output of the thresholding example</p></div></div><p class="calibre10"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note14" class="calibre1"/>Note</h3><p class="calibre7">The function <code class="email">inRange(InputArray src, InputArray lowerb, InputArray upperb, OutputArray dst)</code> is <a id="id275" class="calibre1"/>also useful for thresholding as it checks whether the pixels lie between lower and upper thresholds. Both <code class="email">lowerb</code> and <code class="email">upperb</code> must be provided using Scalar, as in <code class="email">inRange(src, Scalar(bl,gl,rl), Scalar(bh,gh,rh), tgt);</code>.</p></div></div></div>
<div class="book" title="Contours and connected components"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec30" class="calibre1"/>Contours and connected components</h1></div></div></div><p class="calibre7">Contour extraction <a id="id276" class="calibre1"/>operations can be considered halfway between feature extraction and segmentation, since a binary image is produced in which image contours are separated from other homogeneous regions. Contours<a id="id277" class="calibre1"/> will typically correspond to object boundaries.</p><p class="calibre7">While a number of simple methods detect edges in images (for example, the Sobel and Laplace filters), the <span class="strong"><strong class="calibre8">Canny</strong></span> method<a id="id278" class="calibre1"/> is a robust algorithm for doing this.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note15" class="calibre1"/>Note</h3><p class="calibre7">This method uses two thresholds to decide whether a pixel is an edge. In what is called a hysteresis procedure, a<a id="id279" class="calibre1"/> lower and an upper threshold are used (see <a class="calibre1" href="http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html">http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html</a>). Since OpenCV already includes a good example of the Canny edge detector (in <code class="email">[opencv_source_code]/samples/cpp/edge.cpp</code>), we do not include one here (but see the following <code class="email">floodFill</code> example). Instead, we will go on to describe other highly useful functions based on detected edges.</p></div><p class="calibre7">To detect <a id="id280" class="calibre1"/>straight lines, the Hough transform<a id="id281" class="calibre1"/> is a classical method. While the Hough transform method is available in OpenCV (the functions <code class="email">HoughLines</code> and <code class="email">HoughLinesP</code>, for example, <code class="email">[opencv_source_code]/samples/cpp/houghlines.cpp</code>), the more recent <span class="strong"><strong class="calibre8">Line Segment Detector</strong></span> (<span class="strong"><strong class="calibre8">LSD</strong></span>) method is generally a more robust one. LSD works by finding alignments of high-gradient magnitude pixels, given its alignment tolerance feature. This method has been shown to be more <a id="id282" class="calibre1"/>robust and faster than the best previous Hough-based detector (Progressive Probabilistic Hough Transform).</p><p class="calibre7">The LSD method is not available in the 2.4.9 release of OpenCV; although, at the time of this writing, it is already available in the code source's repository in GitHub. The method will be available in Version 3.0. A short example (<code class="email">[opencv_source_code]/samples/cpp/lsd_lines.cpp</code>) in the library covers this functionality. However, we will provide an additional example that shows different features.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note16" class="calibre1"/>Note</h3><p class="calibre7">To test the latest source code available in GitHub, go to <a class="calibre1" href="https://github.com/itseez/opencv">https://github.com/itseez/opencv</a> and download the library code as a ZIP file. Then, unzip it to a local folder and follow the same steps described in <a class="calibre1" title="Chapter 1. Getting Started" href="part0014_split_000.html#page">Chapter 1</a>, <span class="strong"><em class="calibre12">Getting Started</em></span>, to compile and install the library.</p></div><p class="calibre7">The LSD detector<a id="id283" class="calibre1"/> is a C++ class. The function <code class="email">cv::Ptr&lt;LineSegmentDetector&gt; cv::createLineSegmentDetector (int _refine=LSD_REFINE_STD, double _scale=0.8, double_sigma_scale=0.6, double _quant=2.0, double _ang_th=22.5, double _log_eps=0, double _density_th=0.7, int _n_bins=1024)</code> creates an object of the class and returns a pointer to it. Note that <a id="id284" class="calibre1"/>several arguments define the detector created. The meaning of those parameters requires you to know the underlying algorithm, which is out of the scope of this book. Fortunately, the default values will suffice for most purposes, so we refer the reader to the reference manual (for Version 3.0 of the library) for special cases. Having said that, the first parameter scale roughly controls the number of lines that are returned. The input image is automatically rescaled by this factor. At lower resolutions, fewer lines are detected.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note17" class="calibre1"/>Note</h3><p class="calibre7">The <code class="email">cv::Ptr&lt;&gt;</code> type<a id="id285" class="calibre1"/> is a template class for wrapping pointers. This template is available in the 2.x API to facilitate automatic deallocation using reference counting. The <code class="email">cv:: Ptr&lt;&gt;</code> type is analogous to <code class="email">std::unique_ptr</code>.</p></div><p class="calibre7">Detection itself <a id="id286" class="calibre1"/>is accomplished with the method <code class="email">LineSegmentDetector::detect(const InputArray _image, OutputArray _lines, OutputArray width=noArray(), OutputArray prec=noArray(), OutputArraynfa=noArray())</code>. The first parameter is the input image, while the <code class="email">_lines</code> array will be filled with a (STL) vector of <code class="email">Vec4i</code> objects that represent the (x, y) location of one end of the line followed by the location of the other end. The optional parameters <code class="email">width</code>, <code class="email">prec</code>, and <code class="email">noArray</code> return additional information about the lines detected. The first one, <code class="email">width</code>, contains the estimated line widths. Lines can be drawn with the convenient (yet simple) method called <code class="email">LineSegmentDetector::drawSegments(InputOutputArray _image, InputArray lines)</code>. Lines will <a id="id287" class="calibre1"/>be drawn on top of the input, namely, <code class="email">_image</code>.</p><p class="calibre7">The following <code class="email">lineSegmentDetector</code> example<a id="id288" class="calibre1"/> shows the detector in action:</p><div class="informalexample"><pre class="programlisting">#include "opencv2/opencv.hpp"
#include &lt;iostream&gt;

using namespace std;
using namespace cv;

vector&lt;Vec4i&gt; lines;
vector&lt;float&gt; widths;
Mat input_image, output;

inline float line_length(const Point &amp;a, const Point &amp;b)
{
    return (sqrt((b.x-a.x)*(b.x-a.x) + (b.y-a.y)*(b.y-a.y)));
}

void MyDrawSegments(Mat &amp;image, const vector&lt;Vec4i&gt;&amp;lines, const vector&lt;float&gt;&amp;widths,
const Scalar&amp; color, const float length_threshold)
{
    Mat gray;
    if (image.channels() == 1)
    {
        gray = image;
    }
    else if (image.channels() == 3)
    {
        cvtColor(image, gray, COLOR_BGR2GRAY);
    }

    // Create a 3 channel image in order to draw colored lines
    std::vector&lt;Mat&gt; planes;
    planes.push_back(gray);
    planes.push_back(gray);
    planes.push_back(gray);

    merge(planes, image);

    // Draw segments if length exceeds threshold given
    for(int i = 0; i &lt; lines.size(); ++i)
    {
        const Vec4i&amp; v = lines[i];
        Point a(v[0], v[1]);
        Point b(v[2], v[3]);
        if (line_length(a,b) &gt; length_threshold) line(image, a, b, color, widths[i]);
    }
}


void thresholding(int threshold, void*)
{
    input_image.copyTo(output);
    MyDrawSegments(output, lines, widths, Scalar(0, 255, 0), threshold);
    imshow("Detected lines", output);
}

int main(int argc, char** argv)
{
    input_image = imread("building.jpg", IMREAD_GRAYSCALE);

    // Create an LSD detector object
<span class="strong"><strong class="calibre8"> Ptr&lt;LineSegmentDetector&gt; ls = createLineSegmentDetector();</strong></span>

    // Detect the lines
<span class="strong"><strong class="calibre8">ls-&gt;detect(input_image, lines, widths);</strong></span>

    // Create window to show found lines
    output=input_image.clone();
    namedWindow("Detected lines", WINDOW_AUTOSIZE);

    // Create trackbar for line length threshold
    int threshold_value=50;
    createTrackbar( "Line length threshold", "Detected lines", &amp;threshold_value, 1000, thresholding );
    thresholding(threshold_value, 0);

    waitKey();
    return 0;
}</pre></div><p class="calibre7">The preceding<a id="id289" class="calibre1"/> example creates a window with the source image, which is loaded in<a id="id290" class="calibre1"/> grayscale, and shows the <code class="email">drawSegments</code> method. However, it allows you to impose a segment length threshold and specify the line colors (<code class="email">drawSegments</code> will draw all the lines in red). Besides, lines will be drawn with a thickness given by the widths estimated by the detector. A trackbar is associated with the main window to control the length of the threshold. The following screenshot shows an output of the example:</p><div class="mediaobject"><img src="../images/00023.jpeg" alt="Contours and connected components" class="calibre9"/><div class="caption"><p class="calibre13">Output of the lineSegmentDetector example</p></div></div><p class="calibre10"> </p><p class="calibre7">Circles<a id="id291" class="calibre1"/> can be detected using the function <code class="email">HoughCircles(InputArray image, OutputArray circles, int method, double dp, double minDist, double param1=100, double param2=100, intminRadius=0, int maxRadius=0)</code>. The first parameter is a grayscale input image. Output parameter<a id="id292" class="calibre1"/> circles will be filled with a vector of <code class="email">Vec3f</code> objects. Each object represents the <code class="email">(center_x, center_y, radius)</code> components of a circle. The last two parameters represent the minimum and maximum search radii, so they have an effect on the number of circles detected. OpenCV already contains a straightforward example of this function, <code class="email">[opencv_source_code]/samples/cpp/houghcircles.cpp</code>. The example detects circles with a radius between 1 and 30 and displays them on top of the input image.</p><p class="calibre7">Segmentation algorithms<a id="id293" class="calibre1"/> typically form connected components, that is, the regions of connected pixels in a binary image. In the following section, we show how to obtain connected components and their contours from a binary image. Contours can be retrieved using the now classical function, <code class="email">findContours</code>. Examples of this function are available in the reference manual (also see the <code class="email">[opencv_source_code]/samples/cpp/contours2.cpp</code> and <code class="email">[opencv_source_code]/samples/cpp/segment_objects.cpp</code> examples). Also note that in the 3.0 release of OpenCV (and in the code already available in the GitHub repository), the class <code class="email">ShapeDistanceExtractor</code> allows you to compare the contours with the Shape Context descriptor (an example of this is available at <code class="email">[opencv_source_code]/samples/cpp/shape_example.cpp</code>) and the Hausdorff distance. This class is in a new module of the library called <code class="email">shape</code>. Shape transformations are also available through the class <code class="email">ShapeTransformer</code> (example, <code class="email">[opencv_source_code]/samples/cpp/shape_transformation.cpp</code>).</p><p class="calibre7">The new functions <code class="email">connectedComponents</code> and <code class="email">connectedComponentsWithStats</code> retrieve connected components. These functions will be part of the 3.0 release, and they are already available in the GitHub repository. An example of this is included in OpenCV that shows how to use the first one, <code class="email">[opencv_source_code]/samples/cpp/connected_components.cpp</code>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note18" class="calibre1"/>Note</h3><p class="calibre7">The connected component that labels the functionality was actually removed in previous OpenCV 2.4.x versions and has now been added again.</p></div><p class="calibre7">We provide another <a id="id294" class="calibre1"/>example (<code class="email">connectedComponents</code>) that shows how to use the second function, <code class="email">int connectedComponentsWithStats(InputArray image, OutputArray labels, OutputArray stats, OutputArray centroids, int connectivity=8, intltype=CV_32S)</code>, which provides useful statistics about each connected component. These statistics are accessed via <code class="email">stats(label, column)</code> where the column can be the following table:</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/></colgroup><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CC_STAT_LEFT </code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The leftmost (<span><em class="calibre26">x</em></span>) coordinate that is the inclusive start of the bounding box in the horizontal direction</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CC_STAT_TOP </code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The topmost (y) coordinate that is the inclusive start of the bounding box in the vertical direction</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CC_STAT_WIDTH </code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The horizontal size of the bounding box</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CC_STAT_HEIGHT </code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The vertical size of the bounding box</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CC_STAT_AREA </code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The total area (in pixels) of the connected component</p>
</td></tr></tbody></table></div><p class="calibre7">The following is the <a id="id295" class="calibre1"/>code for the example:</p><div class="informalexample"><pre class="programlisting">#include &lt;opencv2/core/utility.hpp&gt;
#include "opencv2/imgproc.hpp"
#include "opencv2/highgui.hpp"
#include &lt;iostream&gt;

using namespace cv;
using namespace std;

Mat img;
int threshval = 227;

static void on_trackbar(int, void*)
{
    Mat bw = threshval &lt; 128 ? (img &lt; threshval) : (img &gt; threshval);
    Mat labelImage(img.size(), CV_32S);

    Mat stats, centroids;
<span class="strong"><strong class="calibre8">    int nLabels = connectedComponentsWithStats(bw, labelImage, stats, centroids);</strong></span>

    // Show connected components with random colors
    std::vector&lt;Vec3b&gt; colors(nLabels);
    colors[0] = Vec3b(0, 0, 0);//background
    for(int label = 1; label &lt; nLabels; ++label){
        colors[label] = Vec3b( (rand()&amp;200), (rand()&amp;200), (rand()&amp;200) );
    }
    Mat dst(img.size(), CV_8UC3);
    for(int r = 0; r &lt; dst.rows; ++r){
        for(int c = 0; c &lt; dst.cols; ++c){
            int label = labelImage.at&lt;int&gt;(r, c);
            Vec3b &amp;pixel = dst.at&lt;Vec3b&gt;(r, c);
            pixel = colors[label];
         }
     }
    // Text labels with area of each cc (except background)
    for (int i=1; i&lt; nLabels;i++)
    {
        float a=stats.at&lt;int&gt;(i,CC_STAT_AREA);
        Point org(centroids.at&lt;double&gt;(i,0), centroids.at&lt;double&gt;(i,1));
        String txtarea;
        std::ostringstream buff;
        buff &lt;&lt; a;
        txtarea=buff.str();
        putText( dst, txtarea, org,FONT_HERSHEY_COMPLEX_SMALL, 1, Scalar(255,255,255), 1);
    }

    imshow( "Connected Components", dst );
}

int main( int argc, const char** argv )
{
    img = imread("stuff.jpg", 0);
    namedWindow( "Connected Components", 1 );
    createTrackbar( "Threshold", "Connected Components", &amp;threshval, 255, on_trackbar );
    on_trackbar(threshval, 0);

    waitKey(0);
    return 0;
}</pre></div><p class="calibre7">The preceding <a id="id296" class="calibre1"/>example creates a window with an associated trackbar. The trackbar<a id="id297" class="calibre1"/> controls the threshold to apply to the source image. Inside the <code class="email">on_trackbar</code> function, a call is made to <code class="email">connectedComponentsWithStats</code> using the result of the thresholding. This is followed by two sections of the code. The first section fills the pixels that correspond to each connected component with a random color. The pixels that belong to<a id="id298" class="calibre1"/> each component are in <code class="email">labelImage</code> (a <code class="email">labelImage</code> output is also given by the function <code class="email">connectedComponents</code>). The second part displays a text with the area of each component. This text is positioned at the centroid of each component. The following screenshot shows the output of the example:</p><div class="mediaobject"><img src="../images/00024.jpeg" alt="Contours and connected components" class="calibre9"/><div class="caption"><p class="calibre13">The output of the connectedComponents example</p></div></div><p class="calibre10"> </p></div>
<div class="book" title="Flood fill"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec31" class="calibre1"/>Flood fill</h1></div></div></div><p class="calibre7">The flood fill operation<a id="id299" class="calibre1"/> fills the connected components with a given color. Starting<a id="id300" class="calibre1"/> from a seed point, the neighboring pixels are colored with a uniform color. The neighboring pixels can be within a specified range of the current pixel. The flood fill function is <code class="email">int floodFill(InputOutputArray image, Point seedPoint, Scalar newVal, Rect* rect=0, Scalar loDiff=Scalar(), Scalar upDiff=Scalar(),int flags=4)</code>. The parameters <code class="email">loDiff</code> and <code class="email">upDiff</code> represent the range to check for every neighboring pixel (note that 3-channel difference thresholds can be specified). The parameter <code class="email">newVal</code> is the color to apply to the pixels that are in range. The lower part of the parameter <code class="email">flags</code> contains the pixel's connectivity value to use (<code class="email">4</code> or <code class="email">8</code>). The upper part defines the mode of the operation. </p><p class="calibre7">Depending on this mode, the flood fill function will color a neighboring pixel in the input image if it is within the specified range (given by <code class="email">loDiff</code> and <code class="email">upDiff</code>) of either the current pixel or if the neighboring pixel is within the specified range of the original seed's value. The function can also be called with a mask image as the second parameter. If specified, the flood-filling operation will not go across non-zero pixels in the mask. Note that the mask should be a single-channel 8-bit image that is 2 pixels wider and 2 pixels taller than the input image.</p><p class="calibre7">The upper bit of <code class="email">flags</code> can be 0 or a combination of the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">FLOODFILL_FIXED_RANGE</code>: If set, the difference between the current pixel and seed pixel is considered. Otherwise, the<a id="id301" class="calibre1"/> difference between neighbor pixels is considered.</li><li class="listitem"><code class="email">FLOODFILL_MASK_ONLY</code>: If set, the<a id="id302" class="calibre1"/> function does not change the image (<code class="email">newVal</code> is ignored) but fills the mask.</li></ul></div><p class="calibre7">In OpenCV's flood fill example (<code class="email">[opencv_source_code]/samples/cpp/ffilldemo.cpp</code>), the mask is used only as an output parameter. In our <code class="email">floodFill</code> example, shown as the following code, we will use it as an input parameter in order to constrain the filling. The idea is to use the output of an edge detector as a mask. This should stop the filling process at the edges:</p><div class="informalexample"><pre class="programlisting">#include "opencv2/opencv.hpp"
#include &lt;iostream&gt;

using namespace std;
using namespace cv;

Mat image, image1, image_orig;
int loDiff = 20, upDiff = 30;
int loCanny=10, upCanny=150;

void onMouse( int event, int x, int y, int, void* )
{
    if( event != CV_EVENT_LBUTTONDOWN ) return;

    Point seed = Point(x,y);
    int flags = 4 + CV_FLOODFILL_FIXED_RANGE;
    int b = (unsigned)theRNG() &amp; 255;
    int g = (unsigned)theRNG() &amp; 255;
    int r = (unsigned)theRNG() &amp; 255;
    Rect ccomp;

    Scalar newVal = Scalar(b, g, r);
    Mat dst = image;

    // flood fill
<span class="strong"><strong class="calibre8">    floodFill(dst, seed, newVal, &amp;ccomp, Scalar(loDiff, loDiff, loDiff), Scalar(upDiff, upDiff, upDiff), flags);</strong></span>
    imshow("image", dst);

    // Using Canny edges as mask
    Mat mask;
    Canny(image_orig, mask, loCanny, upCanny);
    imshow("Canny edges", mask);
    copyMakeBorder(mask, mask, 1, 1, 1, 1, cv::BORDER_REPLICATE);
    Mat dst1 = image1;
    <span class="strong"><strong class="calibre8">floodFill(dst1, mask, seed, newVal, &amp;ccomp, Scalar(loDiff, loDiff, loDiff), Scalar(upDiff, upDiff, upDiff), flags);</strong></span>
    imshow("FF with Canny", dst1);

    moveWindow("Canny edges", image.cols,0);
    moveWindow("FF with Canny", 2*image.cols,0);
}

int main(int argc, char *argv[])
{
    // Read original image and clone it to contain results
    image = imread("lena.jpg", CV_LOAD_IMAGE_COLOR );
    image_orig=image.clone();
    image1=image.clone();

    namedWindow( "image", WINDOW_AUTOSIZE );

    imshow("image", image);
    createTrackbar( "lo_diff", "image", &amp;loDiff, 255, 0 );
    createTrackbar( "up_diff", "image", &amp;upDiff, 255, 0 );
    createTrackbar( "lo_Canny", "image", &amp;loCanny, 255, 0 );
    createTrackbar( "up_Canny", "image", &amp;upCanny, 255, 0 );
    setMouseCallback( "image", onMouse, 0 );

    moveWindow("image", 0,0);

    cout &lt;&lt; "Press any key to exit...\n";
    waitKey(); // Wait for key press
    return 0;
}</pre></div><p class="calibre7">The preceding<a id="id303" class="calibre1"/> example reads and displays a color image and then creates <a id="id304" class="calibre1"/>four trackbars. The first two trackbars control <code class="email">loDiff</code> and <code class="email">upDiffvalues</code> for the <code class="email">floodFill</code> function. The other two trackbars control the lower and upper threshold parameters for the Canny edge detector. In this example, the user can click anywhere on the input image. The click position will be used as a seed point to perform a flood fill operation. Actually, upon each click, two calls are made to the <code class="email">floodFill</code> function. The first one simply fills a region using a random color. The second one uses a mask created from the output of the Canny edge detector. Note that the <code class="email">copyMakeBorder</code> function is necessary to form a 1-pixel wide border around the mask. The following screenshot shows the output of this example:</p><div class="mediaobject"><img src="../images/00025.jpeg" alt="Flood fill" class="calibre9"/><div class="caption"><p class="calibre13">Output of the floodFill example</p></div></div><p class="calibre10"> </p><p class="calibre7">Note that the<a id="id305" class="calibre1"/> output that uses Canny edges (right) has filled in less pixels<a id="id306" class="calibre1"/> than the standard operation (left).</p></div>
<div class="book" title="Watershed segmentation"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec32" class="calibre1"/>Watershed segmentation</h1></div></div></div><p class="calibre7">Watershed is a segmentation<a id="id307" class="calibre1"/> method that is known for its efficiency. The <a id="id308" class="calibre1"/>method essentially starts from user-specified starting (seed) points from which regions grow. Assuming that good starting seeds can be provided, the resulting segmentations are useful for many purposes.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note19" class="calibre1"/>Note</h3><p class="calibre7">For more details and<a id="id309" class="calibre1"/> examples about the watershed transform for image segmentation, see <a class="calibre1" href="http://cmm.ensmp.fr/~beucher/wtshed.html">http://cmm.ensmp.fr/~beucher/wtshed.html</a>.</p></div><p class="calibre7">The function <code class="email">watershed(InputArray image, InputOutputArray markers)</code> accepts a 3-channel input image and an image called <code class="email">markers</code> with the seeds. The latter has to be a 32-bit single-channel image. Seeds may be specified in <code class="email">markers</code> as connected components with positive values (0 cannot be used as a value for seeds). As an output argument, each pixel in <code class="email">markers</code> will be set to a value of the seed components or <code class="email">-1</code> at boundaries between the regions. OpenCV includes a watershed example (<code class="email">[opencv_source_code]/samples/cpp/watershed.cpp</code>) in which the user has to draw the seed's regions.</p><p class="calibre7">Obviously, the<a id="id310" class="calibre1"/> selection of the seed regions is important. Ideally, seeds<a id="id311" class="calibre1"/> will be selected automatically without user intervention. A typical use of watershed is to first threshold the image to separate the object from the background, apply the distance transform, and then use the local maxima of the distance transform image as seed points for segmentation. However, the first thresholding step is critical, as parts of the object may be considered as the background. In this case, the object seed region will be too small and segmentation will be poor. On the other hand, to perform a watershed segmentation, we need seeds for the background too. While we can use points over the corners of the image as seeds, this will not be sufficient. In this case, the background seed region is too small. If we use those seeds, the object region given by the segmentation will be generally much larger than the real object. In our following <code class="email">watershed</code> example, a different approach is followed that produces better results:</p><div class="informalexample"><pre class="programlisting">#include &lt;opencv2/core/utility.hpp&gt;
#include "opencv2/imgproc.hpp"
#include "opencv2/highgui.hpp"
#include "opencv2/core.hpp"
#include &lt;iostream&gt;

using namespace std;
using namespace cv;


void Watershed(const Mat &amp;src)
{
    Mat dst=src.clone();

    // Flood fill outer part of the image
    Point seed(0,0); // top-left corner
    int loDiff=20;
    int upDiff=20;
    int flags=4 + FLOODFILL_FIXED_RANGE + FLOODFILL_MASK_ONLY + (255&lt;&lt;8);
    Mat mask(src.size(), CV_8UC1);
    mask.setTo(0);
    copyMakeBorder(mask, mask, 1, 1, 1, 1, cv::BORDER_REPLICATE);
    Scalar newVal;
    Rect ccomp;
    floodFill(dst, mask, seed, newVal, &amp;ccomp,
         Scalar(loDiff, loDiff, loDiff), Scalar(upDiff, upDiff, upDiff), flags);

    // Flood fill inner part of the image
    seed.x=(float)src.cols/2;   // image center x
    seed.y=(float)src.rows/2;   // image center y
    Mat mask1=mask.clone();
    mask1.setTo(0);
    floodFill(dst, mask1, seed, newVal, &amp;ccomp,
          Scalar(loDiff, loDiff, loDiff), Scalar(upDiff, upDiff, upDiff), flags);

    // Form image with the two seed regions
    Mat Mask = mask.clone();
    mask=mask/2;
    Mask = mask | mask1;
    imshow("Seed regions", Mask);
    moveWindow("Seed regions", src.cols, 0);

    // Perform watershed
    Mat labelImage(src.size(), CV_32SC1);
    labelImage=Mask(Rect(1,1, src.cols, src.rows));
    labelImage.convertTo(labelImage, CV_32SC1);
<span class="strong"><strong class="calibre8">    watershed(src, labelImage);</strong></span>
    labelImage.convertTo(labelImage, CV_8U);
    imshow("Watershed", labelImage);
    moveWindow("Watershed", 2*src.cols, 0);
}


int main(int argc, char *argv[])
{
    // Read original image and clone it to contain results
    Mat src = imread("hand_sample2.jpg", IMREAD_COLOR );

    // Create 3 windows
    namedWindow("Source", WINDOW_AUTOSIZE);
    imshow("Source", src);

    Watershed(src);

    // Position windows on screen
    moveWindow("Source", 0,0);

    cout &lt;&lt; "Press any key to exit...\n";
    waitKey(); // Wait for key press
    return 0;
}</pre></div><p class="calibre7">The <code class="email">Watershed</code> function<a id="id312" class="calibre1"/> in the preceding code performs three steps. First, a background seed region is obtained by performing a flood fill. The flood fill seed is the <a id="id313" class="calibre1"/>upper left corner of the image, that is, pixel (0, 0). Next, another<a id="id314" class="calibre1"/> flood fill is performed to obtain an object's (hand in the sample image) seed region. The seed for this flood fill is taken as the center of the image. Then, a seed region image is formed by performing an <code class="email">OR</code> operation between the previous two flood fill results. The resulting image is used as the seed image for the watershed operation. See the output of the example in the following screenshot where the seed image is shown at the center of the figure:</p><div class="mediaobject"><img src="../images/00026.jpeg" alt="Watershed segmentation" class="calibre9"/><div class="caption"><p class="calibre13">The output of the watershed example</p></div></div><p class="calibre10"> </p></div>
<div class="book" title="GrabCut"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec33" class="calibre1"/>GrabCut</h1></div></div></div><p class="calibre7">GrabCut<a id="id315" class="calibre1"/> is an excellent iterative background/foreground segmentation algorithm that is <a id="id316" class="calibre1"/>available since Version 2.1 of OpenCV. GrabCut is especially useful to separate objects from the background with minimal additional information (a bounding rectangle is sufficient in most cases). However, it is computationally intensive, and so it is only appropriate to segment still images.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note20" class="calibre1"/>Note</h3><p class="calibre7">GrabCut is the underlying algorithm for the Background Removal tool in Microsoft Office 2010. This algorithm was first proposed by researchers at Microsoft Research Cambridge. Starting with a user-provided bounding box of the object to segment, the algorithm estimates the color distributions of both the target object and the background. This estimate is further refined by minimizing an energy function in which connected regions that have the same label receive more weight.</p></div><p class="calibre7">The main <a id="id317" class="calibre1"/>function is <code class="email">grabCut(InputArray img, InputOutputArray mask, Rect rect, InputOutputArray bgdModel, InputOutputArray fgdModel, int iterCount, int mode=GC_EVAL)</code>. The parameters <code class="email">bgdModel</code> and <code class="email">fgdModel</code> are only used internally by the function (though they have to be declared). The <code class="email">iterCount</code> variable is the number of iterations to be performed. In our experience, few iterations of the algorithm are required to produce good segmentations. The algorithm is aided by a bounding rectangle, a mask image, or both. The option chosen is indicated in the <code class="email">mode</code> parameter, which can be <code class="email">GC_INIT_WITH_RECT</code>, <code class="email">GC_INIT_WITH_MASK</code>, or an <code class="email">OR</code> combination of the two. In the former case, <code class="email">rect</code> defines the rectangle. Pixels outside the rectangle are considered as the obvious background. In the latter case, the mask is an 8-bit image in which pixels may have the following values:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">GC_BGD</code>: This defines an obvious background pixel</li><li class="listitem"><code class="email">GC_FGD</code>: This defines an obvious foreground (object) pixel</li><li class="listitem"><code class="email">GC_PR_BGD</code>: This defines a possible background pixel</li><li class="listitem"><code class="email">GC_PR_FGD</code>: This defines a possible foreground pixel</li></ul></div><p class="calibre7">The image mask is also the output image with the resulting segmentation, which is derived using those same previous values. OpenCV includes an example of GrabCut (<code class="email">[opencv_source_code]/samples/cpp/grabcut.cpp</code>) in which the user can draw a bounding rectangle as well as foreground and background pixels.</p><p class="calibre7">The following <code class="email">grabcut</code> example uses the algorithm with an initial bounding rectangle and then copies the <a id="id318" class="calibre1"/>resulting<a id="id319" class="calibre1"/> foreground onto another position in the same image:</p><div class="informalexample"><pre class="programlisting">#include "opencv2/opencv.hpp"
#include &lt;iostream&gt;

using namespace std;
using namespace cv;

int main(int argc, char *argv[])
{
    // Read original image and clone it
    Mat src = imread("stuff.jpg" );
    Mat tgt = src.clone();

    // Create source window
    namedWindow("Source", WINDOW_AUTOSIZE);

    imshow("Source", src);
    moveWindow("Source", 0,0);

    // GrabCut segmentation
    Rect rectangle(180,279,60,60);  // coin position
    Mat result;                     // segmentation result
    Mat bgModel,fgModel;            // used internally
<span class="strong"><strong class="calibre8">grabCut(src, result, rectangle, bgModel,fgModel, 1, GC_INIT_WITH_RECT);</strong></span>

    result=(result &amp; GC_FGD);   // leave only obvious foreground

    // Translation operation
    Mat aff=Mat::eye(2,3,CV_32FC1);
    aff.at&lt;float&gt;(0,2)=50;
    warpAffine(tgt, src, aff, result.size());
    warpAffine(result, result, aff, result.size());
    src.copyTo(tgt, result);

    // Show target window
    imshow("Target", tgt);
    moveWindow("Target", src.cols, 0);

    cout &lt;&lt; "Press any key to exit...\n";
    waitKey(); // Wait for key press
    return 0;
}</pre></div><p class="calibre7">The preceding<a id="id320" class="calibre1"/> example simply uses a fixed rectangle around the coin in the source image (see the fifth screenshot in this chapter) and performs the segmentation. The <code class="email">result</code> image will contain values between 0 (<code class="email">GC_BGD</code>) and 3 (<code class="email">GC_PR_FGD</code>). The <a id="id321" class="calibre1"/>ensuing <code class="email">AND</code> operation is needed to convert values other than <code class="email">GC_FGD</code> to zero and thus get a binary foreground mask. Then, both the source image and the mask are translated by 50 pixels in the horizontal. An affine warping operation is used with an identity matrix in which only the x translation component is changed. </p><p class="calibre7">Finally, the translated image is copied onto the target image, using the (also translated) mask. Both source and target images are shown in the following screenshot. Increasing the number of iterations did not have any significant effect in this particular example:</p><div class="mediaobject"><img src="../images/00027.jpeg" alt="GrabCut" class="calibre9"/><div class="caption"><p class="calibre13">Source and target images in the GrabCut example</p></div></div><p class="calibre10"> </p></div>
<div class="book" title="Summary"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec34" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">This chapter has covered one of the most important subjects in computer vision. Segmentation is often one of the first steps, and also, it is typically one of the trickiest. In this chapter, we have provided the reader with insight and samples to use the most useful segmentation methods in OpenCV, such as thresholding, contours and connected components, flood filling of regions, the watershed segmentation method, and the GrabCut method.</p></div>
<div class="book" title="What else?"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec35" class="calibre1"/>What else?</h1></div></div></div><p class="calibre7">The meanshift segmentation (the function <code class="email">pyrMeanShiftFiltering</code>) has been omitted. OpenCV includes an example showing how to use this function (<code class="email">[opencv_source_code]/samples/cpp/meanshift_segmentation.cpp</code>).This method is, however, relatively slow and tends to produce oversegmented results.</p><p class="calibre7">Background/foreground segmentations can also be achieved using video, which will be covered in <a class="calibre1" title="Chapter 7. What Is He Doing? Motion" href="part0055_split_000.html#page">Chapter 7</a>, <span class="strong"><em class="calibre12">What Is He Doing? Motion</em></span>.</p></div></body></html>