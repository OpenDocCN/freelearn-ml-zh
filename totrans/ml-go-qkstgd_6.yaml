- en: Deploying Machine Learning Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署机器学习应用
- en: In the previous chapters, we learned how to create an application that can prepare
    data ([Chapter 2](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml), *Setting Up the
    Development Environment*) for either a supervised ([Chapter 3](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml),
    *Supervised Learning*) or unsupervised ([Chapter 4](26788e93-3614-413f-bcde-5580516f9c5f.xhtml),
    *Unsupervised Learning*) ML algorithm. We also learned how to evaluate and test
    the output of these algorithms with the added complication that we have incomplete
    knowledge about the algorithm's inner state and workings, and must therefore treat
    it as a black box. In [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using
    Pre-Trained Models,* we looked at model persistence and how Go applications can
    leverage models written in other languages. Together, the skills you have learned
    so far constitute the fundamentals required to successfully prototype ML applications.
    In this chapter, we will look at how to prepare your prototype for commercial
    readiness, focusing on aspects specific to ML applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何创建一个应用，可以为监督学习([第二章](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml)，“设置开发环境”）或无监督学习([第四章](26788e93-3614-413f-bcde-5580516f9c5f.xhtml)，“无监督学习”）的机器学习算法准备数据。我们还学习了如何评估和测试这些算法的输出，增加了额外的复杂性，即我们对算法的内部状态和工作原理的了解不完整，因此必须将其视为黑盒。在[第五章](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml)“使用预训练模型”中，我们探讨了模型持久性和Go应用如何利用其他语言编写的模型。你迄今为止学到的技能构成了成功原型化机器学习应用所需的基本技能。在本章中，我们将探讨如何为商业准备你的原型，重点关注特定于机器学习应用的方面。
- en: 'In this chapter, you will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将涵盖以下主题：
- en: The continuous delivery feedback loop, including how to test, deploy, and monitor
    ML applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续交付反馈循环，包括如何测试、部署和监控机器学习应用
- en: Deployment models for ML applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习应用的部署模型
- en: The continuous delivery feedback loop
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续交付反馈循环
- en: '**Continuous delivery** (**CD**) is the practice of using short feedback loops
    in the software development life cycle to ensure that the resulting application
    can be released at any moment in time^([1]). While there are alternative approaches
    to release management, we will only consider this one because creating a meaningful,
    short—and therefore automated—feedback loop with ML applications presents unique
    challenges that are not created by alternative methodologies that may not require
    this degree of automation.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**持续交付**（**CD**）是在软件开发生命周期中使用短反馈循环的实践，以确保结果应用可以在任何时刻发布^([1]）。虽然存在其他发布管理方法，但我们只考虑这一种，因为创建一个有意义的、短的——因此是自动化的——反馈循环，对于机器学习应用来说，提出了独特的挑战，这些挑战不是由可能不需要这种程度自动化的其他方法所引起的。'
- en: 'The CD feedback loop consists of the following process:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: CD反馈循环包括以下过程：
- en: '![](img/c4985c73-3a46-4ab8-99de-d63683cbf9f6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c4985c73-3a46-4ab8-99de-d63683cbf9f6.png)'
- en: 'Fig. 1: The continuous delivery feedback loop'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：持续交付反馈循环
- en: Developing
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发
- en: The development portion of the feedback loop is what we have covered so far
    in this book. As we argued in [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml),
    *Using Pre-Trained Models*,developing ML models in Go has both advantages and
    disadvantages, and sometimes combining Go with other languages, such as Python,
    to benefit from libraries, such as Keras, can significantly shorten the development
    portion of the cycle. The downside is reduced maintainability and more work to
    test the resulting solution, as it will necessarily contain a Go–Python interface
    (for example).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书到目前为止所涵盖的反馈循环的开发部分。正如我们在[第五章](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml)“使用预训练模型”中所论证的，在Go中开发机器学习模型既有优点也有缺点，有时将Go与其他语言（如Python）结合使用，以利用如Keras等库，可以显著缩短开发周期的一部分。缺点是维护性降低，测试最终解决方案的工作量增加，因为它必然包含Go-Python接口（例如）。
- en: Testing
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试
- en: 'Because humans are prone to making errors, testing the source code we create
    is a critical element of the development life cycle to guarantee an accurate and
    reliable product. Entire books have been dedicated to the subject, and it seems
    there are as many different approaches to software testing as there are software
    engineers (as an internet search for software-testing methodologies will confirm).
    ML applications, on the surface, are particularly difficult to test because they
    seem like a black box, whose output depends on the training set we provide: we
    feed them data, and they feed us answers, but a slight change of the train–test
    split or the hyperparameters could produce a different output for a given input
    vector. How can we determine whether the answers they provide are erroneous because
    the model''s hyperparameters are incorrect, because the input data is corrupt,
    or because the model''s algorithms are flawed? Or is this particular response
    an outlier buried in a population of otherwise acceptable responses?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人类容易犯错，测试我们创建的源代码是开发周期中保证准确和可靠产品的关键要素。已经有许多书籍致力于这个主题，似乎软件测试的方法与软件工程师的数量一样多（如互联网搜索软件测试方法可以证实）。表面上看，机器学习应用尤其难以测试，因为它们看起来像一个黑盒，其输出取决于我们提供的训练集：我们提供数据，它们提供答案，但训练-测试集的微小变化或超参数的微小变化可能会为给定的输入向量产生不同的输出。我们如何确定他们提供的答案是否错误，是因为模型的超参数不正确，输入数据损坏，或者模型的算法有缺陷？或者这个特定的响应是隐藏在大量可接受响应中的异常值？
- en: In the previous chapters, we performed statistical testing of models using the
    validation set to measure the responses of the model to a meaningful sample of
    inputs, comparing them to expected output values when these were available (supervised
    learning). Arguably, this is the only way to test ML models for accuracy or precision
    because retraining them on a different sample of the dataset (or with altered
    hyperparameters) could produce a different output for the same input, but should
    not produce statistically inferior results on a large validation set with regards
    to the same accuracy/precision metrics. In other words, with small changes to
    the model, we could see large changes to the way it responds to one input vector,
    but its response should not be too different when tested against a large enough
    sample of input vectors, such as the validation set.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用验证集对模型进行了统计测试，以衡量模型对有意义的输入样本的反应，并将这些反应与可用的预期输出值进行比较（监督学习）。可以说，这是测试机器学习模型准确度或精度的唯一方法，因为用数据集的不同样本（或改变的超参数）重新训练它们可能会为相同的输入产生不同的输出，但应该在具有相同准确度/精度指标的大型验证集上产生统计上较差的结果。换句话说，对模型进行小的修改，我们可能会看到它对单个输入向量的反应方式发生大的变化，但它的反应在测试足够大的输入向量样本（如验证集）时不应有太大差异。
- en: This has two consequences. First, the way that unit tests are usually constructed,
    where the developer chooses input values and asserts on the output, could break
    down with the slightest change to the model. Therefore, it is best not to rely
    on assertions based on a single response. Rather, it is better to assert using
    an accuracy or precision metric across a larger set, using the techniques we introduced
    in [Chapters 3](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml), *Supervised Learning*,
    and [Chapter 4](26788e93-3614-413f-bcde-5580516f9c5f.xhtml), *Unsupervised Learning*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这有两个后果。首先，单元测试通常的构建方式，即开发者选择输入值并断言输出结果，在模型发生最轻微的变化时可能会崩溃。因此，最好不要依赖于基于单个响应的断言。相反，最好使用跨更大集合并使用我们在[第3章](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml)，“监督学习”，和[第4章](26788e93-3614-413f-bcde-5580516f9c5f.xhtml)，“无监督学习”中介绍的技术，使用准确度或精度指标进行断言。
- en: 'Second, there may be edge cases, where we wish to guarantee the behavior of
    a model, or certain responses that we wish to guarantee will never occur (not
    even as outlying behavior). If we cannot be sure that a black box model can achieve
    this, combining an ML algorithm with traditional logic is the only way to ensure
    that the constraints are met. For example, consider Google''s recent ban of "gorilla"
    as a search term on Google Images in an effort to prevent some accidentally racist
    results from appearing^([2]). Performing statistical testing of the image classifier
    with gorilla images would have been difficult and would only have covered this
    one edge case; however, knowing what an unacceptable response was and adding constraining
    logic to prevent this edge case was a trivial, if embarrassing, affair. As with
    this example, traditional unit tests can be combined with statistical testing,
    with the traditional unit tests asserting on the output of the constraints while
    the statistical tests assert on the model output directly. An holistic strategy
    for ML testing thus emerges:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，可能存在边缘情况，我们希望保证模型的行为，或者我们希望保证某些响应永远不会发生（即使是作为异常行为）。如果我们不能确定一个黑盒模型能够实现这一点，将机器学习算法与传统的逻辑相结合是确保满足约束的唯一方法。例如，考虑谷歌最近禁止在谷歌图片搜索中使用“gorilla”作为搜索词，以防止一些意外出现的种族主义结果。使用gorilla图像对图像分类器进行统计测试将是困难的，并且只能覆盖这一个边缘情况；然而，知道什么是不被接受的反应，并添加约束逻辑来防止这种边缘情况是微不足道的，尽管可能有些尴尬。正如这个例子一样，传统的单元测试可以与统计测试相结合，传统的单元测试断言约束的输出，而统计测试直接断言模型输出。因此，机器学习测试的整体策略就出现了：
- en: '**Define accuracy/precision goals for the model**: This may not be as simple
    as coming up with a single accuracy score, as reducing false positives or false
    negatives may take precedence. For example, a classifier that aims to determine
    whether a mortgage applicant should get a loan may be required to err on the side
    of caution, with more false negatives tolerated than false positives, depending
    on the risk profile of the lender.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为模型定义准确度/精确度目标**：这可能不仅仅是一个单一的准确度分数，因为减少误报或漏报可能更为重要。例如，一个旨在确定抵押贷款申请人是否应该获得贷款的分类器可能需要谨慎行事，容忍更多的漏报而不是误报，这取决于贷款人的风险概况。'
- en: '**Define edge case behavior and codify this into unit tests**: This may require
    traditional logic to restrict the output of the ML model to ensure that these
    constraints are met and traditional unit tests to assert on the constrained output
    of the ML model.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义边缘情况的行为并将其编码到单元测试中**：这可能需要传统的逻辑来限制机器学习模型的输出，以确保满足这些约束，并使用传统的单元测试来断言机器学习模型的约束输出。'
- en: Deployment
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: Once the ML application has been developed and you have tested it to satisfy
    yourself that it works as intended, the next step in the CD life cycle is to deploy
    the software—that is, take steps to ensure that users are able to access it. There
    are different deployment models, depending on factors such as whether you are
    intending to run the application on your own hardware or whether you intend to
    use an **infrastructure-as-a-service** (**IaaS**) or **platform-as-a-service**
    (**PaaS**) cloud, and we will touch upon these differences in the next section.
    Here, we will assume that you are either running the application on your own servers
    or using a virtual infrastructure supplied by an IaaS provider.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦开发出机器学习应用，并且你已经测试过它，确保它按预期工作，CD生命周期中的下一步就是部署软件——也就是说，采取步骤确保用户能够访问它。根据你打算在自己的硬件上运行应用程序还是打算使用**基础设施即服务**（**IaaS**）或**平台即服务**（**PaaS**）云等因素，存在不同的部署模型，我们将在下一节中讨论这些差异。在这里，我们假设你是在自己的服务器上运行应用程序，或者使用由IaaS提供商提供的虚拟基础设施。
- en: 'ML applications can present unique challenges in deployment that are absent
    from simpler software, such as an HTTP server that connects to a database:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习应用在部署时可能面临一些独特的挑战，这些挑战在更简单的软件中是不存在的，例如一个连接到数据库的HTTP服务器：
- en: Dependency on scientific libraries that require LAPACK or BLAS entails complex
    installation processes with many steps, and chances for mistakes.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于需要LAPACK或BLAS的科学库会导致复杂的安装过程，有许多步骤，出错的可能性也很大。
- en: Dependency on deep-learning libraries, such as TensorFlow, entails dynamic linking
    to C libraries, again leading to a complex installation process, with many OS
    and architecture-specific steps, and chances for mistakes.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于深度学习库（如TensorFlow）意味着需要动态链接到C库，这再次导致复杂的安装过程，包括许多与操作系统和架构相关的步骤，以及出错的机会。
- en: Deep learning models may need to run on specialized hardware (for example, servers
    with GPUs), even for testing
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型可能需要在专用硬件（例如，配备GPU的服务器）上运行，即使是测试阶段。
- en: Where should ML models be persisted? Should they be committed as though they
    were source code? If so, how can we be sure we are deploying the correct version?
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML模型应该保存在哪里？是否应该像源代码一样提交它们？如果是这样，我们如何确保我们部署的是正确的版本？
- en: Next, we will present solutions to these challenges and a sample application
    that embodies these solutions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍解决这些挑战的解决方案以及体现这些解决方案的示例应用程序。
- en: Dependencies
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依赖项
- en: Anyone who has tried to build TensorFlow or NumPy from source will sympathize
    with the saying that *anything that can go wrong, will go wrong*. A search on
    Google, Stack Overflow, or their respective GitHub issue pages will reveal many
    obscure potential issues with the build process^([3][4][5]). These are not isolated
    finding in the sense that the scientific computing libraries that ML applications
    rely on tend to be highly complex and depend on a convoluted set of other libraries
    that are also highly complex. An academic ML researcher might have need to build
    dependencies from source to benefit from a certain optimization, or perhaps because
    they need to modify them. On the contrary, an ML application developer must try
    to avoid this process and instead use prebuilt images available as Python wheels^([6]),
    prebuilt packages for their chosen package manager (such as apt on Ubuntu Linux
    or Chocolatey^([7]) on Windows), or Docker images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 任何尝试从源代码构建TensorFlow或NumPy的人都会同情那句俗语：“任何可能出错的事情都会出错”。在Google、Stack Overflow或它们各自的GitHub问题页面上搜索将揭示许多与构建过程相关的隐晦潜在问题^([3][4][5]）。这些并不是孤立发现，因为ML应用程序所依赖的科学计算库往往非常复杂，并且依赖于一个复杂且高度复杂的其他库集合。一个学术ML研究人员可能需要从源代码构建依赖项以利用某种优化，或者可能因为需要修改它们。相反，ML应用程序开发者必须尽量避免这个过程，而是使用作为Python
    wheels^([6])提供的预构建镜像、为所选包管理器（如Ubuntu Linux上的apt或Windows上的Chocolatey^([7]）提供的预构建包，或者Docker镜像。
- en: 'We will focus on Docker as a solution for developing and packaging Go ML applications
    for several reasons:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注Docker作为开发与打包Go ML应用的解决方案，原因有以下几点：
- en: Portability across a wide range of operating systems
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在广泛的操作系统之间的可移植性。
- en: Excellent support from major cloud vendors, such as Microsoft Azure^([8]) and
    Amazon Web Services^([9])
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自主要云供应商（如Microsoft Azure^([8])和Amazon Web Services^([9]）的出色支持。
- en: Support for Docker integration in popular provisioning and infrastructure configuration
    using tools such as Terraform^([10]), Chef^([11]), and Ansible^([12]).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持使用Terraform^([10])、Chef^([11])和Ansible^([12])等工具在流行的配置和基础设施配置中集成Docker。
- en: Availability of ML libraries through prebuilt Docker images
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过预构建Docker镜像提供ML库。
- en: Go's particular suitability for Docker, as it can always be configured to produce
    static binaries, allowing us to greatly reduce the production Docker image size
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Go对Docker的特别适用性，因为它总是可以配置为生成静态二进制文件，这使我们能够大大减小生产Docker镜像的大小。
- en: If you have reduced the size of the Docker image as much as possible (maybe
    by using the `scratch` image), but the size of the Go binary makes the overall
    image still too large for you, consider using the `strip` command or a packer
    like `upx`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经尽可能减小了Docker镜像的大小（可能通过使用`scratch`镜像），但Go二进制文件的大小仍然使整体镜像对你来说太大，考虑使用`strip`命令或像`upx`这样的打包器。
- en: In all the examples we have looked at so far, we have created a single Docker
    image that contains all the dependencies for our application, as well as the application
    files, usually added to the container using the `ADD` or `COPY` command in the
    Dockerfile. While this has the advantage of simplicity (there is only one Dockerfile
    for development and production), it also means that we will need to push or pull
    an oversized Docker image with all the dependencies for developing an application.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止查看的所有示例中，我们创建了一个包含我们应用程序所有依赖项以及应用程序文件的单一Docker镜像，通常使用Dockerfile中的`ADD`或`COPY`命令将这些文件添加到容器中。虽然这具有简单性的优势（开发和生产中只有一个Dockerfile），但也意味着我们需要推送或拉取一个包含所有依赖项的过大Docker镜像来开发应用程序。
- en: However, the dependencies are probably not required to run it because Go can
    always be configured to produce static binaries that run on stripped-down Docker
    images. This means slower deployment times and slower testing times, as intermediate
    Docker images may not be cached in the CI environment, not to mention that a smaller
    container tends to use less disk and memory on its host server. Smaller images
    also have the benefit of added security from reducing the attack surface, as they
    will contain far fewer dependencies that an attacker could exploit. The `scratch`
    image, for example, does not even contain a shell, making it very hard for an
    attacker to compromise, even if the application running in the container is itself
    compromised.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可能不需要依赖关系来运行它，因为Go总是可以被配置为生成在精简的Docker镜像上运行的静态二进制文件。这意味着部署时间和测试时间会变慢，因为中间的Docker镜像可能不会在CI环境中缓存，更不用说一个更小的容器通常在其宿主服务器上使用更少的磁盘和内存。较小的镜像还有通过减少攻击面增加安全性的好处，因为它们将包含远少于攻击者可以利用的依赖项。例如，`scratch`镜像甚至不包含shell，这使得攻击者即使容器中的应用程序本身被破坏，也很难对其进行破坏。
- en: 'The process we advocate is shown in the following diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们倡导的过程在以下图中展示：
- en: '![](img/4c230b82-c158-4b8c-bad6-51a6406f9879.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c230b82-c158-4b8c-bad6-51a6406f9879.png)'
- en: 'Fig 2: Deployment using two separate Docker images (one for development and
    one for testing/production)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用两个独立的Docker镜像进行部署（一个用于开发，一个用于测试/生产）
- en: In the following example, we assume that you already have a development environment,
    where all your dependencies live (which could be Docker based, or not—it does
    not matter). You have developed your ML application, which consists of a `main` package
    and some saved model weights, `model.data`, and would like to create a production-ready
    container. To create this container, we need to do two things.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们假设您已经有一个开发环境，其中所有依赖项都存在（这可能基于Docker，或者不是——这并不重要）。您已经开发了自己的机器学习应用程序，它由一个`main`包和一些保存的模型权重`model.data`组成，并希望创建一个生产就绪的容器。为了创建这个容器，我们需要做两件事。
- en: 'First, we need to compile the Go application to a static binary. If you are
    not using CGO and linking to some C libraries (such as the TensorFlow C library),
    then using `go build` without any additional flags will suffice. However, if your
    application depends on, say, the TensorFlow C library, then you need to add some
    additional command-line arguments to ensure that the resulting binary is static—that
    is, that it includes all the dependent code. At the time of writing, there is
    a proposal for Go 1.13 to have a `-static` flag for the `build` command that will
    achieve this with no further work. Until then, there is an excellent blog post
    by Diogok that explains the different flags in the following command, and how
    to tweak them if it does not work in your particular case:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将Go应用程序编译为静态二进制文件。如果您没有使用CGO并且链接到某些C库（如TensorFlow C库），那么使用不带任何附加标志的`go
    build`就足够了。但是，如果您的应用程序依赖于，比如说，TensorFlow C库，那么您需要添加一些额外的命令行参数以确保生成的二进制文件是静态的——也就是说，它包含所有依赖的代码。在撰写本文时，有一个关于Go
    1.13的提案，该提案为`build`命令添加了一个`-static`标志，这将无需进一步工作即可实现这一点。在此之前，Diogok有一篇出色的博客文章解释了以下命令中的不同标志，以及如何在特定情况下调整它们：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will produce a single output binary `mlapp` with all the required dependencies.
    The purpose of using all these flags is to produce a static binary that contains
    all our dependencies so that we only have the simple task of adding them to a
    "vanilla" Docker image, giving us the Dockerfile:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个包含所有必需依赖项的单个输出二进制文件`mlapp`。使用所有这些标志的目的是生成包含所有依赖项的静态二进制文件，这样我们只需将它们添加到“vanilla”Docker镜像中，从而得到Dockerfile：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: That's it! There is nothing else to add, unlike the long Dockerfiles we previously
    used because we needed all the dependencies. In this case, we already have these
    dependencies inside our Go binary. This is another advantage of Go; unlike some
    other programming languages, Go makes this type of deployment possible.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！没有其他需要添加的内容，与之前我们使用的长Dockerfile不同，因为我们需要所有依赖项。在这种情况下，我们已经在Go二进制文件内部有了这些依赖项。这是Go的另一个优点；与某些其他编程语言不同，Go使得这种类型的部署成为可能。
- en: You can also expose a port using your Dockerfile (for example, if you intend
    to serve your app from an HTTP server) by using the `EXPOSE` command. To expose
    an HTTP server listening on port 80, use the, `EXPOSE 80/tcp` command.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用Dockerfile公开一个端口（例如，如果您打算通过HTTP服务器提供应用程序），使用`EXPOSE`命令。要公开监听端口80的HTTP服务器，使用`EXPOSE
    80/tcp`命令。
- en: In the preceding example, we assumed that our model file containing the trained
    model weights/hyperparameters was persisted to disk and saved alongside our binary,
    ready to be added to the Docker container; however, there are cases where this
    may be impractical or undesirable.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们假设包含训练模型权重/超参数的模型文件已持久化到磁盘，并保存与我们的二进制文件一起，以便添加到Docker容器中；然而，在某些情况下，这可能是不可行或不希望的。
- en: Model persistence
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型持久化
- en: 'Most of the time, you can follow the aforementioned pattern of committing your
    model file alongside the source code and adding it to a Docker image during deployment
    together with your binary; however, there are times when you may want to reconsider
    this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，你可以遵循上述模式，将模型文件与源代码一起提交，并在部署时将它们与二进制文件一起添加到Docker镜像中；然而，有时你可能需要重新考虑这一点：
- en: The model file is very large, so it leads to a very large Docker image and slows
    down deployments.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型文件非常大，因此会导致非常大的Docker镜像，并减慢部署速度。
- en: The number of model files you have is dynamic and each model is associated with
    an object of your application—that is, you train one model per user.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你拥有的模型文件数量是动态的，每个模型都与你的应用程序对象相关联——也就是说，你为每个用户训练一个模型。
- en: The model is retrained much more frequently than the code is likely to change,
    leading to very frequent deployments.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的重新训练频率远高于代码可能发生的变化，导致非常频繁的部署。
- en: 'In these cases, you may want to make the model available from a different source
    and not commit it to source control. At a basic level, model files are just a
    sequence of bytes, so there is no real limit to where they can be stored: on a
    file server elsewhere, cloud file storage, or a database.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，你可能希望从不同的来源提供模型，而不是将其提交到源控制。在基本层面上，模型文件只是一系列字节，因此它们可以存储在几乎任何地方：文件服务器、云文件存储或数据库。
- en: 'The exception is the second case: where you have a dynamic number of model
    files that are associated with application objects, such as users. For example,
    if you are building a system that aims to forecast how much electricity a household
    will consume the following day, you might end up having one model for all households
    or one model per household. In the latter case, you would be better served using
    a database to hold these model files:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例外情况是第二种情况：当你有一个与应用程序对象关联的动态数量的模型文件，例如用户。例如，如果你正在构建一个旨在预测家庭第二天将消耗多少电力的系统，你可能会为所有家庭或每个家庭创建一个模型。在后一种情况下，使用数据库来保存这些模型文件会更好：
- en: The model files could be seen to contain sensitive data that is probably best
    secured and governed in a database.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型文件可能包含敏感数据，最好在数据库中安全存储和管理。
- en: A large number of model files could benefit from advanced compression techniques
    that are leveraged by database software, such as using page-level compression
    instead of row-level compression. This can reduce their overall size on the disk.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大量的模型文件可以从数据库软件利用的高级压缩技术中受益，例如使用页面级压缩而不是行级压缩。这可以减少它们在磁盘上的总体大小。
- en: It may be easier to keep data associated with application objects all in the
    same place to limit the number of queries required with authorize an operation
    related to a model, for example.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将与应用程序对象相关的数据都保存在同一个地方可能更容易，以限制执行与模型相关的操作所需的查询次数，例如。
- en: For these reasons, among others, we recommend saving the model to a database
    in the event that your application requires many models, each associated to an
    application object, such as a user.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，以及其他原因，我们建议在应用程序需要许多模型，每个模型都与一个应用程序对象（如用户）相关联的情况下，将模型保存到数据库中。
- en: This poses a small challenge, because some Go ML libraries, such as GoML, expose
    persistence functions, such as `PersistToFile` of the `linear` package models,
    and these functions persist the model to a file; however, they do not directly
    offer access to serialized model should we want to persist it elsewhere.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这带来了一点挑战，因为一些Go ML库，如GoML，公开了持久化函数，例如`linear`包模型的`PersistToFile`函数，这些函数将模型持久化到文件；然而，它们并不直接提供对序列化模型的访问，如果我们想将其持久化到其他地方。
- en: 'There are two techniques we can apply:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用两种技术：
- en: Look through the Godocs to see if the model struct has any unexported fields.
    If not, we can simply use `encoding/json` to serialize the model.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看Godocs以查看模型结构是否有未导出的字段。如果没有，我们可以简单地使用`encoding/json`来序列化模型。
- en: If there are unexported fields, we can save the model to a temporary file, read
    the temporary file into memory, and delete it again.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有未导出字段，我们可以将模型保存到临时文件中，将临时文件读入内存，然后再删除它。
- en: In Go, an **unexported field** is a struct field with a lowercase name, which
    is not accessible outside the package in which it is defined. Such fields are
    absent from serialization using `encoding/json`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Go中，一个**未导出字段**是一个具有小写名称的结构体字段，它在其定义的包外部不可访问。这些字段在`encoding/json`的序列化中不存在。
- en: In the case of GoML's `LeastSquares` model, there are no unexported fields,
    and a cursory examination of the `PersistToFile` method would reveals that it
    is using encoding/JSON to marshal the model to a byte slice. Therefore, we can
    just use `serializedModel, err := json.Marshal(leastSquaresModel)` to serialize
    it. The resulting `serializedModel` can then be saved anywhere we wish.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在GoML的`LeastSquares`模型的情况下，没有未导出字段，对`PersistToFile`方法的初步检查会显示它正在使用encoding/JSON将模型序列化到一个字节数组。因此，我们可以直接使用`serializedModel,
    err := json.Marshal(leastSquaresModel)`来序列化它。生成的`serializedModel`然后可以被保存在我们希望的地方。
- en: 'But what if, for argument''s sake, we could not do this because the model struct
    had unexported fields? For example, the golearn library''s `linear_models` package
    has an `Export` method that persists models to the file, but this relies on a
    call to a C function, and the model has unexported fields. In this case, we have
    no choice but to first persist the model to a temporary file and then recover
    the file contents:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们为了辩论的目的不能这样做，因为模型结构体有未导出字段怎么办？例如，golearn库的`linear_models`包有一个`Export`方法，它将模型持久化到文件中，但这依赖于对C函数的调用，并且模型有未导出字段。在这种情况下，我们别无选择，只能首先将模型持久化到一个临时文件中，然后恢复文件内容：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: All we are doing in the preceding code is providing a temporary location to
    store the model file on disk and then moving it back to memory. While this is
    not the most performant way to store a model, it is necessary because of the limitations
    on some of the interfaces for some Go ML libraries, and there is already an open
    issue on GoLearn's GitHub page to improve this.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们所做的一切只是提供一个临时的位置来存储模型文件在磁盘上，然后将其移回内存。虽然这不是存储模型的最高效方式，但由于一些Go ML库的接口限制，这是必要的，并且已经在GoLearn的GitHub页面上有一个开放的问题来改进这一点。
- en: Now that the application is deployed, we want some certainty that it is functioning
    correctly, using up an appropriate amount of resources, and that there is no underlying
    issue that could prevent it from being available. In the next subsection, we will
    look at monitoring techniques specific to ML applications.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应用程序已经部署，我们希望有一些确定性，确保它正在正确运行，使用适当的资源量，并且没有潜在的问题可能会阻止它可用。在下一小节中，我们将探讨针对ML应用程序的特定监控技术。
- en: Monitoring
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控
- en: In his book, *Architecting for Scale*, Lee Atchison, Principal Cloud Architect
    at New Relic, argues for the use of a risk matrix, also known as a **risk register**,
    to keep track of what is likely to go wrong with an application and how it should
    be mitigated^([16]). While this may seem like overkill for a simple application,
    it is a great tool for managing risk in a complex environment, especially where
    ML models are involved. This is because the entire team can be aware of the main
    risks, their likelihoods, and mitigation, even if they did not have a hand in
    creating every part of the application in the first place. ML models can sometimes
    be created by a data scientist and then later handed over to a software development
    team via one of the polyglot integration approaches we outlined in [Chapter 5](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml),
    *Supervised Learning*, so this makes knowing any risk associated with their use
    in production all the more important.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的书《规模架构》中，New Relic的首席云架构师Lee Atchison主张使用风险矩阵，也称为**风险登记册**，来跟踪应用程序可能出错的情况以及如何缓解。虽然这可能看起来对于一个简单的应用程序来说有些过度，但它是在复杂环境中管理风险的优秀工具，尤其是在涉及ML模型的情况下。这是因为整个团队可以了解主要风险、它们的可能性和缓解措施，即使他们最初并没有参与创建应用程序的每一个部分。ML模型有时可以由数据科学家创建，然后通过我们在第5章中概述的多语言集成方法之一，由软件开发团队接管，这使得了解与它们在生产中使用相关的任何风险都变得尤为重要。
- en: While this may seem like a rather opinionated approach, remember that the goal
    is simply to make developers think about what can cause their application to become
    unavailable. There is no obligation to write down a risk register or run your
    team using one (although both could be beneficial), and the practice of thinking
    about risk always helps by shining light on dark recesses, where no one had thought
    to look for that elusive Friday night bug that took the whole application offline
    until Monday morning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这看起来可能是一种相当主观的方法，但请记住，目标仅仅是让开发者思考什么可能导致他们的应用程序不可用。没有义务写下风险登记册或让你的团队使用一个（尽管两者都可能有益），但思考风险的做法总是通过照亮黑暗的角落，那里没有人想过要寻找那个难以捉摸的周五晚上导致整个应用程序在周一早上才恢复的虫子。
- en: A **risk** associated with a production application is different from a failure
    of a test, which you would hopefully have caught before deploying it to production
    in the first place. It is the risk that something you assumed constant in testing
    (such as available memory or a training algorithm converging) has changed to a
    critical state.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与生产应用程序相关的**风险**与测试失败不同，你希望在生产部署之前就捕捉到它。这是你在测试中假设为恒定的某些内容（例如可用内存或训练算法收敛）已经变为临界状态的风险。
- en: 'Risks associated with ML applications could include, but are not limited to,
    the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ML 应用程序相关的风险可能包括但不限于以下内容：
- en: Running out of memory to run more instances of the model
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型实例运行内存不足
- en: A model file becoming corrupt, leading to the model being unavailable to be
    run, even though the rest of the application might still be available
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型文件损坏，导致模型无法运行，尽管应用程序的其他部分可能仍然可用
- en: A nonconvergent training procedure, if model retraining is done in production,
    leading to a useless model
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在生产中进行模型重新训练，则训练过程不收敛，导致模型无用
- en: Malicious users crafting input to try to trick the model into producing a desired
    output
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意用户构建输入以试图欺骗模型产生期望的输出
- en: Malicious users crafting badly formatted input (fuzzing) to crash the model
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意用户构建格式错误的输入（模糊测试）以使模型崩溃
- en: Upstream services, such as databases used to store ML models, being unavailable
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上游服务，如存储 ML 模型的数据库不可用
- en: The cloud datacentre, where the model runs runs low on GPU availability, meaning
    that an autoscale feature fails and availability of your deep learning model is
    reduced as a result
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云数据中心在 GPU 可用性不足时运行模型，这意味着自动扩展功能失败，并且你的深度学习模型的可用性因此降低
- en: 'The list is obviously not exhaustive, but hopefully it gives you an idea of
    the kind of issues that could arise so you can look for them in your own applications.
    Because it is very difficult to come up with an exhaustive list, general monitoring
    principles apply:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表显然不是详尽的，但希望它能给你一个可能出现的各种问题的概念，以便你在自己的应用程序中寻找这些问题。因为很难列出详尽的列表，所以一般性的监控原则适用：
- en: Use structured logging in the application wherever possible and centralize these
    logs
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可能的情况下，在应用程序中使用结构化日志并集中管理这些日志
- en: If retraining in production, make sure that you set up alerts for any error
    in the training procedure, since this will necessarily lead to a useless model
    (or falling back to a deprecated one)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在生产中进行重新训练，确保为训练过程中的任何错误设置警报，因为这必然会导致模型无用（或回退到过时的一个）
- en: Capture metrics whose significant change could be used to detect any risks in
    your register materializing (for example, availability of memory space)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获可能用于检测任何风险在注册材料中显现的指标的重大变化（例如，内存空间的可用性）
- en: Go was designed partly to serve web applications^([17]), so there are many third-party
    packages that can help you perform these tasks, and we will now explore some of
    them.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Go 语言部分是为了服务 Web 应用而设计的^([17])，因此有许多第三方包可以帮助你执行这些任务，我们现在将探讨其中的一些。
- en: Structured logging
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化日志
- en: There are many logging libraries for Go, such as the standard library's `log`
    package^([18][19][20]). A significant advantage to using a structured logging
    library—which logs to a standardized format, such as JSON—over unstructured logging
    that simply uses free text is that it is far easier to work with the log data
    once it has been created. Not only is searching by a particular field easier (using,
    say, `jq`^([21]) to work with JSON data), but structured logs allow far richer
    integration with existing monitoring and analytics tooling, such as Splunk^([22])
    or Datadog^([23]).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Go有很多日志库，例如标准库的`log`包^([18][19][20]）。使用结构化日志库（例如记录到JSON这样的标准格式）而不是简单使用自由文本的结构化日志的一个显著优势是，一旦创建，处理日志数据就更容易。不仅通过特定字段进行搜索更容易（例如，使用`jq`^([21])处理JSON数据），而且结构化日志允许与现有的监控和分析工具进行更丰富的集成，例如Splunk^([22])或Datadog^([23]）。
- en: In the following example, we will use the Logrus package to log an error message
    returned by a training procedure. Note that the use of this particular logging
    package is a personal choice, and any other structured logging package would also
    work.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将使用Logrus包来记录由训练过程返回的错误信息。请注意，使用这个特定的日志包是个人选择，任何其他结构化日志包也可以工作。
- en: 'First, we configure the logger:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们配置记录器：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output format can be configured by using the properties of the `JSONFormatter`
    struct^([24]):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用`JSONFormatter`结构体的属性^([24])来配置输出格式：
- en: '`TimestampFormat`: The format of the timestamps using a time-compatible format
    string (for example, `Mon Jan 2 15:04:05 -0700 MST 2006`).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TimestampFormat`：使用兼容时间格式的格式字符串（例如，`Mon Jan 2 15:04:05 -0700 MST 2006`）来设置时间戳的格式。'
- en: '`DisableTimestamp`: Removes the timestamp from the output'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DisableTimestamp`：从输出中移除时间戳'
- en: '`DataKey`: Instead of a flat JSON output, this puts all the log entry parameters
    into a map at the given key'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataKey`：而不是扁平的JSON输出，这个选项将所有日志条目参数放入给定的键中'
- en: '`FieldMap`: Use this to rename the default output properties, such as the timestamp'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FieldMap`：使用此选项重命名默认输出属性，例如时间戳'
- en: '`CallerPrettyfier`: When `ReportCaller` is activated (as shown in the preceding
    code snippet), this function can be called to customize the output—for example,
    stripping the package name from the caller''s method'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CallerPrettyfier`：当`ReportCaller`被激活（如前一个代码片段所示）时，可以通过调用此函数来自定义输出——例如，从调用者的方法中去除包名'
- en: '`PrettyPrint`: This determines whether to indent JSON output'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PrettyPrint`：这决定了是否缩进JSON输出'
- en: 'Here is an example, where we use it in practice:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个示例，其中我们将其用于实际操作：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: While this may produce more output than necessary, because of the addition of
    the two info-level messages, we can filter out this level of output if it is not
    required by using `logrus.SetLevel`; however, in the case of retraining in production,
    the training time is important (as is making sure that the training process completes),
    so it is never a bad idea to have records of the process in the log, even if it
    becomes more verbose as a result.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能会产生比必要的更多输出，因为增加了两个info级别的消息，但我们可以使用`logrus.SetLevel`过滤掉这个级别的输出，如果不需要的话；然而，在生产中重新训练的情况下，训练时间很重要（确保训练过程完成也同样重要），所以记录日志中的过程记录永远不是一个坏主意，即使因此变得更加冗长。
- en: When logging ML-related information, it is a good idea to have a field with
    the model name (which may be something meaningful to a data scientist, if they
    created it). When you have multiple models running concurrently in production,
    it is sometimes hard to tell which one has produced the error!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当记录机器学习相关的信息时，有一个包含模型名称的字段（如果它们是由数据科学家创建的，可能对数据科学家有意义）是个好主意。当你在生产中同时运行多个模型时，有时很难判断哪个模型产生了错误！
- en: The time taken to train an algorithm is one metric that we would recommend computing
    regularly and sending to a dedicated metrics system. We will discuss capturing
    metrics in the next subsection.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 训练算法所需的时间是我们建议定期计算并发送到专用指标系统的一个指标。我们将在下一小节中讨论捕获指标。
- en: Capturing metrics
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 捕获指标
- en: In the preceding example, we inserted info-level messages in the logs to signify
    the start and end of the training process. While we could look at the timestamp
    fields of both messages and compare them to determine how long the training process
    took (Splunk, for example, is able to do this with the right query), a more direct
    and less cumbersome way to achieve the same result is to monitor this specific
    datapoint, or metric, explicitly. We could then raise alerts if the training process
    becomes too long or have a chart that logs and displays the time taken by the
    regular model training processes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的例子中，我们在日志中插入信息级消息以表示训练过程的开始和结束。虽然我们可以查看这两个消息的时间戳字段并将它们进行比较以确定训练过程花费了多长时间（例如，Splunk可以通过正确的查询做到这一点），但实现相同结果的一种更直接、更简单的方法是显式地监控这个特定的数据点或指标。然后，如果训练过程变得过长，我们可以发出警报，或者有一个图表记录并显示常规模型训练过程所花费的时间。
- en: 'There are two approaches that we can use:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用两种方法：
- en: Store the metric as an additional field on the log entry with a `float64` value
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将指标存储在日志条目上的一个附加字段中，字段值为`float64`
- en: Store the metric in a separate analytics system
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将指标存储在单独的分析系统中
- en: Ultimately, the approach you take depends on your current analytics systems,
    team preferences, and application size. As far as ML applications go, either approach
    works equally well, so we will assume the first one, as it reduces the amount
    of third-party application code required.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你采取的方法取决于你当前的分析系统、团队偏好和应用程序的大小。就机器学习应用而言，两种方法都同样有效，因此我们将假设第一种方法，因为它减少了所需的第三方应用程序代码量。
- en: 'Reusing the same example as earlier, let''s set this up:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 重复使用之前的例子，让我们设置如下：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that we did not include any of the logging calls in the timed block. This
    is because we want to measure the time taken by the training process rather than
    any logging around it.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有在计时块中包含任何日志调用。这是因为我们想要测量训练过程所花费的时间，而不是围绕它的任何日志。
- en: If your company uses an analytics system, such as Grafana or InfluxDB, you can
    still use the same approach as previously described—just make sure that you create
    a sensible name for your metric, including the name of the ML model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的公司使用分析系统，例如Grafana或InfluxDB，你仍然可以使用之前描述的相同方法——只需确保为你的指标创建一个合理的名称，包括机器学习模型的名称。
- en: In the final subsection the CD feedback loop, we will consider how accuracy/precision
    metrics can help create a feedback loop in an ML application.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的子节中，我们将考虑准确性/精确度指标如何帮助在机器学习应用中创建反馈循环。
- en: Feedback
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反馈
- en: The process of acquiring feedback in any system is intended to improve the system.
    In the case of an ML application, feedback can help make the application more
    robust with regards to risks on its register (or the addition of new risks that
    were previously unmitigated), but this is not specific to ML applications; all
    production applications benefit from a feedback cycle. There is, however, one
    special feedback cycle that is particular to ML applications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何系统中获取反馈的过程旨在改进系统。在机器学习应用的情况下，反馈可以帮助使应用在注册风险（或之前未缓解的新风险的添加）方面更加稳健，但这并不仅限于机器学习应用；所有生产应用都受益于反馈循环。然而，有一个特殊的反馈循环是特定于机器学习应用的。
- en: An ML model is used on the basis that it satisfies some accuracy/precision criteria
    that make it better or more generic at extracting meaning from data than a naive
    heuristic. In [Chapter 3](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml), *Supervised
    Learning*, and [Chapter 4](26788e93-3614-413f-bcde-5580516f9c5f.xhtml), *Unsupervised
    Learning*, we outlined some of these metrics, such as the mean square error of
    a regression of house prices or the test/validation accuracy of binary classifiers
    on images of clothes. In our CD cycle so far, we have assumed that once a model
    is created, its accuracy will never change with regards to new input; however,
    this is rarely a realistic assumption.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型是基于它满足某些准确性/精确度标准，使其在从数据中提取意义方面比简单的启发式方法更好或更通用。在第3章*监督学习*和第4章*无监督学习*中，我们概述了一些这些指标，例如房价回归的平均平方误差或二进制分类器在衣服图像上的测试/验证准确性。在我们目前的CD周期中，我们假设一旦创建了一个模型，其准确性将不会随着新输入而改变；然而，这很少是一个现实的假设。
- en: Consider our MNIST fashion classifier from [Chapter 3](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml),
    *Supervised Learning*, which aims to determine whether an image represents a pair
    of trousers or not. At the moment, this database does not contain any images of
    flared trousers. What if these come back into fashion and all the images our model
    begins to receive are of flared trousers? We may notice users complaining that
    images are not being correctly classified. Such considerations have led to numerous
    websites that rely on ML models adding "Rate my prediction" models to their websites
    in a bid to ensure that models are still outputting relevant predictions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们来自[第3章](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml)的MNIST时尚分类器，*监督学习*，其目的是确定一张图片是否代表一条裤子。目前，这个数据库中不包含任何喇叭裤的图片。如果这些款式再次流行起来，而我们模型开始接收到的所有图片都是喇叭裤，会怎样呢？我们可能会注意到用户抱怨图片没有被正确分类。这样的考虑导致了许多依赖机器学习模型的网站添加了“对我的预测评分”模型，以期确保模型仍然输出相关的预测。
- en: This is, of course, a valid approach, albeit one that relies on the customer
    to tell you when your product is and is not working. Because customers are more
    likely to use these feedback features during an unsatisfactory experience^([26]),
    any data you gather from this exercise, while still useful, is likely biased toward
    the negative and therefore cannot automatically be used as a proxy accuracy metric.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是一个有效的方法，尽管它依赖于客户告诉你产品何时工作不正常。因为客户更有可能在体验不满意时使用这些反馈功能^([26])，所以从这个练习中获得的数据，尽管仍然有用，但很可能偏向负面，因此不能自动用作代理准确度指标。
- en: In cases where the customer supplies images and your model classifies them,
    this may still be your best option, unless you can write a scraper for new trouser
    images that continuously feeds them to a model and measures its response. That
    would be labor-intensive, but would clearly produce better results, assuming,
    of course, that the types of trousers found by your scraper were representative
    of the types of trouser images supplied by your customers. In other cases, some
    automated feedback loops may be possible, where you are able to directly monitor
    the accuracy of a model, either in testing or production, and use this to make
    a decision on when the model should be retrained.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户提供图片并且你的模型对它们进行分类的情况下，这仍然可能是你的最佳选择，除非你能编写一个用于抓取新裤子图片的爬虫，并将它们持续输入到模型中，并测量其响应。这将是一项劳动密集型的工作，但显然会产生更好的结果，当然，前提是爬虫找到的裤子类型代表了客户提供的裤子图片的类型。在其他情况下，一些自动化的反馈循环可能是可能的，其中你可以直接监控模型的准确性，无论是在测试还是生产中，并据此决定何时重新训练模型。
- en: Consider a different scenario, one where you are asked to forecast the next
    day's individual electricity consumption of a large number of households, given
    data points such as the number of occupants and a forecast temperature curve.
    You decide that you will use, say, one regression per household and store the
    regression parameters in a database once the model is trained. Then, every day,
    you will run every model in your database to generate predictions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个不同的场景，即你被要求根据住户数量和预测温度曲线等数据点预测大量家庭第二天个人的电力消耗。你决定为每个家庭使用一个回归模型，并在模型训练完成后将回归参数存储在数据库中。然后，每天你将运行数据库中的每个模型以生成预测。
- en: A very easy feedback cycle exists in this case because, every day, you can also
    measure the actual electricity consumption of the household and compare this to
    your model's prediction. A scheduled script could then compare the relative difference
    between the two over a certain period, perhaps using a moving average to smooth
    out any anomalies, and should this difference be greater than a certain predefined
    threshold, it would then be entitled to assume that some of the model's input
    data had changed and the model required retraining on a new dataset. An alternative
    would be to retrain that the model if any of its input parameters changed, although
    that could lead to a lot of unnecessary retraining and thus additional cost, as
    forecast temperature curves likely change daily, so every model would likely need
    to be re-trained every day.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，反馈循环非常简单，因为每天您还可以测量家庭的实际电力消耗，并将其与您的模型预测进行比较。然后，一个预定脚本可以比较在一定时期内两者的相对差异，可能使用移动平均来平滑任何异常，如果这种差异大于某个预定义的阈值，那么就可以假设模型的一些输入数据已更改，并且模型需要在新的数据集上重新训练。另一种选择是，如果模型的任何输入参数发生变化，则重新训练该模型，尽管这可能导致大量的不必要重新训练，从而增加额外的成本，因为预测温度曲线可能每天都会变化，因此每个模型可能每天都需要重新训练。
- en: 'The feedback loop for ML applications with continuous validation and retraining
    is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 具有持续验证和重新训练的机器学习应用程序的反馈循环如下：
- en: '![](img/3292a8f0-4375-4ca5-83d3-ea81d2f255ec.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图3：具有持续验证的机器学习应用程序的反馈循环](img/3292a8f0-4375-4ca5-83d3-ea81d2f255ec.png)'
- en: 'Fig. 3: The feedback loop for ML applications with continuous validation'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：具有持续验证的机器学习应用程序的反馈循环
- en: The feedback loop cannot be applied to every ML application, but with a little
    creativity, you can usually find a way in which to find input samples that were
    not in either the training or testing dataset, but are of updated relevance. If
    you can automate the process of generating predictions from these samples and
    storing their difference to a ground truth, then you can still generate the same
    feedback loop.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有机器学习应用程序都可以应用反馈循环，但只要有一点创意，通常可以找到一种方法来找到既不在训练数据集也不在测试数据集中的输入样本，但这些样本具有更新的相关性。如果您可以自动化从这些样本生成预测并存储其与真实值的差异的过程，那么您仍然可以生成相同的反馈循环。
- en: Deployment models for ML applications
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习应用程序的部署模型
- en: In the preceding example, we explained how to deploy an ML application using
    Docker to encompass it and its dependencies. We deliberately stayed away from
    any discussion pertaining to the infrastructure that was going to run these containers
    or any Platform-as-a-Service offerings that could facilitate the development or
    deployment itself. In the current section, we consider different deployment models
    for ML applications under the assumption that the application will be deployed
    to a cloud platform that supports both IAAS and platform-as-a-service models,
    such as Microsoft Azure and Amazon Web Services.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们解释了如何使用Docker部署机器学习应用程序，以包含其依赖项。我们故意避免讨论将要运行这些容器的任何基础设施或任何可能促进开发或部署的平台即服务产品。在本节中，我们考虑了在假设应用程序将被部署到支持IAAS和平台即服务模型的云平台（如Microsoft
    Azure和Amazon Web Services）的情况下，机器学习应用程序的不同部署模型。
- en: This section is specifically written to help you decide what virtual infrastructure
    to use if you are deploying an ML application to the cloud.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本节专门编写，旨在帮助您在将机器学习应用程序部署到云端时决定使用哪种虚拟基础设施。
- en: 'There are two main deployment models for any cloud application:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何云应用程序，主要有两种部署模型：
- en: '**Infrastructure-as-a-service**: This is the cloud service that offers a high-level
    interaction with virtualized hardware, such as virtual machines, without the customer
    needing to maintain the hardware or the virtualization layer.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施即服务**：这是一种云服务，提供与虚拟化硬件（如虚拟机）的高级交互，而无需客户维护硬件或虚拟化层。'
- en: '**Platform-as-a-service**: This is a cloud service that offers Software-as-a-Service
    components that you can then build your application from, such as a serverless
    execution environment (for example, AWS Lambda).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平台即服务**：这是一种云服务，提供软件即服务组件，您可以从这些组件构建应用程序，例如无服务器执行环境（例如，AWS Lambda）。'
- en: 'We will consider both options and how to make best use of them for ML applications.
    We will compare and contrast the three main vendors by market share, as of Q4
    2018: Amazon Web Services, Microsoft Azure, and Google Cloud^([30]).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑这两种选项，以及如何最好地利用它们来为机器学习应用服务。我们将根据截至2018年第四季度的市场份额比较和对比三个主要供应商：亚马逊网络服务、微软Azure和Google
    Cloud^([30])。
- en: Infrastructure-as-a-service
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础设施即服务
- en: Earlier in this chapter, we explained how to package an ML application using
    Docker. In this subsection, we will look at simple ways to deploy an ML application
    using Docker to AWS, Azure, or Google Cloud.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期部分，我们解释了如何使用Docker打包一个机器学习应用。在本小节中，我们将探讨使用Docker将机器学习应用部署到AWS、Azure或Google
    Cloud的简单方法。
- en: 'In each case, we will start by explaining how to push one of your local Docker
    images to a **registry** (that is, a machine that will store images and serve
    them to the rest of your infrastructure). There are several advantages to using
    a Docker registry to store your images:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，我们都会首先解释如何将你的本地Docker镜像推送到一个**注册表**（即一个将存储镜像并向你的基础设施的其他部分提供镜像的机器）。使用Docker注册表存储你的镜像有几种优点：
- en: '**Faster deployments and build times**: Virtual infrastructure components requiring
    images can just pull them from the registry instead of building them from scratch
    every time'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快的部署和构建时间**：需要镜像的虚拟基础设施组件可以直接从注册表中拉取，而不是每次都从头开始构建。'
- en: '**Ease of implementing autoscale in your application**: If you have to wait
    for a long Docker build—say, 20 minutes, for TensorFlow—every time you need to
    scale your service up, you may experience degradation or unavailability'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在应用中实现自动扩展的简便性**：如果你每次需要扩展服务时都必须等待长时间的Docker构建——比如，对于TensorFlow来说，可能需要20分钟——那么你可能会遇到性能下降或不可用的情况。'
- en: '**Security**: Pulling images from a single trusted source reduces the attack
    surface'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：从单个可信源拉取镜像可以减少攻击面'
- en: Amazon Web Services
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亚马逊网络服务
- en: 'The core of AWS''s virtualized IaaS offering is **Elastic Compute** (**EC2**).
    AWS also offers **Elastic Container Registry** (**ECR**) as a registry service
    to serve images from. To set this up, go through the following steps:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: AWS虚拟化IaaS服务的核心是**弹性计算**（**EC2**）。AWS还提供**弹性容器注册表**（**ECR**）作为注册表服务来提供镜像。要设置此服务，请按照以下步骤操作：
- en: Before you can push or pull an image to an ECR registry, you need `ecr:GetAuthorizationToken`
    permissions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在你能够将镜像推送到或从ECR注册表拉取之前，你需要`ecr:GetAuthorizationToken`权限。
- en: 'Tag your image, assuming its ID is `f8``ab2d331c34`:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记你的镜像，假设其ID为`f8ab2d331c34`：
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Push the image to the ECR:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将镜像推送到ECR：
- en: '[PRE7]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The image is now available to use from an EC2 instance. First, SSH into your
    instance where you have installed Docker, following the instructions in [Chapter
    5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using Pre-Trained Models,* and
    then run the following commands to install Docker and start a container from the
    image (amend the `docker run` command to add exposed ports or volumes):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个镜像可以从EC2实例中使用。首先，按照[第5章](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml)，*使用预训练模型*中的说明SSH到你的实例，然后运行以下命令来安装Docker并从镜像启动一个容器（修改`docker
    run`命令以添加公开的端口或卷）：
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Microsoft Azure
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微软Azure
- en: 'Similar to Amazon''s ECR, which we discussed in the previous subsection, Microsoft
    Azure offers a registry, Azure Container Registry. We can use this by following
    the same steps as AWS ECR, but there is a difference, namely the requirement to
    log in via the Docker command-line interface. Once this is done, you can follow
    the same instructions as the previous subsection, but with your registry and image
    details:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在前一小节中讨论的亚马逊的ECR类似，微软Azure提供了一个注册表，即Azure Container Registry。我们可以通过遵循与AWS
    ECR相同的步骤来使用它，但有一个区别，即需要通过Docker命令行界面进行登录。一旦完成，你可以遵循前一小节的相同说明，但使用你的注册表和镜像详细信息：
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Microsoft also allows Docker as a deployment method for App Service Apps, a
    managed web app service based on Microsoft''s **Internet Information Services**
    (**IIS**). If you have followed the preceding steps to deploy your Docker image
    to a registry, you can use the `az` command-line tool to create a web app from
    your image:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 微软还允许将Docker作为App Service Apps的部署方法，这是一个基于微软的**互联网信息服务**（**IIS**）的托管Web应用服务。如果你已经按照前面的步骤将Docker镜像部署到注册表，你可以使用`az`命令行工具从你的镜像创建一个Web应用：
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Google Cloud
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud
- en: 'Like Amazon and Microsoft, Google also offers a registry, called Container
    Registry, which can be used as a Docker registry. The steps to use it are the
    same as for Amazon ECR, except for the addition of a preliminary authentication
    step using the `gcloud` command-line tool:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 与亚马逊和微软一样，谷歌也提供了一个名为 Container Registry 的注册表，它可以作为 Docker 注册表使用。使用它的步骤与 Amazon
    ECR 相同，只是增加了使用 `gcloud` 命令行工具进行初步认证步骤：
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now you can push the image:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以推送镜像：
- en: '[PRE12]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The steps to run a Docker container on a Google Cloud VM are the same as for
    an EC2 VM, with the addition of the authentication step.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Cloud VM 上运行 Docker 容器的步骤与 EC2 VM 相同，只是增加了认证步骤。
- en: Platform-as-a-Service
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平台即服务
- en: With the rising popularity of ML components in applications, cloud vendors have
    scrambled to provide platform-as-a-service offerings that make it easier to deploy
    ML applications in an effort to win over customers. It is worth a brief review
    of each of the three main cloud vendors by market share as of 2018^([30]). This
    is not an attempt to recommend one vendor over another, but rather an attempt
    to explore solutions while remaining agnostic to any decisions regarding cloud
    vendors that you may have already made. In other words, the deployment models
    we will discuss will work in all three clouds—and probably others—but some platforms
    offer specific services that may better suit certain applications or reduce their
    development effort.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习组件在应用中的日益流行，云服务提供商纷纷推出平台即服务产品，以简化机器学习应用的部署，以期吸引客户。简要回顾一下截至 2018 年市场份额的三个主要云服务提供商是值得的^([30])。这并不是要推荐一个供应商而不是另一个，而是试图在保持对您可能已经做出的任何关于云供应商的决定的独立性的同时探索解决方案。换句话说，我们将讨论的部署模型将在所有三个云中（以及可能的其他云）工作——并且某些平台提供特定的服务，可能更适合某些应用或减少其开发工作量。
- en: Cloud vendors make such frequent changes to their offerings that it is possible
    that by the time you are reading this, there will be newer, better services than
    the ones described here. Look in the *Further reading* section for some links
    to Google Cloud, AWS, and Azure ML services^([27][28][29]).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商频繁更改其服务，因此在您阅读此内容时，可能会有比这里描述的更新、更好的服务。请查看 *进一步阅读* 部分中提供的 Google Cloud、AWS
    和 Azure ML 服务的链接^([27][28][29])。
- en: Amazon Web Services
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亚马逊网络服务
- en: '**Amazon Web Services** (**AWS**) has two main types of service offerings regarding
    ML space:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**亚马逊网络服务**（**AWS**）在机器学习领域提供两种主要的服务类型：'
- en: '**AWS Sagemaker**: A hosted environment to run ML notebooks and SDK to efficiently
    perform various ML-related tasks, including data labeling'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS SageMaker**：一个托管环境，用于运行机器学习笔记本和 SDK，以高效地执行各种与机器学习相关的任务，包括数据标注'
- en: '**AWS AI Services**: A set of pretrained models for specific tasks, such as
    image recognition'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS AI 服务**：一组针对特定任务的预训练模型，例如图像识别'
- en: Amazon Sagemaker
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亚马逊 SageMaker
- en: Amazon Sagemaker uses Jupyter as a development environment for ML models, as
    we have done throughout the book. The environment in which these Jupyter notebooks
    run comes with some Python ML libraries. For Python developers, this service can
    be thought of as another environment to run ML code with some features to accelerate
    large-scale learning through AWS resources. An example using Sagemaker to perform
    hyperparameter tuning on a natural language processing task can be found on the
    AWS GitHub^([31]), and for a longer introduction there are some exploratory videos
    available on YouTube^([33]). Unfortunately, at this time, there is no way to use
    Sagemaker with a Go kernel for Jupyter (such as gophernotes), so it is not a pure-Go
    solution for interactively developing ML applications in a remote environment.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 SageMaker 使用 Jupyter 作为机器学习模型的开发环境，正如我们在整本书中所做的那样。这些 Jupyter 笔记本运行的环境包含一些
    Python 机器学习库。对于 Python 开发者来说，这项服务可以被视为另一个运行机器学习代码的环境，并具有一些通过 AWS 资源加速大规模学习的功能。使用
    SageMaker 在自然语言处理任务上进行超参数调整的示例可以在 AWS GitHub^([31]) 上找到，更长的介绍可以在 YouTube^([33])
    上找到。不幸的是，目前还没有办法使用 SageMaker 与 Jupyter 的 Go 内核（如 gophernotes）一起使用，因此它不是在远程环境中交互式开发机器学习应用的纯
    Go 解决方案。
- en: 'For Go developers who need to interact with an existing Sagemaker solution,
    there is an SDK that has much of the same features as the Python SDK^([32]), so
    it is possible to use gophernotes locally to create Sagemaker tasks. In fact,
    the SDK is so powerful that it allows Go developers to access a useful data preprocessing
    service: the Sagemaker Labeling Job service. This service integrates with Mechanical
    Turk to provide ground truth labels for training data where they are either missing
    entirely or from part of the dataset. This saves a lot of time compared to manually
    setting up Mechanical Turk jobs. The function that exposes this functionality
    is `CreateLabelingJob`.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要与现有Sagemaker解决方案交互的Go开发者，有一个SDK具有与Python SDK^([32]) 相当多的相同功能，因此可以使用gophernotes在本地创建Sagemaker任务。事实上，这个SDK非常强大，它允许Go开发者访问一个有用的数据预处理服务：Sagemaker标签作业服务。该服务与Mechanical
    Turk集成，为训练数据提供地面实况标签，这些数据要么完全缺失，要么来自数据集的一部分。与手动设置Mechanical Turk作业相比，这节省了大量时间。暴露此功能的功能是`CreateLabelingJob`。
- en: If you need to use a supervised learning algorithm, but have only an unlabeled
    dataset, consider using Sagemaker's interface to Mechanical Turk to label your
    dataset cheaply. Alternatively, you can create a labeling task through the Mechanical
    Turk UI at [https://www.mturk.com/](https://www.mturk.com/).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要使用监督学习算法，但只有未标记的数据集，请考虑使用Sagemaker的Mechanical Turk接口来便宜地标记您的数据集。或者，您可以通过Mechanical
    Turk UI在[https://www.mturk.com/](https://www.mturk.com/)创建标记任务。
- en: Amazon AI Services
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Amazon AI服务
- en: 'If there is already a model exposed that solves your ML problem, then there
    is no need for you to reinvent the wheel and train a new model, especially considering
    the large resources that AWS will have invested in ensuring the accuracy and efficiency
    of its models. At the time of writing, the following types of algorithms are available
    on a pay-for-usage basis:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已经有一个模型公开解决了您的ML问题，那么您就没有必要重新发明轮子并训练一个新的模型，尤其是考虑到AWS将投入大量资源来确保其模型的准确性和效率。在撰写本文时，以下类型的算法以按使用付费的方式提供：
- en: '**Amazon Personalize**: Built on the same recommendation techniques used by
    Amazon in their online retail store, these allow you to solve problems, such as
    showing customers items similar to those they have already bought'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Personalize**: 建立在亚马逊在线零售店使用的相同推荐技术之上，这些技术允许您解决诸如显示与客户已购买的商品类似的项目等问题'
- en: '**Amazon Forecast**: Timeseries forecasting models'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Forecast**: 时间序列预测模型'
- en: '**Amazon Rekognition**: Image and video analysis'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Rekognition**: 图像和视频分析'
- en: '**Amazon Comprehend**: Natural language processing tasks and text analysis'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Comprehend**: 自然语言处理任务和文本分析'
- en: '**Amazon Textract**: Large-scale document analysis'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Textract**: 大规模文档分析'
- en: '**Amazon Polly**: Text-to-speech'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Polly**: 文本转语音'
- en: '**Amazon Lex**: Build chatbots in a UI environment'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Lex**: 在UI环境中构建聊天机器人'
- en: '**Amazon Translate**: Automated translation to and from a multitude of languages'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Translate**: 自动翻译到和从多种语言'
- en: '**Amazon Transcribe**: Speech-to-text service'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Transcribe**: 语音转文本服务'
- en: While none of these services are Go specific, they all offer Go SDKs that you
    can use to interact with them. This is very similar to the example we saw in [Chapter
    5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using Pre-Trained Models*, where
    a model was exposed over HTTP and we used this protocol to send it data and receive
    predictions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些服务都不是Go特定的，但它们都提供了Go SDK，您可以使用它来与之交互。这与我们在[第5章](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml)中看到的示例非常相似，即*使用预训练模型*，其中模型通过HTTP公开，我们使用此协议发送数据并接收预测。
- en: 'Generally, the methods are synchronous—that is, you will get the result in
    the output argument, and do not need to make a further request later. They also
    have the same type of signature, where the name of the prediction method may vary,
    and the structure of the input/output will also vary:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些方法是同步的——也就是说，您将在输出参数中获得结果，并且以后不需要进一步请求。它们还具有相同类型的签名，其中预测方法的名称可能不同，输入/输出的结构也会不同：
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'By way of example, consider Rekognition, which, like the other services, has
    a Go SDK^([34]). Suppose that we wish to detect faces in an image. For this, we
    use the `DetectFaces` func; this has the following signature:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以Rekognition为例，与其他服务一样，它有一个Go SDK^([34])。假设我们希望检测图像中的面部。为此，我们使用`DetectFaces`函数；它具有以下签名：
- en: '[PRE14]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The input, in this case, contains, among other things, an array of facial attributes
    that we wish to be returned, as well as an image, either as base-64 encoded bytes
    or an S3 object. The output will contain a `FaceDetail` struct, which, among other
    things, will describe an age range for each face, whether it is bearded, a confidence
    in its bounding box, any detected emotions, whether they are wearing glasses,
    and so on. This depends on which facial attributes we requested in the input,
    and necessarily, the more attributes we requested, the more expensive the request
    will be (as Amazon will need to run more models to give us the answer).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，输入包含我们希望返回的数组形式的面部特征，以及一张图片，可以是base-64编码的字节或S3对象。输出将包含一个`FaceDetail`结构体，其中将描述每个面部年龄范围、是否留胡须、其边界框的置信度、检测到的情绪、是否戴眼镜等等。这取决于我们在输入中请求了哪些面部特征，并且必然地，我们请求的特征越多，请求的成本就越高（因为亚马逊需要运行更多的模型来给出答案）。
- en: Generally, if it is possible to build your ML application by composing prebuilt
    models exposed over SDKs, such as AWS, then you will save a lot of time, and it
    will allow you to focus on adding value specific to your business; however, there
    are risks associated with vendor lock-in, and at the time of writing, no other
    cloud platform offers a feature-for-feature alternative to Amazon AI services.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果你可以通过组合通过SDK公开的预构建模型来构建你的ML应用程序，那么你可以节省大量时间，这将使你能够专注于添加特定于你业务的价值；然而，与供应商锁定相关的风险是存在的，并且在撰写本文时，没有其他云平台提供与亚马逊AI服务功能对等的选择。
- en: Microsoft Azure
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Microsoft Azure
- en: 'Azure''s main offerings geared at ML applications are as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Azure针对ML应用程序的主要产品如下：
- en: '**Azure ML Studio**: A UI environment to build ML pipelines and train models'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure ML Studio**：一个用于构建ML管道和训练模型的UI环境'
- en: '**Azure Cognitive Services**: Pretrained models exposed over HTTP'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure认知服务**：通过HTTP公开预训练模型'
- en: Azure ML Studio
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure ML Studio
- en: Azure ML Studio is a cloud-based IDE for ML. It allows users to import data
    from other Azure services (such as Blob Storage), transform the data, and use
    it to train one of the included ML algorithms. The resulting model can then be
    exposed via HTTP or composed with other Azure services, such as Azure Stream Analytics
    for a real-time ML application^([35]).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Azure ML Studio是一个基于云的ML IDE。它允许用户从其他Azure服务（如Blob存储）导入数据，转换数据，并使用它来训练包含的ML算法之一。然后，可以通过HTTP公开该模型，或者与其他Azure服务（如Azure流分析）组合，以用于实时ML应用程序^([35])。
- en: While it is possible to run custom Python code within the Azure ML Studio UI,
    at the time of writing, this does not extend to Go; however, because it is possible
    to expose models via HTTP, you can integrate with an existing Azure ML Studio
    model by following the same pattern that we discussed in [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml),
    *Using Pretrained Models*, where the `net/http` client is used to make requests.
    It is worth using the Azure SDK just to generate authentication tokens rather
    than trying to implement this yourself, as the procedure can be error prone^([36]).
    The JSON structure of the request and response are very simple compared to AWS,
    so the resulting code can be clean and easy to maintain.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在Azure ML Studio UI中可以运行自定义Python代码，但在撰写本文时，这并不包括Go；然而，因为可以通过HTTP公开模型，你可以通过遵循我们在[第5章](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml)“使用预训练模型”中讨论的相同模式来集成现有的Azure
    ML Studio模型，其中使用`net/http`客户端来发送请求。仅为了生成身份验证令牌而使用Azure SDK是值得的，而不是尝试自己实现，因为该过程可能会出错^([36])。与AWS相比，请求和响应的JSON结构非常简单，因此生成的代码可以干净且易于维护。
- en: Azure Cognitive Services
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure认知服务
- en: 'Azure Cognitive Services exposes several pretrained ML models over HTTP:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Azure认知服务通过HTTP公开了几个预训练的ML模型：
- en: '**Computer Vision**: Image recognition'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算机视觉**：图像识别'
- en: '**Speech**: Speech recognition and transcription'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音**：语音识别和转录'
- en: '**LUIS**: Textual intent analysis'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LUIS**：文本意图分析'
- en: '**Bing Image Search**: Retrieves images matching a text string'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bing图像搜索**：检索与文本字符串匹配的图像'
- en: '**Bing Web Search**: Retrieves URLs matching a text string'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bing网络搜索**：检索与文本字符串匹配的URL'
- en: '**Text Analytics**: Sentiment analysis'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分析**：情感分析'
- en: At the time of writing, there is no Go SDK to interact with Cognitive Services,
    but it is possible to invoke the models by using the REST API, and Microsoft provides
    an example of this in a Quickstart article^([37]).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，没有Go SDK可以与认知服务交互，但可以通过使用REST API来调用模型，并且微软在快速入门文章中提供了一个示例^([37])。
- en: Google Cloud
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud
- en: 'Google Cloud currently has two main services to offer ML application developers,
    in addition to the free Google Colaboratory^([29]):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 除了免费的Google Colaboratory^([29])之外，Google Cloud目前为机器学习应用程序开发者提供两项主要服务：
- en: '**AI Platform**: Hosted development environment using Notebooks, VM images,
    or Kubernetes images'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI Platform**：使用Notebooks、VM镜像或Kubernetes镜像的托管开发环境'
- en: '**AI Hub**: Hosted repository of plug-and-play AI components'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI Hub**：托管即插即用AI组件的仓库'
- en: '**AI Building Blocks**: Pretrained models, exposed via SDK or HTTP'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI Building Blocks**：通过SDK或HTTP暴露的预训练模型'
- en: Because AI Hub is targeted only at Python developers and its deployment model
    is the same as AI Platform, we will not discuss it any further.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因为AI Hub仅针对Python开发者，并且其部署模型与AI平台相同，所以我们不会进一步讨论它。
- en: AI Platform
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI平台
- en: Google's AI Hub is a code-based environment aimed at facilitating all aspects
    of the ML application development life cycle, from data ingestion to deployment,
    via AI Platform Prediction (applicable to TensorFlow models exported as a `SavedModel`,
    as in our [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using Pretrained
    Models*, example) or Kubernetes. It has loose integrations with other Google Cloud
    Services, but remains, at its core, a hosted notebook environment.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Google的AI Hub是一个基于代码的环境，旨在促进机器学习应用程序开发生命周期的各个方面，从数据摄取到部署，通过AI Platform Prediction（适用于作为`SavedModel`导出的TensorFlow模型，如我们[第5章](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml)，*使用预训练模型*示例）或Kubernetes。它与其他Google
    Cloud服务有松散的集成，但其核心仍然是一个托管笔记本环境。
- en: Because there is no high-level API to create TensorFlow graphs in Go, analogous
    to Keras in Python, it is unlikely that a Go developer will find the end-to-end
    platform useful. However, if you are interacting with a TensorFlow model, using
    AI Platform Prediction to manage the resources for the model and calling it via
    HTTP^([40]) is an excellent strategy, particularly as the model can be made to
    run on VMs with a Tensor Processing Unit, which can be a significantly cheaper
    way to run TensorFlow workflows^([39]).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 因为没有高级API可以在Go中创建TensorFlow图，类似于Python中的Keras，所以Go开发者不太可能发现端到端平台有用。然而，如果您正在与TensorFlow模型交互，使用AI
    Platform Prediction来管理模型资源并通过HTTP^([40])调用它是一个很好的策略，特别是当模型可以在具有Tensor Processing
    Unit的VM上运行时，这可以是一个显著降低运行TensorFlow工作流程成本的方法^([39])。
- en: AI Building Blocks
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI Building Blocks
- en: 'Google''s AI Building Blocks are a suite of pretrained models, exposed via
    HTTP or through one of Google Cloud''s SDKs:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Google的AI Building Blocks是一套预训练模型，通过HTTP或Google Cloud的SDK之一进行暴露：
- en: '**Sight**: Includes Vision, for image recognition, and Video, for content discovery'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sight**：包括视觉，用于图像识别，以及视频，用于内容发现'
- en: '**Language**: Comprises translation and natural language processing functionality'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言**：包括翻译和自然语言处理功能'
- en: '**Conversation**: Consists of a speech-to-text model, a text-to-speech model,
    and a chatbox builder'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话**：包括语音转文本模型、文本转语音模型和聊天框构建器'
- en: '**Structured data**:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化数据**：'
- en: '**Recommendations AI**: Recommendation engine'
  id: totrans-220
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Recommendations AI**：推荐引擎'
- en: '**AutoML Tables**: UI to generate predictive models'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AutoML Tables**：生成预测模型的UI'
- en: '**Cloud Inference AI**: Time series inference and correlations tool'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cloud Inference AI**：时间序列推理和相关性工具'
- en: 'The Go SDK is very easy to use, as the following example shows. The example
    uses the text-to-speech API to download a recording of the phrase `hello, world`,
    as spoken by the ML model:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Go SDK非常易于使用，以下示例将展示这一点。该示例使用文本转语音API下载由机器模型说出的短语`hello, world`的录音：
- en: '[PRE15]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As with other models-over-HTTP type services, if you can build your application
    by composing these premade models, then you can dedicate your time to work on
    value-adding business logic; however, always consider the downsides of vendor
    lock-in.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他通过HTTP类型的服务模型一样，如果您可以通过组合这些预制的模型来构建您的应用程序，那么您可以专注于工作在增值业务逻辑上；然而，始终要考虑供应商锁定带来的不利影响。
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how to take a prototype ML application to production.
    Along the way, we explored concerns that a software developer or DevOps engineer
    would typically think of, but from an ML application developers point of view.
    Specifically, we learned how to apply a continuous development life cycle to an
    ML application and the different ways to deploy ML applications in the cloud.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何将原型机器学习应用程序推向生产。在这个过程中，我们探讨了软件开发者或DevOps工程师通常会考虑的问题，但都是从机器学习应用程序开发者的角度出发。具体来说，我们学习了如何将持续开发生命周期应用于机器学习应用程序，以及云中部署机器学习应用程序的不同方式。
- en: In the next and final chapter, we will take a step back and look at ML development
    from a project management point of view.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章和最后一章中，我们将退后一步，从项目管理角度审视机器学习开发。
- en: Further readings
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Continuous Software Engineering and Beyond: Trends and Challenges Brian Fitzgerald*,
    1st International Workshop on Rapid Continuous Software Engineering. New York,
    NY: Association for Computing Machinery, pp. 1–9.'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*持续软件工程及其超越：趋势和挑战，布赖恩·菲茨杰拉德*，第1届快速持续软件工程国际研讨会。纽约，纽约：计算机协会出版社，第1-9页。'
- en: '*Google''s solution to accidental algorithmic racism*: ban gorillas: [https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people](https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people).
    Retrieved May 3, 2019.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*谷歌解决算法性种族歧视的方法*：禁止大猩猩：[https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people](https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people).
    2019年5月3日检索。'
- en: '*Building Numpy* from source: [http://robpatro.com/blog/?p=47](http://robpatro.com/blog/?p=47).
    Retrieved May 5, 2019.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*从源代码构建Numpy*：[http://robpatro.com/blog/?p=47](http://robpatro.com/blog/?p=47).
    2019年5月5日检索。'
- en: '*Python—Compiling Numpy with OpenBLAS integration*: [https://stackoverflow.com/questions/11443302/compiling-numpy-with-openblas-integration](https://stackoverflow.com/questions/11443302/compiling-numpy-with-openblas-integration).
    Retrieved May 5, 2019.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Python—使用OpenBLAS集成编译Numpy*：[https://stackoverflow.com/questions/11443302/compiling-numpy-with-openblas-integration](https://stackoverflow.com/questions/11443302/compiling-numpy-with-openblas-integration).
    2019年5月5日检索。'
- en: '*Issues—TensorFlow*: [https://github.com/tensorflow/tensorflow/issues](https://github.com/tensorflow/tensorflow/issues).
    Retrieved May 5, 2019.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*TensorFlow的问题*：[https://github.com/tensorflow/tensorflow/issues](https://github.com/tensorflow/tensorflow/issues).
    2019年5月5日检索。'
- en: '*Python Wheels*: [https://pythonwheels.com/](https://pythonwheels.com/). Retrieved
    May 5, 2019.'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Python Wheels*：[https://pythonwheels.com/](https://pythonwheels.com/). 2019年5月5日检索。'
- en: '*Chocolateay—The Package Manager for Windows*: [https://chocolatey.org/](https://chocolatey.org/).
    Retrieved May 5, 2019.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Chocolateay—Windows的包管理器*：[https://chocolatey.org/](https://chocolatey.org/).
    2019年5月5日检索。'
- en: '*Docker Deployment on Azure*: [https://azure.microsoft.com/en-gb/services/kubernetes-service/docker/](https://azure.microsoft.com/en-gb/services/kubernetes-service/docker/).
    Retrieved May 5, 2019.'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在Azure上部署Docker*：[https://azure.microsoft.com/en-gb/services/kubernetes-service/docker/](https://azure.microsoft.com/en-gb/services/kubernetes-service/docker/).
    2019年5月5日检索。'
- en: '*What is Docker? | AWS*: [https://aws.amazon.com/docker/](https://aws.amazon.com/docker/).
    Retrieved May 5, 2019.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*什么是Docker？| AWS*：[https://aws.amazon.com/docker/](https://aws.amazon.com/docker/).
    2019年5月5日检索。'
- en: '*Docker Provider for Terraform*: [https://www.terraform.io/docs/providers/docker/r/container.html](https://www.terraform.io/docs/providers/docker/r/container.html).
    Retrieved May 5, 2019.'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Terraform的Docker提供者*：[https://www.terraform.io/docs/providers/docker/r/container.html](https://www.terraform.io/docs/providers/docker/r/container.html).
    2019年5月5日检索。'
- en: '*Chef Cookbook for Docker*: [https://github.com/chef-cookbooks/docker](https://github.com/chef-cookbooks/docker).
    Retrieved May 5, 2019.'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Docker的Chef食谱*：[https://github.com/chef-cookbooks/docker](https://github.com/chef-cookbooks/docker).
    2019年5月5日检索。'
- en: '*Docker—manage Docker containers*[: https://docs.ansible.com/ansible/2.6/modules/docker_module.html](https://docs.ansible.com/ansible/2.6/modules/docker_module.html).
    Retrieved May 5, 2019.'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Docker—管理Docker容器*[: https://docs.ansible.com/ansible/2.6/modules/docker_module.html](https://docs.ansible.com/ansible/2.6/modules/docker_module.html).
    2019年5月5日检索。'
- en: 'cmd/go: build: add static flag: [https://github.com/golang/go/issues/26492](https://github.com/golang/go/issues/26492).
    Retrieved May 5, 2019.'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'cmd/go: build: 添加静态标志：[https://github.com/golang/go/issues/26492](https://github.com/golang/go/issues/26492).
    2019年5月5日检索。'
- en: '*On Golang static binaries, cross-compiling, and plugins*: [https://medium.com/@diogok/on-golang-static-binaries-cross-compiling-and-plugins-1aed33499671](https://medium.com/@diogok/on-golang-static-binaries-cross-compiling-and-plugins-1aed33499671).
    Retrieved May 5, 2019.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*关于Golang静态二进制文件、交叉编译和插件*：[https://medium.com/@diogok/on-golang-static-binaries-cross-compiling-and-plugins-1aed33499671](https://medium.com/@diogok/on-golang-static-binaries-cross-compiling-and-plugins-1aed33499671).
    2019年5月5日检索。'
- en: '*Saving model outside filesystem*: [https://github.com/sjwhitworth/golearn/issues/220](https://github.com/sjwhitworth/golearn/issues/220).
    Retrieved May 6, 2019.'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在文件系统外保存模型*：[https://github.com/sjwhitworth/golearn/issues/220](https://github.com/sjwhitworth/golearn/issues/220).
    2019年5月6日检索。'
- en: '*Architecting for Scale*, Lee Atchison, 2016, O''Reilly Press.'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*《为规模而设计》，李·阿奇森，2016年，奥莱利出版社。'
- en: '*Server-side I/O: Node.js vs PHP vs Java vs Go*: [https://www.toptal.com/back-end/server-side-io-performance-node-php-java-go](https://www.toptal.com/back-end/server-side-io-performance-node-php-java-go).
    Retrieved May 6, 2019.'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Server-side I/O: Node.js vs PHP vs Java vs Go*: [https://www.toptal.com/back-end/server-side-io-performance-node-php-java-go](https://www.toptal.com/back-end/server-side-io-performance-node-php-java-go).
    获取日期：2019年5月6日。'
- en: '*Zap*: [https://github.com/uber-go/zap](https://github.com/uber-go/zap). Retrieved
    May 6, 2019.'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Zap*: [https://github.com/uber-go/zap](https://github.com/uber-go/zap). 获取日期：2019年5月6日。'
- en: '*Logrus*: [https://github.com/sirupsen/logrus](https://github.com/sirupsen/logrus).
    Retrieved May 6, 2019.'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Logrus*: [https://github.com/sirupsen/logrus](https://github.com/sirupsen/logrus).
    获取日期：2019年5月6日。'
- en: '*Log*: [https://github.com/apex/log](https://github.com/apex/log). Retrieved
    May 6, 2019.'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Log*: [https://github.com/apex/log](https://github.com/apex/log). 获取日期：2019年5月6日。'
- en: '*jq*: [https://stedolan.github.io/jq/](https://stedolan.github.io/jq/). Retrieved
    May 6, 2019.'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*jq*: [https://stedolan.github.io/jq/](https://stedolan.github.io/jq/). 获取日期：2019年5月6日。'
- en: '*Splunk*: [https://www.splunk.com/](https://www.splunk.com/). Retrieved May
    6, 2019.'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Splunk*: [https://www.splunk.com/](https://www.splunk.com/). 获取日期：2019年5月6日。'
- en: '*Datadog*: [https://www.datadoghq.com/](https://www.datadoghq.com/). Retrieved
    May 6, 2019.'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Datadog*: [https://www.datadoghq.com/](https://www.datadoghq.com/). 获取日期：2019年5月6日。'
- en: '*logrus—GoDoc*: [https://godoc.org/github.com/sirupsen/logrus#JSONFormatter](https://godoc.org/github.com/sirupsen/logrus#JSONFormatter).
    Retrieved May 6, 2019.'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*logrus—GoDoc*: [https://godoc.org/github.com/sirupsen/logrus#JSONFormatter](https://godoc.org/github.com/sirupsen/logrus#JSONFormatter).
    获取日期：2019年5月6日。'
- en: '*Grafana*: [https://grafana.com/](https://grafana.com/). Retrieved May 6, 2019.'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Grafana*: [https://grafana.com/](https://grafana.com/). 获取日期：2019年5月6日。'
- en: '*Bias of bad customer service interactions*: [https://www.marketingcharts.com/digital-28628](https://www.marketingcharts.com/digital-28628).
    Retrieved May 6, 2019.'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Bias of bad customer service interactions*: [https://www.marketingcharts.com/digital-28628](https://www.marketingcharts.com/digital-28628).
    获取日期：2019年5月6日。'
- en: '*Machine Learning on AWS*: [https://aws.amazon.com/machine-learning/](https://aws.amazon.com/machine-learning/).
    Retrieved May 6, 2019.'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Machine Learning on AWS*: [https://aws.amazon.com/machine-learning/](https://aws.amazon.com/machine-learning/).
    获取日期：2019年5月6日。'
- en: '*Azure Machine Learning Service*: [https://azure.microsoft.com/en-gb/services/machine-learning-service/](https://azure.microsoft.com/en-gb/services/machine-learning-service/).
    Retrieved May 6, 2019.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Azure Machine Learning Service*: [https://azure.microsoft.com/en-gb/services/machine-learning-service/](https://azure.microsoft.com/en-gb/services/machine-learning-service/).
    获取日期：2019年5月6日。'
- en: '*Cloud AI*: [https://cloud.google.com/products/ai/](https://cloud.google.com/products/ai/).
    Retrieved May 6, 2019.'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Cloud AI*: [https://cloud.google.com/products/ai/](https://cloud.google.com/products/ai/).
    获取日期：2019年5月6日。'
- en: '*Cloud Market Share Q4 2018 and Full Year 2018*: [https://www.canalys.com/newsroom/cloud-market-share-q4-2018-and-full-year-2018](https://www.canalys.com/newsroom/cloud-market-share-q4-2018-and-full-year-2018).
    Retrieved May 11, 2019.'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Cloud Market Share Q4 2018 and Full Year 2018*: [https://www.canalys.com/newsroom/cloud-market-share-q4-2018-and-full-year-2018](https://www.canalys.com/newsroom/cloud-market-share-q4-2018-and-full-year-2018).
    获取日期：2019年5月11日。'
- en: '*Amazon Sagemaker Example*: [https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb).
    Retrieved May 11, 2019.'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Amazon Sagemaker Example*: [https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb).
    获取日期：2019年5月11日。'
- en: '*Sagemaker SDK for Go*: [https://docs.aws.amazon.com/sdk-for-go/api/service/sagemaker](https://docs.aws.amazon.com/sdk-for-go/api/service/sagemaker)/.
    Retrieved May 11, 2019.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Sagemaker SDK for Go*: [https://docs.aws.amazon.com/sdk-for-go/api/service/sagemaker](https://docs.aws.amazon.com/sdk-for-go/api/service/sagemaker)/.
    获取日期：2019年5月11日。'
- en: '*An overview of Sagemaker*: [https://www.youtube.com/watch?v=ym7NEYEx9x4](https://www.youtube.com/watch?v=ym7NEYEx9x4).
    Retrieved May 11, 2019.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*An overview of Sagemaker*: [https://www.youtube.com/watch?v=ym7NEYEx9x4](https://www.youtube.com/watch?v=ym7NEYEx9x4).
    获取日期：2019年5月11日。'
- en: '*Rekognition Go SDK*: [https://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/](https://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/).
    Retrieved May 11, 2019.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Rekognition Go SDK*: [https://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/](https://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/).
    获取日期：2019年5月11日。'
- en: '*Azure Stream Analytics integration with Azure Machine Learning*: [https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-machine-learning-integration-tutorial](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-machine-learning-integration-tutorial).
    Retrieved May 11, 2019.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Azure 流分析与 Azure 机器学习集成*: [https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-machine-learning-integration-tutorial](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-machine-learning-integration-tutorial).
    获取日期：2019年5月11日。'
- en: '*Azure Go SDK*: [https://github.com/Azure/azure-sdk-for-go](https://github.com/Azure/azure-sdk-for-go).
    Retrieved May 11, 2019.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Azure Go SDK*: [https://github.com/Azure/azure-sdk-for-go](https://github.com/Azure/azure-sdk-for-go).
    获取日期：2019年5月11日。'
- en: '*Consume web service*: [https://docs.microsoft.com/en-us/azure/machine-learning/studio/consume-web-services](https://docs.microsoft.com/en-us/azure/machine-learning/studio/consume-web-services).
    Retrieved May 11, 2019.'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*消费 Web 服务*: [https://docs.microsoft.com/en-us/azure/machine-learning/studio/consume-web-services](https://docs.microsoft.com/en-us/azure/machine-learning/studio/consume-web-services).
    获取日期：2019年5月11日。'
- en: '*Quickstart: Using Go to call the Text Analytics API*. [https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/go](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/go).
    Retrieved May 11, 2019.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*快速入门：使用 Go 调用文本分析 API*. [https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/go](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/go).
    获取日期：2019年5月11日。'
- en: '*Cost comparison of deep learning hardware*: [https://medium.com/bigdatarepublic/cost-comparison-of-deep-learning-hardware-google-tpuv2-vs-nvidia-tesla-v100-3c63fe56c20f](https://medium.com/bigdatarepublic/cost-comparison-of-deep-learning-hardware-google-tpuv2-vs-nvidia-tesla-v100-3c63fe56c20f).
    Retrieved May 11, 2019.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*深度学习硬件成本比较*: [https://medium.com/bigdatarepublic/cost-comparison-of-deep-learning-hardware-google-tpuv2-vs-nvidia-tesla-v100-3c63fe56c20f](https://medium.com/bigdatarepublic/cost-comparison-of-deep-learning-hardware-google-tpuv2-vs-nvidia-tesla-v100-3c63fe56c20f).
    获取日期：2019年5月11日。'
- en: '*Prediction Overview*: [https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview](https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview).
    Retrieved May 11, 2019.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*预测概述*: [https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview](https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview).
    获取日期：2019年5月11日。'
- en: '*Google AI Hub*: [https://cloud.google.com/ai-hub/](https://cloud.google.com/ai-hub/).
    Retrieved May 11, 2019.'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Google AI Hub*: [https://cloud.google.com/ai-hub/](https://cloud.google.com/ai-hub/).
    获取日期：2019年5月11日。'
- en: '*Amazon ECR Managed Policies*: [https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html](https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html).
    Retrieved May 11, 2019.'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Amazon ECR 管理策略*: [https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html](https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html).
    获取日期：2019年5月11日。'
- en: '*App Service - Web App for Containers*: [https://azure.microsoft.com/en-gb/services/app-service/containers/](https://azure.microsoft.com/en-gb/services/app-service/containers/).
    Retrieved May 11, 2019.'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*App Service - 容器 Web 应用*: [https://azure.microsoft.com/en-gb/services/app-service/containers/](https://azure.microsoft.com/en-gb/services/app-service/containers/).
    获取日期：2019年5月11日。'
- en: '*Push Docker Image to Private Registry*: [https://docs.microsoft.com/en-gb/azure/container-registry/container-registry-get-started-docker-cli](https://docs.microsoft.com/en-gb/azure/container-registry/container-registry-get-started-docker-cli).
    Retrieved May 11, 2019.'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*将 Docker 镜像推送到私有注册库*: [https://docs.microsoft.com/en-gb/azure/container-registry/container-registry-get-started-docker-cli](https://docs.microsoft.com/en-gb/azure/container-registry/container-registry-get-started-docker-cli).
    获取日期：2019年5月11日。'
- en: '*Create Docker/Go app on Linux*: [https://docs.microsoft.com/en-gb/azure/app-service/containers/quickstart-docker-go](https://docs.microsoft.com/en-gb/azure/app-service/containers/quickstart-docker-go).
    Retrieved May 11, 2019.'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在 Linux 上创建 Docker/Go 应用*: [https://docs.microsoft.com/en-gb/azure/app-service/containers/quickstart-docker-go](https://docs.microsoft.com/en-gb/azure/app-service/containers/quickstart-docker-go).
    获取日期：2019年5月11日。'
- en: '*Container Registry*: [https://cloud.google.com/container-registry/](https://cloud.google.com/container-registry/).
    Retrieved May 11, 2019.'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*容器注册库*: [https://cloud.google.com/container-registry/](https://cloud.google.com/container-registry/).
    获取日期：2019年5月11日。'
- en: '*Quickstart for Docker*: [https://cloud.google.com/cloud-build/docs/quickstart-docker](https://cloud.google.com/cloud-build/docs/quickstart-docker).
    Retrieved May 11, 2019.'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Docker 快速入门*: [https://cloud.google.com/cloud-build/docs/quickstart-docker](https://cloud.google.com/cloud-build/docs/quickstart-docker).
    获取日期：2019年5月11日。'
- en: '*Mechanical Turk*: [https://www.mturk.com/](https://www.mturk.com/). Retrieved
    May 15, 2019.'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Mechanical Turk*: [https://www.mturk.com/](https://www.mturk.com/). 获取日期：2019年5月15日。'
- en: '*Shrink your Go binaries with this one weird trick*: [https://blog.filippo.io/shrink-your-go-binaries-with-this-one-weird-trick/](https://blog.filippo.io/shrink-your-go-binaries-with-this-one-weird-trick/).
    Retrieved May 16th, 2019.'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用这个奇怪的小技巧缩小你的 Go 可执行文件*：[https://blog.filippo.io/shrink-your-go-binaries-with-this-one-weird-trick/](https://blog.filippo.io/shrink-your-go-binaries-with-this-one-weird-trick/).
    2019年5月16日检索。'
