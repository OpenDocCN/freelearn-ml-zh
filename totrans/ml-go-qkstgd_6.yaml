- en: Deploying Machine Learning Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned how to create an application that can prepare
    data ([Chapter 2](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml), *Setting Up the
    Development Environment*) for either a supervised ([Chapter 3](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml),
    *Supervised Learning*) or unsupervised ([Chapter 4](26788e93-3614-413f-bcde-5580516f9c5f.xhtml),
    *Unsupervised Learning*) ML algorithm. We also learned how to evaluate and test
    the output of these algorithms with the added complication that we have incomplete
    knowledge about the algorithm's inner state and workings, and must therefore treat
    it as a black box. In [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using
    Pre-Trained Models,* we looked at model persistence and how Go applications can
    leverage models written in other languages. Together, the skills you have learned
    so far constitute the fundamentals required to successfully prototype ML applications.
    In this chapter, we will look at how to prepare your prototype for commercial
    readiness, focusing on aspects specific to ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The continuous delivery feedback loop, including how to test, deploy, and monitor
    ML applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment models for ML applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The continuous delivery feedback loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Continuous delivery** (**CD**) is the practice of using short feedback loops
    in the software development life cycle to ensure that the resulting application
    can be released at any moment in time^([1]). While there are alternative approaches
    to release management, we will only consider this one because creating a meaningful,
    short—and therefore automated—feedback loop with ML applications presents unique
    challenges that are not created by alternative methodologies that may not require
    this degree of automation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The CD feedback loop consists of the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4985c73-3a46-4ab8-99de-d63683cbf9f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: The continuous delivery feedback loop'
  prefs: []
  type: TYPE_NORMAL
- en: Developing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The development portion of the feedback loop is what we have covered so far
    in this book. As we argued in [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml),
    *Using Pre-Trained Models*,developing ML models in Go has both advantages and
    disadvantages, and sometimes combining Go with other languages, such as Python,
    to benefit from libraries, such as Keras, can significantly shorten the development
    portion of the cycle. The downside is reduced maintainability and more work to
    test the resulting solution, as it will necessarily contain a Go–Python interface
    (for example).
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because humans are prone to making errors, testing the source code we create
    is a critical element of the development life cycle to guarantee an accurate and
    reliable product. Entire books have been dedicated to the subject, and it seems
    there are as many different approaches to software testing as there are software
    engineers (as an internet search for software-testing methodologies will confirm).
    ML applications, on the surface, are particularly difficult to test because they
    seem like a black box, whose output depends on the training set we provide: we
    feed them data, and they feed us answers, but a slight change of the train–test
    split or the hyperparameters could produce a different output for a given input
    vector. How can we determine whether the answers they provide are erroneous because
    the model''s hyperparameters are incorrect, because the input data is corrupt,
    or because the model''s algorithms are flawed? Or is this particular response
    an outlier buried in a population of otherwise acceptable responses?'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we performed statistical testing of models using the
    validation set to measure the responses of the model to a meaningful sample of
    inputs, comparing them to expected output values when these were available (supervised
    learning). Arguably, this is the only way to test ML models for accuracy or precision
    because retraining them on a different sample of the dataset (or with altered
    hyperparameters) could produce a different output for the same input, but should
    not produce statistically inferior results on a large validation set with regards
    to the same accuracy/precision metrics. In other words, with small changes to
    the model, we could see large changes to the way it responds to one input vector,
    but its response should not be too different when tested against a large enough
    sample of input vectors, such as the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: This has two consequences. First, the way that unit tests are usually constructed,
    where the developer chooses input values and asserts on the output, could break
    down with the slightest change to the model. Therefore, it is best not to rely
    on assertions based on a single response. Rather, it is better to assert using
    an accuracy or precision metric across a larger set, using the techniques we introduced
    in [Chapters 3](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml), *Supervised Learning*,
    and [Chapter 4](26788e93-3614-413f-bcde-5580516f9c5f.xhtml), *Unsupervised Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, there may be edge cases, where we wish to guarantee the behavior of
    a model, or certain responses that we wish to guarantee will never occur (not
    even as outlying behavior). If we cannot be sure that a black box model can achieve
    this, combining an ML algorithm with traditional logic is the only way to ensure
    that the constraints are met. For example, consider Google''s recent ban of "gorilla"
    as a search term on Google Images in an effort to prevent some accidentally racist
    results from appearing^([2]). Performing statistical testing of the image classifier
    with gorilla images would have been difficult and would only have covered this
    one edge case; however, knowing what an unacceptable response was and adding constraining
    logic to prevent this edge case was a trivial, if embarrassing, affair. As with
    this example, traditional unit tests can be combined with statistical testing,
    with the traditional unit tests asserting on the output of the constraints while
    the statistical tests assert on the model output directly. An holistic strategy
    for ML testing thus emerges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define accuracy/precision goals for the model**: This may not be as simple
    as coming up with a single accuracy score, as reducing false positives or false
    negatives may take precedence. For example, a classifier that aims to determine
    whether a mortgage applicant should get a loan may be required to err on the side
    of caution, with more false negatives tolerated than false positives, depending
    on the risk profile of the lender.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define edge case behavior and codify this into unit tests**: This may require
    traditional logic to restrict the output of the ML model to ensure that these
    constraints are met and traditional unit tests to assert on the constrained output
    of the ML model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the ML application has been developed and you have tested it to satisfy
    yourself that it works as intended, the next step in the CD life cycle is to deploy
    the software—that is, take steps to ensure that users are able to access it. There
    are different deployment models, depending on factors such as whether you are
    intending to run the application on your own hardware or whether you intend to
    use an **infrastructure-as-a-service** (**IaaS**) or **platform-as-a-service**
    (**PaaS**) cloud, and we will touch upon these differences in the next section.
    Here, we will assume that you are either running the application on your own servers
    or using a virtual infrastructure supplied by an IaaS provider.
  prefs: []
  type: TYPE_NORMAL
- en: 'ML applications can present unique challenges in deployment that are absent
    from simpler software, such as an HTTP server that connects to a database:'
  prefs: []
  type: TYPE_NORMAL
- en: Dependency on scientific libraries that require LAPACK or BLAS entails complex
    installation processes with many steps, and chances for mistakes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependency on deep-learning libraries, such as TensorFlow, entails dynamic linking
    to C libraries, again leading to a complex installation process, with many OS
    and architecture-specific steps, and chances for mistakes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning models may need to run on specialized hardware (for example, servers
    with GPUs), even for testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where should ML models be persisted? Should they be committed as though they
    were source code? If so, how can we be sure we are deploying the correct version?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will present solutions to these challenges and a sample application
    that embodies these solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anyone who has tried to build TensorFlow or NumPy from source will sympathize
    with the saying that *anything that can go wrong, will go wrong*. A search on
    Google, Stack Overflow, or their respective GitHub issue pages will reveal many
    obscure potential issues with the build process^([3][4][5]). These are not isolated
    finding in the sense that the scientific computing libraries that ML applications
    rely on tend to be highly complex and depend on a convoluted set of other libraries
    that are also highly complex. An academic ML researcher might have need to build
    dependencies from source to benefit from a certain optimization, or perhaps because
    they need to modify them. On the contrary, an ML application developer must try
    to avoid this process and instead use prebuilt images available as Python wheels^([6]),
    prebuilt packages for their chosen package manager (such as apt on Ubuntu Linux
    or Chocolatey^([7]) on Windows), or Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will focus on Docker as a solution for developing and packaging Go ML applications
    for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Portability across a wide range of operating systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Excellent support from major cloud vendors, such as Microsoft Azure^([8]) and
    Amazon Web Services^([9])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for Docker integration in popular provisioning and infrastructure configuration
    using tools such as Terraform^([10]), Chef^([11]), and Ansible^([12]).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Availability of ML libraries through prebuilt Docker images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go's particular suitability for Docker, as it can always be configured to produce
    static binaries, allowing us to greatly reduce the production Docker image size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have reduced the size of the Docker image as much as possible (maybe
    by using the `scratch` image), but the size of the Go binary makes the overall
    image still too large for you, consider using the `strip` command or a packer
    like `upx`.
  prefs: []
  type: TYPE_NORMAL
- en: In all the examples we have looked at so far, we have created a single Docker
    image that contains all the dependencies for our application, as well as the application
    files, usually added to the container using the `ADD` or `COPY` command in the
    Dockerfile. While this has the advantage of simplicity (there is only one Dockerfile
    for development and production), it also means that we will need to push or pull
    an oversized Docker image with all the dependencies for developing an application.
  prefs: []
  type: TYPE_NORMAL
- en: However, the dependencies are probably not required to run it because Go can
    always be configured to produce static binaries that run on stripped-down Docker
    images. This means slower deployment times and slower testing times, as intermediate
    Docker images may not be cached in the CI environment, not to mention that a smaller
    container tends to use less disk and memory on its host server. Smaller images
    also have the benefit of added security from reducing the attack surface, as they
    will contain far fewer dependencies that an attacker could exploit. The `scratch`
    image, for example, does not even contain a shell, making it very hard for an
    attacker to compromise, even if the application running in the container is itself
    compromised.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process we advocate is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c230b82-c158-4b8c-bad6-51a6406f9879.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 2: Deployment using two separate Docker images (one for development and
    one for testing/production)'
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we assume that you already have a development environment,
    where all your dependencies live (which could be Docker based, or not—it does
    not matter). You have developed your ML application, which consists of a `main` package
    and some saved model weights, `model.data`, and would like to create a production-ready
    container. To create this container, we need to do two things.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to compile the Go application to a static binary. If you are
    not using CGO and linking to some C libraries (such as the TensorFlow C library),
    then using `go build` without any additional flags will suffice. However, if your
    application depends on, say, the TensorFlow C library, then you need to add some
    additional command-line arguments to ensure that the resulting binary is static—that
    is, that it includes all the dependent code. At the time of writing, there is
    a proposal for Go 1.13 to have a `-static` flag for the `build` command that will
    achieve this with no further work. Until then, there is an excellent blog post
    by Diogok that explains the different flags in the following command, and how
    to tweak them if it does not work in your particular case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce a single output binary `mlapp` with all the required dependencies.
    The purpose of using all these flags is to produce a static binary that contains
    all our dependencies so that we only have the simple task of adding them to a
    "vanilla" Docker image, giving us the Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: That's it! There is nothing else to add, unlike the long Dockerfiles we previously
    used because we needed all the dependencies. In this case, we already have these
    dependencies inside our Go binary. This is another advantage of Go; unlike some
    other programming languages, Go makes this type of deployment possible.
  prefs: []
  type: TYPE_NORMAL
- en: You can also expose a port using your Dockerfile (for example, if you intend
    to serve your app from an HTTP server) by using the `EXPOSE` command. To expose
    an HTTP server listening on port 80, use the, `EXPOSE 80/tcp` command.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we assumed that our model file containing the trained
    model weights/hyperparameters was persisted to disk and saved alongside our binary,
    ready to be added to the Docker container; however, there are cases where this
    may be impractical or undesirable.
  prefs: []
  type: TYPE_NORMAL
- en: Model persistence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the time, you can follow the aforementioned pattern of committing your
    model file alongside the source code and adding it to a Docker image during deployment
    together with your binary; however, there are times when you may want to reconsider
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: The model file is very large, so it leads to a very large Docker image and slows
    down deployments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of model files you have is dynamic and each model is associated with
    an object of your application—that is, you train one model per user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is retrained much more frequently than the code is likely to change,
    leading to very frequent deployments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In these cases, you may want to make the model available from a different source
    and not commit it to source control. At a basic level, model files are just a
    sequence of bytes, so there is no real limit to where they can be stored: on a
    file server elsewhere, cloud file storage, or a database.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The exception is the second case: where you have a dynamic number of model
    files that are associated with application objects, such as users. For example,
    if you are building a system that aims to forecast how much electricity a household
    will consume the following day, you might end up having one model for all households
    or one model per household. In the latter case, you would be better served using
    a database to hold these model files:'
  prefs: []
  type: TYPE_NORMAL
- en: The model files could be seen to contain sensitive data that is probably best
    secured and governed in a database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large number of model files could benefit from advanced compression techniques
    that are leveraged by database software, such as using page-level compression
    instead of row-level compression. This can reduce their overall size on the disk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may be easier to keep data associated with application objects all in the
    same place to limit the number of queries required with authorize an operation
    related to a model, for example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these reasons, among others, we recommend saving the model to a database
    in the event that your application requires many models, each associated to an
    application object, such as a user.
  prefs: []
  type: TYPE_NORMAL
- en: This poses a small challenge, because some Go ML libraries, such as GoML, expose
    persistence functions, such as `PersistToFile` of the `linear` package models,
    and these functions persist the model to a file; however, they do not directly
    offer access to serialized model should we want to persist it elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two techniques we can apply:'
  prefs: []
  type: TYPE_NORMAL
- en: Look through the Godocs to see if the model struct has any unexported fields.
    If not, we can simply use `encoding/json` to serialize the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are unexported fields, we can save the model to a temporary file, read
    the temporary file into memory, and delete it again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Go, an **unexported field** is a struct field with a lowercase name, which
    is not accessible outside the package in which it is defined. Such fields are
    absent from serialization using `encoding/json`.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of GoML's `LeastSquares` model, there are no unexported fields,
    and a cursory examination of the `PersistToFile` method would reveals that it
    is using encoding/JSON to marshal the model to a byte slice. Therefore, we can
    just use `serializedModel, err := json.Marshal(leastSquaresModel)` to serialize
    it. The resulting `serializedModel` can then be saved anywhere we wish.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if, for argument''s sake, we could not do this because the model struct
    had unexported fields? For example, the golearn library''s `linear_models` package
    has an `Export` method that persists models to the file, but this relies on a
    call to a C function, and the model has unexported fields. In this case, we have
    no choice but to first persist the model to a temporary file and then recover
    the file contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: All we are doing in the preceding code is providing a temporary location to
    store the model file on disk and then moving it back to memory. While this is
    not the most performant way to store a model, it is necessary because of the limitations
    on some of the interfaces for some Go ML libraries, and there is already an open
    issue on GoLearn's GitHub page to improve this.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the application is deployed, we want some certainty that it is functioning
    correctly, using up an appropriate amount of resources, and that there is no underlying
    issue that could prevent it from being available. In the next subsection, we will
    look at monitoring techniques specific to ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In his book, *Architecting for Scale*, Lee Atchison, Principal Cloud Architect
    at New Relic, argues for the use of a risk matrix, also known as a **risk register**,
    to keep track of what is likely to go wrong with an application and how it should
    be mitigated^([16]). While this may seem like overkill for a simple application,
    it is a great tool for managing risk in a complex environment, especially where
    ML models are involved. This is because the entire team can be aware of the main
    risks, their likelihoods, and mitigation, even if they did not have a hand in
    creating every part of the application in the first place. ML models can sometimes
    be created by a data scientist and then later handed over to a software development
    team via one of the polyglot integration approaches we outlined in [Chapter 5](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml),
    *Supervised Learning*, so this makes knowing any risk associated with their use
    in production all the more important.
  prefs: []
  type: TYPE_NORMAL
- en: While this may seem like a rather opinionated approach, remember that the goal
    is simply to make developers think about what can cause their application to become
    unavailable. There is no obligation to write down a risk register or run your
    team using one (although both could be beneficial), and the practice of thinking
    about risk always helps by shining light on dark recesses, where no one had thought
    to look for that elusive Friday night bug that took the whole application offline
    until Monday morning.
  prefs: []
  type: TYPE_NORMAL
- en: A **risk** associated with a production application is different from a failure
    of a test, which you would hopefully have caught before deploying it to production
    in the first place. It is the risk that something you assumed constant in testing
    (such as available memory or a training algorithm converging) has changed to a
    critical state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Risks associated with ML applications could include, but are not limited to,
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Running out of memory to run more instances of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model file becoming corrupt, leading to the model being unavailable to be
    run, even though the rest of the application might still be available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A nonconvergent training procedure, if model retraining is done in production,
    leading to a useless model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malicious users crafting input to try to trick the model into producing a desired
    output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malicious users crafting badly formatted input (fuzzing) to crash the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upstream services, such as databases used to store ML models, being unavailable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cloud datacentre, where the model runs runs low on GPU availability, meaning
    that an autoscale feature fails and availability of your deep learning model is
    reduced as a result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The list is obviously not exhaustive, but hopefully it gives you an idea of
    the kind of issues that could arise so you can look for them in your own applications.
    Because it is very difficult to come up with an exhaustive list, general monitoring
    principles apply:'
  prefs: []
  type: TYPE_NORMAL
- en: Use structured logging in the application wherever possible and centralize these
    logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If retraining in production, make sure that you set up alerts for any error
    in the training procedure, since this will necessarily lead to a useless model
    (or falling back to a deprecated one)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capture metrics whose significant change could be used to detect any risks in
    your register materializing (for example, availability of memory space)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go was designed partly to serve web applications^([17]), so there are many third-party
    packages that can help you perform these tasks, and we will now explore some of
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Structured logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many logging libraries for Go, such as the standard library's `log`
    package^([18][19][20]). A significant advantage to using a structured logging
    library—which logs to a standardized format, such as JSON—over unstructured logging
    that simply uses free text is that it is far easier to work with the log data
    once it has been created. Not only is searching by a particular field easier (using,
    say, `jq`^([21]) to work with JSON data), but structured logs allow far richer
    integration with existing monitoring and analytics tooling, such as Splunk^([22])
    or Datadog^([23]).
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will use the Logrus package to log an error message
    returned by a training procedure. Note that the use of this particular logging
    package is a personal choice, and any other structured logging package would also
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we configure the logger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output format can be configured by using the properties of the `JSONFormatter`
    struct^([24]):'
  prefs: []
  type: TYPE_NORMAL
- en: '`TimestampFormat`: The format of the timestamps using a time-compatible format
    string (for example, `Mon Jan 2 15:04:05 -0700 MST 2006`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DisableTimestamp`: Removes the timestamp from the output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataKey`: Instead of a flat JSON output, this puts all the log entry parameters
    into a map at the given key'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FieldMap`: Use this to rename the default output properties, such as the timestamp'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CallerPrettyfier`: When `ReportCaller` is activated (as shown in the preceding
    code snippet), this function can be called to customize the output—for example,
    stripping the package name from the caller''s method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PrettyPrint`: This determines whether to indent JSON output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example, where we use it in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: While this may produce more output than necessary, because of the addition of
    the two info-level messages, we can filter out this level of output if it is not
    required by using `logrus.SetLevel`; however, in the case of retraining in production,
    the training time is important (as is making sure that the training process completes),
    so it is never a bad idea to have records of the process in the log, even if it
    becomes more verbose as a result.
  prefs: []
  type: TYPE_NORMAL
- en: When logging ML-related information, it is a good idea to have a field with
    the model name (which may be something meaningful to a data scientist, if they
    created it). When you have multiple models running concurrently in production,
    it is sometimes hard to tell which one has produced the error!
  prefs: []
  type: TYPE_NORMAL
- en: The time taken to train an algorithm is one metric that we would recommend computing
    regularly and sending to a dedicated metrics system. We will discuss capturing
    metrics in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding example, we inserted info-level messages in the logs to signify
    the start and end of the training process. While we could look at the timestamp
    fields of both messages and compare them to determine how long the training process
    took (Splunk, for example, is able to do this with the right query), a more direct
    and less cumbersome way to achieve the same result is to monitor this specific
    datapoint, or metric, explicitly. We could then raise alerts if the training process
    becomes too long or have a chart that logs and displays the time taken by the
    regular model training processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two approaches that we can use:'
  prefs: []
  type: TYPE_NORMAL
- en: Store the metric as an additional field on the log entry with a `float64` value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store the metric in a separate analytics system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, the approach you take depends on your current analytics systems,
    team preferences, and application size. As far as ML applications go, either approach
    works equally well, so we will assume the first one, as it reduces the amount
    of third-party application code required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reusing the same example as earlier, let''s set this up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that we did not include any of the logging calls in the timed block. This
    is because we want to measure the time taken by the training process rather than
    any logging around it.
  prefs: []
  type: TYPE_NORMAL
- en: If your company uses an analytics system, such as Grafana or InfluxDB, you can
    still use the same approach as previously described—just make sure that you create
    a sensible name for your metric, including the name of the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: In the final subsection the CD feedback loop, we will consider how accuracy/precision
    metrics can help create a feedback loop in an ML application.
  prefs: []
  type: TYPE_NORMAL
- en: Feedback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of acquiring feedback in any system is intended to improve the system.
    In the case of an ML application, feedback can help make the application more
    robust with regards to risks on its register (or the addition of new risks that
    were previously unmitigated), but this is not specific to ML applications; all
    production applications benefit from a feedback cycle. There is, however, one
    special feedback cycle that is particular to ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: An ML model is used on the basis that it satisfies some accuracy/precision criteria
    that make it better or more generic at extracting meaning from data than a naive
    heuristic. In [Chapter 3](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml), *Supervised
    Learning*, and [Chapter 4](26788e93-3614-413f-bcde-5580516f9c5f.xhtml), *Unsupervised
    Learning*, we outlined some of these metrics, such as the mean square error of
    a regression of house prices or the test/validation accuracy of binary classifiers
    on images of clothes. In our CD cycle so far, we have assumed that once a model
    is created, its accuracy will never change with regards to new input; however,
    this is rarely a realistic assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Consider our MNIST fashion classifier from [Chapter 3](48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml),
    *Supervised Learning*, which aims to determine whether an image represents a pair
    of trousers or not. At the moment, this database does not contain any images of
    flared trousers. What if these come back into fashion and all the images our model
    begins to receive are of flared trousers? We may notice users complaining that
    images are not being correctly classified. Such considerations have led to numerous
    websites that rely on ML models adding "Rate my prediction" models to their websites
    in a bid to ensure that models are still outputting relevant predictions.
  prefs: []
  type: TYPE_NORMAL
- en: This is, of course, a valid approach, albeit one that relies on the customer
    to tell you when your product is and is not working. Because customers are more
    likely to use these feedback features during an unsatisfactory experience^([26]),
    any data you gather from this exercise, while still useful, is likely biased toward
    the negative and therefore cannot automatically be used as a proxy accuracy metric.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where the customer supplies images and your model classifies them,
    this may still be your best option, unless you can write a scraper for new trouser
    images that continuously feeds them to a model and measures its response. That
    would be labor-intensive, but would clearly produce better results, assuming,
    of course, that the types of trousers found by your scraper were representative
    of the types of trouser images supplied by your customers. In other cases, some
    automated feedback loops may be possible, where you are able to directly monitor
    the accuracy of a model, either in testing or production, and use this to make
    a decision on when the model should be retrained.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a different scenario, one where you are asked to forecast the next
    day's individual electricity consumption of a large number of households, given
    data points such as the number of occupants and a forecast temperature curve.
    You decide that you will use, say, one regression per household and store the
    regression parameters in a database once the model is trained. Then, every day,
    you will run every model in your database to generate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: A very easy feedback cycle exists in this case because, every day, you can also
    measure the actual electricity consumption of the household and compare this to
    your model's prediction. A scheduled script could then compare the relative difference
    between the two over a certain period, perhaps using a moving average to smooth
    out any anomalies, and should this difference be greater than a certain predefined
    threshold, it would then be entitled to assume that some of the model's input
    data had changed and the model required retraining on a new dataset. An alternative
    would be to retrain that the model if any of its input parameters changed, although
    that could lead to a lot of unnecessary retraining and thus additional cost, as
    forecast temperature curves likely change daily, so every model would likely need
    to be re-trained every day.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feedback loop for ML applications with continuous validation and retraining
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3292a8f0-4375-4ca5-83d3-ea81d2f255ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: The feedback loop for ML applications with continuous validation'
  prefs: []
  type: TYPE_NORMAL
- en: The feedback loop cannot be applied to every ML application, but with a little
    creativity, you can usually find a way in which to find input samples that were
    not in either the training or testing dataset, but are of updated relevance. If
    you can automate the process of generating predictions from these samples and
    storing their difference to a ground truth, then you can still generate the same
    feedback loop.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment models for ML applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding example, we explained how to deploy an ML application using
    Docker to encompass it and its dependencies. We deliberately stayed away from
    any discussion pertaining to the infrastructure that was going to run these containers
    or any Platform-as-a-Service offerings that could facilitate the development or
    deployment itself. In the current section, we consider different deployment models
    for ML applications under the assumption that the application will be deployed
    to a cloud platform that supports both IAAS and platform-as-a-service models,
    such as Microsoft Azure and Amazon Web Services.
  prefs: []
  type: TYPE_NORMAL
- en: This section is specifically written to help you decide what virtual infrastructure
    to use if you are deploying an ML application to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main deployment models for any cloud application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Infrastructure-as-a-service**: This is the cloud service that offers a high-level
    interaction with virtualized hardware, such as virtual machines, without the customer
    needing to maintain the hardware or the virtualization layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Platform-as-a-service**: This is a cloud service that offers Software-as-a-Service
    components that you can then build your application from, such as a serverless
    execution environment (for example, AWS Lambda).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will consider both options and how to make best use of them for ML applications.
    We will compare and contrast the three main vendors by market share, as of Q4
    2018: Amazon Web Services, Microsoft Azure, and Google Cloud^([30]).'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure-as-a-service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this chapter, we explained how to package an ML application using
    Docker. In this subsection, we will look at simple ways to deploy an ML application
    using Docker to AWS, Azure, or Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each case, we will start by explaining how to push one of your local Docker
    images to a **registry** (that is, a machine that will store images and serve
    them to the rest of your infrastructure). There are several advantages to using
    a Docker registry to store your images:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Faster deployments and build times**: Virtual infrastructure components requiring
    images can just pull them from the registry instead of building them from scratch
    every time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ease of implementing autoscale in your application**: If you have to wait
    for a long Docker build—say, 20 minutes, for TensorFlow—every time you need to
    scale your service up, you may experience degradation or unavailability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Pulling images from a single trusted source reduces the attack
    surface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Web Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The core of AWS''s virtualized IaaS offering is **Elastic Compute** (**EC2**).
    AWS also offers **Elastic Container Registry** (**ECR**) as a registry service
    to serve images from. To set this up, go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Before you can push or pull an image to an ECR registry, you need `ecr:GetAuthorizationToken`
    permissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tag your image, assuming its ID is `f8``ab2d331c34`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Push the image to the ECR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The image is now available to use from an EC2 instance. First, SSH into your
    instance where you have installed Docker, following the instructions in [Chapter
    5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using Pre-Trained Models,* and
    then run the following commands to install Docker and start a container from the
    image (amend the `docker run` command to add exposed ports or volumes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Microsoft Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to Amazon''s ECR, which we discussed in the previous subsection, Microsoft
    Azure offers a registry, Azure Container Registry. We can use this by following
    the same steps as AWS ECR, but there is a difference, namely the requirement to
    log in via the Docker command-line interface. Once this is done, you can follow
    the same instructions as the previous subsection, but with your registry and image
    details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Microsoft also allows Docker as a deployment method for App Service Apps, a
    managed web app service based on Microsoft''s **Internet Information Services**
    (**IIS**). If you have followed the preceding steps to deploy your Docker image
    to a registry, you can use the `az` command-line tool to create a web app from
    your image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like Amazon and Microsoft, Google also offers a registry, called Container
    Registry, which can be used as a Docker registry. The steps to use it are the
    same as for Amazon ECR, except for the addition of a preliminary authentication
    step using the `gcloud` command-line tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can push the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The steps to run a Docker container on a Google Cloud VM are the same as for
    an EC2 VM, with the addition of the authentication step.
  prefs: []
  type: TYPE_NORMAL
- en: Platform-as-a-Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the rising popularity of ML components in applications, cloud vendors have
    scrambled to provide platform-as-a-service offerings that make it easier to deploy
    ML applications in an effort to win over customers. It is worth a brief review
    of each of the three main cloud vendors by market share as of 2018^([30]). This
    is not an attempt to recommend one vendor over another, but rather an attempt
    to explore solutions while remaining agnostic to any decisions regarding cloud
    vendors that you may have already made. In other words, the deployment models
    we will discuss will work in all three clouds—and probably others—but some platforms
    offer specific services that may better suit certain applications or reduce their
    development effort.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud vendors make such frequent changes to their offerings that it is possible
    that by the time you are reading this, there will be newer, better services than
    the ones described here. Look in the *Further reading* section for some links
    to Google Cloud, AWS, and Azure ML services^([27][28][29]).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Web Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Web Services** (**AWS**) has two main types of service offerings regarding
    ML space:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Sagemaker**: A hosted environment to run ML notebooks and SDK to efficiently
    perform various ML-related tasks, including data labeling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS AI Services**: A set of pretrained models for specific tasks, such as
    image recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Sagemaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Sagemaker uses Jupyter as a development environment for ML models, as
    we have done throughout the book. The environment in which these Jupyter notebooks
    run comes with some Python ML libraries. For Python developers, this service can
    be thought of as another environment to run ML code with some features to accelerate
    large-scale learning through AWS resources. An example using Sagemaker to perform
    hyperparameter tuning on a natural language processing task can be found on the
    AWS GitHub^([31]), and for a longer introduction there are some exploratory videos
    available on YouTube^([33]). Unfortunately, at this time, there is no way to use
    Sagemaker with a Go kernel for Jupyter (such as gophernotes), so it is not a pure-Go
    solution for interactively developing ML applications in a remote environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Go developers who need to interact with an existing Sagemaker solution,
    there is an SDK that has much of the same features as the Python SDK^([32]), so
    it is possible to use gophernotes locally to create Sagemaker tasks. In fact,
    the SDK is so powerful that it allows Go developers to access a useful data preprocessing
    service: the Sagemaker Labeling Job service. This service integrates with Mechanical
    Turk to provide ground truth labels for training data where they are either missing
    entirely or from part of the dataset. This saves a lot of time compared to manually
    setting up Mechanical Turk jobs. The function that exposes this functionality
    is `CreateLabelingJob`.'
  prefs: []
  type: TYPE_NORMAL
- en: If you need to use a supervised learning algorithm, but have only an unlabeled
    dataset, consider using Sagemaker's interface to Mechanical Turk to label your
    dataset cheaply. Alternatively, you can create a labeling task through the Mechanical
    Turk UI at [https://www.mturk.com/](https://www.mturk.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon AI Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If there is already a model exposed that solves your ML problem, then there
    is no need for you to reinvent the wheel and train a new model, especially considering
    the large resources that AWS will have invested in ensuring the accuracy and efficiency
    of its models. At the time of writing, the following types of algorithms are available
    on a pay-for-usage basis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Personalize**: Built on the same recommendation techniques used by
    Amazon in their online retail store, these allow you to solve problems, such as
    showing customers items similar to those they have already bought'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Forecast**: Timeseries forecasting models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Rekognition**: Image and video analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Comprehend**: Natural language processing tasks and text analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Textract**: Large-scale document analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Polly**: Text-to-speech'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Lex**: Build chatbots in a UI environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Translate**: Automated translation to and from a multitude of languages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Transcribe**: Speech-to-text service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While none of these services are Go specific, they all offer Go SDKs that you
    can use to interact with them. This is very similar to the example we saw in [Chapter
    5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using Pre-Trained Models*, where
    a model was exposed over HTTP and we used this protocol to send it data and receive
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, the methods are synchronous—that is, you will get the result in
    the output argument, and do not need to make a further request later. They also
    have the same type of signature, where the name of the prediction method may vary,
    and the structure of the input/output will also vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'By way of example, consider Rekognition, which, like the other services, has
    a Go SDK^([34]). Suppose that we wish to detect faces in an image. For this, we
    use the `DetectFaces` func; this has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The input, in this case, contains, among other things, an array of facial attributes
    that we wish to be returned, as well as an image, either as base-64 encoded bytes
    or an S3 object. The output will contain a `FaceDetail` struct, which, among other
    things, will describe an age range for each face, whether it is bearded, a confidence
    in its bounding box, any detected emotions, whether they are wearing glasses,
    and so on. This depends on which facial attributes we requested in the input,
    and necessarily, the more attributes we requested, the more expensive the request
    will be (as Amazon will need to run more models to give us the answer).
  prefs: []
  type: TYPE_NORMAL
- en: Generally, if it is possible to build your ML application by composing prebuilt
    models exposed over SDKs, such as AWS, then you will save a lot of time, and it
    will allow you to focus on adding value specific to your business; however, there
    are risks associated with vendor lock-in, and at the time of writing, no other
    cloud platform offers a feature-for-feature alternative to Amazon AI services.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Azure''s main offerings geared at ML applications are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure ML Studio**: A UI environment to build ML pipelines and train models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Cognitive Services**: Pretrained models exposed over HTTP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure ML Studio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure ML Studio is a cloud-based IDE for ML. It allows users to import data
    from other Azure services (such as Blob Storage), transform the data, and use
    it to train one of the included ML algorithms. The resulting model can then be
    exposed via HTTP or composed with other Azure services, such as Azure Stream Analytics
    for a real-time ML application^([35]).
  prefs: []
  type: TYPE_NORMAL
- en: While it is possible to run custom Python code within the Azure ML Studio UI,
    at the time of writing, this does not extend to Go; however, because it is possible
    to expose models via HTTP, you can integrate with an existing Azure ML Studio
    model by following the same pattern that we discussed in [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml),
    *Using Pretrained Models*, where the `net/http` client is used to make requests.
    It is worth using the Azure SDK just to generate authentication tokens rather
    than trying to implement this yourself, as the procedure can be error prone^([36]).
    The JSON structure of the request and response are very simple compared to AWS,
    so the resulting code can be clean and easy to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Cognitive Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Azure Cognitive Services exposes several pretrained ML models over HTTP:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computer Vision**: Image recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech**: Speech recognition and transcription'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LUIS**: Textual intent analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bing Image Search**: Retrieves images matching a text string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bing Web Search**: Retrieves URLs matching a text string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text Analytics**: Sentiment analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing, there is no Go SDK to interact with Cognitive Services,
    but it is possible to invoke the models by using the REST API, and Microsoft provides
    an example of this in a Quickstart article^([37]).
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Google Cloud currently has two main services to offer ML application developers,
    in addition to the free Google Colaboratory^([29]):'
  prefs: []
  type: TYPE_NORMAL
- en: '**AI Platform**: Hosted development environment using Notebooks, VM images,
    or Kubernetes images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI Hub**: Hosted repository of plug-and-play AI components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI Building Blocks**: Pretrained models, exposed via SDK or HTTP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because AI Hub is targeted only at Python developers and its deployment model
    is the same as AI Platform, we will not discuss it any further.
  prefs: []
  type: TYPE_NORMAL
- en: AI Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google's AI Hub is a code-based environment aimed at facilitating all aspects
    of the ML application development life cycle, from data ingestion to deployment,
    via AI Platform Prediction (applicable to TensorFlow models exported as a `SavedModel`,
    as in our [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using Pretrained
    Models*, example) or Kubernetes. It has loose integrations with other Google Cloud
    Services, but remains, at its core, a hosted notebook environment.
  prefs: []
  type: TYPE_NORMAL
- en: Because there is no high-level API to create TensorFlow graphs in Go, analogous
    to Keras in Python, it is unlikely that a Go developer will find the end-to-end
    platform useful. However, if you are interacting with a TensorFlow model, using
    AI Platform Prediction to manage the resources for the model and calling it via
    HTTP^([40]) is an excellent strategy, particularly as the model can be made to
    run on VMs with a Tensor Processing Unit, which can be a significantly cheaper
    way to run TensorFlow workflows^([39]).
  prefs: []
  type: TYPE_NORMAL
- en: AI Building Blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Google''s AI Building Blocks are a suite of pretrained models, exposed via
    HTTP or through one of Google Cloud''s SDKs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sight**: Includes Vision, for image recognition, and Video, for content discovery'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language**: Comprises translation and natural language processing functionality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversation**: Consists of a speech-to-text model, a text-to-speech model,
    and a chatbox builder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured data**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendations AI**: Recommendation engine'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AutoML Tables**: UI to generate predictive models'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud Inference AI**: Time series inference and correlations tool'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Go SDK is very easy to use, as the following example shows. The example
    uses the text-to-speech API to download a recording of the phrase `hello, world`,
    as spoken by the ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As with other models-over-HTTP type services, if you can build your application
    by composing these premade models, then you can dedicate your time to work on
    value-adding business logic; however, always consider the downsides of vendor
    lock-in.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to take a prototype ML application to production.
    Along the way, we explored concerns that a software developer or DevOps engineer
    would typically think of, but from an ML application developers point of view.
    Specifically, we learned how to apply a continuous development life cycle to an
    ML application and the different ways to deploy ML applications in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we will take a step back and look at ML development
    from a project management point of view.
  prefs: []
  type: TYPE_NORMAL
- en: Further readings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Continuous Software Engineering and Beyond: Trends and Challenges Brian Fitzgerald*,
    1st International Workshop on Rapid Continuous Software Engineering. New York,
    NY: Association for Computing Machinery, pp. 1–9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Google''s solution to accidental algorithmic racism*: ban gorillas: [https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people](https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people).
    Retrieved May 3, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Building Numpy* from source: [http://robpatro.com/blog/?p=47](http://robpatro.com/blog/?p=47).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Python—Compiling Numpy with OpenBLAS integration*: [https://stackoverflow.com/questions/11443302/compiling-numpy-with-openblas-integration](https://stackoverflow.com/questions/11443302/compiling-numpy-with-openblas-integration).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Issues—TensorFlow*: [https://github.com/tensorflow/tensorflow/issues](https://github.com/tensorflow/tensorflow/issues).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Python Wheels*: [https://pythonwheels.com/](https://pythonwheels.com/). Retrieved
    May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Chocolateay—The Package Manager for Windows*: [https://chocolatey.org/](https://chocolatey.org/).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Docker Deployment on Azure*: [https://azure.microsoft.com/en-gb/services/kubernetes-service/docker/](https://azure.microsoft.com/en-gb/services/kubernetes-service/docker/).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*What is Docker? | AWS*: [https://aws.amazon.com/docker/](https://aws.amazon.com/docker/).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Docker Provider for Terraform*: [https://www.terraform.io/docs/providers/docker/r/container.html](https://www.terraform.io/docs/providers/docker/r/container.html).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Chef Cookbook for Docker*: [https://github.com/chef-cookbooks/docker](https://github.com/chef-cookbooks/docker).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Docker—manage Docker containers*[: https://docs.ansible.com/ansible/2.6/modules/docker_module.html](https://docs.ansible.com/ansible/2.6/modules/docker_module.html).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'cmd/go: build: add static flag: [https://github.com/golang/go/issues/26492](https://github.com/golang/go/issues/26492).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*On Golang static binaries, cross-compiling, and plugins*: [https://medium.com/@diogok/on-golang-static-binaries-cross-compiling-and-plugins-1aed33499671](https://medium.com/@diogok/on-golang-static-binaries-cross-compiling-and-plugins-1aed33499671).
    Retrieved May 5, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Saving model outside filesystem*: [https://github.com/sjwhitworth/golearn/issues/220](https://github.com/sjwhitworth/golearn/issues/220).
    Retrieved May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Architecting for Scale*, Lee Atchison, 2016, O''Reilly Press.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Server-side I/O: Node.js vs PHP vs Java vs Go*: [https://www.toptal.com/back-end/server-side-io-performance-node-php-java-go](https://www.toptal.com/back-end/server-side-io-performance-node-php-java-go).
    Retrieved May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Zap*: [https://github.com/uber-go/zap](https://github.com/uber-go/zap). Retrieved
    May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Logrus*: [https://github.com/sirupsen/logrus](https://github.com/sirupsen/logrus).
    Retrieved May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Log*: [https://github.com/apex/log](https://github.com/apex/log). Retrieved
    May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*jq*: [https://stedolan.github.io/jq/](https://stedolan.github.io/jq/). Retrieved
    May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Splunk*: [https://www.splunk.com/](https://www.splunk.com/). Retrieved May
    6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Datadog*: [https://www.datadoghq.com/](https://www.datadoghq.com/). Retrieved
    May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*logrus—GoDoc*: [https://godoc.org/github.com/sirupsen/logrus#JSONFormatter](https://godoc.org/github.com/sirupsen/logrus#JSONFormatter).
    Retrieved May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Grafana*: [https://grafana.com/](https://grafana.com/). Retrieved May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Bias of bad customer service interactions*: [https://www.marketingcharts.com/digital-28628](https://www.marketingcharts.com/digital-28628).
    Retrieved May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Machine Learning on AWS*: [https://aws.amazon.com/machine-learning/](https://aws.amazon.com/machine-learning/).
    Retrieved May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Azure Machine Learning Service*: [https://azure.microsoft.com/en-gb/services/machine-learning-service/](https://azure.microsoft.com/en-gb/services/machine-learning-service/).
    Retrieved May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Cloud AI*: [https://cloud.google.com/products/ai/](https://cloud.google.com/products/ai/).
    Retrieved May 6, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Cloud Market Share Q4 2018 and Full Year 2018*: [https://www.canalys.com/newsroom/cloud-market-share-q4-2018-and-full-year-2018](https://www.canalys.com/newsroom/cloud-market-share-q4-2018-and-full-year-2018).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Amazon Sagemaker Example*: [https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Sagemaker SDK for Go*: [https://docs.aws.amazon.com/sdk-for-go/api/service/sagemaker](https://docs.aws.amazon.com/sdk-for-go/api/service/sagemaker)/.
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*An overview of Sagemaker*: [https://www.youtube.com/watch?v=ym7NEYEx9x4](https://www.youtube.com/watch?v=ym7NEYEx9x4).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Rekognition Go SDK*: [https://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/](https://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Azure Stream Analytics integration with Azure Machine Learning*: [https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-machine-learning-integration-tutorial](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-machine-learning-integration-tutorial).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Azure Go SDK*: [https://github.com/Azure/azure-sdk-for-go](https://github.com/Azure/azure-sdk-for-go).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Consume web service*: [https://docs.microsoft.com/en-us/azure/machine-learning/studio/consume-web-services](https://docs.microsoft.com/en-us/azure/machine-learning/studio/consume-web-services).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Quickstart: Using Go to call the Text Analytics API*. [https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/go](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/go).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Cost comparison of deep learning hardware*: [https://medium.com/bigdatarepublic/cost-comparison-of-deep-learning-hardware-google-tpuv2-vs-nvidia-tesla-v100-3c63fe56c20f](https://medium.com/bigdatarepublic/cost-comparison-of-deep-learning-hardware-google-tpuv2-vs-nvidia-tesla-v100-3c63fe56c20f).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Prediction Overview*: [https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview](https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Google AI Hub*: [https://cloud.google.com/ai-hub/](https://cloud.google.com/ai-hub/).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Amazon ECR Managed Policies*: [https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html](https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*App Service - Web App for Containers*: [https://azure.microsoft.com/en-gb/services/app-service/containers/](https://azure.microsoft.com/en-gb/services/app-service/containers/).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Push Docker Image to Private Registry*: [https://docs.microsoft.com/en-gb/azure/container-registry/container-registry-get-started-docker-cli](https://docs.microsoft.com/en-gb/azure/container-registry/container-registry-get-started-docker-cli).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Create Docker/Go app on Linux*: [https://docs.microsoft.com/en-gb/azure/app-service/containers/quickstart-docker-go](https://docs.microsoft.com/en-gb/azure/app-service/containers/quickstart-docker-go).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Container Registry*: [https://cloud.google.com/container-registry/](https://cloud.google.com/container-registry/).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Quickstart for Docker*: [https://cloud.google.com/cloud-build/docs/quickstart-docker](https://cloud.google.com/cloud-build/docs/quickstart-docker).
    Retrieved May 11, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Mechanical Turk*: [https://www.mturk.com/](https://www.mturk.com/). Retrieved
    May 15, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Shrink your Go binaries with this one weird trick*: [https://blog.filippo.io/shrink-your-go-binaries-with-this-one-weird-trick/](https://blog.filippo.io/shrink-your-go-binaries-with-this-one-weird-trick/).
    Retrieved May 16th, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
