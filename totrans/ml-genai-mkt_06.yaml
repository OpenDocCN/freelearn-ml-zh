- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Leveraging Predictive Analytics and A/B Testing for Customer Engagement
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用预测分析和A/B测试提升客户参与度
- en: There are various ways that we can benefit from data-driven and AI/ML-driven
    marketing techniques. To name a few, you can optimize your marketing strategy
    based on the key drivers behind the success and failures of your previous marketing
    campaigns, as we discussed in *Chapters 2* and *3*. You can also optimize your
    marketing strategy based on the trend and seasonality within your business or
    based on the customer sentiments around your products and business, as we have
    discussed in *Chapters 4* and *5*. Targeted product recommendation (*Chapter 7*)
    and optimizing the marketing content with Generative AI (*Chapters 9* and *10*)
    are some other key benefits of applying AI/ML in marketing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式从数据驱动和AI/ML驱动的营销技术中受益。例如，您可以根据我们已在*第二章*和*第三章*中讨论的先前营销活动的成功和失败背后的关键驱动因素来优化您的营销策略。您还可以根据您业务中的趋势和季节性，或者根据您产品和业务周围的客户情绪来优化您的营销策略，正如我们在*第四章*和*第五章*中所讨论的那样。目标产品推荐(*第七章*)和利用生成式AI优化营销内容(*第九章*和*第十章*)是应用AI/ML在营销中的其他一些关键好处。
- en: Among those mentioned, we are going to experiment with predictive analytics
    in this chapter and how you can utilize these predictive models in your next marketing
    campaign. By intelligently predicting the expected behaviors of customers, you
    can target subgroups of the customer base that are likely to result in your favor.
    This way, instead of mass marketing to the entire potential customer base, you
    can better custom-tailor your marketing messages and also save on marketing costs
    as you are only targeting the group with the higher chance of success. We will
    also discuss how A/B testing can help decide the best predictive model for your
    next marketing effort.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在所提及的主题中，我们将在本章中尝试预测分析，以及如何在您的下一次营销活动中利用这些预测模型。通过智能预测客户的预期行为，您可以针对可能对您有利的客户群体进行定位。这样，您就不必对整个潜在客户群进行大规模营销，而可以更好地定制您的营销信息，并且由于您只针对成功率更高的群体，因此还可以节省营销成本。我们还将讨论如何进行A/B测试来决定您下一次营销活动的最佳预测模型。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Predicting customer conversion with tree-based algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于树的算法预测客户转化
- en: Predicting customer conversion with deep learning algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习算法预测客户转化
- en: Conducting A/B testing for optimal model choice
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行A/B测试以选择最佳模型
- en: Predicting customer conversion with tree-based algorithms
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于树的算法预测客户转化
- en: 'Predictive analytics or modeling can be applied at various stages of the customer
    life cycle. If you recall from *Chapter 2*, there are largely five stages that
    we can break down a customer life cycle into: **Awareness**, **Engagement**, **Conversion**,
    **Retention**, and **Loyalty**, as shown in the following diagram:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分析或建模可以在客户生命周期的各个阶段应用。如果您还记得*第二章*，我们可以将客户生命周期分解为以下五个主要阶段：**意识**、**参与**、**转化**、**保留**和**忠诚度**，如下面的图所示：
- en: '![](img/B30999_06_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_01.png)'
- en: 'Figure 6.1: Customer life cycle diagram from Chapter 2'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：第二章中的客户生命周期图
- en: The applicability of predictive modeling is broad, depending on your marketing
    goal. For example, if you have a new brand or product launch and would like to
    improve new product awareness via ads on social media, you can build predictive
    models that can help you identify the target customers who are likely to click
    on the ads. On the other hand, if you would like to improve product purchase conversion
    rates, you can build predictive models that can identify customers who are more
    likely to make purchases in the next X number of days and target them. This results
    in more effective marketing, as you can avoid creating fatigue among the customers,
    which happens when they are exposed to your marketing campaigns too frequently
    with irrelevant content. This happens often when you do mass marketing without
    targeting the right subgroup of customers. Also, you can reduce marketing costs
    by sending marketing materials only to a specific subgroup of customers. This
    will help you repurpose the remaining marketing budget for other marketing campaigns.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 预测建模的应用范围很广，取决于你的营销目标。例如，如果你有一个新品牌或产品发布，并希望通过社交媒体上的广告提高新产品知名度，你可以构建预测模型来帮助你识别可能点击广告的目标客户。另一方面，如果你想提高产品购买转化率，你可以构建预测模型来识别在接下来的X天内更有可能进行购买的客户，并针对他们进行营销。这会导致更有效的营销，因为你可以避免客户因频繁接触不相关的内容而感到疲劳，这种情况通常发生在你没有针对正确的客户子群体进行大规模营销时。此外，你可以通过只向特定的客户子群体发送营销材料来降低营销成本。这将帮助你将剩余的营销预算重新用于其他营销活动。
- en: Not only can you utilize predictive analytics for better brand awareness, engagement,
    and conversion, but predictive analytics can also be used to improve retention
    rates. Often, customer churn likelihood models are built to identify who is at
    risk of turning away from your business. Through these customer churn predictions,
    you can build marketing strategies and marketing content that is customized for
    this high-churn risk group to bring them back to engaged customers. Discounts,
    free subscription trials, or free plan upgrades are often offered to this high-churn
    risk group as part of the retention strategies.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅你可以利用预测分析来提高品牌知名度、参与度和转化率，预测分析还可以用来提高客户保留率。通常，会建立客户流失可能性模型来识别哪些客户有离开你生意的风险。通过这些客户流失预测，你可以构建针对这一高流失风险群体的定制化营销策略和营销内容，以将他们重新变为活跃客户。折扣、免费订阅试用或免费计划升级通常作为保留策略的一部分提供给这一高流失风险群体。
- en: Tree-based machine learning algorithms
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于树的机器学习算法
- en: 'Numerous AI/ML algorithms can be used for predictive modeling, such as linear
    regression, logistic regression, and decision tree models, which we have discussed
    in previous chapters, as well as deep learning models that are rising in usage.
    In this chapter, we are going to build predictive models with tree-based models,
    such as random forest and gradient boosted trees, and neural network models, which
    are the backbones of deep learning models. Underneath any tree-based ML model,
    there is a decision tree. As we have discussed in *Chapter 3*, a decision tree
    is like a flowchart, where it splits into child nodes based on the information
    gained. Each node represents a question or criteria for a split and each branch
    or edge represents the outcome of the question posited at the node. The following
    diagram shows a high-level overview of how a decision tree may be built:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用多种AI/ML算法进行预测建模，例如线性回归、逻辑回归和决策树模型，这些我们在前面的章节中已经讨论过，以及正在日益普及的深度学习模型。在本章中，我们将使用基于树的模型构建预测模型，例如随机森林和梯度提升树，以及神经网络模型，它们是深度学习模型的核心。任何基于树的机器学习模型下面都有一个决策树。正如我们在*第3章*中讨论的，决策树就像一个流程图，它根据获得的信息分成子节点。每个节点代表一个分割的问题或标准，每个分支或边代表在节点提出的问题的结果。以下图表展示了决策树可能构建的高级概述：
- en: '![](img/B30999_06_02.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_02.png)'
- en: 'Figure 6.2: Illustration of a decision tree'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：决策树的示意图
- en: Among numerous tree-based ML models, the **gradient-boosted decision tree**
    (**GBDT**) and random forest are the two most popular models that are frequently
    used for predictive modeling. Both GBDT and random forest models are built with
    multiple decision trees. However, the main difference is how these decision trees
    are built.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多的基于树的机器学习模型中，**梯度提升决策树**（GBDT）和随机森林是最常用的两种模型，经常用于预测建模。GBDT和随机森林模型都是由多个决策树构建的。然而，主要区别在于这些决策树是如何构建的。
- en: Simply put, a random forest model is one with lots of decision trees, where
    each decision tree is built with a random subsample of the dataset and a random
    subset of features. This way, each decision tree within a random forest learns
    the information or relationships within the data slightly differently and with
    different focus areas.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，随机森林模型是一个包含许多决策树的模型，其中每个决策树都是使用数据集的随机子样本和特征子集构建的。这样，随机森林中的每个决策树都会以略微不同的方式学习数据中的信息或关系，并具有不同的关注区域。
- en: 'The final prediction is the average of all the outcomes or predictions of these
    individual decision trees. The following shows an illustration of a random forest:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最终预测是这些单个决策树的所有结果或预测的平均值。以下是一个随机森林的示意图：
- en: '![](img/B30999_06_03.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_03.png)'
- en: 'Figure 6.3: Illustration of random forest'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：随机森林的示意图
- en: 'A GBDT model, on the other hand, also consists of lots of decision trees, but
    each decision tree is built sequentially, and each subsequent decision tree is
    trained based on the errors that the previous decision tree makes. The final prediction
    of a GBDT model is the weighted average of all of the individual decision trees’
    predictions. The following shows an illustration of a GBDT model:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，GBDT模型也由许多决策树组成，但每个决策树是顺序构建的，每个后续的决策树都是基于前一个决策树所犯的错误进行训练的。GBDT模型的最终预测是所有单个决策树预测的加权平均值。以下是一个GBDT模型的示意图：
- en: '![](img/B30999_06_04.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_04.png)'
- en: 'Figure 6.4: Illustration of GBDTs'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：GBDT的示意图
- en: Building random forest models
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建随机森林模型
- en: In this chapter, we will be using an online purchase dataset as an example to
    build a predictive model to predict whether a customer will convert or not. First,
    we will discuss how we can build a random forest model in Python using the `scikit-learn`
    package.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用在线购买数据集作为示例来构建一个预测模型，以预测客户是否会进行转换。首先，我们将讨论如何使用`scikit-learn`包在Python中构建随机森林模型。
- en: '**Source code and data**: [https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.6](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.6)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**源代码和数据**：[https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.6](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.6)'
- en: '**Data source**: [https://archive.ics.uci.edu/dataset/468/online+shoppers +purchasing+intention+dataset](https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据来源**：[https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset](https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset)'
- en: Target and feature variables
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标变量和特征变量
- en: 'We need to first define the target and feature variables, where the target
    variable is the factor that we want to predict, and the feature variables are
    the factors that will be learned by the models to make the predictions or decisions.
    To do this, you can follow these steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要定义目标和特征变量，其中目标变量是我们想要预测的因素，特征变量是模型将学习以进行预测或决策的因素。为此，你可以遵循以下步骤：
- en: 'Let’s first load the data into a DataFrame and examine what features we can
    use for our random forest model:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先加载数据到一个DataFrame中，并检查我们可以用于我们的随机森林模型的特征：
- en: '[PRE0]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When you run this code, you should see the following output for the information
    about this data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行此代码时，你应该看到以下输出，这是关于此数据的信息：
- en: '![](img/B30999_06_05.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_05.png)'
- en: 'Figure 6.5: Summary of the example dataset'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：示例数据集摘要
- en: 'The first thing to note in the preceding output is the column, `Revenue`, which
    is the target variable that tells us whether a customer made a purchase or converted
    or not, has a type of `Boolean`. The other column, `Weekend`, also has a `Boolean`
    data type. We are going to encode them as `0` for False and `1` for True with
    the following code:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的输出中，首先要注意的是列，`Revenue`，这是目标变量，它告诉我们客户是否进行了购买或转换，其类型为`布尔型`。另一列，`Weekend`，也具有`布尔型`数据类型。我们将使用以下代码将它们编码为`0`表示False和`1`表示True：
- en: '[PRE1]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, two columns have an `object` as the data type, `Month` and `VisitorType`.
    If you look closer, the column, `Month`, has string values for the months, which
    we will convert into corresponding month numbers. The column, `VisitorType`, has
    three unique values, `New_Visitor`, `Returning_Visitor`, and `Other`. We are going
    to encode each as `0`, `1`, and `2` respectively, as shown in the following code:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，有两列的数据类型为 `object`，分别是 `Month` 和 `VisitorType`。如果你仔细观察，`Month` 列的月份值是字符串，我们将将其转换为相应的月份数字。`VisitorType`
    列有三个唯一值，`New_Visitor`、`Returning_Visitor` 和 `Other`。我们将分别将其编码为 `0`、`1` 和 `2`，如下所示：
- en: '[PRE2]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see in this code, we are using the `strptime` function of a `time`
    module to encode three-letter month string values into corresponding month numbers.
    Then, we use the `apply` function of a `pandas` DataFrame to encode each value
    of the `VisitorType` column into corresponding integer values.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如此代码所示，我们正在使用 `time` 模块的 `strptime` 函数将三个字母的月份字符串值编码为相应的月份数字。然后，我们使用 `pandas`
    DataFrame 的 `apply` 函数将 `VisitorType` 列的每个值编码为相应的整数值。
- en: 'Now that we have converted all the column values into numeric values, we are
    going to define the target and feature variables as in the following:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将所有列值转换为数值，我们将定义目标和特征变量，如下所示：
- en: '[PRE3]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see from this code, we have defined the target variable, `TARGET`,
    to use the `Revenue` column and the rest columns as the feature variables, `FEATURES`.
    Then, we create a DataFrame, `X`, which is the feature set, and `Y`, which is
    the target series.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如此代码所示，我们已定义目标变量 `TARGET` 使用 `Revenue` 列，其余列作为特征变量 `FEATURES`。然后，我们创建了一个 DataFrame
    `X`，它是特征集，以及 `Y`，它是目标序列。
- en: 'Lastly, we will split these target and feature sets into train and test sets
    with the following code:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用以下代码将这些目标和特征集拆分为训练集和测试集：
- en: '[PRE4]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We are using the `train_test_split` function within the `sklearn.model_selection`
    module. As you can see from the `test_size` parameter, we are using 80% of the
    dataset for training and the other 20% for testing.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用 `sklearn.model_selection` 模块中的 `train_test_split` 函数。从 `test_size` 参数可以看出，我们正在使用数据集的80%进行训练，其余20%用于测试。
- en: With these train and test sets, we are now ready to train a random forest model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些训练集和测试集，我们现在可以准备训练一个随机森林模型。
- en: Training a random forest model
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练随机森林模型
- en: 'Python’s `scikit-learn` package provides a handy way to a random forest model.
    Take a look at the following code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的 `scikit-learn` 包提供了一个方便的方式来创建随机森林模型。看看以下代码：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s take a closer look at this code. We are using the `RandomForestClassifier`
    class from the `sklearn.ensemble` module and initiated a Random Forest model with
    `n_estimators, max_depth, class_weight,` and `n_jobs` parameters:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这段代码。我们正在使用 `sklearn.ensemble` 模块中的 `RandomForestClassifier` 类，并使用
    `n_estimators`、`max_depth`、`class_weight` 和 `n_jobs` 参数初始化一个随机森林模型：
- en: The `n_estimators` parameter defines how many individual decision trees to build.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators` 参数定义了要构建多少个单独的决策树。'
- en: The `max_depth` parameter defines how deep each decision tree can grow. Along
    with other parameters, such as `min_samples_split`, `min_samples_leaf`, and `max_features`,
    `max_depth` helps prevent overfitting issues by limiting how much a decision tree
    can grow.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth` 参数定义了每个决策树可以生长多深。与其他参数，如 `min_samples_split`、`min_samples_leaf`
    和 `max_features` 一起，`max_depth` 通过限制决策树的生长来帮助防止过拟合问题。'
- en: The `class_weight` parameter defines weights for each class. This parameter
    is useful when the dataset is imbalanced. In our example dataset, only about 15%
    are in the positive class, meaning only 15% of the target variable, Revenue, has
    a value of 1, or only 15% of customers have converted. You can give custom weights
    for each class as a dictionary or use the `"balanced"` option to automatically
    adjust the weights that are inversely proportional to the actual class frequencies.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_weight` 参数定义了每个类的权重。当数据集不平衡时，此参数非常有用。在我们的示例数据集中，只有大约15%属于正类，这意味着只有15%的目标变量，收入，其值为1，或者只有15%的客户已经转化。你可以为每个类提供自定义权重，作为一个字典，或者使用
    `"balanced"` 选项来自动调整与实际类频率成反比的权重。'
- en: Lastly, the `n_jobs` parameter defines how many jobs to run in parallel. If
    you recall from our discussion of random forest and GBDTs, random forest is a
    bag of decision trees, so individual trees can be built in parallel without any
    dependency on other trees. By giving `-1` as the input for this parameter, you
    are instructing it to use all available resources to train this random forest
    model.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`n_jobs` 参数定义了要并行运行多少个作业。如果您还记得我们关于随机森林和GBDTs的讨论，随机森林是一系列决策树，因此可以并行构建单个树，而不依赖于其他树。通过将
    `-1` 作为此参数的输入，您指示它使用所有可用资源来训练此随机森林模型。
- en: With these parameters, we can now train this random forest model, using the
    `fit` function with the train set.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数，我们现在可以使用 `fit` 函数和训练集来训练这个随机森林模型。
- en: '**Overfitting versus underfitting?**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合与欠拟合？**'
- en: Overfitting refers to when models are fit to the training set so closely that
    it does well on the training data but poorly on the data that the models have
    not seen before. Underfitting, on the other hand, is when models are over-generalized
    or do not tune well enough to the training set that they did not learn the relationships
    between the feature variables and the target variable. Multiple iterations of
    hyperparameter tuning are often required to find the sweet spot for minimal overfitting.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是指模型拟合到训练集过于紧密，在训练数据上表现良好，但在模型之前未见过的数据上表现较差。另一方面，欠拟合是指模型过度泛化或没有足够地调整到训练集，以至于没有学习到特征变量和目标变量之间的关系。通常需要多次迭代超参数调整，以找到最小过拟合的最佳点。
- en: Predicting and evaluating random forest model
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测和评估随机森林模型
- en: 'The `RandomForestClassifier` object provides handy functions for making predictions
    from the trained random forest model. Take a look at the following code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier` 对象提供了从训练好的随机森林模型进行预测的便捷函数。请看以下代码：'
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As the names suggest, the `predict` function makes predictions on the given
    input. In our case, the results will be a list of 0s and 1s for each record in
    the test set, as we are predicting whether a customer has converted or not.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，`predict` 函数对给定的输入进行预测。在我们的例子中，结果将是测试集中每个记录的 0 和 1 的列表，因为我们正在预测客户是否已经转换。
- en: The `predict_proba` function also makes predictions on the given input, but
    the difference is it gives predicted probabilities running between `0` and `1`.
    It returns predicted probabilities for each record and for each class, so, in
    our case, it returns two values for each record, where the first element is a
    predicted probability to be a class of `0` and the second element is a predicted
    probability to be a class of `1`. Since we are only interested in the predicted
    probability of class 1, we are slicing it with `[:,1]` so that we have a list
    of predicted probabilities of conversion.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict_proba` 函数也对给定的输入进行预测，但不同之处在于它给出介于 `0` 和 `1` 之间的预测概率。它为每个记录和每个类别返回预测概率，因此，在我们的例子中，它为每个记录返回两个值，其中第一个元素是预测为类别
    `0` 的概率，第二个元素是预测为类别 `1` 的概率。由于我们只对类别 1 的预测概率感兴趣，我们使用 `[:,1]` 来切片，这样我们就有了一个转换预测概率的列表。'
- en: Now that we have the predicted conversion probabilities, we need to evaluate
    how good our predictions are.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了预测的转换概率，我们需要评估我们的预测有多好。
- en: There are multiple ways to evaluate the accuracy and effectiveness of a predictive
    model, but we will mainly look at the overall accuracy, precision, recall, **area
    under the curve** (**AUC**) - **receiver operating characteristics** (**ROC**)
    curve, and confusion matrix. We will go deeper into these metrics with examples.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 评估预测模型的准确性和有效性的方法有很多，但我们将主要关注整体准确率、精确率、召回率、**曲线下面积**（**AUC**）- **接收者操作特征**（**ROC**）曲线和混淆矩阵。我们将通过示例深入探讨这些指标。
- en: 'As the name suggests, the **accuracy** is the percentage of correct predictions
    or **true positives** (**TP**) among all the predictions. The **precision** is
    the percentage of correct predictions among those predictive positive or the percentage
    of TPs among those predicted to be positive that include **false positives** (**FP**).
    The **recall** is the percentage of positive cases identified by the model or
    the percentage of TPs among the actual positives, which are TPs and false negatives.
    The equations for the accuracy, precision, and recall are as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，**准确率**是所有预测中正确预测或**真正例**（**TP**）的百分比。**精确率**是预测正例中正确预测的百分比，或者是预测为正例中包括**假正例**（**FP**）的真正例的百分比。**召回率**是模型识别出的正例的百分比，或者是实际正例中包括真正例和**假负例**的真正例的百分比。准确率、精确率和召回率的公式如下：
- en: '![](img/B30999_06_001.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_001.png)'
- en: '![](img/B30999_06_002.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_002.png)'
- en: '![](img/B30999_06_003.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_003.png)'
- en: 'In Python, the scikit-learn package provides a handy tool for computing these
    metrics, as shown in the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，scikit-learn包提供了一个方便的工具来计算这些指标，如下面的代码所示：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In our example, when these codes are run, the results of these key metrics
    look as in the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，当运行这些代码时，这些关键指标的结果如下：
- en: '![](img/B30999_06_06.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_06.png)'
- en: 'Figure 6.6: Summary of random forest model performance metrics'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：随机森林模型性能指标摘要
- en: These results suggest the random forest model we have trained has decent overall
    accuracy and recall rates but does not seem to do well with precision. This suggests
    that of those customers this model predicted to be positive or likely to convert,
    only about 54% of them have actually converted. However, if you recall, the actual
    overall conversion rate is 15%; in other words, if you randomly guess who will
    convert, you may only be right about 15% of the time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，我们训练的随机森林模型具有相当的整体准确率和召回率，但在精确率方面似乎表现不佳。这表明，在这个模型预测为正例或可能转换的客户中，只有大约54%实际上已经转换。然而，如果你还记得，实际的总体转换率是15%；换句话说，如果你随机猜测谁会转换，你可能只有15%的时间是正确的。
- en: Thus, since this model has predicted converted customers 54% of the time, it
    proves to be way much more effective in selecting customers that are more likely
    to convert than random guesses. Also, the high recall rate suggests that about
    85% of the customers who have actually converted are among those who are predicted
    to be highly likely to convert by this model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于这个模型有54%的时间预测了转换客户，这证明了它在选择更有可能转换的客户方面比随机猜测要有效得多。此外，高召回率表明，实际上已经转换的客户中有大约85%是那些被这个模型预测为高度可能转换的客户。
- en: From an actual marketing perspective, if you have marketed only to the customers
    who have been predicted to be likely to convert by this model, you would have
    still captured most of those conversions. Also, if you have marketed to your entire
    customer base, 85% (100% minus 15%, which is the overall conversion rate) of your
    marketing spend would have been wasted. But if you have marketed only to these
    highly likely customers, only about 46% (100% minus 54%, which is the precision
    of this model) of the marketing spend would have been wasted.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际的市场营销角度来看，如果你只针对这个模型预测可能转换的客户进行营销，你仍然会捕获到大多数这些转换。此外，如果你针对你的整个客户群进行营销，85%（100%减去15%，这是整体转换率）的营销支出将会浪费。但是，如果你只针对这些高度可能的客户进行营销，只有大约46%（100%减去54%，这是该模型的精确度）的营销支出将会浪费。
- en: The other key evaluation metric we are going to look at is the **AUC - ROC curve**.
    The **ROC curve**, simply put, shows the trade-offs between gains in **true positive
    rates** (**TPRs**) for each sacrifice you make for a **false positive rate** (**FPR**).
    The **AUC** is, as the name suggests, the area under the ROC curve and tells us
    how well the model separates the positive cases from negative cases. The AUC ranges
    from `0` to `1` and the higher it is, the better the model is. At the AUC of `0.5`,
    it suggests that the model performs the same as random guessing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要关注的另一个关键评估指标是**AUC-ROC曲线**。简单来说，**ROC曲线**显示了在牺牲**假正率**（**FPR**）的情况下，每个牺牲所获得的**真正率**（**TPR**）的权衡。**AUC**，正如其名所示，是ROC曲线下的面积，告诉我们模型在区分正例和负例方面的表现如何。AUC的范围从`0`到`1`，数值越高，模型越好。在AUC为`0.5`时，表明模型的表现与随机猜测相同。
- en: 'The following code can be used to plot the ROC curve:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码可以用来绘制ROC曲线：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, we are using the `metrics` module again to plot the ROC curve. The main
    difference between computing the accuracy, precision, and recall and computing
    the AUC - ROC curve is how we use the predicted probabilities, `rf_pred_proba`,
    instead of predicted labels, `pred`. This is because the ROC curve examines how
    TPRs and FPRs change at different probability levels. By using predicted probabilities,
    we can calculate how the TPR and FPR change as the decision threshold varies.
    The resulting chart looks like the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们再次使用`metrics`模块来绘制ROC曲线。计算准确率、精确率和召回率与计算AUC-ROC曲线之间的主要区别在于我们如何使用预测概率`rf_pred_proba`而不是预测标签`pred`。这是因为ROC曲线检查在不同概率水平下TPR和FPR如何变化。通过使用预测概率，我们可以计算随着决策阈值的改变，TPR和FPR如何变化。生成的图表如下所示：
- en: '![](img/B30999_06_07.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_07.png)'
- en: 'Figure 6.7: AUC - ROC curve of the random forest model predictions'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：随机森林模型预测的AUC-ROC曲线
- en: As you can see from this chart, you can easily evaluate how each sacrifice in
    the FPR affects the TPR. For example, in this chart, at 20% FPR, we already achieve
    about 90% TPR, which suggests that this model does well separating positive cases
    from negative cases. The AUC here is `0.93`, which also suggests that the model
    does well in identifying positive cases from negative cases.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这张图表中可以看到，您可以轻松评估每个FPR的牺牲如何影响TPR。例如，在这张图表中，在20% FPR的情况下，我们已经实现了大约90%的TPR，这表明该模型在区分正例和负例方面表现良好。这里的AUC为`0.93`，这也表明模型在识别正例和负例方面表现良好。
- en: 'Lastly, we will look at the **confusion matrix**. As the name suggests, the
    confusion matrix is a good way to look at how and where the model gets confused
    the most. It will be easier to understand with an example. Take a look at the
    following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将查看**混淆矩阵**。正如其名所示，混淆矩阵是查看模型在何处以及如何最容易被混淆的好方法。以下是一个示例，可以帮助理解。请看以下代码：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similar to before, we are using the `metrics` module to build a confusion matrix.
    The `confusion_matrix` function takes the actual and predicted values and builds
    a confusion matrix. Then, we are using the `heatmap` function of the `seaborn`
    Python package to plot a heat map. The resulting chart looks like the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前类似，我们使用`metrics`模块构建混淆矩阵。`confusion_matrix`函数接受实际值和预测值，并构建混淆矩阵。然后，我们使用`seaborn`
    Python包中的`heatmap`函数绘制热图。生成的图表如下所示：
- en: '![](img/B30999_06_08.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_08.png)'
- en: 'Figure 6.8: Confusion matrix of the random forest model predictions'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：随机森林模型预测的混淆矩阵
- en: As you can see from this plot, the y-axis represents the actual classes and
    the x-axis represents the predicted classes. For example, the top left box is
    where the actual class was 0 or no conversion, and the predicted class was also
    `0`. The top right box is where the actual class was 0, but the model predicted
    them to be a class of `1` or conversion.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这张图中可以看到，y轴表示实际类别，x轴表示预测类别。例如，左上方的框是实际类别为0或无转换，预测类别也是`0`的地方。右上方的框是实际类别为0，但模型预测它们属于类别`1`或转换的地方。
- en: As you can see, the confusion matrix shows you where the model is the most confused.
    A model that predicts the outcome with high accuracy will have large numbers or
    percentages in the diagonal boxes and small numbers in the other boxes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，混淆矩阵显示了模型最困惑的地方。一个预测结果准确率高的模型将在对角线框中有大量数字或百分比，而在其他框中有小数字。
- en: We have experimented with predicting the customer conversion likelihood with
    a random forest model. Here, we have observed and discussed how this random forest
    predictive model can help target the subset of the customer base without losing
    too many converted customers and how this can result in much more cost-effective
    marketing strategies.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经尝试使用随机森林模型预测客户转换的可能性。在这里，我们观察并讨论了如何使用这个随机森林预测模型帮助定位客户群的一部分，同时不会失去太多已转换的客户，以及这如何导致更具成本效益的营销策略。
- en: Gradient boosted decision tree (GBDT) modeling
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升决策树（GBDT）建模
- en: 'We will use the same dataset and train/test sets for building a GBDT model
    and compare its performance against the random forest model we have just built.
    XGBoost is the most commonly used library in Python for training a GBDT model.
    You can install this package using the following command in the terminal or Jupyter
    Notebook:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的dataset和train/test sets来构建GBDT模型，并将其性能与我们刚刚构建的随机森林模型进行比较。XGBoost是Python中用于训练GBDT模型最常用的库。您可以在终端或Jupyter
    Notebook中使用以下命令安装此包：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Training GBDT model
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练GBDT模型
- en: 'The XGBoost package follows the same pattern as the `scikit-learn` package.
    Take a look at the following code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost包遵循与`scikit-learn`包相同的模式。请看以下代码：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see from this code, we initiate an `XGBClassifier` with the `n_estimators`,
    `max_depth`, and `scale_pos_weight` parameters:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如此代码所示，我们使用`n_estimators`、`max_depth`和`scale_pos_weight`参数初始化了一个`XGBClassifier`：
- en: Similar to the case of the random forest model, the `n_estimators` parameter
    defines how many individual decision trees are to be built. You may have noticed
    we are using a much lower number for this parameter. If you recall, a GBDT model
    is sequentially built where each subsequent decision tree learns the errors the
    previous decision tree makes. This often results in a smaller number of individual
    trees performing as well as or better than the random forest model.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与随机森林模型的情况类似，`n_estimators`参数定义了要构建多少个单个决策树。你可能已经注意到，我们为这个参数使用了一个非常低的数字。如果你还记得，GBDT模型是顺序构建的，其中每个后续的决策树都会学习前一个决策树犯的错误。这通常会导致更少的单个树表现出与随机森林模型相当或更好的性能。
- en: Also, because a GBDT model is sequentially built, a large number of decision
    trees results in much longer training time than a random forest model, which can
    build individual decision trees in parallel.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于GBDT模型是顺序构建的，与可以并行构建单个决策树的随机森林模型相比，大量决策树会导致训练时间更长。
- en: The other parameter, `max_depth`, is the same as in the case of the random forest
    model. This parameter restricts how deep each decision tree can grow.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个参数`max_depth`与随机森林模型的情况相同。此参数限制了每个决策树可以生长的深度。
- en: Lastly, the `scale_pos_weight` parameter defines the balance of the positive
    and negative class weights. As you may recall we have an imbalanced dataset where
    the positive class is only about 15% of the data. By giving inversely proportionate
    weight for the positive class with `1/train_y.mean()`, we are instructing the
    model to adjust for the imbalanced dataset. This enforces the GBDT model to penalize
    more for incorrect predictions of the positive class, which makes the model more
    sensitive to the positive class.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`scale_pos_weight`参数定义了正负类权重的平衡。你可能还记得，我们有一个不平衡的数据集，其中正类仅占数据的约15%。通过使用`1/train_y.mean()`对正类给予成反比的权重，我们指示模型调整不平衡数据集。这强制GBDT模型对正类的不正确预测进行更多惩罚，从而使模型对正类更加敏感。
- en: There are various ways to handle potential overfitting issues that often occur
    when you have large individual decision trees within a GBDT model. On top of the
    `max_depth` parameter, the XGBoost package provides various other parameters,
    such as `max_leaves`, `colsample_bytree`, `subsample`, and `gamma`, that can help
    reduce overfitting.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当GBDT模型中包含大量单个决策树时，通常会存在潜在的过拟合问题，处理这些问题的方法有很多。除了`max_depth`参数之外，XGBoost包还提供了其他各种参数，例如`max_leaves`、`colsample_bytree`、`subsample`和`gamma`，这些参数可以帮助减少过拟合。
- en: We suggest that you look at the documentation and experiment with various parameters
    and see how they help you prevent overfitting.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您查看文档，并尝试各种参数，看看它们如何帮助您防止过拟合。
- en: 'Here is the documentation: [https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是文档：[https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier)。
- en: Predicting and Evaluating GBDT Model
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测和评估GBDT模型
- en: 'Similar to the `RandomForestClassifier` of the random forest model, the `XGBClassifier`
    object also provides the same syntax for predicting with the trained GBDT model.
    Take a look at the following code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林模型的`RandomForestClassifier`类似，`XGBClassifier`对象也提供了相同的语法来预测训练好的GBDT模型。请看以下代码：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see from this code, you can use the `predict` function to predict
    the positive versus negative label for each record of the test set and the `predict_proba`
    function to predict the probabilities for each class of individual records of
    the test set. As before, we are retrieving the second column of the predicted
    probabilities by slicing the output with `[:,1]` to get the predicted probabilities
    of the positive class, which is the predicted probability of a conversion, for
    each record of the test set.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如此代码所示，你可以使用`predict`函数预测测试集中每个记录的正负标签，并使用`predict_proba`函数预测测试集中每个个体记录的每个类别的概率。与之前一样，我们通过使用`[:,1]`切片输出以获取正类预测概率（即转换的预测概率），来获取测试集中每个记录的预测概率。
- en: 'To compare the performance of this GBDT model against the random forest model,
    we will use the same evaluation metrics and approaches. As you may recall, we
    can use the following code to get the key metrics of accuracy, precision, and
    recall:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较这个GBDT模型与随机森林模型的性能，我们将使用相同的评估指标和方法。你可能还记得，我们可以使用以下代码来获取准确率、精确率和召回率的关键指标：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Due to some randomness in building these trees, there can be some variances
    and differences each time a GBDT model is trained, but at the time of this writing,
    the results look as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于构建这些树时存在一些随机性，每次训练GBDT模型时都可能会有一些方差和差异，但在此写作时，结果如下所示：
- en: '![](img/B30999_06_09.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_09.png)'
- en: 'Figure 6.9: Summary of GBDT model performance metrics'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：GBDT模型性能指标总结
- en: These key metrics look very similar to the random forest model. The overall
    accuracy and precision of this GBDT model are slightly higher than the random
    forest model. However, the recall is slightly lower than the random forest model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关键指标看起来与随机森林模型非常相似。这个GBDT模型的整体准确率和精确率略高于随机森林模型。然而，召回率略低于随机森林模型。
- en: 'Similarly, the AUC-ROC curve can be plotted using the following code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，可以使用以下代码绘制AUC-ROC曲线：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting AUC-ROC Curve looks like the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的AUC-ROC曲线如下所示：
- en: '![](img/B30999_06_10.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_10.png)'
- en: 'Figure 6.10: AUC- ROC Curve of the GBDT model predictions'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：GBDT模型预测的AUC-ROC曲线
- en: When you compare this against the random forest model, the results are almost
    identical. AUC is about the same with `0.93` and at the FPR of `0.2`, the TPR
    of the GBDT model is also about `0.9`, which was also the case of the previously
    built random forest model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将此与随机森林模型进行比较时，结果几乎相同。AUC约为`0.93`，在FPR为`0.2`时，GBDT模型的TPR也约为`0.9`，这与之前构建的随机森林模型的情况相同。
- en: 'Lastly, let’s take a look at the confusion matrix with the following code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们通过以下代码查看混淆矩阵：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The resulting confusion matrix looks like the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的混淆矩阵如下所示：
- en: '![](img/B30999_06_11.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_11.png)'
- en: 'Figure 6.11: Confusion matrix of the GBDT model predictions'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：GBDT模型预测的混淆矩阵
- en: When you compare this against the confusion matrix of the random forest model,
    you will notice that the TPs are lower with the GBDT model, but false negatives
    are also lower with the GBDT model. This is expected as we have seen that the
    precision of the GBDT model is higher, but recall is lower compared to the random
    forest model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将此与随机森林模型的混淆矩阵进行比较时，你会注意到GBDT模型的TPs较低，但GBDT模型的假阴性也较低。这是预期的，因为我们已经看到GBDT模型的精确率较高，但与随机森林模型相比，召回率较低。
- en: Overall, the performances of random forest and GBDT models are very similar
    and hard to distinguish from these examples. Depending on how you fine-tune your
    model, you may end up with a better random forest or GBDT model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，随机森林和GBDT模型的性能非常相似，从这些例子中难以区分。根据你如何微调你的模型，你可能会得到一个更好的随机森林或GBDT模型。
- en: There are numerous ways and parameters you can fine-tune the tree-based models.
    We suggest you experiment with various sets of parameters and see how they affect
    the model’s performance!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法和参数可以微调基于树的模型。我们建议你尝试不同的参数集，看看它们如何影响模型性能！
- en: Predicting customer conversion with deep learning algorithms
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习算法预测客户转化
- en: Deep learning has become a hot topic and its popularity and usage are rising,
    as deep learning models are proven to work well when data have complex relationships
    within the variables and learn or extract features autonomously from the data,
    even though tree-based models are also very frequently used and powerful for predictive
    modeling. We touched on deep learning in *Chapter 5* when we used pre-trained
    language models for sentiment analysis and classification. In this section, we
    are going to build on that knowledge and experiment with developing deep learning
    models for predictive modeling and, more specifically, for making predictions
    on which customers are likely to convert.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已成为热门话题，其流行度和使用率都在上升，因为深度学习模型已被证明在数据变量之间存在复杂关系时表现良好，并且可以从数据中自主学习和提取特征，尽管基于树的模型也非常频繁地被使用，并且在预测建模方面非常强大。我们在*第5章*中提到了深度学习，当时我们使用了预训练的语言模型进行情感分析和分类。在本节中，我们将在此基础上构建知识，并实验开发深度学习模型用于预测建模，特别是用于预测哪些客户可能转化的预测。
- en: 'Deep learning is basically an **artificial neural network** (**ANN**) model
    with lots of hidden and complex layers of neurons, or, in other words, a deep
    ANN. An ANN is a model inspired by the biological neural networks in animal and
    human brains. An ANN learns the data through layers of interconnected neurons
    that resemble animal and human brains. The following diagram shows the high-level
    structure of an ANN:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习基本上是一种**人工神经网络**（**ANN**）模型，具有许多隐藏和复杂的神经元层，或者说，是一种深度ANN。ANN是一种受动物和人类大脑中生物神经网络启发的模型。ANN通过类似动物和人类大脑的相互连接的神经元层来学习数据。以下图表显示了ANN的高级结构：
- en: '![](img/B30999_06_12.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/B30999_06_12.png)'
- en: 'Figure 6.12: Example ANN architecture'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12：示例ANN架构
- en: 'As this diagram shows, there are three layers in any deep learning or ANN model:
    the input layer, hidden layer, and output layer. Detailed explanations of how
    ANN models learn or build the hidden layer weights are beyond the scope of this
    book, but at a high level, they go through iterations of forward propagations
    and backward propagations:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如此图表所示，任何深度学习或ANN模型都有三个层次：输入层、隐藏层和输出层。ANN模型如何学习或构建隐藏层权重的详细解释超出了本书的范围，但就高层次而言，它们通过前向传播和反向传播的迭代进行：
- en: Forward propagation is where the data is fed through a network from the input
    layer to the output layer and the activation function is applied regardless of
    whether each neuron is activated or not and returns the output of each node.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播是将数据从输入层通过网络传递到输出层，并应用激活函数的过程，无论每个神经元是否被激活，并返回每个节点的输出。
- en: Backward propagation is the process of moving from the output layer to the input
    layer and adjusting the weights of the network by analyzing the losses or errors
    from the previous iteration.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播是从输出层到输入层的移动过程，通过分析前一次迭代的损失或误差来调整网络的权重。
- en: Through iterations of these forward and backward propagations, a neural network
    learns the weights of each neuron that minimizes the error of the predictions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些前向和反向传播的迭代，神经网络学习每个神经元的权重，以最小化预测误差。
- en: A number of hidden layers and a number of neurons in each layer should be experimented
    with to find the right neural network architecture that works best for each case
    of predictive models. In this section, we are going to experiment with how wide
    neural network architecture performs against deep neural network architecture.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 应该尝试不同的隐藏层数量和每层的神经元数量，以找到最适合每个预测模型案例的最佳神经网络架构。在本节中，我们将实验比较宽神经网络架构与深度神经网络架构的性能。
- en: '**When to use wide versus deep neural networks?**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**何时使用宽神经网络与深度神经网络？**'
- en: 'A wide neural network refers to an ANN model that has fewer layers but more
    neurons in each layer, whereas a deep neural network refers to an ANN model with
    lots of hidden layers. Aside from the model performance comparisons to see which
    architecture works better for your case, there are some key factors you may want
    to consider or limit which approach to take:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 宽神经网络指的是具有较少层但每层有更多神经元的ANN模型，而深度神经网络指的是具有许多隐藏层的ANN模型。除了比较模型性能以确定哪种架构更适合您的案例之外，还有一些关键因素您可能需要考虑或限制采取哪种方法：
- en: '**Training time and compute resources**: As the number of layers grows, it
    takes more time to train as backpropagation through each layer is computationally
    intensive, which also results in higher compute costs. The wide neural network
    may have an advantage in shortening the training time and reducing the compute
    costs.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练时间和计算资源**：随着层数的增加，训练需要更多的时间，因为每一层的反向传播都是计算密集型的，这也导致了更高的计算成本。宽神经网络可能在缩短训练时间和降低计算成本方面具有优势。'
- en: '**Model interpretability**: Shallower architecture may offer better interpretability
    compared to deep neural networks. If explainability is a requirement, shallow
    architecture may be a better fit.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型可解释性**：与深度神经网络相比，较浅的架构可能提供更好的可解释性。如果可解释性是一个要求，浅层架构可能更适合。'
- en: '**Ability to generalize**: Deep neural network models tend to be able to capture
    more abstract patterns through more layers and learn higher-order features compared
    to wide neural networks. This may result in the deep architecture model having
    better performance on new and unseen data.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化能力**：深度神经网络模型通常能够通过更多的层捕获更抽象的模式，并且与宽神经网络相比，能够学习更高阶的特征。这可能导致深度架构模型在新的和未见过的数据上具有更好的性能。'
- en: Train and test sets for deep learning models
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习模型的训练集和测试集
- en: 'For the best performances of neural network models, you need to normalize the
    data before you train the models. We are going to use the same train and test
    sets that we used previously for tree-based models, but normalize them. Take a
    look at the following code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了神经网络模型的最佳性能，您需要在训练模型之前对数据进行归一化。我们将使用之前用于树模型的相同训练集和测试集，但对其进行归一化。看看以下代码：
- en: '[PRE16]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As this code suggests, we first get the mean and standard deviation from the
    train set and normalize both train and test sets by subtracting the mean and dividing
    by the standard deviation. After the standardization, the `normed_train_x` DataFrame
    should have means of `0` and standard deviations of `1` for all the columns.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如此代码所示，我们首先从训练集中获取均值和标准差，并通过减去均值并除以标准差来归一化训练集和测试集。标准化后，`normed_train_x` DataFrame的所有列的均值应为`0`，标准差为`1`。
- en: Wide neural network modeling
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 宽神经网络建模
- en: We will first look at building **wide neural network** models using the `keras`
    package in Python.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用Python中的`keras`包构建**宽神经网络**模型。
- en: 'To install `keras` on your machine, run the following command in your terminal:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的机器上安装`keras`，请在您的终端中运行以下命令：
- en: '[PRE17]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Training the wide neural network model
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练宽神经网络模型
- en: 'With the normalized train and test sets, let’s start building neural network
    models. Take a look at the following code to see how a neural network model can
    be initiated:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用归一化的训练集和测试集，让我们开始构建神经网络模型。看看以下代码，了解如何初始化神经网络模型：
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see from this code, we create a sequence of layers with the `Sequential`
    class. We start with the input layer with the `Input` class, where we define the
    shape or the number of input neurons to match the number of columns or features
    of the train set. Then, we have one wide hidden layer with 2,048 neurons. We added
    a `Dropout` layer, which defines what percentage of neurons to drop and this helps
    reduce the overfitting issues. We are instructing the neural network model to
    drop 20% of the neurons between the hidden and output layers. Lastly, we have
    one `Dense` layer for the output layer with one output neuron, which will output
    the predicted probability of the positive case or the likelihood of a conversion.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从以下代码中可以看到，我们使用`Sequential`类创建了一系列层。我们以`Input`类定义的输入层开始，其中我们定义了形状或输入神经元的数量，以匹配训练集的列数或特征数。然后，我们有一个包含2,048个神经元的宽隐藏层。我们添加了一个`Dropout`层，它定义了要丢弃的神经元百分比，这有助于减少过拟合问题。我们指示神经网络模型在隐藏层和输出层之间丢弃20%的神经元。最后，我们有一个输出层的`Dense`层，它有一个输出神经元，将输出正例的预测概率或转换的可能性。
- en: 'You may have noticed there are two activation functions we use for the hidden
    layer and the output layer. The **rectified linear unit** (**ReLU**) activation
    function is one of the most frequently used activation functions, which only activates
    the positive values and deactivates all the negative values. The equation for
    the ReLU activation function looks as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到我们用于隐藏层和输出层的两个激活函数。**ReLU**（修正线性单元）激活函数是最常用的激活函数之一，它只激活正值并关闭所有负值。ReLU激活函数的方程如下：
- en: '*ReLU*(*x*) = *max*(*0*, *x*)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*ReLU*(*x*) = *max*(*0*, *x*)'
- en: 'The behavior of the ReLU activation function looks like the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_13.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: ReLU activation function'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'The **sigmoid** function, on the other hand, turns values into a range of `0`
    and `1`. This makes the sigmoid function the top choice to predict the probability
    as an output. The equation for the sigmoid function is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_004.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'The behavior of the sigmoid function looks as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_14.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Sigmoid activation function'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: There are various other activation functions other than ReLU and sigmoid, such
    as tanh, leaky ReLU, and softmax. We suggest you do some research and experiment
    with how different activation functions work and affect the neural network models’
    performances!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few more steps involved before we can train a neural network model:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define the key metrics we will track through the iterations
    of model training and compile the model. Take a look at the following code:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As we have discussed previously with the tree-based models, we are going to
    track accuracy, precision, and recall. We will be using the Adam optimizer for
    this exercise. Simply put, optimizers are the algorithms that are used to update
    the weights of the network to minimize the errors based on the loss function,
    which in this case is `binary_crossentropy`, which measures the difference between
    the predicted and actual binary outcomes. Since our output or target variable
    for the prediction is a binary variable of conversion versus no conversion, `binary_crossentropy`
    will be the right fit for the loss function. For multi-class predictions where
    the target variable is not a binary variable and has more than two possible outcomes,
    `categorical_crossentropy` will be a better-suited loss function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to have a checkpoint to save the best model among all the
    iterations or epochs that we will run while training the model. Take a look at
    the following code:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, we are using precision as the metric to monitor and store the best model
    to a local drive based on that metric value. The best model is going to be stored
    as defined in the `best_model_path` variable.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it is now time to train this **wide neural network** model with the
    following code:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Similar to the class weights we have given to our tree-based models previously,
    we are also going to define the class weights that are inversely proportional
    to the class composition. As you can see from the code, we instruct the model
    to run for 30 iterations or epochs with a batch size of 64\. We register the callback
    that we created to store the best model previously with the custom class weights.
    When you run this code, it will run for 30 epochs and report the accuracy, precision,
    and recall metrics that we defined previously. The output of this code should
    look something like the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_15.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Sample output of the neural network model'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Predicting and evaluating wide neural network model
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For making predictions from the model we have just trained, we first need to
    load the best model. You can use the following code to load the best model:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Making predictions with this model is similar to the `scikit-learn` package’s
    syntax. Take a look at the following code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you may notice from this code, you can use the `predict` function of the
    `keras` model to get the predicted probabilities for each record. We use the `flatten`
    function to make the predicted probability output a list of predicted probabilities
    from a two-dimensional array. Then, for illustration purposes, we are going to
    assume any record with a predicted probability above `0.5` is considered to have
    a high chance of converting and assume they are positive cases. The probability
    threshold is another factor you can fine-tune for the final predictions of conversions
    versus no conversions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to use the same codes that we have used previously for the tree-based
    models for key metrics, the AUC-ROC curve, and the confusion matrix. The results
    look as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_16.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Summary of the evaluation results for the wide neural network
    model'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: If you compare these results against the tree-based models, you will notice
    the wide neural network model performed a little worse than the tree-based models.
    The precision and recall are slightly lower and the AUC is also a little lower
    compared to those of the tree-based models. We will go deeper into the practical
    uses of these models and how to choose the best model to use for marketing campaigns
    later when we discuss A/B testing in more depth, but the model performances based
    on the train and test sets look better for the tree-based models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural network modeling
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss how to build deep neural network models and
    compare the results against the wide neural network model that we built in the
    previous section. The only difference between the two neural network models is
    how we structure or architect them.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Training deep neural network model
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Take a look at the following code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As before, we are using `keras.Sequential` to build a model, but this time
    we have more layers between the first `Input` layer and the final output `Dense`
    layer:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: We have four hidden layers with 128 neurons each with the ReLU activation function.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also two `Dropout` layers between the last and the second last hidden
    layers and between the last hidden layer and the output layer.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to the previous wide neural network model, each layer has a smaller
    number of neurons but it has a larger number of layers. As you can see from this
    example, the model architecture is up to the person building the model and should
    be based on the performance. We suggest experimenting with different architectures
    of the model and analyzing how it changes the model performances.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Using the same Keras model training code we used previously for the wide neural
    network model, you can train this deep neural network model as well. Make sure
    you have the callback function to save the best model so that we can use it to
    evaluate the model’s performance.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前用于宽神经网络模型的相同 Keras 模型训练代码，你也可以训练这个深度神经网络模型。确保你有保存最佳模型的回调函数，这样我们就可以用它来评估模型的表现。
- en: Predicting and evaluating the deep neural network model
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测和评估深度神经网络模型
- en: 'As before, you can use the following code to make predictions based on the
    best model found from training the deep neural network model:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你可以使用以下代码根据从训练深度神经网络模型中找到的最佳模型进行预测：
- en: '[PRE25]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'With the predicted probabilities, you can run the same evaluation metrics and
    charts. Due to the randomness in the models, the results may vary slightly each
    time you train the models. At the time of this run, the results looked as in the
    following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预测概率，你可以运行相同的评估指标和图表。由于模型中的随机性，每次训练模型时结果可能会有所不同。在这次运行时，结果如下所示：
- en: '![](img/B30999_06_17.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_06_17.png)'
- en: 'Figure 6.17: Summary of the evaluation results for the deep neural network
    model'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17：深度神经网络模型评估结果总结
- en: Compared to the wide neural network model we have built previously, the deep
    neural network model performs slightly worse. The AUC is about the same, but the
    precision and recall are lower than those with the wide neural network. These
    evaluation metrics and plots help examine the model performances at the facial
    value and at the training time. For more practical evaluations between models,
    you may want to consider running A/B testing to choose the final model for your
    marketing campaigns.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前构建的宽神经网络模型相比，深度神经网络模型的表现略差。AUC 值大致相同，但精确率和召回率低于宽神经网络模型。这些评估指标和图表有助于检查模型在表面价值和训练时间时的表现。对于模型之间更实际的评估，你可能想要考虑运行
    A/B 测试来选择你营销活动的最终模型。
- en: Conducting A/B testing for optimal model choice
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行 A/B 测试以选择最佳模型
- en: '**A/B testing**, simply put, compares two versions of features or models to
    identify which one is better. It plays a critical role in decision-making processes
    across various industries. Web developers may use A/B testing to test which of
    the two versions of the app performs better. Marketers may use A/B testing to
    test which version of the marketing messages may do better in engaging potential
    customers. Similarly, A/B testing can be used to compare two different models
    in terms of their performance and effectiveness. In our example in this chapter,
    we can use A/B testing to choose which of the models we have built based on our
    train and test sets may work the best in the actual real-world setting.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**A/B 测试**，简单来说，就是比较两个版本的功能或模型，以确定哪一个更好。它在各个行业的决策过程中发挥着关键作用。网页开发者可能会使用 A/B
    测试来测试两个版本的软件哪个表现更好。营销人员可能会使用 A/B 测试来测试哪种营销信息版本可能更有效地吸引潜在客户。同样，A/B 测试可以用来比较两个不同模型在性能和有效性方面的差异。在本章的例子中，我们可以使用
    A/B 测试来选择我们基于训练和测试集构建的模型中哪一个在实际的真实世界环境中可能表现最佳。'
- en: A/B testing is typically conducted across a predefined set of periods or until
    a predefined number of samples are collected. This is to ensure you have enough
    samples collected to make your decisions based on. For example, you may want to
    run A/B testing for two weeks and collect the results for the duration of the
    test at the end of the two weeks and analyze the results. Or, you may want to
    set the target of 2,000 samples for both A and B scenarios without setting a certain
    period.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: A/B 测试通常在一个预定义的时期内进行，或者直到收集到预定义数量的样本。这是为了确保你有足够的样本来基于它们做出决策。例如，你可能想要进行为期两周的
    A/B 测试，并在两周结束时收集测试期间的结果进行分析。或者，你可能想要为 A 和 B 两种场景设定 2,000 个样本的目标，而不设定特定的时期。
- en: You may also want to set both the period and the number of samples as the target,
    whichever comes first. For example, you set the sample target at 2,000 samples
    and a 2-week timebox; if you end up collecting 2,000 samples before 2 weeks, you
    may want to terminate the test sooner as you have collected enough samples to
    analyze the results. The period and sample size for A/B testing should be determined
    based on the business needs or limitations and confidence in the test results.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Examining the results of A/B testing is typically done with statistical hypothesis
    testing. The **two-sample t-test** is frequently used to determine whether the
    differences observed in A and B cases of the A/B testing are statistically significant
    or not. There are two important statistics in a t-test – the **t-value** and **p-value**:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: The **t-value** measures the degree of difference between the two means relative
    to the variances of the two populations. The larger the **t-value** is, the more
    different the two groups are.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The equation to get the **t-value** for the two-sample **t-test** is as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_005.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: '*M*[1] and *M*[2] are the averages, *S*[1] and *S*[2] are the standard deviations,
    and *N*[1] and *N*[2] are the number of samples in groups 1 and 2\. As you may
    infer from this equation, the large negative **t-value** will suggest the average
    of group 2 is significantly larger than the average of group 1 and the large positive
    t-value will suggest the average of group 1 is significantly larger than the average
    of group 2\. This is also called a two-tailed **t-test**.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The **p-value**, on the other hand, measures the probability that the results
    occur by chance. Thus, the smaller the **p-value** is, the more statistically
    significant that the two groups are different and the difference is not by a mere
    chance.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulating A/B Testing
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to simulate A/B testing as if we are running this experiment live
    with our models. For illustration purposes, we are going to run A/B testing on
    the XGBoost or GBDT model and the wide neural network model that we have built
    previously. We are going to run this A/B test for 1,000 samples for each scenario
    and analyze the results to see which one of the two models may work better for
    our marketing campaign to maximize customer conversion rates.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we are going to randomly route the incoming traffic to group A (GBDT
    model) and group B (wide neural network model). Take a look at the following code:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, we are using the `numpy` package’s `random.choice` function to randomly
    select 1,000 items from 2,000 items. We route these to group A and the rest to
    group B. Then, we simulate the A/B test as in the following code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see from this code, for the first 2,000 customers, we route half
    to group A and the rest to group B. The group A customers are predicted with their
    likelihood of conversions by the XGBoost or GBDT model. The group B customers
    are predicted with their likelihood of conversions by the wide neural network
    model.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we are going to examine the actual outcomes of these predictions. In
    the actual test setting, these outcomes may come days or weeks after the predictions
    were made, as it takes time for customers to decide whether to purchase or not.
    We are going to evaluate the actual conversions of those customers who are predicted
    to convert and the missed opportunities, which are the conversions of the customers
    who are predicted not to convert. Take a look at the following code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As you can see, we are counting the cumulative number of conversions for those
    customers who are predicted to convert and also counting the number of conversions
    for those customers who are predicted not to convert but have converted. These
    are essentially missed opportunities as we would not have sent our marketing messages
    to these customers but they would have converted. We are going to aggregate these
    results for each group, using the following code:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, let’s look at the cumulative conversion rates over time with the following
    code:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will produce a chart that looks like the following:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_18.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18: Cumulative conversion rates over time for groups A and B'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'As this chart suggests, group B, who are treated with the wide neural network
    model predictions, shows overall higher conversion rates, compared to group A,
    who are treated with the XGBoost model predictions. Let’s also take a look at
    how each group does for the missed opportunities with the following code:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'When you run this code, you should get a chart that looks similar to the following:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_19.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: Cumulative missed opportunity rates over time for groups A and
    B'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: This chart suggests that the missed opportunities are higher for group A with
    the XGBoost model predictions than group B with the wide neural network model
    predictions.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: These visuals are a great way to examine the performances of different groups
    or models. However, to validate whether these differences are statistically significant,
    we will have to run the t-test.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Two-tailed T-test
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `scipy` package in Python provides a handy tool to compute the t-value and
    p-value for a two-tailed t-test.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `scipy` on your machine, run the following command in your terminal:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Once you have installed the `scipy` package, you can run the following commands
    to examine the statistical significance of the differences in the conversion and
    missed opportunity rates between groups A and B:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As you can see from this code, the `ttest_ind` function from the `scipy.stats`
    module lets you easily get the t-value and p-value for the two-tailed t-test between
    two groups. The output of this code should look similar to the following:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_20.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: Two-tailed t-test results'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at this output. First, the t-value and p-value for
    the conversion rate t-test are `-13.168` and `0`. This suggests that the difference
    in the conversion rates between the two groups is statistically significant. The
    negative value of the t-test suggests that the mean of group A is smaller than
    that of group B.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Since from the p-value, we have concluded that the difference is significant,
    this translates to the result that group A’s conversion rate is significantly
    lower than that of group B. In other words, the wide neural network model performs
    significantly better in capturing the high conversion likely customers than the
    XGBoost model.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the t-value and p-value for the missed opportunity rate t-test are
    `2.418` and `0.016`. This suggests that the mean of group A is significantly larger
    than that of group B. This translates to the result that group A’s missed opportunity
    is significantly larger than that of group B. In other words, the XGBoost model
    results in missing more opportunities than the wide neural network model.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this A/B testing simulation results, A/B testing provides
    powerful insights into which model performs better in the real-world setting.
    Everyone building data science or AI/ML models should develop a habit of not only
    evaluating the models based on the train and test sets but also evaluating the
    models via A/B testing so that they can tell how realistic and applicable the
    models being built are in a real-world setting. Through a short period of small
    sample A/B testing, you can choose the optimal model to use for the upcoming marketing
    campaign.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered a lot about building predictive models using
    an Online Purchase dataset. We have explored two different tree-based models,
    random forest and GBDT, and how to build predictive models to forecast who is
    likely to convert. Using the same example, we have also discussed how we can build
    neural network models that are the backbone of deep learning models. There is
    great flexibility in how you architect the neural network model, such as wide
    network, deep network, or wide and deep network. We have briefly touched on the
    activation functions and optimizers while building neural network models, but
    we suggest you do some more in-depth research into how they affect the performances
    of neural network models. Lastly, we have discussed what A/B testing is, how to
    conduct A/B testing, and how to interpret the A/B testing results. We have simulated
    A/B testing with the models we built for a scenario where we want to choose the
    best model for capturing the most amount of customer conversions.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we are going to expand further on targeted marketing
    using AI/ML. More specifically, we will discuss how to build personalized product
    recommendations in various ways and how this can lead to micro-targeted marketing
    strategies.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/genai](https://packt.link/genai)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code12856128601808671.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
