- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leveraging Predictive Analytics and A/B Testing for Customer Engagement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are various ways that we can benefit from data-driven and AI/ML-driven
    marketing techniques. To name a few, you can optimize your marketing strategy
    based on the key drivers behind the success and failures of your previous marketing
    campaigns, as we discussed in *Chapters 2* and *3*. You can also optimize your
    marketing strategy based on the trend and seasonality within your business or
    based on the customer sentiments around your products and business, as we have
    discussed in *Chapters 4* and *5*. Targeted product recommendation (*Chapter 7*)
    and optimizing the marketing content with Generative AI (*Chapters 9* and *10*)
    are some other key benefits of applying AI/ML in marketing.
  prefs: []
  type: TYPE_NORMAL
- en: Among those mentioned, we are going to experiment with predictive analytics
    in this chapter and how you can utilize these predictive models in your next marketing
    campaign. By intelligently predicting the expected behaviors of customers, you
    can target subgroups of the customer base that are likely to result in your favor.
    This way, instead of mass marketing to the entire potential customer base, you
    can better custom-tailor your marketing messages and also save on marketing costs
    as you are only targeting the group with the higher chance of success. We will
    also discuss how A/B testing can help decide the best predictive model for your
    next marketing effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting customer conversion with tree-based algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting customer conversion with deep learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting A/B testing for optimal model choice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting customer conversion with tree-based algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Predictive analytics or modeling can be applied at various stages of the customer
    life cycle. If you recall from *Chapter 2*, there are largely five stages that
    we can break down a customer life cycle into: **Awareness**, **Engagement**, **Conversion**,
    **Retention**, and **Loyalty**, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Customer life cycle diagram from Chapter 2'
  prefs: []
  type: TYPE_NORMAL
- en: The applicability of predictive modeling is broad, depending on your marketing
    goal. For example, if you have a new brand or product launch and would like to
    improve new product awareness via ads on social media, you can build predictive
    models that can help you identify the target customers who are likely to click
    on the ads. On the other hand, if you would like to improve product purchase conversion
    rates, you can build predictive models that can identify customers who are more
    likely to make purchases in the next X number of days and target them. This results
    in more effective marketing, as you can avoid creating fatigue among the customers,
    which happens when they are exposed to your marketing campaigns too frequently
    with irrelevant content. This happens often when you do mass marketing without
    targeting the right subgroup of customers. Also, you can reduce marketing costs
    by sending marketing materials only to a specific subgroup of customers. This
    will help you repurpose the remaining marketing budget for other marketing campaigns.
  prefs: []
  type: TYPE_NORMAL
- en: Not only can you utilize predictive analytics for better brand awareness, engagement,
    and conversion, but predictive analytics can also be used to improve retention
    rates. Often, customer churn likelihood models are built to identify who is at
    risk of turning away from your business. Through these customer churn predictions,
    you can build marketing strategies and marketing content that is customized for
    this high-churn risk group to bring them back to engaged customers. Discounts,
    free subscription trials, or free plan upgrades are often offered to this high-churn
    risk group as part of the retention strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based machine learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Numerous AI/ML algorithms can be used for predictive modeling, such as linear
    regression, logistic regression, and decision tree models, which we have discussed
    in previous chapters, as well as deep learning models that are rising in usage.
    In this chapter, we are going to build predictive models with tree-based models,
    such as random forest and gradient boosted trees, and neural network models, which
    are the backbones of deep learning models. Underneath any tree-based ML model,
    there is a decision tree. As we have discussed in *Chapter 3*, a decision tree
    is like a flowchart, where it splits into child nodes based on the information
    gained. Each node represents a question or criteria for a split and each branch
    or edge represents the outcome of the question posited at the node. The following
    diagram shows a high-level overview of how a decision tree may be built:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Illustration of a decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: Among numerous tree-based ML models, the **gradient-boosted decision tree**
    (**GBDT**) and random forest are the two most popular models that are frequently
    used for predictive modeling. Both GBDT and random forest models are built with
    multiple decision trees. However, the main difference is how these decision trees
    are built.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, a random forest model is one with lots of decision trees, where
    each decision tree is built with a random subsample of the dataset and a random
    subset of features. This way, each decision tree within a random forest learns
    the information or relationships within the data slightly differently and with
    different focus areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final prediction is the average of all the outcomes or predictions of these
    individual decision trees. The following shows an illustration of a random forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Illustration of random forest'
  prefs: []
  type: TYPE_NORMAL
- en: 'A GBDT model, on the other hand, also consists of lots of decision trees, but
    each decision tree is built sequentially, and each subsequent decision tree is
    trained based on the errors that the previous decision tree makes. The final prediction
    of a GBDT model is the weighted average of all of the individual decision trees’
    predictions. The following shows an illustration of a GBDT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: Illustration of GBDTs'
  prefs: []
  type: TYPE_NORMAL
- en: Building random forest models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will be using an online purchase dataset as an example to
    build a predictive model to predict whether a customer will convert or not. First,
    we will discuss how we can build a random forest model in Python using the `scikit-learn`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: '**Source code and data**: [https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.6](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.6)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data source**: [https://archive.ics.uci.edu/dataset/468/online+shoppers +purchasing+intention+dataset](https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: Target and feature variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to first define the target and feature variables, where the target
    variable is the factor that we want to predict, and the feature variables are
    the factors that will be learned by the models to make the predictions or decisions.
    To do this, you can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first load the data into a DataFrame and examine what features we can
    use for our random forest model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When you run this code, you should see the following output for the information
    about this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Summary of the example dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to note in the preceding output is the column, `Revenue`, which
    is the target variable that tells us whether a customer made a purchase or converted
    or not, has a type of `Boolean`. The other column, `Weekend`, also has a `Boolean`
    data type. We are going to encode them as `0` for False and `1` for True with
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, two columns have an `object` as the data type, `Month` and `VisitorType`.
    If you look closer, the column, `Month`, has string values for the months, which
    we will convert into corresponding month numbers. The column, `VisitorType`, has
    three unique values, `New_Visitor`, `Returning_Visitor`, and `Other`. We are going
    to encode each as `0`, `1`, and `2` respectively, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see in this code, we are using the `strptime` function of a `time`
    module to encode three-letter month string values into corresponding month numbers.
    Then, we use the `apply` function of a `pandas` DataFrame to encode each value
    of the `VisitorType` column into corresponding integer values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have converted all the column values into numeric values, we are
    going to define the target and feature variables as in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see from this code, we have defined the target variable, `TARGET`,
    to use the `Revenue` column and the rest columns as the feature variables, `FEATURES`.
    Then, we create a DataFrame, `X`, which is the feature set, and `Y`, which is
    the target series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we will split these target and feature sets into train and test sets
    with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are using the `train_test_split` function within the `sklearn.model_selection`
    module. As you can see from the `test_size` parameter, we are using 80% of the
    dataset for training and the other 20% for testing.
  prefs: []
  type: TYPE_NORMAL
- en: With these train and test sets, we are now ready to train a random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: Training a random forest model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Python’s `scikit-learn` package provides a handy way to a random forest model.
    Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a closer look at this code. We are using the `RandomForestClassifier`
    class from the `sklearn.ensemble` module and initiated a Random Forest model with
    `n_estimators, max_depth, class_weight,` and `n_jobs` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The `n_estimators` parameter defines how many individual decision trees to build.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `max_depth` parameter defines how deep each decision tree can grow. Along
    with other parameters, such as `min_samples_split`, `min_samples_leaf`, and `max_features`,
    `max_depth` helps prevent overfitting issues by limiting how much a decision tree
    can grow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `class_weight` parameter defines weights for each class. This parameter
    is useful when the dataset is imbalanced. In our example dataset, only about 15%
    are in the positive class, meaning only 15% of the target variable, Revenue, has
    a value of 1, or only 15% of customers have converted. You can give custom weights
    for each class as a dictionary or use the `"balanced"` option to automatically
    adjust the weights that are inversely proportional to the actual class frequencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, the `n_jobs` parameter defines how many jobs to run in parallel. If
    you recall from our discussion of random forest and GBDTs, random forest is a
    bag of decision trees, so individual trees can be built in parallel without any
    dependency on other trees. By giving `-1` as the input for this parameter, you
    are instructing it to use all available resources to train this random forest
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these parameters, we can now train this random forest model, using the
    `fit` function with the train set.
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting versus underfitting?**'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting refers to when models are fit to the training set so closely that
    it does well on the training data but poorly on the data that the models have
    not seen before. Underfitting, on the other hand, is when models are over-generalized
    or do not tune well enough to the training set that they did not learn the relationships
    between the feature variables and the target variable. Multiple iterations of
    hyperparameter tuning are often required to find the sweet spot for minimal overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting and evaluating random forest model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `RandomForestClassifier` object provides handy functions for making predictions
    from the trained random forest model. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As the names suggest, the `predict` function makes predictions on the given
    input. In our case, the results will be a list of 0s and 1s for each record in
    the test set, as we are predicting whether a customer has converted or not.
  prefs: []
  type: TYPE_NORMAL
- en: The `predict_proba` function also makes predictions on the given input, but
    the difference is it gives predicted probabilities running between `0` and `1`.
    It returns predicted probabilities for each record and for each class, so, in
    our case, it returns two values for each record, where the first element is a
    predicted probability to be a class of `0` and the second element is a predicted
    probability to be a class of `1`. Since we are only interested in the predicted
    probability of class 1, we are slicing it with `[:,1]` so that we have a list
    of predicted probabilities of conversion.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the predicted conversion probabilities, we need to evaluate
    how good our predictions are.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to evaluate the accuracy and effectiveness of a predictive
    model, but we will mainly look at the overall accuracy, precision, recall, **area
    under the curve** (**AUC**) - **receiver operating characteristics** (**ROC**)
    curve, and confusion matrix. We will go deeper into these metrics with examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name suggests, the **accuracy** is the percentage of correct predictions
    or **true positives** (**TP**) among all the predictions. The **precision** is
    the percentage of correct predictions among those predictive positive or the percentage
    of TPs among those predicted to be positive that include **false positives** (**FP**).
    The **recall** is the percentage of positive cases identified by the model or
    the percentage of TPs among the actual positives, which are TPs and false negatives.
    The equations for the accuracy, precision, and recall are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_001.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B30999_06_002.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B30999_06_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In Python, the scikit-learn package provides a handy tool for computing these
    metrics, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, when these codes are run, the results of these key metrics
    look as in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Summary of random forest model performance metrics'
  prefs: []
  type: TYPE_NORMAL
- en: These results suggest the random forest model we have trained has decent overall
    accuracy and recall rates but does not seem to do well with precision. This suggests
    that of those customers this model predicted to be positive or likely to convert,
    only about 54% of them have actually converted. However, if you recall, the actual
    overall conversion rate is 15%; in other words, if you randomly guess who will
    convert, you may only be right about 15% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, since this model has predicted converted customers 54% of the time, it
    proves to be way much more effective in selecting customers that are more likely
    to convert than random guesses. Also, the high recall rate suggests that about
    85% of the customers who have actually converted are among those who are predicted
    to be highly likely to convert by this model.
  prefs: []
  type: TYPE_NORMAL
- en: From an actual marketing perspective, if you have marketed only to the customers
    who have been predicted to be likely to convert by this model, you would have
    still captured most of those conversions. Also, if you have marketed to your entire
    customer base, 85% (100% minus 15%, which is the overall conversion rate) of your
    marketing spend would have been wasted. But if you have marketed only to these
    highly likely customers, only about 46% (100% minus 54%, which is the precision
    of this model) of the marketing spend would have been wasted.
  prefs: []
  type: TYPE_NORMAL
- en: The other key evaluation metric we are going to look at is the **AUC - ROC curve**.
    The **ROC curve**, simply put, shows the trade-offs between gains in **true positive
    rates** (**TPRs**) for each sacrifice you make for a **false positive rate** (**FPR**).
    The **AUC** is, as the name suggests, the area under the ROC curve and tells us
    how well the model separates the positive cases from negative cases. The AUC ranges
    from `0` to `1` and the higher it is, the better the model is. At the AUC of `0.5`,
    it suggests that the model performs the same as random guessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code can be used to plot the ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are using the `metrics` module again to plot the ROC curve. The main
    difference between computing the accuracy, precision, and recall and computing
    the AUC - ROC curve is how we use the predicted probabilities, `rf_pred_proba`,
    instead of predicted labels, `pred`. This is because the ROC curve examines how
    TPRs and FPRs change at different probability levels. By using predicted probabilities,
    we can calculate how the TPR and FPR change as the decision threshold varies.
    The resulting chart looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: AUC - ROC curve of the random forest model predictions'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this chart, you can easily evaluate how each sacrifice in
    the FPR affects the TPR. For example, in this chart, at 20% FPR, we already achieve
    about 90% TPR, which suggests that this model does well separating positive cases
    from negative cases. The AUC here is `0.93`, which also suggests that the model
    does well in identifying positive cases from negative cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we will look at the **confusion matrix**. As the name suggests, the
    confusion matrix is a good way to look at how and where the model gets confused
    the most. It will be easier to understand with an example. Take a look at the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to before, we are using the `metrics` module to build a confusion matrix.
    The `confusion_matrix` function takes the actual and predicted values and builds
    a confusion matrix. Then, we are using the `heatmap` function of the `seaborn`
    Python package to plot a heat map. The resulting chart looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Confusion matrix of the random forest model predictions'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this plot, the y-axis represents the actual classes and
    the x-axis represents the predicted classes. For example, the top left box is
    where the actual class was 0 or no conversion, and the predicted class was also
    `0`. The top right box is where the actual class was 0, but the model predicted
    them to be a class of `1` or conversion.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the confusion matrix shows you where the model is the most confused.
    A model that predicts the outcome with high accuracy will have large numbers or
    percentages in the diagonal boxes and small numbers in the other boxes.
  prefs: []
  type: TYPE_NORMAL
- en: We have experimented with predicting the customer conversion likelihood with
    a random forest model. Here, we have observed and discussed how this random forest
    predictive model can help target the subset of the customer base without losing
    too many converted customers and how this can result in much more cost-effective
    marketing strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosted decision tree (GBDT) modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the same dataset and train/test sets for building a GBDT model
    and compare its performance against the random forest model we have just built.
    XGBoost is the most commonly used library in Python for training a GBDT model.
    You can install this package using the following command in the terminal or Jupyter
    Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Training GBDT model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The XGBoost package follows the same pattern as the `scikit-learn` package.
    Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from this code, we initiate an `XGBClassifier` with the `n_estimators`,
    `max_depth`, and `scale_pos_weight` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the case of the random forest model, the `n_estimators` parameter
    defines how many individual decision trees are to be built. You may have noticed
    we are using a much lower number for this parameter. If you recall, a GBDT model
    is sequentially built where each subsequent decision tree learns the errors the
    previous decision tree makes. This often results in a smaller number of individual
    trees performing as well as or better than the random forest model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, because a GBDT model is sequentially built, a large number of decision
    trees results in much longer training time than a random forest model, which can
    build individual decision trees in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The other parameter, `max_depth`, is the same as in the case of the random forest
    model. This parameter restricts how deep each decision tree can grow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, the `scale_pos_weight` parameter defines the balance of the positive
    and negative class weights. As you may recall we have an imbalanced dataset where
    the positive class is only about 15% of the data. By giving inversely proportionate
    weight for the positive class with `1/train_y.mean()`, we are instructing the
    model to adjust for the imbalanced dataset. This enforces the GBDT model to penalize
    more for incorrect predictions of the positive class, which makes the model more
    sensitive to the positive class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are various ways to handle potential overfitting issues that often occur
    when you have large individual decision trees within a GBDT model. On top of the
    `max_depth` parameter, the XGBoost package provides various other parameters,
    such as `max_leaves`, `colsample_bytree`, `subsample`, and `gamma`, that can help
    reduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: We suggest that you look at the documentation and experiment with various parameters
    and see how they help you prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the documentation: [https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier).'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting and Evaluating GBDT Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to the `RandomForestClassifier` of the random forest model, the `XGBClassifier`
    object also provides the same syntax for predicting with the trained GBDT model.
    Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, you can use the `predict` function to predict
    the positive versus negative label for each record of the test set and the `predict_proba`
    function to predict the probabilities for each class of individual records of
    the test set. As before, we are retrieving the second column of the predicted
    probabilities by slicing the output with `[:,1]` to get the predicted probabilities
    of the positive class, which is the predicted probability of a conversion, for
    each record of the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare the performance of this GBDT model against the random forest model,
    we will use the same evaluation metrics and approaches. As you may recall, we
    can use the following code to get the key metrics of accuracy, precision, and
    recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Due to some randomness in building these trees, there can be some variances
    and differences each time a GBDT model is trained, but at the time of this writing,
    the results look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: Summary of GBDT model performance metrics'
  prefs: []
  type: TYPE_NORMAL
- en: These key metrics look very similar to the random forest model. The overall
    accuracy and precision of this GBDT model are slightly higher than the random
    forest model. However, the recall is slightly lower than the random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the AUC-ROC curve can be plotted using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting AUC-ROC Curve looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: AUC- ROC Curve of the GBDT model predictions'
  prefs: []
  type: TYPE_NORMAL
- en: When you compare this against the random forest model, the results are almost
    identical. AUC is about the same with `0.93` and at the FPR of `0.2`, the TPR
    of the GBDT model is also about `0.9`, which was also the case of the previously
    built random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let’s take a look at the confusion matrix with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting confusion matrix looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Confusion matrix of the GBDT model predictions'
  prefs: []
  type: TYPE_NORMAL
- en: When you compare this against the confusion matrix of the random forest model,
    you will notice that the TPs are lower with the GBDT model, but false negatives
    are also lower with the GBDT model. This is expected as we have seen that the
    precision of the GBDT model is higher, but recall is lower compared to the random
    forest model.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the performances of random forest and GBDT models are very similar
    and hard to distinguish from these examples. Depending on how you fine-tune your
    model, you may end up with a better random forest or GBDT model.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous ways and parameters you can fine-tune the tree-based models.
    We suggest you experiment with various sets of parameters and see how they affect
    the model’s performance!
  prefs: []
  type: TYPE_NORMAL
- en: Predicting customer conversion with deep learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has become a hot topic and its popularity and usage are rising,
    as deep learning models are proven to work well when data have complex relationships
    within the variables and learn or extract features autonomously from the data,
    even though tree-based models are also very frequently used and powerful for predictive
    modeling. We touched on deep learning in *Chapter 5* when we used pre-trained
    language models for sentiment analysis and classification. In this section, we
    are going to build on that knowledge and experiment with developing deep learning
    models for predictive modeling and, more specifically, for making predictions
    on which customers are likely to convert.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning is basically an **artificial neural network** (**ANN**) model
    with lots of hidden and complex layers of neurons, or, in other words, a deep
    ANN. An ANN is a model inspired by the biological neural networks in animal and
    human brains. An ANN learns the data through layers of interconnected neurons
    that resemble animal and human brains. The following diagram shows the high-level
    structure of an ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Example ANN architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'As this diagram shows, there are three layers in any deep learning or ANN model:
    the input layer, hidden layer, and output layer. Detailed explanations of how
    ANN models learn or build the hidden layer weights are beyond the scope of this
    book, but at a high level, they go through iterations of forward propagations
    and backward propagations:'
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation is where the data is fed through a network from the input
    layer to the output layer and the activation function is applied regardless of
    whether each neuron is activated or not and returns the output of each node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backward propagation is the process of moving from the output layer to the input
    layer and adjusting the weights of the network by analyzing the losses or errors
    from the previous iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through iterations of these forward and backward propagations, a neural network
    learns the weights of each neuron that minimizes the error of the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: A number of hidden layers and a number of neurons in each layer should be experimented
    with to find the right neural network architecture that works best for each case
    of predictive models. In this section, we are going to experiment with how wide
    neural network architecture performs against deep neural network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**When to use wide versus deep neural networks?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A wide neural network refers to an ANN model that has fewer layers but more
    neurons in each layer, whereas a deep neural network refers to an ANN model with
    lots of hidden layers. Aside from the model performance comparisons to see which
    architecture works better for your case, there are some key factors you may want
    to consider or limit which approach to take:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training time and compute resources**: As the number of layers grows, it
    takes more time to train as backpropagation through each layer is computationally
    intensive, which also results in higher compute costs. The wide neural network
    may have an advantage in shortening the training time and reducing the compute
    costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model interpretability**: Shallower architecture may offer better interpretability
    compared to deep neural networks. If explainability is a requirement, shallow
    architecture may be a better fit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ability to generalize**: Deep neural network models tend to be able to capture
    more abstract patterns through more layers and learn higher-order features compared
    to wide neural networks. This may result in the deep architecture model having
    better performance on new and unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train and test sets for deep learning models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the best performances of neural network models, you need to normalize the
    data before you train the models. We are going to use the same train and test
    sets that we used previously for tree-based models, but normalize them. Take a
    look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As this code suggests, we first get the mean and standard deviation from the
    train set and normalize both train and test sets by subtracting the mean and dividing
    by the standard deviation. After the standardization, the `normed_train_x` DataFrame
    should have means of `0` and standard deviations of `1` for all the columns.
  prefs: []
  type: TYPE_NORMAL
- en: Wide neural network modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will first look at building **wide neural network** models using the `keras`
    package in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `keras` on your machine, run the following command in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Training the wide neural network model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the normalized train and test sets, let’s start building neural network
    models. Take a look at the following code to see how a neural network model can
    be initiated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, we create a sequence of layers with the `Sequential`
    class. We start with the input layer with the `Input` class, where we define the
    shape or the number of input neurons to match the number of columns or features
    of the train set. Then, we have one wide hidden layer with 2,048 neurons. We added
    a `Dropout` layer, which defines what percentage of neurons to drop and this helps
    reduce the overfitting issues. We are instructing the neural network model to
    drop 20% of the neurons between the hidden and output layers. Lastly, we have
    one `Dense` layer for the output layer with one output neuron, which will output
    the predicted probability of the positive case or the likelihood of a conversion.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have noticed there are two activation functions we use for the hidden
    layer and the output layer. The **rectified linear unit** (**ReLU**) activation
    function is one of the most frequently used activation functions, which only activates
    the positive values and deactivates all the negative values. The equation for
    the ReLU activation function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ReLU*(*x*) = *max*(*0*, *x*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The behavior of the ReLU activation function looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: ReLU activation function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **sigmoid** function, on the other hand, turns values into a range of `0`
    and `1`. This makes the sigmoid function the top choice to predict the probability
    as an output. The equation for the sigmoid function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The behavior of the sigmoid function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Sigmoid activation function'
  prefs: []
  type: TYPE_NORMAL
- en: There are various other activation functions other than ReLU and sigmoid, such
    as tanh, leaky ReLU, and softmax. We suggest you do some research and experiment
    with how different activation functions work and affect the neural network models’
    performances!
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few more steps involved before we can train a neural network model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define the key metrics we will track through the iterations
    of model training and compile the model. Take a look at the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we have discussed previously with the tree-based models, we are going to
    track accuracy, precision, and recall. We will be using the Adam optimizer for
    this exercise. Simply put, optimizers are the algorithms that are used to update
    the weights of the network to minimize the errors based on the loss function,
    which in this case is `binary_crossentropy`, which measures the difference between
    the predicted and actual binary outcomes. Since our output or target variable
    for the prediction is a binary variable of conversion versus no conversion, `binary_crossentropy`
    will be the right fit for the loss function. For multi-class predictions where
    the target variable is not a binary variable and has more than two possible outcomes,
    `categorical_crossentropy` will be a better-suited loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to have a checkpoint to save the best model among all the
    iterations or epochs that we will run while training the model. Take a look at
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are using precision as the metric to monitor and store the best model
    to a local drive based on that metric value. The best model is going to be stored
    as defined in the `best_model_path` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it is now time to train this **wide neural network** model with the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similar to the class weights we have given to our tree-based models previously,
    we are also going to define the class weights that are inversely proportional
    to the class composition. As you can see from the code, we instruct the model
    to run for 30 iterations or epochs with a batch size of 64\. We register the callback
    that we created to store the best model previously with the custom class weights.
    When you run this code, it will run for 30 epochs and report the accuracy, precision,
    and recall metrics that we defined previously. The output of this code should
    look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Sample output of the neural network model'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting and evaluating wide neural network model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For making predictions from the model we have just trained, we first need to
    load the best model. You can use the following code to load the best model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Making predictions with this model is similar to the `scikit-learn` package’s
    syntax. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you may notice from this code, you can use the `predict` function of the
    `keras` model to get the predicted probabilities for each record. We use the `flatten`
    function to make the predicted probability output a list of predicted probabilities
    from a two-dimensional array. Then, for illustration purposes, we are going to
    assume any record with a predicted probability above `0.5` is considered to have
    a high chance of converting and assume they are positive cases. The probability
    threshold is another factor you can fine-tune for the final predictions of conversions
    versus no conversions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to use the same codes that we have used previously for the tree-based
    models for key metrics, the AUC-ROC curve, and the confusion matrix. The results
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Summary of the evaluation results for the wide neural network
    model'
  prefs: []
  type: TYPE_NORMAL
- en: If you compare these results against the tree-based models, you will notice
    the wide neural network model performed a little worse than the tree-based models.
    The precision and recall are slightly lower and the AUC is also a little lower
    compared to those of the tree-based models. We will go deeper into the practical
    uses of these models and how to choose the best model to use for marketing campaigns
    later when we discuss A/B testing in more depth, but the model performances based
    on the train and test sets look better for the tree-based models.
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural network modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss how to build deep neural network models and
    compare the results against the wide neural network model that we built in the
    previous section. The only difference between the two neural network models is
    how we structure or architect them.
  prefs: []
  type: TYPE_NORMAL
- en: Training deep neural network model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we are using `keras.Sequential` to build a model, but this time
    we have more layers between the first `Input` layer and the final output `Dense`
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: We have four hidden layers with 128 neurons each with the ReLU activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also two `Dropout` layers between the last and the second last hidden
    layers and between the last hidden layer and the output layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to the previous wide neural network model, each layer has a smaller
    number of neurons but it has a larger number of layers. As you can see from this
    example, the model architecture is up to the person building the model and should
    be based on the performance. We suggest experimenting with different architectures
    of the model and analyzing how it changes the model performances.
  prefs: []
  type: TYPE_NORMAL
- en: Using the same Keras model training code we used previously for the wide neural
    network model, you can train this deep neural network model as well. Make sure
    you have the callback function to save the best model so that we can use it to
    evaluate the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting and evaluating the deep neural network model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As before, you can use the following code to make predictions based on the
    best model found from training the deep neural network model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'With the predicted probabilities, you can run the same evaluation metrics and
    charts. Due to the randomness in the models, the results may vary slightly each
    time you train the models. At the time of this run, the results looked as in the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.17: Summary of the evaluation results for the deep neural network
    model'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the wide neural network model we have built previously, the deep
    neural network model performs slightly worse. The AUC is about the same, but the
    precision and recall are lower than those with the wide neural network. These
    evaluation metrics and plots help examine the model performances at the facial
    value and at the training time. For more practical evaluations between models,
    you may want to consider running A/B testing to choose the final model for your
    marketing campaigns.
  prefs: []
  type: TYPE_NORMAL
- en: Conducting A/B testing for optimal model choice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**A/B testing**, simply put, compares two versions of features or models to
    identify which one is better. It plays a critical role in decision-making processes
    across various industries. Web developers may use A/B testing to test which of
    the two versions of the app performs better. Marketers may use A/B testing to
    test which version of the marketing messages may do better in engaging potential
    customers. Similarly, A/B testing can be used to compare two different models
    in terms of their performance and effectiveness. In our example in this chapter,
    we can use A/B testing to choose which of the models we have built based on our
    train and test sets may work the best in the actual real-world setting.'
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing is typically conducted across a predefined set of periods or until
    a predefined number of samples are collected. This is to ensure you have enough
    samples collected to make your decisions based on. For example, you may want to
    run A/B testing for two weeks and collect the results for the duration of the
    test at the end of the two weeks and analyze the results. Or, you may want to
    set the target of 2,000 samples for both A and B scenarios without setting a certain
    period.
  prefs: []
  type: TYPE_NORMAL
- en: You may also want to set both the period and the number of samples as the target,
    whichever comes first. For example, you set the sample target at 2,000 samples
    and a 2-week timebox; if you end up collecting 2,000 samples before 2 weeks, you
    may want to terminate the test sooner as you have collected enough samples to
    analyze the results. The period and sample size for A/B testing should be determined
    based on the business needs or limitations and confidence in the test results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examining the results of A/B testing is typically done with statistical hypothesis
    testing. The **two-sample t-test** is frequently used to determine whether the
    differences observed in A and B cases of the A/B testing are statistically significant
    or not. There are two important statistics in a t-test – the **t-value** and **p-value**:'
  prefs: []
  type: TYPE_NORMAL
- en: The **t-value** measures the degree of difference between the two means relative
    to the variances of the two populations. The larger the **t-value** is, the more
    different the two groups are.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The equation to get the **t-value** for the two-sample **t-test** is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_005.png)'
  prefs: []
  type: TYPE_IMG
- en: '*M*[1] and *M*[2] are the averages, *S*[1] and *S*[2] are the standard deviations,
    and *N*[1] and *N*[2] are the number of samples in groups 1 and 2\. As you may
    infer from this equation, the large negative **t-value** will suggest the average
    of group 2 is significantly larger than the average of group 1 and the large positive
    t-value will suggest the average of group 1 is significantly larger than the average
    of group 2\. This is also called a two-tailed **t-test**.'
  prefs: []
  type: TYPE_NORMAL
- en: The **p-value**, on the other hand, measures the probability that the results
    occur by chance. Thus, the smaller the **p-value** is, the more statistically
    significant that the two groups are different and the difference is not by a mere
    chance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulating A/B Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to simulate A/B testing as if we are running this experiment live
    with our models. For illustration purposes, we are going to run A/B testing on
    the XGBoost or GBDT model and the wide neural network model that we have built
    previously. We are going to run this A/B test for 1,000 samples for each scenario
    and analyze the results to see which one of the two models may work better for
    our marketing campaign to maximize customer conversion rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we are going to randomly route the incoming traffic to group A (GBDT
    model) and group B (wide neural network model). Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are using the `numpy` package’s `random.choice` function to randomly
    select 1,000 items from 2,000 items. We route these to group A and the rest to
    group B. Then, we simulate the A/B test as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, for the first 2,000 customers, we route half
    to group A and the rest to group B. The group A customers are predicted with their
    likelihood of conversions by the XGBoost or GBDT model. The group B customers
    are predicted with their likelihood of conversions by the wide neural network
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we are going to examine the actual outcomes of these predictions. In
    the actual test setting, these outcomes may come days or weeks after the predictions
    were made, as it takes time for customers to decide whether to purchase or not.
    We are going to evaluate the actual conversions of those customers who are predicted
    to convert and the missed opportunities, which are the conversions of the customers
    who are predicted not to convert. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we are counting the cumulative number of conversions for those
    customers who are predicted to convert and also counting the number of conversions
    for those customers who are predicted not to convert but have converted. These
    are essentially missed opportunities as we would not have sent our marketing messages
    to these customers but they would have converted. We are going to aggregate these
    results for each group, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s look at the cumulative conversion rates over time with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce a chart that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18: Cumulative conversion rates over time for groups A and B'
  prefs: []
  type: TYPE_NORMAL
- en: 'As this chart suggests, group B, who are treated with the wide neural network
    model predictions, shows overall higher conversion rates, compared to group A,
    who are treated with the XGBoost model predictions. Let’s also take a look at
    how each group does for the missed opportunities with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you should get a chart that looks similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: Cumulative missed opportunity rates over time for groups A and
    B'
  prefs: []
  type: TYPE_NORMAL
- en: This chart suggests that the missed opportunities are higher for group A with
    the XGBoost model predictions than group B with the wide neural network model
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: These visuals are a great way to examine the performances of different groups
    or models. However, to validate whether these differences are statistically significant,
    we will have to run the t-test.
  prefs: []
  type: TYPE_NORMAL
- en: Two-tailed T-test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `scipy` package in Python provides a handy tool to compute the t-value and
    p-value for a two-tailed t-test.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `scipy` on your machine, run the following command in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have installed the `scipy` package, you can run the following commands
    to examine the statistical significance of the differences in the conversion and
    missed opportunity rates between groups A and B:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from this code, the `ttest_ind` function from the `scipy.stats`
    module lets you easily get the t-value and p-value for the two-tailed t-test between
    two groups. The output of this code should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_06_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: Two-tailed t-test results'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at this output. First, the t-value and p-value for
    the conversion rate t-test are `-13.168` and `0`. This suggests that the difference
    in the conversion rates between the two groups is statistically significant. The
    negative value of the t-test suggests that the mean of group A is smaller than
    that of group B.
  prefs: []
  type: TYPE_NORMAL
- en: Since from the p-value, we have concluded that the difference is significant,
    this translates to the result that group A’s conversion rate is significantly
    lower than that of group B. In other words, the wide neural network model performs
    significantly better in capturing the high conversion likely customers than the
    XGBoost model.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the t-value and p-value for the missed opportunity rate t-test are
    `2.418` and `0.016`. This suggests that the mean of group A is significantly larger
    than that of group B. This translates to the result that group A’s missed opportunity
    is significantly larger than that of group B. In other words, the XGBoost model
    results in missing more opportunities than the wide neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this A/B testing simulation results, A/B testing provides
    powerful insights into which model performs better in the real-world setting.
    Everyone building data science or AI/ML models should develop a habit of not only
    evaluating the models based on the train and test sets but also evaluating the
    models via A/B testing so that they can tell how realistic and applicable the
    models being built are in a real-world setting. Through a short period of small
    sample A/B testing, you can choose the optimal model to use for the upcoming marketing
    campaign.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered a lot about building predictive models using
    an Online Purchase dataset. We have explored two different tree-based models,
    random forest and GBDT, and how to build predictive models to forecast who is
    likely to convert. Using the same example, we have also discussed how we can build
    neural network models that are the backbone of deep learning models. There is
    great flexibility in how you architect the neural network model, such as wide
    network, deep network, or wide and deep network. We have briefly touched on the
    activation functions and optimizers while building neural network models, but
    we suggest you do some more in-depth research into how they affect the performances
    of neural network models. Lastly, we have discussed what A/B testing is, how to
    conduct A/B testing, and how to interpret the A/B testing results. We have simulated
    A/B testing with the models we built for a scenario where we want to choose the
    best model for capturing the most amount of customer conversions.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we are going to expand further on targeted marketing
    using AI/ML. More specifically, we will discuss how to build personalized product
    recommendations in various ways and how this can lead to micro-targeted marketing
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/genai](https://packt.link/genai)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code12856128601808671.png)'
  prefs: []
  type: TYPE_IMG
