<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Employee Attrition Using Ensemble Models</h1>
                </header>
            
            <article>
                
<p class="mce-root">If you reviewed the recent machine learning competitions, one key observation I am sure you would make is that the recipes of all three winning entries in most of the competitions include very good feature engineering, along with well-tuned ensemble models. One conclusion I derive from this observation is that good feature engineering and building well-performing models are two areas that should be given equal emphasis in order to deliver successful machine learning solutions.</p>
<p class="mce-root">While feature engineering most times is something that is dependent on the creativity and domain expertise of the person building the model, building a well-performing model is something that can be achieved through a philosophy called <strong class="calibre3">ensembling</strong>. Machine learning practitioners often use ensembling techniques to beat the performance benchmarks yielded by even the best performing individual ML algorithm. In this chapter, we will learn about the following topics of this exciting area of ML:</p>
<ul class="calibre9">
<li class="calibre10">Philosophy behind ensembling </li>
<li class="calibre10">Understanding the attrition problem and the dataset</li>
<li class="calibre10">K-nearest neighbors model for benchmarking the performance</li>
<li class="calibre10">Bagging</li>
<li class="calibre10">Randomization with random forests</li>
<li class="calibre10">Boosting</li>
<li class="calibre10">Stacking</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Philosophy behind ensembling </h1>
                </header>
            
            <article>
                
<p class="mce-root">Ensembling, which is super-famous among ML practitioners, can be well-understood through a simple real-world, non-ML example.</p>
<p class="mce-root">Assume that you have applied for a job in a very reputable corporate organization and you have been called for an interview. It is unlikely you will be selected for a job just based on one interview with an interviewer. In most cases, you will go through multiple rounds of interviews with several interviewers or with a panel of interviewers. The expectation from the organization is that each of the interviewers is an expert on a particular area and that the interviewer has evaluated your fitness for the job based on your experience in the interviewers' area of expertise. Your selection for the job, of course, depends on consolidated feedback from all of the interviewers that talked to you. The organization deems that you will be more successful in the job as your selection is based on a consolidated decision made by multiple experts and not just based on one expert's decision, which may be prone to certain biases. </p>
<p class="mce-root">Now, when we talk about the consolidation of feedback from all the interviewers, the consolidation can happen through several methods:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Averaging</strong>: Assume that your candidature for the job is based on you clearing a cut-off score in the interviews. Assume that you have met ten interviewers and each one of them have rated you on a maximum score of 10 which represents your experience as perceived by interviewers in his area of expertise. Now, your consolidated score is made by simply averaging all your scores given by all the interviewers.</li>
</ul>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Majority vote</strong>: In this case, there is no actual score out of 10 which is provided by each of the interviewers. However, of the 10 interviewers, eight of them confirmed that you are a good fit for the position. Two interviewers said no to your candidature. You are selected for the job as the majority of the interviewers are happy with your interview performance. </li>
<li class="calibre10"><strong class="calibre1">Weighted average</strong>: Let's consider that four of the interviewers are experts in some minor skills that are good to have for the job you applied for. These are not mandatory skills needed for the position. You are interviewed by all 10 interviewers and each one of them have given you a score out of 10. Similar to the averaging method, in the weighted averaging method as well, your interviews final score is obtained by averaging the scores given by all interviewers.</li>
</ul>
<p class="calibre23">However, not all scores are treated equally to compute the final score. Each interview score is multiplied with a weight and a product is obtained. All the products thus obtained thereby are summed to obtain the final score. The weight for each interview is a function of the importance of the skill it tested in the candidate and the importance of that skill to do the job. It is obvious that a <em class="calibre15">good to have</em> skill for the job carries a lower weight when compared to a <em class="calibre15">must have</em> skill. The final score now inherently represents the proportion of mandatory skills that the candidate possesses and this has more influence on your selection. </p>
<p class="mce-root">Similar to the interviews analogy, ensembling in ML also produces models based on consolidated learning. The term <strong class="calibre3">consolidated learning</strong> essentially represents learning obtained through applying several ML algorithms or it is learning obtained from several data subsets that are part of a large dataset. Analogous to interviews, multiple models are learned from the application of ensembling technique. However, a final consolidation is arrived at regarding the prediction by means of applying one of the averaging, majority voting, or weighted averaging techniques on individual predictions made by each of the individual models. The models created from the application of an ensembling technique along with the prediction consolidation technique is typically termed as an <strong class="calibre3">ensemble</strong>.</p>
<p class="mce-root">Each ML algorithm is special and has a unique way to model the underlying training data. For example, a k-nearest neighbors algorithm learns by computing distances between the elements in dataset; naive Bayes learns by computing the probabilities of each attribute in the data belonging to a particular class. Multiple models may be created using different ML algorithms and predictions can be done by combining predictions of several ML algorithms. Similarly, when a dataset is partitioned to create subsets and if multiple models are trained using an algorithm each focusing on one dataset, each model is very focused and it is specialized in learning the characteristics of the subset of data it is trained on. In both cases, with models based on multiple algorithms and multiple subsets of data, when we combine the predictions of multiple models through consolidation, we get better predictions as we leverage multiple strengths that each model in an ensemble carry. This, otherwise, is not obtained when using a single model for predictions.</p>
<p class="mce-root">The crux of ensembling is that, better predictions are obtained when we combine the predictions of multiple models than just relying on one model for prediction. This is no different from the management philosophy that together we do better, which is otherwise termed as <strong class="calibre3">synergy</strong>! </p>
<p class="mce-root">Now that we understand the core philosophy behind ensembling, we are now ready to explore the different types of ensembling techniques. However, we will learn the ensembling techniques by implementing them in a project to predict the attrition of employees. As we already know, prior to building any ML project, it is very important to have a deep understanding of the problem and the data. Therefore, in the next section, we first focus on understanding the attrition problem at hand, then we study the dataset associated with the problem, and lastly we understand the properties of the dataset through exploratory data analysis (EDA). The key insights we obtain in this section come from a one-time exercise and will hold good for all the ensembling techniques we will apply in the later sections. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started</h1>
                </header>
            
            <article>
                
<p class="mce-root">To get started with this section, you will have to download the <kbd class="calibre11">WA_Fn-UseC_-HR-Employee-Attrition.csv</kbd> dataset from the GitHub link <span class="calibre4">for the code in this chapter.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the attrition problem and the dataset </h1>
                </header>
            
            <article>
                
<p class="mce-root">HR analytics helps with interpreting organizational data. It finds out the people-related trends in the data and helps the HR department take the appropriate steps to keep the organization running smoothly and profitably. Attrition in a corporate setup is one of the complex challenges that the people managers and HR personnel have to deal with. Interestingly, machine learning models can be deployed to predict potential attrition cases, thereby helping the appropriate HR personnel or people managers take the necessary steps to retain the employee.</p>
<p class="mce-root">In this chapter, we are going to build ML ensembles that will predict such potential cases of attrition. The job attrition dataset used for the project is a fictional dataset created by data scientists at IBM. The <kbd class="calibre11">rsample</kbd> library incorporates this dataset and we can make use of this dataset directly from the library.</p>
<p class="mce-root">It is a small dataset that has 1,470 records of 31 attributes. The description of the dataset can be obtained with the following code:</p>
<pre class="calibre16">setwd("~/Desktop/chapter 2") <br class="title-page-name"/>library(rsample) <br class="title-page-name"/>data(attrition) <br class="title-page-name"/>str(attrition) <br class="title-page-name"/>mydata&lt;-attrition </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">'data.frame':1470 obs. of  31 variables: <br class="title-page-name"/> $ Age                     : int  41 49 37 33 27 32 59 30 38 36 ... <br class="title-page-name"/> $ Attrition               : Factor w/ 2 levels "No","Yes": 2 1 2 1 1 1 1 1 1 1 .... <br class="title-page-name"/> $ BusinessTravel          : Factor w/ 3 levels "Non-Travel","Travel_Frequently",..: 3 2 3 2 3 2 3 3 2 3 ... <br class="title-page-name"/> $ DailyRate               : int  1102 279 1373 1392 591 1005 1324 1358 216 1299 ... <br class="title-page-name"/> $ Department              : Factor w/ 3 levels "Human_Resources",..: 3 2 2 2 2 2 2 2 2 2 ... <br class="title-page-name"/> $ DistanceFromHome        : int  1 8 2 3 2 2 3 24 23 27 ... <br class="title-page-name"/> $ Education               : Ord.factor w/ 5 levels "Below_College"&lt;..: 2 1 2 4 1 2 3 1 3 3 ... <br class="title-page-name"/> $ EducationField          : Factor w/ 6 levels "Human_Resources",..: 2 2 5 2 4 2 4 2 2 4 ... <br class="title-page-name"/> $ EnvironmentSatisfaction : Ord.factor w/ 4 levels "Low"&lt;"Medium"&lt;..: 2 3 4 4 1 4 3 4 4 3 ... <br class="title-page-name"/> $ Gender                  : Factor w/ 2 levels "Female","Male": 1 2 2 1 2 2 1 2 2 2 ... <br class="title-page-name"/> $ HourlyRate              : int  94 61 92 56 40 79 81 67 44 94 ... <br class="title-page-name"/> $ JobInvolvement          : Ord.factor w/ 4 levels "Low"&lt;"Medium"&lt;..: 3 2 2 3 3 3 4 3 2 3 ... <br class="title-page-name"/> $ JobLevel                : int  2 2 1 1 1 1 1 1 3 2 ... <br class="title-page-name"/> $ JobRole                 : Factor w/ 9 levels "Healthcare_Representative",..: 8 7 3 7 3 3 3 3 5 1 ... <br class="title-page-name"/> $ JobSatisfaction         : Ord.factor w/ 4 levels "Low"&lt;"Medium"&lt;..: 4 2 3 3 2 4 1 3 3 3 ... <br class="title-page-name"/> $ MaritalStatus           : Factor w/ 3 levels "Divorced","Married",..: 3 2 3 2 2 3 2 1 3 2 ... <br class="title-page-name"/> $ MonthlyIncome           : int  5993 5130 2090 2909 3468 3068 2670 2693 9526 5237 ... <br class="title-page-name"/> $ MonthlyRate             : int  19479 24907 2396 23159 16632 11864 9964 13335 8787 16577 ... <br class="title-page-name"/> $ NumCompaniesWorked      : int  8 1 6 1 9 0 4 1 0 6 ... <br class="title-page-name"/> $ OverTime                : Factor w/ 2 levels "No","Yes": 2 1 2 2 1 1 2 1 1 1 ... <br class="title-page-name"/> $ PercentSalaryHike       : int  11 23 15 11 12 13 20 22 21 13 ... <br class="title-page-name"/> $ PerformanceRating       : Ord.factor w/ 4 levels "Low"&lt;"Good"&lt;"Excellent"&lt;..: 3 4 3 3 3 3 4 4 4 3 ... <br class="title-page-name"/> $ RelationshipSatisfaction: Ord.factor w/ 4 levels "Low"&lt;"Medium"&lt;..: 1 4 2 3 4 3 1 2 2 2 ... <br class="title-page-name"/> $ StockOptionLevel        : int  0 1 0 0 1 0 3 1 0 2 ... <br class="title-page-name"/> $ TotalWorkingYears       : int  8 10 7 8 6 8 12 1 10 17 ... <br class="title-page-name"/> $ TrainingTimesLastYear   : int  0 3 3 3 3 2 3 2 2 3 ... <br class="title-page-name"/> $ WorkLifeBalance         : Ord.factor w/ 4 levels "Bad"&lt;"Good"&lt;"Better"&lt;..: 1 3 3 3 3 2 2 3 3 2 ... <br class="title-page-name"/> $ YearsAtCompany          : int  6 10 0 8 2 7 1 1 9 7 ... <br class="title-page-name"/> $ YearsInCurrentRole      : int  4 7 0 7 2 7 0 0 7 7 ... <br class="title-page-name"/> $ YearsSinceLastPromotion : int  0 1 0 3 2 3 0 0 1 7 ... <br class="title-page-name"/> $ YearsWithCurrManager    : int  5 7 0 0 2 6 0 0 8 7 ... </pre>
<p class="mce-root">To view the <kbd class="calibre11">Attrition</kbd> target variable in the dataset run the following code:</p>
<pre class="calibre16">table(mydata$Attrition) </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16"> No   Yes  <br class="title-page-name"/>1233  237  </pre>
<p class="mce-root">Out of the 1,470 observations in the dataset, we have 1,233 samples (83.87%) that are non-attrition cases and 237 attrition cases (16.12%). Clearly, we are dealing with a <em class="calibre15">class imbalance</em> dataset.</p>
<p class="mce-root">We will now visualize the highly correlated variables in the data through the <kbd class="calibre11">corrplot</kbd> library using the following code:</p>
<pre class="calibre16"># considering only the numeric variables in the dataset <br class="title-page-name"/>numeric_mydata &lt;- mydata[,c(1,4,6,7,10,11,13,14,15,17,19,20,21,24,25,26,28:35)] <br class="title-page-name"/># converting the target variable "yes" or "no" values into numeric <br class="title-page-name"/># it defaults to 1 and 2 however converting it into 0 and 1 to be consistent <br class="title-page-name"/>numeric_Attrition = as.numeric(mydata$Attrition)- 1 <br class="title-page-name"/># create a new data frame with numeric columns and numeric target  <br class="title-page-name"/>numeric_mydata = cbind(numeric_mydata, numeric_Attrition) <br class="title-page-name"/># loading the required library <br class="title-page-name"/>library(corrplot) <br class="title-page-name"/># creating correlation plot <br class="title-page-name"/>M &lt;- cor(numeric_mydata) <br class="title-page-name"/>corrplot(M, method="circle") </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter8" src="assets/00693069-856f-49b3-b946-e3c7e53b1ac1.png"/></p>
<p class="mce-root">In the preceding screenshot, it may be observed that darker and larger blues dot in the cells indicate the existence of a strong correlation between the variables in the corresponding rows and columns that form the cell. High correlation between the independent variables indicates the existence of redundant features in the data. The problem of the existence of highly correlated features in the data is termed as <strong class="calibre3">multicollinearity</strong>. If we were to fit a regression model, then it is required that we treat the highly correlated variables from the data through some techniques such as removing the redundant features or by applying principal component analysis or partial least squares regression, which intuitively cuts down the redundant features.</p>
<p class="mce-root">We infer from the output that the following variables are highly correlated and the person building the model needs to take care of these variables if we are to build a regression-based model:</p>
<p class="mce-root"><kbd class="calibre11">JobLevel</kbd>-<kbd class="calibre11">MonthlyIncome</kbd>; <kbd class="calibre11">JobLevel</kbd>-<kbd class="calibre11">TotalWorkingYears</kbd>; <kbd class="calibre11">MonthlyIncome</kbd>-<kbd class="calibre11">TotalWorkingYears</kbd>; <kbd class="calibre11">PercentSalaryHike</kbd>-<kbd class="calibre11">PerformanceRating</kbd>; <kbd class="calibre11">YearsAtCompany</kbd>-<kbd class="calibre11">YearsInCurrentRole</kbd>; <kbd class="calibre11">YearsAtCompany</kbd>-<kbd class="calibre11">YearsWithCurrManager</kbd>; <kbd class="calibre11">YearsWithCurrManager</kbd>-<kbd class="calibre11">YearsInCurrentRole</kbd></p>
<p class="mce-root">Now, plot the various independent variables with the dependent <kbd class="calibre11">Attrition</kbd> variable in order to understand the influence of the independent variable on the target: </p>
<pre class="calibre16">### Overtime vs Attiriton <br class="title-page-name"/>l &lt;- ggplot(mydata, aes(OverTime,fill = Attrition)) <br class="title-page-name"/>l &lt;- l + geom_histogram(stat="count") <br class="title-page-name"/><br class="title-page-name"/>tapply(as.numeric(mydata$Attrition) - 1 ,mydata$OverTime,mean) <br class="title-page-name"/><br class="title-page-name"/><span>No Yes</span><br class="title-page-name"/><span>0.104364326375712 0.305288461538462</span></pre>
<p class="mce-root">Let's run the following command to get a graph view:</p>
<pre class="calibre16">print(l) </pre>
<p class="mce-root">The preceding command generates the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter9" src="assets/bf9f27c2-82ba-4b9c-b71e-067888466d2b.png"/></p>
<p class="mce-root">In the preceding output, it can be observed that employees that work overtime are more prone to attrition when compared to the ones that do not work overtime:</p>
<p class="mce-root">Let's calculate the attrition of the employees by executing the following commands:</p>
<pre class="calibre16">### MaritalStatus vs Attiriton <br class="title-page-name"/>l &lt;- ggplot(mydata, aes(MaritalStatus,fill = Attrition)) <br class="title-page-name"/>l &lt;- l + geom_histogram(stat="count") <br class="title-page-name"/><br class="title-page-name"/>tapply(as.numeric(mydata$Attrition) - 1 ,mydata$MaritalStatus,mean) <br class="title-page-name"/>Divorced 0.100917431192661 <br class="title-page-name"/>Married 0.12481426448737 <br class="title-page-name"/>Single 0.25531914893617 </pre>
<p class="mce-root"><span class="calibre4">Let's run the following command to get a graph view:</span></p>
<pre class="calibre16">print(l) </pre>
<p class="mce-root">The preceding command generates the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter10" src="assets/b1940a98-684a-401c-b1dc-2f2c8abece69.png"/></p>
<p class="mce-root"><span class="calibre4">In the preceding output, it can be observed that</span> employees that are single have more attrition:</p>
<pre class="calibre16">###JobRole vs Attrition <br class="title-page-name"/>l &lt;- ggplot(mydata, aes(JobRole,fill = Attrition)) <br class="title-page-name"/>l &lt;- l + geom_histogram(stat="count") <br class="title-page-name"/> <br class="title-page-name"/>tapply(as.numeric(mydata$Attrition) - 1 ,mydata$JobRole,mean) <br class="title-page-name"/><br class="title-page-name"/>Healthcare Representative    Human Resources <br class="title-page-name"/>               0.06870229    0.23076923 <br class="title-page-name"/>    Laboratory Technician    Manager <br class="title-page-name"/>               0.23938224    0.04901961 <br class="title-page-name"/>   Manufacturing Director    Research Director <br class="title-page-name"/>               0.06896552    0.02500000 <br class="title-page-name"/>       Research Scientist    Sales Executive <br class="title-page-name"/>               0.16095890    0.17484663 <br class="title-page-name"/>     Sales Representative <br class="title-page-name"/>               0.39759036 <br class="title-page-name"/>mean(as.numeric(mydata$Attrition) - 1) <br class="title-page-name"/>[1] 0.161224489795918 </pre>
<p class="mce-root">Execute the following command to get a graphical representation for the same:</p>
<pre class="calibre16">print(l)</pre>
<p class="mce-root">Take a look at the following output generated by running the preceding command:</p>
<p class="CDPAlignCenter1"><img class="aligncenter11" src="assets/8469586f-dac2-4e9f-a5d3-229b91cb749c.png"/></p>
<p class="mce-root"><span class="calibre4">In the preceding output, it can be observed that the l</span>ab technicians, sales representatives, and employees working in human resources job roles have more attrition than other organizational roles.</p>
<p class="mce-root"><span class="calibre4">Let's execute the following commands to check with the impact of the gender of an employee over attribution:</span></p>
<pre class="calibre16">###Gender vs Attrition <br class="title-page-name"/>l &lt;- ggplot(mydata, aes(Gender,fill = Attrition)) <br class="title-page-name"/>l &lt;- l + geom_histogram(stat="count") <br class="title-page-name"/> <br class="title-page-name"/>tapply(as.numeric(mydata$Attrition) - 1 ,mydata$Gender,mean) <br class="title-page-name"/><br class="title-page-name"/>Female 0.147959183673469 <br class="title-page-name"/>Male 0.170068027210884 </pre>
<p class="mce-root">Run the following command to get a graphical representation for the same:</p>
<pre class="calibre16">print(l)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter12" src="assets/8f15307f-e66a-43cf-bd52-de8ad0dd65ae.png"/></p>
<p class="mce-root">In the preceding output, you can see that the gender of an employee does not have any impact on attrition, in other words attrition is observed to be the same across all genders.</p>
<p class="mce-root">Let's calculate the attribute of the employees from various fields by executing the following:</p>
<pre class="calibre16">###EducationField vs Attrition el &lt;- ggplot(mydata, aes(EducationField,fill = Attrition)) <br class="title-page-name"/>l &lt;- l + geom_histogram(stat="count") <br class="title-page-name"/> <br class="title-page-name"/>tapply(as.numeric(mydata$Attrition) - 1 ,mydata$EducationField,mean) <br class="title-page-name"/><br class="title-page-name"/>Human Resources    Life Sciences    Marketing <br class="title-page-name"/>       0.2592593    0.1468647        0.2201258 <br class="title-page-name"/>         Medical   Other Technical  Degree <br class="title-page-name"/>       0.1357759    0.1341463        0.2424242</pre>
<p class="mce-root">Let's execute the following command to get a graphical representation:</p>
<pre class="calibre16">print(l)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter13" src="assets/84dd7b0e-ee6a-49fe-aea9-c1e7b1d26f00.png"/></p>
<p class="mce-root">Looking at the preceding graph, we can conclude that employees with a technical degree or a degree in human resources are observed to have more attrition. Take a look at the following code:</p>
<pre class="calibre16">###Department vs Attrition <br class="title-page-name"/>l &lt;- ggplot(mydata, aes(Department,fill = Attrition)) <br class="title-page-name"/>l &lt;- l + geom_histogram(stat="count") <br class="title-page-name"/><br class="title-page-name"/>tapply(as.numeric(mydata$Attrition) - 1 ,mydata$Department,mean) <br class="title-page-name"/>Human Resources  Research &amp; Development  Sales <br class="title-page-name"/>   0.1904762       0.1383975              0.2062780 </pre>
<p class="mce-root">Let's execute the following command to check with the attribution of various departments:</p>
<pre class="calibre16">print(l) </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter14" src="assets/5bd5d2df-9dd3-4e4c-aaae-0a027f4d7718.png"/></p>
<p class="mce-root"><span class="calibre4">Looking at the preceding graph, we can conclude that</span> the R and D department has less attrition compared to the sales and HR departments. Take a look at the following code:</p>
<pre class="calibre16">###BusinessTravel vs Attrition <br class="title-page-name"/>l &lt;- ggplot(mydata, aes(BusinessTravel,fill = Attrition)) <br class="title-page-name"/>l &lt;- l + geom_histogram(stat="count") <br class="title-page-name"/><br class="title-page-name"/>tapply(as.numeric(mydata$Attrition) - 1 ,mydata$BusinessTravel,mean) <br class="title-page-name"/> Non-Travel   Travel_Frequently   Travel_Rarely <br class="title-page-name"/>  0.0800000    0.2490975           0.1495686</pre>
<p class="mce-root">Execute the following command to get a graphical representation for the same:</p>
<pre class="calibre16">print(l) </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter15" src="assets/1283f61e-dd01-4435-98da-9c8ca191a61a.png"/></p>
<p class="mce-root"><span class="calibre4">Looking at the preceding graph, we can conclude that</span> employees with frequent travels are prone to more attrition compared to employees with a non-travel status or the ones that rarely travel. </p>
<p class="mce-root"><span class="calibre4">Let's calculate the overtime of the employees by executing the following commands:</span></p>
<pre class="calibre16">### x=Overtime, y= Age, z = MaritalStatus , t = Attrition <br class="title-page-name"/>ggplot(mydata, aes(OverTime, Age)) +   <br class="title-page-name"/>  facet_grid(.~MaritalStatus) + <br class="title-page-name"/>  geom_jitter(aes(color = Attrition),alpha = 0.4) +   <br class="title-page-name"/>  ggtitle("x=Overtime, y= Age, z = MaritalStatus , t = Attrition") +   <br class="title-page-name"/>  theme_light() </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter16" src="assets/ee279cb3-4192-40e4-b608-81e10487a03b.png"/></p>
<p class="mce-root"><span class="calibre4">Looking at the preceding graph, we can conclude that</span><span class="calibre4"> i</span>t can be observed that employees that are young (age &lt; 35 ) and are single, but work overtime, are more prone to attrition: </p>
<pre class="calibre16">### MonthlyIncome vs. Age, by  color = Attrition <br class="title-page-name"/>ggplot(mydata, aes(MonthlyIncome, Age, color = Attrition)) +  <br class="title-page-name"/>  geom_jitter() + <br class="title-page-name"/>  ggtitle("MonthlyIncome vs. Age, by  color = Attrition ") + <br class="title-page-name"/>  theme_light() </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter17" src="assets/5d910b21-22e9-4301-a211-b4ae423cff62.png"/></p>
<p class="mce-root"><span class="calibre4">Looking at the preceding graph, we can conclude that</span> attrition is higher in employees that are young (age &lt; 30) and most attrition is observed with employees that earn less than $7,500.</p>
<p class="mce-root">Although we have learned a number of important details about the data at hand, there is actually so much more to explore and learn. However, so as to move to the next step, we stop here at this EDA step. It should be noted that, in a real-world situation, data would not be so very clean as we see in this attrition dataset. For example, we would have missing values in the data; in which case, we would do missing values imputation. Fortunately, we have an impeccable dataset that is ready for us to create models without having to do any data cleansing or additional preprocessing. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-nearest neighbors model for benchmarking the performance</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we will implement the <strong class="calibre3">k-nearest neighbors</strong> (<strong class="calibre3">KNN</strong>) algorithm to build a model on our IBM attrition dataset. Of course, we are already aware from EDA that we have a class imbalance problem in the dataset at hand. However, we will not be treating the dataset for class imbalance for now as this is an entire area on its own and several techniques are available in this area and therefore out of scope for the ML ensembling topic covered in this chapter. We will, for now, consider the dataset as is and build ML models. Also, for class imbalance datasets, Kappa or precision and recall or the area under the curve of the receiver operating characteristic (AUROC) are the appropriate metrics to use. However, for simplicity, we will use <em class="calibre15">accuracy</em> as a performance metric. We will adapt 10-fold cross validation repeated 10 times to avail the model performance measurement. Let's now build our attrition prediction model with the KNN algorithm as follows:</p>
<pre class="calibre16"># Load the necessary libraries <br class="title-page-name"/># doMC is a library that enables R to use multiple cores available on the sysem thereby supporting multiprocessing.  <br class="title-page-name"/>library(doMC) <br class="title-page-name"/># registerDoMC command instructs R to use the specified number of cores to execute the code. In this case, we ask R to use 4 cores available on the system <br class="title-page-name"/>registerDoMC(cores=4) <br class="title-page-name"/># caret library has the ml algorithms and other routines such as cross validation etc.  <br class="title-page-name"/>library(caret) <br class="title-page-name"/># Setting the working directory where the dataset is located <br class="title-page-name"/>setwd("~/Desktop/chapter 2") <br class="title-page-name"/># Reading the csv file into R variable called mydata <br class="title-page-name"/>mydata &lt;- read.csv("WA_Fn-UseC_-HR-Employee-Attrition.csv") <br class="title-page-name"/>#Removing the non-discriminatory features (as identified during EDA) from the dataset  <br class="title-page-name"/>mydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL <br class="title-page-name"/># setting the seed prior to model building ensures reproducibility of the results obtained <br class="title-page-name"/>set.seed(10000) <br class="title-page-name"/># setting the train control parameters specifying gold standard 10 fold cross validation  repeated 10 times <br class="title-page-name"/>fitControl = trainControl(method="repeatedcv", number=10,repeats=10) <br class="title-page-name"/>###creating a model on the data. Observe that we specified Attrition as the target and that model should learn from rest of the variables. We specified mydata as the dataset to learn. We pass the train control parameters and specify that knn algorithm need to be used to build the model. K can be of any length - we specified 20 as parameter which means the train command will search through 20 different random k values and finally retains the model that produces the best performance measurements. The final model is stored as caretmodel <br class="title-page-name"/>caretmodel = train(Attrition~., data=mydata, trControl=fitControl, method = "knn", tuneLength = 20) <br class="title-page-name"/># We output the model object to the console  <br class="title-page-name"/>caretmodel </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">k-Nearest Neighbors  <br class="title-page-name"/>1470 samples <br class="title-page-name"/>  30 predictors <br class="title-page-name"/>   2 classes: 'No', 'Yes'  <br class="title-page-name"/>No pre-processing <br class="title-page-name"/>Resampling: Cross-Validated (10 fold, repeated 10 times)  <br class="title-page-name"/>Summary of sample sizes: 1323, 1323, 1324, 1323, 1324, 1322, ...  <br class="title-page-name"/>Resampling results across tuning parameters: <br class="title-page-name"/>  k   Accuracy   Kappa        <br class="title-page-name"/>   5  0.8216447  0.0902934591 <br class="title-page-name"/>   7  0.8349033  0.0929511324 <br class="title-page-name"/>   9  0.8374198  0.0752842114 <br class="title-page-name"/>  11  0.8410920  0.0687849122 <br class="title-page-name"/>  13  0.8406861  0.0459679081 <br class="title-page-name"/>  15  0.8406875  0.0337742424 <br class="title-page-name"/>  17  0.8400748  0.0315670261 <br class="title-page-name"/>  19  0.8402770  0.0245499585 <br class="title-page-name"/>  21  0.8398721  0.0143638854 <br class="title-page-name"/>  23  0.8393945  0.0084393721 <br class="title-page-name"/>  25  0.8391891  0.0063246624 <br class="title-page-name"/>  27  0.8389174  0.0013913143 <br class="title-page-name"/>  29  0.8388503  0.0007113939 <br class="title-page-name"/>  31  0.8387818  0.0000000000 <br class="title-page-name"/>  33  0.8387818  0.0000000000 <br class="title-page-name"/>  35  0.8387818  0.0000000000 <br class="title-page-name"/>  37  0.8387818  0.0000000000 <br class="title-page-name"/>  39  0.8387818  0.0000000000 <br class="title-page-name"/>  41  0.8387818  0.0000000000 <br class="title-page-name"/>  43  0.8387818  0.0000000000 <br class="title-page-name"/>Accuracy was used to select the optimal model using the largest value. <br class="title-page-name"/>The final value used for the model was k = 11. </pre>
<p class="mce-root">We can see from the model output that the best performing model is when <kbd class="calibre11">k</kbd> <kbd class="calibre11">= 11</kbd> and we obtained an accuracy of 84% with this <kbd class="calibre11">k</kbd> value. In the rest of the chapter, while experimenting with several ensembling techniques, we will check if this 84% accuracy obtained from KNN will get beaten at all. </p>
<p class="mce-root">In a realistic project-building situation, just identifying the best hyperparameters is not enough. A model needs to be trained on a full dataset with the best hyperparameters and the model needs to be saved for future use. We will review these steps in the rest of this section.</p>
<p class="mce-root">In this case, the <kbd class="calibre11">caretmodel</kbd> object already has the trained model with <kbd class="calibre11">k = 11</kbd>, therefore we do not attempt to retrain the model with the best hyperparameter. To check the final model, you can query the model object with the code:</p>
<pre class="calibre16">caretmodel$finalModel </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">11-nearest neighbor model <br class="title-page-name"/>Training set outcome distribution: <br class="title-page-name"/>  No  Yes  <br class="title-page-name"/>1233  237  </pre>
<p class="mce-root">The next step is to save your best models to a file so that we can load them up later and make predictions on unseen data. A model can be saved to a local directory using the <kbd class="calibre11">saveRDS</kbd> R command:</p>
<pre class="calibre16"> # save the model to disk <br class="title-page-name"/>saveRDS(caretmodel, "production_model.rds") </pre>
<p class="mce-root">In this case, the <kbd class="calibre11">caretmodel</kbd> is saved as <kbd class="calibre11">production_model.rds</kbd> in the working directory. The model is now serialized as a file that can be loaded anytime and it can be used to score unseen data. Loading and scoring can be achieved through the following R code:</p>
<pre class="calibre16"># Set the working directory to the directory where the saved .rds file is located  <br class="title-page-name"/>setwd("~/Desktop/chapter 2") <br class="title-page-name"/>#Load the model  <br class="title-page-name"/>loaded_model &lt;- readRDS("production_model.rds") </pre>
<pre class="calibre16">#Using the loaded model to make predictions on unseen data <br class="title-page-name"/>final_predictions &lt;- predict(loaded_model, unseen_data) </pre>
<div class="packtinfobox">Please note that <kbd class="calibre24">unseen_data</kbd> needs to be read prior to scoring through the <kbd class="calibre24">predict</kbd> command. </div>
<p class="mce-root">The part of the code where the final model is trained on the entire dataset, saving the model, reloading it from the file whenever required and scoring the unseen data collectively, is termed as building an ML productionalization pipeline. This pipeline remains the same for all ML models irrespective of the fact that the model is built using one single algorithm or using an ensembling technique. Therefore, in the later sections when we implement the various ensembling techniques, we will not cover the productionalization pipeline but just stop at obtaining the performance measurement through 10-fold cross validation repeated 10 times.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bagging</h1>
                </header>
            
            <article>
                
<div class="calibre25">
<p class="mce-root">Bootstrap aggregation or <strong class="calibre3">bagging</strong> is the earliest ensemble technique adopted widely by the ML-practicing community. Bagging involves creating multiple different models from a single dataset. It is important to understand an important statistical technique called bootstrapping in order to get an understanding of bagging.</p>
</div>
<div class="calibre25">
<p class="mce-root">Bootstrapping involves multiple random subsets of a dataset being created. It is possible that the same data sample gets picked up in multiple subsets and this is termed as <strong class="calibre3">bootstrapping with replacement</strong>. The advantage with this approach is that the standard error in estimating a quantity that occurs due to the use of whole dataset. This technique can be better explained with an example.</p>
</div>
<div class="calibre25">
<p class="mce-root">Assume you have a small dataset of 1,000 samples. Based on the samples, you are asked to compute the average of the population that the sample represents. Now, a direct way of doing it is through the following formula:</p>
</div>
<div class="calibre25">
<p class="CDPAlignCenter1"><img class="fm-editor-equation" src="assets/1ffab3de-df4f-489c-9a25-be38913f6d33.png"/></p>
</div>
<div class="calibre25">
<p class="mce-root">As this is a small sample, we may have an error in estimating the population average. This error can be reduced by adapting bootstrap sampling with replacement. In the technique, we create 10 subsets of the dataset where each dataset has 100 items in it. A data item may be randomly represented multiple times in a subset and there is no restriction on the number of times an item can be represented within a data subset as well as across the subsets. Now, we take the average of samples in each data subset, therefore, we end up with 10 different averages. Using all these collected averages, we estimate the average of the population with the following formula:</p>
</div>
<div class="calibre25">
<p class="CDPAlignCenter1"><img class="fm-editor-equation1" src="assets/ab180bc6-78d6-497a-ae47-abeabca5e347.png"/></p>
</div>
<div class="calibre25">
<p class="mce-root">Now, we have a better estimate of the average as we have extrapolated the small sample to randomly generate multiple samples that are representative of the original population. </p>
</div>
<div class="calibre25">
<p class="mce-root">In bagging, the actual training dataset is split into multiple bags through bootstrap sampling with replacement. Assuming that we ended up with <em class="calibre15">n</em> bags, when an ML algorithm is applied on each of these bags, we obtain <em class="calibre15">n</em> different models. Each model is focused on one bag. When it comes to making predictions on new unseen data, each of these <em class="calibre15">n</em> models makes independent predictions on the data. A final prediction for an observation is arrived at by combining the predictions of the observation of all the <em class="calibre15">n</em> models. In case of classification, voting is adopted and the majority is considered as the final prediction. For regression, the average of predictions from all models is considered as the final prediction. </p>
</div>
<div class="calibre25">
<p class="mce-root">Decision-tree-based algorithms, such as <strong class="calibre3">classification and regression trees</strong> (<strong class="calibre3">CART</strong>), are unstable learners. The reason is that a small change in the training dataset heavily impacts the model created. Model change essentially means that the predictions also change. Bagging is a very effective technique to handle the high sensitivity to data changes. As we can build multiple decision tree models on subsets of a dataset and then arrive at a final prediction based on predictions from each of the models, the effect of changes in data gets nullified or not experienced very significantly.</p>
</div>
<div class="calibre25">
<p class="mce-root">One intuitive problem experienced with building multiple models on subsets of data is <strong class="calibre3">overfitting</strong>. However, this is overcome by growing deep trees without applying any pruning on the nodes.</p>
</div>
<div class="calibre25">
<p class="mce-root">A downside with bagging is that it takes longer to build the models when compared to building a model with a stand-alone ML algorithm. This is obvious because multiple models gets built in bagging, as opposed to one single model, and it takes time to build these multiple models. </p>
</div>
<p class="mce-root">Now, let's implement the R code to achieve a bagging ensemble and compare the performance obtained with that of the performance obtained from KNN. We will then explore the working mechanics of bagging methodology.</p>
<p class="mce-root">The <kbd class="calibre11">caret</kbd> library provides a framework to implement bagging with any stand-alone ML algorithm. <kbd class="calibre11">ldaBag</kbd>, <kbd class="calibre11">plsBag</kbd>, <kbd class="calibre11">nbBag</kbd>, <kbd class="calibre11">treeBag</kbd>, <kbd class="calibre11">ctreeBag</kbd>, <kbd class="calibre11">svmBag</kbd>, and <kbd class="calibre11">nnetBag</kbd> are some of the example methods provided in caret. In this section, we will implement bagging with three different <kbd class="calibre11">caret</kbd> methods such as <kbd class="calibre11">treebag</kbd>, <kbd class="calibre11">svmbag</kbd>, and <kbd class="calibre11">nbbag</kbd>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bagged classification and regression trees (treeBag) implementation</h1>
                </header>
            
            <article>
                
<p class="mce-root">To begin, load the essential libraries and register the number of cores for parallel processing:</p>
<pre class="calibre16">library(doMC) <br class="title-page-name"/>registerDoMC(cores = 4)  <br class="title-page-name"/>library(caret) <br class="title-page-name"/>#setting the random seed for replication <br class="title-page-name"/>set.seed(1234) <br class="title-page-name"/># setting the working directory where the data is located <br class="title-page-name"/>setwd("~/Desktop/chapter 2") <br class="title-page-name"/># reading the data <br class="title-page-name"/>mydata &lt;- read.csv("WA_Fn-UseC_-HR-Employee-Attrition.csv") <br class="title-page-name"/>#removing the non-discriminatory features identified during EDA <br class="title-page-name"/>mydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL <br class="title-page-name"/>#setting up cross-validation <br class="title-page-name"/>cvcontrol &lt;- trainControl(method="repeatedcv", repeats=10, number = 10, allowParallel=TRUE) <br class="title-page-name"/># model creation with treebag , observe that the number of bags is set as 10 <br class="title-page-name"/>train.bagg &lt;- train(Attrition ~ ., data=mydata, method="treebag",B=10, trControl=cvcontrol, importance=TRUE) <br class="title-page-name"/>train.bagg </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">Bagged CART  <br class="title-page-name"/>1470 samples <br class="title-page-name"/>  30 predictors <br class="title-page-name"/>   2 classes: 'No', 'Yes'  <br class="title-page-name"/>No pre-processing <br class="title-page-name"/>Resampling: Cross-Validated (10 fold, repeated 10 times)  <br class="title-page-name"/>Summary of sample sizes: 1324, 1323, 1323, 1322, 1323, 1322, ...  <br class="title-page-name"/>Resampling results: <br class="title-page-name"/>  Accuracy  Kappa     <br class="title-page-name"/>  0.854478  0.2971994 </pre>
<p class="mce-root">We can see that we achieved a better accuracy of 85.4% compared to 84% accuracy that was obtained with the KNN algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector machine bagging (SVMBag) implementation</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="calibre4">The steps of loading the libraries, registering multiprocessing, setting a working directory, reading data from a working directory, removing nondiscriminatory features from data, and setting up cross-validation parameters remain the same in the SVMBag and NBBag implementations as well. So, we do not repeat these steps in the SVMBag or NBBag code. Rather, we will focus on discussing the SVMBag or NBBag specific code:</span></p>
<pre class="calibre16"># Setting up SVM predict function as the default svmBag$pred function has some code issue <br class="title-page-name"/>svm.predict &lt;- function (object, x) <br class="title-page-name"/>{ <br class="title-page-name"/> if (is.character(lev(object))) { <br class="title-page-name"/>    out &lt;- predict(object, as.matrix(x), type = "probabilities") <br class="title-page-name"/>    colnames(out) &lt;- lev(object) <br class="title-page-name"/>    rownames(out) &lt;- NULL <br class="title-page-name"/>  } <br class="title-page-name"/>  else out &lt;- predict(object, as.matrix(x))[, 1] <br class="title-page-name"/>  out <br class="title-page-name"/>} <br class="title-page-name"/># setting up parameters to build svm bagging model <br class="title-page-name"/>bagctrl &lt;- bagControl(fit = svmBag$fit, <br class="title-page-name"/>                      predict = svm.predict , <br class="title-page-name"/>                      aggregate = svmBag$aggregate) <br class="title-page-name"/># fit the bagged svm model <br class="title-page-name"/>set.seed(300) <br class="title-page-name"/>svmbag &lt;- train(Attrition ~ ., data = mydata, method="bag",trControl = cvcontrol, bagControl = bagctrl,allowParallel = TRUE) <br class="title-page-name"/># printing the model results <br class="title-page-name"/>svmbag </pre>
<div class="calibre25">
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
</div>
<div class="calibre25">
<pre class="calibre16">Bagged Model  <br class="title-page-name"/><br class="title-page-name"/>1470 samples <br class="title-page-name"/>  30 predictors <br class="title-page-name"/>   2 classes: 'No', 'Yes'  <br class="title-page-name"/><br class="title-page-name"/>No pre-processing <br class="title-page-name"/>Resampling: Cross-Validated (10 fold, repeated 10 times)  <br class="title-page-name"/>Summary of sample sizes: 1324, 1324, 1323, 1323, 1323, 1323, ...  <br class="title-page-name"/>Resampling results: <br class="title-page-name"/>  Accuracy   Kappa     <br class="title-page-name"/>  0.8777721  0.4749657 <br class="title-page-name"/><br class="title-page-name"/>Tuning parameter 'vars' was held constant at a value of 44 </pre></div>
<div class="calibre25">
<p class="mce-root">You will see that we achieved an accuracy of 87.7%, which is much higher than the KNN model's 84% accuracy.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes (nbBag) bagging implementation</h1>
                </header>
            
            <article>
                
<p class="mce-root">We will now do the <kbd class="calibre11">nbBag</kbd> implementation by executing the following code:</p>
<pre class="calibre16"># setting up parameters to build svm bagging model <br class="title-page-name"/>bagctrl &lt;- bagControl(fit = nbBag$fit, <br class="title-page-name"/>                      predict = nbBag$pred , <br class="title-page-name"/>                      aggregate = nbBag$aggregate) <br class="title-page-name"/># fit the bagged nb model <br class="title-page-name"/>set.seed(300) <br class="title-page-name"/>nbbag &lt;- train(Attrition ~ ., data = mydata, method="bag", trControl = cvcontrol, bagControl = bagctrl) <br class="title-page-name"/># printing the model results <br class="title-page-name"/>nbbag </pre>
<div class="calibre25">
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
</div>
<div class="calibre25">
<pre class="calibre16">Bagged Model  <br class="title-page-name"/><br class="title-page-name"/>1470 samples <br class="title-page-name"/>  30 predictors <br class="title-page-name"/>   2 classes: 'No', 'Yes'  <br class="title-page-name"/><br class="title-page-name"/>No pre-processing <br class="title-page-name"/>Resampling: Cross-Validated (10 fold, repeated 10 times)  <br class="title-page-name"/>Summary of sample sizes: 1324, 1324, 1323, 1323, 1323, 1323, ...  <br class="title-page-name"/>Resampling results: <br class="title-page-name"/><br class="title-page-name"/>  Accuracy   Kappa      <br class="title-page-name"/>  0.8389878  0.00206872 <br class="title-page-name"/><br class="title-page-name"/>Tuning parameter 'vars' was held constant at a value of 44 </pre></div>
<div class="calibre25">
<p class="mce-root">We see that in this case, we achieved only 83.89% accuracy, which is slightly inferior to the KNN model's performance of 84%. </p>
</div>
<div class="calibre25">
<p class="mce-root">Although we have shown only three examples of the <kbd class="calibre11">caret</kbd> methods for bagging, the code remains the same to implement the other methods. The only change that is needed in the code is to replace the <kbd class="calibre11">fit</kbd>, <kbd class="calibre11">predict</kbd>, and <kbd class="calibre11">aggregate</kbd> parameters in <kbd class="calibre11">bagControl</kbd>. For example, to implement bagging with a neural network algorithm, we need to define <kbd class="calibre11">bagControl</kbd> as follows:</p>
</div>
<div class="calibre25">
<pre class="calibre16">bagControl(fit = nnetBag$fit, predict = nnetBag$pred , aggregate = nnetBag$aggregate) </pre></div>
<div class="calibre25">
<p class="mce-root">It may be noted that an appropriate library needs to be available in R for <kbd class="calibre11">caret</kbd> to run the methods, otherwise it results in error. For example, <kbd class="calibre11">nbBag</kbd> requires the <kbd class="calibre11">klaR</kbd> library to be installed on the system prior to executing the code. Similarly, the <kbd class="calibre11">ctreebag</kbd> function needs the <kbd class="calibre11">party</kbd> package to be installed. Users need to check the availability of an appropriate library on the system prior to including it for use with the <kbd class="calibre11">caret</kbd> bagging.</p>
</div>
<div class="calibre25">
<p class="mce-root">We now have an understanding of implementing a project through bagging technique. The next subsection covers the underlying working mechanism of bagging. This will help get clarity in terms of what bagging did internally with our dataset so as to produce better performance measurements than that of stand-alone model performance.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Randomization with random forests</h1>
                </header>
            
            <article>
                
<div class="calibre25">
<div class="calibre25">
<p class="mce-root">As we've seen in bagging, we create a number of bags on which each model is trained. Each of the bags consists of subsets of the actual dataset, however the number of features or variables remain the same in each of the bags. In other words, what we performed in bagging is subsetting the dataset rows. </p>
</div>
<div class="calibre25">
<p class="mce-root">In random forests, while we create bags from the dataset through subsetting the rows, we also subset the features (columns) that need to be included in each of the bags.</p>
</div>
<div class="calibre25">
<p class="mce-root">Assume that you have 1,000 observations with 20 features in your dataset. We can create 20 bags where each one of the bags has 100 observations (this is possible because of bootstrapping with replacement) and five features. Now 20 models are trained where each model gets to see only the bag it is assigned with. The final prediction is arrived at by voting or averaging based on the fact of whether the problem is a regression problem or a classification problem.</p>
</div>
<div class="calibre25">
<p class="mce-root">Another key difference between bagging and random forests is the ML algorithm that is used to build the model. In bagging, any ML algorithm may be used to create a model however random forest models are built specifically using CART.</p>
</div>
<p class="mce-root">Random forest modeling is yet another very popular machine learning algorithm. It is one of the algorithms that has proved itself multiple times as the best performing of algorithms, despite applying it on noisy datasets. For a person that has understood bootstrapping, understanding random forests is a cakewalk.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing an attrition prediction model with random forests</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's get our attrition model through random forest modeling by executing the following code:</p>
<div class="calibre25">
<pre class="calibre16"># loading required libraries and registering multiple cores to enable parallel processing <br class="title-page-name"/>library(doMC) <br class="title-page-name"/>library(caret) <br class="title-page-name"/>registerDoMC(cores=4) <br class="title-page-name"/># setting the working directory and reading the dataset <br class="title-page-name"/>setwd("~/Desktop/chapter 2") <br class="title-page-name"/>mydata &lt;- read.csv("WA_Fn-UseC_-HR-Employee-Attrition.csv") <br class="title-page-name"/># removing the non-discriminatory features from the dataset as identified during EDA step <br class="title-page-name"/>mydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL <br class="title-page-name"/># setting the seed for reproducibility <br class="title-page-name"/>set.seed(10000) <br class="title-page-name"/># setting the cross validation parameters <br class="title-page-name"/>fitControl = trainControl(method="repeatedcv", number=10,repeats=10) <br class="title-page-name"/># creating the caret model with random forest algorithm <br class="title-page-name"/>caretmodel = train(Attrition~., data=mydata, method="rf", trControl=fitControl, verbose=F) <br class="title-page-name"/># printing the model summary <br class="title-page-name"/>caretmodel </pre></div>
<div class="calibre25">
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
</div>
<div class="calibre25">
<pre class="calibre16">Random Forest  <br class="title-page-name"/><br class="title-page-name"/>1470 samples <br class="title-page-name"/>  30 predictors <br class="title-page-name"/>   2 classes: 'No', 'Yes'  <br class="title-page-name"/><br class="title-page-name"/>No pre-processing <br class="title-page-name"/>Resampling: Cross-Validated (10 fold, repeated 10 times)  <br class="title-page-name"/>Summary of sample sizes: 1323, 1323, 1324, 1323, 1324, 1322, ...  <br class="title-page-name"/>Resampling results across tuning parameters: <br class="title-page-name"/><br class="title-page-name"/>  mtry  Accuracy   Kappa     <br class="title-page-name"/>   2    0.8485765  0.1014859 <br class="title-page-name"/>  23    0.8608271  0.2876406 <br class="title-page-name"/>  44    0.8572929  0.2923997 <br class="title-page-name"/><br class="title-page-name"/>Accuracy was used to select the optimal model using the largest value. <br class="title-page-name"/>The final value used for the model was mtry = 23. </pre></div>
<div class="calibre25">
<p class="mce-root">We see the best random forest model achieved a better accuracy of 86% compared to KNN's 84%.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boosting </h1>
                </header>
            
            <article>
                
<div class="calibre25">
<div class="calibre25">
<p class="mce-root">A weak learner is an algorithm that performs relatively poorly—generally, the accuracy obtained with the weak learners is just above chance. It is often, if not always, observed that weak learners are computationally simple. Decision stumps or 1R algorithms are some examples of weak learners. Boosting converts weak learners into strong learners. This essentially means that boosting is not an algorithm that does the predictions, but it works with an underlying weak ML algorithm to get better performance. </p>
</div>
<div class="calibre25">
<p class="mce-root">A boosting model is a sequence of models learned on subsets of data similar to that of the bagging ensembling technique. The difference is in the creation of the subsets of data. Unlike bagging, all the subsets of data used for model training are not created prior to the start of the training. Rather, boosting builds a first model with an ML algorithm that does predictions on the entire dataset. Now, there are some misclassified instances that are subsets and used by the second model. The second model only learns from this misclassified set of data curated from the first model's output.</p>
<p class="mce-root">The second model's misclassified instances become input to the third model. The process of building models is repeated until the stopping criteria is met. The final prediction for an observation in the unseen dataset is arrived by averaging or voting the predictions from all the models for that specific, unseen observation.</p>
</div>
<div class="calibre25">
<p class="mce-root">There are subtle differences between the various and numerous algorithms in the boosting algorithms family, however we are not going to discuss them in detail as the intent of this chapter is to get a generalized understanding of ML ensembles and not to gain in-depth knowledge of various boosting algorithms.</p>
</div>
<div class="calibre25">
<p class="mce-root">While obtaining better performance, measurement is the biggest advantage with the boosting ensemble; difficulty with model interpretability, higher computational times, and model overfitting are some of the issues encountered with boosting. Of course, these problems can be overruled through the use of specialized techniques.</p>
</div>
<p class="mce-root">Boosting algorithms are undoubtedly super-popular and are observed to be used by winners in many Kaggle and similar competitions. <span class="calibre4">There are a number of boosting algorithms available such as <strong class="calibre3">gradient boosting machines</strong> (<strong class="calibre3">GBMs</strong>), <strong class="calibre3">adaptive boosting</strong> (<strong class="calibre3">AdaBoost</strong>) , gradient tree boosting, <strong class="calibre3">extreme gradient boosting</strong> (<strong class="calibre3">XGBoost</strong>), and <strong class="calibre3">light gradient boosting machine</strong> (<strong class="calibre3">LightGBM</strong>). </span>In this section, we will learn the theory and implementation of two of the most popular boosting algorithms such as GBMs and XGBoost. Prior to learning the theoretical concept of boosting and its pros and cons, let's first start focusing on implementing the attrition prediction models with GBMs and XGBoost.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The GBM implementation</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's implement the attrition prediction model with GBMs:</p>
<div class="calibre25">
<pre class="calibre16"># loading the essential libraries and registering the cores for multiprocessing <br class="title-page-name"/>library(doMC) <br class="title-page-name"/>library(mlbench) <br class="title-page-name"/>library(gbm) <br class="title-page-name"/>library(caret) <br class="title-page-name"/>registerDoMC(cores=4) <br class="title-page-name"/># setting the working directory and reading the dataset <br class="title-page-name"/>setwd("~/Desktop/chapter 2") <br class="title-page-name"/>mydata &lt;- read.csv("WA_Fn-UseC_-HR-Employee-Attrition.csv") <br class="title-page-name"/># removing the non-discriminatory features as identified by EDA step <br class="title-page-name"/>mydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL <br class="title-page-name"/># converting the target attrition feild to numeric as gbm model expects all numeric feilds in the dataset <br class="title-page-name"/>mydata$Attrition = as.numeric(mydata$Attrition) <br class="title-page-name"/># forcing the attrition column values to be 0 and 1 instead of 1 and 2 <br class="title-page-name"/>mydata = transform(mydata, Attrition=Attrition-1) <br class="title-page-name"/># running the gbm model with 10 fold cross validation to identify the number of trees to build - hyper parameter tuning <br class="title-page-name"/>gbm.model = gbm(Attrition~., data=mydata, shrinkage=0.01, distribution = 'bernoulli', cv.folds=10, n.trees=3000, verbose=F) <br class="title-page-name"/># identifying and printing the value of hyper parameter identified through the tuning above <br class="title-page-name"/>best.iter = gbm.perf(gbm.model, method="cv") <br class="title-page-name"/>print(best.iter) <br class="title-page-name"/># setting the seed for reproducibility <br class="title-page-name"/>set.seed(123) <br class="title-page-name"/># creating a copy of the dataset <br class="title-page-name"/>mydata1=mydata <br class="title-page-name"/># converting target to a factor <br class="title-page-name"/>mydata1$Attrition=as.factor(mydata1$Attrition) <br class="title-page-name"/># setting up cross validation controls <br class="title-page-name"/>fitControl = trainControl(method="repeatedcv", number=10,repeats=10) <br class="title-page-name"/># runing the gbm model in tandem with caret  <br class="title-page-name"/>caretmodel = train(Attrition~., data=mydata1, method="gbm", distribution="bernoulli",  trControl=fitControl, verbose=F, tuneGrid=data.frame(.n.trees=best.iter, .shrinkage=0.01, .interaction.depth=1, .n.minobsinnode=1)) <br class="title-page-name"/># printing the model summary <br class="title-page-name"/>print(caretmodel) </pre></div>
<div class="calibre25">
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
</div>
<div class="calibre25">
<pre class="calibre16">2623 <br class="title-page-name"/>Stochastic Gradient Boosting  <br class="title-page-name"/><br class="title-page-name"/>1470 samples <br class="title-page-name"/>  30 predictors <br class="title-page-name"/>   2 classes: '0', '1'  <br class="title-page-name"/><br class="title-page-name"/>No pre-processing <br class="title-page-name"/>Resampling: Cross-Validated (10 fold, repeated 10 times)  <br class="title-page-name"/>Summary of sample sizes: 1323, 1323, 1323, 1322, 1323, 1323, ...  <br class="title-page-name"/>Resampling results: <br class="title-page-name"/>  Accuracy   Kappa     <br class="title-page-name"/>  0.8771472  0.4094991 <br class="title-page-name"/>Tuning parameter 'n.trees' was held constant at a value of 2623 <br class="title-page-name"/>Tuning parameter 'shrinkage' was held constant at a value of 0.01 <br class="title-page-name"/>Tuning parameter 'n.minobsinnode' was held constant at a value of 1 </pre></div>
<div class="calibre25">
<p class="mce-root"><span class="calibre4">You will see that with the GBM model, we have achieved accuracy above 87%, which is better accuracy compared to the 84% achieved with KNN.</span></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building attrition prediction model with XGBoost</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now, let's implement the attrition prediction model with XGBoost:</p>
<div class="calibre25">
<pre class="calibre16"># loading the required libraries and registering the cores for multiprocessing <br class="title-page-name"/>library(doMC) <br class="title-page-name"/>library(xgboost) <br class="title-page-name"/>library(caret) <br class="title-page-name"/>registerDoMC(cores=4) <br class="title-page-name"/># setting the working directory and loading the dataset <br class="title-page-name"/>setwd("~/Desktop/chapter 2") <br class="title-page-name"/>mydata &lt;- read.csv("WA_Fn-UseC_-HR-Employee-Attrition.csv") <br class="title-page-name"/># removing the non-discriminatory features from the dataset as identified in EDA step <br class="title-page-name"/>mydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL <br class="title-page-name"/># setting up cross validation parameters <br class="title-page-name"/>ControlParamteres &lt;- trainControl(method = "repeatedcv",number = 10, repeats=10, savePredictions = TRUE, classProbs = TRUE) <br class="title-page-name"/># setting up hyper parameters grid to tune   <br class="title-page-name"/>parametersGrid &lt;-  expand.grid(eta = 0.1, colsample_bytree=c(0.5,0.7), max_depth=c(3,6),nrounds=100, gamma=1, min_child_weight=2,subsample=0.5) <br class="title-page-name"/># printing the parameters grid to get an intuition <br class="title-page-name"/>print(parametersGrid) <br class="title-page-name"/># xgboost model building <br class="title-page-name"/>modelxgboost &lt;- train(Attrition~., data = mydata, method = "xgbTree", trControl = ControlParamteres, tuneGrid=parametersGrid) <br class="title-page-name"/># printing the model summary <br class="title-page-name"/>print(modelxgboost) </pre></div>
<div class="calibre25">
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter18" src="assets/d37882b4-f89b-42d7-a2d2-69f6c208a1fc.png"/></p>
</div>
<div class="calibre25">
<pre class="calibre16">eXtreme Gradient Boosting  <br class="title-page-name"/>1470 samples <br class="title-page-name"/>  30 predictors <br class="title-page-name"/>   2 classes: 'No', 'Yes'  <br class="title-page-name"/><br class="title-page-name"/>No pre-processing <br class="title-page-name"/>Resampling: Cross-Validated (10 fold, repeated 10 times)  <br class="title-page-name"/>Summary of sample sizes: 1323, 1323, 1322, 1323, 1323, 1322, ...  <br class="title-page-name"/>Resampling results across tuning parameters: <br class="title-page-name"/><br class="title-page-name"/>  max_depth  colsample_bytree  Accuracy   Kappa     <br class="title-page-name"/>  3          0.5               0.8737458  0.3802840 <br class="title-page-name"/>  3          0.7               0.8734728  0.3845053 <br class="title-page-name"/>  6          0.5               0.8730674  0.3840938 <br class="title-page-name"/>  6          0.7               0.8732589  0.3920721 <br class="title-page-name"/><br class="title-page-name"/>Tuning parameter 'nrounds' was held constant at a value of 100 <br class="title-page-name"/>Tuning parameter 'min_child_weight' was held constant at a value of 2 <br class="title-page-name"/>Tuning parameter 'subsample' was held constant at a value of 0.5 <br class="title-page-name"/>Accuracy was used to select the optimal model using the largest value. <br class="title-page-name"/>The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.1, gamma = 1, colsample_bytree = 0.5, min_child_weight = 2 and subsample = 0.5. </pre></div>
<div class="calibre25">
<p class="mce-root">Again, we observed that with XGBoost model, we have achieved an accuracy above 87%, which is a better accuracy compared to the 84% achieved with KNN.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stacking </h1>
                </header>
            
            <article>
                
<div class="calibre25">
<p class="mce-root">In all the ensembles we have learned about so far, we have manipulated the dataset in certain ways and exposed subsets of the data for model building. However, in stacking, we are not going to do anything with the dataset; instead we are going to apply a different technique that involves using multiple ML algorithms instead. <span class="calibre4">In stacking, we build multiple models with various ML algorithms. Each algorithm possesses a unique way of learning the characteristics of data and the final stacked model indirectly incorporates all those unique ways of learning. Stacking gets the combined power of several ML algorithms through getting the final prediction by means of voting or averaging as we do in other types of ensembles.</span></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building attrition prediction model with stacking</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="calibre4">Let's build an attrition prediction model with stacking:</span></p>
<div class="calibre25">
<pre class="calibre16"># loading the required libraries and registering the cpu cores for multiprocessing <br class="title-page-name"/>library(doMC) <br class="title-page-name"/>library(caret) <br class="title-page-name"/>library(caretEnsemble) <br class="title-page-name"/>registerDoMC(cores=4) <br class="title-page-name"/># setting the working directory and loading the dataset <br class="title-page-name"/>setwd("~/Desktop/chapter 2") <br class="title-page-name"/>mydata &lt;- read.csv("WA_Fn-UseC_-HR-Employee-Attrition.csv") <br class="title-page-name"/># removing the non-discriminatory features from the dataset as identified in EDA step <br class="title-page-name"/>mydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL <br class="title-page-name"/># setting up control paramaters for cross validation <br class="title-page-name"/>control &lt;- trainControl(method="repeatedcv", number=10, repeats=10, savePredictions=TRUE, classProbs=TRUE) <br class="title-page-name"/># declaring the ML algorithms to use in stacking <br class="title-page-name"/>algorithmList &lt;- c('C5.0', 'nb', 'glm', 'knn', 'svmRadial') <br class="title-page-name"/># setting the seed to ensure reproducibility of the results <br class="title-page-name"/>set.seed(10000) <br class="title-page-name"/># creating the stacking model <br class="title-page-name"/>models &lt;- caretList(Attrition~., data=mydata, trControl=control, methodList=algorithmList) <br class="title-page-name"/># obtaining the stacking model results and printing them <br class="title-page-name"/>results &lt;- resamples(models) <br class="title-page-name"/>summary(results) </pre></div>
<div class="calibre25">
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
</div>
<div class="calibre25">
<pre class="calibre16">summary.resamples(object = results) <br class="title-page-name"/><br class="title-page-name"/>Models: C5.0, nb, glm, knn, svmRadial  <br class="title-page-name"/>Number of resamples: 100  <br class="title-page-name"/><br class="title-page-name"/>Accuracy  <br class="title-page-name"/>               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's <br class="title-page-name"/>C5.0      0.8082192 0.8493151 0.8639456 0.8625833 0.8775510 0.9054054    0 <br class="title-page-name"/>nb        0.8367347 0.8367347 0.8378378 0.8387821 0.8424658 0.8435374    0 <br class="title-page-name"/>glm       0.8299320 0.8639456 0.8775510 0.8790444 0.8911565 0.9387755    0 <br class="title-page-name"/>knn       0.8027211 0.8299320 0.8367347 0.8370763 0.8438017 0.8630137    0 <br class="title-page-name"/>svmRadial 0.8287671 0.8648649 0.8775510 0.8790467 0.8911565 0.9319728    0 <br class="title-page-name"/><br class="title-page-name"/>Kappa  Min.          1st Qu.     Median     Mean   3rd Qu.      Max.  NA's <br class="title-page-name"/>C5.0   0.03992485 0.29828006 0.37227344 0.3678459 0.4495049 0.6112590    0 <br class="title-page-name"/>nb     0.00000000 0.00000000 0.00000000 0.0000000 0.0000000 0.0000000    0 <br class="title-page-name"/>glm    0.26690604 0.39925723 0.47859218 0.4673756 0.5218094 0.7455280    0 <br class="title-page-name"/>knn   -0.05965697 0.02599388 0.06782465 0.0756081 0.1320451 0.2431312    0 <br class="title-page-name"/>svmRadial 0.24565 0.38667527 0.44195662 0.4497538 0.5192393 0.7423764    0 <br class="title-page-name"/><br class="title-page-name"/># Identifying the correlation between results <br class="title-page-name"/>modelCor(results) </pre></div>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter19" src="assets/6659bd76-cb08-42b4-9b50-2fb804feba66.png"/></p>
<p class="mce-root">We can see from the correlation table results that none of the individual ML algorithm predictions are highly correlated. Very highly correlated results mean that the algorithms have produced very similar predictions. Combining the very similar predictions may not really yield significant benefit compared with what one <span class="calibre4">would </span>avail from accepting the individual predictions. In this specific case, we can observe that none of the algorithm predictions are highly correlated so we can straightforwardly move to the next step of stacking the predictions:</p>
<pre class="calibre16"># Setting up the cross validation control parameters for stacking the predictions from individual ML algorithms <br class="title-page-name"/>stackControl &lt;- trainControl(method="repeatedcv", number=10, repeats=10, savePredictions=TRUE, classProbs=TRUE) <br class="title-page-name"/># stacking the predictions of individual ML algorithms using generalized linear model <br class="title-page-name"/>stack.glm &lt;- caretStack(models, method="glm", trControl=stackControl) <br class="title-page-name"/># printing the stacked final results <br class="title-page-name"/>print(stack.glm) </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">A glm ensemble of 2 base models: C5.0, nb, glm, knn, svmRadial <br class="title-page-name"/>Ensemble results: <br class="title-page-name"/>Generalized Linear Model  <br class="title-page-name"/>14700 samples <br class="title-page-name"/>    5 predictors <br class="title-page-name"/>    2 classes: 'No', 'Yes'  <br class="title-page-name"/>No pre-processing <br class="title-page-name"/>Resampling: Cross-Validated (10 fold, repeated 10 times)  <br class="title-page-name"/>Summary of sample sizes: 13230, 13230, 13230, 13230, 13230, 13230, ...  <br class="title-page-name"/>Resampling results: <br class="title-page-name"/>  Accuracy   Kappa     <br class="title-page-name"/>  0.8844966  0.4869556 </pre>
<p class="mce-root">With GLM-based stacking, we have 88% accuracy. Let's now examine the effect of using random forest modeling instead of GLM to stack the individual predictions from each of the five ML algorithms on the observations:</p>
<pre class="calibre16"># stacking the predictions of individual ML algorithms using random forest <br class="title-page-name"/>stack.rf &lt;- caretStack(models, method="rf", trControl=stackControl) <br class="title-page-name"/># printing the summary of rf based stacking <br class="title-page-name"/>print(stack.rf) </pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">A rf ensemble of 2 base models: C5.0, nb, glm, knn, svmRadial <br class="title-page-name"/>Ensemble results: <br class="title-page-name"/>Random Forest  <br class="title-page-name"/>14700 samples <br class="title-page-name"/>    5 predictors <br class="title-page-name"/>    2 classes: 'No', 'Yes'  <br class="title-page-name"/>No pre-processing <br class="title-page-name"/>Resampling: Cross-Validated (10 fold, repeated 10 times)  <br class="title-page-name"/>Summary of sample sizes: 13230, 13230, 13230, 13230, 13230, 13230, ...  <br class="title-page-name"/>Resampling results across tuning parameters: <br class="title-page-name"/>  mtry  Accuracy   Kappa     <br class="title-page-name"/>  2     0.9122041  0.6268108 <br class="title-page-name"/>  3     0.9133605  0.6334885 <br class="title-page-name"/>  5     0.9132925  0.6342740 <br class="title-page-name"/>Accuracy was used to select the optimal model using the largest value.<br class="title-page-name"/>The final value used for the model was mtry = 3. </pre>
<p class="mce-root">We see that without much effort, we were able to achieve an accuracy of 91% by stacking the predictions. Now, let's explore the working principle of stacking.</p>
<p class="mce-root">At last, we have discovered the various ensembling techniques that can provide us with better performing models. However, before ending the chapter, there are a couple of things we need to take a note of.</p>
<div class="packtinfobox"><span>There is not just one way to implement ML models in R. For example, bagging can be implemented using functions available in the <kbd class="calibre24">ipred</kbd> library and not by using <kbd class="calibre24">caret</kbd> as we did in this chapter. We should be aware that hyperparameter tuning forms an important part of model building to avail the best performing model. The number of hyperparameters and the acceptable values for those hyperparameters vary depending on the library that we intend to use. This is the reason why we paid less attention to hyperparameter tuning in the models we built in this chapter. Nevertheless, it is very important to read up the library documentation to understand the hyperparameters that can be tuned with a library function. In most cases, incorporating hyperparameter tuning in models significantly improves the model's performance.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">To recollect, we were using a class-imbalanced dataset to build the attrition model. Using techniques to resolve the class imbalance prior to model building is another key aspect of getting better model performance measurements. We used bagging, randomization, boosting, and stacking to implement and predict the attrition model. We were able to accomplish 91% accuracy just by using the features that were readily available in the models. Feature engineering is a crucial aspect whose role cannot be ignored in ML models. This may be one other path to explore to improve model performance <span class="calibre4">further</span>.</p>
<div class="title-page-name">
<p class="mce-root">In the next chapter, we will explore the secret recipe of recommending products or content through building a personalized recommendation engines. I am all set to implement a project to recommend jokes. Turn to the next chapter to continue the journey of learning.</p>
</div>


            </article>

            
        </section>
    </body></html>