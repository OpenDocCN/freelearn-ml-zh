<html><head></head><body>
		<div id="_idContainer101">
			<h1 id="_idParaDest-112"><em class="italic"><a id="_idTextAnchor115"/>Chapter 6</em>: AWS Services for Data Processing</h1>
			<p>In the previous chapter, we learned about several ways of storing data in AWS. In this chapter, we will explore the techniques for using that data and gaining some insight from the data. There are use cases where you have to process your data or load the data to a hive data warehouse to query and analyze the data. If you are on AWS and your data is in S3, then you have to create a table in hive on AWS EMR to query them. To provide the same as a managed service, AWS has a product called Athena, where you have to create a data catalog and query your data on S3. If you need to transform the data, then AWS Glue is the best option to transform and restore it to S3. Let's imagine a use case where we need to stream the data and create analytical reports on that data. For such scenarios, we can opt for AWS Kinesis Data Streams to stream data and store it in S3. Using Glue, the same data can be copied to Redshift for further analytical utilization. Now, let's learn about them and we will cover the following in brief. </p>
			<ul>
				<li>Using Glue for designing ETL jobs</li>
				<li>Querying S3 data using Athena</li>
				<li>Streaming data through AWS Kinesis Data Streams and storing it using Kinesis Firehose</li>
				<li>Ingesting data from on-premises locations to AWS</li>
				<li>Migrating data to AWS and extending on-premises data centers to AWS</li>
				<li>Processing data on AWS</li>
			</ul>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor116"/>Technical requirements</h1>
			<p>You can download the data used in the examples from GitHub, available here: <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6</a>.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor117"/>Creating ETL jobs on AWS Glue</h1>
			<p>In a modern data pipeline, there are<a id="_idIndexMarker472"/> multiple stages, such as Generate Data, Collect Data, Store Data, Perform ETL, Analyze, and Visualize. In this section, we will cover each of these at <a id="_idIndexMarker473"/>a high level and <a id="_idIndexMarker474"/>understand the <strong class="bold">ETL</strong> (<strong class="bold">extract</strong>, <strong class="bold">transform</strong>, <strong class="bold">load</strong>) part in-depth:</p>
			<ul>
				<li>Data can be generated from several devices, including mobile devices or IoT, weblogs, social media, transactional data, online games, and many more besides. </li>
				<li>This huge amount of generated data can be collected by using polling services or through API gateways integrated with AWS Lambda to collect the data, or via streams such as AWS Kinesis or AWS-managed Kafka or Kinesis Firehose. If you have an on-premises database and you want to collect that data to AWS, then you choose AWS DMS for that. You can sync your on-premises data to Amazon S3, Amazon EFS, or Amazon FSx via AWS DataSync. AWS Snowball is used to collect/transfer data into and out of AWS.</li>
				<li>The next step involves storing data, and we have learned some of the services in the previous chapter, such as AWS S3, EBS, EFS, Amazon RDS, Amazon Redshift, and DynamoDB.</li>
				<li>Once we know our data storage, an ETL job can be designed to extract-transform-load or extract-load-transform our structured or unstructured data into our desired format for further analysis. For example, we can think of AWS Lambda to transform the data on the fly and store the transformed data into S3, or we can run a Spark application on an EMR cluster to transform the data and store it in S3 or Redshift or RDS.</li>
				<li>There are many services available in AWS for performing an analysis on transformed data; for example, Amazon EMR, Amazon Athena, Amazon Redshift, Amazon Redshift Spectrum, and Kinesis Analytics. </li>
				<li>Once the data is analyzed, you can visualize the data using AWS Quicksight to understand the pattern or trend. Data scientists or machine learning professionals would love to apply statistical analysis to understand data distribution in a better way. Business users use it to prepare reports. We have already learned various ways to present and visualize data in <a href="B16735_04_Final_VK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Understanding and Visualizing Data</em>.</li>
			</ul>
			<p>What we understood<a id="_idIndexMarker475"/> from the traditional data pipeline is that ETL is all about coding and maintaining code on the servers to run smoothly. If the data format changes in any way, then the code needs to be changed, and that results in a <a id="_idIndexMarker476"/>change to the target schema. If the data source changes, then the code must be able to handle that too and it's an overhead. <em class="italic">Should we write code to recognize these changes in data sources? Should we need a system to adapt to the change and discover the data for us?</em> The answer is <strong class="bold">AWS Glue</strong>. Now, let's understand why AWS Glue is so famous.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor118"/>Features of AWS Glue</h2>
			<p>AWS Glue is a completely managed <a id="_idIndexMarker477"/>serverless ETL service on AWS. It has the following features:</p>
			<ul>
				<li>It automatically discovers and categorizes your data by connecting to the data sources and generates a data catalog.</li>
				<li>Services such as Amazon Athena, Amazon Redshift, and Amazon EMR can use the data catalog to query the data.</li>
				<li>AWS Glue generates the ETL code, which is an extension to Spark in Python or Scala, which can be modified, too.</li>
				<li>It scales out automatically to match your Spark application requirements for running the ETL job and loading the data into the destination.</li>
			</ul>
			<p>AWS Glue has <strong class="bold">Data Catalog</strong>, and that's the<a id="_idIndexMarker478"/> secret of its success. It helps to discover the data from data sources and lets us understand a bit about it:</p>
			<ul>
				<li>Data Catalog automatically discovers new data and extracts schema definitions. It detects schema changes and version tables. It detects Apache Hive-style partitions on Amazon S3.</li>
				<li>Data Catalog comes with built-in classifiers for popular data types. Custom classifiers can be<a id="_idIndexMarker479"/> written using <strong class="bold">Grok expressions</strong>. The classifiers help to detect the schema.</li>
				<li>Glue crawlers can be run ad hoc or in a scheduled fashion to update the metadata in Glue Data Catalog. Glue crawlers must be associated with an IAM role with sufficient access to read the data sources, such as Amazon RDS, Amazon Redshift, and Amazon S3.</li>
			</ul>
			<p>As we now have a<a id="_idIndexMarker480"/> brief idea of AWS Glue, let's run the following example to get our hands dirty.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor119"/>Getting hands-on with AWS Glue data catalog components</h2>
			<p>In this example, we will <a id="_idIndexMarker481"/>create a job to copy data from S3 to Redshift by using AWS Glue. All my components are created in the <strong class="source-inline">us-east-1</strong> region. Let's start by creating a bucket: </p>
			<ol>
				<li>Navigate to AWS S3 Console and create a bucket. I have named the bucket <strong class="source-inline">aws-glue-example-01</strong>.</li>
				<li>Click on <strong class="bold">Create Folder</strong> and name it <strong class="source-inline">input-data</strong>.</li>
				<li>Navigate inside the folder and click on the <strong class="bold">Upload</strong> button to upload the <strong class="source-inline">sales-records.csv</strong> dataset. The data is available in the following GitHub location: <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6/AWS-Glue-Demo/input-data">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6/AWS-Glue-Demo/input-data</a>.<p>As we have the data uploaded in the S3 bucket, let's create a VPC in which we will create our Redshift cluster.</p></li>
				<li>Navigate to the VPC console by accessing the <a href="https://console.aws.amazon.com/vpc/home?region=us-east-1#">https://console.aws.amazon.com/vpc/home?region=us-east-1#</a> URL and click on <strong class="bold">Endpoints</strong> on the left-hand side menu. Click on <strong class="bold">Create Endpoint</strong> and then fill in the fields as shown here:<p>a) <strong class="bold">Service Category</strong>: <strong class="source-inline">AWS services</strong></p><p>b) <strong class="bold">Select a service</strong>: <strong class="source-inline">com.amazonaws.us-east-1.s3</strong> (Gateway type)</p><p>c) <strong class="bold">VPC</strong>: <strong class="source-inline">Select the Default VPC</strong> (we will use this default VPC in which our Redshift cluster will <a id="_idIndexMarker482"/>be created)</p></li>
				<li>Leave the other fields as-is and click on <strong class="bold">Create Endpoint</strong>. </li>
				<li>Click on <strong class="bold">Security Groups</strong> from the VPC console. Give a name to your security group, such as <strong class="source-inline">redshift-self</strong>, and choose the default VPC drop-down menu. Provide an appropriate description of <strong class="source-inline">Redshift Security Group</strong>. Click on <strong class="bold">Create security group</strong>.</li>
				<li>Click on the <strong class="bold">Actions</strong> dropdown and select <strong class="bold">Edit Inbound rules</strong>. Click on the Add rule and complete the fields as shown here: <p>a) <strong class="bold">Type</strong>: <strong class="source-inline">All traffic</strong></p><p>b) <strong class="bold">Source</strong>: <strong class="source-inline">Custom</strong></p><p>c) In the search field, select the same security group <strong class="source-inline">(redshift-self)</strong></p></li>
				<li>Click on <strong class="bold">Save Rules</strong>.<p>Now, let's create our Redshift cluster.</p></li>
				<li>Navigate to the Amazon Redshift console. Click on <strong class="bold">Create Cluster</strong> and complete the highlighted fields, as shown in <em class="italic">Figure 6.1</em>:<div id="_idContainer093" class="IMG---Figure"><img src="image/B16735_06_01.jpg" alt="Figure 6.1 – A screenshot of Amazon Redshift's Create cluster&#13; &#10;"/></div><p class="figure-caption">Figure 6.1 – A screenshot of Amazon Redshift's Create cluster</p></li>
				<li>Scroll down <a id="_idIndexMarker483"/>and fill in the highlighted fields, as shown in <em class="italic">Figure 6.2</em>: <div id="_idContainer094" class="IMG---Figure"><img src="image/B16735_06_02.jpg" alt="Figure 6.2 – A screenshot of Amazon Redshift Cluster's Database configurations section&#13;&#10;"/></div><p class="figure-caption">Figure 6.2 – A screenshot of Amazon Redshift Cluster's Database configurations section</p></li>
				<li>Scroll down and <a id="_idIndexMarker484"/>change the <strong class="bold">Additional configurations</strong> field, as shown in <em class="italic">Figure 6.3</em>:<div id="_idContainer095" class="IMG---Figure"><img src="image/B16735_06_03.jpg" alt="Figure 6.3 – A screenshot of Amazon Redshift Cluster's Additional configurations section&#13;&#10;"/></div><p class="figure-caption">Figure 6.3 – A screenshot of Amazon Redshift Cluster's Additional configurations section</p></li>
				<li>Change the IAM<a id="_idIndexMarker485"/> permissions, too, as shown in <em class="italic">Figure 6.4</em>:<div id="_idContainer096" class="IMG---Figure"><img src="image/B16735_06_04.jpg" alt="Figure 6.4 – A screenshot of Amazon Redshift Cluster's Cluster permissions section&#13;&#10;"/></div><p class="figure-caption">Figure 6.4 – A screenshot of Amazon Redshift Cluster's Cluster permissions section</p></li>
				<li>Scroll down and click on <strong class="bold">Create Cluster</strong>. It will take a minute or two to get the cluster in the available state.<p>Next, we will <a id="_idIndexMarker486"/>create an IAM role.</p></li>
				<li>Navigate to the AWS IAM console and select <strong class="bold">Roles</strong> in the <strong class="bold">Access Management</strong> section on the screen. </li>
				<li>Click on the <strong class="bold">Create role</strong> button and choose <strong class="bold">Glue</strong> from the services. Click on the <strong class="bold">Next: permissions</strong> button to navigate to the next page.</li>
				<li>Search <strong class="bold">AmazonS3FullAccess</strong> and select. Then, search <strong class="bold">AWSGlueServiceRole</strong> and select. As we are writing our data to Redshift as part of this example, select <strong class="bold">AmazonRedshiftFullAccess</strong>. Click on <strong class="bold">Next: Tags</strong>, followed by the <strong class="bold">Next: Review</strong> button.</li>
				<li>Provide a name, <strong class="source-inline">Glue-IAM-Role</strong>, and then click on the <strong class="bold">Create role</strong> button. The role appears as shown in <em class="italic">Figure 6.5</em>:<div id="_idContainer097" class="IMG---Figure"><img src="image/B16735_06_05.jpg" alt="Figure 6.5 – A screenshot of the IAM role&#13;&#10;"/></div><p class="figure-caption">Figure 6.5 – A screenshot of the IAM role</p><p>Now, we have<a id="_idIndexMarker487"/> the input data source and the output data storage handy. The next step is to create the Glue crawler from the AWS Glue console.</p></li>
				<li>Select <strong class="bold">Connections</strong> under <strong class="bold">Databases</strong>. Click on the <strong class="bold">Add connection</strong> button and complete the fields as shown here:<p>a) <strong class="bold">Connection name</strong>: <strong class="source-inline">glue-redshift-connection</strong></p><p>b) <strong class="bold">Connection type</strong>: <strong class="source-inline">Amazon Redshift</strong></p></li>
				<li>Click on <strong class="bold">Next</strong> and then fill in the fields as shown here:<p>a) <strong class="bold">Cluster</strong>: <strong class="source-inline">redshift-glue-example</strong></p><p>b) <strong class="bold">Database name</strong>: <strong class="source-inline">glue-dev</strong></p><p>c) <strong class="bold">Username</strong>: <strong class="source-inline">awsuser</strong></p><p>d) <strong class="bold">Password</strong>: <strong class="source-inline">********</strong> (enter the value as created in <em class="italic">step 10</em>)</p></li>
				<li>Click on <strong class="bold">Next</strong> and then <strong class="bold">Finish</strong>. To verify the working, click on <strong class="bold">Test Connection</strong>, select <strong class="source-inline">Glue-IAM-Role</strong> for the IAM role section, and then click <strong class="bold">Test Connection</strong>.</li>
				<li>Go to <strong class="bold">Crawler</strong> and select <strong class="bold">Add Crawler</strong>. Provide a name for the crawler, <strong class="source-inline">s3-glue-crawler</strong>, and then click <strong class="bold">Next</strong>. In the <strong class="bold">Specify crawler source type</strong> page, leave everything as<a id="_idIndexMarker488"/> their default settings and then click <strong class="bold">Next</strong>. </li>
				<li>On <strong class="bold">the Add a data store</strong> page, choose <strong class="bold">Include path</strong>: <strong class="source-inline">s3://aws-glue-example-01/input-data/sales-records.csv</strong>.</li>
				<li>Click <strong class="bold">Next</strong>.</li>
				<li><strong class="bold">Add another datastore</strong>: <strong class="source-inline">No</strong>. Click <strong class="bold">Next</strong>.</li>
				<li><strong class="bold">Choose an existing IAM Role</strong>: <strong class="source-inline">Glue-IAM-Role</strong>. Then click <strong class="bold">Next</strong>.</li>
				<li><strong class="bold">Frequency</strong>: <strong class="source-inline">Run on demand</strong>. Click <strong class="bold">Next</strong>.</li>
				<li>There's no database created, so click on <strong class="bold">Add database</strong>, provide a name, <strong class="source-inline">s3-data</strong>, click <strong class="bold">Next</strong>, and then click <strong class="bold">Finish</strong>.</li>
				<li>Select the crawler, <strong class="source-inline">s3-glue-crawler</strong>, and then click on <strong class="bold">Run Crawler</strong>. Once the run is complete, you can see that there is a <strong class="source-inline">1</strong> in the <strong class="bold">Tables Added</strong> column. This means that a database, <strong class="source-inline">s3-data</strong>, has been created, as mentioned in the previous step, and a table is added. Click on <strong class="bold">Tables</strong> and select the newly created table, <strong class="source-inline">sales_records_csv</strong>. You can see that the schema has been discovered now. You can change the data type if the inferred data type does not meet your requirements.</li>
			</ol>
			<p>In this hands-on section, we learned about database tables, database connections, crawlers on S3, and creation of the Redshift cluster. In the next hands-on section, we will learn about creating ETL jobs using Glue.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor120"/>Getting hands-on with AWS Glue ETL components</h2>
			<p>In this section, we will use the <a id="_idIndexMarker489"/>Data Catalog components created earlier to build our job. Let's start by creating jobs:</p>
			<ol>
				<li value="1">Navigate to the AWS Glue console and click on <strong class="bold">Jobs</strong> under the <strong class="bold">ETL</strong> section.</li>
				<li>Click on the <strong class="bold">Add Job</strong> button and complete the fields as shown here:<ul><li><strong class="bold">Name</strong>: <strong class="source-inline">s3-glue-redshift</strong></li><li><strong class="bold">IAM role</strong>: <strong class="source-inline">Glue-IAM-Role</strong> (this is the same role we created in the previous section)</li><li><strong class="bold">Type</strong>: <strong class="source-inline">Spark</strong></li><li><strong class="bold">Glue version</strong>: <strong class="source-inline">Spark 2.4, Python 3 with improved job start up times (Glue version 2.0)</strong></li></ul></li>
				<li>Leave the other fields as they are and then click on <strong class="bold">Next</strong>.</li>
				<li>Select <strong class="source-inline">sales_records_csv</strong> and click on <strong class="bold">Next</strong>.</li>
				<li>Select <strong class="bold">Change Schema</strong> by default and then click <strong class="bold">Next</strong> (at the time of writing this book, machine learning transformations are not supported for Glue 2.0).</li>
				<li>Select <strong class="bold">Create tables in your data target</strong>. Choose <strong class="source-inline">JDBC</strong> as the data store and <strong class="source-inline">glue-redshift-connection</strong> as the connection. Provide <strong class="source-inline">glue-dev</strong> as the database name (as created in the previous section) and then click <strong class="bold">Next</strong>.</li>
				<li>Next comes the <strong class="bold">Output Schema Definition</strong> page, where you can choose the desired columns to be removed from the target schema. Scroll down and click on <strong class="bold">Save job and edit script</strong>.</li>
				<li>You can now see the pipeline being created on the left-hand side of the screen and the suggested code on the right-hand side, as shown in <em class="italic">Figure 6.6</em>. You can modify the code based on your requirements. Click on the <strong class="bold">Run job</strong> button. A pop-up window appears, asking you to edit any details that you wish to change. This is optional. Then, click on the <strong class="bold">Run job</strong> button:<div id="_idContainer098" class="IMG---Figure"><img src="image/B16735_06_06.jpg" alt="Figure 6.6 – A screenshot of the AWS Glue ETL job&#13;&#10;"/></div><p class="figure-caption">Figure 6.6 – A screenshot of the AWS Glue ETL job</p></li>
				<li>Once the job is<a id="_idIndexMarker490"/> successful, navigate to Amazon Redshift and click on <strong class="bold">Query editor</strong>.</li>
				<li>Set the database name as <strong class="source-inline">glue-dev</strong> and then provide the username and password to create a connection.</li>
				<li>Select the <strong class="source-inline">public</strong> schema and now you can query the table to see the records, as shown in <em class="italic">Figure 6.7</em>:</li>
			</ol>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B16735_06_07.jpg" alt="Figure 6.7 – A screenshot of Amazon Redshift's Query editor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – A screenshot of Amazon Redshift's Query editor</p>
			<p>We now understand how to<a id="_idIndexMarker491"/> create an ETL job using AWS Glue to copy the data from the S3 bucket to Amazon Redshift. We also queried the data in Amazon Redshift using the query editor from the UI console. It is recommended to delete the Redshift cluster and AWS Glue job if you have completed the steps successfully. AWS creates two buckets in your account to store the AWS Glue scripts and AWS Glue temporary results. Therefore, delete these as well to save costs. We will use the data catalog created on S3 data in the next section.</p>
			<p>In the following section, we will learn about querying the S3 data using Athena.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor121"/>Querying S3 data using Athena</h1>
			<p>Athena is a serverless service<a id="_idIndexMarker492"/> designed for querying data stored in S3. It is serverless because the client doesn't manage the servers that are used for computation:</p>
			<ul>
				<li>Athena uses a schema to present the<a id="_idIndexMarker493"/> results against the query on the data stored in S3. You define how you want your data to appear in the form of a schema and Athena reads the raw data from S3 to show the results as per the defined schema. </li>
				<li>The output can be used by other services for visualization, storing, or various analytics purposes. The source data in S3 can be in any of the following structured, semi-structured, and unstructured data formats, including XML, JSON, CSV/TSV, AVRO, Parquet, ORC, and more. CloudTrail, ELB Logs, and VPC flow logs can also be stored in S3 and analyzed by Athena. </li>
				<li>This follows the schema-on-read technique. Unlike traditional techniques, tables are defined in advance in a data catalog, and data is projected when it reads. SQL-like queries are allowed on data without transforming the source data.</li>
			</ul>
			<p>Now, let's understand this <a id="_idIndexMarker494"/>with the help of an example, where we will use <strong class="bold">AWSDataCatlog</strong> created in AWS Glue on the S3 data and query them using Athena:</p>
			<ol>
				<li value="1">Navigate to the<a id="_idIndexMarker495"/> AWS Athena console. Select <strong class="source-inline">AWSDataCatalog</strong> from <strong class="bold">Data source</strong> (if you're doing this for the first time, then a <strong class="source-inline">sampledb</strong> database will be created with a table, <strong class="source-inline">elb_logs</strong>, in the AWS Glue data catalog).</li>
				<li>Select <strong class="source-inline">s3-data</strong> as the database. </li>
				<li>Click on <strong class="bold">Settings</strong> from the top-right corner and fill in the details as shown in <em class="italic">Figure 6.8</em> (I have used the same bucket as in the previous example and a different folder):<div id="_idContainer100" class="IMG---Figure"><img src="image/B16735_06_08.jpg" alt="Figure 6.8 – A screenshot of Amazon Athena's settings&#13;&#10;"/></div><p class="figure-caption">Figure 6.8 – A screenshot of Amazon Athena's settings</p></li>
				<li>The next<a id="_idIndexMarker496"/> step is to write your query in the query editor and<a id="_idIndexMarker497"/> execute it. Once your execution is complete, please delete your S3 buckets and AWS Glue data catalogs. This will save you money.</li>
			</ol>
			<p>In this section, we learned about querying S3 data using Amazon Athena through the AWS data catalog. You can also create your schema and query the data from S3. In the next section, we will learn about Amazon Kinesis Data Streams.</p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor122"/>Processing real-time data using Kinesis data streams</h1>
			<p>Kinesis is Amazon's streaming<a id="_idIndexMarker498"/> service and can be scaled based on requirements. It is highly available in a region. It has a level of persistence that retains data for 24 hours by default or optionally up to 365 days. Kinesis data streams are used for large-scale data ingestion, analytics, and monitoring:</p>
			<ul>
				<li>Kinesis can be ingested by multiple producers and multiple consumers can also read data from the stream. Let's understand this by means of an example in real time. Suppose you have a producer ingesting data to a Kinesis stream and the default retention period is 24 hours, which means the data ingested at 05:00:00 A.M. today will be available in the stream until 04:59:59 A.M. tomorrow. This data won't be available beyond that point and ideally, this should be consumed before it expires or can be stored somewhere if it's critical. The retention period can be extended to a maximum of 365 days at an extra cost.</li>
				<li>Kinesis can<a id="_idIndexMarker499"/> be used for real-time analytics or dashboard visualization. Producers can be imagined as a piece of code pushing data into the Kinesis stream, and it can be an EC2 instance running the code, a Lambda function, an IoT device, on-premises servers, mobile applications or devices, and so on. </li>
				<li>Similarly, the consumer can also be a piece of code running on an EC2 instance, Lambda function, or on-premises servers that know how to connect to a kinesis stream, read the data, and apply some action to the data. AWS provides triggers to invoke a lambda consumer as soon as data arrives at the kinesis stream. </li>
				<li>Kinesis is scalable due to its <a id="_idIndexMarker500"/>shard architecture, which is the fundamental throughput unit of a kinesis stream. <em class="italic">What is a shard?</em> A shard is a logical structure that partitions the data based on a partition key. A shard supports a writing capacity of <em class="italic">1 MB/sec</em> and a reading capacity of <em class="italic">2 MB/sec</em>. <em class="italic">1,000</em> <strong class="source-inline">PUT</strong> records per second are supported by a single shard. If you have created a stream with <em class="italic">3 shards</em>, then <em class="italic">3 MB/sec write throughput</em> and <em class="italic">6 MB/sec read throughput</em> can be achieved and this allows <em class="italic">3,000</em> <strong class="source-inline">PUT</strong> records. So, with more shards, you have to pay an extra amount to get higher performance. </li>
				<li>The data in a shard is stored via a Kinesis data record and can be a maximum of 1 MB. Kinesis data records are stored across the shard based on the partition key. It also has a sequence number. A sequence number is assigned by kinesis as soon as a <strong class="source-inline">putRecord</strong> or <strong class="source-inline">putRecords</strong> API operation is performed so as to uniquely identify a record. The partition key is specified by the producer while adding the data to the Kinesis data stream, and the partition key is responsible for segregating and routing the record to different shards in the stream to balance the load. </li>
				<li>There are two ways to<a id="_idIndexMarker501"/> encrypt the data in a kinesis stream: server-side encryption and client-side encryption. Client-side encryption is hard to implement and manage the keys because the client has to encrypt the data before putting it into the stream and decrypt the data after reading from the stream. With server-side encryption enabled via <strong class="bold">AWS/KMS</strong>, the data is automatically encrypted and decrypted as you put the data and get it from a stream. <p class="callout-heading">Note</p><p class="callout">Amazon Kinesis shouldn't be confused with Amazon SQS. Amazon SQS supports one production group and one consumption group. If your use case demands multiple users sending data and receiving data, then Kinesis is the solution. </p><p class="callout">For decoupling and asynchronous communications, SQS is the solution, because the sender and receiver do not need to be aware of one another. </p><p class="callout">In SQS, there is no concept of persistence. Once the message is read, the next step is deletion. There's no concept of the retention time window for Amazon SQS. If your use case demands large-scale ingestion, then Kinesis should be used. </p></li>
			</ul>
			<p>In the next section, we will learn about storing the streamed data for further analysis.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor123"/>Storing and transforming real-time data using Kinesis Data Firehose</h1>
			<p>There are a lot of use<a id="_idIndexMarker502"/> cases demanding the data to be streamed and stored for future analytics purposes. To overcome such problems, you can write a kinesis consumer to read the Kinesis stream and store the data in S3. This solution needs an instance or a machine to run the code with the required access to read from the stream and write to S3. The other possible option would be to run a Lambda function that gets triggered on the <strong class="source-inline">putRecord</strong> or <strong class="source-inline">putRecords</strong> API made to the stream and reads the data from the stream to store in the S3 bucket:</p>
			<ul>
				<li>To make this easy, Amazon <a id="_idIndexMarker503"/> provides a separate service called Kinesis Data Firehose. This can easily be plugged into a Kinesis data stream and it will require essential IAM roles to write data into S3. This is a fully managed service to reduce the load of managing servers and code. It also supports loading the streamed data into Amazon Redshift, Amazon Elasticsearch Service, and Splunk. Kinesis Data Firehose scales automatically to match the throughput of the data.</li>
				<li>Data can be transformed via an AWS Lambda function before storing or delivering it to the destination. If you want to build a raw data lake with the untransformed data, then by enabling source record backup, you can store it in another S3 bucket prior to the transformation. </li>
				<li>With the help of AWS/KMS, data can be encrypted following delivery to the S3 bucket. It has to be enabled while creating the delivery stream. Data can also be compressed in supported formats such as GZIP, ZIP, and SNAPPY compression formats.</li>
			</ul>
			<p>In the next section, we will learn about different AWS services used for ingesting data from on-premises servers to AWS.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor124"/>Different ways of ingesting data from on-premises into AWS</h1>
			<p>With th<a id="_idTextAnchor125"/>e increasing demand for <a id="_idIndexMarker504"/>data-driven use cases, managing data on on-premises servers is pretty tough at the moment. Taking backups is not easy when you deal with a huge amount of data. This data in data lakes is being used to build deep neural networks, to create a data warehouse to extract meaningful information from it, to run analytics, and to generate reports. </p>
			<p>Now, if we look at the available options to migrate data into AWS, then it comes with various challenges, too. For example, if you want to send data to S3, then you have to write a few lines of code to send your data to AWS. You will have to manage the code and servers to run the code. It has to be ensured that the data is commuting via the HTTPS network. You need to verify whether the data transfer was successful. This adds complexity as well as time and effort challenges to the process. To avoid such scenarios, AWS provides services to <a id="_idIndexMarker505"/>match or solve your use cases by designing hybrid infrastructure that allows data sharing between the on-premises data centers and AWS. Let's learn about these in the following sections.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor126"/>AWS Storage Gateway</h2>
			<p>Storage Gateway is a hybrid storage <a id="_idIndexMarker506"/>virtual appliance. It can run in three different modes – <strong class="bold">File Gateway</strong>, <strong class="bold">Tape Gateway</strong>, and <strong class="bold">Volume Gateway</strong>. It can be used for extension, migration, and <a id="_idIndexMarker507"/>backups of the<a id="_idIndexMarker508"/> on-premises data center to AWS: </p>
			<ul>
				<li>In Tape Gateway mode, the<a id="_idIndexMarker509"/> storage gateway stores virtual tapes on S3, and when ejected and archived, the tapes are moved from S3 to Glacier. Active tapes are stored in S3 for storage and retrieval. Archived or exported tapes are stored in <strong class="bold">Virtual Tape Shelf </strong>(<strong class="bold">VTS</strong>) in Glacier. Virtual <a id="_idIndexMarker510"/>tapes can be created and can range in size from 100 GiB to 5 TiB. A total of 1 petabyte of storage can be configured locally and an unlimited number of tapes can be archived to Glacier. This is ideal for an existing backup system on tape and where there is a need to migrate backup data into AWS. You can decommission the physical tape hardware later.</li>
				<li>In File Gateway mode, the storage gateway maps files onto S3 objects, which can be stored using one of the available storage classes. This helps you to extend the data center into AWS. You can load more files to your File Gateway and these are stored as S3 objects. It can run on your on-premises virtual server, which connects to various <a id="_idIndexMarker511"/>devices using <strong class="bold">Server Message Block (SMB)</strong> or <strong class="bold">Network File System (NFS)</strong>. The<a id="_idIndexMarker512"/> File Gateway connects to AWS using an HTTPS public endpoint to store the data on S3 objects. Life cycle policies can be applied to those S3 objects. You can easily integrate your <strong class="bold">Active</strong> <strong class="bold">Directory</strong> (<strong class="bold">AD</strong>) with File Gateway to control access to the files on the file share. </li>
				<li>In Volume Gateway mode, the storage gateway presents block storage. There are two ways of using this, one is <strong class="bold">Gateway Cached</strong>, and the other is <strong class="bold">Gateway Stored</strong>:</li>
				<li><strong class="bold">Gateway Stored</strong> is a volume storage gateway running locally on-premises and it has local <a id="_idIndexMarker513"/>storage and an upload buffer. A total of 32 volumes can be created and each volume can be up to 16 TB in size for a total capacity of 512 TB. Primary data <a id="_idIndexMarker514"/>is stored on-premises and backup data is asynchronously replicated to AWS in the background. Volumes are made available via <strong class="bold">Internet Small Computer Systems Interface (iSCSI)</strong> for network-based servers to access. It connects to <strong class="bold">Storage Gateway Endpoint</strong> via an HTTPS public endpoint and creates <a id="_idIndexMarker515"/>EBS snapshots from backup data. These snapshots can be used to create standard EBS volumes. This option is ideal for migration to AWS or disaster recovery or business continuity. The local system will still use the local volume, but the EBS snapshots are in AWS, which can be used instead of backups. It's not the best option for data center extension because you require a huge amount of local storage.</li>
				<li><strong class="bold">Gateway Cached</strong> is a volume <a id="_idIndexMarker516"/>storage gateway running locally on-premises and it has cache storage and an upload buffer. The difference is that the data that is added to the storage gateway is not local but uploaded to AWS. Primary data is stored in AWS. Frequently accessed data is cached locally. This is an ideal option for extending <a id="_idIndexMarker517"/>the on-premises data center to AWS. It connects to <strong class="bold">Storage Gateway Endpoint</strong> via an HTTPS public endpoint and creates <strong class="bold">S3 Backed Volume (AWS-managed bucket)</strong> snapshots that are stored as standard EBS snapshots.</li>
			</ul>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor127"/>Snowball, Snowball Edge, and Snowmobile</h2>
			<p>They belong to the same product clan for the physical transfer of data between business operating locations and AWS. For moving a large amount of data into and out of AWS, you can use any of the three:</p>
			<ul>
				<li><strong class="bold">Snowball</strong>: This can be ordered from <a id="_idIndexMarker518"/>AWS by logging a job. AWS delivers the device to load your data and send it back. Data in <a id="_idIndexMarker519"/>Snowball is encrypted using KMS. It comes with two capacity ranges, one 50 TB and the other 80 TB. It is economical to order one or more Snowball devices for data between 10 TB and 10 PB. The device can be sent to different premises. It does not have any compute capability; it only comes with storage capability.</li>
				<li><strong class="bold">Snowball Edge</strong>: This is like<a id="_idIndexMarker520"/> Snowball, but it comes with both storage and computes capability. It has a larger capacity than<a id="_idIndexMarker521"/> Snowball. It offers fastened networking, such as 10 Gbps over RJ45, 10/25 Gb over SFP28, and 40/100 Gb+ over QSFP+ copper. This is ideal for the secure and quick transfer of terabytes to petabytes of data into AWS.</li>
				<li><strong class="bold">Snowmobile</strong>: This is a portable data <a id="_idIndexMarker522"/>center within a shipping container on a truck. This allows you to move exabytes of<a id="_idIndexMarker523"/> data from on-premises to AWS. If your data size exceeds 10 PB, then Snowmobile is preferred. Essentially, upon requesting Snowmobile, a truck is driven to your location, you plug your data center into the truck, and transfer the data. If you have multiple sites, choosing Snowmobile for data transfer is not an ideal option. </li>
			</ul>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor128"/>AWS DataSync</h2>
			<p>AWS DataSync is designed to <a id="_idIndexMarker524"/>move data from on-premises storage to AWS, or vice versa:</p>
			<ul>
				<li>It is an ideal product from AWS for data processing transfers, archival or cost-effective storage, disaster recovery, business continuity, and data migrations. </li>
				<li>It has a special data validation feature that verifies the original data with the data in AWS, as soon as the data arrives in AWS. In other words, it checks the integrity of the data.</li>
				<li>Let's understand this product in depth by considering an example of an on-premises data center that has SAN/NAS storage. When we run the AWS DataSync agent on a VMWare platform, this agent is capable of communicating with the NAS/SAN storage via an NFS/SMB protocol. Once it is on, it communicates with the AWS DataSync endpoint and, from there, it can connect with several different types of location, including various S3 storage<a id="_idIndexMarker525"/> classes or VPC-based resources, such as the <strong class="bold">Elastic</strong> <strong class="bold">File</strong> <strong class="bold">System</strong> (<strong class="bold">EFS</strong>) and FSx for Windows Server.</li>
				<li>It allows you to schedule data transfers during specific periods. By configuring the built-in bandwidth throttle, you can limit the amount of network bandwidth that DataSync uses.</li>
			</ul>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor129"/>Processing stored data on AWS</h1>
			<p>There are several services for processing the data stored in AWS. We will go through AWS Batch and AWS EMR (Elastic MapReduce) in this section. EMR is a product from AWS that primarily runs<a id="_idIndexMarker526"/> MapReduce jobs and Spark applications in a managed way. AWS Batch is used for long-running, compute-heavy workloads.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor130"/>AWS EMR</h2>
			<p>EMR is a managed<a id="_idIndexMarker527"/> implementation of Apache Hadoop provided as a service by AWS. It includes other components of the Hadoop ecosystem, such as Spark, HBase, Flink, Presto, Hive, Pig, and many more. We will not<a id="_idIndexMarker528"/> cover these in detail for the certification exam:</p>
			<ul>
				<li>EMR clusters can be launched from the AWS console or via the AWS CLI with a specific number of nodes. The cluster can be a long-term cluster or an ad hoc cluster. If you have a long-running traditional cluster, then you have to configure the machines and manage them yourself. If you have jobs to be executed faster, then you need to manually add a cluster. In the case of EMR, these admin overheads are gone. You can request any number of nodes to EMR and it manages and launches the nodes for you. If you have autoscaling enabled on the cluster, then with the increased requirement, EMR launches new nodes in the cluster and decommissions the nodes once the load is reduced.</li>
				<li>EMR uses EC2 instances in the background and runs in one availability zone in a VPC. This enables faster network speeds between the nodes. AWS Glue uses EMR clusters in the background, where users do not need to worry about the operational understanding of AWS EMR.</li>
				<li>From a use case standpoint, EMR can be used for processing or transforming the data stored in S3 and outputs data to be stored in S3. EMR uses nodes (EC2 instances) as the computing units for data processing. EMR nodes come in different variants, including Master node, Core node, and Task node.</li>
				<li>The EMR master node acts as a Hadoop namenode and manages the cluster and its health. It is responsible for distributing the job workload among the other core nodes and task nodes. If you have SSH enabled, then you can connect to the master node instance and access the cluster. </li>
				<li>The EMR cluster can have one or more core nodes. If you relate to the Hadoop ecosystem, then core nodes are similar to Hadoop data nodes for HDFS and they are responsible for running tasks therein. </li>
				<li>Task nodes are optional and they don't have HDFS storage. They are responsible for running tasks. If a task node fails for some reason, then this does not impact HDFS storage, but a core node failure causes HDFS storage interruptions.</li>
				<li>EMR has its filesystem called <a id="_idIndexMarker529"/>EMRFS. It is backed by S3 and this feature makes it regionally resilient. If a core node fails, the data is still safe in S3. HDFS is efficient in terms of I/O and faster than EMRFS.</li>
			</ul>
			<p>In the following<a id="_idIndexMarker530"/> section, we will learn about AWS Batch, which is a managed batch processing compute service that can be used for long-running services. </p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor131"/>AWS Batch</h2>
			<p>This is a managed batch processing<a id="_idIndexMarker531"/> product. If you are using <a id="_idIndexMarker532"/>AWS Batch, then jobs can be run without end user interaction or can be scheduled to run:</p>
			<ul>
				<li>Imagine an event-driven application that launches a Lambda function to process the data stored in S3. If the processing time goes beyond 15 minutes, then Lambda has the execution time limit. For such scenarios, AWS Batch is a better solution, where computation-heavy workloads can be scheduled or driven through API events. </li>
				<li>AWS Batch is a good fit for use cases where a longer processing time is required or more computation resources are needed. </li>
				<li>AWS Batch runs a job that can be a script or an executable. One job can depend on another job. A job needs a definition, such as who can run a job (with IAM permissions), where the job can be run (resources to be used), mount points, and other metadata. </li>
				<li>Jobs are submitted to queues where they wait for compute environment capacity. These queues are associated with one or more compute environments. </li>
				<li>Compute environments do the actual work of executing the jobs. These can be ECS or EC2 instances, or any computing resources. You can define their sizes and capacities too. </li>
				<li>Environments<a id="_idIndexMarker533"/> receive jobs from the queues based on their priority and execute the jobs. They can be managed or unmanaged compute environments. </li>
				<li>AWS Batch can<a id="_idIndexMarker534"/> store the metadata in DynamoDB for further use and can also store the output to the S3 bucket. <p class="callout-heading">Note</p><p class="callout">If you get a question in exams on an event-style workload that requires flexible compute, a higher disk space, no time limit (more than 15 minutes), or an effective resource limit, then choose AWS Batch.</p></li>
			</ul>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor132"/>Summary</h1>
			<p>In this chapter, we learned about different ways of processing data in AWS. We also learned the capabilities in terms of extending our data centers to AWS, migrating data to AWS, and the ingestion process. We also learned the various ways of using the data to process it and make it ready for analysis in our way. We understood the magic of having a data catalog that helps us to query our data via AWS Glue and Athena.</p>
			<p>In the next chapter, we will learn about various machine learning algorithms and their usages.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor133"/>Questions</h2>
			<ol>
				<li value="1">If you have a large number of IoT devices sending data to AWS to be consumed by a large number of mobile devices, which of the following should you choose?<p>A. SQS standard queue</p><p>B. SQS FIFO queue</p><p>C. Kinesis stream</p></li>
				<li>If you have a requirement to decouple a high-volume application, which of the following should you choose?<p>A. SQS standard queue</p><p>B. SQS FIFO queue</p><p>C. Kinesis stream</p></li>
				<li>Which of the following do I need to change to improve the performance of a Kinesis stream?<p>A. The read capacity unit of a stream</p><p>B. The write capacity unit of a stream</p><p>C. Shards of a stream</p><p>D. The region of a stream </p></li>
				<li>Which of the following ensures data integrity?<p>A. AWS Kinesis</p><p>B. AWS DataSync</p><p>C. AWS EMR</p><p>D. Snowmobile</p></li>
				<li>Which storage gateway mode can replace a tape drive with S3 storage?<p>A. Volume Gateway Stored</p><p>B. Volume Gateway Cached</p><p>C. File</p><p>D. VTL</p></li>
				<li>Which storage gateway mode can be used to present storage over SMB to clients?<p>A. Volume Gateway Stored</p><p>B. Volume Gateway Cached</p><p>C. File</p><p>D. VTL</p></li>
				<li>Which storage gateway mode is ideal for data center extension to AWS?<p>A. Volume Gateway Stored</p><p>B. Volume Gateway Cached</p><p>C. File</p><p>D. VTL</p></li>
				<li>What storage product in AWS can be used for Windows environments for shared storage?<p>A. S3</p><p>B. FSx</p><p>C. EBS</p><p>D. EFS</p></li>
				<li>Which node within an EMR cluster handles operations?<p>A. Master node</p><p>B. Core node</p><p>C. Task node</p><p>D. Primary node</p></li>
				<li>Which nodes in an EMR cluster are good for spot instances?<p>A. Master node</p><p>B. Core node</p><p>C. Task node</p><p>D. Primary node</p></li>
				<li>If you have large quantities of streaming data and add it to Redshift, which services would you use (choose three)?<p>A. Kinesis Data Streams</p><p>B. Kinesis Data Firehose</p><p>C. S3</p><p>D. SQS</p><p>E. Kinesis Analytics</p></li>
				<li>Kinesis Firehose supports data transformation using Lambda.<p>A. True</p><p>B. False</p></li>
				<li>Which of the following are valid destinations for Kinesis Data Firehose (choose five)?<p>A. HTTP</p><p>B. Splunk</p><p>C. Redshift</p><p>D. S3</p><p>E. Elastic Search</p><p>F. EC2</p><p>G. SQS</p></li>
			</ol>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor134"/>Answers</h2>
			<p>1. C</p>
			<p>2. A</p>
			<p>3. C</p>
			<p>4. B</p>
			<p>5. D</p>
			<p>6. C</p>
			<p>7. B</p>
			<p>8. B</p>
			<p>9. A</p>
			<p>10. C</p>
			<p>11. A, B, C</p>
			<p>12. A</p>
			<p>13. A, B, C, D, E</p>
		</div>
	</body></html>