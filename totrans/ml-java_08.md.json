["```py\n<dependency> \n    <groupId>org.deeplearning4j</groupId> \n    <artifactId>deeplearning4j-nlp</artifactId> \n   <version>${dl4j.version}</version> \n</dependency> \n\n<dependency> \n    <groupId>org.deeplearning4j</groupId> \n    <artifactId>deeplearning4j-core</artifactId> \n    <version>${dl4j.version}</version> \n</dependency> \n```", "```py\nimport org.deeplearning4j.datasets.iterator.DataSetIterator; \nimport org.deeplearning4j.datasets.iterator.impl.*; \n```", "```py\nint numRows = 28; \nint numColumns = 28; \nint outputNum = 10;\nint numSamples = 60000;\nint batchSize = 100;\nint iterations = 10;\nint seed = 123;\nDataSetIterator iter = new MnistDataSetIterator(batchSize, \nnumSamples,true);  \n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() \n```", "```py\n.seed(seed) \n.gradientNormalization(GradientNormalization.ClipElementWiseAbsolu\n   teValue) \n   .gradientNormalizationThreshold(1.0) \n   .iterations(iterations) \n   .momentum(0.5) \n   .momentumAfter(Collections.singletonMap(3, 0.9)) \n   .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT) \n```", "```py\n.list(1) \n.layer(0, new  \nOutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD) \n.activation(\"softmax\") \n.nIn(numRows*numColumns).nOut(outputNum).build()) \n```", "```py\n   .pretrain(true).backprop(false) \n   .build(); \n```", "```py\nMultiLayerNetwork model = new MultiLayerNetwork(conf); \nmodel.init(); \n```", "```py\nmodel.setListeners(Collections.singletonList((IterationListener) \n   new ScoreIterationListener(listenerFreq))); \n```", "```py\nmodel.fit(iter);  \n```", "```py\nEvaluation eval = new Evaluation(outputNum); \n```", "```py\nDataSetIterator testIter = new MnistDataSetIterator(100,10000); \nwhile(testIter.hasNext()) { \n    DataSet testMnist = testIter.next(); \n    INDArray predict2 =  \n    model.output(testMnist.getFeatureMatrix()); \n    eval.eval(testMnist.getLabels(), predict2); \n} \n```", "```py\nlog.info(eval.stats()); \n```", "```py\n    Accuracy:  0.8945 \n    Precision: 0.8985\n    Recall:    0.8922\n    F1 Score:  0.8953\n\n```", "```py\nMultiLayerConfiguration conf = new \n   NeuralNetConfiguration.Builder() \n```", "```py\n    .seed(seed) \n    .gradientNormalization( \n    GradientNormalization.ClipElementWiseAbsoluteValue) \n    .gradientNormalizationThreshold(1.0) \n    .iterations(iterations) \n    .momentum(0.5) \n    .momentumAfter(Collections.singletonMap(3, 0.9)) \n    .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT) \n```", "```py\n   .list(4) \n```", "```py\n.layer(0, new RBM.Builder() \n.nIn(numRows*numColumns) \n.nOut(500)          \n.weightInit(WeightInit.XAVIER) \n.lossFunction(LossFunction.RMSE_XENT) \n.visibleUnit(RBM.VisibleUnit.BINARY) \n.hiddenUnit(RBM.HiddenUnit.BINARY) \n.build()) \n```", "```py\n.layer(1, new RBM.Builder() \n.nIn(500) \n.nOut(250) \n.weightInit(WeightInit.XAVIER) \n.lossFunction(LossFunction.RMSE_XENT) \n.visibleUnit(RBM.VisibleUnit.BINARY) \n.hiddenUnit(RBM.HiddenUnit.BINARY) \n.build()) \n.layer(2, new RBM.Builder() \n.nIn(250) \n.nOut(200) \n.weightInit(WeightInit.XAVIER) \n.lossFunction(LossFunction.RMSE_XENT) \n.visibleUnit(RBM.VisibleUnit.BINARY) \n.hiddenUnit(RBM.HiddenUnit.BINARY) \n.build()) \n```", "```py\n.layer(3, new OutputLayer.Builder() \n.nIn(200) \n.nOut(outputNum) \n.lossFunction(LossFunction.NEGATIVELOGLIKELIHOOD) \n.activation(\"softmax\") \n.build()) \n.pretrain(true).backprop(false) \n.build(); \n```", "```py\nMultiLayerConfiguration.Builder conf = new \n   NeuralNetConfiguration.Builder() \n```", "```py\n.seed(seed) \n.iterations(iterations) \n.activation(\"sigmoid\") \n.weightInit(WeightInit.DISTRIBUTION) \n.dist(new NormalDistribution(0.0, 0.01)) \n.learningRate(1e-3) \n.learningRateScoreBasedDecayRate(1e-1) \n.optimizationAlgo( \nOptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) \n```", "```py\n.list(7) \n```", "```py\n.layer(0, new ConvolutionLayer.Builder( \n    new int[]{5, 5}, new int[]{1, 1}) \n    .name(\"cnn1\") \n    .nIn(numRows*numColumns) \n    .nOut(6) \n    .build()) \n```", "```py\n.layer(1, new SubsamplingLayer.Builder( \nSubsamplingLayer.PoolingType.MAX,  \nnew int[]{2, 2}, new int[]{2, 2}) \n.name(\"maxpool1\") \n.build()) \n```", "```py\n.layer(2, new ConvolutionLayer.Builder(new int[]{5, 5}, new \n   int[]{1, 1}) \n    .name(\"cnn2\") \n    .nOut(16) \n    .biasInit(1) \n    .build()) \n.layer(3, new SubsamplingLayer.Builder\n   (SubsamplingLayer.PoolingType.MAX, new \n   int[]{2, 2}, new int[]{2, 2}) \n    .name(\"maxpool2\") \n    .build()) \n```", "```py\n.layer(4, new DenseLayer.Builder() \n    .name(\"ffn1\") \n    .nOut(120) \n    .build()) \n.layer(5, new DenseLayer.Builder() \n    .name(\"ffn2\") \n    .nOut(84) \n    .build()) \n```", "```py\n.layer(6, new OutputLayer.Builder\n   (LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD) \n    .name(\"output\") \n    .nOut(outputNum) \n    .activation(\"softmax\") // radial basis function required \n    .build()) \n.backprop(true) \n.pretrain(false) \n.cnnInputSize(numRows,numColumns,1); \n```"]