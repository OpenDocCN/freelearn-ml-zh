<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Feature Selection</h1>
                </header>
            
            <article>
                
<p class="mce-root">We're halfway through our text and we have gotten our hands dirty with about a dozen datasets and have seen a great deal of feature selection methods that we, as data scientists and machine learning engineers, may utilize in our work and lives to ensure that we are getting the most out of our predictive modeling. So far, in dealing with data, we have worked with methods including:</p>
<ul>
<li>Feature understanding through the identification of levels of data</li>
<li>Feature improvements and imputing missing values</li>
<li>Feature standardization and normalization</li>
</ul>
<p>Each of the preceding methods has a place in our data pipeline and, more often than not, two or more methods are used in tandem with one another.</p>
<p>The remainder of this text will focus on other methods of feature engineering that are, by nature, a bit more mathematical and complex than in the first half of this book. As the preceding workflow grows, we will do our best to spare the reader the inner workings of each and every statistical test we invoke and instead convey a broader picture of what the tests are trying to achieve. As authors and instructors, we are always open to your questions about any of the inner mechanisms of this work.</p>
<p>We have come across one problem quite frequently in our discussion of features, and that problem is <strong>noise</strong>. Often, we are left working with features that may not be highly predictive of the response and, sometimes, can even hinder our models' performance in the prediction of the response. We used tools such as standardization and normalization to try to mitigate such damage, but at the end of the day, noise must be dealt with.</p>
<p>In this chapter, we will address a subset of feature engineering called <strong>feature selection</strong>, which is the process of selecting which features, from the original batch of features, are the <em>best</em> when it comes to the model prediction pipeline. More formally, given <em>n</em> features, we search for a subset of <em>k</em>, where <em>k &lt; n</em> features that improve our machine learning pipeline. This generally comes down to the statement:</p>
<div><em>Feature Selection attempts to weed out the noise in our data and remove it.</em></div>
<p>The definition of feature selection touches on two major points that must be addressed:</p>
<ul>
<li>The methods in which we may find the subset of <em>k</em> features</li>
<li>The definition of <em>better</em> in the context of machine learning</li>
</ul>
<p>The majority of this chapter is dedicated to the methods in which we may find such subsets of features and the basis on which such methods work. This chapter will break up the methods of feature selection into two broad subsections: <strong>statistical-based</strong> and <strong>model-based</strong> feature selection. This separation may not 100% capture the complexity of the science and art of feature selection, but will work to drive real and actionable results in our machine learning pipeline.</p>
<p>Before we dive into the deep end of many of these methods, let's first discuss how we may better understand and define the idea of <em>better</em>, as it will frame the remainder of this chapter, as well as framing the remainder of this text.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Achieving better performance in feature engineering</li>
<li>Creating a baseline machine learning pipeline</li>
<li>The types of feature selection</li>
<li>Choosing the right feature selection method</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Achieving better performance in feature engineering</h1>
                </header>
            
            <article>
                
<p>Throughout this book, we have relied on a base definition of <em>better</em> when it came to the various feature engineering methods we put into place. Our implicit goal was to achieve better predictive performance measured purely on simple metrics such as accuracy for classification tasks and RMSE for regression tasks (mostly accuracy). There are other metrics we may measure and track to gauge predictive performance. For example, we will use the following metrics for classification:</p>
<ul>
<li>True and false positive rate</li>
<li>Sensitivity (AKA true positive rate) and specificity</li>
<li>False negative and false positive rate</li>
</ul>
<p>and for regression, the metrics that will be applied are:</p>
<ul>
<li>Mean absolute error</li>
<li>R<sup>2</sup></li>
</ul>
<p>These lists go on, and while we will not be abandoning the idea of quantifying performance through metrics such as the ones precedingly listed, we may also measure other <em>meta metrics</em>, or metrics that do not directly correlate to the performance of the prediction of the model, rather, so-called <strong>meta metrics</strong> attempt to measure the performance <em>around</em> the prediction and include such ideas as:</p>
<ul>
<li>Time in which the model needs to fit/train to the data</li>
<li>Time it takes for a fitted model to predict new instances of data</li>
<li>The size of the data in case data must be persisted (stored for later)</li>
</ul>
<p>These ideas will add to our definition of <em>better</em> machine learning as they help to encompass a much larger picture of our machine learning pipeline outside of model predictive performance. In order to help us track these metrics, let's create a function that is generic enough to evaluate several models but specific enough to give us metrics for each one. We will call our function <kbd>get_best_model_and_accuracy</kbd> and it will do many jobs, such as:</p>
<ul>
<li>It will search across all given parameters in order to optimize the machine learning pipeline</li>
<li>It will spit out some metrics that will help us assess the quality of the pipeline entered</li>
</ul>
<p>Let's go ahead and define such a function with the help of the following code:</p>
<pre># import out grid search module<br/>from sklearn.model_selection import GridSearchCV<br/><br/><br/>def get_best_model_and_accuracy(model, params, X, y):<br/>    grid = GridSearchCV(model, # the model to grid search<br/>                        params, # the parameter set to try <br/>                        error_score=0.) # if a parameter set raises an error, continue and set the performance as a big, fat 0<br/>    grid.fit(X, y) # fit the model and parameters<br/>    # our classical metric for performance<br/>    print "Best Accuracy: {}".format(grid.best_score_)<br/>    # the best parameters that caused the best accuracy<br/>    print "Best Parameters: {}".format(grid.best_params_)<br/>    # the average time it took a model to fit to the data (in seconds)<br/>    print "Average Time to Fit (s): {}".format(round(grid.cv_results_['mean_fit_time'].mean(), 3))<br/>    # the average time it took a model to predict out of sample data (in seconds)<br/>    # this metric gives us insight into how this model will perform in real-time analysis<br/>    print "Average Time to Score (s): {}".format(round(grid.cv_results_['mean_score_time'].mean(), 3))</pre>
<p><span>The overall goal of this function is to act as a ground truth in that we will use it to evaluate every feature selection method in this chapter to give us a sense of standardization of evaluation. This is not really any different to what we have been doing already, but we are now formalizing our work as a function, and also using metrics other than accuracy to grade our feature selection modules and machine learning pipelines.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A case study – a credit card defaulting dataset</h1>
                </header>
            
            <article>
                
<p><span>By intelligently extracting the most important signals from our data and ignoring noise, feature selection algorithms achieve two major outcomes:</span></p>
<ul>
<li><strong>Improved model performance</strong>: By removing redundant data, we are less likely to make decisions based on noisy and irrelevant data, and it also allows our models to hone in on the important features, thereby improving model pipeline predictive performance</li>
<li><strong>Reduced training and predicting time</strong>: By fitting pipelines to less data, this generally results in improved model fitting and predicting times, making our pipelines faster overall</li>
</ul>
<p>In order to gain a realistic understanding of how and why noisy data gets in the way, let's introduce our newest dataset, a credit card defaulting dataset. We will work with 23 features and one response variable. That response variable will be a Boolean, meaning it will either be True or False. The reason we are working with 23 features is that we want to see if we can find which of the 23 features will help us in our machine learning pipelines and which ones will hurt us. We can import the datasets using the following code:</p>
<pre>import pandas as pd<br/>import numpy as np<br/><br/># we will set a random seed to ensure that whenever we use random numbers <br/># which is a good amount, we will achieve the same random numbers<br/>np.random.seed(123)</pre>
<p>To start, let's bring in two common modules, <kbd>numpy</kbd> and <kbd>pandas</kbd>, and also set a random seed so that you and we will achieve the same results for consistency. Now, let's bring in our latest dataset, using the following code:</p>
<pre># archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients<br/># import the newest csv<br/>credit_card_default = pd.read_csv('../data/credit_card_default.csv')</pre>
<p>Let's go ahead and do some mandatory EDA. Let's begin by checking how big a dataset we are working with, using the following code:</p>
<pre># 30,000 rows and 24 columns<br/>credit_card_default.shape </pre>
<p>So, we have <kbd>30,000 rows</kbd> (<span>observations</span>) and <kbd>24 columns</kbd> (1 response and 23 features). We will not go in depth to describe the columns meanings at this time, but we do encourage the reader to check out the source of the data (<a href="http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#" target="_blank">http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#</a>). For now, we will rely on good old-fashioned statistics to tell us more:</p>
<pre># Some descriptive statistics<br/># We invoke the .T to transpose the matrix for better viewing<br/>credit_card_default.describe().T</pre>
<p>The output is as follows:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>count</strong></p>
</td>
<td>
<p><strong>mean</strong></p>
</td>
<td>
<p><strong>std</strong></p>
</td>
<td>
<p><strong>min</strong></p>
</td>
<td>
<p><strong>25%</strong></p>
</td>
<td>
<p><strong>50%</strong></p>
</td>
<td>
<p><strong>75%</strong></p>
</td>
<td>
<p><strong>max</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>LIMIT_BAL</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>167484.322667</span></p>
</td>
<td>
<p><span>129747.661567</span></p>
</td>
<td>
<p><span>10000.0</span></p>
</td>
<td>
<p><span>50000.00</span></p>
</td>
<td>
<p><span>140000.0</span></p>
</td>
<td>
<p><span>240000.00</span></p>
</td>
<td>
<p><span>1000000.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>SEX</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>1.603733</span></p>
</td>
<td>
<p><span>0.489129</span></p>
</td>
<td>
<p><span>1.0</span></p>
</td>
<td>
<p><span>1.00</span></p>
</td>
<td>
<p><span>2.0</span></p>
</td>
<td>
<p><span>2.00</span></p>
</td>
<td>
<p><span>2.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>EDUCATION</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>1.853133</span></p>
</td>
<td>
<p><span>0.790349</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>1.00</span></p>
</td>
<td>
<p><span>2.0</span></p>
</td>
<td>
<p><span>2.00</span></p>
</td>
<td>
<p><span>6.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>MARRIAGE</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>1.551867</span></p>
</td>
<td>
<p><span>0.521970</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>1.00</span></p>
</td>
<td>
<p><span>2.0</span></p>
</td>
<td>
<p><span>2.00</span></p>
</td>
<td>
<p><span>3.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>AGE</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>35.485500</span></p>
</td>
<td>
<p><span>9.217904</span></p>
</td>
<td>
<p><span>21.0</span></p>
</td>
<td>
<p><span>28.00</span></p>
</td>
<td>
<p><span>34.0</span></p>
</td>
<td>
<p><span>41.00</span></p>
</td>
<td>
<p><span>79.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_0</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>-0.016700</span></p>
</td>
<td>
<p><span>1.123802</span></p>
</td>
<td>
<p><span>-2.0</span></p>
</td>
<td>
<p><span>-1.00</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>0.00</span></p>
</td>
<td>
<p><span>8.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_2</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>-0.133767</span></p>
</td>
<td>
<p><span>1.197186</span></p>
</td>
<td>
<p><span>-2.0</span></p>
</td>
<td>
<p><span>-1.00</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>0.00</span></p>
</td>
<td>
<p><span>8.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_3</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>-0.166200</span></p>
</td>
<td>
<p><span>1.196868</span></p>
</td>
<td>
<p><span>-2.0</span></p>
</td>
<td>
<p><span>-1.00</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>0.00</span></p>
</td>
<td>
<p><span>8.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_4</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>-0.220667</span></p>
</td>
<td>
<p><span>1.169139</span></p>
</td>
<td>
<p><span>-2.0</span></p>
</td>
<td>
<p><span>-1.00</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>0.00</span></p>
</td>
<td>
<p><span>8.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_5</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>-0.266200</span></p>
</td>
<td>
<p><span>1.133187</span></p>
</td>
<td>
<p><span>-2.0</span></p>
</td>
<td>
<p><span>-1.00</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>0.00</span></p>
</td>
<td>
<p><span>8.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_6</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>-0.291100</span></p>
</td>
<td>
<p><span>1.149988</span></p>
</td>
<td>
<p><span>-2.0</span></p>
</td>
<td>
<p><span>-1.00</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>0.00</span></p>
</td>
<td>
<p><span>8.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>BILL_AMT1</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>51223.330900</span></p>
</td>
<td>
<p><span>73635.860576</span></p>
</td>
<td>
<p><span>-165580.0</span></p>
</td>
<td>
<p><span>3558.75</span></p>
</td>
<td>
<p><span>22381.5</span></p>
</td>
<td>
<p><span>67091.00</span></p>
</td>
<td>
<p><span>964511.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>BILL_AMT2</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>49179.075167</span></p>
</td>
<td>
<p><span>71173.768783</span></p>
</td>
<td>
<p><span>-69777.0</span></p>
</td>
<td>
<p><span>2984.75</span></p>
</td>
<td>
<p><span>21200.0</span></p>
</td>
<td>
<p><span>64006.25</span></p>
</td>
<td>
<p><span>983931.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>BILL_AMT3</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>47013.154800</span></p>
</td>
<td>
<p><span>69349.387427</span></p>
</td>
<td>
<p><span>-157264.0</span></p>
</td>
<td>
<p><span>2666.25</span></p>
</td>
<td>
<p><span>20088.5</span></p>
</td>
<td>
<p><span>60164.75</span></p>
</td>
<td>
<p><span>1664089.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>BILL_AMT4</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>43262.948967</span></p>
</td>
<td>
<p><span>64332.856134</span></p>
</td>
<td>
<p><span>-170000.0</span></p>
</td>
<td>
<p><span>2326.75</span></p>
</td>
<td>
<p><span>19052.0</span></p>
</td>
<td>
<p><span>54506.00</span></p>
</td>
<td>
<p><span>891586.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>BILL_AMT5</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>40311.400967</span></p>
</td>
<td>
<p><span>60797.155770</span></p>
</td>
<td>
<p><span>-81334.0</span></p>
</td>
<td>
<p><span>1763.00</span></p>
</td>
<td>
<p><span>18104.5</span></p>
</td>
<td>
<p><span>50190.50</span></p>
</td>
<td>
<p><span>927171.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>BILL_AMT6</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>38871.760400</span></p>
</td>
<td>
<p><span>59554.107537</span></p>
</td>
<td>
<p><span>-339603.0</span></p>
</td>
<td>
<p><span>1256.00</span></p>
</td>
<td>
<p><span>17071.0</span></p>
</td>
<td>
<p><span>49198.25</span></p>
</td>
<td>
<p><span>961664.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_AMT1</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>5663.580500</span></p>
</td>
<td>
<p><span>16563.280354</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>1000.00</span></p>
</td>
<td>
<p><span>2100.0</span></p>
</td>
<td>
<p><span>5006.00</span></p>
</td>
<td>
<p><span>873552.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_AMT2</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>5921.163500</span></p>
</td>
<td>
<p><span>23040.870402</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>833.00</span></p>
</td>
<td>
<p><span>2009.0</span></p>
</td>
<td>
<p><span>5000.00</span></p>
</td>
<td>
<p><span>1684259.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_AMT3</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>5225.681500</span></p>
</td>
<td>
<p><span>17606.961470</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>390.00</span></p>
</td>
<td>
<p><span>1800.0</span></p>
</td>
<td>
<p><span>4505.00</span></p>
</td>
<td>
<p><span>891586.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_AMT4</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>4826.076867</span></p>
</td>
<td>
<p><span>15666.159744</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>296.00</span></p>
</td>
<td>
<p><span>1500.0</span></p>
</td>
<td>
<p><span>4013.25</span></p>
</td>
<td>
<p><span>621000.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_AMT5</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>4799.387633</span></p>
</td>
<td>
<p><span>15278.305679</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>252.50</span></p>
</td>
<td>
<p><span>1500.0</span></p>
</td>
<td>
<p><span>4031.50</span></p>
</td>
<td>
<p><span>426529.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>PAY_AMT6</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>5215.502567</span></p>
</td>
<td>
<p><span>17777.465775</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>117.75</span></p>
</td>
<td>
<p><span>1500.0</span></p>
</td>
<td>
<p><span>4000.00</span></p>
</td>
<td>
<p><span>528666.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>default payment next month</strong></p>
</td>
<td>
<p><span>30000.0</span></p>
</td>
<td>
<p><span>0.221200</span></p>
</td>
<td>
<p><span>0.415062</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>0.00</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p><span>0.00</span></p>
</td>
<td>
<p><span>1.0</span></p>
</td>
</tr>
</tbody>
</table>
<div class="cell code_cell rendered selected">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<p> </p>
<p class="output_subarea output_html rendered_html output_result">The <strong>default payment next month</strong> <span>is our response column and everything else is a feature/potential predictor of default. It is wildly clear that our features exist on wildly different scales, so that will be a factor in how we handle the data and which models we will pick. In previous chapters, we dealt heavily with data and features on different scales using solutions such as <kbd>StandardScalar</kbd> and normalization to alleviate some of these issues; however, in this chapter, we will largely choose to ignore such problems in order to focus on more relevant issues.</span></p>
</div>
</div>
</div>
</div>
<div class="packt_infobox">In the final chapter of this book, we will focus on several case studies that will marry almost all of the techniques in this book on a longer-term analysis of a dataset.</div>
<p>As we have seen in previous chapters, we know that null values are a big issue when dealing with machine learning, so let's do a quick check to make sure that we don't have any to deal with:</p>
<pre># check for missing values, none in this dataset<br/>credit_card_default.isnull().sum()<br/>LIMIT_BAL                     0
SEX                           0
EDUCATION                     0
MARRIAGE                      0
AGE                           0
PAY_0                         0
PAY_2                         0
PAY_3                         0
PAY_4                         0
PAY_5                         0
PAY_6                         0
BILL_AMT1                     0
BILL_AMT2                     0
BILL_AMT3                     0
BILL_AMT4                     0
BILL_AMT5                     0
BILL_AMT6                     0
PAY_AMT1                      0
PAY_AMT2                      0
PAY_AMT3                      0
PAY_AMT4                      0
PAY_AMT5                      0
PAY_AMT6                      0
default payment next month    0
dtype: int64</pre>
<p>Phew! No missing values here. Again, we will deal with missing values again in future case studies, but for now, we have bigger fish to fry. Let's go ahead and set up some variables for our machine learning pipelines, using the following code:</p>
<pre># Create our feature matrix<br/>X = credit_card_default.drop('default payment next month', axis=1)<br/><br/># create our response variable<br/>y = credit_card_default['default payment next month']</pre>
<p>As usual, we created our <kbd>X</kbd> and <kbd>y</kbd> variables. Our <kbd>X</kbd> matrix will have 30,000 rows and 23 columns and our <kbd>y</kbd> is, as always, a 30,000 long pandas Series. Because we will be performing classification, we will, as usual, need to ascertain a null accuracy to ensure that our machine learning models are performing better than a baseline. We can get the null accuracy rate using the following code:</p>
<pre># get our null accuracy rate<br/>y.value_counts(normalize=True)<br/><br/>0    0.7788
1    0.2212</pre>
<p>So, the accuracy to beat, in this case, is <strong>77.88%</strong>, which is the percentage of people who did not default (0 meaning false to default). </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a baseline machine learning pipeline</h1>
                </header>
            
            <article>
                
<p>In previous chapters, we offered to you, the reader, a single machine learning model to use throughout the chapter. In this chapter, we will do some work to find the best machine learning model for our needs and then work to enhance that model with feature selection. We will begin by importing four different machine learning models:</p>
<ul>
<li>Logistic Regression</li>
<li>K-Nearest Neighbors</li>
<li>Decision Tree</li>
<li>Random Forest</li>
</ul>
<p>The code for importing the learning models is given as follows:</p>
<pre># Import four machine learning models<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import RandomForestClassifier</pre>
<p>Once we are finished importing these modules, we will run them through our <kbd>get_best_model_</kbd>and<kbd>_accuracy</kbd> functions to get a baseline on how each one handles the raw data. We will have to first establish some variables to do so. We will use the following code to do this:</p>
<pre># Set up some parameters for our grid search<br/># We will start with four different machine learning model parameters<br/><br/># Logistic Regression<br/>lr_params = {'C':[1e-1, 1e0, 1e1, 1e2], 'penalty':['l1', 'l2']}<br/><br/># KNN<br/>knn_params = {'n_neighbors': [1, 3, 5, 7]}<br/><br/># Decision Tree<br/>tree_params = {'max_depth':[None, 1, 3, 5, 7]}<br/><br/># Random Forest<br/>forest_params = {'n_estimators': [10, 50, 100], 'max_depth': [None, 1, 3, 5, 7]}</pre>
<div class="packt_tip">If you feel uncomfortable with any of the models listed above, we recommend reading up on documentation, or referring to the Packt book, <em>The Principles of Data Science</em>, <a href="https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science" target="_blank">https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science</a>, for a more detailed explanation of the algorithms.</div>
<p>Because we will be sending each model through our function, which invokes a grid search module, we need only create blank state models with no customized parameters set, as shown in the following code:</p>
<pre># instantiate the four machine learning models<br/>lr = LogisticRegression()<br/>knn = KNeighborsClassifier()<br/>d_tree = DecisionTreeClassifier()<br/>forest = RandomForestClassifier()</pre>
<p>We are now going to run each of the four machine learning models through our evaluation function to see how well (or not) they do against our dataset. Recall that our number to beat at the moment is .7788, the baseline null accuracy. We will use the following code to run the models:</p>
<pre><span>get_best_model_and_accuracy(lr, lr_params, X, y)</span><br/><br/>Best Accuracy: 0.809566666667
Best Parameters: {'penalty': 'l1', 'C': 0.1}
Average Time to Fit (s): 0.602
Average Time to Score (s): 0.002</pre>
<p>We can see that the logistic regression has already beaten the null accuracy using the raw data and, on average, took 6/10 of a second to fit to a training set and only 20 milliseconds to score. This makes sense if we know that to fit, a logistic regression in <strong>scikit-learn</strong> must create a large matrix in memory, but to predict, it need only multiply and add scalars to one another.</p>
<p>Now, let's do the same with the KNN model, using the following code:</p>
<pre>get_best_model_and_accuracy(knn, knn_params, X, y)<br/><br/>Best Accuracy: 0.760233333333
Best Parameters: {'n_neighbors': 7}
Average Time to Fit (s): 0.035
Average Time to Score (s): 0.88</pre>
<p>Our KNN model, as expected, does much better on the fitting time. This is because, to fit to the data, the KNN only has to store the data in such a way that it is easily retrieved at prediction time, where it takes a hit on time. It's also worth mentioning the painfully obvious fact that the accuracy is not even better than the null accuracy! You might be wondering why, and if you're saying <em>hey wait a minute, doesn't KNN utilize the Euclidean Distance in order to make predictions, which can be thrown off by non-standardized data, a flaw that none of the other three machine learning models suffer?</em>, then you're 100% correct.</p>
<p>KNN is a distance-based model, in that it uses a metric of closeness in space that assumes that all features are on the same scale, which we already know that our data is not on. So, for KNN, we will have to construct a more complicated pipeline to more accurately assess its baseline performance, using the following code:</p>
<pre># bring in some familiar modules for dealing with this sort of thing<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.preprocessing import StandardScaler<br/><br/># construct pipeline parameters based on the parameters<br/># for KNN on its own<br/>knn_pipe_params = {'classifier__{}'.format(k): v for k, v in knn_params.iteritems()}<br/><br/># KNN requires a standard scalar due to using Euclidean distance # as the main equation for predicting observations<br/>knn_pipe = Pipeline([('scale', StandardScaler()), ('classifier', knn)])<br/><br/># quick to fit, very slow to predict<br/>get_best_model_and_accuracy(knn_pipe, knn_pipe_params, X, y)<br/><br/>print knn_pipe_params  # {'classifier__n_neighbors': [1, 3, 5, 7]} <br/><br/>Best Accuracy: 0.8008 <br/>Best Parameters: {'classifier__n_neighbors': 7} <br/>Average Time to Fit (s): 0.035 <br/>Average Time to Score (s): 6.723</pre>
<p>The first thing to notice is that our modified code pipeline, which now includes a <kbd>StandardScalar</kbd> (which z-score normalizes our features) now beats the null accuracy at the very least, but also seriously hurts our predicting time, as we have added a step of preprocessing. So far, the logistic regression is in the lead with the best accuracy and the better overall timing of the pipeline. Let's move on to our two tree-based models and start with the simpler of the two, the decision tree, with the help of the following code:</p>
<div class="cell code_cell rendered selected">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_text output_stream output_stdout">
<pre>get_best_model_and_accuracy(d_tree, tree_params, X, y)<br/><br/>Best Accuracy: 0.820266666667
Best Parameters: {'max_depth': 3}
Average Time to Fit (s): 0.158
Average Time to Score (s): 0.002</pre></div>
</div>
</div>
</div>
</div>
<div class="cell code_cell rendered unselected">
<div class="input">
<p class="prompt input_prompt">Amazing! Already, we have a new lead in accuracy and, also, the decision tree is quick to both fit and predict. In fact, it beats logistic regression in its time to fit and beats the KNN in its time to predict. Let's finish off our test by evaluating a random forest, using the following code:</p>
</div>
<div>
<pre>get_best_model_and_accuracy(forest, forest_params, X, y)<br/><br/>Best Accuracy: 0.819566666667
Best Parameters: {'n_estimators': 50, 'max_depth': 7}
Average Time to Fit (s): 1.107
Average Time to Score (s): 0.044</pre></div>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython"><span>Much better than either the Logistic Regression or the KNN, but not better than the decision tree. Let's aggregate these results to see which model we should move forward with in optimizing using feature selection:</span></p>
</div>
</div>
</div>
</div>
<table>
<tbody>
<tr>
<td>
<p><strong>Model Name</strong></p>
</td>
<td>
<p><strong>Accuracy (%)</strong></p>
</td>
<td>
<p><strong>Fit Time (s)</strong></p>
</td>
<td>
<p><strong>Predict Time (s)</strong></p>
</td>
</tr>
<tr>
<td>
<p>Logistic Regression</p>
</td>
<td>
<p>.8096</p>
</td>
<td>
<p>.602</p>
</td>
<td>
<p><strong>.002</strong></p>
</td>
</tr>
<tr>
<td>
<p>KNN (with scaling)</p>
</td>
<td>
<p>.8008</p>
</td>
<td>
<p><strong>.035</strong></p>
</td>
<td>
<p>6.72</p>
</td>
</tr>
<tr>
<td>
<p>Decision Tree</p>
</td>
<td>
<p><strong>.8203</strong></p>
</td>
<td>
<p>.158</p>
</td>
<td>
<p><strong>.002</strong></p>
</td>
</tr>
<tr>
<td>
<p>Random Forest</p>
</td>
<td>
<p>.8196</p>
</td>
<td>
<p>1.107</p>
</td>
<td>
<p>.044</p>
</td>
</tr>
</tbody>
</table>
<p>The decision tree comes in first for accuracy and tied for first for predict time with logistic regression, while KNN with scaling takes the trophy for being the fastest to fit to our data. Overall, the decision tree appears to be the best model to move forward with, as it came in first for, arguably, our two most important metrics:</p>
<ul>
<li>We definitely want the best accuracy to ensure that out of sample predictions are accurate</li>
<li>Having a prediction time is useful considering that the models are being utilized for real-time production usage</li>
</ul>
<div class="packt_tip">The approach we are taking is one that selects a model before selecting any features. It is not required to work in this fashion, but we find that it generally saves the most time when working under pressure of time. For your purposes, we recommend that you experiment with many models concurrently and don't limit yourself to a single model.</div>
<p>Knowing that we will be using the decision tree for the remainder of this chapter, we know two more things:</p>
<ul>
<li>The new baseline accuracy to beat is .8203, the accuracy the tree obtained when fitting to the entire dataset</li>
<li>We no longer have to use our <kbd>StandardScaler</kbd>, as decision trees are unaffected by it when it comes to model performance</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The types of feature selection</h1>
                </header>
            
            <article>
                
<p>Recall that our goal with feature selection is to improve our machine learning capabilities by increasing predictive power and reducing the time cost. To do this, we introduce two broad categories of feature selection: statistical-based and model-based. Statistical-based feature selection will rely heavily on statistical tests that are separate from our machine learning models in order to select features during the training phase of our pipeline. Model-based selection relies on a preprocessing step that involves training a secondary machine learning model and using that model's predictive power to select features.</p>
<p>Both of these types of feature selection attempt to reduce the size of our data by subsetting from our original features only the best ones with the highest predictive power. We may intelligently choose which feature selection method might work best for us, but in reality, a very valid way of working in this domain is to work through examples of each method and measure the performance of the resulting pipeline.</p>
<p>To begin, let's take a look at the subclass of feature selection modules that are reliant on statistical tests to select viable features from a dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Statistical-based feature selection</h1>
                </header>
            
            <article>
                
<p>Statistics provides us with relatively quick and easy methods of interpreting both quantitative and qualitative data. We have used some statistical measures in previous chapters to obtain new knowledge and perspective around our data, specifically in that we recognized mean and standard deviation as metrics that enabled us to calculate z-scores and scale our data. In this chapter, we will rely on two new concepts to help us with our feature selection:</p>
<ul>
<li>Pearson correlations</li>
<li>hypothesis testing</li>
</ul>
<p>Both of these methods are known as <strong>univariate</strong> methods of feature selection, meaning that they are quick and handy when the problem is to select out <em>single </em>features at a time in order to create a better dataset for our machine learning pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Pearson correlation to select features</h1>
                </header>
            
            <article>
                
<p>We have actually looked at correlations in this book already, but not in the context of feature selection. We already know that we can invoke a correlation calculation in pandas by calling the following method:</p>
<pre>credit_card_default.corr()</pre>
<p>The output of the preceding code produces is the following:</p>
<p><img src="assets/8f4e5d5e-793b-4831-b227-1119091011c5.png"/></p>
<p>As a continuation of the preceding table we have:</p>
<p><img src="assets/6d3f21ce-2e87-4bb3-8121-2689f6fa4807.png"/></p>
<p><span>The Pearson correlation coefficient (which is the default for pandas) measures the <em>linear</em> relationship between columns. The value of the coefficient varies between -1 and +1, where 0 implies no correlation between them. Correlations closer to -1 or +1 imply an extremely strong linear relationship. </span></p>
<div class="packt_infobox"><span>It is worth noting that Pearson’s correlation generally requires that each column be normally distributed (which we are not assuming). We can also largely ignore this requirement because our dataset is large (over 500 is the threshold).</span></div>
<p>The <kbd>pandas .corr()</kbd> method calculates a Pearson correlation coefficient for every column versus every other column. This 24 column by 24 row matrix is very unruly, and in the past, we used <kbd>heatmaps</kbd> to try and make the information more digestible:</p>
<pre># using seaborn to generate heatmaps<br/>import seaborn as sns<br/>import matplotlib.style as style<br/># Use a clean stylizatino for our charts and graphs<br/>style.use('fivethirtyeight')<br/><br/>sns.heatmap(credit_card_default.corr())</pre>
<p>The <kbd>heatmap</kbd> generated will be as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="257" src="assets/895d47ee-91af-4695-8494-a20af98bf76f.png" style="color: #333333;font-size: 1em" width="363"/></div>
<p>Note that the <kbd>heatmap</kbd> function automatically chose the most correlated features to show us. That being said, we are, for the moment, concerned with the features correlations to the response variable. We will assume that the more correlated a feature is to the response, the more useful it will be. Any feature that is not as strongly correlated will not be as useful to us.</p>
<div class="packt_infobox">Correlation coefficients are also used to determine feature interactions and redundancies. A key method of reducing overfitting in machine learning is spotting and removing these redundancies. We will be tackling this problem in our model-based selection methods.</div>
<p>Let's isolate the correlations between the features and the response variable, using the following code:</p>
<pre># just correlations between every feature and the response<br/>credit_card_default.corr()['default payment next month'] <br/><br/>LIMIT_BAL                    -0.153520
SEX                          -0.039961
EDUCATION                     0.028006
MARRIAGE                     -0.024339
AGE                           0.013890
PAY_0                         0.324794
PAY_2                         0.263551
PAY_3                         0.235253
PAY_4                         0.216614
PAY_5                         0.204149
PAY_6                         0.186866
BILL_AMT1                    -0.019644
BILL_AMT2                    -0.014193
BILL_AMT3                    -0.014076
BILL_AMT4                    -0.010156
BILL_AMT5                    -0.006760
BILL_AMT6                    -0.005372
PAY_AMT1                     -0.072929
PAY_AMT2                     -0.058579
PAY_AMT3                     -0.056250
PAY_AMT4                     -0.056827
PAY_AMT5                     -0.055124
PAY_AMT6                     -0.053183
default payment next month    1.000000</pre>
<p>We can ignore the final row, as is it is the response variable correlated perfectly to itself. We are looking for features that have correlation coefficient values close to -1 or +1. These are the features that we might assume are going to be useful. Let's use pandas filtering to isolate features that have at least .2 correlation (positive or negative).</p>
<p>Let's do this by first defining a pandas <em>mask</em>, which will act as our filter, using the following code:</p>
<pre># filter only correlations stronger than .2 in either direction (positive or negative)<br/><br/>credit_card_default.corr()['default payment next month'].abs() &gt; .2<br/><br/>LIMIT_BAL                     False
SEX                           False
EDUCATION                     False
MARRIAGE                      False
AGE                           False
PAY_0                          True
PAY_2                          True
PAY_3                          True
PAY_4                          True
PAY_5                          True
PAY_6                         False
BILL_AMT1                     False
BILL_AMT2                     False
BILL_AMT3                     False
BILL_AMT4                     False
BILL_AMT5                     False
BILL_AMT6                     False
PAY_AMT1                      False
PAY_AMT2                      False
PAY_AMT3                      False
PAY_AMT4                      False
PAY_AMT5                      False
PAY_AMT6                      False
default payment next month     True</pre>
<p>Every <kbd>False</kbd> in the preceding pandas Series represents a feature that has a correlation value between -.2 and .2 inclusive, while <kbd>True</kbd> values correspond to features with preceding correlation values .2 or less than -0.2. Let's plug this mask into our pandas filtering, using the following code:</p>
<pre># store the features<br/>highly_correlated_features = credit_card_default.columns[credit_card_default.corr()['default payment next month'].abs() &gt; .2]<br/><br/>highly_correlated_features<br/><br/>Index([u'PAY_0', u'PAY_2', u'PAY_3', u'PAY_4', u'PAY_5',
       u'default payment next month'],
      dtype='object')</pre>
<p>The variable <kbd>highly_correlated_features</kbd> is supposed to hold the features of the dataframe that are highly correlated to the response; however, we do have to get rid of the name of the response column, as including that in our machine learning pipeline would be cheating:</p>
<div class="cell code_cell rendered selected">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_text output_result">
<pre># drop the response variable<br/>highly_correlated_features = highly_correlated_features.drop('default payment next month')<br/><br/>highly_correlated_features<br/><br/>Index([u'PAY_0', u'PAY_2', u'PAY_3', u'PAY_4', u'PAY_5'], dtype='object')</pre></div>
</div>
</div>
</div>
</div>
<div class="cell code_cell unselected rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">So, now we have five features from our original dataset that are meant to be predictive of the response variable, so let's try it out with the help of the following code:</p>
</div>
</div>
</div>
<div>
<pre># only include the five highly correlated features<br/>X_subsetted = X[highly_correlated_features]<br/><br/>get_best_model_and_accuracy(d_tree, tree_params, X_subsetted, y) <br/><br/># barely worse, but about 20x faster to fit the model<br/>Best Accuracy: 0.819666666667 <br/>Best Parameters: {'max_depth': 3} <br/>Average Time to Fit (s): 0.01 <br/>Average Time to Score (s): 0.002</pre></div>
</div>
<p>Our accuracy is definitely worse than the accuracy to beat, .8203, but also note that the fitting time saw about a 20-fold increase. Our model is able to learn almost as well as with the entire dataset with only five features. Moreover, it is able to learn as much in a much shorter timeframe.</p>
<p>Let's bring back our scikit-learn pipelines and include our correlation choosing methodology as a part of our preprocessing phase. To do this, we will have to create a custom transformer that invokes the logic we just went through, as a pipeline-ready class.</p>
<p>We will call our class the <kbd>CustomCorrelationChooser</kbd> and it will have to implement both a fit and a transform logic, which are:</p>
<ul>
<li>The fit logic will select columns from the features matrix that are higher than a specified threshold</li>
<li>The transform logic will subset any future datasets to only include those columns that were deemed important</li>
</ul>
<pre>from sklearn.base import TransformerMixin, BaseEstimator<br/><br/>class CustomCorrelationChooser(TransformerMixin, BaseEstimator):<br/>    def __init__(self, response, cols_to_keep=[], threshold=None):<br/>        # store the response series<br/>        self.response = response<br/>        # store the threshold that we wish to keep<br/>        self.threshold = threshold<br/>        # initialize a variable that will eventually<br/>        # hold the names of the features that we wish to keep<br/>        self.cols_to_keep = cols_to_keep<br/>        <br/>    def transform(self, X):<br/>        # the transform method simply selects the appropiate<br/>        # columns from the original dataset<br/>        return X[self.cols_to_keep]<br/>        <br/>    def fit(self, X, *_):<br/>        # create a new dataframe that holds both features and response<br/>        df = pd.concat([X, self.response], axis=1)<br/>        # store names of columns that meet correlation threshold<br/>        self.cols_to_keep = df.columns[df.corr()[df.columns[-1]].abs() &gt; self.threshold]<br/>        # only keep columns in X, for example, will remove response variable<br/>        self.cols_to_keep = [c for c in self.cols_to_keep if c in X.columns]<br/>        return self</pre>
<p>Let's take our new correlation feature selector for a spin, with the help of the following code:</p>
<pre># instantiate our new feature selector<br/>ccc = CustomCorrelationChooser(threshold=.2, response=y)<br/>ccc.fit(X)<br/><br/>ccc.cols_to_keep<br/><br/>['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5']</pre>
<p>Our class has selected the same five columns as we found earlier. Let's test out the transform functionality by calling it on our <kbd>X</kbd> matrix, using the following code:</p>
<pre>ccc.transform(X).head()</pre>
<p>The preceding code produces the following table as the output:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>PAY_0</strong></p>
</td>
<td>
<p><strong>PAY_2</strong></p>
</td>
<td>
<p><strong>PAY_3</strong></p>
</td>
<td>
<p><strong>PAY_4</strong></p>
</td>
<td>
<p><strong>PAY_5</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>-1</p>
</td>
<td>
<p>-1</p>
</td>
<td>
<p>-2</p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>-1</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>-1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>-1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
</tbody>
</table>
<div class="cell code_cell rendered selected">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_html rendered_html output_result"/>
</div>
</div>
</div>
</div>
<div class="cell code_cell unselected rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">We see that the <kbd>transform</kbd> method has eliminated the other columns and kept only the features that met our <kbd>.2</kbd> correlation threshold. Now, let's put it all together in our pipeline, with the help of the following code:</p>
<pre># instantiate our feature selector with the response variable set<br/>ccc = CustomCorrelationChooser(response=y)<br/><br/># make our new pipeline, including the selector<br/>ccc_pipe = Pipeline([('correlation_select', ccc), <br/> ('classifier', d_tree)])<br/><br/># make a copy of the decisino tree pipeline parameters<br/>ccc_pipe_params = deepcopy(tree_pipe_params)<br/><br/># update that dictionary with feature selector specific parameters<br/>ccc_pipe_params.update({<br/> 'correlation_select__threshold':[0, .1, .2, .3]})<br/><br/>print ccc_pipe_params  #<span>{'correlation_select__threshold': [0, 0.1, 0.2, 0.3], 'classifier__max_depth': [None, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]}</span><br/><br/># better than original (by a little, and a bit faster on <br/># average overall<br/>get_best_model_and_accuracy(ccc_pipe, ccc_pipe_params, X, y) <br/><br/>Best Accuracy: <strong>0.8206</strong><br/>Best Parameters: {'correlation_select__threshold': 0.1, 'classifier__max_depth': 5}<br/>Average Time to Fit (s): 0.105<br/>Average Time to Score (s): 0.003</pre></div>
</div>
</div>
</div>
<p>Wow! Our first attempt at feature selection and we have already beaten our goal (albeit by a little bit). Our pipeline is showing us that if we threshold at <kbd>0.1</kbd>, we have eliminated noise enough to improve accuracy and also cut down on the fitting time (from .158 seconds without the selector). Let's take a look at which columns our selector decided to keep:</p>
<pre># check the threshold of .1<br/>ccc = CustomCorrelationChooser(threshold=0.1, response=y)<br/>ccc.fit(X)<br/><br/># check which columns were kept<br/>ccc.cols_to_keep<br/>['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']</pre>
<div class="cell code_cell unselected rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">It appears that our selector has decided to keep the five columns that we found, as well as two more, the <kbd>LIMIT_BAL</kbd> and the <kbd>PAY_6</kbd> columns. Great! This is the beauty of automated pipeline gridsearching in scikit-learn. It allows our models to do what they do best and intuit things that we could not have on our own.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature selection using hypothesis testing</h1>
                </header>
            
            <article>
                
<p>Hypothesis testing is a methodology in statistics that allows for a bit more complex statistical testing for individual features. Feature selection via hypothesis testing will attempt to select only the best features from a dataset, just as we were doing with our custom correlation chooser, but these tests rely more on formalized statistical methods and are interpreted through what are known as <strong>p-values</strong>.</p>
<div class="page">
<div class="layoutArea">
<div class="column CDPAlignLeft CDPAlign">
<p><span>A</span> hypothesis test<strong> </strong><span>is a statistical test that is used to figure out whether we can apply a certain condition for an entire population, given a data sample. The result of a hypothesis test tells us whether we should</span> <span>believe the hypothesis or reject it for an alternative one. </span><span>Based on sample data from a population, a hypothesis test determines whether or not to</span> <span>reject</span> <span>the null hypothesis. We usually use a</span><span> </span><strong>p-value</strong><strong> </strong><span>(a non-negative decimal with an upper bound of 1, which is based on our</span> <span>significance level) to make this conclusion.</span></p>
<p>In the case of feature selection, the hypothesis we wish to test is along the lines of: <em>True or False: This feature has no relevance to the response variable. </em>We want to test this hypothesis for every feature and decide whether the features hold some significance in the prediction of the response. In a way, this is how we dealt with the correlation logic. We basically said that, if a column's correlation with the response is too weak, then we say that the hypothesis that the feature has no relevance is true. If the correlation coefficient was strong enough, then we can reject the hypothesis that the feature has no relevance in favor of an alternative hypothesis, that the feature does have some relevance.</p>
<p>To begin to use this for our data, we will have to bring in two new modules: <kbd>SelectKBest</kbd> and <kbd>f_classif</kbd>, using the following code:</p>
</div>
</div>
<pre># SelectKBest selects features according to the k highest scores of a given scoring function<br/>from sklearn.feature_selection import SelectKBest<br/><br/># This models a statistical test known as ANOVA<br/>from sklearn.feature_selection import f_classif<br/><br/># f_classif allows for negative values, not all do<br/># chi2 is a very common classification criteria but only allows for positive values<br/># regression has its own statistical tests</pre>
<p class="mce-root"><kbd>SelectKBest</kbd> is basically just a wrapper that keeps a set amount of features that are the highest ranked according to some criterion. In this case, we will use the p-values of completed hypothesis testings as a ranking.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interpreting the p-value</h1>
                </header>
            
            <article>
                
<p>The p-values are a decimals between 0 and 1 that represent the probability that the data given to us occurred by chance under the hypothesis test. Simply put, the lower the p-value, the better the chance that we can reject the null hypothesis. For our purposes, the smaller the p-value, the better the chances that the feature has some relevance to our response variable and we should keep it.</p>
<div class="packt_tip">For a more in-depth handling of statistical testing, check out<em> Principles of Data Science</em>, <a href="https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science" target="_blank">https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science</a>, by Packt Publishing.</div>
<p>The big take away from this is that the <kbd>f_classif</kbd> function will perform an ANOVA test (a type of hypothesis test) on each feature on its own (hence the name univariate testing) and assign that feature a p-value. The <kbd>SelectKBest</kbd> will rank the features by that p-value (the lower the better) and keep only the best k (a human input) features. Let's try this out in Python.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ranking the p-value</h1>
                </header>
            
            <article>
                
<p>Let's begin by instantiating a <kbd>SelectKBest</kbd> module. We will manually enter a <kbd>k</kbd> value, <kbd>5</kbd>, meaning we wish to keep only the five best features according to the resulting p-values:</p>
<pre># keep only the best five features according to p-values of ANOVA test<br/>k_best = SelectKBest(f_classif, k=5)</pre>
<p>We can then fit and transform our <kbd>X</kbd> matrix to select the features we want, as we did before with our custom selector:</p>
<pre># matrix after selecting the top 5 features<br/>k_best.fit_transform(X, y)<br/><br/># 30,000 rows x 5 columns<br/>array([[ 2,  2, -1, -1, -2],
       [-1,  2,  0,  0,  0],
       [ 0,  0,  0,  0,  0],
       ..., 
       [ 4,  3,  2, -1,  0],
       [ 1, -1,  0,  0,  0],
       [ 0,  0,  0,  0,  0]])</pre>
<p>If we want to inspect the <kbd>p-values</kbd> directly and see which columns were chosen, we can dive deeper into the select <kbd>k_best</kbd> variables:</p>
<pre class="mce-root"># get the p values of columns<br/>k_best.pvalues_<br/><br/># make a dataframe of features and p-values<br/># sort that dataframe by p-value<br/>p_values = pd.DataFrame({'column': X.columns, 'p_value': k_best.pvalues_}).sort_values('p_value')<br/><br/># show the top 5 features<br/>p_values.head()</pre>
<p>The preceding code produces the following table as the output:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>column</strong></p>
</td>
<td>
<p><strong>p_value</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>5</strong></p>
</td>
<td>
<p><span>PAY_0</span></p>
</td>
<td>
<p><span>0.000000e+00</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>6</strong></p>
</td>
<td>
<p><span>PAY_2</span></p>
</td>
<td>
<p><span>0.000000e+00</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>7</strong></p>
</td>
<td>
<p><span>PAY_3</span></p>
</td>
<td>
<p><span>0.000000e+00</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>8</strong></p>
</td>
<td>
<p><span>PAY_4</span></p>
</td>
<td>
<p><span>1.899297e-315</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>9</strong></p>
</td>
<td>
<p><span>PAY_5</span></p>
</td>
<td>
<p><span>1.126608e-279</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We can see that, once again, our selector is choosing the <kbd>PAY_X</kbd> columns as the most important. If we take a look at our <kbd>p-value</kbd> column, we will notice that our values are extremely small and close to zero. A common threshold for p-values is <kbd>0.05</kbd>, meaning that anything less than 0.05 may be considered significant, and these columns are extremely significant according to our tests. We can also directly see which columns meet a threshold of 0.05 using the pandas filtering methodology:</p>
<pre># features with a low p value<br/>p_values[p_values['p_value'] &lt; .05]</pre>
<p>The preceding code produces the following table as the output:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>column</strong></p>
</td>
<td>
<p><strong>p_value</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>5</strong></p>
</td>
<td>
<p><span>PAY_0</span></p>
</td>
<td>
<p><span>0.000000e+00</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>6</strong></p>
</td>
<td>
<p><span>PAY_2</span></p>
</td>
<td>
<p><span>0.000000e+00</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>7</strong></p>
</td>
<td>
<p><span>PAY_3</span></p>
</td>
<td>
<p><span>0.000000e+00</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>8</strong></p>
</td>
<td>
<p><span>PAY_4</span></p>
</td>
<td>
<p><span>1.899297e-315</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>9</strong></p>
</td>
<td>
<p><span>PAY_5</span></p>
</td>
<td>
<p><span>1.126608e-279</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>10</strong></p>
</td>
<td>
<p><span>PAY_6</span></p>
</td>
<td>
<p><span>7.296740e-234</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p><span>LIMIT_BAL</span></p>
</td>
<td>
<p><span>1.302244e-157</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>17</strong></p>
</td>
<td>
<p><span>PAY_AMT1</span></p>
</td>
<td>
<p><span>1.146488e-36</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>18</strong></p>
</td>
<td>
<p><span>PAY_AMT2</span></p>
</td>
<td>
<p><span>3.166657e-24</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>20</strong></p>
</td>
<td>
<p><span>PAY_AMT4</span></p>
</td>
<td>
<p><span>6.830942e-23</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>19</strong></p>
</td>
<td>
<p><span>PAY_AMT3</span></p>
</td>
<td>
<p><span>1.841770e-22</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>21</strong></p>
</td>
<td>
<p><span>PAY_AMT5</span></p>
</td>
<td>
<p><span>1.241345e-21</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>22</strong></p>
</td>
<td>
<p><span>PAY_AMT6</span></p>
</td>
<td>
<p><span>3.033589e-20</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p><span>SEX</span></p>
</td>
<td>
<p><span>4.395249e-12</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p><span>EDUCATION</span></p>
</td>
<td>
<p><span>1.225038e-06</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p><span>MARRIAGE</span></p>
</td>
<td>
<p><span>2.485364e-05</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>11</strong></p>
</td>
<td>
<p><span>BILL_AMT1</span></p>
</td>
<td>
<p><span>6.673295e-04</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>12</strong></p>
</td>
<td>
<p><span>BILL_AMT2</span></p>
</td>
<td>
<p><span>1.395736e-02</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>13</strong></p>
</td>
<td>
<p><span>BILL_AMT3</span></p>
</td>
<td>
<p><span>1.476998e-02</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p><span>AGE</span></p>
</td>
<td>
<p><span>1.613685e-02</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The majority of the columns have a low <kbd>p-value</kbd>, but not all. Let's see the columns with a higher <kbd>p_value</kbd>, using the following code:</p>
<pre># features with a high p value<br/>p_values[p_values['p_value'] &gt;= .05]</pre>
<p>The preceding code produces the following table as the output:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>column</strong></p>
</td>
<td>
<p><strong>p_value</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>14</strong></p>
</td>
<td>
<p><span>BILL_AMT4</span></p>
</td>
<td>
<p><span>0.078556</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>15</strong></p>
</td>
<td>
<p><span>BILL_AMT5</span></p>
</td>
<td>
<p><span>0.241634</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>16</strong></p>
</td>
<td>
<p><span>BILL_AMT6</span></p>
</td>
<td>
<p><span>0.352123</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>These three columns have quite a high <kbd>p-value</kbd>. Let's use our <kbd>SelectKBest</kbd> in a pipeline to see if we can grid search our way into a better machine learning pipeline, using the following code:</p>
<pre>k_best = SelectKBest(f_classif)<br/><br/># Make a new pipeline with SelectKBest<br/>select_k_pipe = Pipeline([('k_best', k_best), <br/> ('classifier', d_tree)])<br/><br/>select_k_best_pipe_params = deepcopy(tree_pipe_params)<br/># the 'all' literally does nothing to subset<br/>select_k_best_pipe_params.update({'k_best__k':range(1,23) + ['all']})<br/><br/>print select_k_best_pipe_params # {'k_best__k': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 'all'], 'classifier__max_depth': [None, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]}<br/><br/># comparable to our results with correlationchooser<br/>get_best_model_and_accuracy(select_k_pipe, select_k_best_pipe_params, X, y)<br/><br/><br/>Best Accuracy: 0.8206
Best Parameters: {'k_best__k': 7, 'classifier__max_depth': 5}
Average Time to Fit (s): 0.102
Average Time to Score (s): 0.002</pre>
<p>It seems that our <kbd>SelectKBest</kbd> module is getting about the same accuracy as our custom transformer, but it's getting there a bit quicker! Let's see which columns our tests are selecting for us, with the help of the following code:</p>
<pre>k_best = SelectKBest(f_classif, k=7)<br/><br/># lowest 7 p values match what our custom correlationchooser chose before<br/># ['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']<br/><br/>p_values.head(7)</pre>
<p>The preceding code produces the following table as the output:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>column</strong></p>
</td>
<td>
<p><strong>p_value</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>5</strong></p>
</td>
<td>
<p><span>PAY_0</span></p>
</td>
<td>
<p><span>0.000000e+00</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>6</strong></p>
</td>
<td>
<p><span>PAY_0</span></p>
</td>
<td>
<p><span>0.000000e+00</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>7</strong></p>
</td>
<td>
<p><span>PAY_0</span></p>
</td>
<td>
<p><span>0.000000e+00</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>8</strong></p>
</td>
<td>
<p><span>PAY_0</span></p>
</td>
<td>
<p><span>1.899297e-315</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>9</strong></p>
</td>
<td>
<p><span>PAY_0</span></p>
</td>
<td>
<p><span>1.126608e-279</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>10</strong></p>
</td>
<td>
<p><span>PAY_0</span></p>
</td>
<td>
<p><span>7.296740e-234</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p><span>LIMIT_BAL</span></p>
</td>
<td>
<p><span>1.302244e-157</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>They appear to be the same columns that were chosen by our other statistical method. It's possible that our statistical method is limited to continually picking these seven columns for us.</p>
<div class="packt_infobox">There are other tests available besides ANOVA, such as Chi<sup>2</sup> and others, for regression tasks. They are all included in scikit-learn's documentation. <span>For more info on feature selection through univariate testing, check out the scikit-learn documentation here: </span><br/>
<a href="http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection" target="_blank">http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection</a></div>
<p>Before we move on to model-based feature selection, it's helpful to do a quick sanity check to ensure that we are on the right track. So far, we have seen two statistical methods for feature selection that gave us the same seven columns for optimal accuracy. But what if we were to take every column <strong>except</strong> those seven? We should expect a much lower accuracy and worse pipeline overall, right? Let's make sure. The following code helps us to implement sanity checks:</p>
<pre># sanity check<br/># If we only the worst columns<br/>the_worst_of_X = X[X.columns.drop(['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'])]<br/><br/># goes to show, that selecting the wrong features will <br/># hurt us in predictive performance<br/>get_best_model_and_accuracy(d_tree, tree_params, the_worst_of_X, y)<br/><br/>Best Accuracy: 0.783966666667
Best Parameters: {'max_depth': 5}
Average Time to Fit (s): 0.21
Average Time to Score (s): 0.002</pre>
<p>OK, so by selecting the columns except those seven, we see not only worse accuracy (almost as bad as the null accuracy), but also slower fitting times on average. With this, I believe we may move on to our next subset of feature selection techniques, the model-based methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-based feature selection</h1>
                </header>
            
            <article>
                
<p>Our last section dealt with using statistical methods and testing in order to select features from the original dataset to improve our machine learning pipeline, both in predictive performance, as well as in time-complexity. In doing so, we were able to see first-hand the effects of using feature selection.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief refresher on natural language processing</h1>
                </header>
            
            <article>
                
<p>If talking about feature selection has sounded familiar from the very beginning of this chapter, almost as if we were doing it even before we began with correlation coefficients and statistical testing, well, you aren't wrong. In <a href="430d621e-7ce6-48c0-9990-869e82a0d0c6.xhtml" target="_blank">Chapter 4</a>, <em>Feature Construction</em> when dealing with feature construction, we introduced the concept of the <kbd>CountVectorizer</kbd>, a module in scikit-learn designed to construct features from text columns and use them in machine learning pipelines.</p>
<p>The <kbd>CountVectorizer</kbd> had many parameters that we could alter in search of the best pipeline. Specifically, there were a few built-in feature selection parameters:</p>
<ul>
<li><kbd>max_features</kbd>: This integer set a hard limit of the maximum number of features that the featurizer could remember. The features that were remembered were decided based on a ranking system where the rank of a token was the count of the token in the corpus. </li>
<li><kbd>min_df</kbd>: This float limited the number of features by imposing a rule stating that a token may only appear in the dataset if it appeared in the corpus as a rate strictly greater than the value for <kbd>min_df</kbd>.</li>
<li><kbd>max_df</kbd>: Similar to <kbd>min_df</kbd>, this float limits the number of features by only allowing tokens that appear in the corpus at a rate strictly lower than the value set for <kbd>max_df</kbd>.</li>
<li><kbd>stop_words</kbd>: Limits the type of tokens allowed by matching them against a static list of tokens. If a token is found that exists in the <kbd>stop_words</kbd> set, that word, no matter if it occurs at the right amount to be allowed by <kbd>min_df</kbd> and <kbd>max_df</kbd>, is ignored.</li>
</ul>
<p>In the previous chapter, we briefly introduced a dataset aimed at predicting the sentiment of a tweet based purely on the words in that tweet. Let's take some time to refresh our memories on how to use these parameters. Let's begin by bringing in our <kbd>tweet</kbd> dataset, with the help of the following code:</p>
<pre># bring in the tweet dataset<br/>tweets = pd.read_csv('../data/twitter_sentiment.csv', <br/> encoding='latin1')</pre>
<p>To refresh our memory, let's look at the first five <kbd>tweets</kbd>, using the following code:</p>
<pre>tweets.head()</pre>
<p>The preceding code produces the following table as the output:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>ItemID</strong></p>
</td>
<td>
<p><strong>Sentiment</strong></p>
</td>
<td>
<p><strong>SentimentText</strong></p>
</td>
</tr>
<tr>
<td>
<p><span>0</span></p>
</td>
<td>
<p><span>1</span></p>
</td>
<td>
<p><span>0</span></p>
</td>
<td>
<p><span>is so sad for my APL frie...</span></p>
</td>
</tr>
<tr>
<td>
<p><span>1</span></p>
</td>
<td>
<p><span>2</span></p>
</td>
<td>
<p><span>0</span></p>
</td>
<td>
<p><span>I missed the New Moon trail...</span></p>
</td>
</tr>
<tr>
<td>
<p><span>2</span></p>
</td>
<td>
<p><span>3</span></p>
</td>
<td>
<p><span>1</span></p>
</td>
<td>
<p><span>omg its already 7:30 :O</span></p>
</td>
</tr>
<tr>
<td>
<p><span>3</span></p>
</td>
<td>
<p><span>4</span></p>
</td>
<td>
<p><span>0</span></p>
</td>
<td>
<p><span>.. Omgaga. Im sooo im gunna CRy. I'...</span></p>
</td>
</tr>
<tr>
<td>
<p><span>4</span></p>
</td>
<td>
<p><span>5</span></p>
</td>
<td>
<p><span>0</span></p>
</td>
<td>
<p><span>i think mi bf is cheating on me!!! ...</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Let's create a feature and a response variable. Recall that, because we are working with text, our feature variable will simply be the text column and not a two-dimensional matrix like it usually is:</p>
<pre>tweets_X, tweets_y = tweets['SentimentText'], tweets['Sentiment']</pre>
<p>Let's set up a pipeline and evaluate it using the same function that we've been using in this chapter, with the help of the following code:</p>
<pre>from sklearn.feature_extraction.text import CountVectorizer<br/># import a naive bayes to help predict and fit a bit faster <br/>from sklearn.naive_bayes import MultinomialNB<br/><br/>featurizer = CountVectorizer()<br/><br/>text_pipe = Pipeline([('featurizer', featurizer), <br/>                 ('classify', MultinomialNB())])<br/><br/>text_pipe_params = {'featurizer__ngram_range':[(1, 2)], <br/>               'featurizer__max_features': [5000, 10000],<br/>               'featurizer__min_df': [0., .1, .2, .3], <br/>               'featurizer__max_df': [.7, .8, .9, 1.]}<br/><br/><br/>get_best_model_and_accuracy(text_pipe, text_pipe_params, <br/>                            tweets_X, tweets_y)<br/><br/>Best Accuracy: 0.755753132845
Best Parameters: {'featurizer__min_df': 0.0, 'featurizer__ngram_range': (1, 2), 'featurizer__max_df': 0.7, 'featurizer__max_features': 10000}
Average Time to Fit (s): 5.808
Average Time to Score (s): 0.957</pre>
<p><span>A decent score (recalling that the null accuracy was .564), but we were able to beat this in the last chapter by using a <kbd>FeatureUnion</kbd> module to combine features from <kbd>TfidfVectorizer</kbd> and <kbd>CountVectorizer</kbd>.</span></p>
<p><span>To try out the techniques we've seen in this chapter, let's go ahead and apply a <kbd>SelectKBest</kbd> in a pipeline with a <kbd>CountVectorizer</kbd>. Let's see if we can rely not on the built-in <kbd>CountVectorizer</kbd> feature selection parameters, but instead, on statistical testing:</span></p>
<pre># Let's try a more basic pipeline, but one that relies on SelectKBest as well<br/>featurizer = CountVectorizer(ngram_range=(1, 2))<br/><br/>select_k_text_pipe = Pipeline([('featurizer', featurizer), <br/>                      ('select_k', SelectKBest()),<br/>                      ('classify', MultinomialNB())])<br/><br/>select_k_text_pipe_params = {'select_k__k': [1000, 5000]}<br/><br/>get_best_model_and_accuracy(select_k_text_pipe, <br/>                            select_k_text_pipe_params, <br/>                            tweets_X, tweets_y)<br/><br/>Best Accuracy: 0.755703127344
Best Parameters: {'select_k__k': 10000}
Average Time to Fit (s): 6.927
Average Time to Score (s): 1.448</pre>
<p class="mce-root"> </p>
<p class="mce-root">It seems that <kbd>SelectKBest</kbd> didn't do as well for text tokens, and without <kbd>FeatureUnion</kbd>, we were unable to compete with the previous chapter's accuracy scores. Either way, for both pipelines, it is worth noting that the time it takes to both fit and predict are extremely poor. This is because statistical univariate methods are not optimal for a very large number of features, such as the features obtained from text vectorization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using machine learning to select features</h1>
                </header>
            
            <article>
                
<p>Using <kbd>CountVectorizer</kbd> built-in feature selection tools is great when you are dealing with text; however, we are usually dealing with data already built into a row/column structure. We've seen the power of using purely statistical methodology for feature selection, and now let's see how we can invoke the awesome power of machine learning to, hopefully, do even more. The two main machine learning models that we will use in this section for the purposes of feature selection are tree-based models and linear models. They both have a notion of feature ranking that are useful when subsetting feature sets.</p>
<p>Before we go further, we believe it is worth mentioning again that these methods, while different in their methodology of selection, are attempting to find the optimal subset of features to improve our machine learning pipelines. The first method we will dive into will involve the internal importance metrics that algorithms such as decision trees and random forest models generate whilst fitting to training data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tree-based model feature selection metrics</h1>
                </header>
            
            <article>
                
<p>When fitting decision trees, the tree starts at the root node and greedily chooses the optimal split at every junction that optimizes a certain metric of <strong>node purity</strong>. By default, scikit-learn optimizes for the <strong>gini</strong> metric at every step. While each split is created, the model keeps track of how much each split helps the overall optimization goal. In doing so, tree-based models that choose splits based on such metrics have a notion of<strong> feature importance</strong>.</p>
<p>To illustrate this further, let's go ahead and fit a decision tree to our data and output the feature importance' with the help of the following code:</p>
<pre># create a brand new decision tree classifier<br/>tree = DecisionTreeClassifier()<br/><br/>tree.fit(X, y)</pre>
<p>Once our tree has fit to the data, we can call on the <kbd>feature_importances_ attribute</kbd> to capture the importance of the feature, relative to the fitting of the tree:</p>
<pre># note that we have some other features in play besides what our last two selectors decided for us<br/><br/>importances = pd.DataFrame({'importance': tree.feature_importances_, 'feature':X.columns}).sort_values('importance', ascending=False)<br/><br/>importances.head()</pre>
<p>The preceding code produces the following table as the output:</p>
<table>
<tbody>
<tr>
<td/>
<td><strong>feature</strong></td>
<td><strong>importance</strong></td>
</tr>
<tr>
<td><span>5</span></td>
<td><span>PAY_0</span></td>
<td><span>0.161829</span></td>
</tr>
<tr>
<td><span>4</span></td>
<td><span>AGE</span></td>
<td><span>0.074121</span></td>
</tr>
<tr>
<td><span>11</span></td>
<td><span>BILL_AMT1</span></td>
<td><span>0.064363</span></td>
</tr>
<tr>
<td><span>0</span></td>
<td><span>LIMIT_BAL</span></td>
<td><span>0.058788</span></td>
</tr>
<tr>
<td><span>19</span></td>
<td><span>PAY_AMT3</span></td>
<td><span>0.054911</span></td>
</tr>
</tbody>
</table>
<p> </p>
<p>What this table is telling us is that the most important feature while fitting was the column <kbd>PAY_0</kbd>, which matches up to what our statistical models were telling us earlier in this chapter. What is more notable are the second, third, and fifth most important features, as they didn't really show up before using our statistical tests. This is a good indicator that this method of feature selection might yield some new results for us.</p>
<p>Recall that, earlier, we relied on a built in scikit-learn wrapper called SelectKBest to capture the top <em>k</em> features based on a ranking function like ANOVA p-values. We will introduce another similar style of wrapper called <kbd>SelectFromModel</kbd> which, like <kbd>SelectKBest</kbd>, will capture the top k most importance features. However, it will do so by listening to a machine learning model's internal metric for feature importance rather than the p-values of a statistical test. We will use the following code to define the <kbd>SelectFromModel</kbd>:</p>
<pre># similar to SelectKBest, but not with statistical tests<br/>from sklearn.feature_selection import SelectFromModel</pre>
<p>The biggest difference in usage between <kbd>SelectFromModel</kbd> and <kbd>SelectKBest</kbd> is that <span><kbd>SelectFromModel</kbd> doesn't take in an integer k, which represents the number of features to keep, but rather <kbd>SelectFromModel</kbd> uses a threshold for selection which acts as a hard minimum of importance to be selected. In this way, the model-based selectors of this chapter are able to move away from a human-inputted number of features to keep and instead rely on relative importance to include only as many features as the pipeline needs. Let's instantiate our class as follows:</span></p>
<pre># instantiate a class that choses features based<br/># on feature importances according to the fitting phase<br/># of a separate decision tree classifier<br/>select_from_model = SelectFromModel(DecisionTreeClassifier(), <br/> threshold=.05)</pre>
<p>Let's fit this <kbd>SelectFromModel</kbd> class to our data and invoke the transform method to watch our data get subsetted, with the help of the following code:</p>
<div class="cell code_cell rendered selected">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_text output_result">
<pre>selected_X = select_from_model.fit_transform(X, y)<br/>selected_X.shape<br/><br/>(30000, 9)</pre></div>
</div>
</div>
</div>
</div>
<div class="cell code_cell rendered unselected">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">Now that we know the basic mechanics of the module, let's use it to select features for us in a pipeline. Recall that the accuracy to beat is .8206, which we got both from our correlation chooser and our ANOVA test (because they both returned the same features):</p>
</div>
<pre># to speed things up a bit in the future<br/>tree_pipe_params = {'classifier__max_depth': [1, 3, 5, 7]}<br/><br/>from sklearn.pipeline import Pipeline<br/><br/># create a SelectFromModel that is tuned by a DecisionTreeClassifier<br/>select = SelectFromModel(DecisionTreeClassifier())<br/><br/>select_from_pipe = Pipeline([('select', select),<br/>                             ('classifier', d_tree)])<br/><br/>select_from_pipe_params = deepcopy(tree_pipe_params)<br/><br/>select_from_pipe_params.update({<br/> 'select__threshold': [.01, .05, .1, .2, .25, .3, .4, .5, .6, "mean", "median", "2.*mean"],<br/> 'select__estimator__max_depth': [None, 1, 3, 5, 7]<br/> })<br/><br/>print select_from_pipe_params  # {'select__threshold': [0.01, 0.05, 0.1, 'mean', 'median', '2.*mean'], 'select__estimator__max_depth': [None, 1, 3, 5, 7], 'classifier__max_depth': [1, 3, 5, 7]}<br/><br/><br/>get_best_model_and_accuracy(select_from_pipe, <br/> select_from_pipe_params, <br/> X, y)<br/><br/># not better than original<br/>Best Accuracy: 0.820266666667<br/>Best Parameters: {'select__threshold': 0.01, 'select__estimator__max_depth': None, 'classifier__max_depth': 3} <br/>Average Time to Fit (s): 0.192 <br/>Average Time to Score (s): 0.002</pre></div>
</div>
</div>
<p>Note first that, as part of the threshold parameter, we are able to include some reserved words rather than a float that represents the minimum importance to use. For example, the threshold of <kbd>mean</kbd> only selects features with an importance that is higher than average. Similarly, a value of median as a threshold only selects features that are more important than the median value. We may also include multiples to these reserved words so that <kbd>2.*mean</kbd> will only include features that are more important than twice the mean importance value.</p>
<p>Let's take a peak as to which features our decision tree-based selector is choosing for us. We can do this by invoking a method within <kbd>SelectFromModel</kbd> called <kbd>get_support()</kbd>. It will return an array of Booleans, one for each original feature column, and tell us which of the features it decided to keep, as follows:</p>
<pre># set the optimal params to the pipeline<br/>select_from_pipe.set_params(**{'select__threshold': 0.01, <br/> 'select__estimator__max_depth': None, <br/> 'classifier__max_depth': 3})<br/><br/># fit our pipeline to our data<br/>select_from_pipe.steps[0][1].fit(X, y)<br/><br/># list the columns that the SVC selected by calling the get_support() method from SelectFromModel<br/>X.columns[select_from_pipe.steps[0][1].get_support()]<br/><br/><br/>[u'LIMIT_BAL', u'SEX', u'EDUCATION', u'MARRIAGE', u'AGE', u'PAY_0', u'PAY_2', u'PAY_3', u'PAY_6', u'BILL_AMT1', u'BILL_AMT2', u'BILL_AMT3', u'BILL_AMT4', u'BILL_AMT5', u'BILL_AMT6', u'PAY_AMT1', u'PAY_AMT2', u'PAY_AMT3', u'PAY_AMT4', u'PAY_AMT5', u'PAY_AMT6']</pre>
<p>Wow! So the tree decided to keep all but two features, and still only did just as good as the tree did without selecting anything:</p>
<div class="packt_infobox">For more information on decision trees and how they are fit using gini or entropy, look into the scikit-learn documentation or other texts that handle this topic in more depth.</div>
<p>We could continue onward by trying several other tree-based models, such as RandomForest, ExtraTreesClassifier, and others, but perhaps we may be able to do better by utilizing a model other than a tree-based model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear models and regularization</h1>
                </header>
            
            <article>
                
<p>The <kbd>SelectFromModel</kbd> selector is able to handle any machine learning model that exposes a <kbd>feature_importances_</kbd> or<strong><kbd> coef_ attribute</kbd></strong> post-fitting. Tree-based models expose the former, while linear models expose the latter. After fitting, linear models such as Linear Regression, Logistic Regression, Support Vector Machines, and others all place coefficients in front of features that represent the slope of that feature/how much it affects the response when that feature is changed. <kbd>SelectFromModel</kbd> can equate this to a feature importance and choose features based on the coefficients given to features while fitting.</p>
<p>Before we can use these models, however, we must introduce a concept called <strong>regularization</strong>, which will help us select truly only the most important features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief introduction to regularization</h1>
                </header>
            
            <article>
                
<p><span>In linear models, <strong>regularization</strong> is a method for imposing additional constraints to a learning model, where the goal is to prevent overfitting and improve the generalization of the data. This is done by adding extra terms to the <em>loss function</em> being optimized, meaning that, while fitting, regularized linear models may severely diminish, or even destroy features along the way. There are two widely used regularization methods, called L1 and L2 regularization. Both regularization techniques rely on the L-p Norm, which is defined for a vector as being:</span></p>
<p style="padding-left: 150px"><img class="fm-editor-equation" height="60" src="assets/68897f7a-1aea-478f-a42f-0f6a740ffa75.png" width="224"/></p>
<ul>
<li><strong>L1 </strong><span><strong>regularization</strong>, also known as <strong>lasso</strong> regularization, uses the L1 Norm, which, using the above formula, reduces to the sum of the absolute values of the entries of a vector to limit the coefficients in such a way that they may disappear entirely and become 0. If the coefficient of a feature drops to 0, then that feature will not have any say in the prediction of new data observations and definitely will not be chosen by a <kbd>SelectFromModel</kbd> selector.</span></li>
<li><strong>L2 regularization</strong>, also known as <strong>ridge</strong> regularization, imposes the L2 norm as a penalty (sum of the square of vector entries) so that coefficients cannot drop to 0, but they can become very, very tiny.</li>
</ul>
<div class="packt_infobox">Regularization also helps with multicollinearity, the problem of having multiple features in a dataset that are linearly related to one another. A Lasso Penalty (L1) will force coefficients of dependent features to 0, ensuring that they aren't chosen by the selector module, helping combat overfitting.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear model coefficients as another feature importance metric</h1>
                </header>
            
            <article>
                
<p>We can use L1 and L2 regularization to find optimal coefficients for our feature selection, just as we did with our tree-based models. Let's use a logistic regression model as our model-based selector and gridsearch across both the L1 and L2 norm:</p>
<pre># a new selector that uses the coefficients from a regularized logistic regression as feature importances<br/>logistic_selector = SelectFromModel(LogisticRegression())<br/><br/># make a new pipeline that uses coefficients from LogistisRegression as a feature ranker<br/>regularization_pipe = Pipeline([('select', logistic_selector), <br/> ('classifier', tree)])<br/><br/>regularization_pipe_params = deepcopy(tree_pipe_params)<br/><br/># try l1 regularization and l2 regularization<br/>regularization_pipe_params.update({<br/> 'select__threshold': [.01, .05, .1, "mean", "median", "2.*mean"],<br/> 'select__estimator__penalty': ['l1', 'l2'],<br/> })<br/><br/>print regularization_pipe_params  # {'select__threshold': [0.01, 0.05, 0.1, 'mean', 'median', '2.*mean'], 'classifier__max_depth': [1, 3, 5, 7], 'select__estimator__penalty': ['l1', 'l2']}<br/><br/><br/>get_best_model_and_accuracy(regularization_pipe, <br/> regularization_pipe_params, <br/> X, y)<br/><br/><br/># better than original, in fact the best so far, and much faster on the scoring side<br/>Best Accuracy: <strong>0.82116</strong>6666667 Best Parameters: {'select__threshold': 0.01, 'classifier__max_depth': 5, 'select__estimator__penalty': 'l1'} <br/>Average Time to Fit (s): 0.51 <br/>Average Time to Score (s): 0.001</pre>
<p>Finally! We got an accuracy better than our statistical testing selector. Let's see which features our model-based selector decided to keep by invoking the <kbd>get_support()</kbd> method of <kbd>SelectFromModel</kbd> again:</p>
<pre># set the optimal params to the pipeline<br/>regularization_pipe.set_params(**{'select__threshold': 0.01, <br/> 'classifier__max_depth': 5, <br/> 'select__estimator__penalty': 'l1'})<br/><br/># fit our pipeline to our data<br/>regularization_pipe.steps[0][1].fit(X, y)<br/><br/># list the columns that the Logisti Regression selected by calling the get_support() method from SelectFromModel<br/>X.columns[regularization_pipe.steps[0][1].get_support()]<br/><br/>[u'SEX', u'EDUCATION', u'MARRIAGE', u'PAY_0', u'PAY_2', u'PAY_3', u'PAY_4', u'PAY_5']</pre>
<p>Fascinating! Our logistic regression based selector kept most of the <kbd>PAY_X</kbd> columns but was also able to figure out that the sex, education, and marriage status of the person was going to play a hand in prediction. Let's continue our adventure by using one more model with our <kbd>SelectFromModel</kbd> selector module, a support vector machine classifier.</p>
<p>If you are unfamiliar with support vector machines, they are classification models that attempt to draw linear boundaries in space to separate binary labels. These linear boundaries are known as support vectors. For now, the most important difference between logistic regression and support vector classifiers are that SVCs are usually better equipped to optimize coefficients for maximizing accuracy for binary classification tasks, while logistic regression is better at modeling the probabilistic attributes of binary classification tasks. Let's implement a Linear SVC model from scikit-learn as we did for decision trees and logistic regression and see how it fares, using the following code:</p>
<pre># SVC is a linear model that uses linear supports to <br/># seperate classes in euclidean space<br/># This model can only work for binary classification tasks<br/>from sklearn.svm import LinearSVC<br/><br/># Using a support vector classifier to get coefficients<br/>svc_selector = SelectFromModel(LinearSVC())<br/><br/>svc_pipe = Pipeline([('select', svc_selector), <br/> ('classifier', tree)])<br/><br/>svc_pipe_params = deepcopy(tree_pipe_params)<br/><br/>svc_pipe_params.update({<br/> 'select__threshold': [.01, .05, .1, "mean", "median", "2.*mean"],<br/> 'select__estimator__penalty': ['l1', 'l2'],<br/> 'select__estimator__loss': ['squared_hinge', 'hinge'],<br/> 'select__estimator__dual': [True, False]<br/> })<br/><br/>print svc_pipe_params  # 'select__estimator__loss': ['squared_hinge', 'hinge'], 'select__threshold': [0.01, 0.05, 0.1, 'mean', 'median', '2.*mean'], 'select__estimator__penalty': ['l1', 'l2'], 'classifier__max_depth': [1, 3, 5, 7], 'select__estimator__dual': [True, False]}<br/><br/>get_best_model_and_accuracy(svc_pipe, <br/> svc_pipe_params, <br/> X, y) <br/><br/><br/># better than original, in fact the best so far, and much faster on the scoring side<br/>Best Accuracy: <strong>0.821233</strong>333333
Best Parameters: {'select__estimator__loss': 'squared_hinge', 'select__threshold': 0.01, 'select__estimator__penalty': 'l1', 'classifier__max_depth': 5, 'select__estimator__dual': False}
Average Time to Fit (s): 0.989
Average Time to Score (s): 0.001</pre>
<p>Great! The best accuracy we've gotten so far. We can see that the fitting time took a hit but if we are OK with that, couple the best accuracy so far with an outstandingly quick predicting time and we've got a great machine learning pipeline on our hands; one that leverages the power of regularization in the context of support vector classification to feed significant features into a decision tree classifier. Let's see which features our selector chose to give us our best accuracy to date:</p>
<pre># set the optimal params to the pipeline<br/>svc_pipe.set_params(**{'select__estimator__loss': 'squared_hinge', <br/> 'select__threshold': 0.01, <br/> 'select__estimator__penalty': 'l1', <br/> 'classifier__max_depth': 5, <br/> 'select__estimator__dual': False})<br/><br/># fit our pipeline to our data<br/>svc_pipe.steps[0][1].fit(X, y)<br/><br/># list the columns that the SVC selected by calling the get_support() method from SelectFromModel<br/>X.columns[svc_pipe.steps[0][1].get_support()]<br/><br/>[u'SEX', u'EDUCATION', u'MARRIAGE', u'PAY_0', u'PAY_2', u'PAY_3', u'PAY_5']</pre>
<p>The only difference between these features and what our logistic regression got was the <kbd>PAY_4</kbd> column. But we can see that even removing a single column can affect our entire pipeline's performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing the right feature selection method</h1>
                </header>
            
            <article>
                
<p>At this point, you may be feeling a bit overwhelmed with the information in this chapter. We have presented several ways of performing feature selection, some based on pure statistics and others based on the output of secondary machine learning models. It is natural to wonder how to decide which feature selection method is right for your data. In theory, if you are able to try multiple options, as we did in this chapter, that would be ideal, but we understand that it might not be feasible to do so. The following are some rules of thumbs that you can follow when you are trying to prioritize which feature selection module is more likely to offer greater results:</p>
<ul>
<li>If your features are mostly categorical, you should start by trying to implement a <kbd>SelectKBest</kbd> with a Chi<sup>2</sup> ranker or a tree-based model selector.</li>
<li>If your features are largely quantitative (like ours were), using linear models as model-based selectors and relying on correlations tends to yield greater results, as was shown in this chapter.</li>
<li>If you are solving a binary classification problem, using a Support Vector Classification model along with a <kbd>SelectFromModel</kbd> selector will probably fit nicely, as the SVC tries to find coefficients to optimize for binary classification tasks.</li>
<li>A little bit of EDA can go a long way in manual feature selection. The importance of having domain knowledge in the domain from which the data originated cannot be understated.</li>
</ul>
<p>That being said, these are meant only to be used as guidelines. As a data scientist, ultimately you decide which features you wish to keep to optimize the metric of your choosing. The methods that we provide in this text are here to help you in your discovery of the latent power of features hidden by noise and multicollinearity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned a great deal about methodologies for selecting subsets of features in order to increase the performance of our machine learning pipelines in both a predictive capacity as well in-time-complexity.</p>
<p><span>The dataset that we chose had a relatively low number of features. If selecting, however, from a very large set of features (over a hundred), then the methods in this chapter will likely start to become entirely too cumbersome. We saw that in this chapter, when attempting to optimize a <kbd>CountVectorizer</kbd> pipeline, t</span><span>he time it would take to run a univariate test on every feature is not only astronomical; we would run a greater risk of experiencing multicollinearity in our features by sheer coincidence. </span></p>
<p>In the next chapter, we will introduce purely mathematical transformations that we may apply to our data matrices in order to alleviate the trouble of working with vast quantities of features, or even a few highly uninterpretable features. We will begin to work with datasets that stray away from what we have seen before, such as image data, topic modeling data, and more.</p>


            </article>

            
        </section>
    </body></html>