<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer100">
<h1 id="_idParaDest-79"><em class="italic"><a id="_idTextAnchor078"/>Chapter 6</em>: Preparing for Model Evaluation</h1>
<p>It is a good idea to think through how you will evaluate your model’s performance before you begin to run it. A common technique is to separate data into training and testing datasets. We do this relatively early in the process to avoid what is known as data leakage; that is, conducting analyses based on data that is intended to be set aside for model evaluation. In this chapter, we will look at approaches for creating training datasets, including how to ensure that training data is representative. We will look into cross-validation strategies such as <strong class="bold">K-fold</strong>, which address some of the limitations of using static training/testing splits. We will also begin to look more closely at assessing the performance of models.</p>
<p>You might be wondering why we are discussing model evaluation before going over any algorithms in detail. This is because there is a practical consideration. We tend to use the same metrics and evaluation techniques across algorithms with similar purposes. We examine accuracy and sensitivity when evaluating classification models, and mean absolute error and R-squared when examining regression models. We do cross-validation with all supervised learning models. So, we will repeat the strategies introduced here several times in the following chapters. You may even find yourself coming back to these pages when the concepts are re-introduced later.</p>
<p>Beyond those practical considerations, our modeling work improves when we do not see data extraction, data cleaning, exploratory analysis, feature engineering and Preprocessing, model specification, and model evaluation as discrete, sequential tasks. If you have been building machine learning models for just 6 months or over 30 years, you probably appreciate that such rigid sequencing is inconsistent with our workflow as data scientists. We are always preparing for model validation, and always cleaning data. This is a good thing. We do better work when we integrate these tasks; when we continue to interrogate our data cleaning as we select features, and when we look back at bivariate correlations or scatter plots after calculating precision or root mean squared error.</p>
<p>We will also spend a fair bit of time constructing visualizations of these concepts. It is a good idea to get in the habit of looking at confusion matrices and cumulative accuracy profiles when working on classification problems, and plots of residuals when working with a continuous target. This, too, will serve us well in subsequent chapters.</p>
<p>Specifically, in this chapter, we will cover the following topics:</p>
<ul>
<li>Measuring accuracy, sensitivity, specificity, and precision for binary classification </li>
<li>Examining CAP, ROC, and precision-sensitivity curves for binary classification</li>
<li>Evaluating multiclass models</li>
<li>Evaluating regression models</li>
<li>Using K-fold cross-validation</li>
<li>Preprocessing data with pipelines</li>
</ul>
<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Technical requirements</h1>
<p>In this chapter, we will work with the <strong class="source-inline">feature_engine</strong> and <strong class="source-inline">matplotlib</strong> libraries, in addition to the scikit-learn library. You can use <strong class="source-inline">pip</strong> to install these packages. The code files for this chapter can be found in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning">https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning</a>.</p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Measuring accuracy, sensitivity, specificity, and precision for binary classification</h1>
<p>When<a id="_idIndexMarker503"/> assessing a classification model, we typically want to know how often we are right. In the case of a binary target – one where the target has two possible categorical values – we calculate <strong class="bold">accuracy</strong> as the ratio <a id="_idIndexMarker504"/>of times we predict the correct classification against the total number of observations.</p>
<p>But, depending on the classification problem, accuracy may not be the most important performance measure. Perhaps we are willing to accept more false positives for a model that can identify more true positives, even if that means lower accuracy. This might be true for a model that would predict the likelihood of having breast cancer, a security breach, or structural damage in a bridge. In these cases, we<a id="_idIndexMarker505"/> may emphasize <strong class="bold">sensitivity</strong> (the propensity to identify positive cases) over accuracy. </p>
<p>On the other hand, we may want a model that could identify negative cases with high reliability, even if that meant it did not do as good a job of identifying positives. <strong class="bold">Specificity</strong> is a<a id="_idIndexMarker506"/> measure of the percentage of all negatives identified by the model.</p>
<p><strong class="bold">Precision</strong>, the<a id="_idIndexMarker507"/> percentage of predicted positives that are actually positives, is another important measure. For some applications, it is important to limit false positives, even if we have to tolerate lower sensitivity. An apple grower, using image recognition to identify bad apples, may prefer a high-precision model to a more sensitive one, not wanting to discard apples unnecessarily.</p>
<p>This can be made clearer by looking<a id="_idIndexMarker508"/> at a confusion matrix:</p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<img alt="Figure 6.1 – Confusion matrix " height="562" src="image/B17978_06_001.jpg" width="1635"/>
</div>
</div>
<p class="figure-caption">Figure 6.1 – Confusion matrix</p>
<p>The confusion matrix helps us conceptualize accuracy, sensitivity, specificity, and precision. Accuracy<a id="_idIndexMarker509"/> is the<a id="_idIndexMarker510"/> percentage of observations for which our prediction was correct. This can be stated more precisely as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<img alt="" height="149" src="image/B17978_06_0011.jpg" width="964"/>
</div>
</div>
<p>Sensitivity <a id="_idIndexMarker511"/>is the number of times we predicted positives correctly<a id="_idIndexMarker512"/> divided by the number of positives. It might be helpful to glance again at the confusion matrix and confirm that actual positive values can either <a id="_idIndexMarker513"/>be <strong class="bold">predicted positives</strong> (<strong class="bold">TP</strong>) or <strong class="bold">predicted negatives</strong> (<strong class="bold">FN</strong>). Sensitivity <a id="_idIndexMarker514"/>is also referred to<a id="_idIndexMarker515"/> as <strong class="bold">recall</strong> or the <strong class="bold">true positive rate</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<img alt="" height="148" src="image/B17978_06_002.jpg" width="715"/>
</div>
</div>
<p>Specificity is <a id="_idIndexMarker516"/>the<a id="_idIndexMarker517"/> number of times we correctly predicted<a id="_idIndexMarker518"/> a <strong class="bold">negative value</strong> (<strong class="bold">TN</strong>) divided by the number of actual negative values (<strong class="bold">TN + FP</strong>). Specificity is also known<a id="_idIndexMarker519"/> as the <strong class="bold">true negative rate</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<img alt="" height="133" src="image/B17978_06_003.jpg" width="662"/>
</div>
</div>
<p>Precision <a id="_idIndexMarker520"/>is the <a id="_idIndexMarker521"/>number of times we correctly <a id="_idIndexMarker522"/>predicted a <strong class="bold">positive value</strong> (<strong class="bold">TP</strong>) divided by the number of positive values predicted:</p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<img alt="" height="158" src="image/B17978_06_004.jpg" width="730"/>
</div>
</div>
<p>When there is class imbalance, measures such as accuracy and sensitivity can give us very different estimates of the performance of our model. An extreme example will illustrate this. Chimpanzees sometimes <em class="italic">termite fish</em>, putting a stick in a termite mound with the hopes of catching a few termites. This is only occasionally successful. I am no primatologist, but we can perhaps model a successful fishing attempt as a function of the size of the stick used, the time of year, and the age of the chimpanzee. In our testing data, fishing attempts are only successful 2% of the time. (This data has been made up for this demonstration.)</p>
<p>Let’s also say that we build a classification model of successful termite fishing that has a sensitivity of 50%. So, if there are 100 fishing attempts in our testing data, we would predict only one of the two successful attempts correctly. There is also one false positive, where our model predicted successful fishing when the fishing failed. This gives us the following confusion matrix:</p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<img alt="Figure 6.2 – Successful termite fishing confusion matrix " height="453" src="image/B17978_06_0021.jpg" width="1586"/>
</div>
</div>
<p class="figure-caption">Figure 6.2 – Successful termite fishing confusion matrix</p>
<p>Notice that we get a very high accuracy of 98% – that is, (97+1) / 100. We get high accuracy and low sensitivity because a large percentage of the fishing attempts are negative and that is easy to predict. A model that just predicts failure always would also have an accuracy of 98%.</p>
<p>Now, let’s look <a id="_idIndexMarker523"/>at these <a id="_idIndexMarker524"/>model evaluation measures with real data. We can experiment with a <strong class="bold">k-nearest neighbors</strong> (<strong class="bold">KNN</strong>) model<a id="_idIndexMarker525"/> to predict bachelor’s degree <a id="_idIndexMarker526"/>attainment and <a id="_idIndexMarker527"/>evaluate its accuracy, sensitivity, specificity, and precision:</p>
<ol>
<li>We will start by loading libraries for encoding and standardizing data, and for creating training and testing DataFrames. We will also load scikit-learn’s KNN classifier <a id="_idIndexMarker528"/>and the <strong class="source-inline">metrics</strong> library:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.neighbors import KNeighborsClassifier</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
<li>Now, we <a id="_idIndexMarker529"/>can create training and <a id="_idIndexMarker530"/>testing DataFrames and encode <a id="_idIndexMarker531"/>and scale the data:<p class="source-code">nls97compba = pd.read_csv("data/nls97compba.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpaoverall',</p><p class="source-code">  'parentincome','gender']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97compba[feature_cols],\</p><p class="source-code">  nls97compba[['completedba']], test_size=0.3, random_state=0)</p><p class="source-code">ohe = OneHotEncoder(drop_last=True, variables=['gender'])</p><p class="source-code">ohe.fit(X_train)</p><p class="source-code">X_train_enc, X_test_enc = \</p><p class="source-code">  ohe.transform(X_train), ohe.transform(X_test)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">scaler.fit(X_train_enc[standcols])</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Female']])</p><p class="source-code">X_test_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_test_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_test_enc.index).\</p><p class="source-code">  join(X_test_enc[['gender_Female']])</p></li>
<li>Let’s <a id="_idIndexMarker532"/>create a KNN classification <a id="_idIndexMarker533"/>model. We will not <a id="_idIndexMarker534"/>worry too much about <a id="_idIndexMarker535"/>how we specify it since we just want to focus on evaluation measures in this section. We will use all of the features listed in <strong class="source-inline">feature_cols</strong> We use the predict method of the KNN classifier to generate predictions from the testing data:<p class="source-code">knn = KNeighborsClassifier(n_neighbors = 5)</p><p class="source-code">knn.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">pred = knn.predict(X_test_enc)</p></li>
<li>We can use scikit-learn to plot a confusion matrix. We will pass the actual values in the testing data (<strong class="source-inline">y_test</strong>) and predicted values to the <strong class="source-inline">confusion_matrix</strong> method:<p class="source-code">cm = skmet.confusion_matrix(y_test, pred, labels=knn.classes_)</p><p class="source-code">cmplot = skmet.ConfusionMatrixDisplay(</p><p class="source-code">  confusion_matrix=cm, </p><p class="source-code">  display_labels=['Negative', 'Positive'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(title='Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This generates <a id="_idIndexMarker536"/>the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<img alt="Figure 6.3 – Confusion matrix of actual and predicted values " height="443" src="image/B17978_06_0031.jpg" width="551"/>
</div>
</div>
<p class="figure-caption">Figure 6.3 – Confusion matrix of actual and predicted values</p>
<ol>
<li value="5">We <a id="_idIndexMarker537"/>can <a id="_idIndexMarker538"/>also just return the true negative, false <a id="_idIndexMarker539"/>positive, false negative, and true positive counts:<p class="source-code">tn, fp, fn, tp = skmet.confusion_matrix(</p><p class="source-code">  y_test.values.ravel(), pred).ravel()</p><p class="source-code">tn, fp, fn, tp</p><p class="source-code"><strong class="bold">(53, 63, 31, 126)</strong></p></li>
<li>We now <a id="_idIndexMarker540"/>have what we need to calculate accuracy, sensitivity, specificity, and precision:<p class="source-code">accuracy = (tp + tn) / pred.shape[0]</p><p class="source-code">accuracy</p><p class="source-code"><strong class="bold">0.6556776556776557</strong></p><p class="source-code">sensitivity = tp / (tp + fn)</p><p class="source-code">sensitivity</p><p class="source-code"><strong class="bold">0.802547770700637</strong></p><p class="source-code">specificity = tn / (tn+fp)</p><p class="source-code">specificity</p><p class="source-code"><strong class="bold">0.45689655172413796</strong></p><p class="source-code">precision = tp / (tp + fp)</p><p class="source-code">precision</p><p class="source-code"><strong class="bold">0.6666666666666666</strong></p></li>
</ol>
<p>This <a id="_idIndexMarker541"/>model has relatively low accuracy, but somewhat better sensitivity; that is, it does a better job of identifying those in the testing data who have completed a bachelor’s degree than of correctly identifying both degree completers and non-completers overall. If we look back <a id="_idIndexMarker542"/>at the confusion matrix, we will see that there are a fair number of false positives, as our model predicts that 63 individuals in the testing data would <a id="_idIndexMarker543"/>have a bachelor’s degree who did not.</p>
<ol>
<li value="7">We could <a id="_idIndexMarker544"/>have also used scikit-learn handy methods for generating these statistics directly:<p class="source-code">skmet.accuracy_score(y_test.values.ravel(), pred)</p><p class="source-code"><strong class="bold">0.6556776556776557</strong></p><p class="source-code">skmet.recall_score(y_test.values.ravel(), pred)</p><p class="source-code"><strong class="bold">0.802547770700637</strong></p><p class="source-code">skmet.precision_score(y_test.values.ravel(), pred)</p><p class="source-code"><strong class="bold">0.6666666666666666</strong></p></li>
</ol>
<p>Just for comparison, let’s try a random forest classifier and see if we get any better results.</p>
<ol>
<li value="8">Let’s<a id="_idIndexMarker545"/> fit a random forest classifier to the same data and call <strong class="source-inline">confusion_matrix</strong> again:<p class="source-code">rfc = RandomForestClassifier(n_estimators=100, </p><p class="source-code">  max_depth=2, n_jobs=-1, random_state=0)</p><p class="source-code">rfc.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">pred = rfc.predict(X_test_enc)</p><p class="source-code">tn, fp, fn, tp = skmet.confusion_matrix(</p><p class="source-code">  y_test.values.ravel(), pred).ravel()</p><p class="source-code">tn, fp, fn, tp</p><p class="source-code">(<strong class="bold">49, 67, 17, 140)</strong></p><p class="source-code">accuracy = (tp + tn) / pred.shape[0]</p><p class="source-code">accuracy</p><p class="source-code"><strong class="bold">0.6923076923076923</strong></p><p class="source-code">sensitivity = tp / (tp + fn)</p><p class="source-code">sensitivity</p><p class="source-code"><strong class="bold">0.89171974522293</strong></p><p class="source-code">specificity = tn / (tn+fp)</p><p class="source-code">specificity</p><p class="source-code"><strong class="bold">0.4224137931034483</strong></p><p class="source-code">precision = tp / (tp + fp)</p><p class="source-code">precision</p><p class="source-code"><strong class="bold">0.6763285024154589</strong></p></li>
</ol>
<p>The <a id="_idIndexMarker546"/>second model gets us significantly<a id="_idIndexMarker547"/> fewer false negatives and more <a id="_idIndexMarker548"/>true positives than<a id="_idIndexMarker549"/> the first model. It is less likely to predict no bachelor’s degree when individuals in the test data have completed a bachelor’s degree, and more likely to predict a bachelor’s degree when the person has completed one. The main impact of the lower FP and higher TP is a significantly higher sensitivity. The second model identifies actual positives 89% of the time, compared with 80% for the first model.</p>
<p>The measures we have discussed in this section – accuracy, sensitivity, specificity, and precision – are worth looking at whenever we are evaluating a classification model. But it can be hard to get a good sense of the tradeoffs we are sometimes confronted with, between precision and sensitivity, for example. Data scientists rely on several standard visualizations to improve our sense of these tradeoffs when building classification models. We will examine these visualizations in the next section.</p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/>Examining CAP, ROC, and precision-sensitivity curves for binary classification</h1>
<p>There are several ways to visualize the performance of a binary classification model. A relatively straightforward visualization is the <strong class="bold">Cumulative Accuracy Profile</strong> (<strong class="bold">CAP</strong>), which <a id="_idIndexMarker550"/>shows the ability of our model to identify in-class, or positive, cases. It shows the<a id="_idIndexMarker551"/> cumulative cases on the <em class="italic">X</em>-axis and the cumulative positive outcomes on the <em class="italic">Y</em>-axis. A CAP curve is a good way to see how good a job our model does at discriminating in-class observations. (When discussing binary classification models, I will use the terms <em class="italic">in-class</em> and <em class="italic">positive</em> interchangeably.)</p>
<p><strong class="bold">Receiver operating characteristic</strong> (<strong class="bold">ROC</strong>) curves<a id="_idIndexMarker552"/> illustrate the tradeoff between model sensitivity (being able to identify positive values) and the false positive rate as we adjust the threshold for classifying a positive value. Similarly, precision-sensitivity curves show the relationship between the reliability of our positive predictions (their precision) and sensitivity (our model’s ability to identify positive actual values) as we adjust the threshold.</p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/>Constructing CAP curves</h2>
<p>Let’s start with <a id="_idIndexMarker553"/>CAP curves for our bachelor’s completion KNN model. Let’s also compare that with a decision tree model. Again, we will not do much with feature selection here. The previous chapter went over feature selection in some detail.</p>
<p>In addition to curves for our models, CAP curves also have plots of a <strong class="bold">random model</strong> and a <strong class="bold">perfect model</strong> to view for comparison. The random model provides no information other than the overall distribution of positive values. The perfect model predicts positive values precisely. To illustrate how those plots are drawn, we will start with a hypothetical example. Imagine that you sample the first six cards of a nicely shuffled deck of playing cards. You create a table with the cumulative card total in one column and the number of red cards in the next column. It may look something like this:</p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<img alt="Figure 6.4 – Sample of playing cards  " height="549" src="image/B17978_06_0041.jpg" width="972"/>
</div>
</div>
<p class="figure-caption">Figure 6.4 – Sample of playing cards </p>
<p>We can plot a random model based on just our knowledge of the number of red cards. The random model has just two points, (0,0) and (6,3), but that is all we need. </p>
<p>The perfect model plot requires a bit more explanation. If our model predicted red cards perfectly and we sorted by the prediction in descending order, we would get <em class="italic">Figure 6.5</em>. The cumulative in-class count matches the number of cards until the red cards have been exhausted, which is 3 in this case. A plot of the cumulative in-class total with a perfect <a id="_idIndexMarker554"/>model would have two slopes; equal to 1 up until the in-class total was reached, and then 0 after that:</p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<img alt="Figure 6.5 – Sample of playing cards " height="618" src="image/B17978_06_005.jpg" width="1283"/>
</div>
</div>
<p class="figure-caption">Figure 6.5 – Sample of playing cards</p>
<p>We now know enough to plot both the random model and the perfect model. The perfect model will have three points: (0,0), (in-class count, in-class count), and (number of cards, in-class count). In this case, in-class count is <strong class="source-inline">3</strong> and the number of cards is <strong class="source-inline">6</strong>:</p>
<pre class="source-code">numobs = 6</pre>
<pre class="source-code">inclasscnt = 3</pre>
<pre class="source-code">plt.yticks([1,2,3])</pre>
<pre class="source-code">plt.plot([0, numobs], [0, inclasscnt], c = 'b', label = 'Random Model')</pre>
<pre class="source-code">plt.plot([0, inclasscnt, numobs], [0, inclasscnt, inclasscnt], c = 'grey', linewidth = 2, label = 'Perfect Model')</pre>
<pre class="source-code">plt.title("Cumulative Accuracy Profile")</pre>
<pre class="source-code">plt.xlabel("Total Cards")</pre>
<pre class="source-code">plt.ylabel("In-class (Red) Cards")</pre>
<p>This <a id="_idIndexMarker555"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer076">
<img alt="Figure 6.6 – CAP with playing card data " height="443" src="image/B17978_06_006.jpg" width="548"/>
</div>
</div>
<p class="figure-caption">Figure 6.6 – CAP with playing card data</p>
<p>One way to understand the improvement of the perfect model over the random model is to consider how many red cards the random model would predict at the midpoint – that is, 3 cards. At that point, the random model would predict 1.5 red cards. However, the perfect model would predict 3. (Remember that we have sorted the cards by prediction in descending order.)</p>
<p>Having constructed plots for random and perfect models with made-up data, let’s try it with our bachelor’s degree completion data:</p>
<ol>
<li value="1">First, we must import the same modules as in the previous section:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.neighbors import KNeighborsClassifier</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import seaborn as sb</p></li>
<li>Then, we <a id="_idIndexMarker556"/> load, encode, and scale the NLS bachelor’s degree data:<p class="source-code">nls97compba = pd.read_csv("data/nls97compba.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpaoverall',</p><p class="source-code">  'parentincome','gender']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97compba[feature_cols],\</p><p class="source-code">  nls97compba[['completedba']], test_size=0.3, random_state=0)</p><p class="source-code">ohe = OneHotEncoder(drop_last=True, variables=['gender'])</p><p class="source-code">ohe.fit(X_train)</p><p class="source-code">X_train_enc, X_test_enc = \</p><p class="source-code">  ohe.transform(X_train), ohe.transform(X_test)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">scaler.fit(X_train_enc[standcols])</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Female']])</p><p class="source-code">X_test_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_test_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_test_enc.index).\</p><p class="source-code">  join(X_test_enc[['gender_Female']])</p></li>
<li>Next, we <a id="_idIndexMarker557"/> create <strong class="source-inline">KNeighborsClassifier</strong> and <strong class="source-inline">RandomForestClassifier</strong> instances:<p class="source-code">knn = KNeighborsClassifier(n_neighbors = 5)</p><p class="source-code">rfc = RandomForestClassifier(n_estimators=100, max_depth=2, </p><p class="source-code">  n_jobs=-1, random_state=0)</p></li>
</ol>
<p>We are now ready to start plotting our CAP curves. We will start by drawing a random model and then a perfect model. These are models that use no information (other than the overall distribution of positive values) and that provide perfect information, respectively.</p>
<ol>
<li value="4">We count the number of observations in the test data and the number of positive values. We will use (0,0) and (the number of observations, in-class count) to draw the random model line. For the perfect model, we will plot a line from (0,0) to (in-class count, in-class count) since that model can perfectly discriminate in-class values (it is never wrong). It is flat to the right of that point since there are no more positive values to find.</li>
</ol>
<p>We will <a id="_idIndexMarker558"/>also draw a vertical line at the midpoint and a horizontal line where that intersects the random model line. This will be more useful later:</p>
<p class="source-code">numobs = y_test.shape[0]</p>
<p class="source-code">inclasscnt = y_test.iloc[:,0].sum()</p>
<p class="source-code">plt.plot([0, numobs], [0, inclasscnt], c = 'b', label = 'Random Model')</p>
<p class="source-code">plt.plot([0, inclasscnt, numobs], [0, inclasscnt, inclasscnt], c = 'grey', linewidth = 2, label = 'Perfect Model')</p>
<p class="source-code">plt.axvline(numobs/2, color='black', linestyle='dashed', linewidth=1)</p>
<p class="source-code">plt.axhline(numobs/2, color='black', linestyle='dashed', linewidth=1)</p>
<p class="source-code">plt.title("Cumulative Accuracy Profile")</p>
<p class="source-code">plt.xlabel("Total Observations")</p>
<p class="source-code">plt.ylabel("In-class Observations")</p>
<p class="source-code">plt.legend()</p>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<img alt="Figure 6.7 – CAP with just random and perfect models " height="446" src="image/B17978_06_007.jpg" width="563"/>
</div>
</div>
<p class="figure-caption">Figure 6.7 – CAP with just random and perfect models</p>
<ol>
<li value="5">Next, we <a id="_idIndexMarker559"/>define a function to plot a CAP curve for a model we pass to it. We will use the <strong class="source-inline">predict_proba</strong> method to get an array with the probability that each observation in the test data is in-class (in this case, has completed a bachelor’s degree). Then, we will create a DataFrame with those probabilities and the actual target value, sort it by probability in reverse order, and calculate a running total of positive actual target values.</li>
</ol>
<p>We will also get the value of the running total at the middle observation and draw a horizontal line at that point. Finally, we will plot a line that has an array from 0 to the number of observations as <em class="italic">x</em> values, and the running in-class totals as <em class="italic">y</em> values:</p>
<p class="source-code">def addplot(model, X, Xtest, y, modelname, linecolor):</p>
<p class="source-code">  model.fit(X, y.values.ravel())</p>
<p class="source-code">  probs = model.predict_proba(Xtest)[:, 1]</p>
<p class="source-code">  </p>
<p class="source-code">  probdf = pd.DataFrame(zip(probs, y_test.values.ravel()),</p>
<p class="source-code">    columns=(['prob','inclass']))</p>
<p class="source-code">  probdf.loc[-1] = [0,0]</p>
<p class="source-code">  probdf = probdf.sort_values(['prob','inclass'],</p>
<p class="source-code">    ascending=False).\</p>
<p class="source-code">    assign(inclasscum = lambda x: x.inclass.cumsum())</p>
<p class="source-code">  inclassmidpoint = \</p>
<p class="source-code">    probdf.iloc[int(probdf.shape[0]/2)].inclasscum</p>
<p class="source-code">  plt.axhline(inclassmidpoint, color=linecolor,</p>
<p class="source-code">    linestyle='dashed', linewidth=1)</p>
<p class="source-code">  plt.plot(np.arange(0, probdf.shape[0]),</p>
<p class="source-code">    probdf.inclasscum, c = linecolor,</p>
<p class="source-code">    label = modelname, linewidth = 4)</p>
<ol>
<li value="6">Now, let’s <a id="_idIndexMarker560"/>run the function for the KNN and random forest classifier models using the same data:<p class="source-code">addplot(knn, X_train_enc, X_test_enc, y_train,</p><p class="source-code">  'KNN', 'red')</p><p class="source-code">addplot(rfc, X_train_enc, X_test_enc, y_train,</p><p class="source-code">  'Random Forest', 'green')</p><p class="source-code">plt.legend()</p></li>
</ol>
<p>This updates our earlier plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<img alt="Figure 6.8 – CAP updated with KNN and random forest models " height="449" src="image/B17978_06_008.jpg" width="569"/>
</div>
</div>
<p class="figure-caption">Figure 6.8 – CAP updated with KNN and random forest models</p>
<p>Not <a id="_idIndexMarker561"/>surprisingly, the CAP curves show that our KNN and random forest models are better than randomly guessing, but not as good as a perfect model. The question is, how much better and how much worse, respectively. The horizontal lines give us some idea. A perfect model would have correctly identified 138 positive values out of 138 observations. (Recall that the observations are sorted so that the observations with the highest likelihood of being positive are first.) The random model would have identified 70 (line not shown), while the KNN and random forest models would have identified 102 and 103, respectively. Our two models are 74% and 75% as good as a perfect model would have been at discriminating positive values. Anything between 70% and 80% is considered to be a good model; percentages above that are very good, while percentages below that are poor.</p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/>Plotting a receiver operating characteristic (ROC) curve</h2>
<p>ROC curves<a id="_idIndexMarker562"/> illustrate the tradeoff between the false positive rate and the true positive rate (also known as sensitivity) as we adjust the threshold. We should discuss the false positive rate before going further. It is the percentage of actual negatives (true negatives plus false positives) that our model falsely identifies as positive:</p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<img alt="" height="136" src="image/B17978_06_0051.jpg" width="856"/>
</div>
</div>
<p>Here, you can see the relationship that the false positive rate has with specificity, which was discussed at the beginning of this chapter. The difference is the numerator. Specificity is the percentage of actual negatives that our model correctly identifies as negative:</p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="" height="133" src="image/B17978_06_0061.jpg" width="661"/>
</div>
</div>
<p>We can also compare the false positive rate with sensitivity, which is the percentage of actual positives (true positives plus false negatives) that our model correctly identifies as positive:</p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="" height="131" src="image/B17978_06_0071.jpg" width="648"/>
</div>
</div>
<p>We are typically confronted with a tradeoff between sensitivity and the false positive rate. We want our models to be able to identify a large percentage of the actual positives, but we do not want a problematically high false positive rate. What is <em class="italic">problematically high</em> depends on your context.</p>
<p>The tradeoff between <a id="_idIndexMarker563"/>sensitivity and the false positive rate is trickier the more difficult it is to discriminate between negative and positive cases. We can see this with our bachelor’s degree completion model when we plot the predicted probabilities:</p>
<ol>
<li value="1">First, let’s fit our random forest classifier again and generate predictions and prediction probabilities. We will see that this model predicts that the person completes a bachelor’s degree when the predicted probability is greater than <strong class="source-inline">0.500</strong>:<p class="source-code">rfc.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">pred = rfc.predict(X_test_enc)</p><p class="source-code">pred_probs = rfc.predict_proba(X_test_enc)[:, 1]</p><p class="source-code">probdf = pd.DataFrame(zip(</p><p class="source-code">  pred_probs, pred, y_test.values.ravel()),</p><p class="source-code">  columns=(['prob','pred','actual']))</p><p class="source-code">probdf.groupby(['pred'])['prob'].agg(['min','max'])</p><p class="source-code"><strong class="bold">                min             max</strong></p><p class="source-code"><strong class="bold">pred             </strong></p><p class="source-code"><strong class="bold">0.000           0.305           0.500</strong></p><p class="source-code"><strong class="bold">1.000           0.502           0.883</strong></p></li>
<li>It is helpful to compare the distribution of these probabilities with the actual class values. We can do this with density plots:<p class="source-code">sb.kdeplot(probdf.loc[probdf.actual==1].prob, </p><p class="source-code">  shade=True, color='red',</p><p class="source-code">  label="Completed BA")</p><p class="source-code">sb.kdeplot(probdf.loc[probdf.actual==0].prob,  </p><p class="source-code">  shade=True, color='green',</p><p class="source-code">  label="Did Not Complete")</p><p class="source-code">plt.axvline(0.5, color='black', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.axvline(0.65, color='black', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Predicted Probability Distribution")</p><p class="source-code">plt.legend(loc="upper left")</p></li>
</ol>
<p>This <a id="_idIndexMarker564"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 6.9 – Density plot of in-class and out-of-class observations " height="446" src="image/B17978_06_009.jpg" width="561"/>
</div>
</div>
<p class="figure-caption">Figure 6.9 – Density plot of in-class and out-of-class observations</p>
<p>Here, we can see that our model has some trouble discriminating between actual positive and negative values since there is a fair bit of in-class and out-of-class overlap. A threshold of 0.500 (the left dotted line) gets us a lot of false positives since a good portion of the distribution of out-of-class observations (those not completing bachelor’s degrees) have predicted probabilities greater than 0.500. If we move the threshold higher, say to 0.650, we get many more false negatives since many in-class observations have probabilities lower than 0.65.</p>
<ol>
<li value="3">It is easy<a id="_idIndexMarker565"/> to construct a ROC curve based on the testing data and the random forest model. The <strong class="source-inline">roc_curve</strong> method returns both the false positive rate (<strong class="source-inline">fpr</strong>) and sensitivity (true positive rate, <strong class="source-inline">tpr</strong>) at different thresholds (<strong class="source-inline">ths</strong>).</li>
</ol>
<p>First, let’s draw separate false positive rate and sensitivity lines by threshold:</p>
<p class="source-code">fpr, tpr, ths = skmet.roc_curve(y_test, pred_probs)</p>
<p class="source-code">ths = ths[1:]</p>
<p class="source-code">fpr = fpr[1:]</p>
<p class="source-code">tpr = tpr[1:]</p>
<p class="source-code">fig, ax = plt.subplots()</p>
<p class="source-code">ax.plot(ths, fpr, label="False Positive Rate")</p>
<p class="source-code">ax.plot(ths, tpr, label="Sensitivity")</p>
<p class="source-code">ax.set_title('False Positive Rate and Sensitivity by Threshold')</p>
<p class="source-code">ax.set_xlabel('Threshold')</p>
<p class="source-code">ax.set_ylabel('False Positive Rate and Sensitivity')</p>
<p class="source-code">ax.legend()</p>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="Figure 6.10 – False positive rate and sensitivity lines " height="449" src="image/B17978_06_010.jpg" width="567"/>
</div>
</div>
<p class="figure-caption">Figure 6.10 – False positive rate and sensitivity lines</p>
<p>Here, we <a id="_idIndexMarker566"/>can see that increasing the threshold will improve (reduce) our false positive rate, but also lower our sensitivity.</p>
<ol>
<li value="4">Now, let’s draw the associated ROC curve, which plots the false positive rate against sensitivity for each threshold:<p class="source-code">fig, ax = plt.subplots()</p><p class="source-code">ax.plot(fpr, tpr, linewidth=4, color="black")</p><p class="source-code">ax.set_title('ROC curve')</p><p class="source-code">ax.set_xlabel('False Positive Rate')</p><p class="source-code">ax.set_ylabel('Sensitivity')</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="Figure 6.11 – ROC curve with false positive rate and sensitivity " height="440" src="image/B17978_06_011.jpg" width="561"/>
</div>
</div>
<p class="figure-caption">Figure 6.11 – ROC curve with false positive rate and sensitivity</p>
<p>The <a id="_idIndexMarker567"/>ROC curve indicates that the tradeoff between the false positive rate and sensitivity is pretty steep until the false positive rate is about 0.5 or higher. Let’s see what that means for the threshold of 0.5 that was used for the random forest model predictions. </p>
<ol>
<li value="5">Let’s select an index from the threshold array that is near 0.5, and also one near 0.4 and 0.6 for comparison. Then, we will draw vertical lines for the false positive rate at those indexes, and horizontal lines for the sensitivity values at those indexes:<p class="source-code">tholdind = np.where((ths&gt;0.499) &amp; (ths&lt;0.501))[0][0]</p><p class="source-code">tholdindlow = np.where((ths&gt;0.397) &amp; (ths&lt;0.404))[0][0]</p><p class="source-code">tholdindhigh = np.where((ths&gt;0.599) &amp; (ths&lt;0.601))[0][0]</p><p class="source-code">plt.vlines((fpr[tholdindlow],fpr[tholdind],</p><p class="source-code">  fpr[tholdindhigh]), 0, 1, linestyles ="dashed", </p><p class="source-code">  colors =["green","blue","purple"])</p><p class="source-code">plt.hlines((tpr[tholdindlow],tpr[tholdind],</p><p class="source-code">  tpr[tholdindhigh]), 0, 1, linestyles ="dashed", </p><p class="source-code">  colors =["green","blue","purple"])</p></li>
</ol>
<p>This updates <a id="_idIndexMarker568"/>our plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<img alt="Figure 6.12 – ROC curve with lines for thresholds " height="443" src="image/B17978_06_012.jpg" width="560"/>
</div>
</div>
<p class="figure-caption">Figure 6.12 – ROC curve with lines for thresholds</p>
<p>This illustrates the tradeoff between the false positive rate and sensitivity at the 0.5 threshold (the blue dashed line) used for predictions. The ROC curve<a id="_idIndexMarker569"/> has very little slope with thresholds above 0.5, such as with the 0.6 threshold (the green dashed line). So, reducing the threshold from 0.6 to 0.5 results in a substantially lower false positive rate (from above 0.8 to below 0.6), but not much reduction in sensitivity. However, improving (reducing) the false positive rate by reducing the threshold from 0.5 to 0.4 (from the blue to the purple line) leads to significantly worse sensitivity. It drops from nearly 90% to just above 70%.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor084"/>Plotting precision-sensitivity curves</h2>
<p>It is often helpful to <a id="_idIndexMarker570"/>examine the relationship between precision and sensitivity as the threshold is adjusted. Remember that precision tells us the percentage of the time we are correct when we predict a positive value:</p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<img alt="" height="135" src="image/B17978_06_0081.jpg" width="610"/>
</div>
</div>
<p>We can improve precision by increasing the threshold for classifying a value as positive. However, this will likely mean a reduction in sensitivity. As we improve how often we are correct when we predict a positive value (precision), we will decrease the number of positive values we are able to identify (sensitivity). Precision-sensitivity curves, often called precision-recall curves, illustrate this tradeoff.</p>
<p>Before drawing<a id="_idIndexMarker571"/> the precision-sensitivity curve, let’s look at separate precision and sensitivity lines plotted against thresholds:</p>
<ol>
<li value="1">We can get the points for the precision-sensitivity curves with the <strong class="source-inline">precision_recall_curve</strong> method. We remove some squirreliness at the highest threshold values, which can sometimes happen:<p class="source-code">prec, sens, ths = skmet.precision_recall_curve(y_test, pred_probs)</p><p class="source-code">prec = prec[1:-10]</p><p class="source-code">sens = sens[1:-10]</p><p class="source-code">ths  = ths[:-10]</p><p class="source-code">fig, ax = plt.subplots()</p><p class="source-code">ax.plot(ths, prec, label='Precision')</p><p class="source-code">ax.plot(ths, sens, label='Sensitivity')</p><p class="source-code">ax.set_title('Precision and Sensitivity by Threshold')</p><p class="source-code">ax.set_xlabel('Threshold')</p><p class="source-code">ax.set_ylabel('Precision and Sensitivity')</p><p class="source-code">ax.set_xlim(0.3,0.9)</p><p class="source-code">ax.legend()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<img alt="Figure 6.13 – Precision and sensitivity lines " height="439" src="image/B17978_06_013.jpg" width="569"/>
</div>
</div>
<p class="figure-caption">Figure 6.13 – Precision and sensitivity lines</p>
<p>Here, we <a id="_idIndexMarker572"/>can see that sensitivity declines more steeply with thresholds above 0.5. This decline does not buy us much improved precision beyond the 0.6 threshold.</p>
<ol>
<li value="2">Now, let’s plot sensitivity against precision to view the precision-sensitivity curve:<p class="source-code">fig, ax = plt.subplots()</p><p class="source-code">ax.plot(sens, prec)</p><p class="source-code">ax.set_title('Precision-Sensitivity Curve')</p><p class="source-code">ax.set_xlabel('Sensitivity')</p><p class="source-code">ax.set_ylabel('Precision')</p><p class="source-code">plt.yticks(np.arange(0.2, 0.9, 0.2))</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<img alt="Figure 6.14 – Precision-sensitivity curve " height="444" src="image/B17978_06_014.jpg" width="560"/>
</div>
</div>
<p class="figure-caption">Figure 6.14 – Precision-sensitivity curve</p>
<p>The precision-sensitivity<a id="_idIndexMarker573"/> curve reflects the fact that sensitivity is much more responsive to threshold than is precision with this particular model. This means that we could decrease the threshold below 0.5 to get greater sensitivity, without a significant reduction in precision.</p>
<p class="callout-heading">Note</p>
<p class="callout">The choice of threshold is partly a matter of judgment and domain knowledge, and is mostly an issue when we have significant class imbalance.  However, in <a href="B17978_10_ePub.xhtml#_idTextAnchor126"><em class="italic">Chapter 10</em></a><em class="italic">, Logistic Regression</em> we will explore how to calculate an optimal threshold.</p>
<p>This section, and the previous one, demonstrated how to evaluate binary classification models. They showed that model evaluation is not just a thumbs up and thumbs down process. It is much more like tasting your batter as you make a cake. We make good initial assumptions about our model specification and use the model evaluation process to make improvements. This often involves tradeoffs between accuracy, sensitivity, specificity, and precision, and modeling decisions that resist one-size-fits-all recommendations. These decisions are very much domain-dependent and a matter of professional judgment. </p>
<p>The discussion <a id="_idIndexMarker574"/>in this section, and most of the techniques, apply as much to multiclass modeling. We discuss evaluating multiclass models in the next section.</p>
<h1 id="_idParaDest-86"><a id="_idTextAnchor085"/>Evaluating multiclass models</h1>
<p>All of the same <a id="_idIndexMarker575"/>principles that we used to evaluate binary classification models apply to multiclass model evaluation. Computing a confusion matrix is just as important, though a fair bit more difficult to interpret. We also still need to examine somewhat competing measures, such as precision and sensitivity. This, too, is messier than doing so with binary classification.</p>
<p>Once again, we will work with the NLS degree completion data. We will alter the target in this case, from bachelor’s degree completion or not to high school completion, bachelor’s degree completion, and post-graduate degree completion:</p>
<ol>
<li value="1">We will start by loading the necessary libraries. These are the same libraries we used in the previous two sections:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.neighbors import KNeighborsClassifier</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
<li>Next, we will load the NLS degree attainment data, create training and testing DataFrames, and encode and scale the data:<p class="source-code">nls97degreelevel = pd.read_csv("data/nls97degreelevel.csv")</p><p class="source-code">feature_cols = ['satverbal','satmath','gpaoverall',</p><p class="source-code">  'parentincome','gender']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nls97degreelevel[feature_cols],\</p><p class="source-code">  nls97degreelevel[['degreelevel']], test_size=0.3, random_state=0)</p><p class="source-code">ohe = OneHotEncoder(drop_last=True, variables=['gender'])</p><p class="source-code">ohe.fit(X_train)</p><p class="source-code">X_train_enc, X_test_enc = \</p><p class="source-code">  ohe.transform(X_train), ohe.transform(X_test)</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">standcols = X_train_enc.iloc[:,:-1].columns</p><p class="source-code">scaler.fit(X_train_enc[standcols])</p><p class="source-code">X_train_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_train_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_train_enc.index).\</p><p class="source-code">  join(X_train_enc[['gender_Female']])</p><p class="source-code">X_test_enc = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_test_enc[standcols]),</p><p class="source-code">  columns=standcols, index=X_test_enc.index).\</p><p class="source-code">  join(X_test_enc[['gender_Female']])</p></li>
<li>Now, we will run a KNN model and predict values for each degree level category:<p class="source-code">knn = KNeighborsClassifier(n_neighbors = 5)</p><p class="source-code">knn.fit(X_train_enc, y_train.values.ravel())</p><p class="source-code">pred = knn.predict(X_test_enc)</p><p class="source-code">pred_probs = knn.predict_proba(X_test_enc)[:, 1]</p></li>
<li>We can<a id="_idIndexMarker576"/> use those predictions to generate a confusion matrix:<p class="source-code">cm = skmet.confusion_matrix(y_test, pred)</p><p class="source-code">cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['High School', 'Bachelor','Post-Graduate'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(title='Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This generates the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 6.15 – Confusion matrix with a multiclass target " height="475" src="image/B17978_06_015.jpg" width="621"/>
</div>
</div>
<p class="figure-caption">Figure 6.15 – Confusion matrix with a multiclass target</p>
<p>It is <a id="_idIndexMarker577"/>possible to calculate evaluation measures by hand. Precision is the percentage of our in-class predictions that are actually in-class. So, for our prediction of high school, it is 48 / (48 + 38 + 8) = 0.51. Sensitivity for the high school class – that is, the percentage of actual values of high school that our model predicts – is 48 / (48 + 19 +5) = 0.67. However, this is fairly tedious. Fortunately, scikit-learn can do this for us.</p>
<ol>
<li value="5">We can call the <strong class="source-inline">classification_report</strong> method to get these statistics, passing actual and predicted values (remember that recall and sensitivity are the same measure):<p class="source-code">print(skmet.classification_report(y_test, pred,</p><p class="source-code">  target_names=['High School', 'Bachelor', 'Post-Graduate']))</p><p class="source-code">               <strong class="bold">precision    recall  f1-score   support</strong></p><p class="source-code"><strong class="bold">  High School       0.51      0.67      0.58        72</strong></p><p class="source-code"><strong class="bold">     Bachelor       0.51      0.49      0.50        92</strong></p><p class="source-code"><strong class="bold">Post-Graduate       0.42      0.24      0.30        42</strong></p><p class="source-code"><strong class="bold">     accuracy                           0.50       206</strong></p><p class="source-code"><strong class="bold">    macro avg       0.48      0.46      0.46       206</strong></p><p class="source-code"><strong class="bold"> weighted avg       0.49      0.50      0.49       206</strong></p></li>
</ol>
<p>In addition to<a id="_idIndexMarker578"/> precision and sensitivity rates by class, we get some other statistics. The F1-score is the harmonic mean of precision and sensitivity.</p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="" height="114" src="image/B17978_06_0091.jpg" width="444"/>
</div>
</div>
<p>Here, <em class="italic">p</em> is precision and <em class="italic">s</em> is sensitivity.</p>
<p>To get the average precision, sensitivity, and F1-score across classes, we can either use the simple average (macro average) or a weighted average that adjusts for class size. Using the weighted average, we get precision, sensitivity, and F1-score values of 0.49, 0.50, and 0.49, respectively. (Since the classes are relatively balanced here, there is not much difference between the macro average and the weighted average.)</p>
<p>This demonstrates how to extend the evaluation measures we discussed for binary classification models to multiclass evaluation. The same concepts and techniques apply, though they are more difficult to implement.</p>
<p>So far, we have focused on metrics and visualizations to help us evaluate classification models. We have not examined metrics for evaluating regression models yet. These metrics can be somewhat more straightforward than those for classification. We will discuss them in the next section. </p>
<h1 id="_idParaDest-87"><a id="_idTextAnchor086"/>Evaluating regression models</h1>
<p>Metrics for regression evaluation <a id="_idIndexMarker579"/>are typically based on the distance between the actual values for the target variable and a model’s predicted values. The most common measures – mean squared error, root mean squared error, mean absolute error, and R-squared – all track how successfully our predictions capture variation in a target.</p>
<p>The distance between the actual value and our prediction is known as the residual, or error. The <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) is the<a id="_idIndexMarker580"/> mean of the square of the residuals:</p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="" height="164" src="image/B17978_06_0101.jpg" width="490"/>
</div>
</div>
<p>Here, <img alt="" height="37" src="image/B17978_06_011.png" width="40"/> is the actual target variable value at the ith observation and <img alt="" height="47" src="image/B17978_06_012.png" width="40"/> is our prediction for the target. The residuals are squared to handle negative values, where the predicted value is higher than the actual value. To return our measurement to a more meaningful scale, we often use the square root of MSE. That is <a id="_idIndexMarker581"/>known as <strong class="bold">root mean squared error</strong> (<strong class="bold">RMSE</strong>).</p>
<p>Due to the squaring, MSE will penalize larger residuals much more than it will smaller residuals. For example, if we have predictions for five observations, with one having a residual of 25, and the other four having a residual of 0, we will get an MSE of <em class="italic">(0+0+0+0+625)/5 = 125</em>. However, if all five observations had residuals of 5, the MSE would be <em class="italic">(25+25+25+25+25)/5 = 25</em>.</p>
<p>A good alternative to squaring the residuals is to take their absolute value. This gives us the mean absolute error:</p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<img alt="" height="164" src="image/B17978_06_0131.jpg" width="480"/>
</div>
</div>
<p>R-squared, also known as the coefficient of determination, is an estimate of the proportion of the variation in the target variable captured by our model. We square the residuals, as we do when calculating MSE, and divide that by the deviation of each actual target value from its sample mean. This gives us the still unexplained variation, which we subtract from 1 to get the explained variation:</p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<img alt="" height="121" src="image/B17978_06_0141.jpg" width="1023"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer096">
<img alt="" height="133" src="image/B17978_06_0151.jpg" width="394"/>
</div>
</div>
<p>Fortunately, scikit-learn <a id="_idIndexMarker582"/>makes it easy to generate these statistics. In this section, we will build a linear regression model of land temperatures and use these statistics to evaluate it. We will work with data from the United States National Oceanic and Atmospheric Administration on average annual temperatures, elevation, and latitude at weather stations in 2019.</p>
<p class="callout-heading">Note</p>
<p class="callout">The land temperature dataset contains the average temperature readings (in Celsius) in 2019 from over 12,000 stations across the world, though the majority of the stations are in the United States. The raw data was retrieved from the Global Historical Climatology Network integrated database. It has been made available for public use by the United States National Oceanic and Atmospheric Administration at <a href="https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4">https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4</a>.</p>
<p>Let’s start building a linear regression model:</p>
<ol>
<li value="1">We will start by loading the libraries we need and the land temperatures data. We will also<a id="_idIndexMarker583"/> create training and testing DataFrames:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">landtemps = pd.read_csv("data/landtemps2019avgs.csv")</p><p class="source-code">feature_cols = ['latabs','elevation']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(landtemps[feature_cols],\</p><p class="source-code">  landtemps[['avgtemp']], test_size=0.3, random_state=0)</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">latabs</strong> feature<a id="_idIndexMarker584"/> is the value of latitude without the North or South indicators; so, Cairo, Egypt, at approximately 30 degrees north, and Porto Alegre, Brazil, at about 30 degrees south, have the same value.</p></li>
<li>Now, we scale our data:<p class="source-code">scaler = StandardScaler()</p><p class="source-code">scaler.fit(X_train)</p><p class="source-code">X_train = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_train),</p><p class="source-code">  columns=feature_cols, index=X_train.index)</p><p class="source-code">X_test = \</p><p class="source-code">  pd.DataFrame(scaler.transform(X_test),</p><p class="source-code">  columns=feature_cols, index=X_test.index)</p><p class="source-code">scaler.fit(y_train)</p><p class="source-code">y_train, y_test = \</p><p class="source-code">  pd.DataFrame(scaler.transform(y_train),</p><p class="source-code">  columns=['avgtemp'], index=y_train.index),\</p><p class="source-code">  pd.DataFrame(scaler.transform(y_test),</p><p class="source-code">  columns=['avgtemp'], index=y_test.index)</p></li>
<li>Next, we <a id="_idIndexMarker585"/> instantiate a scikit-learn <strong class="source-inline">LinearRegression</strong> object and fit a model on the training data. Our target is the annual average temperature (<strong class="source-inline">avgtemp</strong>), while the features are latitude (<strong class="source-inline">latabs</strong>) and <strong class="source-inline">elevation</strong>. The <strong class="source-inline">coef_</strong> attribute gives us the coefficient for each feature:<p class="source-code">lr = LinearRegression()</p><p class="source-code">lr.fit(X_train, y_train)</p><p class="source-code">np.column_stack((lr.coef_.ravel(),</p><p class="source-code">  X_test.columns.values))</p><p class="source-code"><strong class="bold">array([[-0.8538957537748768, 'latabs'],</strong></p><p class="source-code"><strong class="bold">       [-0.3058979822791853, 'elevation']], dtype=object)</strong></p></li>
</ol>
<p>The interpretation of the <strong class="source-inline">latabs</strong> coefficient is that standardized average annual temperature will decline by 0.85 for every one standard deviation increase in latitude. (The <strong class="source-inline">LinearRegression</strong> module does not return p-values, a measure of the statistical significance of the coefficient estimate. You can use <strong class="source-inline">statsmodels</strong> instead to see a full summary of an ordinary least squares model.)</p>
<ol>
<li value="4">Now, we <a id="_idIndexMarker586"/>can get predicted values. Let’s also join the returned NumPy array with the features and the target from the testing data. Then, we can calculate the residuals by subtracting the predicted values from the actual values (<strong class="source-inline">avgtemp</strong>). The residuals do not look bad, though there is a little negative skew and excessive kurtosis:<p class="source-code">pred = lr.predict(X_test)</p><p class="source-code">preddf = pd.DataFrame(pred, columns=['prediction'],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf['resid'] = preddf.avgtemp-preddf.prediction</p><p class="source-code">preddf.resid.agg(['mean','median','skew','kurtosis'])</p><p class="source-code"><strong class="bold">mean             -0.021</strong></p><p class="source-code"><strong class="bold">median           0.032</strong></p><p class="source-code"><strong class="bold">skew              -0.641</strong></p><p class="source-code"><strong class="bold">kurtosis        6.816</strong></p><p class="source-code"><strong class="bold">Name: resid, dtype: float64</strong></p></li>
</ol>
<p>It is worth noting that we will be generating predictions and calculated residuals in this way most of the time we work with regression models in this book. If you feel a little unclear about what we just did in the preceding code block, it may be a good idea to go over it again.</p>
<ol>
<li value="5">We should plot the residuals to get a better sense of how they are distributed.<p class="source-code">Plt.hist(preddf.resid, color="blue")</p><p class="source-code">plt.axvline(preddf.resid.mean(), color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Histogram of Residuals for Temperature Model")</p><p class="source-code">plt.xlabel("Residuals")</p><p class="source-code">plt.ylabel("Frequency")</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="Figure 6.16 – Histogram of residuals for the linear regression model " height="442" src="image/B17978_06_016.jpg" width="572"/>
</div>
</div>
<p class="figure-caption">Figure 6.16 – Histogram of residuals for the linear regression model</p>
<p>This does<a id="_idIndexMarker587"/> not look too bad, but we have more positive residuals, where we have predicted a lower temperature in the testing data than the actual temperature, than negative residuals.</p>
<ol>
<li value="6">Plotting our predictions by the residuals may give us a better sense of what is happening:<p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color="blue")</p><p class="source-code">plt.axhline(0, color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Scatterplot of Predictions and Residuals")</p><p class="source-code">plt.xlabel("Predicted Temperature")</p><p class="source-code">plt.ylabel("Residuals")</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<img alt="Figure 6.17 – Scatterplot of predictions by residuals for the linear regression model " height="446" src="image/B17978_06_017.jpg" width="560"/>
</div>
</div>
<p class="figure-caption">Figure 6.17 – Scatterplot of predictions by residuals for the linear regression model</p>
<p>This does <a id="_idIndexMarker588"/>not look horrible. The residuals hover somewhat randomly around 0. However, predictions between 1 and 2 standard deviations are much more likely to be too low (to have positive residuals) than too high. Above 2, the predictions are always too high (they have negative residuals). This model’s assumption of linearity might not be sound. We should explore a couple of the transformations we discussed in <a href="B17978_04_ePub.xhtml#_idTextAnchor043"><em class="italic">Chapter 4</em></a>, <em class="italic">Encoding, Transforming, and Scaling Features</em>, or try a non-parametric model such as KNN regression.</p>
<p>It is also likely that extreme values are tugging our coefficients around a fair bit. A good next move might be to remove outliers, as we discussed in the <em class="italic">Identifying extreme values and outliers</em> section of <a href="B17978_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Examining the Distribution of Features and Targets</em>. We will not do that here, however.</p>
<ol>
<li value="7">Let’s look<a id="_idIndexMarker589"/> at some evaluation measures. This can easily be done with scikit-learn’s <strong class="source-inline">metrics</strong> library. We can call the same function to get RMSE as MSE. We just need to set the squared parameter to <strong class="source-inline">False</strong>:<p class="source-code">mse = skmet.mean_squared_error(y_test, pred)</p><p class="source-code">mse</p><p class="source-code"><strong class="bold">0.18906346144036693</strong></p><p class="source-code">rmse = skmet.mean_squared_error(y_test, pred, squared=False)</p><p class="source-code">rmse</p><p class="source-code"><strong class="bold">0.4348142838504353</strong></p><p class="source-code">mae = skmet.mean_absolute_error(y_test, pred)</p><p class="source-code">mae</p><p class="source-code"><strong class="bold">0.318307379728143</strong></p><p class="source-code">r2 = skmet.r2_score(y_test, pred)</p><p class="source-code">r2</p><p class="source-code"><strong class="bold">0.8162525715296725</strong></p></li>
</ol>
<p>An MSE of less than 0.2 of a standard deviation and an MAE of less than 0. 3 of a standard deviation look pretty decent, especially for such a sparse model. An R-squared above 80% is also fairly promising.</p>
<ol>
<li value="8">Let’s see<a id="_idIndexMarker590"/> what we get if we use a KNN model instead:<p class="source-code">knn = KNeighborsRegressor(n_neighbors=5)</p><p class="source-code">knn.fit(X_train, y_train)</p><p class="source-code">pred = knn.predict(X_test)</p><p class="source-code">mae = skmet.mean_absolute_error(y_test, pred)</p><p class="source-code">mae</p><p class="source-code"><strong class="bold">0.2501829988751876</strong></p><p class="source-code">r2 = skmet.r2_score(y_test, pred)</p><p class="source-code">r2</p><p class="source-code"><strong class="bold">0.8631113217183314</strong></p></li>
</ol>
<p>This model is actually an improvement in both MAE and R-squared.</p>
<ol>
<li value="9">We should also take a look at the residuals again:<p class="source-code">preddf = pd.DataFrame(pred, columns=['prediction'],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf['resid'] = preddf.avgtemp-preddf.prediction</p><p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color="blue")</p><p class="source-code">plt.axhline(0, color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Scatterplot of Predictions and Residuals with KNN Model")</p><p class="source-code">plt.xlabel("Predicted Temperature")</p><p class="source-code">plt.ylabel("Residuals")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces<a id="_idIndexMarker591"/> the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<img alt="Figure 6.18 – Scatterplot of predictions by residuals for the KNN model " height="443" src="image/B17978_06_018.jpg" width="556"/>
</div>
</div>
<p class="figure-caption">Figure 6.18 – Scatterplot of predictions by residuals for the KNN model</p>
<p>This plot of the residuals looks better as well. There are no parts of the target’s distribution where we are much more likely to over-predict or under-predict.</p>
<p>This section has introduced key measures for evaluating regression models, and how to interpret them. It has also demonstrated how visualizations, particularly of model residuals, can improve that interpretation.</p>
<p>However, we have been limited so far, in both our use of regression and classification measures, by how we have constructed our training and testing DataFrames. What if, for some reason, the testing data is unusual in some way? More generally, what is our basis for concluding that our evaluation measures are accurate? We can be more confident in these measures if we use K-fold cross-validation, which we will cover in the next section.</p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/>Using K-fold cross-validation</h1>
<p>So far, we<a id="_idIndexMarker592"/> have held back 30% of our data for validation. This is not a bad strategy. It prevents us from peeking ahead to the testing data as we train our model. However, this approach does not take full advantage of all the available data, either for training or for testing. If we use K-fold cross-validation instead, we can use all of our data while also avoiding data leakage. Perhaps that seems too good to be true. But it’s not because of a neat little trick.</p>
<p><strong class="bold">K-fold cross-validation</strong> trains <a id="_idIndexMarker593"/>our model on all but one of the K folds, or parts, leaving one out for testing. This is repeated <em class="italic">k</em> times, each time excluding a different fold for testing. Performance metrics are then based on the average scores across the K folds.</p>
<p>Before we start, though, we need to think again about the possibility of data leakage. If we scale all of the data that we will use to train our model and then split it up into folds, we will be using information from all the folds in our training. To avoid this, we need to do the scaling, as well as any other Preprocessing, on just the training folds for each iteration. While we could do this manually, scikit-learn’s <strong class="source-inline">pipeline</strong> library can do much of this work for us. We will go over how to use pipelines for cross-validation in this section.</p>
<p>Let’s try evaluating the two models we specified in the previous section using K-fold cross-validation. While we are at it, let’s also see how well a random forest regressor may work:</p>
<ol>
<li value="1">In addition to the libraries we have worked with so far, we need scikit-learn’s<em class="italic"> </em><strong class="source-inline">make_pipeline</strong>, <strong class="source-inline">cross_validate</strong>, and <strong class="source-inline">Kfold</strong> libraries: <p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.neighbors import KNeighborsRegressor</p><p class="source-code">from sklearn.ensemble import RandomForestRegressor</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.model_selection import cross_validate</p><p class="source-code">from sklearn.model_selection import KFold</p></li>
<li>We <a id="_idIndexMarker594"/>load the land temperatures data again and create training and testing DataFrames. We still want to leave some data out for final validation, but this time, we will only leave out 10%. We will do both training and testing with the remaining 90%:<p class="source-code">landtemps = pd.read_csv("data/landtemps2019avgs.csv")</p><p class="source-code">feature_cols = ['latabs','elevation']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(landtemps[feature_cols],\</p><p class="source-code">  landtemps[['avgtemp']],test_size=0.1,random_state=0)</p></li>
<li>Now, we create a <strong class="source-inline">KFold</strong> object and indicate that we want five folds and for the data to be shuffled (shuffling the data is a good idea if it is not already sorted randomly):<p class="source-code">kf = Kfold(n_splits=5, shuffle=True, random_state=0)</p></li>
<li>Next, we define a function to create a pipeline. The function then runs <strong class="source-inline">cross_validate</strong>, which takes the pipeline and the <strong class="source-inline">KFold</strong> object we created earlier:<p class="source-code">def getscores(model):</p><p class="source-code">  pipeline = make_pipeline(StandardScaler(), model)</p><p class="source-code">  scores = cross_validate(pipeline, X=X_train, </p><p class="source-code">    y=y_train, cv=kf, scoring=['r2'], n_jobs=1)</p><p class="source-code">  scorelist.append(dict(model=str(model),</p><p class="source-code">    fit_time=scores['fit_time'].mean(),</p><p class="source-code">    r2=scores['test_r2'].mean()))</p></li>
<li>Now, we <a id="_idIndexMarker595"/>are ready to call the <strong class="source-inline">getscores</strong> function for the linear regression, random forest regression, and KNN regression models:<p class="source-code">scorelist = []</p><p class="source-code">getscores(LinearRegression())</p><p class="source-code">getscores(RandomForestRegressor(max_depth=2))</p><p class="source-code">getscores(KNeighborsRegressor(n_neighbors=5))</p></li>
<li>We can print the <strong class="source-inline">scorelist</strong> list to see our results:<p class="source-code">scorelist</p><p class="source-code">[{'model': 'LinearRegression()',</p><p class="source-code">  'fit_time': 0.004968833923339844,</p><p class="source-code">  'r2': 0.8181125031214872},</p><p class="source-code"> {'model': 'RandomForestRegressor(max_depth=2)',</p><p class="source-code">  'fit_time': 0.28124608993530276,</p><p class="source-code">  'r2': 0.7122492698889024},</p><p class="source-code"> {'model': 'KNeighborsRegressor()',</p><p class="source-code">  'fit_time': 0.006945991516113281,</p><p class="source-code">  'r2': 0.8686733636724104}]</p></li>
</ol>
<p>The KNN regressor model performs better than either the linear regression or random forest regression model, based on R-squared. The random forest regressor also has a significant disadvantage in that it has a much longer fit time.</p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor088"/>Preprocessing data with pipelines</h1>
<p>We just <a id="_idIndexMarker596"/>scratched the surface of what we can do with scikit-learn pipelines in the previous section. We often need to fold all of our Preprocessing <a id="_idIndexMarker597"/>and feature engineering into a pipeline, including scaling, encoding, and handling outliers and missing values. This can be complicated as different features may need to be handled differently. We may need to impute the median for missing values with numeric features and the most frequent value for categorical features. We may also need to transform our target variable. We will explore how to do that in this section.</p>
<p>Follow these steps:</p>
<ol>
<li value="1">We will start by loading the libraries we have already worked with in this chapter. Then, we will add the <strong class="source-inline">ColumnTransformer</strong> and <strong class="source-inline">TransformedTargetRegressor</strong> classes. We will use those classes to transform our features and target, respectively:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from feature_engine.encoding import OneHotEncoder</p><p class="source-code">from sklearn.impute import KNNImputer</p><p class="source-code">from sklearn.model_selection import cross_validate, KFold</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.compose import TransformedTargetRegressor</p></li>
<li>The column transformer is quite flexible. We can even use it with the Preprocessing functions that we have defined ourselves. The following code block imports<a id="_idIndexMarker598"/> the <strong class="source-inline">OutlierTrans</strong> class from the <strong class="source-inline">preprocfunc</strong> module in the <strong class="source-inline">helperfunctions</strong> subfolder:<p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>The <strong class="source-inline">OutlierTrans</strong> class<a id="_idIndexMarker599"/> identifies extreme values by distance from the interquartile range. This is a technique we demonstrated in <a href="B17978_03_ePub.xhtml#_idTextAnchor034"><em class="italic">Chapter 3</em></a>, <em class="italic">Identifying and Fixing Missing Values</em>. </li>
</ol>
<p>To work in a scikit-learn pipeline, our class has to have fit and transform methods. We also need to inherit the <strong class="source-inline">BaseEstimator</strong> and <strong class="source-inline">TransformerMixin</strong> classes.</p>
<p>In this class, almost all of the action happens in the <strong class="source-inline">transform</strong> method. Any value that is more than 1.5 times the interquartile range above the third quartile or below the first quartile is assigned missing:</p>
<p class="source-code">class OutlierTrans(BaseEstimator,TransformerMixin):</p>
<p class="source-code">  def __init__(self,threshold=1.5):</p>
<p class="source-code">    self.threshold = threshold</p>
<p class="source-code">  </p>
<p class="source-code">  def fit(self,X,y=None):</p>
<p class="source-code">    return self</p>
<p class="source-code">  </p>
<p class="source-code">  def transform(self,X,y=None):</p>
<p class="source-code">    Xnew = X.copy()</p>
<p class="source-code">    for col in Xnew.columns:</p>
<p class="source-code">      thirdq, firstq = Xnew[col].quantile(0.75),\</p>
<p class="source-code">        Xnew[col].quantile(0.25)</p>
<p class="source-code">      inlierrange = self.threshold*(thirdq-firstq)</p>
<p class="source-code">      outlierhigh, outlierlow = inlierrange+thirdq,\</p>
<p class="source-code">        firstq-inlierrange</p>
<p class="source-code">      Xnew.loc[(Xnew[col]&gt;outlierhigh) | \</p>
<p class="source-code">        (Xnew[col]&lt;outlierlow),col] = np.nan</p>
<p class="source-code">    return Xnew.values                                 </p>
<p>Our <strong class="source-inline">OutlierTrans</strong> class <a id="_idIndexMarker600"/>can be used later in our pipeline in the same way we used <strong class="source-inline">StandardScaler</strong> in the previous section. We will do that later.</p>
<ol>
<li value="4">Now, we <a id="_idIndexMarker601"/>are ready to load the data that needs to be processed. We will work with the NLS weekly wage data in this section. Weekly wages will be our target, and we will use high school GPA, mother’s and father’s highest grade completed, parent income, gender, and whether the individual completed a bachelor’s degree as features.</li>
</ol>
<p>We will create lists of features to handle in different ways here. This will be helpful later when we instruct our pipeline to carry out different operations on numerical, categorical, and binary features:</p>
<p class="source-code">nls97wages = pd.read_csv("data/nls97wagesb.csv")</p>
<p class="source-code">nls97wages.set_index("personid", inplace=True)</p>
<p class="source-code">nls97wages.dropna(subset=['wageincome'], inplace=True)</p>
<p class="source-code">nls97wages.loc[nls97wages.motherhighgrade==95,</p>
<p class="source-code">  'motherhighgrade'] = np.nan</p>
<p class="source-code">nls97wages.loc[nls97wages.fatherhighgrade==95,</p>
<p class="source-code">  'fatherhighgrade'] = np.nan</p>
<p class="source-code">num_cols = ['gpascience','gpaenglish','gpamath','gpaoverall',</p>
<p class="source-code">  'motherhighgrade','fatherhighgrade','parentincome']</p>
<p class="source-code">cat_cols = ['gender']</p>
<p class="source-code">bin_cols = ['completedba']</p>
<p class="source-code">target = nls97wages[['wageincome']]</p>
<p class="source-code">features = nls97wages[num_cols + cat_cols + bin_cols]</p>
<p class="source-code">X_train, X_test, y_train, y_test =  \</p>
<p class="source-code">  train_test_split(features,\</p>
<p class="source-code">  target, test_size=0.2, random_state=0)</p>
<ol>
<li value="5">Let’s look<a id="_idIndexMarker602"/> at some descriptive statistics. Some<a id="_idIndexMarker603"/> variables have over a thousand missing values (<strong class="source-inline">gpascience</strong>, <strong class="source-inline">gpaenglish</strong>, <strong class="source-inline">gpamath</strong>, <strong class="source-inline">gpaoverall</strong>, and <strong class="source-inline">parentincome</strong>):<p class="source-code">nls97wages[['wageincome'] + num_cols].agg(['count','min','median','max']).T</p><p class="source-code"><strong class="bold">                 count    min      median    max</strong></p><p class="source-code"><strong class="bold">wageincome       5,091    0        40,000    235,884</strong></p><p class="source-code"><strong class="bold">gpascience       3,521    0        284       424</strong></p><p class="source-code"><strong class="bold">gpaenglish       3,558    0        288       418</strong></p><p class="source-code"><strong class="bold">gpamath          3,549    0        280       419</strong></p><p class="source-code"><strong class="bold">gpaoverall       3,653    42       292       411</strong></p><p class="source-code"><strong class="bold">motherhighgrade  4,734    1        12        20</strong></p><p class="source-code"><strong class="bold">fatherhighgrade  4,173    1        12        29</strong></p><p class="source-code"><strong class="bold">parentincome     3,803   -48,100   40,045    246,474</strong></p></li>
<li>Now, we can set up a column transformer. First, we will create pipelines for handling numerical data (<strong class="source-inline">standtrans</strong>), categorical data, and binary data.</li>
</ol>
<p>For the numerical data, we want to assign outlier values as missing. Here, we will pass a value of <strong class="source-inline">2</strong> to the threshold parameter of <strong class="source-inline">OutlierTrans</strong>, indicating that we want values two times the interquartile range above or below that range to be <a id="_idIndexMarker604"/>set to missing. Recall that the default is 1.5, so we are being somewhat more conservative.</p>
<p>Then, we <a id="_idIndexMarker605"/>will create a <strong class="source-inline">ColumnTransformer</strong> object, passing to it the three pipelines we just created, and indicating which features to use with which pipeline:</p>
<p class="source-code">standtrans = make_pipeline(OutlierTrans(2),</p>
<p class="source-code">  StandardScaler())</p>
<p class="source-code">cattrans = make_pipeline(SimpleImputer(strategy="most_frequent"),</p>
<p class="source-code">  OneHotEncoder(drop_last=True))</p>
<p class="source-code">bintrans = make_pipeline(SimpleImputer(strategy="most_frequent"))</p>
<p class="source-code">coltrans = ColumnTransformer(</p>
<p class="source-code">  transformers=[</p>
<p class="source-code">    ("stand", standtrans, num_cols),</p>
<p class="source-code">    ("cat", cattrans, ['gender']),</p>
<p class="source-code">    ("bin", bintrans, ['completedba'])</p>
<p class="source-code">  ]</p>
<p class="source-code">)</p>
<ol>
<li value="7">Now, we can add the column transformer to a pipeline that also includes the linear model that we would like to run. We will add KNN imputation to the pipeline to <a id="_idIndexMarker606"/>handle missing values.</li>
</ol>
<p>We also <a id="_idIndexMarker607"/>need to scale the target, which cannot be done in our pipeline. We will use scikit-learn’s <strong class="source-inline">TransformedTargetRegressor</strong> for that. We will pass the pipeline we just created to the target regressor’s <strong class="source-inline">regressor</strong> parameter:</p>
<p class="source-code">lr = LinearRegression()</p>
<p class="source-code">pipe1 = make_pipeline(coltrans,</p>
<p class="source-code">  KNNImputer(n_neighbors=5), lr)</p>
<p class="source-code">ttr=TransformedTargetRegressor(regressor=pipe1,</p>
<p class="source-code">  transformer=StandardScaler())</p>
<ol>
<li value="8">Let’s do K-fold cross validation using this pipeline. We can pass our pipeline, via the target regressor, <strong class="source-inline">ttr</strong>, to the <strong class="source-inline">cross_validate</strong> function:<p class="source-code">kf = KFold(n_splits=10, shuffle=True, random_state=0)</p><p class="source-code">scores = cross_validate(ttr, X=X_train, y=y_train,</p><p class="source-code">  cv=kf, scoring=('r2', 'neg_mean_absolute_error'),</p><p class="source-code">  n_jobs=1)</p><p class="source-code">print("Mean Absolute Error: %.2f, R-squared: %.2f" % </p><p class="source-code">  (scores['test_neg_mean_absolute_error'].mean(),</p><p class="source-code">  scores['test_r2'].mean()))</p><p class="source-code">Mean Absolute Error: -23781.32, R-squared: 0.20</p></li>
</ol>
<p>These scores<a id="_idIndexMarker608"/> are not very good, though that was not quite the<a id="_idIndexMarker609"/> point of this exercise. The key takeaway here is that we typically want to fold most of the Preprocessing we will do into a pipeline. This is the best way to avoid data leakage. The column transformer is an extremely flexible tool, allowing us to apply different transformations to different features. </p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>Summary</h1>
<p>This chapter introduced key model evaluation measures and techniques so that they will be familiar when we make extensive use of them, and extend them, in the remaining chapters of this book. We examined the very different approaches to evaluation for classification and regression models. We also explored how to use visualizations to improve our analysis of our predictions. Finally, we used pipelines and cross-validation to get reliable estimates of model performance.</p>
<p>I hope this chapter also gave you a chance to get used to the general approach of this book going forward. Although a large number of algorithms will be discussed in the remaining chapters, we will continue to surface the Preprocessing issues we have discussed in the first few chapters. We will discuss the core concepts of each algorithm, of course. But, in a true <em class="italic">hands-on</em> fashion, we will also deal with the messiness of real-world data. Each chapter will go from relatively raw data to feature engineering to model specification and model evaluation, relying heavily on scikit-learn’s pipelines to pull it all together.</p>
<p>We will discuss regression algorithms in the next few chapters – those algorithms that allow us to model a continuous target. We will explore some of the most popular regression algorithms – linear regression, support vector regression, K-nearest neighbors regression, and decision tree regression. We will also consider making modifications to regression models that address underfitting and overfitting, including nonlinear transformations and regularization.</p>
</div>
</div>
</body></html>