<html><head></head><body>
		<div id="_idContainer071">
			<h1 id="_idParaDest-83"><a id="_idTextAnchor093"/>Chapter 5: Model Evaluation and Packaging</h1>
			<p><a id="_idTextAnchor094"/>In this chapter, we will learn in detail about ML model evaluation and interpretability metrics. This will enable us to have a comprehensive understanding of the performance of ML models after training them. We will also learn how to package the models and deploy them for further use (such as in production systems). We will study in detail how we evaluated and packaged the models in the previous chapter and explore new ways of evaluating and explaining the models to ensure a comprehensive understanding of the trained models and their potential usability in production systems.</p>
			<p><a id="_idTextAnchor095"/><a id="_idTextAnchor096"/>We begin this chapter by learning various ways of measuring, evaluating, and interpreting the model's performance. We look at multiple ways of testing the models for production and packaging ML models for production and inference. An in-depth study of the ML models' evaluation will be carried out as you will be presented with a framework to assess any kind of ML model and package it for production. Get ready to build a solid foundation in terms of evaluation and get ML models ready for production. For this, we are going to cover the following main topics in this chapter:</p>
			<ul>
				<li><a id="_idTextAnchor097"/><a id="_idTextAnchor098"/>Model evaluation and interpretability metrics </li>
				<li>Production testing methods</li>
				<li>Why package ML models?</li>
				<li>How to package ML models</li>
				<li>Inference ready models</li>
			</ul>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor099"/>Model evaluation and interpretability metrics</h1>
			<p>Acquiring <a id="_idIndexMarker294"/>data and training ML models is a good start toward creating business value. After <a id="_idIndexMarker295"/>training models, it is vital to measure the models' performance and understand why and how a model is predicting or performing in a certain way. Hence, model evaluation and interpretability are essential parts of the MLOps workflow. They enable us to understand and validate the ML models to determine the business value they will produce. As there are several types of ML models, there are numerous evaluation techniques as well. </p>
			<p>Looking back at <a href="B16572_02_Final_JM_ePub.xhtml#_idTextAnchor028"><em class="italic">Chapter 2</em></a>, <em class="italic">Characterizing Your Machine Learning Problem</em>, where we studied various types of models categorized as learning models, hybrid models, statistical <a id="_idIndexMarker296"/>models, and <strong class="bold">HITL</strong> (<strong class="bold">Human-in-the-loop</strong>) models, we will now discuss different metrics to evaluate these models. Here are some of <a id="_idIndexMarker297"/>the key model evaluation and <a id="_idIndexMarker298"/>interpretability techniques as shown in <em class="italic">Figure 5.1</em>. These have become standard in research and industry for evaluating model performance and justifying model performance:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B16572_05_01.jpg" alt="Figure 5.1 – Model evaluation and interpretation taxonomy &#13;&#10;(The techniques in this taxonomy can be applied to almost any business problem when carefully navigated, selected, and executed.)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Model evaluation and interpretation taxonomy </p>
			<p class="figure-caption">(The techniques in this taxonomy can be applied to almost any business problem when carefully navigated, selected, and executed.)</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor100"/>Learning models' metrics</h2>
			<p><strong class="bold">Learning models</strong> are of two types – supervised learning (supervised learning models or algorithms are trained <a id="_idIndexMarker299"/>based on labeled data) and unsupervised learning (unsupervised learning models or algorithms can learn from unlabeled data). </p>
			<p>As we have studied in previous chapters, examples of supervised learning algorithms include classification (random forest, support vector machine, and so on) and regression (linear regression, logistic regression, and so on) algorithms. On the other hand, examples of unsupervised learning include clustering (k-means, DBSCAN, Gaussian mixture models, and more) and dimensionality reduction (PCA, random forest, forward and backward feature elimination, and so on) algorithms. In order to measure these algorithms efficiently, the following are examples of some commonly used and efficient metrics.</p>
			<h3>Supervised learning models</h3>
			<p>Supervised <a id="_idIndexMarker300"/>learning models train on labeled data. In the training data, the outcome of the input is marked or known. Hence, a model is trained to learn to predict the outcome when given an input based on the labeled data. After training the model, it is important to gauge the model's potential and performance. Here are some metrics to gauge supervised models.</p>
			<h4>Cross-validation</h4>
			<p>Evaluating an <a id="_idIndexMarker301"/>ML model is vital to understanding its <a id="_idIndexMarker302"/>behaviour and this can be tricky. Normally, the dataset is split into two sub-sets: the training and the test sets. First, the training set is used to train the model, and then the test set is used to test the model. After this, the model's performance is evaluated to determine the error using metrics such as the accuracy percentage of the model on test data. </p>
			<p>This methodology is not reliable and comprehensive because accuracy for one test set can be different from another test set. To avoid this problem, cross-validation provides a solution by fragmenting or splitting the dataset into folds and ensuring that each fold is used as a test set at some point, as shown in <em class="italic">Figure 5.2</em>: </p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B16572_05_02.jpg" alt="Figure 5.2 – K-Fold cross-validation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – K-Fold cross-validation</p>
			<p>There are multiple cross-validation methods, including stratified cross-validation, leave-one-out cross-validation, and K-fold cross-validation. K-fold cross-validation is widely <a id="_idIndexMarker303"/>used and is worth noting as this technique involves splitting the dataset into k folds/fragments and then using each fold as a test set in successive iterations. This process is useful because each iteration has a unique test set on which accuracy is measured. Then, the accuracy for each iteration is used to find the average test results (calculated by simply taking the average of all test results). </p>
			<p>Average accuracy acquired by cross-validation is a more reliable and comprehensive metric than the conventional accuracy <a id="_idIndexMarker304"/>measure. For example, in <em class="italic">Figure 5.2</em>, we can <a id="_idIndexMarker305"/>see five iterations. Each of these iterations has a unique test set, and upon testing accuracy for each iteration and averaging all accuracies, we get an average accuracy for the model using K-fold cross-validation. It is worth noting that K-fold is not a good choice if you have a very large training dataset or if the model requires a large amount of time, CPU, and/or GPU processing for running.</p>
			<h4>Precision</h4>
			<p>When a classifier is <a id="_idIndexMarker306"/>trained, precision can be a <a id="_idIndexMarker307"/>vital metric in quantifying positive class predictions made by the classifier that are actually true and belong to the positive class. Precision quantifies the number of correct positive predictions.</p>
			<p>For example, let's say we have trained a classifier to predict cats and dogs from images. Upon inferring the trained model on the test images, the model is used for predicting/detecting dogs from images (in other words, dogs being the positive class). Precision, in this case, quantifies the number of correct dog predictions (positive predictions).</p>
			<p>Precision is calculated as the ratio of correctly predicted positive examples to the total number of predicted positive examples.</p>
			<p><em class="italic">Precision = TruePositives / (TruePositives + FalsePositives)</em></p>
			<p><strong class="bold">Precision</strong> focuses on minimizing false positives. High precision ranges from 0 to 1, and it relates to a low false positive rate. The higher the precision, the better it is; for example, an image classifier <a id="_idIndexMarker308"/>model that predicts whether <a id="_idIndexMarker309"/>a cancer patient requires chemotherapy treatment. If the model predicts that a patient should be submitted for chemotherapy when it is not really necessary, this can be very harmful as the effects of chemotherapy can be detrimental when not required. This case is a dangerous false positive. A high-precision score will result in fewer false positives, while having a low-precision score will result in a high number of false positives. Hence, regarding the chemotherapy treatment the prediction model should have a high-precision score.</p>
			<h4>Recall</h4>
			<p>When a classifier is <a id="_idIndexMarker310"/>trained, recall can be used to quantify the <a id="_idIndexMarker311"/>positive class predictions established from the total number of positive examples in the dataset. Recall measures the number of correct positive predictions made out of the total number of positive predictions that could have been made. Recall provides evidence of missed positive predictions, unlike precision, which only tells us the correct positive predictions out of the total number of positive predictions.</p>
			<p>For example, take the same example discussed earlier, where we trained a classifier to predict cats and dogs from images. Upon inferring the trained model on the test images, the model is used for predicting/detecting dogs from images (in other words, dogs being the positive class). Recall, in this case, quantifies the number of missed dog predictions (positive predictions).</p>
			<p>In this fashion, recall provides an empirical indication of the coverage of the positive class.</p>
			<p><em class="italic">Recall = TruePositives / (TruePositives + FalseNegatives)</em></p>
			<p><strong class="bold">Recall</strong> focuses on minimizing false negatives. High recall relates to a low false negative rate. The higher the recall, the better it is. For example, a model that analyzes the profile data from a passenger in an airport tries to predict whether that passenger is a potential terrorist. In this case, it is more secure to have false positives than false negatives. If the models <a id="_idIndexMarker312"/>predict that an innocent <a id="_idIndexMarker313"/>person is a terrorist, this could be checked following a more in-depth investigation. But if a terrorist passes, a number of lives could be in danger. In this case, it is more secure to have false negatives than false positives as false negatives can be checked with the help of an in-depth investigation. Recall should be high to avoid false negatives. In this case, having a high recall score is prioritized over high precision.</p>
			<h4>F-score</h4>
			<p>In a case where <a id="_idIndexMarker314"/>we need to avoid both high false positives <a id="_idIndexMarker315"/>and high false negatives, f-score is a useful measure for reaching this state. F-measure provides a way to consolidate both precision and recall into single metrics that reflect both properties.</p>
			<p>Neither precision nor recall portrays the whole story.</p>
			<p>We can have the best precision with terrible recall, or alternatively, F-measure expresses both precision and recall. It is measured according to the following formula:</p>
			<p><em class="italic">F-Measure = (2 * Precision * Recall) / (Precision + Recall)</em></p>
			<p>The harmonic mean of your precision and recall is the F-measure. In most cases, you must choose between precision and recall. The harmonic mean rapidly decreases if you optimize your classifier to favor one and disfavor the other. When both precision and recall are similar, it is at its best; for example, a model that predicts cancer early by taking as input a patient's images and blood exams. In this real scenario, this could bring a lot of unnecessary costs to a hospital and possible harm to a patient's health if the model outputs a high number of false positives. On the other hand, if the model fails to detect genuine cancer patients, a number of lives would be in danger. In such cases, we need to avoid both high false positives and high false negatives and here, the f-score is a useful measure for avoiding high false positives and false negatives. F-score measures between 0 and 1. The higher the f-score, the better it is. We can expect a smaller number of false positives and false negatives with a high f-score.</p>
			<h4>Confusion matrix </h4>
			<p>The confusion matrix is a metric that <a id="_idIndexMarker316"/>reports the performance of classification models on a set of test data samples for which prediction <a id="_idIndexMarker317"/>values are pre-known. It is a metric in matrix form where a confusion matrix is an N X N matrix, and N is the number of classes being predicted. For example, let's say we have two classes to predict (binary classification), then N=2, and, as a result, we will have a 2 X 2 matrix, like the one shown here in <em class="italic">Figure 5.3</em>:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B16572_05_03.jpg" alt="Figure 5.3 – Confusion matrix for binary classification&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Confusion matrix for binary classification</p>
			<p><em class="italic">Figure 5.3</em> is an example of a confusion matrix for binary classification between diabetic and non diabetic patients. There are 181 test data samples on which predictions are made to classify patient data samples into diabetic and non diabetic categories. Using a confusion matrix, you can get critical insights to interpret the model's performance. For instance, at a glance, you will know how many predictions made are actually true and how many are false positives. Such insights are invaluable for interpreting the model's performance in many cases. Here are <a id="_idIndexMarker318"/>what these terms mean in the context of the confusion matrix:</p>
			<ul>
				<li><strong class="bold">True positives</strong> (<strong class="bold">TP</strong>): These are cases predicted to be <strong class="bold">yes</strong> and are actually <strong class="bold">yes</strong> as per test data samples.</li>
				<li><strong class="bold">True negatives</strong> (<strong class="bold">TN</strong>): These are the cases predicted to be <strong class="bold">no</strong> and these cases are actually <strong class="bold">no</strong> as per test data samples.</li>
				<li><strong class="bold">False positives</strong> (<strong class="bold">FP</strong>): The model predicted <strong class="bold">yes</strong>, but they are <strong class="bold">no</strong> as per test data samples. This type of <a id="_idIndexMarker319"/>error is known as a <strong class="bold">Type I error</strong>.</li>
				<li><strong class="bold">False negatives</strong> (<strong class="bold">FN</strong>): The model predicted <strong class="bold">no</strong>, but they are <strong class="bold">yes</strong> as per test data samples. This type of <a id="_idIndexMarker320"/>error is <a id="_idIndexMarker321"/>known as a <strong class="bold">Type II error</strong>.</li>
			</ul>
			<p> In <em class="italic">Figure 5.3</em>, the following applies: </p>
			<ul>
				<li>The <em class="italic">x</em>-axis represents the predictions made by the ML models.</li>
				<li>The <em class="italic">y</em>-axis represents the actual labels.</li>
				<li>The first and fourth boxes in the matrix (diagonal boxes) depict the correctly predicted images.</li>
				<li>The second and third boxes in the matrix represent false predictions. </li>
				<li>In the first box, (<strong class="bold">Non Diabetic</strong> x <strong class="bold">Non Diabetic</strong>), 108 data samples (<strong class="bold">True negatives</strong> – <strong class="bold">TN</strong>) were predicted to be <strong class="bold">Non Diabetic</strong> (correct predictions).</li>
				<li>In the fourth box, (<strong class="bold">Diabetes</strong> x <strong class="bold">Diabetes</strong>), 36 data samples (<strong class="bold">True positives</strong> – <strong class="bold">TP</strong>) were predicted correctly. </li>
				<li>The rest of the images in the second box (<strong class="bold">Cats</strong> x <strong class="bold">Dogs</strong>) 11 images are false positives. </li>
				<li>The third box (<strong class="bold">Dogs</strong> x <strong class="bold">Cats</strong>), which has 26 images, contains false negatives. </li>
			</ul>
			<p>The <a id="_idIndexMarker322"/>confusion matrix can provide a big picture of the predictions made on the test data samples and such insights are <a id="_idIndexMarker323"/>significant in terms of interpreting the performance of the model. The confusion matrix is the <em class="italic">de facto</em> error analysis metric for classification problems, as most other metrics are derived from this one.</p>
			<h4>AUC-ROC</h4>
			<p>A different <a id="_idIndexMarker324"/>perspective for observing model performance can enable us to interpret model <a id="_idIndexMarker325"/>performance and fine-tune it to derive better results. ROC and AUC curves can enable such insights. Let's <a id="_idIndexMarker326"/>see how the <strong class="bold">Receiver Operating Characteristic</strong> (<strong class="bold">ROC</strong>) curve can enable us to interpret model performance. The ROC curve is a <a id="_idIndexMarker327"/>graph exhibiting the performance of a classification model at all classification thresholds. The <a id="_idIndexMarker328"/>graph uses two parameters <a id="_idIndexMarker329"/>to depict the model's performance: <strong class="bold">True Positive Rate</strong> (<strong class="bold">TPR</strong>=<em class="italic">TP/TP+FN</em>) and <strong class="bold">False Positive Rate</strong> (<strong class="bold">FPR</strong>=<em class="italic">FPFP+TN</em>).</p>
			<p>The following diagram shows a typical ROC curve:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B16572_05_04.jpg" alt="Figure 5.4 – ROC-AUC curve&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – ROC-AUC curve</p>
			<p>An ROC curve depicts the TPR versus FPR for different thresholds for classification. Lowering the threshold for classification enables more items to be classified as positive, which in turn increases both false positives and <a id="_idIndexMarker330"/>true positives. The <strong class="bold">Area Under the Curve</strong> (<strong class="bold">AUC</strong>) is a metric used to quantify the effectiveness or ability of a classifier to distinguish between classes and is used to summarize the ROC curve.</p>
			<p>The AUC value varies from <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>, and the classifier is able to correctly distinguish between all the positive and negative class points if the AUC value is <strong class="source-inline">1</strong>, and the classifier is unable to correctly distinguish between all the positive and negative class points if the AUC value is <strong class="source-inline">0</strong>. When the AUC value is <strong class="source-inline">0.5</strong> (without manually setting a threshold), then this is a random classifier. </p>
			<p>AUC helps us to rank predictions according to their accuracy, but it does not give us absolute values. Hence <a id="_idIndexMarker331"/>it is scale-independent. Additionally, AUC is independent of the classification threshold. The <a id="_idIndexMarker332"/>classification threshold chosen does not matter when using AUC as AUC estimates the quality of the model's predictions irrespective of what classification threshold is chosen.</p>
			<h4>The Matthew's Correlation Coefficient</h4>
			<p>Brian Matthews <a id="_idIndexMarker333"/>developed the <strong class="bold">Matthews correlation coefficient</strong> (<strong class="bold">MCC</strong>) in 1975 as a method for model <a id="_idIndexMarker334"/>evaluation. It calculates the discrepancies between real and expected values. It is an extension of confusion matrix results to measure the inefficiency of a classifier. TP, TN, FP, and FN are the four entries in a confusion matrix. These entries are factored into the coefficient: </p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/Formula_01.jpg" alt=""/>
				</div>
			</div>
			<p>This measure results in high scores only when a prediction returns good rates for all these four categories. The MCC score ranges from <strong class="source-inline">-1</strong> to <strong class="source-inline">+1</strong>:</p>
			<ul>
				<li><strong class="source-inline">1</strong> is the best agreement between actuals and predictions.</li>
				<li>When the score is <strong class="source-inline">0</strong>, this means there is no agreement at all between actuals and predictions. The prediction is random with respect to actuals.</li>
			</ul>
			<p>For example, an MCC score of <strong class="source-inline">0.12</strong> suggests that the classifier is very random. If it is <strong class="source-inline">0.93</strong>, this <a id="_idIndexMarker335"/>suggests that the classifier <a id="_idIndexMarker336"/>is good. MCC is a useful metric for helping to measure the ineffectiveness of a classifier.</p>
			<h3>Unsupervised learning models</h3>
			<p>Unsupervised learning <a id="_idIndexMarker337"/>models or algorithms can learn from unlabeled data. Unsupervised learning can be used to mine insights and identify patterns from unlabeled data. Unsupervised algorithms are widely used for clustering or anomaly detection without relying on any labels. Here are some metrics for gauging the performance of unsupervised learning algorithms.</p>
			<h4>The Rand index</h4>
			<p>The Rand index is <a id="_idIndexMarker338"/>a metric for evaluating the quality of the <a id="_idIndexMarker339"/>clustering technique. It depicts the degree of similarity between the clusters. The Rand index measures the percentage of correct decisions. Decisions assign a pair of data points (for example, documents) to the same cluster. </p>
			<p>If <strong class="source-inline">N</strong> data points exist, the total number of <em class="italic">decisions = N(N-1)/2</em>, which denotes the pair of data points involved in the decision.</p>
			<p><em class="italic">Rand index = TP + TN / TP + FP + FN + TN</em></p>
			<h4>Purity</h4>
			<p><strong class="bold">Purity</strong> is an external <a id="_idIndexMarker340"/>evaluation metric for cluster <a id="_idIndexMarker341"/>quality. To calculate purity, the clusters are labeled according to the most common class in the cluster and then the accuracy of this cluster assignment is measured by calculating the number of correctly assigned data points and dividing by N (total number of data points clustered). Good clustering has a purity value close to <strong class="source-inline">1</strong>, and bad clustering has a purity value close to <strong class="source-inline">0</strong>. <em class="italic">Figure 5.5</em> is a visual representation of an example of calculating <em class="italic">purity</em>, as explained below:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B16572_05_05.jpg" alt="Figure 5.5 – Clusters after clustering&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Clusters after clustering</p>
			<p>Purity is an external evaluation criterion regarding cluster quality. In <em class="italic">Figure 5.5</em>, the majority class and the <a id="_idIndexMarker342"/>number of members of the <a id="_idIndexMarker343"/>majority class for the three clusters are as follows: green drops x 5 (cluster 1), red dots x 5 (cluster 2), and crosses x 4 (cluster 3). Hence, purity is (1/17) x (5 + 5 + 3) = ~0.76.</p>
			<h4>The Silhouette coefficient</h4>
			<p>For clustering algorithms, determining the <a id="_idIndexMarker344"/>quality of clusters is important. To determine the quality or goodness of <a id="_idIndexMarker345"/>the clusters, the silhouette score, or silhouette coefficient, is used as a metric. Its value ranges from <strong class="source-inline">-1</strong> to <strong class="source-inline">1</strong>. When clusters are clearly distinguishable or well apart from one another, then the silhouette score is <strong class="source-inline">1</strong>. On the contrary, <strong class="source-inline">-1</strong> means clusters are wrongly allocated, and <strong class="source-inline">0</strong> means clusters are indifferent from one another. This is how the silhouette score is calculated: </p>
			<p><em class="italic">Silhouette Score = (b-a)/max(a,b)</em></p>
			<p><strong class="source-inline">a</strong> = the average distance between each point within a cluster (average intra-cluster distance).</p>
			<p><strong class="source-inline">b</strong> = the average distance between all clusters (average inter-cluster distance).</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor101"/>Hybrid models' metrics</h2>
			<p>There have been <a id="_idIndexMarker346"/>rapid developments in ML by combining conventional methods to develop hybrid methods to solve diverse business and research problems. Hybrid models include semi-supervised, self-supervised, multi-instance, multi-task, reinforcement, ensemble, transfer, and federated learning models. To evaluate and validate these models, a range of metrics are used depending on the use case and model <a id="_idIndexMarker347"/>type. It is good to know these metrics to be able to use the right metrics as per the model you will develop and evaluate in the future. Here are the metrics for evaluating hybrid models:</p>
			<h3>Human versus machine test</h3>
			<p>Zeal to reach human-level performance is quite common while training and testing ML and deep <a id="_idIndexMarker348"/>learning models. In <a id="_idIndexMarker349"/>order to validate the models and conclude that the <a id="_idIndexMarker350"/>models have reached or surpassed human-level performance, human versus machine experiments are performed on tasks. The same task is implemented using an ML model and the human performance is evaluated against the ML model's performance. There are various metrics for evaluating human versus machine performance according to the context and tasks. Some examples are mentioned here:</p>
			<ul>
				<li><strong class="bold">Bilingual evaluation understudy</strong> (<strong class="bold">BLEU</strong>) is a method for assessing the quality of text <a id="_idIndexMarker351"/>for the task of machine translation <a id="_idIndexMarker352"/>from one language to another. The quality of text generated by a machine translation algorithm is compared to the output of a human. The evaluation is done to observe how close a machine translation is to a professional human translation.</li>
				<li><strong class="bold">Recall-Oriented Understudy for Gisting Evaluation</strong> (<strong class="bold">ROUGE</strong>) is a human versus <a id="_idIndexMarker353"/>machine performance evaluation metric used to evaluate tasks <a id="_idIndexMarker354"/>such as automatic summarization and machine translation. This metric compares an automatically generated summary or translation versus summary/translations produced by humans.</li>
			</ul>
			<h4>The Turing test</h4>
			<p>The Turing test was <a id="_idIndexMarker355"/>engineered by the famous Alan Turing. He referred to it as the imitation game in the 1950s. The Turing test is a test of a machine to evaluate its ability to exhibit intelligent behavior similar to that of a human. In another sense, the Turing test is also a test to evaluate the ability of a machine to fool a human into believing a task done by machine is human-like or done by a human. For instance, we can see the Turing <a id="_idIndexMarker356"/>test in operation in <em class="italic">Figure 5.6</em>, where a text-based interaction is happening between the human interrogator, X, and a computer or machine subject (Bob), and the interrogator, X, and a human subject (Alice): </p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B16572_05_06.jpg" alt="Figure 5.6 – Turing test&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Turing test</p>
			<p>During the Turing test, the human interrogator, X, performs a series of interactions, both with Bob (computer) and Alice (human) with an intent to distinguish between the human and the machine correctly. The machine passes the Turing test if/when the interrogator cannot distinguish them correctly or mistakes the machine for a human (Bob for Alice).</p>
			<h4>Reward per return </h4>
			<p>Reinforcement learning models <a id="_idIndexMarker357"/>are hybrid models that involve continuous learning mechanisms between the agent and operating environment in order to achieve pre-defined goals. The agent learns based on rewards earned for efficient or optimal steps toward reaching a goal. When the goal is optimal control, you will want to measure the agent by how well it does at the task. To quantify how well the agent performs the task, the aggregate measures of reward, such as total reward per <a id="_idIndexMarker358"/>episode (otherwise known as "return") or mean reward per time step, can be used to assess and optimize control for the agent with respect to the environment and the goals. </p>
			<h4>Regret</h4>
			<p>Regret is a <a id="_idIndexMarker359"/>commonly used metric for hybrid models such as reinforcement learning models. At each time step, you calculate the difference between the reward of the optimal decision and the decision taken by your algorithm. Cumulative regret is then calculated by summing this up. The minimum regret is 0 with the optimal policy. The smaller the regret, the better an algorithm has performed. </p>
			<p>Regret enables the actions of the agent to be assessed with respect to the best policy for the optimal performance of the agent as shown in <em class="italic">Figure 5.7</em>. The shaded region in red is the regret:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B16572_05_07.jpg" alt="Figure 5.7 – Regret for reinforcement learning&#13;&#10;Shapely Additive Explanations (SHAP)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Regret for reinforcement learning</p>
			<p class="figure-caption">SHapley Additive exPlanations (SHAP)</p>
			<p>Model interpretability and explaining why the model is making certain decisions or predictions can be vital in a number of business problems or industries. Using techniques discussed earlier, we can interpret the model's performance, but there are still some gray areas, such as deep learning models, which are black-box models. It is noticeable in general that these models can be trained to achieve great results or accuracies on test data, but it is hard to say why. In such scenarios, <strong class="bold">SHapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>) can be useful <a id="_idIndexMarker360"/>to decode what is happening with the predicted results and which feature predictions correlate to the most. SHAP was proposed in this paper (at NIPS): <a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions">http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions</a>.</p>
			<p>SHAP works both for classification and regression models. The primary goal of SHAP is to explain the model <a id="_idIndexMarker361"/>output prediction by computing the contribution of each feature. The SHAP explanation method uses Shapley values to explain the feature importance for model outputs or predictions. Shapley values are computed from cooperative game theory, and these values range from <strong class="source-inline">-1</strong> to <strong class="source-inline">1</strong>. Shapley values describe the distribution of model outputs among the features, as shown in <em class="italic">Figure 5.8</em>:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B16572_05_08.jpg" alt="Figure 5.8 – Shapley values bar chart depicting feature importance&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – Shapley values bar chart depicting feature importance</p>
			<p>There are several SHAP explainer techniques, such as SHAP Tree Explainer, SHAP Deep Explainer, SHAP Linear Explainer, and SHAP Kernel Explainer. Depending on the use case, these explainers can provide useful information on model predictions and help us to understand black-box models. Read more here: <a href="https://christophm.github.io/interpretable-ml-book/shap.html">https://christophm.github.io/interpretable-ml-book/shap.html</a></p>
			<h4>MIMIC explainer</h4>
			<p>Mimic explainer is an approach mimicking black-box models by training an interpretable global <a id="_idIndexMarker362"/>surrogate model. These trained global surrogate models are interpretable models that are trained to approximate the predictions of any black-box model as accurately as possible. By using the surrogate model, a black-box model can be gauged or interpreted as follows.</p>
			<p>The following steps are implemented to train a surrogate model:</p>
			<ol>
				<li>To train a surrogate model, start by selecting a dataset, X. This dataset can be the same as the one used for training the black-box model or it can be another dataset of similar distributions depending on the use case.</li>
				<li>Get the predictions of the black-box model for the selected dataset, X.</li>
				<li>Select an interpretable model type (linear model, decision tree, random forest, and so on).</li>
				<li>Using the dataset, X, and its predictions, train the interpretable model.</li>
				<li>Now you have a trained surrogate model. Kudos!</li>
				<li>Evaluate how well the surrogate model has reproduced predictions of the black-box model, for example, using R-square or F-score.</li>
				<li>Get an understanding of black-box model predictions by interpreting the surrogate model.</li>
			</ol>
			<p>The following interpretable <a id="_idIndexMarker363"/>models can be used as surrogate models: <strong class="bold">Light Gradient boosting model</strong> (<strong class="bold">LightGBM</strong>), linear regression, stochastic gradient descent, or random forest and decision tree.</p>
			<p>Surrogate models can enable ML solution developers to gauge and understand the black-box model's performance. </p>
			<h4>Permutation feature importance explainer (PFI)</h4>
			<p><strong class="bold">Permutation Feature Importance</strong> (<strong class="bold">PFI</strong>) is a technique used to explain classification and regression models. This <a id="_idIndexMarker364"/>technique is useful for interpreting and understanding a feature to model output or prediction correlation. PFI is an alternative to SHAP. It works by randomly assessing one feature at a time for the entire dataset and calculating the change in performance evaluation metrics. The change in performance metric is evaluated for each feature; the more significant the change, the more important the feature is. </p>
			<p>PFI can describe the overall behavior of any model, but does not explain individual predictions of the model. PFI is an alternative to SHAP, but is still quite different as PFI is based on the decrease in performance of the model, while SHAP is based on the magnitude of feature attributions.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor102"/>Statistical models' metrics</h2>
			<p>As we learned in <a href="B16572_02_Final_JM_ePub.xhtml#_idTextAnchor028"><em class="italic">Chapter 2</em></a>, <em class="italic">Characterizing Your Machine Learning Problem</em>, there are three types of <a id="_idIndexMarker365"/>statistical models: inductive learning, deductive learning, and transduction learning. Statistical models offer a good degree of interpretability.</p>
			<h4>Mean</h4>
			<p>The mean, or average, is the <a id="_idIndexMarker366"/>central value of the dataset. It is <a id="_idIndexMarker367"/>calculated by summing all the values and dividing the sum by the number of values:</p>
			<p><em class="italic">mean = x1 + x2 + x3 +.... + xn / n</em></p>
			<h4>Standard deviation</h4>
			<p>The standard <a id="_idIndexMarker368"/>deviation measures the dispersion of the values <a id="_idIndexMarker369"/>in the dataset. The lower the standard deviation, the closer the data points to the mean. A widely spread dataset would have a higher standard deviation.</p>
			<h4>Bias</h4>
			<p>Bias measures the strength (or rigidity) of the mapping function between the independent (input) <a id="_idIndexMarker370"/>and dependent (output) variables. The <a id="_idIndexMarker371"/>stronger the assumptions of the model regarding the functional form of the mapping, the greater the bias.</p>
			<p>High bias is helpful when the underlying true (but unknown) model has matching properties as the assumptions of the mapping function. However, you could get completely off-track if the underlying model does not exhibit similar properties as the functional form of the mapping. For example, the assumption that there is a linear relationship in the variables when in reality it is highly non-linear and it would lead to a bad fit:</p>
			<ul>
				<li><strong class="bold">Low bias</strong>: Weak assumptions with <a id="_idIndexMarker372"/>regard to the functional form of the mapping of inputs to outputs</li>
				<li><strong class="bold">High bias</strong>: Strong <a id="_idIndexMarker373"/>assumptions with regard to the functional form of the mapping of inputs to outputs</li>
			</ul>
			<p>The bias is always a positive value. Here is an additional resource for learning more about bias in ML. This article offers a broader explanation: <a href="https://kourentzes.com/forecasting/2014/12/17/the-bias-coefficient-a-new-metric-for-forecast-bias/">https://kourentzes.com/forecasting/2014/12/17/the-bias-coefficient-a-new-metric-for-forecast-bias/</a>.</p>
			<h4>Variance</h4>
			<p>The variance of the model <a id="_idIndexMarker374"/>is the degree to which the model's performance <a id="_idIndexMarker375"/>changes when it is fitted on different training data. The impact of the specifics on the model is captured by the variance.</p>
			<p>A high variance model will change a lot with even small changes in the training dataset. On the other hand, a low variance model wouldn't change much even with large changes in the training dataset. The variance is always positive.</p>
			<h4>R-squared</h4>
			<p>R-squared, also <a id="_idIndexMarker376"/>known as the <a id="_idIndexMarker377"/>coefficient of determination, measures <a id="_idIndexMarker378"/>the variation in the dependent variable that can be explained by the model. It is calculated as the explained variation divided by the total variation. In simple terms, R-squared measures how close the data points are to the fitted regression line.</p>
			<p>The value of R-squared lies between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. A low R-squared value indicates that most of the variation in <a id="_idIndexMarker379"/>the response variable is not explained by the model, but by other factors not included <a id="_idIndexMarker380"/>in it. In general, you should aim for a higher R-squared value because this indicates that the model better fits the data.</p>
			<h4>RMSE</h4>
			<p>The <strong class="bold">root mean square error</strong> (<strong class="bold">RMSE</strong>) measures the <a id="_idIndexMarker381"/>difference between the predicted values of the model and <a id="_idIndexMarker382"/>the observed (true) values.</p>
			<p>Options are many, and you need to choose the right metric for real-world production scenarios to have well-justified evaluations; for example, why a data scientist or a data science team might want to select one evaluation metric over another, for instance, R-squared over mean for a regression problem. It depends on the use case and type of data. </p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor103"/>HITL model metrics</h2>
			<p>There are two types of <strong class="bold">HITL</strong> models – human reinforcement learning and <a id="_idIndexMarker383"/>active learning models. In these models, human-machine collaboration fosters the algorithm to mimic human-like behaviors and outcomes. A key driver for these ML solutions is the human in the loop. Humans validate, label, and retrain the models to maintain the accuracy of the model.</p>
			<h3>Human bias</h3>
			<p>Just like the <a id="_idIndexMarker384"/>human brain, ML systems <a id="_idIndexMarker385"/>are subject to cognitive bias. Human cognitive biases are processes that disrupt your decision making and reasoning ability, ending up in the production of errors. Human bias occurrences include stereotyping, selective perception, the bandwagon effect, priming, affirmation predisposition, observational selection bias, and the speculator's false notion. In many cases, it is vital to avoid such biases for ML systems in order to make rational and optimal decisions. This will make ML systems more pragmatic than humans if we manage to deduce human bias and rectify it. This will be especially useful in HITL-based systems. Using bias testing, three types of human <a id="_idIndexMarker386"/>biases can be identified and <a id="_idIndexMarker387"/>worked upon to maintain the ML system's decision making such that it is free from human bias. These three human biases are as follows:</p>
			<p><strong class="bold">Interaction bias </strong></p>
			<p>When an ML system is <a id="_idIndexMarker388"/>fed a dataset containing entries of one particular type, an interaction bias is introduced that prevents the algorithm from recognizing any other types of entries. This type of bias can be identified in inference testing for trained models. Methods such as SHAP and PFI can be useful in identifying feature bias.</p>
			<p><strong class="bold">Latent bias</strong> </p>
			<p>Latent bias is <a id="_idIndexMarker389"/>experienced when multiple examples in the training set have a characteristic that stands out. Then, the ones without that characteristic fail to be recognized by the algorithm. For example, recently, the Amazon HR algorithm for selecting people based on applications for roles within the company showed bias against women, the reason being latent bias. </p>
			<p><strong class="bold">Selection bias </strong></p>
			<p>Selection bias is <a id="_idIndexMarker390"/>introduced to an algorithm when the selection of data for analysis is not properly randomized. For example, in designing a high-performance face recognition system, it is vital to include all possible types of facial structures and shapes and from all ethnic and geographical samples, so as to avoid selection bias. Selection bias can be identified by methods such as SHAP or PFI to observe model feature bias.</p>
			<h3>Optimal policy</h3>
			<p>In the case of <a id="_idIndexMarker391"/>human reinforcement learning, the <a id="_idIndexMarker392"/>goal of the system is to maximize the rewards of the action in the current state. In order to maximize the rewards for actions, optimal policy can be used as a metric to gauge the system. The optimal policy is the policy where the action that maximizes the reward/return of the current state is chosen. The optimal policy is the metric or state that is ideal for a system to perform at its best. In a human reinforcement learning-based system, a human operator or teacher sets the optimal policy as the goal of the system is to reach human-level performance. </p>
			<h3>Rate of automation</h3>
			<p>Automation is the process of <a id="_idIndexMarker393"/>automatically producing goods or getting a task done through the use of robots or algorithms with <a id="_idIndexMarker394"/>no direct human assistance.</p>
			<p>The level of automation of an ML system can be calculated using the rate of automation of the total tasks. It is basically the percentage of tasks fully automated by the system, and these tasks do not require any human assistance. It shows what percentage of tasks are fully automated out of all the tasks. For example, AlphaGo, by DeepMind, has achieved 100% automation to operate on its own to defeat human world champion players.</p>
			<h3>Risk rate</h3>
			<p>The probability <a id="_idIndexMarker395"/>of an ML model performing <a id="_idIndexMarker396"/>errors is known as <a id="_idIndexMarker397"/>the error rate. The error rate is calculated based on the model's performance for production systems. The lower the error rate, the better it is for an ML system. The goal of a human in the loop is to reduce the error rate and teach the ML model to function at its most optimal. </p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor104"/>Production testing methods</h1>
			<p>As there are <a id="_idIndexMarker398"/>various businesses in operation, so are different types of production systems serving these businesses. In this section, we look into the different types of production systems or setups commonly used and how to test them.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor105"/>Batch testing</h2>
			<p>Batch testing <a id="_idIndexMarker399"/>validates your model by performing testing <a id="_idIndexMarker400"/>in an environment that is different from its training environment. Batch testing is carried out on a set of samples of data to test model inference using metrics of choice, such as accuracy, RMSE, or f1-score. Batch testing can be done in various types of computes, for example, in the cloud, or on a remote server or a test server. The model is usually served as a serialized file, and the file is loaded as an object and inferred on test data. </p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor106"/>A/B testing</h2>
			<p>You will surely <a id="_idIndexMarker401"/>have come across A/B testing. It is <a id="_idIndexMarker402"/>often used in service design (websites, mobile apps, and so on) and for assessing marketing campaigns. For instance, it is used to evaluate whether a specific change in the design or tailoring content to a specific audience positively affects business metrics such as user engagement, the click-through rate, or the sales rate. A similar technique is applied in testing ML models using A/B testing. When models are tested using A/B testing, the test will answer important questions such as the following:</p>
			<ul>
				<li>Does the new model B work better in production than the current model A? </li>
				<li>Which of the two models' nominees work better in production to drive positive business metrics?</li>
			</ul>
			<p>To evaluate the results of A/B testing, statistical techniques are used based on the business or operations to determine which model will perform better in production. A/B testing is usually conducted in this manner, and real-time or live data is fragmented or split into two sets, Set A and Set B. Set A data is routed to the old model, and Set B data is routed to the new model. In order to evaluate whether the new model (model B) performs better than the old model (model A), various statistical techniques can be used to evaluate model performance (for example, accuracy, precision, recall, f-score, and RMSE), depending on the business use case or operations. Depending on a statistical analysis of model performance in correlation to business metrics (a positive change in business metrics), a decision is made to replace the new model with the old one or determine which model is better. </p>
			<p>A/B testing is performed methodically using statistical hypothesis testing, and this hypothesis validates two sides of a coin – the null hypothesis and the alternate hypothesis. The null hypothesis asserts that the new model does not increase the average value of the monitoring business metrics. The alternate hypothesis asserts that the new model improves the average value of the monitoring business metrics. Ultimately, A/B testing is used to evaluate whether <em class="italic">the new model drives a significant boost in specific business metrics</em>. There are various types of A/B testing, depending on business use cases and operations, for example, <strong class="source-inline">Z-test</strong>, <strong class="source-inline">G-test</strong> (I recommend knowing about these and others), and others. Choosing the right A/B test and metrics to evaluate can be a win-win for your business and ML operations. </p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor107"/>Stage test or shadow test</h2>
			<p>Before <a id="_idIndexMarker403"/>deploying a model for production, which would then <a id="_idIndexMarker404"/>lead to making business decisions, it can be valuable to replicate a production-like environment (staging environment) to <a id="_idIndexMarker405"/>test the model's <a id="_idIndexMarker406"/>performance. This is especially important for testing the robustness of the model and assessing its performance on real-time data. It could be facilitated by deploying the develop branch or a model to be tested on a staging server and inferring the same data as the production pipeline. The only shortcoming here will be that end users will not see the results of the develop branch or business decisions will not be made in the staging server. The results of the staging environment will statistically be evaluated using suitable metrics (for example, accuracy, precision, recall, f-score, and RMSE) to determine the robustness and performance of the model in correlation to business metrics. </p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor108"/>Testing in CI/CD</h2>
			<p>Implementing <a id="_idIndexMarker407"/>testing as part of CI/CD pipelines can be <a id="_idIndexMarker408"/>rewarding in terms of automating and evaluating (based on set criteria) the model's performance. CI/CD pipelines can be set up in multiple ways depending on the operations and architecture in place, for instance: </p>
			<ul>
				<li>Upon a successful run of an ML pipeline, CI/CD pipelines can trigger a new model's A/B test in the staging environment.</li>
				<li>When a new model is trained, it is beneficial to set up a dataset separate from the test set to measure its performance against suitable metrics, and this step can be fully automated. </li>
				<li>CI/CD pipelines can periodically trigger ML pipelines at a set time in a day to train a new model, which uses live or real-time data to train a new model or fine-tune an existing model. </li>
				<li>CI/CD pipelines can monitor the ML model's performance of the deployed model in production, and this can be triggered or managed using time-based triggers or manual triggers (initiated by team members responsible for quality assurance).</li>
				<li>CI/CD pipelines can provision two or more staging environments to perform A/B testing on unique datasets to perform more diverse and comprehensive testing.</li>
			</ul>
			<p>These are a variety of scenarios, and depending on requirements, CI/CD pipelines offer various <a id="_idIndexMarker409"/>workflows and operations tailored to the needs of <a id="_idIndexMarker410"/>the business and tech requirements. Selecting an efficient architecture and CI/CD process can augment tech operations and team performance overall. CI/CD testing can augment and automate testing to great lengths.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor109"/>Why package ML models?</h1>
			<p>MLOps enables a systematic approach to train and evaluate models. After models are trained and evaluated, the next steps are to bring them to production. As we know, ML doesn't work like traditional software engineering, which is deterministic in nature and where a piece of <a id="_idIndexMarker411"/>code or module is imported into the existing system and it works. Engineering ML solutions is non-deterministic and involves serving ML models to make predictions or analyze data. </p>
			<p>In order to serve the models, they need to be packed into software artifacts to be shipped to the testing or production environments. Usually, these software artifacts are packaged into a file or a bunch of files or containers. This allows the software to be environment- and deployment-agnostic. ML models need to be packaged for the following reasons:</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor110"/>Portability </h2>
			<p>Packaging ML models <a id="_idIndexMarker412"/>into software artifacts enables them to be shipped or transported from one environment to another. This can be done by shipping a file or bunch of files or a container. Either way, we can transport the artifacts and replicate the model in various setups. For example, a packaged model can be deployed in a virtual machine or serverless setup.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor111"/>Inference</h2>
			<p>ML inference is a process that involves processing real-time data using ML models to calculate an output, for <a id="_idIndexMarker413"/>example, a prediction or numerical score. The purpose of packaging ML models is to be able to serve the ML models in real time for ML inference. Effective ML model packaging (for example, a serialized model or container) can facilitate deployment and serve the model to make predictions and analyze data in real time or in batches.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor112"/>Interoperability </h2>
			<p>ML model interoperability is the ability of two or more models or components to exchange <a id="_idIndexMarker414"/>information and to use exchanged information in order to learn or fine-tune from each other and perform operations with efficiency. Exchanged information can be in the form of data or software artifacts or model parameters. Such information enables models to fine-tune, retrain, or adapt to various environments from the experience of other software artifacts in order to perform and be efficient. Packaging ML models is the foundation for enabling ML model interoperability.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor113"/>Deployment agnosticity </h2>
			<p>Packaging ML models into software artifacts such as serialized files or containers enables the <a id="_idIndexMarker415"/>models to be shipped and deployed in various runtime environments, such as in a virtual machine, a container serverless environment, a streaming service, microservices, or batch services. It opens opportunities for portability and deployment agnosticity using the same software artifacts that an ML model is packaged in.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor114"/>How to package ML models</h1>
			<p>ML models can be <a id="_idIndexMarker416"/>packaged in various ways depending on business and tech requirements and as per operations for ML. ML models can be packaged and shipped in three ways, as discussed in the following sub-sections.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor115"/>Serialized files</h2>
			<p>Serialization is a vital process for packaging an ML model as it enables model portability, interoperability, and <a id="_idIndexMarker417"/>model inference. Serialization is the method of converting an object or a data structure (for example, variables, arrays, and tuples) into a storable artefact, for example, into a file or a memory buffer that can be transported or transmitted (across computer networks). The main purpose of serialization is to reconstruct the serialized file into its previous data structure (for example, a serialized file into an ML model variable) in a different environment. This way, a newly trained ML model can be serialized into a file and exported into a new environment where it can de-serialized back into an ML model variable or data structure for ML inferencing. A serialized file does not save or include any of the previously associated methods or implementation. It only saves the data structure as it is in a storable artefact such as a file.</p>
			<p>Here are some popular serialization formats in <em class="italic">figure 5.1</em>:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/Table_01.jpg" alt="Table 5.1 – Popular ML model serialization formats&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 5.1 – Popular ML model serialization formats</p>
			<p>All these serialized formats (except ONNX) have one problem in common, the problem of interoperability. To <a id="_idIndexMarker418"/>address that, ONNX is developed as an open source project supported by Microsoft, Baidu, Amazon, and other big companies. This enables a model to be trained using one framework (for example, in scikit-learn) and then retrained again using TensorFlow. This has become a game changer for industrialized AI as models can be rendered interoperable and framework-independent. </p>
			<p>ONNX has unlocked new avenues, such as federated learning and transfer learning. Serialized models enable portability and also batch inferencing (batch inference, or offline inference, is the method of generating predictions on a batch of data points or samples) in different environments.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor116"/>Packetizing or containerizing </h2>
			<p>We often <a id="_idIndexMarker419"/>encounter diverse environments for <a id="_idIndexMarker420"/>production systems. Every environment possesses different challenges when it comes to deploying ML models, in terms of compatibility, robustness, and scalability. These challenges can be avoided by standardizing some processes or modules and containers are a great way to standardize ML models and software modules. </p>
			<p>A container is a standard unit of software made up of code and all its dependencies. It enables the quick and reliable operation of applications from one computing environment to another. It enables the software to be environment- and deployment-agnostic. Containers are managed and orchestrated by Docker. Docker has become an industry standard at developing and orchestrating containers. </p>
			<p>Docker is an open source (<a href="https://opensource.com/resources/what-open-source">https://opensource.com/resources/what-open-source</a>) tool. It has been developed to make it convenient to build, deploy, and run applications by using containers. By using containers, a developer can package an application with its components and modules, such as files, libraries, and other dependencies, and deploy it as one package. Containers are a reliable way to run applications using a Linux OS with customized settings. Docker containers are built using Dockerfiles, which are used to containerize an application. After building a Docker image, a Docker container is built. A Docker <a id="_idIndexMarker421"/>container is an application running with custom <a id="_idIndexMarker422"/>settings as orchestrated by the developer. <em class="italic">Figure 5.8</em> shows the process of building and running a Docker container from a Dockerfile. A Dockerfile is built into a Docker image, which is then run as a Docker container:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B16572_05_09.jpg" alt="Figure 5.9 – Docker artifacts&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – Docker artifacts</p>
			<p>A Dockerfile, Docker image, and a Docker container are foundational components for building and running containers. These <a id="_idIndexMarker423"/>are each described here:</p>
			<ul>
				<li><strong class="bold">Dockerfile</strong>: A Dockerfile is a text <a id="_idIndexMarker424"/>document containing a set of Docker commands ordered by the developer to build a Docker image. Docker is able to read the Dockerfile and build a Docker image.</li>
				<li><strong class="bold">Docker image</strong>: This is a sequential <a id="_idIndexMarker425"/>collection of execution parameters to use within a collection of root filesystems within a container during runtime. Docker images are like a snapshot of containers. Containers are constructed from Docker images. </li>
				<li><strong class="bold">Docker container</strong>: Containers are <a id="_idIndexMarker426"/>constructed from Docker images. A container is a runtime instance of a Docker image.</li>
			</ul>
			<p>ML models can be served in Docker containers for robustness, scalability, and deployment agnosticity. In later chapters, we will deploy ML models using Docker for the purpose of hands-on experience, hence, it is good to have a general understanding of this tool.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor117"/>Microservice generation and deployment</h2>
			<p>Microservices <a id="_idIndexMarker427"/>enable the collection of services that are independently deployable. Each of these services is highly maintainable, testable, and loosely coupled. Microservices <a id="_idIndexMarker428"/>are orchestrated by architecture that is organized around business capabilities to enable a system to serve business needs. For example, Spotify has transitioned from a monolithic complex system to a microservices-based system. It was done by splitting the complex system into fragmented services, with specific goals such as a search engine, content tagging, content classification, user behavioral analytics for a recommendation engine, and autogenerated playlists. Fragmented microservices are now developed by a dedicated team. Each microservice is isolated and less dependent on one another. This way, it is easier to develop and maintain. The company can be consistent with customer service and continuously improve without putting the service down.</p>
			<p>Typically, a microservice is generated by tailoring serialized files into a containerized Docker image. These Docker images can then be deployed and orchestrated into any Docker - supported environment. Deploying and managing Docker images can be performed using container management tools, such as Kubernetes. Docker enables extreme portability and interoperability, Docker images can be easily deployed to any popular cloud service, such as Google Cloud, Azure, or AWS. Docker images can be deployed and managed to any Docker corporate server or data center or real-time environment as long as it supports Docker. </p>
			<p>Microservices can be served in a REST API format, and this is a popular way to serve ML models. Some Python frameworks, such as Flask, Django, and FastAPI, have become popular in enabling ML models to serve as REST API microservices. To facilitate robust and scalable system operations, software developers can sync with Dockerized microservices via a REST API. To orchestrate Docker-based microservice deployments on Kubernetes-supported infrastructure, Kubeflow is a good option. It is cloud-agnostic and can be run on-premises or on local machines. Besides that, Kubeflow is based on Kubernetes, but keeps the Kubernetes details and difficulties inside a box. Kubeflow is a robust way of serving a model. This is a tool worth exploring: <a href="https://www.kubeflow.org/docs/started/kubeflow-overview/">https://www.kubeflow.org/docs/started/kubeflow-overview/</a>.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor118"/>Inference ready models </h1>
			<p>We have previously worked on a business problem to predict the weather at a port. To build a solution for this <a id="_idIndexMarker429"/>business problem, data processing and ML model training were performed, followed by serializing models. Now, in this section, we explore how inference is done on the serialized model. This section's code is available from the attached Jupyter notebook in the chapter's corresponding folder in the book's GitHub repository. Here are the instructions for running the code:</p>
			<ol>
				<li value="1">Log in to the Azure portal again.</li>
				<li>From <strong class="bold">Recent Resources</strong>, select the <strong class="source-inline">MLOps_WS</strong> workspace, and then click on the <strong class="bold">Launch Studio</strong> button. This will direct you to the <strong class="source-inline">MLOps_WS</strong> workspace.</li>
				<li>In the <strong class="bold">Manage</strong> section, click on the <strong class="bold">Compute </strong>section, and then select the machine created in <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines</em>. Click on the <strong class="bold">Start</strong> button to start the instance. When the VM is ready, click on the JupyterLab link.</li>
				<li>Now, in JupyterLab, navigate to the chapter's corresponding folder (<strong class="source-inline">05_model_evaluation_packaging</strong>) and open the notebook (<strong class="source-inline">model_evaluation_packaging.ipynb</strong>).</li>
			</ol>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor119"/>Connecting to the workspace and importing model artifacts</h2>
			<p>First, we import the <a id="_idIndexMarker430"/>requisite packages, connect to the ML <a id="_idIndexMarker431"/>workspace using the <strong class="source-inline">Workspace()</strong> function, and then download the serialized scaler and model to perform predictions. <strong class="source-inline">Scaler</strong> will be used to scale input data into the same scale of data that was used for model training. The <strong class="source-inline">Model</strong> file is serialized in ONNX format. Both the <strong class="source-inline">Scaler</strong> and <strong class="source-inline">Model</strong> files are imported using the <strong class="source-inline">Model()</strong> function:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import warnings</p>
			<p class="source-code">import pickle</p>
			<p class="source-code">from math import sqrt</p>
			<p class="source-code">warnings.filterwarnings('ignore')</p>
			<p class="source-code">from azureml.core.run import Run</p>
			<p class="source-code">from azureml.core.experiment import Experiment</p>
			<p class="source-code">from azureml.core.workspace import Workspace</p>
			<p class="source-code">from azureml.core.model import Model</p>
			<p class="source-code"># Connect to Workspace</p>
			<p class="source-code">ws = Workspace.from_config()</p>
			<p class="source-code">print(ws)</p>
			<p class="source-code"># Load Scaler and model to test</p>
			<p class="source-code">scaler = Model(ws,'scaler').download(exist_ok=True)</p>
			<p class="source-code">svc_model = Model(ws,'support-vector-classifier').download(exist_ok=True)</p>
			<p>After running this code, you will see new files downloaded in the left panel in the JupyterLab window.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor120"/>Loading model artifacts for inference</h2>
			<p>We open and <a id="_idIndexMarker432"/>load the <strong class="source-inline">Scaler</strong> and <strong class="source-inline">Model</strong> files into variables that can be used for ML model inference. <strong class="source-inline">Scaler</strong> is read and loaded into a variable using pickle, and the ONNX runtime is used to load the ONNX file using <strong class="source-inline">InferenceSession()</strong> for making ML model predictions as follows: </p>
			<p class="source-code">with open('scaler.pkl', 'rb') as file:</p>
			<p class="source-code">    scaler = pickle.load(file)</p>
			<p class="source-code"># Compute the prediction with ONNX Runtime</p>
			<p class="source-code">import onnxruntime as rt</p>
			<p class="source-code">import numpy</p>
			<p class="source-code">sess = rt.InferenceSession("svc.onnx")</p>
			<p class="source-code">input_name = sess.get_inputs()[0].name</p>
			<p class="source-code">label_name = sess.get_outputs()[0].name</p>
			<h3>ML model inference</h3>
			<p>To perform ML model <a id="_idIndexMarker433"/>inference, scale the test data and set it up for inference u<a id="_idTextAnchor121"/>sing the <strong class="source-inline">fit_transform()</strong> method. Now, perform inference on the test data by using the ONNX session and run <strong class="source-inline">sess.run()</strong> by passing the input data, <strong class="source-inline">test_data</strong>, in <strong class="source-inline">float</strong> <strong class="source-inline">32</strong> format. Lastly, print the results of model inference:</p>
			<p class="source-code">test_data = np.array([34.927778, 0.24, 7.3899, 83, 16.1000, 1])</p>
			<p class="source-code">test_data = scaler.fit_transform(test_data.reshape(1, 6))</p>
			<p class="source-code"># Inference </p>
			<p class="source-code">pred_onx = sess.run([label_name], {input_name: test_data.astype(numpy.float32)})[0]</p>
			<p class="source-code">print(pred_onx[0])</p>
			<p>With these steps, we have successfully downloaded the serialized model, loaded it to a variable, and performed inference on a test data sample. The expected result of the block code is the value <strong class="source-inline">1</strong>.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor122"/>Summary</h1>
			<p>In this chapter, we have explored the various methods to evaluate and interpret ML models. We have learned about production testing methods and the importance of packaging models, why and how to package models, and the various practicalities and tools for packaging models for ML model inference in production. Lastly, to understand the workings of packaging and de-packaging serialized models for inference, we performed the hands-on implementation of ML model inference using serialized models on test data. </p>
			<p>In the next chapter, we will learn more about deploying your ML models. Fasten your seatbelts and get ready to deploy your models to production!</p>
		</div>
	</body></html>