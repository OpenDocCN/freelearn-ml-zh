- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative artificial intelligence** (**GenAI**) is certainly the term on
    everybody’s lips at the moment. If you haven’t already had an opportunity to “get
    your hands dirty” with GenAI, then you will soon learn why it is currently taking
    the world by storm as we dive into the kinds of amazing things we can do with
    this relatively new set of technologies. In this chapter, we will explore some
    of the concepts underpinning what GenAI is and its distinctions from other **artificial
    intelligence** (**AI**)/**machine learning** (**ML**) approaches. We’ll also cover
    some of the major historical developments that have led to its meteoric rise,
    and examples of how it is being used today.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin the chapter by introducing some fundamental concepts before moving
    on to more complex topics and the evolution of various GenAI approaches, such
    as **autoencoders** (**AEs**), **generative adversarial networks** (**GANs**),
    diffusion, and **large language models** (**LLMs**). Considering that this is
    an introductory chapter, we will mainly lay the groundwork for the deeper dives
    that will follow in later chapters. Specifically, this chapter covers the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentals of GenAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GenAI techniques and evolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As makes logical sense, let’s begin with the fundamentals!
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentals of GenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section introduces the basic concepts we need to understand when discussing
    GenAI, starting with an overview of what GenAI is!
  prefs: []
  type: TYPE_NORMAL
- en: What is GenAI?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I’ll begin explaining this topic by focusing on what distinguishes GenAI from
    the other AI/ML approaches we’ve covered. Think about all of the various algorithms
    and approaches I’ve described so far throughout this book, and, more specifically,
    think about the primary goal that was being pursued in each approach. Whether
    we were using linear regression in `scikit-learn` to predict a numeric value for
    some target variable based on its features, logistic regression in XGBoost to
    implement a binary classifier model or using time-series data to predict the future
    in TensorFlow, there is one common theme, which is that we were trying to **predict**
    or **estimate** something.
  prefs: []
  type: TYPE_NORMAL
- en: The key concept to understand here is that the output from our various sophisticated
    models was generally a distinct data point that was either right or wrong, or
    at least as close to right as possible. Our models typically produced a single,
    simple answer based on relationships the model had learned (or estimated) in the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Even in the case of **unsupervised learning** (**UL**) approaches such as K-means
    clustering, the model simply finds mathematical relationships in the data and
    categorizes the data points into groups based on those relationships.
  prefs: []
  type: TYPE_NORMAL
- en: The huge leap forward that has been achieved with the introduction of GenAI
    is that the models can now go beyond simple “yes”/“no” answers or numeric estimates
    based on sheer mathematical number crunching and pattern recognition. With GenAI,
    the models can now create (or **generate**) new data. That’s the big difference,
    and it turns out that the implications of this are pretty huge!
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a step back for a moment: when I was young, I thought that AI research
    could design machines that think like humans, and that fascinated me, so I started
    learning about how ML algorithms work. I was quite disappointed to learn that,
    although some models can do an amazing job at “understanding” consumer behavior
    and accurately recommending products that a given person might like to purchase,
    or even diagnose potential illnesses based on input data related to a patient,
    it was all just based on feeding large amounts of data into a mathematical algorithm
    that “learns” to detect a pattern in the data. There was no actual “intelligence”
    there, although the science is still fascinating.'
  prefs: []
  type: TYPE_NORMAL
- en: The fact that GenAI models can go beyond specific mathematical answers and create
    new content is a significant leap forward in the pursuit of AI. Have a chat with
    any of the latest and greatest GenAI models out there, and I don’t doubt for a
    moment that you will be very impressed by the kinds of mind-boggling things they
    can do, such as composing music, painting an imaginative scene, writing a catchy
    poem, or creating a web application. You’ll learn how to harness GenAI for implementing
    many different kinds of use cases later in this book, and I’m sure you’ll agree
    that it’s a dramatic technological advancement.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to understand, however, that the amazing feats performed by GenAI
    models are still powered by many of the AI/ML concepts we have already covered
    in this book. At its core, GenAI works by understanding and replicating the underlying
    patterns and structures in the data it has been trained on, and it still uses
    algorithms and **neural networks** (**NNs**) to perform these kinds of activities,
    although the network architectures used, and the novel ways in which they are
    used, are considerably more advanced. In the case of GenAI, the goal is not just
    to interpret the data but to form an understanding that can be used to create
    something new. Let’s dive into these topics in more detail to better understand
    what sets GenAI apart from other AI approaches.
  prefs: []
  type: TYPE_NORMAL
- en: What is non-GenAI?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that GenAI has taken the world by storm, how can we refer to all of the
    other AI/ML approaches that existed before it (that is, much of the stuff we already
    covered in this book)? One of the most popular emerging terms for this is “traditional
    AI/ML.” You may also hear it being referred to as “predictive AI/ML” because the
    goal is generally to predict something, or “discriminative AI/ML” in the case
    of classification use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper into GenAI versus non-GenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand the basic differences between GenAI and non-GenAI, we need to
    revisit some of the mathematical concepts that underpin AI/ML. Don’t worry – we’ll
    just cover the concepts at a level required to define the distinctions between
    GenAI and non-GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: The role of probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that mathematical probability is a fundamental concept in many ML use
    cases. For example, when we ask a model to classify a data point into a specific
    category, it rarely will do so with 100% certainty. Instead, it will calculate
    the probability of the data point belonging to each category. Then we, or the
    model itself, depending on the implementation, can select the category with the
    highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key factors that distinguishes GenAI from traditional AI is how probability
    is used in the learning process. Let’s explore this in more detail, starting with
    traditional AI.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional AI and conditional probability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the case of traditional AI, the models try to estimate the probability that
    the target variable `Y` contains a specific value based on the values of the predictor
    variables (or features) in the dataset. This is referred to as the **conditional
    probability** of the target variable’s values based on the values of the input
    features. Conditional probability is the probability of an event occurring given
    that another event has already happened, and it is represented by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: P(B|A) = P(A∩B) / P(A)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: P(B|A) is the conditional probability of B given A.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(A ∩ B) is the probability of both A and B occurring. This is also referred
    to as the “joint probability,” which we will explore in more detail later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(A) is the probability of A occurring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand conditional probability, imagine a game scenario in which there
    are three straws hidden from our view. Of the three straws, two are long straws,
    and one is a short straw. We will take turns picking straws, and whoever picks
    the short straw loses. Each time it’s our turn to pick a straw, there’s a specific
    probability that we will pick the short straw.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, no straws were picked, so in the first turn, the probability of picking
    the short straw is 1/3 (or approximately 33.3%). Now, let’s imagine that we pick
    a straw, and it turns out to be a long straw. This means that two straws remain,
    and since we picked a long straw, it means that one long straw remains, as well
    as the short straw. In the next turn, the probability of picking the short straw
    is, therefore, 1/2 (or 50%).
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to note here is that since we already picked a long straw,
    it influences the probability distribution of the overall scenario. This is a
    very simple example of the concept. In ML, there can be many factors (that is,
    features) involved, leading to very large numbers of potential combinations of
    those factors that could influence the outcome. This is why ML models usually
    need to crunch through large amounts of data to learn patterns in those various
    combinations of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mapping this back to ML use cases, consider the dataset represented in *Figure
    15**.1*, which shows a simplified version of the Titanic dataset from OpenML ([https://www.openml.org/search?type=data&id=40945](https://www.openml.org/search?type=data&id=40945)).
    The target variable is represented by the `survived` column, which is highlighted
    by the green box. This represents *B* in our preceding equation. All of the other
    columns combined constitute the input features, and they represent *A* in our
    previous equation, as highlighted by the red box. Essentially, this depicts that
    when an ML model learns to predict the value of the target variable based on the
    values of the input variables, it is asking the question: “What is the probability
    of *B* (that is, the target variable), given the values of *A* (that is, the input
    variables)?” In other words: “What is the probability of *B*, given *A*?”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1: Separation of target variable from input feature](img/B18143_15_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: Separation of target variable from input feature'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at how probability can be used in a slightly different way
    in GenAI use cases.
  prefs: []
  type: TYPE_NORMAL
- en: GenAI and joint probability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the case of GenAI, the models are designed to learn the **joint probability
    distribution** of the dataset. As we briefly saw in the preceding equation for
    conditional probability, joint probability refers to the probability of two or
    more events happening at the same time. We can use this to understand how different
    variables or events are connected and how they influence each other. It is represented
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: P(A∩B) = P(A) × P(B)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: P(A) is the probability of A occurring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(B) is the probability of B occurring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s again use an example to describe this concept in more detail. A common
    analogy is to imagine rolling two fair six-sided dice and calculating the probability
    of both dice showing a certain number; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event A**: The first die shows a 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event B**: The second die shows a 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, the joint probability P(A ∩ B) is the probability of both the first die
    showing a 3 and the second die showing a 5 when they come to rest. Since each
    die roll is independent and each side has a 1/6 chance of appearing, the joint
    probability is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: P(A∩B) = P(A) × P(B) = 1/6 x 1/6 = 1/36
  prefs: []
  type: TYPE_NORMAL
- en: This means there’s a 1/36 chance that both dice will show the exact numbers
    you predicted. The key thing to understand here is the events are independent,
    but there’s a shared probability distribution that governs the overall outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mapping this back to ML use cases again, the joint probability distribution
    in a dataset includes the target variable as well as the input variables, as represented
    by the red box in *Figure 15**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2: Joining target variable with input features](img/B18143_15_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: Joining target variable with input features'
  prefs: []
  type: TYPE_NORMAL
- en: A major difference here is that, while discriminative models use conditional
    probability to predict the value of the target variable given the values of the
    input variables, generative models try to learn the overall joint probability
    distribution of the dataset, including the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: By learning the overall joint probability distribution of the dataset, the model
    can develop an understanding of how the dataset is constructed and how all of
    the features relate to each other, including the target variable. In this manner,
    by accurately approximating how the training dataset is constructed, it can estimate
    how to create similar data points that do not already exist in the dataset but
    are of similar structure and composition – that is, it can generate new, similar
    content.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the joint probability distribution, today’s generative models
    learn hidden relationships and structure in the dataset by using the attention
    mechanism outlined in the famous *Attention Is All You Need* (Vaswani et al.,
    2017) paper that we’ve referenced numerous times in this book. Before the attention
    mechanism was developed, models mostly treated all inputs equally. And, considering
    that all input features (and related hidden features) carry information, treating
    all inputs equally results in a low signal-to-noise ratio that limits the effectiveness
    of the learning and prediction processes. For example, when a model tries to predict
    the next word in a sentence, not all prior words in the sentence contribute equally
    to predicting the next word. Instead, the most probable next word may be more
    dependent on (or more highly correlated with) the existence of specific other
    words in the sentence. For example, consider the sentence: “I added a pinch of
    salt and a dash of [BLANK].” When a modern generative model attempts to predict
    the next word in that sentence, it will likely predict the word “pepper.” While
    the word “pepper” must make sense in the overall context of the sentence (that
    is, it will have varying degrees of contextual relationships to the other words
    in the sentence), the word “salt” likely has a higher impact on the prediction
    than other words, such as “pinch,” and the attention mechanism helps the models
    to learn these kinds of important relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: While this section focuses mainly on the differences between GenAI and “traditional
    AI,” I also want to mention that the demarcation is not always so clear, and sometimes
    the lines may be blurred.
  prefs: []
  type: TYPE_NORMAL
- en: Blurring the lines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s important to understand that the demarcations between non-GenAI and GenAI
    often become blurred because many applications combine both approaches. For example,
    a model or application that analyzes text and generates a summary may use both
    discriminative and generative processes.
  prefs: []
  type: TYPE_NORMAL
- en: Consider chatbots as an example. Chatbots commonly generate responses by constructing
    sentences “on the fly.” A popular approach in constructing sentences is to predict
    the most appropriate next word based on previous words in the sentence. This is
    an instance of using conditional probability. However, if the application or model
    only did this, then its abilities would be quite limited. In order to generate
    complex and coherent responses that adhere to **natural language** (**NL**) constructs,
    the model needs to have learned an accurate representation of the structure (that
    is, the joint probability distribution) of the language in which it generates
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: An additional interesting scenario is the use of GenAI to produce new data that
    can then be used as features for traditional ML models. An example of this use
    case, as suggested by my colleague, Jeremy Wortz, would be to ask a generative
    model to rate songs with a score (let’s say between 1 and 5) based on a customer
    persona, and then make playlists with those generated features from the ratings
    and outputs from a process called “chain-of-thought reasoning,” which essentially
    asks the LLM to elaborate on its thought process. I will describe the chain-of-thought
    concept in more detail later.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve discussed the differences between GenAI and “traditional AI,”
    we will explore the development of techniques that have led to the evolution of
    GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: GenAI techniques and evolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve already discussed, while there are some distinctions between GenAI
    and “traditional AI,” they share much of the same history. For example, in [*Chapter
    1*](B18143_01.xhtml#_idTextAnchor015), we discussed a brief history of AI/ML,
    and in *Chapters 9* and *14*, we learned about the evolution of various types
    of NNs, such as **recurrent NNs** (**RNNs**), **long short-term memory** (**LSTM**)
    networks, and Transformers. All of those concepts and milestones also apply to
    the history and evolution of GenAI. Considering that GenAI is a relatively newer
    subset of AI, we can view its evolutionary timeline as an extension of the evolution
    of AI in general. Therefore, the topics in this section build upon what we’ve
    already covered in that regard.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of GenAI could comprise an entire book by itself, and for the
    purpose of this current book, it would be an unnecessary level of information
    to cover all of the major milestones and developments that contributed to the
    evolution of GenAI in great detail. Instead, I will summarize some of the most
    prominent milestones and contributory developments at a high level, such as Markov
    chains and **hidden Markov models** (**HMMs**), **restricted Boltzmann machines**
    (**RBMs**), **deep belief networks** (**DBNs**), AEs, and GANs.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s important also to note that some mechanisms that were originally developed
    primarily for discriminative use cases can be reappropriated for generative use
    cases. For example, while simple Naïve Bayes classifiers are commonly used to
    predict a given class based on the features in the dataset (that is, an application
    of conditional probability for discriminative use cases), the algorithm is also
    capable of learning an approximation of the joint probability distribution of
    the dataset during training due to how it applies Bayes’ Theorem with the (naïve)
    assumption that each feature is independent. (To learn more about how Naïve Bayes
    classifiers work, I recommend reviewing the paper at the following URL: [https://doi.org/10.48550/arXiv.1404.0933](https://doi.org/10.48550/arXiv.1404.0933)).
    A similar reappropriation can be done with more complex applications of Bayes’
    Theorem, such as Bayesian networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into specific GenAI approaches, I want to introduce two important
    concepts that will form the foundation for many of the topics that we will discuss
    throughout the remainder of this book, referred to as **embeddings** and **latent
    space**.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings and latent space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B18143_07.xhtml#_idTextAnchor215), we discussed the topic of
    dimensionality reduction, and we used mechanisms such as **principal component
    analysis** (**PCA**) to project features from our datasets into lower-dimensional
    feature spaces. These lower-dimensional feature spaces can be referred to as “latent
    spaces,” and the representations of our data in the latent space can be referred
    to as “embeddings.” Allow me to explain these important concepts in more detail,
    starting with embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Embeddings are numerical representations of data in a lower-dimensional space
    (that is, the latent space). They can be seen as numerical “fingerprints” that
    capture the meaning or characteristics of the original data. For example, word
    embeddings capture the meaning of words, and if words such as “king” and “queen”
    have similar embeddings, a language model can infer a relationship between the
    two. Models can also embed other types of data, such as images, audio, and graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s dive into the concept of latent space in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Latent space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the term “latent space” to define the abstract feature space where the
    intrinsic properties or features of a dataset are represented. This space captures
    underlying structure and patterns within the data that might not be apparent in
    its original form (hence the term “latent”).
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to understand that the latent space and its dimensions map in
    some way to the original features. This means that relationships among the original
    features are captured as relationships among the projected features in the latent
    space (that is, semantic context is represented in some way). These representations
    and mappings are generally learned by models during the training process and are
    therefore often not easily interpretable by humans, but there are also methods
    by which we can explicitly create embeddings for the contents of our datasets,
    which I will describe in more detail in [*Chapter 16*](B18143_16.xhtml#_idTextAnchor383).
    For now, I’ll briefly explain how embeddings and latent spaces are used.
  prefs: []
  type: TYPE_NORMAL
- en: Using embeddings and latent space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When models have created embeddings for our data, these representations in the
    latent space can be used in interesting ways. One of the most useful ways to use
    this data is to measure the distances between embeddings in the latent space,
    using familiar distance metrics such as Euclidean distance or cosine similarity.
    Considering that the embeddings in the latent space capture the essence of the
    concepts they represent, we can identify concepts that may be similar or related
    to the original space. An example would be products in a retail website’s product
    catalog. By embedding the product details and identifying which ones are near
    each other in the latent space, our models can get an understanding of which products
    may be related to each other. A recommender system could then use this information
    to display insights such as “customers who purchased this item also purchased
    these other items.”
  prefs: []
  type: TYPE_NORMAL
- en: You may ask, “Why not just perform the same kinds of actions in the original
    space?” The process of encoding embeddings converts the objects to vectors, providing
    a much more efficient way to represent the concepts. Also, rather than dealing
    with words and images, ML algorithms and models love to work with vectors, so
    these vectorized representations are more well suited to ML use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Another example that highlights the efficiency of embeddings is when we use
    ML models for image-processing use cases. Consider high-definition images that
    contain millions of pixels, in which each pixel is considered a feature. If we
    have a dataset consisting of hundreds of millions of images, and each image has
    millions of features, this could lead to extremely compute-intensive training
    and inference processing. Instead, mapping the features to a lower-dimensional
    feature space will significantly optimize processing efficiency. Also, individual
    pixels often don’t convey much meaning; rather, it’s often the relationships between
    pixels and patterns that define what an image represents.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll revisit the concepts of embeddings and the latent space many more times
    throughout the rest of this book because they are such foundational concepts in
    GenAI. Now that I’ve introduced these important concepts, we’ll begin our journey
    through various important milestones and approaches that led to the development
    of GenAI, as outlined at a high level in *Figure 15**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3: Milestones in GenAI evolution](img/B18143_15_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: Milestones in GenAI evolution'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, many additional GenAI models and approaches have been developed and
    released in the past few decades beyond those shown in *Figure 15**.3*, but here
    I’m focusing on high-level milestones and developments that have most notably
    influenced the evolution of GenAI. Let’s begin diving into each one, starting
    with Markov chains and HMMs.
  prefs: []
  type: TYPE_NORMAL
- en: Markov chains and HMMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of Markov chains was introduced by Andrey Markov in 1906, and they
    are based on what’s referred to as the Markov property, which posits that the
    next or future state in a sequence or scenario depends only on the current state.
    They can be used to predict (and therefore generate) the next item in a sequence.
    HMMs extend this concept to include hidden states that cannot be directly observed
    in a given scenario. A very simple example would be the potential correlation
    between current weather conditions and umbrella sales. If we can directly observe
    the weather, and we see that it is currently raining, then that’s an observable
    state we can use in a Markov chain to predict an increase in umbrella sales. On
    the other hand, if we cannot observe the weather for some reason (perhaps we’re
    working in a store that’s below ground level in a mall) but we notice an increase
    in umbrella sales, then we could surmise that it’s currently raining outside.
    In this case, the state of the weather is hidden, but we can speculate the probability
    of rain based on the secondary observable state of increased umbrella sales. This
    would be a very simple representation of a hidden state in an HMM.
  prefs: []
  type: TYPE_NORMAL
- en: The next concept I’d like to introduce briefly is RBMs.
  prefs: []
  type: TYPE_NORMAL
- en: RBMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Boltzmann machines** (**BMs**) are a type of **energy-based model** (**EBM**)
    that borrows some concepts from physics. “Boltzmann” refers to Ludwig Boltzmann,
    a physicist associated with statistical mechanics, which uses statistics and probability
    theory to model the behavior of microscopic particles. The Boltzmann probability
    distribution (also called the Gibbs distribution) provides the probability of
    a system being in a particular state based on its energy and temperature. A key
    concept here is that states with lower energy are more likely to occur than states
    with higher energy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mapping this to data science: Unlike the conditional probability used in traditional
    AI as described earlier in this chapter, EBMs use an “energy function” to assign
    a value to every possible configuration of variables, where lower “energy” values
    represent a more likely or desirable configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the work by Geoffrey Hinton and Ruslan Salakhutdinov, BMs were refined
    to form RBMs, which are a type of **artificial NN** (**ANN**) that consists of
    two layers:'
  prefs: []
  type: TYPE_NORMAL
- en: The visible layer, which represents the input data (for example, the pixels
    in an image or the words in a sentence).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hidden layer, which learns to capture higher-level features and dependencies
    in the data. This relates to the concept of “latent space” that we introduced
    earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With RBMs, the learning process involves adjusting the weights between the visible
    and hidden layers to minimize what’s referred to as the **reconstruction error**,
    which measures how well the RBM can reproduce the original input data. By doing
    this, the RBM learns to model the probability distribution of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: In the original BM, all of the layers were connected, and the models were quite
    complex and compute-intensive to train. However, in RBMs, while the visible layers
    and hidden layers are fully connected to each other, there are no connections
    within a layer (hence the term “restricted”), which makes them less computationally
    intensive to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'RBMs are commonly used for UL use cases, especially for feature extraction
    and dimensionality reduction, and they can also be seen as a kind of building
    block for **deep learning** (**DL**), which can be used to build many other kinds
    of models, including classification and regression. The idea of using RBMs as
    building blocks brings me to the next topic: DBNs.'
  prefs: []
  type: TYPE_NORMAL
- en: DBNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DBNs can be viewed as a composition of simple, unsupervised networks such as
    RBMs, where each sub-network’s hidden layer serves as the visible layer for the
    next sub-network. By stacking multiple RBMs, each layer can learn increasingly
    abstract and complex representations of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training DBNs consists of two main phases: pre-training and **fine-tuning**
    (**FT**). During pre-training, the DBN is trained one layer at a time in an unsupervised
    manner. Each layer is trained as an RBM, which learns to represent the features
    passed from the layer below. This layer-wise pre-training helps initialize the
    weights of the network in a way that makes the subsequent FT phase more effective.'
  prefs: []
  type: TYPE_NORMAL
- en: After pre-training, a DBN can sample from the representations it has learned
    to generate new data similar to the original dataset. We can also use **supervised
    learning** (**SL**) approaches to fine-tune the network for more specific applications.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the techniques and algorithms I’ve outlined so far in this chapter are
    what I would consider to be fundamental milestones and conceptual building blocks
    that contribute to the development of the more advanced GenAI applications we
    see today. The next two approaches I will outline are significant steps forward
    in the evolution of GenAI. The first I will introduce is the concept of AEs.
  prefs: []
  type: TYPE_NORMAL
- en: AEs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most – if not all – of the datasets we use for ML use cases represent specific
    concepts with intrinsic properties (or features); for example, people, cars, medical
    images, or even more specific concepts, such as people who boarded the Titanic
    or items purchased on a company’s website.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of discriminative ML use cases, the models generally try to predict
    some kind of output based on the values of the features associated with each instance
    of the concept being represented. For example, which customers will likely purchase
    a particular product based on their prior purchasing history?
  prefs: []
  type: TYPE_NORMAL
- en: In the case of GenAI, on the other hand, the models are often expected to generate
    new data points that validly represent the concept originally represented in the
    training dataset. For example, having been trained on images of many different
    concepts, such as cats and motorcycles, the model may be asked to draw a cartoon
    image of a cat on a motorcycle. To do this, the model needs to “understand” what
    those concepts are.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter, we introduced the topics of embeddings and the latent
    space, in which latent representations of the data points in our dataset are defined.
    The process of creating an embedding in the latent space can be referred to as
    **encoding**, and AEs are a type of ANN used for UL of efficient representations
    (encodings) for our datasets. At a high level, an AE takes input data, converts
    it into a smaller, dense representation that captures the essence of the data,
    and then reconstructs the input data as closely as possible from this representation.
    Let’s dive into how they work.
  prefs: []
  type: TYPE_NORMAL
- en: How AEs work
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'AEs are generally made up of three main components: the **encoder**, the latent
    space (also referred to as the **bottleneck**), and the **decoder**, as depicted
    in *Figure 15**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.4: AE architecture](img/B18143_15_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: AE architecture'
  prefs: []
  type: TYPE_NORMAL
- en: During the forward pass in an AE, each input data point is passed through the
    encoder part of the network. This section of the network compresses the input
    into a smaller, dense representation in the latent space (hidden layer). The encoded
    data is then passed through the decoder part of the network, and the decoder tries
    to reconstruct the original input data from the compressed representation. This
    is the interesting difference between AEs and traditional, discriminative models
    – that is, while traditional models generally try to predict an output (*Y*) based
    on an input (*X*), AEs try to predict (or generate) the original input (*X*).
  prefs: []
  type: TYPE_NORMAL
- en: Just as we did in traditional NNs, we can calculate the difference between the
    output value and the expected value (*X*), which is referred to as the **reconstruction
    error**, and then we can use backpropagation to update the network and continue
    the training cycle as usual. The quality of the reconstruction depends on how
    well the AE has learned to represent the data in the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: During this training process, the encoder learns to retain only the most relevant
    features of the data, effectively learning a new way to represent the input data
    in a smaller-dimensional space. The latent space then contains the compressed
    knowledge that the network has learned about the data – that is, the essence of
    the data. To again paraphrase a description by my colleague, Jeremy Wortz, the
    concept of bottleneck features relates to the quality of the embeddings; since
    we are typically “squeezing” our most critical information between the encoder
    and decoder, feature importance is implicitly optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Note, however, that while AEs can learn to reconstruct the original dataset
    and thereby learn how to perform dimensionality reduction to accurately represent
    the data in a lower-dimensional space, they are not designed to generate new data.
    To enable that functionality, we need to bring probability distributions into
    the image, and this is what has given rise to the development of **variational
    AEs** (**VAEs**), which I’ll describe next.
  prefs: []
  type: TYPE_NORMAL
- en: VAEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: VAEs extend the AE concept by introducing probabilistic approaches to enable
    the generation of new data points that are similar in structure to the training
    data. Building on what we’ve discussed about how AEs work, the easiest way to
    describe VAEs is to highlight their subtle differences from regular AEs.
  prefs: []
  type: TYPE_NORMAL
- en: As with traditional AEs, VAEs consist of an encoder and a decoder. However,
    rather than mapping input data to a fixed, deterministic vector as in regular
    AEs, the encoder in a VAE maps the input data to a probability distribution over
    the latent space. That’s a lot to take in, so let’s clarify that further.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned in the previous section, regular AEs encode the input data into
    a vector that contains latent features representing the original inputs. However,
    in the case of VAEs, instead of learning a single point in the latent space for
    each data sample, the encoder learns probability distributions. To be more precise,
    instead of outputting specific feature values in the latent space, the encoder
    outputs parameters (such as mean and standard deviation) that describe the probability
    distribution for features in the latent space. This distribution represents where
    the encoder believes the input data should be encoded. This approach introduces
    randomness or variability into the encoding process, which is important for potentially
    generating new data points that are similar to the input data, and not just reconstructions
    of the original data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the high-level steps that are performed when an input is
    passed through the VAE:'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder provides the parameters of a probability distribution within the
    latent space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A point is sampled from this learned distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This sampled point then gets passed through the decoder, which tries to reconstruct
    the input data from this probabilistic encoding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The goal, again, is to minimize the difference between the original input and
    its reconstruction, similar to regular AEs.
  prefs: []
  type: TYPE_NORMAL
- en: However, in addition to the reconstruction error, VAEs have an extra term in
    their loss function referred to as the **Kullback-Leibler** (**KL**) divergence,
    which measures how much the learned distributions for each input deviate from
    a standard normal distribution. Without getting into too much detail regarding
    the mathematics involved, the important thing to understand is that the KL divergence
    regularization enforces smoothness and continuity in the latent space, which makes
    the models more robust and can help reduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: We covered some complex details in this section, but the key takeaway is that
    VAEs introduce probabilistic mechanisms that enable them to go beyond simply reconstructing
    the input data and begin to generate new, similar data points.
  prefs: []
  type: TYPE_NORMAL
- en: We are now firmly getting into the realm of generative models, and the next
    approach I will outline is that of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We briefly introduced GANs in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245),
    and in this section, we will explore them in more detail. You may be familiar
    with the concept of “deepfakes,” in which AI is used to create photorealistic
    images or movies or audiorealistic tracks. By “photorealistic,” we mean synthetic
    data that looks like real photographs or video, and by “audiorealistic,” we mean
    synthetic data that sounds like real audio recordings. GANs are a popular mechanism
    for generating synthetic data that mimics real-world data. As with any technology,
    GANs could be used for malicious purposes, such as generating deepfakes of real
    people without their consent, but they have many useful applications that we will
    cover shortly. However, let’s first dive into what GANs are and how they work.
  prefs: []
  type: TYPE_NORMAL
- en: High-level concepts – the generator and the discriminator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The name “generative adversarial networks” (Goodfellow et al., 2014) may sound
    somewhat abstract, but it perfectly describes the concepts used in this approach
    to GenAI. In short, GAN implementations consist of two NNs that work in an adversarial
    manner (that is, they work against each other), in which one of the networks generates
    synthetic data, and the other network tries to ascertain whether the data is real
    or not. The network that generates the data is called, not surprisingly, the **generator**,
    and the network that tries to ascertain the realness of the data is called the
    **discriminator**. The main premise is that the generator tries to fool the discriminator
    into believing that the data is real.
  prefs: []
  type: TYPE_NORMAL
- en: One could imagine that having an effective generator is the most important requirement
    for a GAN, but it is also essential to have an effective discriminator because
    the discriminator can be seen as the **quality control** (**QC**) mechanism for
    generated data. If you have an ineffective discriminator, it could easily be fooled
    by data that does not accurately mimic real data. The more effective your discriminator
    is at identifying fake data, the harder your generator will need to work to create
    realistic data. Therefore, both sides of the adversarial partnership need to be
    trained effectively in order for the GAN to create high-quality data.
  prefs: []
  type: TYPE_NORMAL
- en: The analogy that is often used to describe this process is to imagine a person
    who wants to create forgeries of expensive art pieces (that is, a forger) and
    another person who is dedicated to identifying whether a piece of art is real
    or a forgery (that is, an art expert). In the beginning, the forgeries might be
    amateur attempts that are easy to identify as fakes. However, as the forger refines
    their work and learns to create more convincing forgeries, the art expert must
    become more skilled at identifying minute nuances that distinguish real art from
    fake art pieces. This cycle then repeats until, ideally, the generator creates
    data that is indistinguishable from the real data.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper – GAN training process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a GAN, both networks are usually **convolutional NNs** (**CNNs**). The discriminator
    is usually a binary classifier that classifies the data as either real or fake.
    This means that its training process involves many of the same steps that are
    already familiar to us from earlier chapters in this book, in which we trained
    classifier models.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s imagine that we want to generate images of cats. The discriminator
    could learn to identify images of cats by being trained on a labeled dataset.
    In this dataset, some of the inputs are real images of cats, and they are labeled
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: As we briefly mentioned in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245),
    the generator and the discriminator engage in a kind of adversarial game. In this
    game, the objective of the discriminator is to *minimize* the error rate for classifying
    images of cats (that is, it wants to accurately identify images of cats as much
    as possible). On the other hand, the generator is trying to *maximize* the discriminator’s
    error rate. Due to the opposing objectives of each of the game’s participants,
    where one participant wants to minimize a certain metric and the other participant
    wants to maximize that same metric, this is referred to as a **minimax** game.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to training the discriminator on the labeled training dataset,
    the discriminator also receives outputs from the generator and is asked to classify
    those outputs, as depicted in *Figure 15**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.5: GAN (source: https://developers.google.com/machine-learning/gan/gan_structure)](img/B18143_15_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: GAN (source: https://developers.google.com/machine-learning/gan/gan_structure)'
  prefs: []
  type: TYPE_NORMAL
- en: If the discriminator thinks a data point is real when, in fact, it was created
    by the generator, that counts as an error. This, therefore, increases the error
    rate for the discriminator, which is a win for the generator. Conversely, if the
    discriminator accurately classifies the data point as fake, then it reduces the
    error rate for the discriminator (which is, of course, a win for the discriminator).
  prefs: []
  type: TYPE_NORMAL
- en: While the discriminator uses a typical mechanism such as gradient descent to
    learn from its mistakes and minimize the loss, the generator can also learn from
    the discriminator’s mistakes, and adjust its weights in a reverse process that
    aims to increase the discriminator’s error rate. This is the novel approach used
    by GANs, in which the generator doesn’t receive direct labels of “good” or “bad”
    during training, but instead receives feedback on the discriminator’s performance.
    If the discriminator accurately identifies a fake, then the gradients are sent
    back to the generator so that it can update its weights to create an output that
    better fools the discriminator in the next round.
  prefs: []
  type: TYPE_NORMAL
- en: During this process, the generator learns the underlying probability distribution
    of the real data and becomes more adept at generating new data that aligns with
    that probability distribution (that is, new data with similar properties as the
    real data). It’s also important to understand that each network takes turns in
    the training process. When the generator is being trained, the discriminator’s
    weights are frozen, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: By pitting the models against each other in this fashion, they must continually
    get better in order to outperform each other. As the discriminator gets better
    at identifying fakes, the generator learns to generate more realistic data to
    fool the discriminator, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: GANs gained a lot of popularity since they were first created by Ian Goodfellow
    and colleagues in 2014, and multiple different types of GAN implementations have
    emerged, such as **conditional GANs** (**cGANs**), CycleGANs, StyleGANs, **deep
    convolutional GANs** (**DCGANs**), and progressive GANs. A comprehensive discussion
    of all of these variants would constitute more detail than is necessary in this
    book. I encourage you to research these variants further if you have a specific
    interest in them.
  prefs: []
  type: TYPE_NORMAL
- en: Important note – look out for “mode collapse”
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the typical challenges faced when training many types of NN architectures,
    such as exploding and vanishing gradients, GANs also often suffer from a specific
    challenge that is related to their adversarial training process. This problem
    is called “mode collapse,” and it refers to a situation in which the generator
    may start producing a limited variety of outputs, especially if those outputs
    successfully trick the discriminator. Since the generator’s primary goal is to
    fool the discriminator, it may learn to generate patterns that are effective in
    doing so but do not accurately represent the target data distribution. This is
    an ongoing area of research, in which new mechanisms are being introduced to combat
    this phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss some common applications of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of GANs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to generating photorealistic images and audiorealistic data, GANs
    can be used for other interesting use cases, such as style transfer and image-to-image
    translation. For example, some online services enable you to upload a photo and
    convert it into a cartoon or make it look like an oil painting in the style of
    a famous artist such as Van Gogh. While these are fun applications, GANs are also
    utilized in various use cases such as text generation, new drug discovery, and
    other types of synthetic data generation.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll discuss the importance of high-quality synthetic data later, but next,
    I’d like to introduce another popular GenAI approach, referred to as **diffusion**.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Diffusion is utilized for many of the same kinds of use cases as GANs, such
    as image and audio generation, image-to-image translation, style transfer, and
    even new drug discovery. However, diffusion uses a different approach and often
    provides more favorable results for some use cases. We’ll dive into some of the
    differences between diffusion models and GANs in this section, but first, as always,
    let’s learn about what diffusion is and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: High-level concepts – noising and denoising
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At a high level, diffusion consists of two main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise to images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reversing the process – that is, removing noise to work back toward the original
    image
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I’ll describe these concepts in much more detail, starting with clarifying
    what it means to add noise to an image. *Figure 15**.6* shows an image consisting
    of noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.6: Image of noise (source: https://commons.wikimedia.org/wiki/File:256x256_Dissolve_Noise_Texture.png)](img/B18143_15_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: Image of noise (source: https://commons.wikimedia.org/wiki/File:256x256_Dissolve_Noise_Texture.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Some of you may be familiar with the kind of image shown in *Figure 15**.6*
    if you’ve ever seen noise displayed on a TV screen when it is not tuned to a channel
    that provides a image signal. This image consists purely of noise, in which we
    cannot make out any discernable shapes. However, if we were to take a high-quality
    image and add some noise to it, then it would become somewhat blurry, but we could
    still make out what’s in the image, as long as not too much noise is added. The
    more noise we add, the more difficult it becomes to identify the contents of the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: This concept is used in the **noising** process when training diffusion models.
    We take images and add small amounts of noise to them to make it progressively
    more difficult to identify what’s in the images. This is also referred to as the
    **forward** **diffusion** process.
  prefs: []
  type: TYPE_NORMAL
- en: The **reverse diffusion** or **denoising** process then tries to take a noisy
    image and work back toward the original image. This might sound a bit pointless
    – that is, why bother adding noise to images and then trying to train a model
    to learn how to remove noise to generate the original images? Well, just as in
    the case of GANs, we are training our model to understand the probability distribution
    of the original dataset. Diving into a bit more detail, our model actually learns
    to predict noise in the input. The next step of the process, then, is to simply
    remove that noise from the input in order to estimate or generate the desired
    denoised output.
  prefs: []
  type: TYPE_NORMAL
- en: The reason this is called diffusion may be somewhat self-explanatory, but more
    specifically, the name relates to a concept in the field of non-equilibrium thermodynamics.
    That’s a bit of a mouthful, and, don’t worry – we’re not going to dive into the
    physical concept except to briefly introduce why it is named as such.
  prefs: []
  type: TYPE_NORMAL
- en: The analogy often used to explain this concept is to think of a drop of ink
    being added to a bucket of water. When the ink is added to the water, at first,
    it occupies a small space in a specific location in the bucket. However, over
    time, the ink dissipates throughout the bucket of water. Soon, it has spread throughout
    the bucket, and the bucket then contains a mixture of water and ink. The color
    of the contents of the bucket may be different from the color that existed before
    you added the ink, but it’s no longer possible to pinpoint the specific location
    of the ink in the bucket. This is the forward diffusion process, in which the
    ink diffuses throughout the water in the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: In the physical world, this process of diffusion is usually impossible to reverse
    – that is, no matter what you try to do, you will not be able to separate the
    ink from the water and condense the drop of ink back into the original position
    it occupied when it was first added to the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of diffusion in ML, however, we try to reverse the process and get
    back to the original input state.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It’s important to understand that noise is added to the data in a controlled
    manner. We don’t simply add completely random noise, but instead, we add what
    is referred to as **Gaussian noise**, meaning the noise is characterized by the
    “normal” (or “Gaussian”) probability distribution. As a refresher, a Gaussian
    distribution is represented by the familiar “bell curve” as depicted in *Figure
    15**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.7: Gaussian distribution](img/B18143_15_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.7: Gaussian distribution'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 15**.7*, the Gaussian distribution is symmetrical,
    with its mean at the center of the bell curve. The width of the curve represents
    the variance (or its square root, the standard deviation) of the distribution
    – that is, how far from the mean we are likely to see data points that fit the
    distribution. In a Gaussian distribution, data points that are most likely to
    occur appear closer to the mean, while rarer data points are further from the
    mean. In *Figure 15**.7*, the mean is 0, and the standard deviation is 1, which
    is a special type of Gaussian distribution referred to as the **standard normal
    distribution**. By using normally distributed noise, we can easily change the
    intensity of noise by tweaking the variance of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to controlling noise by ensuring it fits the normal distribution,
    we also introduce noise in a controlled manner by incrementally adding noise to
    our source images during training, rather than adding too much noise all at once.
    The addition of noise in this manner is performed according to a **schedule**,
    in which more noise is added at different time intervals in the process. Remember
    that, as we discussed in earlier sections of this chapter, randomization is important
    in generative processes because we don’t just want to produce exact images from
    the original dataset but instead want to produce similar images.
  prefs: []
  type: TYPE_NORMAL
- en: To introduce some controllable randomization into the process, the mean and
    the variance of noise added in each step are different, and we also add different
    amounts of noise at various parts of the schedule. Each time we add noise, we
    form a step in a Markov chain. As you may remember from earlier in this chapter,
    each step in a Markov chain is dependent only on the step that directly precedes
    it, and this is important to understand in the context of the reverse diffusion
    process. As we add noise in each step (during the forward diffusion process),
    we are training our model to identify (or predict) noise that has been added.
    Then, during the reverse diffusion process, we start with a noisy image, and we
    want to try to generate the image in the previous step in the Markov chain and
    progressively work our way through the chain in that manner until we get back
    to an approximation of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: To describe this process in more detail, imagine that we start with a image
    of a cat, and we progressively add noise in each step until we finally end up
    with a image similar to the image depicted in *Figure 15**.6*, which consists
    almost purely of noise. It would be very difficult to try to jump from such an
    extremely noisy image back to a image of a cat, so we instead train our model
    on how to progressively work backward through each step in the Markov chain, predicting
    the small amount of noise that was added in each step. As our model gets better
    and better, it can more clearly distinguish between noise and the underlying probability
    distribution of the original dataset. This understanding of probability distributions
    allows us to sample new images by starting with pure noise and iteratively denoising
    them according to what the model has learned.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of this section, I promised that I would outline some important
    differences between diffusion and GANs. Let’s take a look at those differences
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Differences between diffusion and GANs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: I mentioned the concept of mode collapse in the context of training GANs and
    how that can introduce instability into the training process. Diffusion uses a
    more stable training process, but diffusion models require many steps to generate
    data, which can be computationally intensive. Some recent advances aim to reduce
    the number of required steps and computational costs, but this is still a notable
    consideration for diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, at inference time, generating samples from diffusion models can be
    computationally expensive, whereas GANs can generate samples more quickly because
    they require only a single forward pass through the generator network.
  prefs: []
  type: TYPE_NORMAL
- en: Which approach is best, then? Well, it’s a matter of selecting the right tool
    for the job based on the business requirements for a specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, it’s time to move on to discussing perhaps the crowning glory of the
    GenAI world in recent times: LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is another case in which the name of the technology very accurately describes
    what the technology is; LLMs are large models that are particularly useful for
    language-based use cases, such as the summarization of large amounts of textual
    data, or chatbots that can have a conversation with a human.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, they use statistical methods to process, predict, and generate
    language based on the context provided to them. They are trained on diverse datasets
    that include large amounts of textual information, from books and articles to
    websites and human interactions, enabling them to learn language patterns, grammar,
    and semantics.
  prefs: []
  type: TYPE_NORMAL
- en: How large are they? Well, some of the latest models at the time of writing this
    in February 2024 consist of billions or even trillions of parameters. That’s pretty
    huge! Somebody might read this book 20 years from now and laugh at the fact that
    we considered these sizes to be huge, but these are the biggest models on the
    planet at the moment, and they constitute an enormous advancement from any models
    that have existed before now.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into details on how LLMs are created, let’s take a look at some
    historical milestones that have led to their evolution.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The history of language models began with simple rule-based systems that used
    hand-coded rules to interpret and generate language. It turns out that trying
    to hand-code all of the complex rules of human language is quite impractical,
    but the research in this field had to start somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Next, statistical methods such as N-gram models used probabilities to predict
    the likelihood of a sequence of words. They were trained on large bodies of text
    and could capture more language nuances than rule-based systems, but they still
    struggled with long-term dependencies in text.
  prefs: []
  type: TYPE_NORMAL
- en: The next step forward was to use models such as HMMs and simple NNs for language
    tasks, and while these models offered better performance by learning more advanced
    patterns in data, they were still limited in complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Major breakthroughs began when scientists started applying **deep NNs** (**DNNs**)
    to language use cases. In [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245) and
    [*Chapter 14*](B18143_14.xhtml#_idTextAnchor348), we discussed the evolution of
    RNNs, LSTM networks, and Transformers, and how they each enabled progressively
    more complex language processing. The pivotal event that occurred in the industry
    was when Google invented the Transformer architecture (Vaswani et al., 2017),
    which has become the primary technology used in all of the biggest and most advanced
    LLMs in the industry today.
  prefs: []
  type: TYPE_NORMAL
- en: When we discussed the overall evolution of AI/ML in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    I mentioned that it was not only the development of more complex ML algorithms
    that led to the kinds of breakthroughs we’ve seen in recent decades but also advancements
    in computing power and the proliferation of available data for training models.
    These factors all work together to form a kind of ecosystem in which we continue
    to advance all of these technologies in concert.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll dive into how LLMs are created.
  prefs: []
  type: TYPE_NORMAL
- en: Building LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s get one thing out of the way, right from the beginning; it’s unlikely
    that you or I could create our own LLMs from scratch, for two main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Training LLMs requires obscenely large amounts of data. Some of the LLMs you
    or I would interact with are trained on the entire internet, for example. This
    is one of the things that makes them so powerful and knowledgeable; they are trained
    on all publicly available data that humans have ever created, in addition to some
    private and proprietary datasets owned or sourced by the companies that trained
    them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is generally extremely expensive to train LLMs due to the sheer amount of
    computing power needed. I’m referring to using thousands of high-performance accelerators,
    such as the latest and greatest GPUs and TPUs, for months on end to train these
    models, and those things don’t come cheap!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, most people and companies will use LLMs that have already been
    **pre-trained**, which I will describe next.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM training process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While the companies that build commercially available LLMs almost certainly
    use a lot of secret magic behind the scenes that they are unlikely to share externally,
    the following high-level steps are generally involved in the LLM training process:'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised or semi-supervised pre-training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at each of these in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised or semi-supervised pre-training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Remember from our activities earlier in this book that supervised training requires
    labeled datasets, which can be cumbersome and expensive to source or create. Bearing
    in mind that LLMs are often trained on enormous bodies of data such as the entire
    internet, it would be impossible to label all of that data. Instead, a very clever
    trick is used to train LLMs in such a way that they can learn from these large
    bodies of text without the need for explicit labeling, and that trick is called
    **masked language** **modeling** (**MLM**).
  prefs: []
  type: TYPE_NORMAL
- en: 'MLM is actually a very simple yet effective concept; it’s essentially a game
    of “fill in the blanks.” You may remember playing this game as a child or encountering
    it in exam scenarios. Quite simply, we take a sentence and blank out (or mask)
    one or more of the words, and the task is then to guess what the missing words
    should be. For example, consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: “You really hit the nail on the [BLANK] with that last comment.”
  prefs: []
  type: TYPE_NORMAL
- en: What do you think would be the most appropriate word to use for filling in the
    *[BLANK]* portion of that sentence? If you’re familiar with the term, “hit the
    nail on the head,” then you may have guessed that “head” would be the best word
    to use, and you’d be correct in guessing that.
  prefs: []
  type: TYPE_NORMAL
- en: This approach can be extremely effective in training LLMs because we can feed
    millions of sentences to the LLM, randomly masking out various words and asking
    the LLM to predict what the correct words should be. The beauty of this approach
    is that the masked-out words become the labels, so we don’t need to explicitly
    label the dataset. This is why we refer to this process as UL or **semi-supervised
    learning** (**SSL**). For example, after the LLM predicts the word to use, we
    can reveal the word that the sentence actually contained in the masked-out position,
    and the model can then learn from that. If the model predicted a different word,
    it would count as an error, and the loss function would reflect this accordingly
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important thing to note is that the model gets to see billions of different
    sentences during training, and by doing so, it sees words being used in various
    contexts. Over time, it builds up an understanding of what each word means and
    which other words it commonly appears alongside. Conceptually, it can build up
    a graph of how various words relate to each other, as depicted in *Figure 15**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.8: Graph of word associations](img/B18143_15_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.8: Graph of word associations'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 15**.8*, we can see how various words in the English language relate
    to each other. Notice how some words may be ambiguous, such as the word “Turkey,”
    which could relate to the country, to the animal, or to food. Consider the following
    sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: “I really like eating turkey.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Last year, we went on vacation in Turkey.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “I really liked the food in Turkey.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assume that the LLM has seen all of those sentences during training. By seeing
    these sentences (and billions more) and learning how words are used in various
    contexts, the LLM forms a probabilistic understanding of the meaning (or **semantic
    context**) of the words. For example, how does it know that the word “Turkey”
    might refer to a country based on a sentence about people going on vacation there?
    Well, from seeing the word “vacation” in millions or billions of sentences, it
    comes to understand that people go on vacation in places, so in this context,
    “Turkey” must be a place. It also would have seen other sentences containing the
    words “Turkey” and “immigration,” and it would have developed an understanding
    that immigration refers not only to places but, more specifically, to countries.
    It would also have learned what the word “country” means and that its plural form
    is “countries,” based on seeing those words in many other contexts. Similarly,
    it learns that the word “eat” refers to food, so in the sentence “I really like
    eating turkey,” it understands that “turkey” must be a type of food.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve presented just a handful of simple sentences here, and we can already
    see a proliferation of different potential combinations of words and how they
    relate to each other. Imagine the complexity of the combinations that exist when
    we bring every sentence on the internet into scope. This is what LLMs learn: extremely
    complex webs of associations and the underlying meaning of the concepts those
    words represent. This is an enormous step forward in terms of intelligence that
    LLMs bring to the AI industry, and this is actually how our own minds learn to
    understand words, also, which is fascinating. As children, we don’t just learn
    individual words, but we learn words as representations of concepts, usually by
    forming graphs of associations. For example, imagine you were a young child who
    was trying to climb up a tree, and your parent said, “Don’t climb that tree, or
    you might fall and injure yourself.” You may not have heard all of those words
    before, but from hearing that sentence, you might learn about the concept of a
    tree, or of climbing, or falling, or injuring, and you also may intuit that falling
    and injuring are undesirable things to happen.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to masking words within sentences, another method called **next
    sentence prediction** (**NSP**) may also be used to train our LLMs to not only
    predict words but, as the name suggests, also to predict entire sentences based
    on the provided context. In this case, the LLM is presented with pairs of sentences,
    and sometimes these sentences have a natural sequential relationship (for example,
    two consecutive lines from an article), or in other cases, they are unrelated
    sentences. The LLM’s task is to determine if the second sentence logically follows
    from the first, and by doing this, it learns to discriminate between logical and
    incoherent sequences of sentences. This pushes the LLM to reason about context
    across multiple sentences, which helps it generate coherent text in longer responses
    to requests.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that there’s an additional layer of abstraction in how
    the LLMs learn. Earlier in this chapter, we discussed embeddings and latent space.
    Generally, the kinds of associations I’ve just outlined occur in the latent space.
    The latent space is the LLM’s representation of the world and all of the various
    concepts and associations it learns. Concepts with similar meaning or semantic
    context may be located nearer to each other in the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: After the LLM has learned everything it can learn from the available data via
    the process I’ve just described, the pre-training phase is complete. At this point,
    the LLM has built its latent space representation of the world, and the next step
    is to teach it how to be useful in responding to requests from humans or other
    machines. Many different processes can be used for this purpose, but I’ll start
    with the common practice of supervised tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised tuning of LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we know by now, supervised training means that we have a dataset that contains
    labels that can be used to teach a model. For example, if we want our model to
    perform sentiment analysis, we can label phrases in the dataset with the sentiments
    expressed by those phrases, such as positive, negative, or neutral. Similarly,
    for summarization, we could show the model examples of good summarizations, and
    the model could learn from those examples.
  prefs: []
  type: TYPE_NORMAL
- en: While enormous datasets are usually needed for the pre-training phase, supervised
    tuning of LLMs requires smaller datasets that are carefully curated and labeled
    for the particular task at hand. Considering that LLMs usually contain a lot of
    knowledge from the pre-training phase, surprisingly good tuning results can come
    from just a few (or perhaps a few hundred) examples in the tuning dataset. Again,
    this is similar to how humans incrementally learn new skills. For example, imagine
    there are two people, one of whom has never learned to drive, and another who
    has been driving cars for 20 years. Now, we want each of those people to learn
    how to drive a large truck by next week. Who is most likely to succeed? The person
    who has been driving cars for 20 years already has a lot of knowledge of the rules
    of the road and how to operate vehicles, so learning how to drive a large truck
    will just require a relatively small amount of incremental learning in comparison
    to the person who has never driven any vehicles before.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to understand that there are various approaches or levels
    of tuning that we can apply. For example, **few-shot learning** (**FSL**) refers
    to a practice in which we provide just a few examples to the LLM. We could even
    provide these examples along with our prompt (that is, our request to the LLM)
    rather than needing to provide a dedicated training dataset. This can be effective
    for some tasks, but, of course, it is quite limited since we are only providing
    a few examples. When examples are provided along with a prompt, this is an example
    of prompt engineering, which is an emerging science in itself that focuses on
    how to craft requests to LLMs in ways that produce the best results. In these
    cases, the underlying LLM generally does not get retrained on the examples we
    provide. Instead, the LLM simply uses the examples provided to tailor its responses
    in alignment with those examples.
  prefs: []
  type: TYPE_NORMAL
- en: On the other side of the spectrum is **full FT** (**FFT**). This level of tuning
    could require thousands of examples, and the model’s parameters get updated based
    on those examples (that is, the model learns and incorporates the new data into
    its core structure).
  prefs: []
  type: TYPE_NORMAL
- en: Then, there are levels in between, such as **adapter tuning**, in which **adapter
    layers** are inserted into the original model, and only those layers get tuned
    and updated while the original model weights are frozen. Another type of tuning
    may focus only on updating the output layer of the model while leaving the rest
    of the model untouched.
  prefs: []
  type: TYPE_NORMAL
- en: On a slightly separate but related note, I want to briefly introduce the topic
    of **reinforcement learning from human feedback** (**RLHF**). Rather than updating
    the LLM with more data, this method mainly focuses on teaching the LLM to respond
    in ways that are more favorable to humans, such as aligning with human values.
    In this approach, the LLM may provide multiple responses to a prompt, and a human
    can select (and therefore label) which response is preferred, based on nuances
    of human communication and culture, such as tone, sentiment, ethical considerations,
    and many other factors. We introduced the concept of **reinforcement learning**
    (**RL**) in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015). In the case of RLHF
    for tuning LLMs, the human feedback is used as the reward signal, teaching the
    LLM to generate outputs that better align with human preferences, even if there
    might be multiple technically “correct” ways to respond.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore tuning in more detail in the next chapter, but before we move
    on to that chapter, let’s reflect on what we’ve learned so far.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by introducing the fundamental topics that underpin
    GenAI, including concepts such as embeddings and latent space. We then described
    what GenAI is and how it contrasts against “traditional AI,” whereby traditional
    AI typically tries to predict a specific answer, such as a revenue forecast based
    on historical data or identifying whether a image contains a cat, but GenAI goes
    beyond those kinds of tasks and creates new content.
  prefs: []
  type: TYPE_NORMAL
- en: We dived into the role of probability in GenAI versus traditional AI, and we
    discussed how traditional AI often uses conditional probability to predict the
    values of a target variable based on the values of features in the dataset. On
    the other hand, GenAI approaches typically try to learn the joint probability
    distribution of both the features and the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we explored the evolution of GenAI and various model development milestones
    that led to the kinds of models we use today. We began this exploration by covering
    early developments such as Markov chains, HMMs, RBMs, and DBNs. We then covered
    more recent developments such as AEs, GANs, and diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we dived into the world of LLMs. We highlighted major milestones in
    their evolution by refreshing what we had learned in previous chapters about RNNs,
    LSTM networks, and Transformers. We then discussed how LLMs are trained, including
    the kinds of mechanisms used in the pre-training phase, such as MLM and NSP, and
    how FFT can be used to further train the models for specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Although this is an introduction chapter for the GenAI section of our book,
    we’ve covered a lot of important concepts in a considerable amount of depth, and
    this will give us a strong foundation for the remaining chapters.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, let’s continue our journey into the world of GenAI.
  prefs: []
  type: TYPE_NORMAL
