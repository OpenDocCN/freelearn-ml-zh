- en: '*Chapter 14*: Conclusions and Next Steps'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第14章*: 结论与下一步'
- en: Congratulations on finishing this book! You have been introduced to a lot of
    interesting concepts, methods, and implementations related to hyperparameter tuning
    throughout the previous chapters. This chapter summarizes all the important lessons
    learned in the previous chapters, and will introduce you to several topics or
    implementations that you may benefit from that we have not covered yet in this
    book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成这本书！在前面的章节中，你已经接触到了许多与超参数调整相关的有趣的概念、方法和实现。本章总结了前几章中学到的重要课程，并将介绍一些你可能从中受益的话题或实现，这些内容我们在这本书中没有涉及。
- en: 'The following are the main topics that will be discussed in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下主要内容：
- en: Revisiting hyperparameter tuning methods and packages
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新审视超参数调整方法和包
- en: Revisiting HTDM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新审视HTDM
- en: What’s next?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: Revisiting hyperparameter tuning methods and packages
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视超参数调整方法和包
- en: Throughout this book, we have discussed four groups of hyperparameter tuning
    methods, including exhaustive search, Bayesian optimization, heuristic search,
    and multi-fidelity optimization. All the methods within each group have similar
    characteristics to each other. For example, manual search, grid search, and random
    search, which are part of the exhaustive search group, all work by exhaustively
    searching through the hyperparameter space, and can be categorized as uninformed
    search methods.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们讨论了四组超参数调整方法，包括穷举搜索、贝叶斯优化、启发式搜索和多保真优化。每组方法中的所有方法都具有相似的特征。例如，手动搜索、网格搜索和随机搜索，它们都是穷举搜索组的一部分，都是通过穷举搜索超参数空间来工作的，可以归类为无信息搜索方法。
- en: Bayesian optimization hyperparameter tuning methods are categorized as informed
    search methods, where all of them work by utilizing both surrogate model and acquisition
    function. Hyperparameter tuning methods, which are part of the heuristic search
    group, work by performing trial and error. As for hyperparameter tuning methods
    from the multi-fidelity optimization group, they all utilize the cheap approximation
    of the whole hyperparameter tuning pipeline, so that we can have similar performance
    results with much lesser computational cost and faster experiment time.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化超参数调整方法被归类为信息搜索方法，其中所有方法都是通过利用代理模型和获取函数来工作的。启发式搜索组中的超参数调整方法通过试错来工作。至于多保真优化组中的超参数调整方法，它们都利用整个超参数调整管道的廉价近似，这样我们就可以以更少的计算成本和更快的实验时间获得相似的性能结果。
- en: 'The following table summarizes all of the hyperparameter tuning methods discussed
    in this book, along with the supported packages:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了本书中讨论的所有超参数调整方法，以及支持的包：
- en: '![Figure 14.1 – Hyperparameter tuning methods and packages summary'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.1 – 超参数调整方法和包总结'
- en: '](img/B18753_14_table_001(a).png)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_14_table_001(a).png)'
- en: '![Figure 14.1 – Hyperparameter tuning methods and packages summary'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.1 – 超参数调整方法和包总结'
- en: '](img/B18753_14_table_001(b).jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_14_table_001(b).jpg)'
- en: Figure 14.1 – Hyperparameter tuning methods and packages summary
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 – 超参数调整方法和包总结
- en: In this section, we have revisited all of the hyperparameter tuning methods
    and packages discussed throughout the book. In the next section, we will revisit
    the HTDM.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了本书中讨论的所有超参数调整方法和包。在下一节中，我们将重新审视HTDM。
- en: Revisiting HTDM
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视HTDM
- en: 'The **Hyperparameter Tuning Decision Map** (**HTDM**) is a map that you can
    use to help you decide which hyperparameter tuning method should be adopted in
    a particular situation. We discussed in detail how you can utilize HTDM, along
    with several use cases, in [*Chapter 11*](B18753_11_ePub.xhtml#_idTextAnchor110),
    *Introducing the Hyperparameter Tuning Decision Map*. Here, we will only revisit
    the map, as shown in the following figure:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数调整决策图**（**HTDM**）是一个你可以用来帮助你决定在特定情况下应该采用哪种超参数调整方法的图。我们在[*第11章*](B18753_11_ePub.xhtml#_idTextAnchor110)，“介绍超参数调整决策图”中详细讨论了如何利用HTDM，以及一些用例。在这里，我们只重新审视一下以下图所示的地图：'
- en: '![Figure 14.2 – Hyperparameter Tuning Decision Map'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.2 – 超参数调整决策图'
- en: '](img/B18753_14_002.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_14_002.jpg)'
- en: Figure 14.2 – HTDM
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 – HTDM
- en: In this section, we have revisited the HTDM. In the next section, we’ll discuss
    other topics you may find interesting to further boost your hyperparameter tuning
    knowledge.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了HTDM。在下一节中，我们将讨论其他你可能感兴趣的主题，以进一步加深你的超参数调优知识。
- en: What’s next?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: Even though we have discussed a lot of hyperparameter tuning methods and their
    implementations in various packages, there are several important concepts you
    may need to know about that have not been discussed in this book. As for the hyperparameter
    tuning method, you can also read more about the **CMA-ES** method, which is part
    of the heuristic search group ([https://cma-es.github.io/](https://cma-es.github.io/)).
    You can also read more about the **meta-learning** concept to further boost the
    performance of your Bayesian optimization tuning results ([https://lilianweng.github.io/posts/2018-11-30-meta-learning/](https://lilianweng.github.io/posts/2018-11-30-meta-learning/)).
    It is also worth noting that we can combine the manual search method with other
    hyperparameter tuning methods to boost the efficiency of our experiments, especially
    when we already have prior knowledge about the good range of the hyperparameter
    values.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经讨论了许多超参数调优方法和它们在各种包中的实现，但还有一些重要的概念你可能需要了解，这些概念在本书中没有讨论。至于超参数调优方法，你还可以了解更多关于**CMA-ES**方法的信息，它是启发式搜索组的一部分([https://cma-es.github.io/](https://cma-es.github.io/))。你还可以了解更多关于**元学习**概念的信息，以进一步提高你的贝叶斯优化调优结果的表现([https://lilianweng.github.io/posts/2018-11-30-meta-learning/](https://lilianweng.github.io/posts/2018-11-30-meta-learning/))。还值得注意的是，我们可以将手动搜索方法与其他超参数调优方法相结合，以提高我们实验的效率，尤其是在我们已经对超参数值的良好范围有先验知识的情况下。
- en: As for the packages, you can also learn more about the **HpBandSter** package,
    which implements the Hyper Band, BOHB, and random search methods ([https://github.com/automl/HpBandSter](https://github.com/automl/HpBandSter)).
    Finally, there are also several packages that automatically create a scikit-learn
    wrapper from the non-scikit-learn model. For example, you can utilize the **Skorch**
    package to create scikit-learn wrappers from PyTorch models ([https://skorch.readthedocs.io/en/stable/](https://skorch.readthedocs.io/en/stable/)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于包，你还可以了解更多关于**HpBandSter**包的信息，该包实现了超带宽、BOHB和随机搜索方法([https://github.com/automl/HpBandSter](https://github.com/automl/HpBandSter))。最后，还有一些包可以自动从非scikit-learn模型创建scikit-learn包装器。例如，你可以使用**Skorch**包从PyTorch模型创建scikit-learn包装器([https://skorch.readthedocs.io/en/stable/](https://skorch.readthedocs.io/en/stable/))。
- en: Summary
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have summarized all the important concepts discussed throughout
    all chapters in this book. You have also been introduced to several new concepts
    that you may want to learn to further boost your hyperparameter tuning knowledge.
    From now on, you will have the skills you need to take full control over your
    machine learning models and get the best models for the best results via hyperparameter
    tuning experiments.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们总结了本书所有章节中讨论的所有重要概念。你还被介绍了几种你可能想要学习以进一步加深你的超参数调优知识的新概念。从现在开始，你将拥有控制你的机器学习模型并通过超参数调优实验获得最佳模型以获得最佳结果所需的技能。
- en: Thanks for investing your interest and time in reading this book. Best of luck
    on your hyperparameter tuning learning journey!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你投入兴趣和时间阅读这本书。祝你在超参数调优学习之旅中好运！
