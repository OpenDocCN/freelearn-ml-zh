- en: '*Chapter 14*: Conclusions and Next Steps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on finishing this book! You have been introduced to a lot of
    interesting concepts, methods, and implementations related to hyperparameter tuning
    throughout the previous chapters. This chapter summarizes all the important lessons
    learned in the previous chapters, and will introduce you to several topics or
    implementations that you may benefit from that we have not covered yet in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the main topics that will be discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting hyperparameter tuning methods and packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting HTDM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s next?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting hyperparameter tuning methods and packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we have discussed four groups of hyperparameter tuning
    methods, including exhaustive search, Bayesian optimization, heuristic search,
    and multi-fidelity optimization. All the methods within each group have similar
    characteristics to each other. For example, manual search, grid search, and random
    search, which are part of the exhaustive search group, all work by exhaustively
    searching through the hyperparameter space, and can be categorized as uninformed
    search methods.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian optimization hyperparameter tuning methods are categorized as informed
    search methods, where all of them work by utilizing both surrogate model and acquisition
    function. Hyperparameter tuning methods, which are part of the heuristic search
    group, work by performing trial and error. As for hyperparameter tuning methods
    from the multi-fidelity optimization group, they all utilize the cheap approximation
    of the whole hyperparameter tuning pipeline, so that we can have similar performance
    results with much lesser computational cost and faster experiment time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes all of the hyperparameter tuning methods discussed
    in this book, along with the supported packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Hyperparameter tuning methods and packages summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_14_table_001(a).png)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Hyperparameter tuning methods and packages summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_14_table_001(b).jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – Hyperparameter tuning methods and packages summary
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have revisited all of the hyperparameter tuning methods
    and packages discussed throughout the book. In the next section, we will revisit
    the HTDM.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting HTDM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Hyperparameter Tuning Decision Map** (**HTDM**) is a map that you can
    use to help you decide which hyperparameter tuning method should be adopted in
    a particular situation. We discussed in detail how you can utilize HTDM, along
    with several use cases, in [*Chapter 11*](B18753_11_ePub.xhtml#_idTextAnchor110),
    *Introducing the Hyperparameter Tuning Decision Map*. Here, we will only revisit
    the map, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Hyperparameter Tuning Decision Map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_14_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.2 – HTDM
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have revisited the HTDM. In the next section, we’ll discuss
    other topics you may find interesting to further boost your hyperparameter tuning
    knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: What’s next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though we have discussed a lot of hyperparameter tuning methods and their
    implementations in various packages, there are several important concepts you
    may need to know about that have not been discussed in this book. As for the hyperparameter
    tuning method, you can also read more about the **CMA-ES** method, which is part
    of the heuristic search group ([https://cma-es.github.io/](https://cma-es.github.io/)).
    You can also read more about the **meta-learning** concept to further boost the
    performance of your Bayesian optimization tuning results ([https://lilianweng.github.io/posts/2018-11-30-meta-learning/](https://lilianweng.github.io/posts/2018-11-30-meta-learning/)).
    It is also worth noting that we can combine the manual search method with other
    hyperparameter tuning methods to boost the efficiency of our experiments, especially
    when we already have prior knowledge about the good range of the hyperparameter
    values.
  prefs: []
  type: TYPE_NORMAL
- en: As for the packages, you can also learn more about the **HpBandSter** package,
    which implements the Hyper Band, BOHB, and random search methods ([https://github.com/automl/HpBandSter](https://github.com/automl/HpBandSter)).
    Finally, there are also several packages that automatically create a scikit-learn
    wrapper from the non-scikit-learn model. For example, you can utilize the **Skorch**
    package to create scikit-learn wrappers from PyTorch models ([https://skorch.readthedocs.io/en/stable/](https://skorch.readthedocs.io/en/stable/)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have summarized all the important concepts discussed throughout
    all chapters in this book. You have also been introduced to several new concepts
    that you may want to learn to further boost your hyperparameter tuning knowledge.
    From now on, you will have the skills you need to take full control over your
    machine learning models and get the best models for the best results via hyperparameter
    tuning experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for investing your interest and time in reading this book. Best of luck
    on your hyperparameter tuning learning journey!
  prefs: []
  type: TYPE_NORMAL
