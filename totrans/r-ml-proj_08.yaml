- en: Automatic Prose Generation with Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用循环神经网络进行自动散文生成
- en: 'We have been interacting through this book for almost 200 pages, but I realized
    that I have not introduced myself properly to you! I guess it''s time. You already
    know some bits about me through the author profile in this book; however, I want
    to tell you a bit about the city I live in. I am based in South India, in a city
    called Bengaluru, also know as Bangalore. The city is known for its IT talent
    and population diversity. I love the city, as it is filled with loads of positive
    energy. Each day, I get to meet people from all walks of life—people from multiple
    ethnicities, multiple backgrounds, people who speak multiple languages, and so
    on. Kannada is the official language spoken in the state of Karnataka where Bangalore
    is located. Though I can speak bits and pieces of Kannada, my proficiency with
    speaking the language is not as good as a native Kannada speaker. Of course, this
    is an area of improvement for me and I am working on it. Like me, many other migrants
    that moved to the city from other places also face problems while conversing in
    Kannada. Interestingly, not knowing the language does not stop any of us from
    interacting with locals in their own language. Guess what comes to our rescue:
    mobile apps such as Google translate, Google text-to-speech, and the like. These
    applications are built on NLP technologies called machine translation and speech
    recognition. These technologies in turn work on things known as **language models**.
    Language models is the topic we will delve into in this chapter.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎通过这本书互动了将近200页，但我意识到我还没有向你正确地介绍自己！我想现在是时候了。通过这本书的作者简介，你已经了解了我的一些信息；然而，我想告诉你一些关于我居住的城市的情况。我住在南印度的一个城市，名叫班加罗尔，也被称为班加罗尔。这座城市以其IT人才和人口多样性而闻名。我喜欢这座城市，因为它充满了大量的正能量。每天，我都会遇到来自各行各业的人——来自多个民族、多个背景、说多种语言的人等等。卡纳达语是位于班加罗尔的卡纳塔克邦的官方语言。尽管我能说一些卡纳达语，但我的口语水平并不如本土的卡纳达语使用者。当然，这是我要改进的一个领域，我正在努力提高。像我一样，许多从其他地方迁移到这个城市的人在与卡纳达语交流时也遇到了问题。有趣的是，不懂语言并没有阻止我们用当地人的语言与他们互动。猜猜看，是什么帮助我们解决了这个问题：像谷歌翻译、谷歌文本转语音等移动应用程序。这些应用程序是基于称为机器翻译和语音识别的自然语言处理技术构建的。这些技术反过来又作用于被称为**语言模型**的东西。语言模型是我们将在本章深入探讨的主题。
- en: 'The objectives of the chapter include exploring the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标包括探讨以下主题：
- en: The need for language modeling to address natural language processing tasks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要语言建模来解决自然语言处理任务
- en: The working principle of language models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型的工作原理
- en: Application of language models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型的应用
- en: Relationship between language modeling and neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模与神经网络之间的关系
- en: Recurrent neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Differences between a normal feedforward network and a recurrent neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正常前馈网络与循环神经网络之间的区别
- en: Long short-term memory networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: A project to autogenerate text using recurrent neural networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个使用循环神经网络自动生成文本的项目
- en: Understanding language models
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解语言模型
- en: 'In the English language, the character *a* appears much more often in words
    and sentences than the character *x*. Similarly, we can also observe that the
    word *is* occurs more frequently than the word *specimen*. It is possible to learn
    the probability distributions of characters and words by examining large volumes
    of text. The following screenshot is a chart showing the probability distribution
    of letters given a corpus (text dataset):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在英语中，字符*a*在单词和句子中出现的频率远高于字符*x*。同样，我们也可以观察到单词*is*出现的频率高于单词*specimen*。通过检查大量文本，我们可以学习字符和单词的概率分布。以下截图是一个显示给定语料库（文本数据集）中字母概率分布的图表：
- en: '![](img/fca6b8ed-0b69-43c5-9860-2f2822b70e52.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fca6b8ed-0b69-43c5-9860-2f2822b70e52.png)'
- en: Probability distribution of letters in a corpus
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库中字母的概率分布
- en: We can observe that the probability distributions of characters are non-uniform.
    This essentially means that we can recover the characters in a word, even if they
    are lost due to noise. If a particular character is missing in a word, it can
    be reconstructed just based on the characters that are surrounding the missing
    character. The reconstruction of the missing character is not done randomly, but
    is done by picking the character that has the highest probability distribution
    of occurrence, given the characters that are surrounding the missing character.
    Technically speaking, the statistical structure of words in a sentence or characters
    in words follows the distance from maximal entropy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到字符的概率分布是非均匀的。这本质上意味着，即使由于噪声而丢失，我们也可以恢复单词中的字符。如果一个特定的字符在单词中缺失，它可以根据缺失字符周围的字符重建。缺失字符的重建不是随机进行的，而是通过选择给定缺失字符周围字符具有最高概率分布的字符来完成的。从技术上讲，句子中单词或词中字符的统计结构遵循最大熵的距离。
- en: 'A language model exploits the statistical structure of a language to express
    the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型利用语言的统计结构来表达以下内容：
- en: Given *w_1, w_2, w_3,...w_N* words in a sentence, a language model assigns a
    probability to a sentence *P(w_1, w_2, w_3,.... w_N)*.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定句子中的*w_1, w_2, w_3,...w_N*个词，语言模型会给这个句子分配一个概率*P(w_1, w_2, w_3,.... w_N)*。
- en: It then assigns probability of an upcoming word (*w_4* in this case) as *P(w_4
    | w_1, w_2, w_3)*.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后它将下一个单词（在这种情况下为*w_4*）的概率分配为*P(w_4 | w_1, w_2, w_3)*。
- en: 'Language models enable a number of applications to be developed in NLP, and
    some of them are listed as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型使得在NLP中开发许多应用成为可能，以下列出其中一些：
- en: '**Machine translation**: P(enormous cyclone tonight) > P(gain typhoon this
    evening)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：P(enormous cyclone tonight) > P(gain typhoon this evening)'
- en: '**Spelling correction**: P(satellite constellation) > P(satelitte constellation)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拼写纠正**：P(satellite constellation) > P(satelitte constellation)'
- en: '**Speech recognition**: P(I saw a van) > P(eyes awe of an)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音识别**：P(I saw a van) > P(eyes awe of an)'
- en: '**Typing prediction**: Auto completion of in Google search, typing assistance
    apps'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**打字预测**：在谷歌搜索中的自动完成，打字辅助应用'
- en: 'Let''s now look at how the probabilities are calculated for the words. Consider
    a simple sentence, *Decembers are cold*. The probability of this sentence is expressed
    as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下如何计算单词的概率。考虑一个简单的句子，*Decembers are cold*。这个句子的概率如下表示：
- en: '*P("Decembers are cold") = P("December") * P ("are" | "Decembers") * P("cold"
    | "Decembers are")*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*P("Decembers are cold") = P("December") * P ("are" | "Decembers") * P("cold"
    | "Decembers are")*'
- en: 'Mathematically, the probability computation of words in a sentence (or letters
    in a word) can be expressed as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，句子中单词（或词中的字母）的概率计算可以表示如下：
- en: '![](img/c05cd3c8-05cf-49c4-a4e6-d282ae3c708f.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c05cd3c8-05cf-49c4-a4e6-d282ae3c708f.png)'
- en: Andrey Markov, a Russian mathematician, described a stochastic process with
    a property called **Markov Property** or **Markov Assumption**. This basically
    states that one can make predictions for the future of the process based solely
    on its present state, just as well as one could knowing the process's full history,
    hence independently from such history.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 俄罗斯数学家安德烈·马尔可夫描述了一个具有**马尔可夫属性**或**马尔可夫假设**的随机过程。这基本上意味着，人们可以仅基于过程的当前状态对未来进行预测，就像知道过程的全貌一样，因此独立于这样的历史。
- en: 'Based on Markov''s assumption, we can rewrite the conditional probability of
    *cold* as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于马尔可夫的假设，我们可以将*cold*的条件概率重新写为如下：
- en: '*P("cold" | "Decembers are") is congruent to* *P("cold" | "are")*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*P("cold" | "Decembers are") 等同于 *P("cold" | "are")*'
- en: 'Mathematically, Markov''s assumption can be expressed as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，马尔可夫的假设可以表示如下：
- en: '![](img/1d051944-7841-4762-86ff-9ebf71f50ad6.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1d051944-7841-4762-86ff-9ebf71f50ad6.png)'
- en: While this mathematical formulation represents the bigram model (two words taken
    into consideration at a time), it can be easily extended to an n-gram model. In
    the n-gram model, the conditional probability depends on just a couple more previous
    words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个数学公式代表了二元模型（一次考虑两个词），但它可以很容易地扩展到n元模型。在n元模型中，条件概率仅依赖于更多一些的先前词。
- en: 'Mathematically, an n-gram model is expressed as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，n元模型可以表示如下：
- en: '![](img/f04d8634-6de8-4781-a727-58d55404307f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f04d8634-6de8-4781-a727-58d55404307f.png)'
- en: 'Consider the famous poem *A Girl* by *Ezra Pound* as our corpus for building
    a **bigram** model. The following is the text corpus:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以埃兹拉·庞德（Ezra Pound）的著名诗歌 *A Girl* 作为我们构建 **bigram** 模型的语料库。以下就是文本语料库：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We are already aware that in a bigram model, the conditional probability is
    computed just based on the previous word. So, the probability of a word can be
    computed as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，在大词模型中，条件概率仅基于前一个单词来计算。因此，一个单词的概率可以按以下方式计算：
- en: '![](img/1b973bd5-ea2c-4c61-a4f6-756b8f7a49f5.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b973bd5-ea2c-4c61-a4f6-756b8f7a49f5.png)'
- en: If we were to compute the probability of the word *arms* given the word *my*
    in the poem, it is computed as the number of times the words *arms* and *my* appear
    together in the poem, divided by the number of times the word *my* appears in
    the poem.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要计算在诗歌中给定单词 *my* 的条件下单词 *arms* 的概率，它是通过在诗歌中单词 *arms* 和 *my* 同时出现的次数除以单词
    *my* 在诗歌中出现的次数来计算的。
- en: We see that the words *my arms* appeared in the poem only once (in the sentence
    *The sap has ascended my arms*). However, the word *my* appeared in the poem three
    times (in the sentences *The tree has entered my hands*, *The sap has ascended
    my arms*, and *The tree has grown in my breast-Downward*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，单词 *my arms* 在诗歌中只出现了一次（在句子 *The sap has ascended my arms* 中）。然而，单词 *my*
    在诗歌中出现了三次（在句子 *The tree has entered my hands*，*The sap has ascended my arms* 和
    *The tree has grown in my breast-Downward* 中）。
- en: 'Therefore, the conditional probability of the word *arms* given *my* is 1/3,
    formally represented as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定 *my* 的条件下单词 *arms* 的条件概率是 1/3，形式上表示如下：
- en: '*P("arms" | "my") = P("arms", "my") / P("my") = 1 / 3*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*P("arms" | "my") = P("arms", "my") / P("my") = 1 / 3*'
- en: To calculate probability of the first and last words, the special tags <BOS>
    and <EOS> are added at the start and end of sentences, respectively. Similarly,
    the probability of a sentence or sequence of words can be calculated using the
    same approach by multiplying all the bigram probabilities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算第一个和最后一个单词的概率，分别在句子的开始和结尾添加了特殊标记 <BOS> 和 <EOS>。同样，可以通过乘以所有bigram概率来使用相同的方法计算句子或单词序列的概率。
- en: As language modeling involves predicting the next word in a sequence, given
    the sequence of words already present, we can train a language model to create
    subsequent words in a sequence from a given starting sequence.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语言建模涉及根据已存在的单词序列预测序列中的下一个单词，我们可以训练一个语言模型，从给定的起始序列创建序列中的后续单词。
- en: Exploring recurrent neural networks
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索循环神经网络
- en: '**Recurrent neural networks** (**RNNs**) are a family of neural networks for
    processing sequential data. RNNs are generally used to implement language models. We,
    as humans, base much of our language understanding on the context. For example,
    let''s consider the sentence *Christmas falls in the month of --------*. It is
    easy to fill in the blank with the word *December*. The essential idea here is
    that there is information about the last word encoded in the previous elements
    of the sentence.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）是一组用于处理序列数据的神经网络。RNN 通常用于实现语言模型。作为人类，我们的大部分语言理解都基于上下文。例如，让我们考虑句子
    *Christmas falls in the month of --------*。用单词 *December* 来填补空白很容易。这里的本质思想是，关于最后一个单词的信息被编码在句子的前几个元素中。'
- en: The central theme behind the RNN architecture is to exploit the sequential structure
    of the data. As the name suggests, RNNs operate in a recurrent way. Essentially,
    this means that the same operation is performed for every element of a sequence
    or sentence, with its output depending on the current input and the previous operations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 架构背后的中心主题是利用数据的序列结构。正如其名所示，RNN 以循环的方式运行。本质上，这意味着对序列或句子中的每个元素执行相同的操作，其输出取决于当前输入和之前的操作。
- en: 'An RNN works by looping an output of the network at time *t* with the input
    of the network at time *t+1*. These loops allow persistence of information from
    one time step to the next one. The following diagram is a circuit diagram representing
    an RNN:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 通过将网络在时间 *t* 的输出与网络在时间 *t+1* 的输入循环连接来工作。这些循环允许信息从一个时间步到下一个时间步的持续。以下是一个表示
    RNN 的电路图：
- en: '![](img/94bd6b4e-b724-432d-b9f0-4f5c28137562.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94bd6b4e-b724-432d-b9f0-4f5c28137562.png)'
- en: Circuit diagram representing a RNN
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表示 RNN 的电路图
- en: The diagram indicates an RNN that remembers what it knows from previous input
    using a simple loop. This loop takes the information from the previous timestamp
    and adds it to the input of the current timestamp. At a particular time step *t*,
    *X[t]* is the input to the network, *O[t]* is the output of the network, and *h[t]*
    is the detail it remembered from previous nodes in the network. In between, there
    is the RNN cell, which contains neural networks just like a feedforward network.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表示一个RNN，它通过一个简单的循环来记住之前输入的信息。这个循环将来自上一个时间戳的信息添加到当前时间戳的输入中。在特定的时间步长*t*，*X[t]*是网络的输入，*O[t]*是网络的输出，*h[t]*是它从网络中先前节点记住的细节。在中间，有一个RNN单元，它包含类似于前馈网络的神经网络。
- en: One key point to ponder in terms of the definition of an RNN is the timestamps.
    The timestamps referred to in the definition have nothing to do with past, present,
    and future. They simply represent a word or an item in a sequence or a sentence.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN定义方面，一个需要深思的关键点是时间戳。定义中提到的时间戳与过去、现在和未来无关。它们只是代表序列或句子中的一个词或项目。
- en: 'Let''s consider an example sentence: *Christmas Holidays are Awesome*. In this
    sentence, take a look at the following timestamp:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子句子：*圣诞节假期棒极了*。在这个句子中，看一下以下时间戳：
- en: '*Christmas* is x[0]'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*圣诞节*是x[0]'
- en: '*Holidays* is x[1]'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假期*是x[1]'
- en: '*are* is x[2];'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*是*是x[2]；'
- en: '*Awesome* is x[3]'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*棒极了*是x[3]'
- en: 'If t=1, then take a look at the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果t=1，那么看一下以下：
- en: x[t] = *Holidays* → event at current timestamp
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x[t] = *假期* → 当前时间戳的事件
- en: x[t-1] = *Christmas* → event at previous timestamp
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x[t-1] = *圣诞节* → 上一个时间戳的事件
- en: 'It can be observed from the preceding circuit diagram that the same operation
    is performed in the RNN repeatedly on different nodes. There is also a black square
    in the diagram that represents a time delay of a single time step. It may be confusing
    to understand the RNN with the loops, so let''s unfold the computational graph.
    The unfolded RNN computational graph is shown in the following diagram:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的电路图中可以观察到，在RNN中，相同的操作在不同的节点上反复执行。图中还有一个代表单个时间步长延迟的黑方块。理解带有循环的RNN可能会有些困惑，所以让我们展开计算图。展开的RNN计算图如下所示：
- en: '![](img/6a1b4211-5c6e-4d7e-b757-1459e993d077.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6a1b4211-5c6e-4d7e-b757-1459e993d077.png)'
- en: RNN—unfolded computational graph view
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: RNN—展开的计算图视图
- en: In the preceding diagram, each node is associated with a particular time. In
    the RNN architecture, each node receives different inputs at each time step **x[t]**.
    It also has the capability of producing outputs at each time step **o[t]**. The
    network also maintains a memory state **h[t]**, which contains information about
    what happened in the network up to the time *t*. As this is the same process that
    is run across all the nodes in the network, it is possible to represent the whole
    network in a simplified form, as shown in the RNN circuit diagram.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，每个节点都与一个特定的时间相关联。在RNN架构中，每个节点在每个时间步长**x[t]**接收不同的输入。它还具有在每个时间步长**o[t]**产生输出的能力。网络还维护一个记忆状态**h[t]**，它包含关于时间*t*之前网络中发生的事情的信息。由于这是在网络的各个节点上运行的同一步骤，因此可以将整个网络以简化的形式表示，如图中RNN电路图所示。
- en: Now, we understand that we see the word **recurrent** in RNNs because it performs
    the same task for every element of a sequence, with the output depending on previous
    computations. It may be noted that, theoretically, RNNs can make use of information
    in arbitrarily long sequences, but in practice, they are implemented to looking
    back only a few steps.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们明白为什么在RNN中看到**循环**这个词，因为它对序列中的每个元素执行相同的任务，输出取决于之前的计算。理论上，RNN可以利用任意长序列中的信息，但在实践中，它们被实现为只回顾几个步骤。
- en: 'Formally, an RNN can be defined in an equation as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，一个RNN可以用以下方程定义：
- en: '![](img/e30363fe-7188-4b4e-9d21-010c52cf2326.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e30363fe-7188-4b4e-9d21-010c52cf2326.png)'
- en: In the equation, *h[t]* is the hidden state at timestamp *t*. An activation
    function such as Tanh, Sigmoid, or ReLU can be applied to compute the hidden state
    and it is represented in the equation as [![](img/de0d48dd-c787-4efe-a5e2-1f26ce1485de.png)].
    *W* is the weight matrix for the input to the hidden layer at timestamp *t*. *X[t]*
    is the input at timestamp *t*. *U* is the weight matrix for the hidden layer at
    time *t-1* to the hidden layer at time *t*, and *h[t-1]* is the hidden state at
    timestamp *t*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程中，*h[t]*是时间戳*t*处的隐藏状态。可以使用Tanh、Sigmoid或ReLU等激活函数来计算隐藏状态，并在方程中表示为[![](img/de0d48dd-c787-4efe-a5e2-1f26ce1485de.png)]。*W*是时间戳*t*处输入到隐藏层的权重矩阵。*X[t]*是时间戳*t*处的输入。*U*是时间戳*t-1*处的隐藏层到时间戳*t*处的隐藏层的权重矩阵，*h[t-1]*是时间戳*t*处的隐藏状态。
- en: 'During backpropagation, *U* and *W* weights are learned by the RNN. At each
    node, the contribution of the hidden state and the contribution of the current
    input are decided by *U* and *W*. The proportions of *U* and *W* in turn result
    in the generation of output at the current node. The activation functions add
    non-linearity in RNNs, thus enabling the simplification of gradient calculations
    during the backpropagation process. The following diagram illustrates the idea
    of backpropagation:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，RNN通过学习*U*和*W*权重。在每个节点上，隐藏状态和当前输入的贡献由*U*和*W*决定。*U*和*W*的比例进而导致当前节点的输出生成。激活函数在RNN中添加了非线性，从而在反向传播过程中简化了梯度计算。以下图示展示了反向传播的概念：
- en: '![](img/4794f067-f59f-4ebd-9046-7519eddc4c4c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4794f067-f59f-4ebd-9046-7519eddc4c4c.png)'
- en: Backpropagation in neural networks
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的反向传播
- en: 'The following diagram depicts the overall working mechanism of an RNN and the
    way the weights *U* and *W* are learned through backpropagation. It also depicts
    the use of the *U* and *W* weight matrices in the network to generate the output,
    as shown in the following diagram:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了RNN的整体工作机制以及通过反向传播学习权重*U*和*W*的方式。它还展示了网络中使用*U*和*W*权重矩阵生成输出的情况，如图所示：
- en: '![](img/7848cff1-e4ef-40cb-a5fa-f439878690be.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7848cff1-e4ef-40cb-a5fa-f439878690be.png)'
- en: Role of weights in RNN
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: RNN中权重的作用
- en: Comparison of feedforward neural networks and RNNs
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈神经网络与RNN的比较
- en: 'One fundamental difference between other neural networks and RNNs is that,
    in all other networks, the inputs are independent of each other. However, in an
    RNN, all the inputs are related to each other. In an application, to predict the
    next word in a given sentence, the relationship between all the previous words
    helps to predict the current output. In other words, an RNN remembers all these
    relationships while training itself. This is not the case with other types of
    neural networks. A representation of a feedforward network is illustrated in the
    following diagram:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他神经网络相比，RNN的一个基本区别在于，在所有其他网络中，输入之间是相互独立的。然而，在RNN中，所有输入都是相互关联的。在应用中，为了预测给定句子中的下一个单词，所有先前单词之间的关系有助于预测当前输出。换句话说，RNN在训练过程中会记住所有这些关系。这与其他类型的神经网络不同。以下图示展示了前馈网络的表示：
- en: '![](img/f72615fe-2f5b-4848-8b9c-970e2468b790.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f72615fe-2f5b-4848-8b9c-970e2468b790.png)'
- en: Feedforward neural network architecture
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络架构
- en: From the preceding diagram, we can see that no loops are involved in the feedforward
    network architecture. This is in contrast to that of the RNN architecture depicted
    in the diagrams of the RNN circuit diagram and the RNN unfolded computational
    graph. The series of mathematical operations in a feedforward network is performed
    at the nodes and the information is processed straight through, with no loops
    whatsoever.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看到前馈网络架构中不涉及任何循环。这与RNN电路图和RNN展开计算图中展示的RNN架构形成对比。前馈网络中的数学操作是在节点上执行的，信息直接通过，没有任何循环。
- en: Using supervised learning, the input that is fed to a feedforward network is
    transformed into output. The output in this case could be a label if it is classification,
    or it is a number in the case of regression. If we consider image classification,
    a label can be *cat* or *dog* for an image given as input.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用监督学习，输入被馈送到前馈网络后转换为输出。在这种情况下，输出可以是分类时的标签，或者回归时的数字。如果我们考虑图像分类，输入图像的标签可以是*猫*或*狗*。
- en: 'A feedforward neural network is trained on labeled images until errors are
    minimized in predicting the labels. Once trained, the model is able to classify
    even images that it has not seen previously. A trained feedforward network can
    be exposed to any random collection of photographs; the categorization of the
    first photograph does not have any impact or influence on the second or subsequent
    photographs that the model needs to categorize. Let''s discuss this with an example
    for better clarity on the concept: if the first image is seen as a *dog* by the
    feedforward network, it does not imply that the second image will be classified
    as *cat*. In other words, the predictions that the model arrives at have no notion
    of order in time, and the decision regarding the label is arrived at just based
    on the current input that is provided. To summarize, in feedforward networks no
    information on historical predictions is used for current predictions. This is
    very different from RNNs, where the previous prediction is considered in order
    to aid the current prediction.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络在标记图像上训练，直到预测标签的错误最小化。一旦训练完成，该模型能够对之前未见过的图像进行分类。一个训练好的前馈网络可以暴露于任何随机的照片集合；第一个照片的分类对模型需要分类的第二张或后续照片没有任何影响或影响。让我们通过一个例子来讨论这个问题，以便更好地理解这个概念：如果第一个图像被前馈网络识别为*狗*，这并不意味着第二个图像将被分类为*猫*。换句话说，模型得出的预测没有时间顺序的概念，标签的决定仅基于当前提供的输入。总结来说，在前馈网络中，不会使用历史预测信息来为当前预测提供信息。这与RNN非常不同，在RNN中，前一个预测被考虑是为了帮助当前预测。
- en: 'Another important difference is that feedforward networks, by design, map one
    input to one output, whereas RNNs can take multiple forms: map one input to multiple
    outputs, many inputs to many outputs, or many inputs to one output. The following
    diagram depicts the various input-output mappings possible with RNNs:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的区别是，前馈网络按设计将一个输入映射到一个输出，而RNN可以有多种形式：将一个输入映射到多个输出，多个输入到多个输出，或多个输入到一个输出。以下图展示了RNN可能实现的多种输入输出映射：
- en: '![](img/7c169342-447e-45c2-8568-cc4001515979.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c169342-447e-45c2-8568-cc4001515979.png)'
- en: Input-output mapping possibilities with RNNs
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的输入输出映射可能性
- en: Let's review some of the practical applications of the various input-output
    mappings possible with an RNN. Each rectangle in the preceding diagram is a vector
    and the arrows represent functions, for example, a matrix multiplication. The
    input vectors are the lower rectangles (colored in red), and the output vectors
    are the upper rectangles (colored in blue color). The middle rectangles (colored
    in green) are vectors that hold the RNN's state.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下使用RNN可能实现的输入输出映射的一些实际应用。前面图中的每个矩形都是一个向量，箭头代表函数，例如矩阵乘法。输入向量是下方的矩形（用红色着色），输出向量是上方的矩形（用蓝色着色）。中间的矩形（用绿色着色）是包含RNN状态的向量。
- en: 'The following are the various forms of mapping illustrated in the diagram:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是图中展示的各种映射形式：
- en: '**One input to one output**: The leftmost one is a vanilla mode of processing
    without RNN, from fixed-sized input to fixed-sized output; for example, image
    classification.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对一输入输出**：最左边的是没有使用RNN的普通处理模式，从固定大小的输入到固定大小的输出；例如，图像分类。'
- en: '**One input to many outputs**: Sequence output, for example, image captioning
    takes an image as input and it outputs a sentence of words.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对一输入到多输出**：序列输出，例如，图像字幕将图像作为输入，然后输出一个单词句子。'
- en: '**Many inputs to one output**: Sequence input, for example, sentiment analysis
    where a given sentence is given as input to the RNN, and the output is a classification
    expressing positive or negative sentiment of the sentence.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多输入到一对一输出**：序列输入，例如，情感分析，其中给定的句子作为输入提供给RNN，输出是一个表示句子积极或消极情感的分类。'
- en: '**Many inputs to many outputs**: Sequence input and sequence output; for example,
    for a machine translation task, an RNN reads a sentence in English as input and
    then outputs a sentence in Hindi or some other language.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多输入到多输出**：序列输入和序列输出；例如，对于机器翻译任务，RNN读取英语句子作为输入，然后输出印地语或其他语言的句子。'
- en: '**Many inputs to many outputs**: Synced sequence input and output, for example,
    video classification where we wish to label each frame of the video.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多输入到多输出**：同步序列输入和输出，例如，视频分类，我们希望为视频的每一帧进行标记。'
- en: 'Let''s now review the final difference between a feedforward network and an
    RNN. The way backpropagation is done in order to set the weights in a feedforward
    neural network is different from that of what is called **backpropagation through
    time** (**BPTT**), which is carried out in an RNN. We are already aware that the
    objective of the backpropagation algorithm in neural networks is to adjust the
    weights of a neural network to minimize the error of the network outputs compared
    to some expected output in response to corresponding inputs. Backpropagation itself
    is a supervised learning algorithm that allows the neural network to be corrected
    with regard to the specific errors made. The backpropagation algorithm involves
    the following steps:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来回顾一下前馈网络和RNN之间的最终区别。为了在前馈神经网络中设置权重而执行的反向传播的方式与在RNN中执行所谓的**时间反向传播**（**BPTT**）的方式不同。我们已经知道，神经网络中反向传播算法的目的是调整神经网络的权重，以最小化网络输出与对应输入的预期输出之间的误差。反向传播本身是一种监督学习算法，它允许神经网络根据特定错误进行纠正。反向传播算法包括以下步骤：
- en: Provide training input to the neural network and propagate it through the network
    to get the output
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向神经网络提供训练输入，并通过网络传播以获得输出
- en: Compare the predicted output to the actual output and calculate the error
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测输出与实际输出进行比较，并计算误差
- en: Calculate the derivatives of the error with respect to the learned network weights
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算误差相对于学习到的网络权重的导数
- en: Modify the weights to minimize the error
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改权重以最小化误差
- en: Repeat
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复
- en: In feedforward networks, it makes sense to run backpropagation at the end as
    the output is available only at the end. In RNNs, the output is produced at each
    time step and this output influences the output in the subsequent time steps.
    In other words, in RNNs, the error of a time step depends on the previous time
    step. Therefore, the normal backpropagation algorithms are not suitable for RNNs.
    Hence, a different algorithm known as BPTT is used to modify the weights in an
    RNN.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈网络中，在输出仅在结束时才可用的情况下，在结束时运行反向传播是有意义的。在RNN中，输出在每个时间步产生，并且这种输出会影响后续时间步的输出。换句话说，在RNN中，一个时间步的误差取决于前一个时间步。因此，正常的反向传播算法不适用于RNN。因此，使用称为BPTT（时间反向传播）的不同算法来修改RNN中的权重。
- en: Backpropagation through time
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间反向传播
- en: 'We are already aware that RNNs are cyclical graphs, unlike feedforward networks,
    which are acyclic directional graphs. In feedforward networks, the error derivatives
    are calculated from the layer above. However, in an RNN we don''t have such layering
    to perform error derivative calculations. A simple solution to this problem is
    to unroll the RNN and make it similar to a feedforward network. To enable this,
    the hidden units from the RNN are replicated at each time step. Each time step
    replication forms a layer that is similar to layers in a feedforward network.
    Each time step *t* layer connects to all possible layers in the time step *t+1*.
    Therefore, we randomly initialize the weights, unroll the network, and then use
    backpropagation to optimize the weights in the hidden layer. The lowest layer
    is initialized by passing parameters. These parameters are also optimized as a
    part of backpropagation. The backpropagation through time algorithm involves the
    following steps:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，循环神经网络（RNNs）是循环图，与无环方向图的前馈网络不同。在前馈网络中，误差导数是从上层计算的。然而，在RNN中，我们没有这样的层来进行误差导数计算。解决这个问题的简单方法是将RNN展开，使其类似于前馈网络。为了实现这一点，RNN中的隐藏单元在每个时间步都会被复制。每个时间步的复制形成一层，类似于前馈网络中的层。每个时间步`t`层连接到时间步`t+1`中所有可能的层。因此，我们随机初始化权重，展开网络，然后使用反向传播来优化隐藏层中的权重。最低层通过传递参数进行初始化。这些参数也是作为反向传播的一部分进行优化的。时间反向传播算法包括以下步骤：
- en: Provide a sequence of time steps of input and output pairs to the network
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向网络提供一系列输入和输出对的时序步骤
- en: Unroll the network, then calculate and accumulate errors across each time step
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展开网络，然后计算并累积每个时间步的误差
- en: Roll up the network and update weights
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收缩网络并更新权重
- en: Repeat
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复
- en: 'In summary, with BPTT, the error is back propagated from the last to the first
    time step, while unrolling all the time steps. The error for each time step is
    calculated, which allows updating the weights. The following diagram is a visualization
    showing the backpropagation through time:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，使用BPTT时，错误是从最后一个时间步向后传播到第一个时间步，同时展开所有时间步。计算每个时间步的错误，这允许更新权重。以下图表是时间反向传播的可视化：
- en: '![](img/da6bf332-cf3c-4b93-b0d5-3267c455859b.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/da6bf332-cf3c-4b93-b0d5-3267c455859b.png)'
- en: Backpropagation through time in an RNN
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: RNN中的时间反向传播
- en: It should be noted that as the number of time steps increases, the BPTT algorithm
    can get computationally very expensive.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 应当注意，随着时间步数的增加，BPTT算法的计算成本可能会变得非常高。
- en: Problems and solutions to gradients in RNN
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN中梯度的问题和解决方案
- en: RNNs are not perfect, there are two main issues namely **exploding gradients**
    and **vanishing gradients** that they suffer from. To understand the issues, let's
    first understand what a gradient means. A gradient is a partial derivative with
    respect to its inputs. In simple layman's terms, a gradient measures how much
    the output of a function changes, if one were to change the inputs a little bit.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: RNN并不完美，它们有两个主要问题，即**梯度爆炸**和**梯度消失**。为了理解这些问题，我们首先了解什么是梯度。梯度是相对于其输入的偏导数。用简单的话来说，梯度衡量的是函数的输出在输入略有变化时会发生多少变化。
- en: Exploding gradients
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度爆炸
- en: Exploding gradients relate to a situation where the BPTT algorithm assigns an
    insanely high importance to the weights, without a rationale. The problem results
    in an unstable network. In extreme situations, the values of weights can become
    so large that the values overflow and result in NaN values.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度爆炸与BPTT算法赋予权重极高重要性但缺乏合理依据的情况相关。这个问题导致网络不稳定。在极端情况下，权重的值可能会变得非常大，导致溢出并产生NaN值。
- en: 'The exploding gradients problem can be detected through observing the following
    subtle signs while training the network:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练网络时，可以通过观察以下细微迹象来检测梯度爆炸问题：
- en: The model weights quickly become very large during training
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，模型权重会迅速变得非常大。
- en: The model weights become NaN values during training
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，模型权重变为NaN值。
- en: The error gradient values are consistently above 1.0 for each node and layer
    during training
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，每个节点和层的错误梯度值始终大于1.0。
- en: 'There are several ways in which one could handle the exploding gradients problem.
    The following are some of the popular techniques:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以处理梯度爆炸问题。以下是一些流行的技术：
- en: This problem can be easily solved if we can truncate or squash the gradients.
    This is known as **gradient clipping**.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们可以截断或压缩梯度，这个问题就可以轻易解决。这被称为**梯度裁剪**。
- en: Updating weights across fewer prior time steps during training may also reduce
    the exploding gradient problem. This technique of having fewer step updates is
    called **truncated backpropagation through time** (**TBPTT**). It is an altered
    version of the BPTT training algorithm where the sequence is processed one time
    step at a time, and periodically (*k1* time steps) the BPTT update is performed
    back for a fixed number of time steps (*k2* time steps). *k1* is the number of
    forward-pass time steps between updates. *k2* is the number of time steps to which
    to apply BPTT.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，通过减少先前时间步的权重更新也可能减少梯度爆炸问题。这种具有较少步更新技术的称为**时间截断反向传播**（**TBPTT**）。它是BPTT训练算法的一个修改版本，其中序列一次处理一个时间步，并且定期（*k1*时间步）对固定数量的时间步（*k2*时间步）执行BPTT更新。*k1*是更新之间的前向传递时间步数。*k2*是应用BPTT的时间步数。
- en: Weight regularization can be done by checking the size of network weights and
    applying a penalty to the networks loss function for large weight values.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检查网络权重的大小并对大权重值应用惩罚来执行权重正则化。
- en: By using **long short-term memory units** (**LSTMs**) or **gated recurrent units**
    (**GRUs**) instead of plain vanilla RNNs.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用**长短期记忆单元**（**LSTMs**）或**门控循环单元**（**GRUs**）而不是普通的RNN。
- en: Careful initialization of weights such as **Xavier** initialization or **He**
    initialization.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细初始化权重，如**Xavier**初始化或**He**初始化。
- en: Vanishing gradients
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度消失
- en: 'We are already aware that long-term dependencies are very important for RNNs
    to function correctly. RNNs can become too deep because of the long-term dependencies.
    The vanishing gradient problem arises in cases where the gradient of the activation
    function is very small. During backpropagation, when the weights are multiplied
    with the low gradients, they tend to become very small and vanish as they go further
    into the network. This makes the neural network forget the long-term dependency.
    The following diagram is an illustration showing the cause that leads to vanishing
    gradients:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，长期依赖对于 RNN 正确运行非常重要。由于长期依赖，RNN 可能会变得太深。当激活函数的梯度非常小的时候，就会出现梯度消失问题。在反向传播过程中，当权重与低梯度相乘时，它们往往会变得非常小，并在网络中进一步消失。这使得神经网络忘记长期依赖。以下图表展示了导致梯度消失的原因：
- en: '![](img/eea2eb79-2b6a-44fe-8b72-e246a405ec79.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/eea2eb79-2b6a-44fe-8b72-e246a405ec79.png)'
- en: The cause of vanishing gradients
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失的原因
- en: To summarize, due to the vanishing gradients problem, RNNs experience difficulty
    in memorizing previous words very far away in the sequence and are only able to
    make predictions based on the most recent words. This can impact the accuracy
    of RNN predictions. At times, the model may fail to predict or classify what it
    is supposed to do.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，由于梯度消失问题，RNN 在记忆序列中非常远的先前单词时遇到困难，并且只能根据最近的单词进行预测。这可能会影响 RNN 预测的准确性。有时，模型可能无法预测或分类它应该执行的操作。
- en: 'There are several ways in which one could handle the vanishing gradients problem.
    The following are some of the most popular techniques:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以处理梯度消失问题。以下是一些最流行的技术：
- en: Initialize network weights for the identity matrix so that the potential for
    a vanishing gradient is minimized.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化网络权重为恒等矩阵，以最大限度地减少梯度消失的可能性。
- en: Setting the activation functions to ReLU instead of `sigmoid` or `tanh`. This
    makes the network computations stay close to the identity function. This works
    well because when the error derivatives are being propagated backwards through
    time, they remain constants of either 0 or 1, and so aren't likely to suffer from
    vanishing gradients.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将激活函数设置为 ReLU 而不是 `sigmoid` 或 `tanh`。这使得网络计算接近恒等函数。这效果很好，因为当错误导数在时间上向后传播时，它们保持为
    0 或 1 的常数，因此不太可能遭受梯度消失问题。
- en: Using LSTMs, which are a variant of the regular recurrent network designed to
    make it easy to capture long-term dependencies in sequence data. The standard
    RNN operates in such a way that the hidden state activation is influenced by the
    other local activations closest to it, which corresponds to a **short-term memory**,
    while the network weights are influenced by the computations that take place over
    entire long sequences, which corresponds to a **long-term memory**. The RNN was
    redesigned so that it has an activation state that can also act like weights and
    preserve information over long distances, hence the name **long short-term memory**.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LSTM，它是常规循环网络的变体，旨在使捕获序列数据中的长期依赖变得容易。标准的 RNN 以一种方式运行，即隐藏状态激活受其附近的其他局部激活的影响，这对应于**短期记忆**，而网络权重受整个长序列中发生的计算的影响，这对应于**长期记忆**。RNN
    被重新设计，使其具有可以像权重一样起作用并能够在长距离上保留信息的激活状态，因此得名 **长短期记忆**。
- en: In LSTMs, rather than each hidden node being simply a node with a single activation
    function, each node is a memory cell in itself that can store other information.
    Specifically, it maintains its own cell state. Normal RNNs take in their previous
    hidden state and the current input, and output a new hidden state. An LSTM does
    the same, except it also takes in its old cell state and will output its new cell
    state.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LSTM 中，而不是每个隐藏节点只是一个具有单个激活函数的节点，每个节点本身就是一个可以存储其他信息的记忆单元。具体来说，它维护自己的细胞状态。正常的
    RNN 接收先前的隐藏状态和当前输入，并输出一个新的隐藏状态。LSTM 做的是同样的，但它还接收旧的细胞状态，并将输出新的细胞状态。
- en: Building an automated prose generator with an RNN
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RNN 构建自动散文生成器
- en: In this project, we will attempt to build a character-level language model using
    an RNN to generate prose given some initial seed characters. The main task of
    a character-level language model is to predict the next character given all previous
    characters in a sequence of data. In other words, the function of an RNN is to
    generate text character by character.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将尝试使用RNN（递归神经网络）构建一个字符级语言模型，根据一些初始种子字符生成散文。字符级语言模型的主要任务是预测数据序列中所有先前字符之后的下一个字符。换句话说，RNN的功能是逐字符生成文本。
- en: To start with, we feed the RNN a huge chunk of text as input and ask it to model
    the probability distribution of the next character in the sequence, given a sequence
    of previous characters. These probability distributions conceived by the RNN model
    will then allow us to generate new text, one character at a time.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们向RNN提供一大块文本作为输入，并要求它根据先前字符序列的概率分布来建模序列中下一个字符的概率分布。这些由RNN模型构思的概率分布将允许我们逐字符生成新文本。
- en: The first requirement for building a language model is to secure a corpus of
    text that the model can use to compute the probability distribution of various
    characters. The larger the input text corpus, the better the RNN will model the
    probabilities.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 构建语言模型的第一要求是确保模型可以使用它来计算各种字符的概率分布的文本语料库。输入文本语料库越大，RNN对概率的建模就越好。
- en: We do not have to strive a lot to secure the big text corpus that is required
    to train the RNN. There are classical texts (books) such as *The Bible* that can
    be used as a corpus. The best part is many of the classical texts are no longer
    protected under copyright. Therefore, the texts can be downloaded and used freely
    in our models.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要费很大力气来获取训练RNN所需的庞大文本语料库。有一些经典文本（书籍），如《圣经》，可以用作语料库。最好的部分是许多经典文本已不再受版权保护。因此，这些文本可以自由下载并用于我们的模型中。
- en: 'Project Gutenberg is the best place to get access to free books that are no
    longer protected by copyright. Project Gutenberg can be accessed through the URL
    [http://www.gutenberg.org](http://www.gutenberg.org). There are several books
    such as *The Bible*, *Alice''s Adventures in Wonderland*, and so on are available
    from Project Gutenberg. As of December 2018, there are 58,486 books available
    for download. The books are available in several formats for us to be able to
    download and use, not just for this project, but for any project where huge text
    corpus input is required. The following screenshot is of a sample book from Project
    Gutenberg and the various formats in which the book is available for download:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Project Gutenberg是获取不再受版权保护的免费书籍的最佳场所。可以通过以下网址访问Project Gutenberg：[http://www.gutenberg.org](http://www.gutenberg.org)。例如，《圣经》、《爱丽丝梦游仙境》等书籍都可以从Project
    Gutenberg获得。截至2018年12月，有58,486本书可供下载。这些书籍以多种格式提供，以便我们下载和使用，不仅限于本项目，还适用于任何需要大量文本语料库输入的项目。以下截图是Project
    Gutenberg的一个样本书籍及其可供下载的多种格式：
- en: '![](img/87b584f1-a1ca-477f-8cad-974bc49ba6aa.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/87b584f1-a1ca-477f-8cad-974bc49ba6aa.png)'
- en: Sample book available from Project Gutenberg in various formats
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Sample book available from Project Gutenberg in various formats
- en: 'Irrespective of the format of the file that is downloaded, Project Gutenberg
    adds standard header and footer text to the actual book text. The following is
    an example of the header and footer that can be seen in a book:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 无论下载的文件格式如何，Project Gutenberg都会在实际书籍文本中添加标准的页眉和页脚文本。以下是在一本书中可以看到的页眉和页脚的示例：
- en: '[PRE1]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It is essential that we remove this header and footer text from the book text
    downloaded from Project Gutenberg website. For a text file that is downloaded,
    one can open the file in a text editor and delete the header and footer.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从Project Gutenberg网站下载的书籍文本中删除此页眉和页脚文本是至关重要的。对于下载的文本文件，可以在文本编辑器中打开文件并删除页眉和页脚。
- en: 'For our project in this chapter, let''s use a favorite book from childhood
    as the text corpus: *Alice’s Adventures in Wonderland* by Lewis Carroll. While
    we have an option to download the text format of this book from Project Gutenberg
    and make use of it as a text corpus, the R language''s `languageR` library makes
    the task even more easier for us. The `languageR` library already has the *Alice’s
    Adventures in Wonderland* book text. After installing the `languageR` library,
    use the following code to load the text data into the memory and print the loaded
    text:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的项目，让我们使用童年的最爱书籍作为文本语料库：刘易斯·卡罗尔的《爱丽丝梦游仙境》。虽然我们有从Project Gutenberg下载这本书的文本格式并将其用作文本语料库的选择，但R语言的`languageR`库使这项任务对我们来说更加容易。`languageR`库已经包含了《爱丽丝梦游仙境》的文本。在安装了`languageR`库之后，使用以下代码将文本数据加载到内存中并打印出加载的文本：
- en: '[PRE2]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You will get the following output:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '[PRE3]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We see from the output that the book text is stored as a character vector, where
    each of the vector items is a word from the book text that is split by punctuation.
    It may also be noted that not all the punctuation is retained in the book text.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，书籍文本存储为一个字符向量，其中向量的每个项目都是从书籍文本中通过标点符号分割出来的单词。也可能注意到，书籍文本中并没有保留所有的标点符号。
- en: 'The following code reconstructs the sentences from the words in the character
    vector. Of course, we do not get things like sentence boundaries during the reconstruction
    process, as the character vector does not have as much punctuation as character
    vector items. Now, let''s do the reconstruction of the book text from individual
    words:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码从字符向量中的单词重构句子。当然，在重构过程中，我们不会得到诸如句子边界之类的东西，因为字符向量中的标点符号没有字符向量项目那么多。现在，让我们从单个单词重构书籍文本：
- en: '[PRE4]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You will get the following output:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '[PRE5]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: From the output, we see that a long string of text is constructed from the words.
    Now, we can move on to doing some preprocessing on this text to feed it to the
    RNN so that the model learns the dependencies between characters and the conditional
    probabilities of characters in sequences.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，我们可以看到一段长文本是由单词构成的。现在，我们可以继续对这段文本进行一些预处理，以便将其输入到RNN中，使模型学习字符之间的依赖关系以及序列中字符的条件概率。
- en: One of the things to note is that, as with a character-level language model
    that generates the next character in a sequence, you can build a word-level language
    model too. However, the character-level language model has an advantage in that
    it can create its own unique words that are not in the vocabulary we train it
    on.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一件事是，与生成序列中下一个字符的字符级语言模型一样，你还可以构建一个词级语言模型。然而，字符级语言模型的优势在于它可以创建其自身的独特单词，而这些单词不在我们训练它的词汇表中。
- en: 'Let''s now learn how RNN works to conceive the dependencies between characters
    in sequences. Assume that we only had a vocabulary of four possible letters, [*a*,
    *p*, *l*, *e*], and the intent is to train an RNN on the training sequence *apple*.
    This training sequence is in fact a source of four separate training examples:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习RNN是如何工作的，以便理解序列中字符之间的依赖关系。假设我们只有四种可能的字母词汇，[*a*，*p*，*l*，*e*]，并且我们的目的是在训练序列*apple*上训练一个RNN。这个训练序列实际上是四个独立的训练示例的来源：
- en: The probability of the letter *p* should be likely, given the context of *a,
    , *in other words, the conditional probability of *p* given the letter *a* in
    the word *apple*
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定上下文*a*的情况下，字母*p*的概率应该是可能的，换句话说，在单词*apple*中，给定字母*a*的条件概率*p*。
- en: Similar to the first point, *p* should be likely in the context of *ap*
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与第一点类似，在上下文*ap*中，*p*应该是可能的。
- en: The *letter l* should also be likely given the context of *app*
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上下文*app*中，*字母l*也应该是有可能的。
- en: The *letter e* should be likely given the context of *appl*
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上下文*appl*中，*字母e*应该是有可能的。
- en: 'We start to encode each character in the word *apple* into a vector using 1-of-k
    encoding. 1-of-k encoding represents each character in the word as all zeros,
    except for the single 1 at the index of the character in the vocabulary. Each
    character thus represented with 1-of-k encoding is then fed into the RNN one at
    a time with the help of a step function. The RNN takes this input and generates
    a four-dimensional output vectors (one dimension per character, and recollect
    we only have four characters in our vocabulary). This output vector can be interpreted
    as the confidence that the RNN currently assigns to each character coming next
    in the sequence. The following diagram is a visualization of the RNN learning
    the characters:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始使用 1-of-k 编码将单词 *apple* 中的每个字符编码成一个向量。1-of-k 编码表示单词中的每个字符都是零，除了在词汇表中字符索引处的单个
    1。这样用 1-of-k 编码表示的每个字符随后通过步函数逐个输入到 RNN 中。RNN 接收这个输入并生成一个四维输出向量（每个字符一个维度，并且我们词汇表中有四个字符）。这个输出向量可以解释为
    RNN 当前分配给序列中下一个字符的置信度。下面的图是 RNN 学习字符的可视化：
- en: '![](img/709fd0f0-e868-420e-b67d-2100b54c96f3.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/709fd0f0-e868-420e-b67d-2100b54c96f3.png)'
- en: RNN learning the character language model
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 学习字符语言模型
- en: In the preceding diagram, we see an RNN with four-dimensional input and output
    layers. There is also a hidden layer with three neurons. The diagram displays
    the activations in the forward pass when the RNN is fed with the input of the
    characters *a*, *p*, *p*, and *l*. The output layer contains the confidence that
    the RNN assigned to each of the following characters. The expectation of the RNN
    is for the green numbers in the output layer to be higher than the red numbers.
    The high values of green numbers enable the prediction of the right characters
    as per the input.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们看到一个具有四维输入和输出层的 RNN。还有一个包含三个神经元的隐藏层。该图显示了当 RNN 接收字符 *a*、*p*、*p* 和 *l*
    的输入时，正向传递中的激活情况。输出层包含 RNN 分配给每个后续字符的置信度。RNN 的期望是输出层中的绿色数字高于红色数字。绿色数字的高值使得可以根据输入预测正确的字符。
- en: We see that in the first time step, when the RNN is fed the input character
    *a*, it assigned a confidence of 1.0 to the next letter being *a*, 2.2 as confidence
    to letter *p*, -3.0 to *l*, and 4.1 to *e*. As per our training data, the sequence
    we considered is *apple*; therefore, the next correct character is *p* given the
    character *a* as input in the first time step. We would like our RNN to maximize
    the confidence in the first step (indicated in green) and minimize the confidence
    of all other letters (indicated in red). Likewise, we have a desired output character
    at each one of the four time steps that we would like our RNN to assign a greater
    confidence to.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到在第一次步长中，当 RNN 接收输入字符 *a* 时，它将 1.0 的置信度分配给下一个字母是 *a*，将 2.2 的置信度分配给字母 *p*，将
    -3.0 分配给 *l*，将 4.1 分配给 *e*。根据我们的训练数据，我们考虑的序列是 *apple*；因此，在第一次步长中，以 *a* 作为输入时，下一个正确的字符是
    *p*。我们希望我们的 RNN 在第一步（用绿色表示）中最大化置信度，并最小化所有其他字母（用红色表示）的置信度。同样，在每个四个时间步长中，我们都希望我们的
    RNN 分配更高的置信度给期望的输出字符。
- en: Since the RNN consists entirely of differentiable operations, we can run the
    backpropagation algorithm to figure out in what direction we should adjust each
    one of its weights to increase the scores of the correct targets (the bold green
    numbers).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 RNN 完全由可微操作组成，我们可以运行反向传播算法来确定我们应该调整每个权重的方向，以增加正确目标（粗体绿色数字）的分数。
- en: Based on the gradient direction, the parameters are updated and the algorithm
    actually alters the weight by a tiny amount in the same direction as that of the
    gradient. Ideally, if gradient decent has successfully run and updated the weights,
    we would see a slightly higher weight for the right choice and lower weights for
    the incorrect characters. For example, we would find that the scores of the correct
    character *p* in the first time step would be slightly higher, say 2.3 instead
    of 2.2\. At the same time, the scores for the other characters *a*, *l*, and *e*
    would be observed as lower than that of the score that was assigned prior to gradient
    descent.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度方向，参数被更新，算法实际上通过一个很小的量在梯度相同方向上改变权重。理想情况下，如果梯度下降法成功运行并更新了权重，我们会看到正确选择的权重略有增加，而对于错误的字符，权重会降低。例如，我们会发现第一次步长中正确字符
    *p* 的分数略有提高，比如说从 2.2 提高到 2.3。同时，字符 *a*、*l* 和 *e* 的分数会被观察到低于梯度下降之前分配的分数。
- en: The process of updating the parameters through gradient descent is repeated
    multiple times in the RNN until the network converges, in other words, until the
    predictions are consistent with the training data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN中，通过梯度下降更新参数的过程会重复多次，直到网络收敛，换句话说，直到预测与训练数据一致。
- en: Technically speaking, we run the standard softmax classifier, otherwise called
    the cross-entropy loss, on every output vector simultaneously. The RNN is trained
    with mini-batch stochastic gradient descent or adaptive learning rate methods
    such as RMSProp or Adam to stabilize the updates.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来说，我们同时对每个输出向量运行标准的softmax分类器，也称为交叉熵损失。RNN通过小批量随机梯度下降或自适应学习率方法（如RMSProp或Adam）进行训练，以稳定更新。
- en: You may notice that the first time the character *p* is input, the output is
    *p*; however, the second time the same input is fed, the output is *l*. An RNN,
    therefore, cannot rely only on the input that is given. This is where an RNN uses
    its recurrent connection to keep track of the context to perform the task and
    make the correct predictions. Without the context, it would have been challenging
    for the network to predict the right output specifically, given the same input.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，当第一次输入字符*p*时，输出是*p*；然而，当相同的输入第二次被输入时，输出是*l*。因此，RNN不能仅依赖于给定的输入。这就是RNN使用其循环连接来跟踪上下文以执行任务并做出正确预测的地方。没有上下文，网络要预测特定的正确输出将会非常困难。
- en: When we have to generate text using the trained RNN model, we provide a seed
    input character to the network and get the distribution over what characters are
    likely to come next. The distribution is then sampled and fed it right back in,
    to get the next letter. The process is repeated until the maximum number of characters
    is reached (until a specific user-defined character length), or until the model
    encounters an end of line character such as <EOS> or <END>.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们必须使用训练好的RNN模型生成文本时，我们将种子输入字符提供给网络，并得到关于下一个可能出现的字符的分布。然后从这个分布中进行采样，并将其反馈回网络，以获取下一个字母。这个过程会重复，直到达到最大字符数（直到达到特定用户定义的字符长度），或者直到模型遇到行尾字符，如<EOS>或<END>。
- en: Implementing the project
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施项目
- en: 'Now that we know how an RNN is able to build a character-level model, let''s
    implement the project to generate our own words and sentences through an RNN.
    Generally, RNN training is computationally intensive and it is suggested that
    we run the code on a **graphical processing unit** (**GPU**). However, due to
    infrastructure limitations, we are not going to use a GPU for the project code.
    The `mxnet` library allows a character-level language model with an RNN to be
    executed on the CPU itself, so let''s start coding our project:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了RNN如何构建字符级模型，让我们通过RNN实现项目，生成我们自己的单词和句子。通常，RNN训练计算量很大，建议我们在**图形处理单元**（**GPU**）上运行代码。然而，由于基础设施限制，我们不会在项目代码中使用GPU。`mxnet`库允许字符级语言模型（带有RNN）在CPU上执行，因此让我们开始编写我们的项目代码：
- en: '[PRE6]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To use the `languageR` library''s *ALICE''S ADVENTURES IN WONDERLAND* book
    text and load it into memory, use the following code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`languageR`库的《爱丽丝梦游仙境》书籍文本并将其加载到内存中，请使用以下代码：
- en: '[PRE7]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we transform the test into feature vectors that is fed into the RNN model.
    The `make_data` function reads the dataset, cleans it of any non-alphanumeric
    characters, splits it into individual characters and groups it into sequences
    of length `seq.len`. In this case, `seq.len` is set to `100`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将测试数据转换为特征向量，并将其输入到RNN模型中。`make_data`函数读取数据集，清除任何非字母数字字符，将其拆分为单个字符，并将其分组为长度为`seq.len`的序列。在这种情况下，`seq.len`设置为`100`：
- en: '[PRE8]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To remove those terms that are not part of dictionary, use the following code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除字典中不存在的术语，请使用以下代码：
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To build a dictionary and adjust it by `-1` to have a `1-lag` for labels, use
    the following code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建字典并通过`-1`调整它以具有`1-lag`标签，请使用以下代码：
- en: '[PRE10]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Set the sequence length as `100`, then build the long sequence of text from
    individual words in `alice` data character vector. Then call the `make_data()`
    function on the `alice_in_wonderland` text file. Observe that `seq.ln` and an
    empty dictionary is passed as input. `seq.ln` dictates the context that is the
    number of characters that the RNN need to look back inorder to generate the next
    character. During the training `seq.ln` is utilized to get the right weights:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 将序列长度设置为`100`，然后从`alice`数据字符向量中构建长文本序列。接着在`alice_in_wonderland`文本文件上调用`make_data()`函数。观察`seq.ln`和空字典作为输入。`seq.ln`决定了上下文，即RNN需要回溯多少个字符来生成下一个字符。在训练过程中，`seq.ln`被用来获取正确的权重：
- en: '[PRE11]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To view the prepared data, use the following code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看准备好的数据，使用以下代码：
- en: '[PRE12]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will give the following output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To view the `features` array, use the following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看`features`数组，使用以下代码：
- en: '[PRE14]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will give the following output:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/17b2ca08-f59e-455d-9cf1-370a3bfcf2cd.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/17b2ca08-f59e-455d-9cf1-370a3bfcf2cd.png)'
- en: 'To view the `labels` array, use the following code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看`labels`数组，使用以下代码：
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You will get the following output:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '![](img/b8899b0a-8a08-4677-84d7-18a40051849c.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b8899b0a-8a08-4677-84d7-18a40051849c.png)'
- en: 'Now, let''s print the dictionary, which includes the unique characters, using
    the following code:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打印字典，它包括唯一的字符，使用以下代码：
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You will get the following output:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '[PRE17]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Use the following code to print the indexes of the characters:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码打印字符的索引：
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will give the following output:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Use the following code block to fetch the features and labels to train the
    model, split the data into training and evaluation in a 90:10 ratio:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码块获取特征和标签以训练模型，将数据分成90:10的训练和评估比例：
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Use the following code to create iterators for training and evaluation datasets:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码创建训练和评估数据集的迭代器：
- en: '[PRE21]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Create a multi-layer RNN model to sample from character-level language models.
    It has a one-to-one model configuration since, for each character, we want to
    predict the next one. For a sequence of length `100`, there are also `100` labels,
    corresponding to the same sequence of characters but offset by a position of +1\.
    The parameter's `output_last_state` is set to `TRUE`, this is to access the state
    of the RNN cells when performing inference and we can see `lstm` cells are used.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个多层RNN模型以从字符级语言模型中采样。它有一个一对一的模型配置，因为对于每个字符，我们想要预测下一个字符。对于长度为`100`的序列，也有`100`个标签，对应相同的字符序列，但偏移量为+1。参数的`output_last_state`设置为`TRUE`，这是为了在推理时访问RNN单元的状态，我们可以看到使用了`lstm`单元。
- en: '[PRE22]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Use the following code to visualize the RNN model:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码可视化RNN模型：
- en: '[PRE23]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following diagram shows the resultant output:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了结果输出：
- en: '![](img/51bf4444-de3a-4cff-b28d-5740549bfac6.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/51bf4444-de3a-4cff-b28d-5740549bfac6.png)'
- en: 'Now, use the following line of code to set the CPU as the device to execute
    the code:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下代码行将CPU设置为执行代码的设备：
- en: '[PRE24]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, initializing the weights of the network through the Xavier initializer:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过Xavier初始化器初始化网络的权重：
- en: '[PRE25]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Use the `adadelta` optimizer to update the weights in the network through the
    learning process:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`adadelta`优化器通过学习过程更新网络中的权重：
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Use the following lines of code to set up logging of metrics and define a custom
    measurement function:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码行设置指标记录并定义一个自定义测量函数：
- en: '[PRE27]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Perplexity** is a measure of how variable a prediction model is. If perplexity
    is a measure of prediction error, define a function to compute the error, using
    the following lines of code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**困惑度**是衡量预测模型变化性的一个指标。如果困惑度是预测错误的度量，定义一个函数来计算错误，使用以下代码行：'
- en: '[PRE28]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Use the following code to execute the model creation and you will see that
    in this project we are running it for 20 iterations:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码执行模型创建，你将看到在这个项目中我们运行了20次迭代：
- en: '[PRE29]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will give the following output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE30]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, save the model for later use, then load the model from the disk to infer
    and sample the text character by character, and finally merge the predicted characters
    into a sentence using the following code:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，保存模型以供以后使用，然后从磁盘加载模型以进行推理和逐字符采样文本，最后使用以下代码将预测的字符合并成一个句子：
- en: '[PRE31]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Use the following code to provide the seed character to start the text with:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码提供种子字符以开始文本：
- en: '[PRE32]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Use the following lines of code to print the predicted text, after processing
    the predicted characters and merging them together into one sentence:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码行来打印预测的文本，在处理预测字符并将它们合并成一个句子之前：
- en: '[PRE33]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will give the following output:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE34]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We see from the output that our RNN is able to autogenerate text. Of course,
    the generated text is not very cohesive and it needs some improvement. There are
    several techniques we could rely upon to improve the cohesion and generate more
    meaningful text from an RNN. The following are some of these techniques:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，我们的RNN能够自动生成文本。当然，生成的文本并不非常连贯，需要一些改进。我们可以依赖几种技术来提高连贯性，并从RNN生成更有意义的文本。以下是一些这些技术：
- en: Implement a word-level language model instead of a character-level language
    model.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个词级语言模型而不是字符级语言模型。
- en: Use a larger RNN network.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更大的RNN网络。
- en: In our project, we used LTSM cells to build our RNN. Instead of LSTM cells,
    we could use GRU cells, which are more advanced.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的项目中，我们使用了LTSM单元来构建我们的RNN。我们本可以使用更先进的GRU单元而不是LSTM单元。
- en: We ran our RNN training for 20 iterations; this may be too little to get the
    right weights in place. We could try increasing the number of iterations and verifying
    the RNN yields better predictions.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们运行了20次RNN训练迭代；这可能太少，无法得到正确的权重。我们可以尝试增加迭代次数，并验证RNN是否能产生更好的预测。
- en: The current model used a dropout of 20%. This can be altered to check the effect
    on the overall predictions.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前模型使用了20%的dropout。这可以改变以检查对整体预测的影响。
- en: Our corpus retained very little punctuation; therefore, our model did not have
    the ability to predict punctuation as characters while generating text. Including
    punctuation in the corpus on which an RNN gets trained may yield better sentences
    and word endings.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的语料库保留了很少的标点符号；因此，我们的模型在生成文本时没有预测标点符号作为字符的能力。在RNN训练的语料库中包含标点符号可能会产生更好的句子和单词结尾。
- en: The `seq.ln` parameter decides the number of characters that need to be looked
    up in the history, prior to predicting the next character. In our model, we have
    set this as 100\. This may be altered to check whether the model produces better
    words and sentences.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seq.ln`参数决定了在预测下一个字符之前需要查找历史中的字符数量。在我们的模型中，我们将此设置为100。这可能需要改变以检查模型是否产生更好的单词和句子。'
- en: Due to space and time constraints, we are not going to be trying these options
    in this chapter. One or more of these options may be experimented with by interested
    readers to produce better words and sentences using a character RNN.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于空间和时间限制，我们不会在本章尝试这些选项。感兴趣的读者可以通过实验这些选项来使用字符RNN生成更好的单词和句子。
- en: Summary
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The major theme of this chapter was generating text automatically using RNNs.
    We started the chapter with a discussion about language models and their applications
    in the real world. We then carried out an in-depth overview of recurrent neural
    networks and their suitability for language model tasks. The differences between
    traditional feedforward networks and RNNs were discussed to get a clearer understanding
    of RNNs. We then went on to discuss problems and solutions related to the exploding
    gradients and vanishing gradients experienced by RNNs. After acquiring a detailed
    theoretical foundation of RNNs, we went ahead with implementing a character-level
    language model with an RNN. We used *Alice's Adventures in Wonderland* as a text
    corpus input to train the RNN model and then generated a string as output. Finally,
    we discussed some ideas for improving our character RNN model.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要主题是使用RNN自动生成文本。我们以关于语言模型及其在现实世界中的应用的讨论开始本章。然后，我们对循环神经网络及其在语言模型任务中的适用性进行了深入概述。讨论了传统前馈网络和RNN之间的差异，以更清楚地理解RNN。然后，我们讨论了RNN经历的梯度爆炸和梯度消失问题及其解决方案。在获得RNN的详细理论基础后，我们继续实现了一个使用RNN的字符级语言模型。我们使用《爱丽丝梦游仙境》作为文本语料库输入来训练RNN模型，然后生成一个字符串作为输出。最后，我们讨论了一些改进我们的字符RNN模型的想法。
- en: How about implementing a project to win more often when playing casino slot
    machines? This is something we will explore in the last but one chapter of this
    book. [Chapter 9](4b80233e-4fbe-4d90-ba32-5053930433c1.xhtml) is titled *Winning
    the Casino Slot Machine with Reinforcement Learning*. Come on, let's learn to
    earn free money.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 想必在玩老虎机时实施一个能让你更常赢的项目怎么样？这正是本书倒数第二章将要探讨的内容。[第9章](4b80233e-4fbe-4d90-ba32-5053930433c1.xhtml)的标题是《用强化学习赢得老虎机》。来吧，让我们一起学习如何赚取免费的钱。
