- en: Automatic Prose Generation with Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have been interacting through this book for almost 200 pages, but I realized
    that I have not introduced myself properly to you! I guess it''s time. You already
    know some bits about me through the author profile in this book; however, I want
    to tell you a bit about the city I live in. I am based in South India, in a city
    called Bengaluru, also know as Bangalore. The city is known for its IT talent
    and population diversity. I love the city, as it is filled with loads of positive
    energy. Each day, I get to meet people from all walks of life—people from multiple
    ethnicities, multiple backgrounds, people who speak multiple languages, and so
    on. Kannada is the official language spoken in the state of Karnataka where Bangalore
    is located. Though I can speak bits and pieces of Kannada, my proficiency with
    speaking the language is not as good as a native Kannada speaker. Of course, this
    is an area of improvement for me and I am working on it. Like me, many other migrants
    that moved to the city from other places also face problems while conversing in
    Kannada. Interestingly, not knowing the language does not stop any of us from
    interacting with locals in their own language. Guess what comes to our rescue:
    mobile apps such as Google translate, Google text-to-speech, and the like. These
    applications are built on NLP technologies called machine translation and speech
    recognition. These technologies in turn work on things known as **language models**.
    Language models is the topic we will delve into in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The objectives of the chapter include exploring the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The need for language modeling to address natural language processing tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The working principle of language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application of language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relationship between language modeling and neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences between a normal feedforward network and a recurrent neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long short-term memory networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A project to autogenerate text using recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the English language, the character *a* appears much more often in words
    and sentences than the character *x*. Similarly, we can also observe that the
    word *is* occurs more frequently than the word *specimen*. It is possible to learn
    the probability distributions of characters and words by examining large volumes
    of text. The following screenshot is a chart showing the probability distribution
    of letters given a corpus (text dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fca6b8ed-0b69-43c5-9860-2f2822b70e52.png)'
  prefs: []
  type: TYPE_IMG
- en: Probability distribution of letters in a corpus
  prefs: []
  type: TYPE_NORMAL
- en: We can observe that the probability distributions of characters are non-uniform.
    This essentially means that we can recover the characters in a word, even if they
    are lost due to noise. If a particular character is missing in a word, it can
    be reconstructed just based on the characters that are surrounding the missing
    character. The reconstruction of the missing character is not done randomly, but
    is done by picking the character that has the highest probability distribution
    of occurrence, given the characters that are surrounding the missing character.
    Technically speaking, the statistical structure of words in a sentence or characters
    in words follows the distance from maximal entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'A language model exploits the statistical structure of a language to express
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Given *w_1, w_2, w_3,...w_N* words in a sentence, a language model assigns a
    probability to a sentence *P(w_1, w_2, w_3,.... w_N)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It then assigns probability of an upcoming word (*w_4* in this case) as *P(w_4
    | w_1, w_2, w_3)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language models enable a number of applications to be developed in NLP, and
    some of them are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine translation**: P(enormous cyclone tonight) > P(gain typhoon this
    evening)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spelling correction**: P(satellite constellation) > P(satelitte constellation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech recognition**: P(I saw a van) > P(eyes awe of an)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Typing prediction**: Auto completion of in Google search, typing assistance
    apps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now look at how the probabilities are calculated for the words. Consider
    a simple sentence, *Decembers are cold*. The probability of this sentence is expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P("Decembers are cold") = P("December") * P ("are" | "Decembers") * P("cold"
    | "Decembers are")*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the probability computation of words in a sentence (or letters
    in a word) can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c05cd3c8-05cf-49c4-a4e6-d282ae3c708f.png)'
  prefs: []
  type: TYPE_IMG
- en: Andrey Markov, a Russian mathematician, described a stochastic process with
    a property called **Markov Property** or **Markov Assumption**. This basically
    states that one can make predictions for the future of the process based solely
    on its present state, just as well as one could knowing the process's full history,
    hence independently from such history.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on Markov''s assumption, we can rewrite the conditional probability of
    *cold* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P("cold" | "Decembers are") is congruent to* *P("cold" | "are")*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, Markov''s assumption can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d051944-7841-4762-86ff-9ebf71f50ad6.png)'
  prefs: []
  type: TYPE_IMG
- en: While this mathematical formulation represents the bigram model (two words taken
    into consideration at a time), it can be easily extended to an n-gram model. In
    the n-gram model, the conditional probability depends on just a couple more previous
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, an n-gram model is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f04d8634-6de8-4781-a727-58d55404307f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consider the famous poem *A Girl* by *Ezra Pound* as our corpus for building
    a **bigram** model. The following is the text corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We are already aware that in a bigram model, the conditional probability is
    computed just based on the previous word. So, the probability of a word can be
    computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b973bd5-ea2c-4c61-a4f6-756b8f7a49f5.png)'
  prefs: []
  type: TYPE_IMG
- en: If we were to compute the probability of the word *arms* given the word *my*
    in the poem, it is computed as the number of times the words *arms* and *my* appear
    together in the poem, divided by the number of times the word *my* appears in
    the poem.
  prefs: []
  type: TYPE_NORMAL
- en: We see that the words *my arms* appeared in the poem only once (in the sentence
    *The sap has ascended my arms*). However, the word *my* appeared in the poem three
    times (in the sentences *The tree has entered my hands*, *The sap has ascended
    my arms*, and *The tree has grown in my breast-Downward*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the conditional probability of the word *arms* given *my* is 1/3,
    formally represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P("arms" | "my") = P("arms", "my") / P("my") = 1 / 3*'
  prefs: []
  type: TYPE_NORMAL
- en: To calculate probability of the first and last words, the special tags <BOS>
    and <EOS> are added at the start and end of sentences, respectively. Similarly,
    the probability of a sentence or sequence of words can be calculated using the
    same approach by multiplying all the bigram probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: As language modeling involves predicting the next word in a sequence, given
    the sequence of words already present, we can train a language model to create
    subsequent words in a sequence from a given starting sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNNs**) are a family of neural networks for
    processing sequential data. RNNs are generally used to implement language models. We,
    as humans, base much of our language understanding on the context. For example,
    let''s consider the sentence *Christmas falls in the month of --------*. It is
    easy to fill in the blank with the word *December*. The essential idea here is
    that there is information about the last word encoded in the previous elements
    of the sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: The central theme behind the RNN architecture is to exploit the sequential structure
    of the data. As the name suggests, RNNs operate in a recurrent way. Essentially,
    this means that the same operation is performed for every element of a sequence
    or sentence, with its output depending on the current input and the previous operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'An RNN works by looping an output of the network at time *t* with the input
    of the network at time *t+1*. These loops allow persistence of information from
    one time step to the next one. The following diagram is a circuit diagram representing
    an RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94bd6b4e-b724-432d-b9f0-4f5c28137562.png)'
  prefs: []
  type: TYPE_IMG
- en: Circuit diagram representing a RNN
  prefs: []
  type: TYPE_NORMAL
- en: The diagram indicates an RNN that remembers what it knows from previous input
    using a simple loop. This loop takes the information from the previous timestamp
    and adds it to the input of the current timestamp. At a particular time step *t*,
    *X[t]* is the input to the network, *O[t]* is the output of the network, and *h[t]*
    is the detail it remembered from previous nodes in the network. In between, there
    is the RNN cell, which contains neural networks just like a feedforward network.
  prefs: []
  type: TYPE_NORMAL
- en: One key point to ponder in terms of the definition of an RNN is the timestamps.
    The timestamps referred to in the definition have nothing to do with past, present,
    and future. They simply represent a word or an item in a sequence or a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider an example sentence: *Christmas Holidays are Awesome*. In this
    sentence, take a look at the following timestamp:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Christmas* is x[0]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Holidays* is x[1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*are* is x[2];'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Awesome* is x[3]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If t=1, then take a look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: x[t] = *Holidays* → event at current timestamp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x[t-1] = *Christmas* → event at previous timestamp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It can be observed from the preceding circuit diagram that the same operation
    is performed in the RNN repeatedly on different nodes. There is also a black square
    in the diagram that represents a time delay of a single time step. It may be confusing
    to understand the RNN with the loops, so let''s unfold the computational graph.
    The unfolded RNN computational graph is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a1b4211-5c6e-4d7e-b757-1459e993d077.png)'
  prefs: []
  type: TYPE_IMG
- en: RNN—unfolded computational graph view
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, each node is associated with a particular time. In
    the RNN architecture, each node receives different inputs at each time step **x[t]**.
    It also has the capability of producing outputs at each time step **o[t]**. The
    network also maintains a memory state **h[t]**, which contains information about
    what happened in the network up to the time *t*. As this is the same process that
    is run across all the nodes in the network, it is possible to represent the whole
    network in a simplified form, as shown in the RNN circuit diagram.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we understand that we see the word **recurrent** in RNNs because it performs
    the same task for every element of a sequence, with the output depending on previous
    computations. It may be noted that, theoretically, RNNs can make use of information
    in arbitrarily long sequences, but in practice, they are implemented to looking
    back only a few steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, an RNN can be defined in an equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e30363fe-7188-4b4e-9d21-010c52cf2326.png)'
  prefs: []
  type: TYPE_IMG
- en: In the equation, *h[t]* is the hidden state at timestamp *t*. An activation
    function such as Tanh, Sigmoid, or ReLU can be applied to compute the hidden state
    and it is represented in the equation as [![](img/de0d48dd-c787-4efe-a5e2-1f26ce1485de.png)].
    *W* is the weight matrix for the input to the hidden layer at timestamp *t*. *X[t]*
    is the input at timestamp *t*. *U* is the weight matrix for the hidden layer at
    time *t-1* to the hidden layer at time *t*, and *h[t-1]* is the hidden state at
    timestamp *t*.
  prefs: []
  type: TYPE_NORMAL
- en: 'During backpropagation, *U* and *W* weights are learned by the RNN. At each
    node, the contribution of the hidden state and the contribution of the current
    input are decided by *U* and *W*. The proportions of *U* and *W* in turn result
    in the generation of output at the current node. The activation functions add
    non-linearity in RNNs, thus enabling the simplification of gradient calculations
    during the backpropagation process. The following diagram illustrates the idea
    of backpropagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4794f067-f59f-4ebd-9046-7519eddc4c4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation in neural networks
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the overall working mechanism of an RNN and the
    way the weights *U* and *W* are learned through backpropagation. It also depicts
    the use of the *U* and *W* weight matrices in the network to generate the output,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7848cff1-e4ef-40cb-a5fa-f439878690be.png)'
  prefs: []
  type: TYPE_IMG
- en: Role of weights in RNN
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of feedforward neural networks and RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One fundamental difference between other neural networks and RNNs is that,
    in all other networks, the inputs are independent of each other. However, in an
    RNN, all the inputs are related to each other. In an application, to predict the
    next word in a given sentence, the relationship between all the previous words
    helps to predict the current output. In other words, an RNN remembers all these
    relationships while training itself. This is not the case with other types of
    neural networks. A representation of a feedforward network is illustrated in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f72615fe-2f5b-4848-8b9c-970e2468b790.png)'
  prefs: []
  type: TYPE_IMG
- en: Feedforward neural network architecture
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding diagram, we can see that no loops are involved in the feedforward
    network architecture. This is in contrast to that of the RNN architecture depicted
    in the diagrams of the RNN circuit diagram and the RNN unfolded computational
    graph. The series of mathematical operations in a feedforward network is performed
    at the nodes and the information is processed straight through, with no loops
    whatsoever.
  prefs: []
  type: TYPE_NORMAL
- en: Using supervised learning, the input that is fed to a feedforward network is
    transformed into output. The output in this case could be a label if it is classification,
    or it is a number in the case of regression. If we consider image classification,
    a label can be *cat* or *dog* for an image given as input.
  prefs: []
  type: TYPE_NORMAL
- en: 'A feedforward neural network is trained on labeled images until errors are
    minimized in predicting the labels. Once trained, the model is able to classify
    even images that it has not seen previously. A trained feedforward network can
    be exposed to any random collection of photographs; the categorization of the
    first photograph does not have any impact or influence on the second or subsequent
    photographs that the model needs to categorize. Let''s discuss this with an example
    for better clarity on the concept: if the first image is seen as a *dog* by the
    feedforward network, it does not imply that the second image will be classified
    as *cat*. In other words, the predictions that the model arrives at have no notion
    of order in time, and the decision regarding the label is arrived at just based
    on the current input that is provided. To summarize, in feedforward networks no
    information on historical predictions is used for current predictions. This is
    very different from RNNs, where the previous prediction is considered in order
    to aid the current prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important difference is that feedforward networks, by design, map one
    input to one output, whereas RNNs can take multiple forms: map one input to multiple
    outputs, many inputs to many outputs, or many inputs to one output. The following
    diagram depicts the various input-output mappings possible with RNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c169342-447e-45c2-8568-cc4001515979.png)'
  prefs: []
  type: TYPE_IMG
- en: Input-output mapping possibilities with RNNs
  prefs: []
  type: TYPE_NORMAL
- en: Let's review some of the practical applications of the various input-output
    mappings possible with an RNN. Each rectangle in the preceding diagram is a vector
    and the arrows represent functions, for example, a matrix multiplication. The
    input vectors are the lower rectangles (colored in red), and the output vectors
    are the upper rectangles (colored in blue color). The middle rectangles (colored
    in green) are vectors that hold the RNN's state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the various forms of mapping illustrated in the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '**One input to one output**: The leftmost one is a vanilla mode of processing
    without RNN, from fixed-sized input to fixed-sized output; for example, image
    classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One input to many outputs**: Sequence output, for example, image captioning
    takes an image as input and it outputs a sentence of words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many inputs to one output**: Sequence input, for example, sentiment analysis
    where a given sentence is given as input to the RNN, and the output is a classification
    expressing positive or negative sentiment of the sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many inputs to many outputs**: Sequence input and sequence output; for example,
    for a machine translation task, an RNN reads a sentence in English as input and
    then outputs a sentence in Hindi or some other language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many inputs to many outputs**: Synced sequence input and output, for example,
    video classification where we wish to label each frame of the video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now review the final difference between a feedforward network and an
    RNN. The way backpropagation is done in order to set the weights in a feedforward
    neural network is different from that of what is called **backpropagation through
    time** (**BPTT**), which is carried out in an RNN. We are already aware that the
    objective of the backpropagation algorithm in neural networks is to adjust the
    weights of a neural network to minimize the error of the network outputs compared
    to some expected output in response to corresponding inputs. Backpropagation itself
    is a supervised learning algorithm that allows the neural network to be corrected
    with regard to the specific errors made. The backpropagation algorithm involves
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide training input to the neural network and propagate it through the network
    to get the output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the predicted output to the actual output and calculate the error
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the derivatives of the error with respect to the learned network weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the weights to minimize the error
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In feedforward networks, it makes sense to run backpropagation at the end as
    the output is available only at the end. In RNNs, the output is produced at each
    time step and this output influences the output in the subsequent time steps.
    In other words, in RNNs, the error of a time step depends on the previous time
    step. Therefore, the normal backpropagation algorithms are not suitable for RNNs.
    Hence, a different algorithm known as BPTT is used to modify the weights in an
    RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are already aware that RNNs are cyclical graphs, unlike feedforward networks,
    which are acyclic directional graphs. In feedforward networks, the error derivatives
    are calculated from the layer above. However, in an RNN we don''t have such layering
    to perform error derivative calculations. A simple solution to this problem is
    to unroll the RNN and make it similar to a feedforward network. To enable this,
    the hidden units from the RNN are replicated at each time step. Each time step
    replication forms a layer that is similar to layers in a feedforward network.
    Each time step *t* layer connects to all possible layers in the time step *t+1*.
    Therefore, we randomly initialize the weights, unroll the network, and then use
    backpropagation to optimize the weights in the hidden layer. The lowest layer
    is initialized by passing parameters. These parameters are also optimized as a
    part of backpropagation. The backpropagation through time algorithm involves the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a sequence of time steps of input and output pairs to the network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unroll the network, then calculate and accumulate errors across each time step
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Roll up the network and update weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In summary, with BPTT, the error is back propagated from the last to the first
    time step, while unrolling all the time steps. The error for each time step is
    calculated, which allows updating the weights. The following diagram is a visualization
    showing the backpropagation through time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da6bf332-cf3c-4b93-b0d5-3267c455859b.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation through time in an RNN
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that as the number of time steps increases, the BPTT algorithm
    can get computationally very expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Problems and solutions to gradients in RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are not perfect, there are two main issues namely **exploding gradients**
    and **vanishing gradients** that they suffer from. To understand the issues, let's
    first understand what a gradient means. A gradient is a partial derivative with
    respect to its inputs. In simple layman's terms, a gradient measures how much
    the output of a function changes, if one were to change the inputs a little bit.
  prefs: []
  type: TYPE_NORMAL
- en: Exploding gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploding gradients relate to a situation where the BPTT algorithm assigns an
    insanely high importance to the weights, without a rationale. The problem results
    in an unstable network. In extreme situations, the values of weights can become
    so large that the values overflow and result in NaN values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The exploding gradients problem can be detected through observing the following
    subtle signs while training the network:'
  prefs: []
  type: TYPE_NORMAL
- en: The model weights quickly become very large during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model weights become NaN values during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The error gradient values are consistently above 1.0 for each node and layer
    during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several ways in which one could handle the exploding gradients problem.
    The following are some of the popular techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: This problem can be easily solved if we can truncate or squash the gradients.
    This is known as **gradient clipping**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating weights across fewer prior time steps during training may also reduce
    the exploding gradient problem. This technique of having fewer step updates is
    called **truncated backpropagation through time** (**TBPTT**). It is an altered
    version of the BPTT training algorithm where the sequence is processed one time
    step at a time, and periodically (*k1* time steps) the BPTT update is performed
    back for a fixed number of time steps (*k2* time steps). *k1* is the number of
    forward-pass time steps between updates. *k2* is the number of time steps to which
    to apply BPTT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight regularization can be done by checking the size of network weights and
    applying a penalty to the networks loss function for large weight values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using **long short-term memory units** (**LSTMs**) or **gated recurrent units**
    (**GRUs**) instead of plain vanilla RNNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Careful initialization of weights such as **Xavier** initialization or **He**
    initialization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanishing gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are already aware that long-term dependencies are very important for RNNs
    to function correctly. RNNs can become too deep because of the long-term dependencies.
    The vanishing gradient problem arises in cases where the gradient of the activation
    function is very small. During backpropagation, when the weights are multiplied
    with the low gradients, they tend to become very small and vanish as they go further
    into the network. This makes the neural network forget the long-term dependency.
    The following diagram is an illustration showing the cause that leads to vanishing
    gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eea2eb79-2b6a-44fe-8b72-e246a405ec79.png)'
  prefs: []
  type: TYPE_IMG
- en: The cause of vanishing gradients
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, due to the vanishing gradients problem, RNNs experience difficulty
    in memorizing previous words very far away in the sequence and are only able to
    make predictions based on the most recent words. This can impact the accuracy
    of RNN predictions. At times, the model may fail to predict or classify what it
    is supposed to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways in which one could handle the vanishing gradients problem.
    The following are some of the most popular techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize network weights for the identity matrix so that the potential for
    a vanishing gradient is minimized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting the activation functions to ReLU instead of `sigmoid` or `tanh`. This
    makes the network computations stay close to the identity function. This works
    well because when the error derivatives are being propagated backwards through
    time, they remain constants of either 0 or 1, and so aren't likely to suffer from
    vanishing gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LSTMs, which are a variant of the regular recurrent network designed to
    make it easy to capture long-term dependencies in sequence data. The standard
    RNN operates in such a way that the hidden state activation is influenced by the
    other local activations closest to it, which corresponds to a **short-term memory**,
    while the network weights are influenced by the computations that take place over
    entire long sequences, which corresponds to a **long-term memory**. The RNN was
    redesigned so that it has an activation state that can also act like weights and
    preserve information over long distances, hence the name **long short-term memory**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In LSTMs, rather than each hidden node being simply a node with a single activation
    function, each node is a memory cell in itself that can store other information.
    Specifically, it maintains its own cell state. Normal RNNs take in their previous
    hidden state and the current input, and output a new hidden state. An LSTM does
    the same, except it also takes in its old cell state and will output its new cell
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Building an automated prose generator with an RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this project, we will attempt to build a character-level language model using
    an RNN to generate prose given some initial seed characters. The main task of
    a character-level language model is to predict the next character given all previous
    characters in a sequence of data. In other words, the function of an RNN is to
    generate text character by character.
  prefs: []
  type: TYPE_NORMAL
- en: To start with, we feed the RNN a huge chunk of text as input and ask it to model
    the probability distribution of the next character in the sequence, given a sequence
    of previous characters. These probability distributions conceived by the RNN model
    will then allow us to generate new text, one character at a time.
  prefs: []
  type: TYPE_NORMAL
- en: The first requirement for building a language model is to secure a corpus of
    text that the model can use to compute the probability distribution of various
    characters. The larger the input text corpus, the better the RNN will model the
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: We do not have to strive a lot to secure the big text corpus that is required
    to train the RNN. There are classical texts (books) such as *The Bible* that can
    be used as a corpus. The best part is many of the classical texts are no longer
    protected under copyright. Therefore, the texts can be downloaded and used freely
    in our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project Gutenberg is the best place to get access to free books that are no
    longer protected by copyright. Project Gutenberg can be accessed through the URL
    [http://www.gutenberg.org](http://www.gutenberg.org). There are several books
    such as *The Bible*, *Alice''s Adventures in Wonderland*, and so on are available
    from Project Gutenberg. As of December 2018, there are 58,486 books available
    for download. The books are available in several formats for us to be able to
    download and use, not just for this project, but for any project where huge text
    corpus input is required. The following screenshot is of a sample book from Project
    Gutenberg and the various formats in which the book is available for download:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87b584f1-a1ca-477f-8cad-974bc49ba6aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample book available from Project Gutenberg in various formats
  prefs: []
  type: TYPE_NORMAL
- en: 'Irrespective of the format of the file that is downloaded, Project Gutenberg
    adds standard header and footer text to the actual book text. The following is
    an example of the header and footer that can be seen in a book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It is essential that we remove this header and footer text from the book text
    downloaded from Project Gutenberg website. For a text file that is downloaded,
    one can open the file in a text editor and delete the header and footer.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our project in this chapter, let''s use a favorite book from childhood
    as the text corpus: *Alice’s Adventures in Wonderland* by Lewis Carroll. While
    we have an option to download the text format of this book from Project Gutenberg
    and make use of it as a text corpus, the R language''s `languageR` library makes
    the task even more easier for us. The `languageR` library already has the *Alice’s
    Adventures in Wonderland* book text. After installing the `languageR` library,
    use the following code to load the text data into the memory and print the loaded
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We see from the output that the book text is stored as a character vector, where
    each of the vector items is a word from the book text that is split by punctuation.
    It may also be noted that not all the punctuation is retained in the book text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code reconstructs the sentences from the words in the character
    vector. Of course, we do not get things like sentence boundaries during the reconstruction
    process, as the character vector does not have as much punctuation as character
    vector items. Now, let''s do the reconstruction of the book text from individual
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: From the output, we see that a long string of text is constructed from the words.
    Now, we can move on to doing some preprocessing on this text to feed it to the
    RNN so that the model learns the dependencies between characters and the conditional
    probabilities of characters in sequences.
  prefs: []
  type: TYPE_NORMAL
- en: One of the things to note is that, as with a character-level language model
    that generates the next character in a sequence, you can build a word-level language
    model too. However, the character-level language model has an advantage in that
    it can create its own unique words that are not in the vocabulary we train it
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now learn how RNN works to conceive the dependencies between characters
    in sequences. Assume that we only had a vocabulary of four possible letters, [*a*,
    *p*, *l*, *e*], and the intent is to train an RNN on the training sequence *apple*.
    This training sequence is in fact a source of four separate training examples:'
  prefs: []
  type: TYPE_NORMAL
- en: The probability of the letter *p* should be likely, given the context of *a,
    , *in other words, the conditional probability of *p* given the letter *a* in
    the word *apple*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to the first point, *p* should be likely in the context of *ap*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *letter l* should also be likely given the context of *app*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *letter e* should be likely given the context of *appl*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We start to encode each character in the word *apple* into a vector using 1-of-k
    encoding. 1-of-k encoding represents each character in the word as all zeros,
    except for the single 1 at the index of the character in the vocabulary. Each
    character thus represented with 1-of-k encoding is then fed into the RNN one at
    a time with the help of a step function. The RNN takes this input and generates
    a four-dimensional output vectors (one dimension per character, and recollect
    we only have four characters in our vocabulary). This output vector can be interpreted
    as the confidence that the RNN currently assigns to each character coming next
    in the sequence. The following diagram is a visualization of the RNN learning
    the characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/709fd0f0-e868-420e-b67d-2100b54c96f3.png)'
  prefs: []
  type: TYPE_IMG
- en: RNN learning the character language model
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we see an RNN with four-dimensional input and output
    layers. There is also a hidden layer with three neurons. The diagram displays
    the activations in the forward pass when the RNN is fed with the input of the
    characters *a*, *p*, *p*, and *l*. The output layer contains the confidence that
    the RNN assigned to each of the following characters. The expectation of the RNN
    is for the green numbers in the output layer to be higher than the red numbers.
    The high values of green numbers enable the prediction of the right characters
    as per the input.
  prefs: []
  type: TYPE_NORMAL
- en: We see that in the first time step, when the RNN is fed the input character
    *a*, it assigned a confidence of 1.0 to the next letter being *a*, 2.2 as confidence
    to letter *p*, -3.0 to *l*, and 4.1 to *e*. As per our training data, the sequence
    we considered is *apple*; therefore, the next correct character is *p* given the
    character *a* as input in the first time step. We would like our RNN to maximize
    the confidence in the first step (indicated in green) and minimize the confidence
    of all other letters (indicated in red). Likewise, we have a desired output character
    at each one of the four time steps that we would like our RNN to assign a greater
    confidence to.
  prefs: []
  type: TYPE_NORMAL
- en: Since the RNN consists entirely of differentiable operations, we can run the
    backpropagation algorithm to figure out in what direction we should adjust each
    one of its weights to increase the scores of the correct targets (the bold green
    numbers).
  prefs: []
  type: TYPE_NORMAL
- en: Based on the gradient direction, the parameters are updated and the algorithm
    actually alters the weight by a tiny amount in the same direction as that of the
    gradient. Ideally, if gradient decent has successfully run and updated the weights,
    we would see a slightly higher weight for the right choice and lower weights for
    the incorrect characters. For example, we would find that the scores of the correct
    character *p* in the first time step would be slightly higher, say 2.3 instead
    of 2.2\. At the same time, the scores for the other characters *a*, *l*, and *e*
    would be observed as lower than that of the score that was assigned prior to gradient
    descent.
  prefs: []
  type: TYPE_NORMAL
- en: The process of updating the parameters through gradient descent is repeated
    multiple times in the RNN until the network converges, in other words, until the
    predictions are consistent with the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Technically speaking, we run the standard softmax classifier, otherwise called
    the cross-entropy loss, on every output vector simultaneously. The RNN is trained
    with mini-batch stochastic gradient descent or adaptive learning rate methods
    such as RMSProp or Adam to stabilize the updates.
  prefs: []
  type: TYPE_NORMAL
- en: You may notice that the first time the character *p* is input, the output is
    *p*; however, the second time the same input is fed, the output is *l*. An RNN,
    therefore, cannot rely only on the input that is given. This is where an RNN uses
    its recurrent connection to keep track of the context to perform the task and
    make the correct predictions. Without the context, it would have been challenging
    for the network to predict the right output specifically, given the same input.
  prefs: []
  type: TYPE_NORMAL
- en: When we have to generate text using the trained RNN model, we provide a seed
    input character to the network and get the distribution over what characters are
    likely to come next. The distribution is then sampled and fed it right back in,
    to get the next letter. The process is repeated until the maximum number of characters
    is reached (until a specific user-defined character length), or until the model
    encounters an end of line character such as <EOS> or <END>.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how an RNN is able to build a character-level model, let''s
    implement the project to generate our own words and sentences through an RNN.
    Generally, RNN training is computationally intensive and it is suggested that
    we run the code on a **graphical processing unit** (**GPU**). However, due to
    infrastructure limitations, we are not going to use a GPU for the project code.
    The `mxnet` library allows a character-level language model with an RNN to be
    executed on the CPU itself, so let''s start coding our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the `languageR` library''s *ALICE''S ADVENTURES IN WONDERLAND* book
    text and load it into memory, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we transform the test into feature vectors that is fed into the RNN model.
    The `make_data` function reads the dataset, cleans it of any non-alphanumeric
    characters, splits it into individual characters and groups it into sequences
    of length `seq.len`. In this case, `seq.len` is set to `100`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To remove those terms that are not part of dictionary, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To build a dictionary and adjust it by `-1` to have a `1-lag` for labels, use
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the sequence length as `100`, then build the long sequence of text from
    individual words in `alice` data character vector. Then call the `make_data()`
    function on the `alice_in_wonderland` text file. Observe that `seq.ln` and an
    empty dictionary is passed as input. `seq.ln` dictates the context that is the
    number of characters that the RNN need to look back inorder to generate the next
    character. During the training `seq.ln` is utilized to get the right weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the prepared data, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the `features` array, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17b2ca08-f59e-455d-9cf1-370a3bfcf2cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To view the `labels` array, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8899b0a-8a08-4677-84d7-18a40051849c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s print the dictionary, which includes the unique characters, using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to print the indexes of the characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code block to fetch the features and labels to train the
    model, split the data into training and evaluation in a 90:10 ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to create iterators for training and evaluation datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Create a multi-layer RNN model to sample from character-level language models.
    It has a one-to-one model configuration since, for each character, we want to
    predict the next one. For a sequence of length `100`, there are also `100` labels,
    corresponding to the same sequence of characters but offset by a position of +1\.
    The parameter's `output_last_state` is set to `TRUE`, this is to access the state
    of the RNN cells when performing inference and we can see `lstm` cells are used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to visualize the RNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the resultant output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51bf4444-de3a-4cff-b28d-5740549bfac6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, use the following line of code to set the CPU as the device to execute
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, initializing the weights of the network through the Xavier initializer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `adadelta` optimizer to update the weights in the network through the
    learning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following lines of code to set up logging of metrics and define a custom
    measurement function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**Perplexity** is a measure of how variable a prediction model is. If perplexity
    is a measure of prediction error, define a function to compute the error, using
    the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to execute the model creation and you will see that
    in this project we are running it for 20 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, save the model for later use, then load the model from the disk to infer
    and sample the text character by character, and finally merge the predicted characters
    into a sentence using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to provide the seed character to start the text with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following lines of code to print the predicted text, after processing
    the predicted characters and merging them together into one sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We see from the output that our RNN is able to autogenerate text. Of course,
    the generated text is not very cohesive and it needs some improvement. There are
    several techniques we could rely upon to improve the cohesion and generate more
    meaningful text from an RNN. The following are some of these techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement a word-level language model instead of a character-level language
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a larger RNN network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our project, we used LTSM cells to build our RNN. Instead of LSTM cells,
    we could use GRU cells, which are more advanced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We ran our RNN training for 20 iterations; this may be too little to get the
    right weights in place. We could try increasing the number of iterations and verifying
    the RNN yields better predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current model used a dropout of 20%. This can be altered to check the effect
    on the overall predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our corpus retained very little punctuation; therefore, our model did not have
    the ability to predict punctuation as characters while generating text. Including
    punctuation in the corpus on which an RNN gets trained may yield better sentences
    and word endings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `seq.ln` parameter decides the number of characters that need to be looked
    up in the history, prior to predicting the next character. In our model, we have
    set this as 100\. This may be altered to check whether the model produces better
    words and sentences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to space and time constraints, we are not going to be trying these options
    in this chapter. One or more of these options may be experimented with by interested
    readers to produce better words and sentences using a character RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The major theme of this chapter was generating text automatically using RNNs.
    We started the chapter with a discussion about language models and their applications
    in the real world. We then carried out an in-depth overview of recurrent neural
    networks and their suitability for language model tasks. The differences between
    traditional feedforward networks and RNNs were discussed to get a clearer understanding
    of RNNs. We then went on to discuss problems and solutions related to the exploding
    gradients and vanishing gradients experienced by RNNs. After acquiring a detailed
    theoretical foundation of RNNs, we went ahead with implementing a character-level
    language model with an RNN. We used *Alice's Adventures in Wonderland* as a text
    corpus input to train the RNN model and then generated a string as output. Finally,
    we discussed some ideas for improving our character RNN model.
  prefs: []
  type: TYPE_NORMAL
- en: How about implementing a project to win more often when playing casino slot
    machines? This is something we will explore in the last but one chapter of this
    book. [Chapter 9](4b80233e-4fbe-4d90-ba32-5053930433c1.xhtml) is titled *Winning
    the Casino Slot Machine with Reinforcement Learning*. Come on, let's learn to
    earn free money.
  prefs: []
  type: TYPE_NORMAL
