["```py\nfrom sklearn import preprocessing\ndata = load_articles()\nenc = preprocessing.LabelEncoder()\nenc.fit(data)\nenc.transform(data)\n```", "```py\nfrom sklearn import preprocessing\ndata = load_ratings()\nenc = preprocessing.LabelEncoder()\nenc.fit(data)\nenc.transform(data)\n```", "```py\ngood < very good < bad < average\n```", "```py\nvery bad < bad < average < good < very good\n```", "```py\nimport pandas as pd\ndata = load_ratings()\ncategories = [\n    'very bad', 'bad', 'average', 'good', 'very good']\ndata = pd.Categorical(data,\n categories=categories,\n ordered=True)\nprint(data.codes)\n```", "```py\nfrom sklearn import preprocessing\ndata = [load_articles()]\nenc = preprocessing.OneHotEncoder()\nenc.fit(data)\nenc.transform(data)\n```", "```py\nAlmost before we knew it, we had left the ground. The unknown holds its grounds.\n```", "```py\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\ntokens = word_tokenize(document)\nprint(tokens)\n```", "```py\n['Almost', 'before', 'we', 'knew', 'it', ',', 'we', 'had', 'left', 'the', 'ground', '.', 'The', 'unknown', 'holds', 'its', 'grounds', '.']\n```", "```py\nwords = [word.lower() for word in tokens if word.isalnum()]\nprint(words)\n```", "```py\n['almost', 'before', 'we', 'knew', 'it', 'we', 'had', 'left', 'the', 'ground', 'the', 'unknown', 'holds', 'its', 'grounds']\n```", "```py\nfrom nltk.corpus import stopwords\nstopword_set = set(stopwords.words('english'))\nwords = [word for word in words if word not in stopword_set]\nprint(words)\n```", "```py\n['almost', 'knew', 'left', 'ground', 'unknown', 'holds', 'grounds']\n```", "```py\ncars   -> car\nsaying -> say\nflies  -> fli\n```", "```py\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nwords = [stemmer.stem(word) for word in words]\nprint(words)\n```", "```py\n['almost', 'knew', 'left', 'ground', 'unknown', 'hold', 'ground']\n```", "```py\nare    -> be\nis     -> be\ntaught -> teach\nbetter -> good\n```", "```py\nimport nltk\nnltk.download('averaged_perceptron_tagger')\ntags = nltk.pos_tag(tokens)\nprint(tags)\n```", "```py\n[('Almost', 'RB'), ('before', 'IN'), ('we', 'PRP'), ('knew', 'VBD'), ('it', 'PRP'), (',', ','), ('we', 'PRP'), ('had', 'VBD'), ('left', 'VBN'), ('the', 'DT'), ('ground', 'NN'), ('.', '.'), ('The', 'DT'), ('unknown', 'JJ'), ('holds', 'VBZ'), ('its', 'PRP$'), ('grounds', 'NNS'), ('.', '.')]\n```", "```py\nimport nltk\nnltk.download('tagsets')\nnltk.help.upenn_tagset()\n```", "```py\nCC: conjunction, coordinating\n    & 'n and both but either et for less minus neither nor or \n    plus so therefore times v. versus vs. whether yet\nCD: numeral, cardinal\n    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 \n    one forty- seven 1987 twenty '79 zero two 78-degrees \n    eighty-four IX '60s .025 fifteen 271,124 dozen quintillion \n    DM2,000 ...\nDT: determiner\n    all an another any both del each either every half la many \n    much nary neither no some such that the them these this \n    those\nEX: existential there\n    there\nFW: foreign word\n    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si \n    vous lutihaw alai je jour objets salutaris fille quibusdam \n    pas \n...\n```", "```py\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')       \nlemmatizer = WordNetLemmatizer()\ntag_dict = {\n    \"J\": wordnet.ADJ,\n    \"N\": wordnet.NOUN,\n    \"V\": wordnet.VERB,\n    \"R\": wordnet.ADV\n}\npos = [tag_dict.get(t[0].upper(), wordnet.NOUN) \\\n        for t in zip(*tags)[1]]\nwords = [lemmatizer.lemmatize(w, pos=p) \\\n        for w, p in zip(words, pos)]\nprint(words)\n```", "```py\n['almost', 'know', 'leave', 'ground', 'unknown', 'hold', 'ground']\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\ndata = [\" \".join(words)]\nX_train_counts = count_vect.fit_transform(data)\nprint(X_train_counts)\n```", "```py\n  (0, 0)        1\n  (0, 3)        1\n  (0, 4)        1\n  (0, 1)        2\n  (0, 5)        1\n  (0, 2)        1\n```", "```py\nprint(count_vect.vocabulary_)\n```", "```py\n{'almost': 0, 'know': 3, 'leave': 4, 'ground': 1, 'unknown': 5, 'hold': 2}\n```", "```py\nA, B, C, D -> 1-Gram: A, B, C, D\nA, B, C, D -> 2-Gram: AB, BC, CD\nA, B, C, D -> 3-Gram: ABC, BCD\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(ngram_range=(1,2))\nX_train_counts = count_vect.fit_transform(data)\nprint(count_vect.vocabulary_)\n```", "```py\n{'almost': 0, 'before': 2, 'we': 24, 'knew': 15, 'it': 11, 'had': 7, 'left': 17, 'the': 19, 'ground': 4, 'unknown': 22, 'holds': 9, 'its': 13, 'grounds': 6, 'almost before': 1, 'before we': 3, 'we knew': 26, 'knew it': 16, 'it we': 12, 'we had': 25, 'had left': 8, 'left the': 18, 'the ground': 20, 'ground the': 5, 'the unknown': 21, 'unknown holds': 23, 'holds its': 10, 'its grounds': 14}\n```", "```py\nA, B, C, D -> 2-Gram (1 skip): AB, AC, BC, BD, CD\nA, B, C, D -> 2-Gram (2 skip): AB, AC, AD, BC, BD, CD\n```", "```py\nterms = list(nltk.skipgrams(document.split(' '), 2, 1))\nprint(terms)\n```", "```py\n[('Almost', 'before'), ('Almost', 'we'), ('before', 'we'), ('before', 'knew'), ('we', 'knew'), ('we', 'it,'), ('knew', 'it,'), ('knew', 'we'), ('it,', 'we'), ('it,', 'had'), ('we', 'had'), ('we', 'left'), ('had', 'left'), ('had', 'the'), ('left', 'the'), ('left', 'ground.'), ('the', 'ground.'), ('the', 'The'), ('ground.', 'The'), ('ground.', 'unknown'), ('The', 'unknown'), ('The', 'holds'), ('unknown', 'holds'), ('unknown', 'its'), ('holds', 'its'), ('holds', 'grounds.'), ('its', 'grounds.')]\n```", "```py\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=5)\nX_lsa = svd.fit_transform(X_train_counts) \n```", "```py\nPrint(svd.explained_variance_ratio_.sum())\n```", "```py\n0.19693920498587408\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer()\ndata = [\" \".join(words)]\nX_train_counts = vect.fit_transform(data)\nprint(X_train_counts)\n```", "```py\n(0, 2)        0.3333333333333333\n(0, 5)        0.3333333333333333\n(0, 1)        0.6666666666666666\n(0, 4)        0.3333333333333333\n(0, 3)        0.3333333333333333\n(0, 0)        0.3333333333333333\n```", "```py\nprint(vect.get_feature_names())\n```", "```py\n['almost', 'ground', 'hold', 'know', 'leave', 'unknown']\n```", "```py\nKing – Man + Woman = Queen\n```", "```py\n    from gensim.models import Word2Vec\n    model = Word2Vec(words, size=100, window=5)\n    vector = model.wv['ground']\n    ```", "```py\n    dim = len(model.wv.vectors[0])\n    X = np.mean([model.wv[w] for w in words if w in model.wv] \\\n            or [np.zeros(dim)], axis=0)\n    ```", "```py\n    # download pre-trained dictionary from \n    # http://nlp.stanford.edu/data/glove.6B.zip\n    glove = {}\n    with open('glove.6B.100d.txt') as f:\n      for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, 'f', sep=' ')\n        glove[word] = coefs\n    ```", "```py\n    X = np.mean([glove[w] for w in words if w in glove] \\\n          or [np.zeros(dim)], axis=0)\n    ```", "```py\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    from tensorflow.keras.preprocessing.sequence import \\\n           pad_sequences\n    num_words = 1000\n    tokenizer = Tokenizer(num_words=num_words)\n    tokenizer.fit_on_texts(X_words)\n    X = tokenizer.texts_to_sequences(X_words)\n    X = pad_sequences(X, maxlen=2000)\n    ```", "```py\n    from tensorflow.keras.layers import Embedding, LSTM, Dense\n    from tensorflow.keras.models import Sequential\n    embed_dim = 128\n    lstm_out = 196\n    model = Sequential()\n    model.add(Embedding(\n        num_words, embed_dim, input_length=X.shape[1]))\n    model.add(LSTM(\n        lstm_out, recurrent_dropout=0.2, dropout=0.2))\n    model.add(Dense(\n        len(labels), activation='softmax'))\n    model.compile(loss='categorical_crossentropy', \n                  optimizer='adam',\n                  metrics=['categorical_crossentropy'])\n    ```", "```py\nfrom transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\")\nresult = classifier(\"Azure ML is quite good.\")[0]\nprint(\"Label: %s, with score: %.2f\" %\n         (result['label'], result['score']))\n```", "```py\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\ninputs = tokenizer(\"Azure ML is quite good.\",   \n                   return_tensors=\"tf\")\n```", "```py\nfrom transformers import TFBertModel\nmodel = TFBertModel.from_pretrained('bert-base-uncased')\noutputs = model(**inputs)\nprint(outputs.last_hidden_state)\n```", "```py\n<tf.Tensor: shape=(1, 10, 768), dtype=float32, numpy=\narray([[[-0.30760652,  0.19552925,  0.1440584 , ...,  0.08283961,\n          0.16151786,  0.23049755],…\n```", "```py\nimport requests\nregion='westeurope'\nlanguage='en'\nversion='v3.1'\nkey = '<insert access key>'\nurl = \"https://{region}.api.cognitive.microsoft.com\" + \\\n    + \"/text/analytics/{version}/sentiment\".format(\n           region=region, version=version)\n```", "```py\nparams = {\n    'showStats': False\n}\nheaders = {\n    'Content-Type': 'application/json',\n    'Ocp-Apim-Subscription-Key': key\n}\npayload = {\n    'documents': [{\n        'id': '1',\n        'text': 'This is some input text that I love.',\n        'language': language   \n    }]\n}\n```", "```py\nresponse = requests.post(url,\n                         json=payload,\n                         params=params,\n                         headers=headers)\nresult = response.json()\nprint(result)\n```", "```py\n{\n  'documents': [{\n    'id': '1',\n    'sentiment': 'positive',\n    'confidenceScores': {\n      'positive': 1.0,\n      'neutral': 0.0,\n      'negative': 0.0},\n  ...}],\n  ...\n}\n```"]