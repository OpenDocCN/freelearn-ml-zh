- en: Chapter 4. Evaluating the Recommender Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapter showed you how to build recommender systems. There are
    a few options, and some of them can be developed using the `recommenderlab` package.
    In addition, each technique has some parameters. After we build the models, how
    can we decide which one to use? How can we determine its parameters? We can first
    test the performance of some models and/or parameter configurations and then choose
    the one that performs best.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will show you how to evaluate recommender models, compare their
    performances, and choose the most appropriate model. In this chapter, we will
    cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data to evaluate performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the performance of some models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the best performing models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data to evaluate the models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To evaluate models, you need to build them with some data and test them on some
    other data. This chapter will show you how to prepare the two sets of data. The
    `recommenderlab` package contains prebuilt tools that help in this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The target is to define two datasets, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set**: These are the models from which users learn'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing set**: These are the models that users apply and test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to evaluate the models, we need to compare the recommendations with
    the user preferences. In order to do so, we need to forget about some user preferences
    in the test set and see whether the techniques are able to identify them. For
    each user in the test set, we ignore some purchases and build the recommendations
    based on the others. Let''s load the packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The data-set that we will use is called `MovieLense`. Let''s define `ratings_movies`
    containing only the most relevant users and movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to prepare the data.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The easiest way to build a training and test set is to split the data in two
    parts. First, we need to decide how many users to put into each part. For instance,
    we can put 80 percent of the users into the training set. We can define `percentage_training`
    by specifying the percentage of the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For each user in the test set, we need to define how many items to use to generate
    recommendations. The remaining items will be used to test the model accuracy.
    It''s better that this parameter is lower than the minimum number of items purchased
    by any user so that we don''t have users without items to test the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, we can keep `15` items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluating a model consists of comparing the recommendations with the unknown
    purchases. The ratings are between 1 and 5, and we need to define what constitutes
    good and bad items. For this purpose, we will define a threshold with the minimum
    rating that is considered good:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'There is an additional parameter defining how many times we want to run the
    evaluation. For the moment, let''s set it to `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We are ready to split the data. The `recommenderlab` function is `evaluationScheme`
    and its parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data`: This is the initial dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`method`: This is the way to split the data. In this case, it''s `split`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train`: This is the percentage of data in the training set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`given`: This is the number of items to keep'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`goodRating`: This is the rating threshold'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k`: This is the number of times to run the evaluation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s build `eval_sets` containing the sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to extract the sets, we need to use `getData`. There are three sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train`: This is the training set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`known`: This is the test set, with the item used to build the recommendations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unknown`: This is the test set, with the item used to test the recommendations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a look at the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s a `realRatingMatrix` object, so we can apply methods such as `nrow` and
    `rowCounts` to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, about 80 percent of the users are in the training set. Let''s
    take a look at the two test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'They both have the same number of users. There should be about 20 percent of
    data in the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything is as expected. Let''s see how many items we have for each user
    in the `known` set. It should be equal to `items_to_keep`, that is, `15`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The same is not true for the users in the test set, since the number of remaining
    items depends on the initial number of purchases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image displays the unknown items by the users:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the data](img/B03888_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the number of items by users varies a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous subsection, we split the data into two parts, and the training
    set contained 80 percent of the rows. What if, instead, we sample the rows with
    replacement? The same user can be sampled more than once and, if the training
    set has the same size as it did earlier, there will be more users in the test
    set. This approach is called bootstrapping, and it''s supported by `recommenderlab`.
    The parameters are the same as the previous approach. The only difference is that
    we specify `method = "bootstrap"` instead of `method = "split"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of users in the training set is still equal to 80 percent of the
    total:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the same is not true for the items in the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The test set is more than twice as big as the previous set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extract the unique users in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The percentage of unique users in the training set should be complementary
    to the percentage of users in the test set, which is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can count how many times each user is repeated in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image displays the number of repetitions in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bootstrapping data](img/B03888_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Most of the users have been sampled fewer than four times.
  prefs: []
  type: TYPE_NORMAL
- en: Using k-fold to validate models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two previous approaches tested the recommender on part of the users. If,
    instead, we test the recommendation on each user, we could measure the performances
    much more accurately. We can split the data into some chunks, take a chunk out
    as the test set, and evaluate the accuracy. Then, we can do the same with each
    other chunk and compute the average accuracy. This approach is called k-fold and
    it's supported by `recommenderlab`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `evaluationScheme` and the difference is that, instead of specifying
    the percentage of data to put in the training set, we will define how many chunks
    we want. The argument is *k*, like the number of repetitions in the previous examples.
    Clearly, we don''t need to specify `train` anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can count how many items we have in each set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As expected, all the sets have the same size.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is the most accurate one, although it's computationally heavier.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we've seen different approaches to prepare the training and
    the test set. In the next chapter, we will start with the evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating recommender techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will show you two popular approaches to evaluate recommendations.
    They are both based on the cross-validation framework described in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: The first approach is to evaluate the ratings estimated by the algorithm. The
    other approach is to evaluate the recommendations directly. There is a subsection
    for each approach.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the ratings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to recommend items to new users, collaborative filtering estimates
    the ratings of items that are not yet purchased. Then, it recommends the top-rated
    items. At the moment, let's forget about the last step. We can evaluate the model
    by comparing the estimated ratings with the real ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s prepare the data for validation, as shown in the previous section.
    Since the *k*-fold is the most accurate approach, we will use it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to define the model to evaluate. For instance, we can evaluate an item-based
    collaborative filtering recommender. Let''s build it using the Recommender function.
    We need to specify the name of the model and the list of its parameters. If we
    use their defaults, then it''s NULL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to build the model, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The IBCF can recommend new items and predict their ratings. In order to build
    the model, we need to specify how many items we want to recommend, for example,
    `10`, even if we don''t need to use this parameter in the evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can build the matrix with the predicted ratings using the `predict` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `eval_prediction` object is a rating matrix. Let''s see how many movies
    we are recommending to each user. For this purpose, we can visualize the distribution
    of the number of movies per user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image displays the distribution of movies per user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the ratings](img/B03888_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The number of movies per user is roughly between 150 and 300.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function to measure the accuracy is `calcPredictionAccuracy` and it computes
    the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Root mean square error (RMSE)**: This is the standard deviation of the difference
    between the real and predicted ratings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean squared error (MSE)**: This is the mean of the squared difference between
    the real and predicted ratings. It''s the square of RMSE, so it contains the same
    information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean absolute error (MAE)**: This is the mean of the absolute difference
    between the real and predicted ratings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can compute these measures about each user by specifying `byUser = TRUE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|   | RMSE | MSE | MAE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | `1.217` | `1.481` | `0.8205` |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | `0.908` | `0.8244` | `0.727` |'
  prefs: []
  type: TYPE_TB
- en: '| **6** | `1.172` | `1.374` | `0.903` |'
  prefs: []
  type: TYPE_TB
- en: '| **14** | `1.405` | `1.973` | `1.027` |'
  prefs: []
  type: TYPE_TB
- en: '| **15** | `1.601` | `2.562` | `1.243` |'
  prefs: []
  type: TYPE_TB
- en: '| **18** | `0.8787` | `0.7721` | `0.633` |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s take a look at the RMSE by a user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image displays the distribution of the RSME by user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the ratings](img/B03888_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Most of the RMSEs are in the range of 0.8 to 1.4\. We evaluated the model for
    each user. In order to have a performance index of the whole model, we need to
    compute the average indices, specifying `byUser = FALSE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: These measures are useful to compare the performance of different models on
    the same data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another way to measure accuracies is by comparing the recommendations with
    the purchases having a positive rating. For this purpose, we can use the prebuilt
    `evaluate` function. Its inputs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: This is the object containing the evaluation scheme.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`method`: This is the recommendation technique.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n`: This is the number of items to recommend to each user. If we can specify
    a vector of `n`, the function will evaluate the recommender performance depending
    on `n`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have already defined a threshold, `rating_threshold <- 3`, for positive
    ratings, and this parameter is already stored inside `eval_sets`. The `progress
    = FALSE` argument suppresses a progress report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `results` object is an `evaluationResults` object containing the results
    of the evaluation. Using `getConfusionMatrix`, we can extract a list of confusion
    matrices. Each element of the list corresponds to a different split of the *k*-fold.
    Let''s take a look at the first element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '|   | TP | FP | FN | TN | precision | recall | TPR | FPR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **10** | `3.443` | `6.557` | `70.61` | `236.4` | `0.3443` | `0.04642` | `0.04642`
    | `0.02625` |'
  prefs: []
  type: TYPE_TB
- en: '| **20** | `6.686` | `13.31` | `67.36` | `229.6` | `0.3343` | `0.09175` | `0.09175`
    | `0.05363` |'
  prefs: []
  type: TYPE_TB
- en: '| **30** | `10.02` | `19.98` | `64.03` | `223` | `0.334` | `0.1393` | `0.1393`
    | `0.08075` |'
  prefs: []
  type: TYPE_TB
- en: '| **40** | `13.29` | `26.71` | `60.76` | `216.2` | `0.3323` | `0.1849` | `0.1849`
    | `0.1081` |'
  prefs: []
  type: TYPE_TB
- en: '| **50** | `16.43` | `33.57` | `57.62` | `209.4` | `0.3286` | `0.2308` | `0.2308`
    | `0.1362` |'
  prefs: []
  type: TYPE_TB
- en: '| **60** | `19.61` | `40.39` | `54.44` | `202.6` | `0.3268` | `0.2759` | `0.2759`
    | `0.164` |'
  prefs: []
  type: TYPE_TB
- en: 'The first four columns contain the true-false positives/negatives, and they
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positives (TP)**: These are recommended items that have been purchased'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives (FP)**: These are recommended items that haven''t been purchased'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives(FN)**: These are not recommended items that have been purchased'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negatives (TN)**: These are not recommended items that haven''t been
    purchased'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A perfect (or overfitted) model would have only TP and TN.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to take account of all the splits at the same time, we can just
    sum up the indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '|   | TP | FP | FN | TN |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **10** | `13.05` | `26.95` | `279.3` | `948.7` |'
  prefs: []
  type: TYPE_TB
- en: '| **20** | `25.4` | `54.6` | `267` | `921` |'
  prefs: []
  type: TYPE_TB
- en: '| **30** | `37.74` | `82.26` | `254.7` | `893.4` |'
  prefs: []
  type: TYPE_TB
- en: '| **40** | `50.58` | `109.4` | `241.8` | `866.2` |'
  prefs: []
  type: TYPE_TB
- en: '| **50** | `62.35` | `137.7` | `230` | `838` |'
  prefs: []
  type: TYPE_TB
- en: '| **60** | `74.88` | `165.1` | `217.5` | `810.5` |'
  prefs: []
  type: TYPE_TB
- en: Note that we could have used `avg(results)` instead.
  prefs: []
  type: TYPE_NORMAL
- en: The other four columns contain performance indices, and it's harder to summarize
    them across all the folds. However, we can visualize them by building some charts.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s build the ROC curve. It displays these factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive Rate (TPR)**: This is the percentage of purchased items that
    have been recommended. It''s the number of TP divided by the number of purchased
    items (TP + FN).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive Rate (FPR)**: This is the percentage of not purchased items
    that have been recommended. It''s the number of FP divided by the number of not
    purchased items (FP + TN).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `plot` method will build a chart with the `ROC curve`. In order to visualize
    the labels, we add the `annotate = TRUE` input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image displays the ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the recommendations](img/B03888_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Two accuracy metrics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: This is the percentage of recommended items that have been purchased.
    It''s the number of FP divided by the total number of positives (TP + FP).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: This is the percentage of purchased items that have been recommended.
    It''s the number of TP divided by the total number of purchases (TP + FN). It''s
    also equal to the True Positive Rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If a small percentage of purchased items are recommended, the precision usually
    decreases. On the other hand, a higher percentage of purchased items will be recommended
    so that the recall increases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image displays the precision-recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the recommendations](img/B03888_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This chart reflects the tradeoff between precision and recall. Even if the curve
    is not perfectly monotonic, the trends are as expected.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we've seen how to evaluate a model. In the next section, we
    will see how to compare two or more models.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the most suitable model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapter showed you how to evaluate a model. The performance indices
    are useful to compare different models and/or parameters. Applying different techniques
    on the same data, we can compare a performance index to pick the most appropriate
    recommender. Since there are different evaluation metrics, there is no objective
    way to do it.
  prefs: []
  type: TYPE_NORMAL
- en: The starting point is the k-fold evaluation framework that we defined in the
    previous section. It is stored inside `eval_sets`.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to compare different models, we first need to define them. Each model
    is stored in a list with its name and parameters. The components of the list are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: This is the model name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`param`: This is a list with its parameters. It can be NULL, if all the parameters
    are left at their defaults.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, that''s how we can define an item-based collaborative filtering
    by setting the `k` parameter to `20`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to evaluate different models, we can define a list with them. We can
    build the following filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: Item-based collaborative filtering, using the Cosine as the distance function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Item-based collaborative filtering, using the Pearson correlation as the distance
    function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User-based collaborative filtering, using the Cosine as the distance function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User-based collaborative filtering, using the Pearson correlation as the distance
    function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random recommendations to have a base line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding points are defined in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to evaluate the models properly, we need to test them, varying the
    number of items. For instance, we might want to recommend up to 100 movies to
    each user. Since 100 is already a big number of recommendations, we don''t need
    to include higher values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We are ready to run and evaluate the models. Like in the previous chapter,
    the function is `evaluate`. The only difference is that now the input method is
    a list of models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The `list_results` object is an `evaluationResultList` object and it can be
    treated as a list. Let''s take a look at its first element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The first element of `list_results` is an `evaluationResults` object, and this
    object is the same as the output of evaluate with a single model. We can check
    whether the same is true for all its elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Each element of `list_results` is an `evaluationResults` object. We can extract
    the related average confusion matrices using `avg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `avg_matrices` to explore the performance evaluation. For instance,
    let''s take a look at the IBCF with Cosine distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '|   | precision | recall | TPR | FPR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | `0.3589` | `0.004883` | `0.004883` | `0.002546` |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | `0.3371` | `0.02211` | `0.02211` | `0.01318` |'
  prefs: []
  type: TYPE_TB
- en: '| **10** | `0.3262` | `0.0436` | `0.0436` | `0.02692` |'
  prefs: []
  type: TYPE_TB
- en: '| **20** | `0.3175` | `0.08552` | `0.08552` | `0.0548` |'
  prefs: []
  type: TYPE_TB
- en: '| **30** | `0.3145` | `0.1296` | `0.1296` | `0.08277` |'
  prefs: []
  type: TYPE_TB
- en: '| **40** | `0.3161` | `0.1773` | `0.1773` | `0.1103` |'
  prefs: []
  type: TYPE_TB
- en: We have all the metrics of the previous chapter. In the next section, we will
    explore these metrics to identify the best performing model.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the most suitable model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can compare the models by building a chart displaying their ROC curves.
    Like the previous section, we can use `plot`. The annotate argument specifies
    which curves will contain the labels. For instance, the first and second curves
    are labeled by defining `annotate = c(1, 2)`. In our case, we will label only
    the first curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![Identifying the most suitable model](img/B03888_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A good performance index is the **area under the curve** (**AUC**), that is,
    the area under the ROC curve. Even without computing it, we can notice that the
    highest is UBCF with cosine distance, so it's the best-performing technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like we did in the previous section, we can build the precision-recall chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows the precision-recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Identifying the most suitable model](img/B03888_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The UBCF with cosine distance is still the top model. Depending on what we want
    to achieve, we can set an appropriate number of items to recommend.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing a numeric parameter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recommendation models often contain some numeric parameters. For instance, IBCF
    takes account of the *k*-closest items. How can we optimize *k*?
  prefs: []
  type: TYPE_NORMAL
- en: In a similar way to categoric parameters, we can test different values of a
    numeric parameter. In this case, we also need to define which values we want to
    test.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we left *k* to its default value: `30`. Now, we can explore more values,
    ranging between `5` and `40`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `lapply`, we can define a list of models to evaluate. The distance metric
    is the cosine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Building a chart with the ROC curve, we should be able to identify the best-performing
    *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![Optimizing a numeric parameter](img/B03888_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *k* having the biggest AUC is 10\. Another good candidate is 5, but it can
    never have a high TPR. This means that, even if we set a very high n value, the
    algorithm won't be able to recommend a big percentage of items that the user liked.
    The IBCF with `k = 5` recommends only a few items similar to the purchases. Therefore,
    it can't be used to recommend many items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the precision-recall chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image displays the precision-recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optimizing a numeric parameter](img/B03888_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To achieve the highest recall, we need to set `k = 10`. If we are more interested
    in the precision, we set `k = 5`.
  prefs: []
  type: TYPE_NORMAL
- en: This section evaluated four techniques using different methods. Then, it optimized
    a numeric parameter of one of them. Depending on what we want to achieve, the
    choice of parameters might be slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter showed you how to evaluate the performance of different models
    in order to choose the most accurate one. There are different ways to evaluate
    performances that might potentially lead to different choices. Depending on the
    business target, the evaluation metric is different. This is an example of how
    business and data should be combined to achieve the final result.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will explain a complete use case in which we will prepare the
    data, build different models, and test them.
  prefs: []
  type: TYPE_NORMAL
