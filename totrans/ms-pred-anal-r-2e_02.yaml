- en: Chapter 2. Tidying Data and Measuring Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover the topics of tidying your data in preparation
    for predictive modeling, performance metrics, cross-validation, and learning curves.
  prefs: []
  type: TYPE_NORMAL
- en: 'In statistics, it is an accepted concept that there are two types of data,
    which are:'
  prefs: []
  type: TYPE_NORMAL
- en: Untidy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tidy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Untidy data is considered to be raw or messy; tidy data is data that has gone
    through a quality assurance process and is ready to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get started with discussing the process of tidying data, it would
    be very prudent to point out that whatever you do to tidy your data, you should
    be sure to:'
  prefs: []
  type: TYPE_NORMAL
- en: Create and save your scripts so that you can use them again for new or similar
    data sources. This is referred to as **reusability**. Why spend time recreating
    the same code, rules, or logic if you don't have to? This applies to *new data*
    within the *same project* (that the scripts were developed for) or new projects
    you may be involved with in the future.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tidy your data as "far upstream" as possible, perhaps even at the original source.
    In other words, save and maintain the original data, but use programmatic scripts
    to clean it, fix mistakes, and save that *cleaned* dataset for further analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tidying data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is worth clarifying what the idea of *tidying data* means. Tidying data is
    the process of *reorganizing* (or perhaps just *organizing*) data, as well as
    addressing perceived issues or concerns someone has identified within your data.
    Issues affect the quality of data. Data quality, of course, is relative to the
    proposed purpose of use (of the data).
  prefs: []
  type: TYPE_NORMAL
- en: Categorizing data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is perhaps an accepted notion that issues with data quality may be categorized
    into one of the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completeness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistency (across sources)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appropriateness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quality or level of quality of your data can be affected by the way it is
    entered, stored, and managed. The process of addressing data quality (referred
    to most often as **data quality assurance** (**DQA**)) requires a routine and
    regular review and evaluation of the data and performing ongoing processes termed
    *profiling* and *scrubbing* (this is vital even if the data is stored in multiple
    disparate systems, making these processes difficult).
  prefs: []
  type: TYPE_NORMAL
- en: Here, tidying the data will be much more project centric in that we're probably
    not concerned with creating a formal DQA process, but are only concerned with
    making certain that the data is correct for your particular predictive project.
  prefs: []
  type: TYPE_NORMAL
- en: In statistics, data unobserved or not yet reviewed by the data scientist is
    considered *raw* and cannot be reliably used in predictive projects. The process
    of tidying the data will usually involve several steps. Taking the extra time
    to break out the work is strongly recommended (rather than haphazardly addressing
    multiple data issues together).
  prefs: []
  type: TYPE_NORMAL
- en: The first step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step requires bringing the data to what may be called *mechanical*
    correctness. In this first step, you focus on things such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**File format and organization**: Field order, column headers, number of records,
    and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Record data typing** (such as numeric values stored as strings)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Date and time processing** (typically reformatting values into standard formats
    or consistent formats)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Miss-content**: Wrong category labels, unknown or unexpected character encoding,
    and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second step is to address the *statistical soundness* of the data. Here
    we correct issues that may be *mechanically correct* but will most likely (depending
    upon the subject matter) impact a statistical outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'These issues may include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Positive/negative mismatch**: Age variables may be reported as negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invalid (based on accepted logic) data**: An under-aged person may be registered
    to possess a driver''s license'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing data**: Key data values may just be missing from the data source'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, the last step (before actually attempting to use the data) may be the
    *re-formatting* step. In this step, the data scientist will determine the form
    that the data must be in in order to most efficiently process it, based upon the
    intended use or objective.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, one might decide to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reorder or repeat** columns; that is to say, some final processing may require
    redundant or repeated data be generated within a file source to be correctly or
    more easily processed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drop** columns and/or records (based upon specific criteria)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Set decimal places**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pivot** data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Truncate or rename** values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a variety of somewhat routine methods for using R to resolve the aforementioned
    data errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Changing a data type**: Also referred to as "data type conversion," one can
    utilize the R `is` functions to test for an object''s data type and the `as` functions
    for an explicit conversion. A simplest example is shown here:![The final step](img/00018.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Date and time**: There are multiple ways to manage date information with
    R. In fact, we can extend the preceding example and mention the `as.Date` function.
    Typically, date values are important to a statistical model and therefore it is
    important to take the time to understand the format of a model''s date fields
    and ensure that they are properly dealt with. Mostly, dates and times will appear
    in raw data format as strings, which can be converted and formatted as required.
    In the following code, the string fields containing a `saledate` and a `returndate`
    are converted to date type values and used with a common time function, `difftime`:![The
    final step](img/00019.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Category labels are critical to statistical modeling as well as data visualization.
    An example of using labels with a sample of categorized data might be assigning
    a label to a participant in a study, perhaps by *level of education*: 1 = Doctoral,
    2 = Masters, 3 = Bachelors, 4 = Associates, 5 = Nondegree, 6 = Some College, 7
    = High School, or 8 = None:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Assigning labels to data not only helps with readability, but allows a machine
    learning algorithm to learn from the sample, and apply the same labels to other,
    unlabeled data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing data parameters**: many times missing data can be excluded from a
    calculation simply by setting an appropriate parameter value. For example, the
    R functions `var`, `cov`, and `cor` compute variance, covariance or correlation
    of variables. These functions have the option to set `na.rm` to TRUE. Doing this
    tells R to exclude any and all records or cases with missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various other *data tidying* nuisances can exist within your data, such as incorrectly
    signed numeric data (that is, a negative value for data such as a participant's
    age), invalid data values based upon accepted scenario logic (for example, participant's
    age versus level of education, in that it isn't feasible that a 10-year-old would
    have earned a Master's degree), data values simply missing (is a participant's
    lack of response an indication of a not applicable question or an error?), and
    more. Thankfully, there are at least several approaches to these data scenarios
    with R.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, where we talked about the predictive modeling process,
    we delved into the importance of assessing a trained model's performance using
    training and test datasets. In this section, we will look at specific measures
    of performance that we will frequently encounter when describing the predictive
    accuracy of different models. It turns out that depending on the class of the
    problem, we will need to use slightly different ways of assessing (the model's)
    performance. As we focus on supervised models in this book, we will look at how
    to assess regression models and classification models. For classification models,
    we will also discuss some additional metrics used for the binary classification
    task, which is a very important and frequently encountered type of problem.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note: In statistics, the term performance is usually interchangeable with accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: Assessing regression models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a regression scenario, let's recall that through our model we are building
    a function that is an estimate of a theoretical underlying target function *f*.
    The model's inputs are the values of our chosen input features. If we apply this
    function to every observation, *x[i]*, in our training data, which is labeled
    with the true value of the function, *y[i]*, we will obtain a set of pairs. To
    make sure we are clear on this last point, the first entry is the actual value
    of the output variable in our training data for the *i^(th)* observation, and
    the second entry is the predicted value for this particular observation produced
    by using our model on the feature values for this observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our model has fit the data well, both values will be very close to each
    other in the training set. If this is also true for our test set, then we consider
    that our model is likely to perform well for future unseen observations. To quantify
    the notion that the predicted and correct values are close to each other for all
    the observations in a dataset, we define a measure known as the **Mean Square
    Error** (**MSE**), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assessing regression models](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n* is the total number of observations in the dataset. Consequently,
    this equation tells us to first compute the squared difference between an output
    value and its predicted value for every observation, *i*, in the test set, and
    then take the average of all these values by summing them up and dividing by the
    number of observations. Thus, it should be clear why this measure is called the
    mean square error. The lower this number, the lower the average error between
    the actual value of the output variable in our observations and what we predict
    and, therefore, the more accurate our model. We sometimes make reference to the
    **Root Mean Square Error** (**RMSE**), which is just the square root of the MSE
    and the **Sum of Squared Error** (**SSE**), which is similar to the MSE but without
    the normalization which results from dividing by the number of training examples,
    *n*. These quantities, when computed on the training dataset, are valuable in
    the sense that a low number will indicate that we have trained a model sufficiently
    well. We know that we aren't expecting this to be zero in general, and we also
    cannot decide between models on the basis of these quantities because of the problem
    of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The key place to compute these measures is on the test data. In a majority of
    cases, a model's training data MSE (or equally, RMSE or SSE) will be lower than
    the corresponding measure computed on the test data. A model *m[1]* that overfits
    the data compared to another model *m[2]* can often be identified as such when
    the *m[1]* model produces a lower training MSE but higher test MSE than model
    *m[2]*.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing classification models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In regression models, the degree to which our predicted function incorrectly
    approximates an output, *y[i]*, for a particular observation, *x[i]*, is taken
    into account by the MSE. Specifically, large errors are squared, so a very large
    deviation on one data point can have a more significant impact than a few small
    deviations across more than one data point. It is precisely because we are dealing
    with a numerical output in regression that we can measure not only for which observations
    we aren't doing a good job at predicting, but also how far off we are.
  prefs: []
  type: TYPE_NORMAL
- en: 'For models that perform classification, we can again define an error rate,
    but here we can only talk about the number of misclassifications that were made
    by our model. Specifically, we have an error rate given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assessing classification models](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This measure uses the `indicator` function to return the value of 1 when the
    predicted class is not the same as the labeled class. Thus, the error rate is
    computed by counting the number of times the class of the output variable is incorrectly
    predicted, and dividing this count by the number of observations in the dataset.
    In this way, we can see that the error rate is actually the percentage of misclassified
    observations made by our model. It should be noted that this measure treats all
    types of misclassifications as equal. If the cost of some misclassifications is
    higher than others, then this measure can be adjusted by adding in weights that
    multiply each misclassification by an amount proportional to its cost.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to diagnose the greatest source of error in a regression problem,
    we tend to look at the points for which we have the largest error between our
    predicted value and the actual value. When doing classifications, it is often
    very useful to compute what is known as the confusion matrix. This is a matrix
    that shows all pairwise misclassifications that were made on our data. We shall
    now return to our iris species classification problem. In a previous section,
    we trained three kNN models. We'll now see how we can assess their performance.
    Like many classification models, kNN can return predictions either as final class
    labels or via a set of scores pertaining to each possible output class. Sometimes,
    as is the case here, these scores are actually probabilities that the model has
    assigned to every possible output. Regardless of whether the scores are actual
    probabilities, we can decide on which output label to pick on the basis of these
    scores, typically by simply choosing the label with the highest score.
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, the most common function to make model predictions is the `predict()`
    function, which we will use with our kNN models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the kNN model, we can assign output scores as direct probabilities by computing
    the ratio of the nearest neighbors that belong to each output label. In the three
    test examples shown, the virginica species has unit probabilities in two of them,
    but only 60 percent probability for the remaining example. The other 40 percent
    belong to the versicolor species, so it seems that in the latter case, three out
    of five nearest neighbors were of the virginica species, whereas the other two
    were of the versicolor species. It is clear that we should be more confident about
    the two former classifications than the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll now compute class predictions for the three models on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `postResample()` function from the `caret` package to display
    test set accuracy metrics for our models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, accuracy is one minus the error rate and is thus the percentage of correctly
    classified observations. We can see that all the models perform very closely in
    terms of accuracy, with the model that uses a Z-score normalization prevailing.
    This difference is not significant given the small size of the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assessing classification models](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The Kappa statistic is designed to counterbalance the effect of random chance
    and takes values in the interval, [-1,1], where 1 indicates perfect accuracy,
    -1 indicates perfect inaccuracy, and 0 occurs when the accuracy is exactly what
    would be obtained by a random guesser. Note that a random guesser for a classification
    model guesses the most frequent class. In the case of our iris classification
    model, the three species are equally represented in the data, and so the expected
    accuracy is one-third. The reader is encouraged to check that by using this value
    for the expected accuracy; we can obtain the observed values of the Kappa statistic
    from the accuracy values.
  prefs: []
  type: TYPE_NORMAL
- en: We can also examine the specific misclassifications that our model makes, using
    a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be simply constructed by cross-tabulating the predictions with the
    correct output labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `caret` package also has the very useful `confusionMatrix()` function, which
    automatically computes this table as well as several other performance metrics,
    the explanation of which can be found at [http://topepo.github.io/caret/other.html](http://topepo.github.io/caret/other.html).
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding confusion matrix, we can see that the total number of correctly
    classified observations is 28, which is the sum of the numbers `10`, `9`, and
    `9` on the leading diagonal. The table in the output shows us that the setosa
    species seems to be easier to predict with our model, as it is never confused
    with other species. The `versicolor` and `virginica` species, however, can be
    confused with each other, and the model has misclassified one instance of each.
    We can therefore surmise that computing the confusion matrix serves as a useful
    exercise. Spotting class pairs that are frequently confused will guide us to improve
    our model, for example, by looking for features that might help distinguish these
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing binary classification models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A special case of classification known as a binary classification occurs when
    we have two classes. Here are some typical binary classification scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: We want to classify incoming emails as spam or not spam using the email's content
    and header
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to classify a patient as having a disease or not using their symptoms
    and medical history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to classify a document from a large database of documents as being relevant
    to a search query, based on the words in the query and the words in the document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to classify a product from an assembly line as faulty or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to predict whether a customer applying for credit at a bank will default
    on their payments, based on their credit score and financial situation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a binary classification task, we usually refer to our two classes as the
    positive class and the negative class. By convention, the positive class corresponds
    to a special case that our model is trying to predict, and is often rarer than
    the negative class. From the preceding examples, we would use the positive class
    label for our spam emails, faulty assembly line products, defaulting customers,
    and so on. Now consider an example in the medical diagnosis domain, where we are
    trying to train a model to diagnose a disease that we know is only present in
    1 in 10,000 of the population. We would assign the positive class to patients
    that have this disease. Notice that in such a scenario, the error rate alone is
    not an adequate measure of a model. For example, we can design the simplest of
    classifiers that will have an error rate of only 0.01 percent by predicting that
    every patient will be healthy, but such a classifier would be useless. We can
    come up with more useful metrics by examining the confusion matrix. Suppose that
    we had built a model to diagnose our rare disease and on a test sample of 100,000
    patients, we obtained the following confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The binary classification problem is so common that the cells of the binary
    confusion matrix have their own names. On the leading diagonal, which contains
    the correctly classified entries, we refer to the elements as the true negatives
    and true positives. In our case, we had 99900 true negatives and 13 true positives.
    When we misclassify an observation as belonging to the positive class when it
    actually belongs to the negative class, then we have a false positive, also known
    as a Type I error. A false negative or Type II error occurs when we misclassify
    a positive observation as belonging to the negative class. In our case, our model
    had 78 false positives and 9 false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll now introduce two very important measures in the context of binary classification,
    which are precision and recall. Precision is defined as the ratio of the number
    of correctly predicted instances of the positive class to the total number of
    predicted instances of the positive class. Using the labels from the preceding
    binary confusion matrix, precision is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assessing binary classification models](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Precision, thus, essentially measures how accurate we are in making predictions
    for the positive class. By definition, we can achieve 100 percent precision by
    never making any predictions for the positive class, as this way we are guaranteed
    to never make any mistakes. Recall, by contrast, is defined as the number of correct
    predictions for the positive class over all the members of the positive class
    in our dataset. Once again, using the labels from the binary confusion matrix,
    we can see the definition of recall as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assessing binary classification models](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall measures our ability to identify all the positive class members from
    our dataset. We can easily achieve maximum recall by always predicting the positive
    class for all our data points. We will make a lot of mistakes, but we will never
    have any false negatives. Notice that precision and recall form a tradeoff in
    our model''s performance. At one end, if we don''t predict the positive class
    for any of our data points, we will have zero recall but maximum precision. At
    the other end, if all our data points are predicted as belonging to the positive
    class (which, remember, is usually a rare class), we will have maximum recall
    but extremely low precision. Put differently, trying to reduce the Type I error
    leads to increasing the Type II error and vice versa. This inverse relationship
    is often plotted for a particular problem on a precision-recall curve. By using
    an appropriate threshold parameter, we can often tune the performance of our model
    in such a way that we achieve a specific point on this precision-recall curve
    that is appropriate for our circumstances. For example, in some problem domains,
    we tend to be biased toward having a higher recall than a higher precision, because
    of the high cost of misclassifying an observation from the positive class into
    the negative class. As we often want to describe the performance of a model using
    a single number, we define a measure known as the F1 score, which combines precision
    and recall. Specifically, the F1 score is defined as the harmonic mean between
    precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assessing binary classification models](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The reader should verify that in our example confusion matrix, precision is
    14.3 percent, recall is 59.1 percent, and the F1 score is 0.23.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation (which you may hear some data scientists refer to as *rotation
    estimation*, or simply a general technique for assessing models), is another method
    for assessing a model's performance (or its accuracy).
  prefs: []
  type: TYPE_NORMAL
- en: Mainly used with predictive modeling to estimate how accurately a model might
    perform in practice, one might see cross-validation used to check how a model
    will potentially generalize; in other words, how the model will apply what it
    infers from samples, to an entire population (or dataset).
  prefs: []
  type: TYPE_NORMAL
- en: With cross-validation, you identify a (known) dataset as your validation dataset
    on which training is run, along with a dataset of unknown data (or first seen
    data) against which the model will be tested (this is known as your testing dataset).
    The objective is to ensure that problems such as overfitting (allowing non-inclusive
    information to influence results) are controlled, as well as provide an insight
    on how the model will generalize a real problem or on a real data file.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process will consist of separating data into samples of similar subsets,
    performing the analysis on one subset (called the training set), and validating
    the analysis on the other subset (called the validation set or testing set):'
  prefs: []
  type: TYPE_NORMAL
- en: Separation → Analysis → Validation
  prefs: []
  type: TYPE_NORMAL
- en: To reduce variability, multiple iterations (also called folds or rounds) of
    cross-validation are performed using different partitions, and the validation
    results are averaged over the rounds. Typically, a data scientist will use a model's
    stability to determine the actual number of rounds of cross-validation that should
    be performed.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the cross-validation method can perhaps be better understood by thinking
    about selecting a subset of data and manually calculating the results. Once you
    know the correct results, they can be compared to the model-produced results (using
    a separate subset of data). This is one round. Multiple rounds would be performed
    and the compared results averaged and reviewed, eventually providing a fair estimate
    of a model's prediction performance.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a university provides data on its student body over time. The students
    are described as having various characteristics, such as having a High School
    GPA greater or less than 3.0, if they have a family member that graduated from
    the school, if the student was active in non-program activities, was a resident
    (lived on campus), was a student athlete, and so on. Our predictive model wants
    to predict what characteristics students who graduate early have.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table is a representation of the results of using a five-round
    cross-validation process to predict our model''s expected accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross-validation](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Given the preceding figures, I'd say our predictive model is expected to be
    very accurate!
  prefs: []
  type: TYPE_NORMAL
- en: In summary, cross-validation combines (averages) measures of fit (prediction
    error) to derive a more accurate estimate of model prediction performance. This
    method is typically used in cases where there is not enough data available to
    test without losing significant modeling or testing quality.
  prefs: []
  type: TYPE_NORMAL
- en: Learning curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another method of assessing a model's performance is by evaluating the model's
    growth of learning or the model's ability to improve learning (obtain a better
    score) with additional experience (for example, more rounds of cross-validation).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning is the act of acquiring new, or modifying and reinforcing existing,
    knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: The information indicating a model's result or score with a data file population
    can be combined with other scores to show a line or curve, which is known as a
    model's learning curve.
  prefs: []
  type: TYPE_NORMAL
- en: A learning curve is a graphical representation of the growth of learning (the
    scores shown in a vertical axis) with practice (the individual data files or rounds
    shown in the horizontal axis).
  prefs: []
  type: TYPE_NORMAL
- en: 'This can also be conceptualized as:'
  prefs: []
  type: TYPE_NORMAL
- en: The same task repeated in a series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A body of knowledge learned over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure illustrates a hypothetical learning curve, showing the
    improved learning of a predictive model using resultant scores by cross-validation
    round:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning curves](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source link: [https://en.wikipedia.org/wiki/File:Alanf777_Lcd_fig01.png](https://en.wikipedia.org/wiki/File:Alanf777_Lcd_fig01.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's funny; one might know that the familiar expression *it's a steep learning
    curve* is intended to describe an activity that is tough to learn, but in statistics,
    a learning curve with a steep start would actually represent a rapidly improving
    progress.
  prefs: []
  type: TYPE_NORMAL
- en: Learning curves relating model performance to experience are commonly found
    to be used when performing model assessments.
  prefs: []
  type: TYPE_NORMAL
- en: As we have mentioned earlier in this section, performance (or the scores) is
    meant to be the accuracy of a model while experience (or round) may be the number
    of training examples, datasets, or iterations used in optimizing the model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Plot and ping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using two generic R functions, we can demonstrate a simple learning curve visualization.
    Ping will open an image file which will hold our learning curve visualization
    so we can easily include it in a document later, and plot will draw our graphic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are our example R code statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding statements create the following graphic as a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Plot and ping](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the fundamental ideas surrounding issues and concerns
    with data quality and how to categorize quality issues by their type, as well
    as presented ideas for tidying up your data.
  prefs: []
  type: TYPE_NORMAL
- en: In order to compare the performance of the different models that one may create,
    we went on to establish some fundamental notions of model performance, such as
    the **mean squared error** (**MSE**) for regression and the classification error
    rate for classification.
  prefs: []
  type: TYPE_NORMAL
- en: We also introduced cross-validation as a generic assessment technique to be
    used in cases where there is a limited amount of data available.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, learning curves were discussed as a way to judge the ability of a model
    to improve its scores or ability to learn.
  prefs: []
  type: TYPE_NORMAL
- en: With a firm grounding in the basics of the predictive modeling process, we will
    look at linear regression in the next chapter.
  prefs: []
  type: TYPE_NORMAL
