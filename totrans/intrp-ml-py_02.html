<html><head></head><body>
  <div id="_idContainer031" class="Basic-Text-Frame">
    <h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-36" class="chapterTitle">Key Concepts of Interpretability</h1>
    <p class="normal">This book covers many model interpretation methods. Some produce metrics, others create visuals, and some do both; some depict models broadly and others granularly. In this chapter, we will learn about two methods, feature importance and decision regions, as well as the taxonomies used to describe these methods. We will also detail what elements hinder machine learning interpretability as a primer to what lies ahead.</p>
    <p class="normal">The following are the main topics we are going to cover in this chapter:</p>
    <ul>
      <li class="bulletList">Learning about interpretation method types and scopes</li>
      <li class="bulletList">Appreciating what hinders machine learning interpretability</li>
    </ul>
    <p class="normal">Let’s start with our technical requirements.</p>
    <h1 id="_idParaDest-37" class="heading-1">Technical requirements</h1>
    <p class="normal">Although we began the book with a “toy example,” we will be leveraging real datasets throughout this book to be used in specific interpretation use cases. These come from many different sources and are often used only once.</p>
    <p class="normal">To avoid that, readers spend a lot of time downloading, loading, and preparing datasets for single examples; there’s a library called <code class="inlineCode">mldatasets</code> that takes care of most of this. Instructions on how to install this library are located in the <em class="italic">Preface</em>. In addition to <code class="inlineCode">mldatasets</code>, this chapter’s examples also use the <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">statsmodel</code>, <code class="inlineCode">sklearn</code>, <code class="inlineCode">seaborn</code>, and <code class="inlineCode">matplotlib</code> libraries.</p>
    <div class="note">
      <p class="normal">The code for this chapter is located here: <a href="https://packt.link/DgnVj"><span class="url">https://packt.link/DgnVj</span></a>.</p>
    </div>
    <h1 id="_idParaDest-38" class="heading-1">The mission</h1>
    <p class="normal">Imagine you <a id="_idIndexMarker076"/>are an analyst <a id="_idIndexMarker077"/>for a national health ministry, and there’s a <strong class="keyWord">Cardiovascular Diseases</strong> (<strong class="keyWord">CVDs</strong>) epidemic. The minister has made it a priority to reverse the growth and reduce the caseload to a 20-year low. To this end, a task force has been created to find clues in the data to ascertain the following:</p>
    <ul>
      <li class="bulletList">What risk factors can be addressed.</li>
      <li class="bulletList">If future cases can be predicted, interpret predictions on a case-by-case basis.</li>
    </ul>
    <p class="normal">You are part of this task force!</p>
    <h2 id="_idParaDest-39" class="heading-2">Details about CVD</h2>
    <p class="normal">Before we dive into the data, we must gather some important details about CVD in order to do the following:</p>
    <ul>
      <li class="bulletList">Understand the problem’s context and relevance.</li>
      <li class="bulletList">Extract domain knowledge information that can inform our data analysis and model interpretation.</li>
      <li class="bulletList">Relate an expert-informed background to a dataset’s features.</li>
    </ul>
    <p class="normal">CVDs are a group of disorders, the most common of which is coronary heart disease (also known as <em class="italic">Ischaemic Heart Disease</em>). According to the World Health Organization, CVD is the leading cause of death globally, killing close to 18 million people annually. Coronary heart disease and strokes (which are, for the most part, a byproduct of CVD) are the most significant contributors to that. It is estimated that 80% of CVD is made up of modifiable risk factors. In other words, some of the preventable factors that cause CVD include the following:</p>
    <ul>
      <li class="bulletList">Poor diet</li>
      <li class="bulletList">Smoking and alcohol consumption habits</li>
      <li class="bulletList">Obesity</li>
      <li class="bulletList">Lack of physical activity</li>
      <li class="bulletList">Poor sleep</li>
    </ul>
    <p class="normal">Also, many of <a id="_idIndexMarker078"/>the risk factors are non-modifiable and, therefore, known to be unavoidable, including the following:</p>
    <ul>
      <li class="bulletList">Genetic predisposition</li>
      <li class="bulletList">Old age</li>
      <li class="bulletList">Male (varies with age)</li>
    </ul>
    <p class="normal">We won’t go into more domain-specific details about CVD because it is not required to make sense of the example. However, <em class="italic">it can’t be stressed enough how central domain knowledge is to model interpretation</em>. So, if this example was your job and many lives depended on your analysis, it would be advisable to read the latest scientific research on the subject and consult with domain experts to inform your interpretations.</p>
    <h1 id="_idParaDest-40" class="heading-1">The approach</h1>
    <p class="normal">Logistic regression<a id="_idIndexMarker079"/> is one common way to rank risk factors in medical <a id="_idIndexMarker080"/>use cases. Unlike linear regression, it doesn’t try to predict a continuous value for each of our observations, but it predicts a probability score that an observation belongs to a particular class. In this case, what we are trying to predict is, given <em class="italic">x</em> data for each patient, what is the <em class="italic">y</em> probability, from 0 to 1, that they have CVD?</p>
    <h1 id="_idParaDest-41" class="heading-1">Preparations</h1>
    <p class="normal">We will <a id="_idIndexMarker081"/>find the code for this example here: <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/02/CVD.ipynb"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/02/CVD.ipynb</span></a>.</p>
    <h2 id="_idParaDest-42" class="heading-2">Loading the libraries</h2>
    <p class="normal">To run this<a id="_idIndexMarker082"/> example, we need to install the following libraries:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">mldatasets</code> to load the dataset</li>
      <li class="bulletList"><code class="inlineCode">pandas</code> and <code class="inlineCode">numpy</code> to manipulate it</li>
      <li class="bulletList"><code class="inlineCode">statsmodels</code> to fit the logistic regression model</li>
      <li class="bulletList"><code class="inlineCode">sklearn</code> (scikit-learn) to split the data</li>
      <li class="bulletList"><code class="inlineCode">matplotlib</code> and <code class="inlineCode">seaborn</code> to visualize the interpretations</li>
    </ul>
    <p class="normal">We should load <a id="_idIndexMarker083"/>all of them first:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> mldatasets
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
</code></pre>
    <h2 id="_idParaDest-43" class="heading-2">Understanding and preparing the data</h2>
    <p class="normal">The data <a id="_idIndexMarker084"/>to be used in this example should then be loaded into a DataFrame we call <code class="inlineCode">cvd_df</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">cvd_df = mldatasets.load(<span class="hljs-string">"cardiovascular-disease"</span>)
</code></pre>
    <p class="normal">From this, we should get 70,000 records and 12 columns. We can take a peek at what was loaded with <code class="inlineCode">info()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">cvd_df.info()
</code></pre>
    <p class="normal">The preceding command will output the names of each column with its type and how many non-null records it contains:</p>
    <pre class="programlisting con"><code class="hljs-con">RangeIndex: 70000 entries, 0 to 69999
Data columns (total 12 columns):
age            70000 non-null int64
gender         70000 non-null int64
height         70000 non-null int64
weight         70000 non-null float64
ap_hi          70000 non-null int64
ap_lo          70000 non-null int64
cholesterol    70000 non-null int64
gluc           70000 non-null int64
smoke          70000 non-null int64
alco           70000 non-null int64
active         70000 non-null int64
cardio         70000 non-null int64
dtypes: float64(1), int64(11)
</code></pre>
    <h3 id="_idParaDest-44" class="heading-3">The data dictionary</h3>
    <p class="normal">To<a id="_idIndexMarker085"/> understand what was loaded, the following is the data dictionary, as described in the source:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">age</code>: Of the patient in days (objective feature)</li>
      <li class="bulletList"><code class="inlineCode">height</code>: In centimeters (objective feature)</li>
      <li class="bulletList"><code class="inlineCode">weight</code>: In kg (objective feature)</li>
      <li class="bulletList"><code class="inlineCode">gender</code>: A binary where 1: female, 2: male (objective feature)</li>
      <li class="bulletList"><code class="inlineCode">ap_hi</code>: Systolic blood pressure, which is the arterial pressure exerted when blood is ejected during ventricular contraction. Normal value: &lt; 120 mmHg (objective feature)</li>
      <li class="bulletList"><code class="inlineCode">ap_lo</code>: Diastolic blood pressure, which is the arterial pressure in between heartbeats. Normal value: &lt; 80 mmHg (objective feature)</li>
      <li class="bulletList"><code class="inlineCode">cholesterol</code>: An ordinal where 1: normal, 2: above normal, and 3: well above normal (objective feature)</li>
      <li class="bulletList"><code class="inlineCode">gluc</code>: An ordinal where 1: normal, 2: above normal, and 3: well above normal (objective feature)</li>
      <li class="bulletList"><code class="inlineCode">smoke</code>: A binary where 0: non-smoker and 1: smoker (subjective feature)</li>
      <li class="bulletList"><code class="inlineCode">alco</code>: A binary where 0: non-drinker and 1: drinker (subjective feature)</li>
      <li class="bulletList"><code class="inlineCode">active</code>: A binary where 0: non-active and 1: active (subjective feature)</li>
      <li class="bulletList"><code class="inlineCode">cardio</code>: A binary where 0: no CVD and 1: has CVD (objective and target feature)</li>
    </ul>
    <p class="normal">It’s essential to understand the data generation process of a dataset, which is why the features are split into two categories:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Objective</strong>: A feature <a id="_idIndexMarker086"/>that is a product of official documents or a clinical examination. It is expected to have a rather insignificant margin of error due to clerical or machine errors.</li>
      <li class="bulletList"><strong class="keyWord">Subjective</strong>: Reported<a id="_idIndexMarker087"/> by the patient and not verified (or unverifiable). In this case, due to lapses of memory, differences in understanding, or dishonesty, it is expected to be less reliable than objective features.</li>
    </ul>
    <p class="normal">At the end of the day, trusting the model is often about trusting the data used to train it, so how much patients lie about smoking can make a difference.</p>
    <h3 id="_idParaDest-45" class="heading-3">Data preparation</h3>
    <p class="normal">For the sake<a id="_idIndexMarker088"/> of interpretability and model performance, there are several data preparation tasks that we can perform, but the one that stands out right now is <code class="inlineCode">age</code>. Age is not something we usually measure in days. In fact, for health-related predictions like this one, we might even want to bucket them into <strong class="keyWord">age groups</strong> since health differences observed between individual year-of-birth cohorts aren’t as evident as those observed between generational cohorts, especially when cross tabulating with other features like lifestyle differences. For now, we will convert all ages into years:</p>
    <pre class="programlisting code"><code class="hljs-code">cvd_df[<span class="hljs-string">'age'</span>] = cvd_df[<span class="hljs-string">'age'</span>] / <span class="hljs-number">365.24</span>
</code></pre>
    <p class="normal">The result is a more understandable column because we expect age values to be between 0 and 120. We took existing data and transformed it. This is an example of <strong class="keyWord">feature engineering</strong>, which<a id="_idIndexMarker089"/> is when we use the domain knowledge of our data to create features that better represent our problem, thereby improving our models. We will discuss this further in <em class="chapterRef">Chapter 11</em>, <em class="italic">Bias Mitigation and Causal Inference Methods</em>. There’s value in performing feature engineering simply to make model outcomes more <em class="italic">interpretable</em> as long as this doesn’t significantly hurt model performance. In fact, it might improve predictive performance. Note that there was no loss in data in the feature engineering performed on the age column, as the decimal value for years is maintained.</p>
    <p class="normal">Now we are going to take a peek at what the summary statistics are for each one of our features using the <code class="inlineCode">describe()</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code">cvd_df.describe(percentiles=[<span class="hljs-number">.01</span>,<span class="hljs-number">.99</span>]).transpose()
</code></pre>
    <p class="normal"><em class="italic">Figure 2.1</em> shows the summary statistics outputted by the preceding code. It includes the 1% and 99% percentiles, which tell us what are among the highest and lowest values for each feature: </p>
    <figure class="mediaobject"><img src="../Images/B18406_02_01.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 2.1: Summary statistics for the dataset</p>
    <p class="normal">In <em class="italic">Figure 2.1</em>, <code class="inlineCode">age</code> appears<a id="_idIndexMarker090"/> valid because it ranges between 29 and 65 years, which is not out of the ordinary, but there are some anomalous outliers for <code class="inlineCode">ap_hi</code> and <code class="inlineCode">ap_lo</code>. Blood pressure can’t be negative, and the highest ever recorded was <code class="inlineCode">370</code>. Keeping these outliers in there can lead to poor model performance and interpretability. Given that the 1% and 99% percentiles still show values in normal ranges according to <em class="italic">Figure 2.1</em>, there’s close to 2% of records with invalid values. If you dig deeper, you’ll realize it’s closer to 1.8%.</p>
    <pre class="programlisting code"><code class="hljs-code">incorrect_l = cvd_df[
    (cvd_df[<span class="hljs-string">'ap_hi'</span>]&gt;<span class="hljs-number">370</span>)
    | (cvd_df[<span class="hljs-string">'ap_hi'</span>]&lt;=<span class="hljs-number">40</span>)
    | (cvd_df[<span class="hljs-string">'ap_lo'</span>] &gt; <span class="hljs-number">370</span>)
    | (cvd_df[<span class="hljs-string">'ap_lo'</span>] &lt;= <span class="hljs-number">40</span>)
].index
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(incorrect_l) / cvd_df.shape[<span class="hljs-number">0</span>])
</code></pre>
    <p class="normal">There are many ways we could handle these incorrect values, but because they are relatively few records and we lack the domain expertise to guess if they were mistyped (and correct them accordingly), we will delete them:</p>
    <pre class="programlisting code"><code class="hljs-code">cvd_df.drop(incorrect_l, inplace=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">For good measure, we ought to make sure that <code class="inlineCode">ap_hi</code> is always higher than <code class="inlineCode">ap_lo</code>, so any record with that discrepancy should also be dropped:</p>
    <pre class="programlisting code"><code class="hljs-code">cvd_df = cvd_df[cvd_df[<span class="hljs-string">'ap_hi'</span>] &gt;=\
                cvd_df[<span class="hljs-string">'ap_lo'</span>]].reset_index(drop=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Now, in <a id="_idIndexMarker091"/>order to fit a logistic regression model, we must put all objective, examination, and subjective features together as <em class="italic">X</em> and the target feature alone as <em class="italic">y</em>. After this, we split <em class="italic">X</em> and <em class="italic">y</em> into training and test datasets, but make sure to include <code class="inlineCode">random_state</code> for reproducibility:</p>
    <pre class="programlisting code"><code class="hljs-code">y = cvd_df[<span class="hljs-string">'cardio'</span>]
X = cvd_df.drop([<span class="hljs-string">'cardio'</span>], axis=<span class="hljs-number">1</span>).copy()
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=<span class="hljs-number">0.15</span>, random_state=<span class="hljs-number">9</span>
)
</code></pre>
    <p class="normal">The scikit-learn <code class="inlineCode">train_test_split</code> function puts 15% of the observations in the test dataset and the remainder in the train dataset, so you end up with <em class="italic">X</em> and <em class="italic">y</em> pairs for both.</p>
    <p class="normal">Now that we have our data ready for training, let’s train a model and interpret it.</p>
    <h1 id="_idParaDest-46" class="heading-1">Interpretation method types and scopes</h1>
    <p class="normal">Now <a id="_idIndexMarker092"/>that we have prepared our data and split it into training/test datasets, we can fit the model using the training data and print a summary of<a id="_idIndexMarker093"/> the results:</p>
    <pre class="programlisting code"><code class="hljs-code">log_model = sm.Logit(y_train, sm.add_constant(X_train))
log_result = log_model.fit()
<span class="hljs-built_in">print</span>(log_result.summary2())
</code></pre>
    <p class="normal">Printing <code class="inlineCode">summary2</code> on the fitted model produces the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">Optimization terminated successfully.
         Current function value: 0.561557
         Iterations 6
                         Results: Logit
=================================================================
Model:              Logit            Pseudo R-squared: 0.190     
Dependent Variable: cardio           AIC:              65618.3485
Date:               2020-06-10 09:10 BIC:              65726.0502
No. Observations:   58404            Log-Likelihood:   -32797.   
Df Model:           11               LL-Null:          -40481.   
Df Residuals:       58392            LLR p-value:      0.0000    
Converged:          1.0000           Scale:            1.0000    
No. Iterations:     6.0000                                       
-----------------------------------------------------------------
               Coef.   Std.Err.    z     P&gt;|z|   [0.025   0.975]
-----------------------------------------------------------------
const         -11.1730   0.2504 -44.6182 0.0000 -11.6638 -10.6822
age             0.0510   0.0015  34.7971 0.0000   0.0482   0.0539
gender         -0.0227   0.0238  -0.9568 0.3387  -0.0693   0.0238
height         -0.0036   0.0014  -2.6028 0.0092  -0.0063  -0.0009
weight          0.0111   0.0007  14.8567 0.0000   0.0096   0.0125
ap_hi           0.0561   0.0010  56.2824 0.0000   0.0541   0.0580
ap_lo           0.0105   0.0016   6.7670 0.0000   0.0075   0.0136
cholesterol     0.4931   0.0169  29.1612 0.0000   0.4600   0.5262
gluc           -0.1155   0.0192  -6.0138 0.0000  -0.1532  -0.0779
smoke          -0.1306   0.0376  -3.4717 0.0005  -0.2043  -0.0569
alco           -0.2050   0.0457  -4.4907 0.0000  -0.2945  -0.1155
active         -0.2151   0.0237  -9.0574 0.0000  -0.2616  -0.1685
=================================================================
</code></pre>
    <p class="normal">The <a id="_idIndexMarker094"/>preceding <a id="_idIndexMarker095"/>summary helps us to understand which <em class="italic">X</em> features contributed the most to the <em class="italic">y</em> CVD diagnosis using the model coefficients (labeled <code class="inlineCode">Coef.</code> in the table). Much like with linear regression, the coefficients are weights applied to the predictors. However, the linear combination exponent is<a id="_idIndexMarker096"/> a <strong class="keyWord">logistic function</strong>. This makes the interpretation more difficult. We explain this function further in <em class="chapterRef">Chapter 3</em>, <em class="italic">Interpretation Challenges</em>.</p>
    <p class="normal">We can tell by looking at it that the features with the absolute highest values are <code class="inlineCode">cholesterol</code> and <code class="inlineCode">active</code>, but it’s not very intuitive in terms of what this means. A more interpretable way of looking at these values is revealed once we calculate the exponential of these coefficients:</p>
    <pre class="programlisting code"><code class="hljs-code">np.exp(log_result.params).sort_values(ascending=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">The preceding code outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">cholesterol    1.637374
ap_hi          1.057676
age            1.052357
weight         1.011129
ap_lo          1.010573
height         0.996389
gender         0.977519
gluc           0.890913
smoke          0.877576
alco           0.814627
active         0.806471
const          0.000014
dtype: float64
</code></pre>
    <p class="normal">Why the exponential? The coefficients are<a id="_idIndexMarker097"/> the <strong class="keyWord">log odds</strong>, which are the logarithms of the <em class="italic">odds</em>. Also, <em class="italic">odds</em> are the probability of a positive case over the probability of a negative case, where the <strong class="keyWord">positive case</strong> is the label we are trying to predict. It doesn’t necessarily indicate what<a id="_idIndexMarker098"/> is favored by anyone. For instance, if we are trying to <a id="_idIndexMarker099"/>predict the odds of a rival team winning the championship today, the positive case would be that they own, regardless of whether we favor them or not. Odds are often expressed as a ratio. The news could say the probability of them winning today is 60% or say the odds are 3:2 or 3/2 = 1.5. In log odds form, this would be 0.176, which is the logarithm of 1.5. They are basically the same thing but expressed differently. An exponential function is the inverse of a logarithm, so it can take any <em class="italic">log odds</em> and return the <em class="italic">odds</em>, as we have done.</p>
    <p class="normal">Back to our CVD case. Now that we have the odds, we can interpret what it means. For example, what do the odds mean in the case of cholesterol? It means that the odds of CVD increase by a factor of 1.64 for each additional unit of cholesterol, provided every other feature stays unchanged. Being able to explain the impact of a feature on the model in such tangible terms is one of the advantages of an <em class="italic">intrinsically interpretable</em> model such as logistic regression.</p>
    <p class="normal">Although the <em class="italic">odds</em> provide us with useful information, they don’t tell us what matters the most and, therefore, by themselves, cannot be used to measure feature importance. But how could that be? If something has higher odds, then it must matter more, right? Well, for starters, they all have different scales, so that makes a huge difference. This is because if we measure the odds of how much something increases, we have to know by how much it typically increases because that provides context. For example, we could say that the odds of a specific species of butterfly living one day more are 0.66 after their first eggs hatch. This statement is meaningless unless we know the lifespan and reproductive cycle of this species.</p>
    <p class="normal">To provide context to our odds, we can easily calculate the standard deviation of our features using the <code class="inlineCode">np.std</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">np.std(X_train, <span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">The following <a id="_idIndexMarker100"/>series is what is outputted by the <code class="inlineCode">np.std</code> function:</p>
    <pre class="programlisting con"><code class="hljs-con">age             6.757537
gender          0.476697
height          8.186987
weight         14.335173
ap_hi          16.703572
ap_lo           9.547583
cholesterol     0.678878
gluc            0.571231
smoke           0.283629
alco            0.225483
active          0.397215
dtype: float64
</code></pre>
    <p class="normal">As we can tell <a id="_idIndexMarker101"/>by the output, binary and ordinal features only typically vary by one at most, but continuous features, such as <code class="inlineCode">weight</code> or <code class="inlineCode">ap_hi</code>, can vary 10–20 times more, as evidenced by the standard deviation of the features.</p>
    <p class="normal">Another reason why <em class="italic">odds</em> cannot be used to measure feature importance is that despite favorable odds, sometimes features are not statistically significant. They are entangled with other features in such a way they might appear to be significant, but we can prove that they aren’t. This can be seen in the summary table for the model, under the <code class="inlineCode">P&gt;|z|</code> column. This value is <a id="_idIndexMarker102"/>called the <strong class="keyWord">p-value</strong>, and when it’s less than 0.05, we reject the null hypothesis that states that the coefficient is equal to zero. In other words, the corresponding feature is statistically significant. However, when it’s above this number, especially by a large margin, there’s no statistical evidence that it affects the predicted score. Such is the case with <code class="inlineCode">gender</code>, at least in this dataset.</p>
    <p class="normal">If we are trying to obtain what features matter most, one way to approximate this is to multiply the coefficients by the standard deviations of the features. Incorporating the standard deviations accounts for differences in variances between features. Hence, it is better if we get <code class="inlineCode">gender</code> out of the way too while we are at it:</p>
    <pre class="programlisting code"><code class="hljs-code">coefs = log_result.params.drop(labels=[<span class="hljs-string">'const'</span>,<span class="hljs-string">'gender'</span>])
stdv = np.std(X_train, <span class="hljs-number">0</span>).drop(labels=<span class="hljs-string">'gender'</span>)
<span class="hljs-built_in">abs</span>(coefs * stdv).sort_values(ascending=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">The preceding code produced this output:</p>
    <pre class="programlisting con"><code class="hljs-con">ap_hi          0.936632
age            0.344855
cholesterol    0.334750
weight         0.158651
ap_lo          0.100419
active         0.085436
gluc           0.065982
alco           0.046230
smoke          0.037040
height         0.029620
</code></pre>
    <p class="normal">The<a id="_idIndexMarker103"/> preceding table can be interpreted as an <strong class="keyWord">approximation of risk factors</strong> from <a id="_idIndexMarker104"/>high to low according to the model. It is also <a id="_idIndexMarker105"/>a <strong class="keyWord">model-specific</strong> feature importance method, in other words, a <strong class="keyWord">global model</strong> (<strong class="keyWord">modular</strong>) <strong class="keyWord">interpretation method</strong>. There <a id="_idIndexMarker106"/>are a lot of new concepts to unpack here, so let’s break them down.</p>
    <h2 id="_idParaDest-47" class="heading-2">Model interpretability method types</h2>
    <p class="normal">There are<a id="_idIndexMarker107"/> two model interpretability method types:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Model-specific</strong>: When <a id="_idIndexMarker108"/>the method can only be used for a specific model class, then it’s model-specific. The method detailed in the previous example can only work with logistic regression because it uses its coefficients.</li>
      <li class="bulletList"><strong class="keyWord">Model-agnostic</strong>: These <a id="_idIndexMarker109"/>are methods that can work with any model class. We cover these in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>, and the next two chapters.</li>
    </ul>
    <h2 id="_idParaDest-48" class="heading-2">Model interpretability scopes</h2>
    <p class="normal">There are<a id="_idIndexMarker110"/> several model interpretability scopes:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Global holistic interpretation</strong>: We can explain how a model makes predictions <a id="_idIndexMarker111"/>simply because we can comprehend the entire model at once with a complete understanding of the data, and it’s a trained model. For instance, the simple linear regression example in <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em>, can be visualized in a two-dimensional graph. We can conceptualize this in memory, but this is only possible because the simplicity of the model allows us to do so, and it’s not very common nor expected.</li>
      <li class="bulletList"><strong class="keyWord">Global modular interpretation</strong>: In the same way that we can explain the role of <em class="italic">parts</em> of an <a id="_idIndexMarker112"/>internal combustion engine in the <em class="italic">whole</em> process of turning fuel into movement, we can also do so with a model. For instance, in the CVD risk factor example, our feature importance method tells us that <code class="inlineCode">ap_hi</code> (systolic blood pressure), <code class="inlineCode">age</code>, <code class="inlineCode">cholesterol</code>, and <code class="inlineCode">weight</code> are the <em class="italic">parts</em> that impact the <em class="italic">whole</em> the most. Feature importance is only one of many global modular interpretation methods but arguably the most important one. <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>, goes into more detail on feature importance.</li>
      <li class="bulletList"><strong class="keyWord">Local single-prediction interpretation</strong>: We can explain why a single prediction <a id="_idIndexMarker113"/>was made. The next example will illustrate this concept and <em class="chapterRef">Chapter 5</em>, <em class="italic">Local Model-Agnostic Interpretation Methods</em>, will go into more detail.</li>
      <li class="bulletList"><strong class="keyWord">Local group-prediction interpretation</strong>: The <a id="_idIndexMarker114"/>same as single-prediction, except that it applies to groups of predictions.</li>
    </ul>
    <p class="normal">Congratulations! You’ve already determined the risk factors with a <strong class="keyWord">global model interpretation method</strong>, but the health minister also wants to know whether the model can be used to interpret individual cases. So, let’s look into that.</p>
    <h2 id="_idParaDest-49" class="heading-2">Interpreting individual predictions with logistic regression</h2>
    <p class="normal">What if <a id="_idIndexMarker115"/>we used the model to predict <a id="_idIndexMarker116"/>CVD for the entire test dataset? We could do so like this:</p>
    <pre class="programlisting code"><code class="hljs-code">y_pred = log_result.predict(sm.add_constant(X_test)).to_numpy()
<span class="hljs-built_in">print</span>(y_pred)
</code></pre>
    <p class="normal">The<a id="_idIndexMarker117"/> resulting array is the probabilities that each test case is positive for CVD:</p>
    <pre class="programlisting con"><code class="hljs-con">[0.40629892 0.17003609 0.13405939 ... 0.95575283 0.94095239 0.91455717]
</code></pre>
    <p class="normal">Let’s take one of the positive cases; test case #2872:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(y_pred[<span class="hljs-number">2872</span>])
</code></pre>
    <p class="normal">We <a id="_idIndexMarker118"/>know that it predicted positive for CVD because the score exceeds 0.5.</p>
    <p class="normal">And these are the details for test case #2872:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(X_test.iloc[<span class="hljs-number">2872</span>])
</code></pre>
    <p class="normal">The following is the output:</p>
    <pre class="programlisting con"><code class="hljs-con">age             60.521849
gender           1.000000
height         158.000000
weight          62.000000
ap_hi          130.000000
ap_lo           80.000000
cholesterol      1.000000
gluc             1.000000
smoke            0.000000
alco             0.000000
active           1.000000
Name: 46965, dtype: float64
</code></pre>
    <p class="normal">So, by the looks of the preceding series, we know that the following applies to this individual:</p>
    <ul>
      <li class="bulletList">A borderline high <code class="inlineCode">ap_hi</code> (systolic blood pressure) since anything above or equal to 130 is considered<a id="_idIndexMarker119"/> high according to the <strong class="keyWord">American Heart Association</strong> (<strong class="keyWord">AHA</strong>).</li>
      <li class="bulletList">Normal <code class="inlineCode">ap_lo</code> (diastolic blood pressure) also according to AHA. Having high systolic blood pressure and normal diastolic blood pressure is what is known as <em class="italic">isolated systolic hypertension</em>. It could be causing a positive prediction, but <code class="inlineCode">ap_hi</code> is borderline; therefore, the condition of <em class="italic">isolated systolic hypertension</em> is borderline.</li>
      <li class="bulletList"><code class="inlineCode">age</code> is not too old, but among the oldest in the dataset.</li>
      <li class="bulletList"><code class="inlineCode">cholesterol</code> is normal.</li>
      <li class="bulletList"><code class="inlineCode">weight</code> also appears to be in the healthy range.</li>
    </ul>
    <p class="normal">There<a id="_idIndexMarker120"/> are also no other risk<a id="_idIndexMarker121"/> factors: glucose is normal, the individual does not smoke nor drink alcohol, and does not live a sedentary lifestyle, as the individual is active. It is not clear exactly why it’s positive. Are the age and borderline <em class="italic">isolated systolic hypertension</em> enough to tip the scales? It’s tough to understand the reasons for the prediction without putting all the predictions into context, so let’s try to do that!</p>
    <p class="normal">But how do we put everything in context at the same time? We can’t possibly visualize how one prediction compares with the other 10,000 for every single feature and their respective predicted CVD diagnosis. Unfortunately, humans can’t process that level of dimensionality, even if it were possible to visualize a ten-dimensional hyperplane!</p>
    <p class="normal">However, we can do it for two features at a time, resulting in a graph that conveys where the decision boundary for the model lies for those features. On top of that, we can overlay what the predictions were for the test dataset based on all the features. This is to visualize the discrepancy between the effect of two features and all eleven features.</p>
    <p class="normal">This graphical interpretation method is what is termed<a id="_idIndexMarker122"/> a <strong class="keyWord">decision boundary</strong>. It draws boundaries for the classes, leaving areas that belong to one class or another. Such areas are <a id="_idIndexMarker123"/>called <strong class="keyWord">decision regions</strong>. In this case, we have two classes, so we will see a graph with a single boundary between <code class="inlineCode">cardio=0</code> and <code class="inlineCode">cardio=1</code>, only concerning the two features we are comparing.</p>
    <p class="normal">We have managed to visualize the two decision-based features at a time, with one big assumption that if all the other features are held constant, we can observe only two in isolation. This is also known as <a id="_idIndexMarker124"/>the <strong class="keyWord">ceteris paribus</strong> assumption and is critical in a scientific inquiry, allowing us to <em class="italic">control</em> some variables in order to <em class="italic">observe</em> others. One way to do this is to fill them with a value that won’t affect the outcome. Using the table of odds we produced, we can tell whether a feature increases as it will increase the odds of CVD. So, in aggregates, a lower value is less risky for CVD.</p>
    <p class="normal">For instance, <code class="inlineCode">age=30</code> is the least risky value of those present in the dataset for <code class="inlineCode">age</code>. It can also go in the opposite direction, so <code class="inlineCode">active=1</code> is known to be less risky than <code class="inlineCode">active=0</code>. We can <a id="_idIndexMarker125"/>come up with optimal values for the remainder of the features:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">height=165</code>.</li>
      <li class="bulletList"><code class="inlineCode">weight=57</code> (optimal for that <code class="inlineCode">height</code>).</li>
      <li class="bulletList"><code class="inlineCode">ap_hi=110</code>.</li>
      <li class="bulletList"><code class="inlineCode">ap_lo=70</code>.</li>
      <li class="bulletList"><code class="inlineCode">smoke=0</code>.</li>
      <li class="bulletList"><code class="inlineCode">cholesterol=1</code> (this means normal).</li>
      <li class="bulletList"><code class="inlineCode">gender</code> can be coded for male or female, which doesn’t matter because the odds for gender (<code class="inlineCode">0.977519</code>) are so close to 1.</li>
    </ul>
    <p class="normal">The <a id="_idIndexMarker126"/>following <code class="inlineCode">filler_feature_values</code> dictionary exemplifies what should be done with the features matching their index to their least risky values:</p>
    <pre class="programlisting code"><code class="hljs-code">filler_feature_values = {
    <span class="hljs-string">"age"</span>: <span class="hljs-number">30</span>,
    <span class="hljs-string">"gender"</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">"height"</span>: <span class="hljs-number">165</span>,
    <span class="hljs-string">"weight"</span>: <span class="hljs-number">57</span>,
    <span class="hljs-string">"ap_hi"</span>: <span class="hljs-number">110</span>,
    <span class="hljs-string">"ap_lo"</span>: <span class="hljs-number">70</span>,
    <span class="hljs-string">"cholesterol"</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">"gluc"</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">"smoke"</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">"alco"</span>:<span class="hljs-number">0</span>,
    <span class="hljs-string">"active"</span>:<span class="hljs-number">1</span>
}
</code></pre>
    <p class="normal">The next thing to do is to create a <code class="inlineCode">(1,12)</code> shaped NumPy array with test case #2872 so that the plotting function can highlight it. To this end, we first convert it into NumPy and then prepend the <em class="italic">constant</em> of <code class="inlineCode">1</code>, which must be the first feature, and then reshape it so that it meets the <code class="inlineCode">(1,12)</code> dimensions. The reason for the constant is that in <code class="inlineCode">statsmodels</code>, we must explicitly <a id="_idIndexMarker127"/>define the <strong class="keyWord">intercept</strong>. For this reason, the logistic model has an additional <code class="inlineCode">0</code> feature, which always equals <code class="inlineCode">1</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">X_highlight = np.reshape(
    np.concatenate(([<span class="hljs-number">1</span>], X_test.iloc[<span class="hljs-number">2872</span>].to_numpy())), (<span class="hljs-number">1</span>, <span class="hljs-number">12</span>))
<span class="hljs-built_in">print</span>(X_highlight)
</code></pre>
    <p class="normal">The following is the output:</p>
    <pre class="programlisting con"><code class="hljs-con">[[  1.       60.52184865   1.       158.        62.       130.          
   80.        1.           1.         0.         0.         1.     ]]
</code></pre>
    <p class="normal">We are good to go now! Let’s visualize some decision region plots! We will compare the feature<a id="_idIndexMarker128"/> that is thought to be the highest <em class="italic">risk factor</em>, <code class="inlineCode">ap_hi</code>, with the following four most important risk factors: <code class="inlineCode">age</code>, <code class="inlineCode">cholesterol</code>, <code class="inlineCode">weight</code>, and <code class="inlineCode">ap_lo</code>.</p>
    <p class="normal">The <a id="_idIndexMarker129"/>following code will generate the plots in <em class="italic">Figure 2.2</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.rcParams.update({<span class="hljs-string">'font.size'</span>: <span class="hljs-number">14</span>})
fig, axarr = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>), sharex=<span class="hljs-literal">True</span>,
                          sharey=<span class="hljs-literal">False</span>)
mldatasets.create_decision_plot(
    X_test,
    y_test,
    log_result,
    [<span class="hljs-string">"ap_hi"</span>, <span class="hljs-string">"age"</span>],
    <span class="hljs-literal">None</span>,
    X_highlight,
    filler_feature_values,
    ax=axarr.flat[<span class="hljs-number">0</span>]
)
mldatasets.create_decision_plot(
    X_test,
    y_test,
    log_result,
    [<span class="hljs-string">"ap_hi"</span>, <span class="hljs-string">"cholesterol"</span>],
     <span class="hljs-literal">None</span>,
    X_highlight,
    filler_feature_values,
    ax=axarr.flat[<span class="hljs-number">1</span>]
)
mldatasets.create_decision_plot(
    X_test,
    y_test,
    log_result,
    [<span class="hljs-string">"ap_hi"</span>, <span class="hljs-string">"ap_lo"</span>],
    <span class="hljs-literal">None</span>,
    X_highlight,
    filler_feature_values,
    ax=axarr.flat[<span class="hljs-number">2</span>],
)
mldatasets.create_decision_plot(
    X_test,
    y_test,
    log_result,
    [<span class="hljs-string">"ap_hi"</span>, <span class="hljs-string">"weight"</span>],
    <span class="hljs-literal">None</span>,
    X_highlight,
    filler_feature_values,
    ax=axarr.flat[<span class="hljs-number">3</span>],
)
plt.subplots_adjust(top=<span class="hljs-number">1</span>, bottom=<span class="hljs-number">0</span>, hspace=<span class="hljs-number">0.2</span>, wspace=<span class="hljs-number">0.2</span>)
plt.show()
</code></pre>
    <p class="normal">In the <a id="_idIndexMarker130"/>plot in <em class="italic">Figure 2.2</em>, the circle<a id="_idIndexMarker131"/> represents test case #2872. In all the plots bar one, this test case is on the negative (left-hand side) decision region, representing <code class="inlineCode">cardio=0</code> classification. The borderline high <code class="inlineCode">ap_hi</code> (systolic blood pressure) and the relatively high <code class="inlineCode">age</code> are barely enough for a positive prediction in the top-left chart. Still, in any case, for test case #2872, we have predicted a 57% score for CVD, so this could very well explain most of it.</p>
    <p class="normal">Not surprisingly, by themselves, <code class="inlineCode">ap_hi</code> and a healthy <code class="inlineCode">cholesterol</code> value are not enough to tip the scales in favor of a definitive CVD diagnosis according to the model because it’s decidedly in the negative decision region, and neither is a normal <code class="inlineCode">ap_lo</code> (diastolic blood pressure). You can tell from these three charts that although there’s some overlap in the distribution of squares and triangles, there is a tendency for more triangles to gravitate toward the positive side as the <em class="italic">y</em>-axis increases, while fewer squares populate this region:</p>
    <figure class="mediaobject"><img src="../Images/B18406_02_02.png" alt="Calendar  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.2: The decision regions for ap_hi and other top risk factors, with test case #2872</p>
    <p class="normal">The <a id="_idIndexMarker132"/>overlap across the decision<a id="_idIndexMarker133"/> boundary is expected because, after all, these squares and triangles are based on the effects of <strong class="keyWord">all</strong> features. Still, you expect to find a somewhat consistent pattern. The chart with <code class="inlineCode">ap_hi</code> versus <code class="inlineCode">weight</code> doesn’t have this pattern vertically as <code class="inlineCode">weight</code> increases, which suggests something is missing in this story… Hold that thought because we are going to investigate that in the next section!</p>
    <p class="normal">Congratulations! You have completed the second part of the minister’s request.</p>
    <p class="normal">Decision region plotting, a <strong class="keyWord">local model interpretation method</strong>, provided the health ministry with <a id="_idIndexMarker134"/>a tool to interpret individual case predictions. You could now extend this to explain several cases at a time, or plot all-important feature combinations to find the ones where the<a id="_idIndexMarker135"/> circle is decidedly in the positive decision region. You can also change some of the <a id="_idIndexMarker136"/>filler variables one at a time to see how they make a difference. For instance, what if you increase the filler age to the median age of 54 or even to the age of test case #2872? Would a borderline high <code class="inlineCode">ap_hi</code> and healthy <code class="inlineCode">cholesterol</code> now be enough to tip the scales? We will answer this question later, but first, let’s understand what can make machine learning interpretation so difficult.</p>
    <h1 id="_idParaDest-50" class="heading-1">Appreciating what hinders machine learning interpretability</h1>
    <p class="normal">In the last <a id="_idIndexMarker137"/>section, we were wondering why the chart with <code class="inlineCode">ap_hi</code> versus <code class="inlineCode">weight</code> didn’t have a conclusive pattern. It could very well be that although <code class="inlineCode">weight</code> is a risk factor, there are other critical <em class="italic">mediating variables</em> that could explain the increased risk of CVD. A <strong class="keyWord">mediating variable</strong> is<a id="_idIndexMarker138"/> one that influences the strength between the independent and target (<em class="italic">dependent</em>) variable. We probably don’t have to think too hard to find what is missing. In <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em>, we performed linear regression on <code class="inlineCode">weight</code> and <code class="inlineCode">height</code> because there’s a linear relationship between these variables. In the context of human health, <code class="inlineCode">weight</code> is not nearly as <em class="italic">meaningful</em> without <code class="inlineCode">height</code>, so you need to look at both.</p>
    <p class="normal">Perhaps if we plot the decision regions for these two variables, we will get some clues. We can plot them with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">fig, ax = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>))
mldatasets.create_decision_plot(
    X_test,
    y_test,
    log_result,
    [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>],
    [<span class="hljs-string">'height [cm]'</span>,
    <span class="hljs-string">'weight [kg]'</span>],
    X_highlight,
    filler_feature_values,
    filler_feature_ranges,
    ax=ax
)
plt.show()
</code></pre>
    <p class="normal">The preceding snippet will generate the plot in <em class="italic">Figure 2.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_02_03.png" alt="Chart, scatter chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.3: The decision regions for weight and height, with test case #2872</p>
    <p class="normal">No decision <a id="_idIndexMarker139"/>boundary was ascertained in <em class="italic">Figure 2.3</em> because if all other variables are held constant (at a less risky value), no <code class="inlineCode">height</code> and <code class="inlineCode">weight</code> combination is enough to predict CVD. However, we can tell that there is a pattern for the orange triangles, mostly located in one ovular area. This provides exciting insight that even though we expect <code class="inlineCode">weight</code> to increase when <code class="inlineCode">height</code> increases, the concept of an inherently unhealthy <code class="inlineCode">weight</code> value is not one that increases linearly with <code class="inlineCode">height</code>.</p>
    <p class="normal">In fact, for almost two centuries, this relationship has been mathematically understood by the <a id="_idIndexMarker140"/>name <strong class="keyWord">body mass index</strong> (<strong class="keyWord">BMI</strong>):</p>
    <p class="center"><img src="../Images/B18406_02_001.png" alt="" role="presentation"/></p>
    <p class="normal">Before we <a id="_idIndexMarker141"/>discuss BMI further, you must consider complexity. Dimensionality aside, there are chiefly three things that introduce complexity that makes interpretation difficult:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Non-linearity</li>
      <li class="numberedList">Interactivity</li>
      <li class="numberedList">Non-monotonicity</li>
    </ol>
    <h2 id="_idParaDest-51" class="heading-2">Non-linearity</h2>
    <p class="normal">Linear <a id="_idIndexMarker142"/>equations such as <img src="../Images/B18406_02_002.png" alt="" role="presentation"/> are easy to understand. They are additive, so it is easy to separate and quantify the effects of each of its terms (<em class="italic">a</em>, <em class="italic">bx</em>, and <em class="italic">cz</em>) from the outcome of the model (<em class="italic">y</em>). Many model classes have linear equations incorporated in the math. These equations can both be used to fit the data to the model and describe the model.</p>
    <p class="normal">However, there are model classes that are inherently non-linear because they introduce non-linearity in their training. Such is the case for <em class="italic">deep learning</em> models because they have non-linear activation functions such as <em class="italic">sigmoid</em>. However, logistic regression is considered a <strong class="keyWord">Generalized Linear Model</strong> (<strong class="keyWord">GLM</strong>) because<a id="_idIndexMarker143"/> it’s additive. In other words, the outcome is a sum of weighted inputs and parameters. We will discuss GLMs further in <em class="chapterRef">Chapter 3</em>, <em class="italic">Interpretation Challenges</em>.</p>
    <p class="normal">However, even if your model is linear, the relationships between the variables may not be linear, which can lead to poor performance and interpretability. What you can do in these cases is adopt either of the following approaches:</p>
    <ul>
      <li class="bulletList"><em class="italic">Use a non-linear model class</em>, which will fit these non-linear feature relationships much better, possibly improving model performance. Nevertheless, as we will explore in more detail in the next chapter, this can make the model less interpretable.</li>
      <li class="bulletList"><em class="italic">Use domain knowledge to engineer a feature that can help “linearize” it</em>. For instance, if you had a feature that increased exponentially against another, you can engineer a new variable with the logarithm of that feature. In the case of our CVD prediction, we know BMI is a better way to understand weight in the company of height. Best of all, it’s not an <em class="italic">arbitrary</em> made-up feature, so it’s easier to interpret. We can prove this point by making a copy of the dataset, engineering the BMI feature in it, training the model with this extra feature, and performing local model interpretation. The following code snippet does just that:
        <pre class="programlisting code"><code class="hljs-code">X2 = cvd_df.drop([<span class="hljs-string">'cardio'</span>], axis=<span class="hljs-number">1</span>).copy()
X2[<span class="hljs-string">"bmi"</span>] = X2[<span class="hljs-string">"weight"</span>] / (X2[<span class="hljs-string">"height"</span>]/<span class="hljs-number">100</span>)**<span class="hljs-number">2</span>
</code></pre>
      </li>
    </ul>
    <p class="normal">To illustrate<a id="_idIndexMarker144"/> this new feature, let’s plot <code class="inlineCode">bmi</code> against both <code class="inlineCode">weight</code> and <code class="inlineCode">height</code> using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">fig, (ax1, ax2, ax3) = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>, figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">4</span>))
sns.regplot(x=<span class="hljs-string">"weight"</span>, y=<span class="hljs-string">"bmi"</span>, data=X2, ax=ax1)
sns.regplot(x=<span class="hljs-string">"height"</span>, y=<span class="hljs-string">"</span><span class="hljs-string">bmi"</span>, data=X2, ax=ax2)
sns.regplot(x=<span class="hljs-string">"height"</span>, y=<span class="hljs-string">"weight"</span>, data=X2, ax=ax3)
plt.subplots_adjust(top = <span class="hljs-number">1</span>, bottom=<span class="hljs-number">0</span>, hspace=<span class="hljs-number">0.2</span>, wspace=<span class="hljs-number">0.3</span>)
plt.show()
</code></pre>
    <p class="normal"><em class="italic">Figure 2.4</em> is produced with the preceding code:</p>
    <figure class="mediaobject"><img src="../Images/B18406_02_04.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 2.4: Bivariate comparison between weight, height, and bmi</p>
    <p class="normal">As you can appreciate by the plots in <em class="italic">Figure 2.4</em>, there is a more definite linear relationship between <code class="inlineCode">bmi</code> and <code class="inlineCode">weight</code> than between <code class="inlineCode">height</code> and <code class="inlineCode">weight</code> and, even, between <code class="inlineCode">bmi</code> and <code class="inlineCode">height</code>.</p>
    <p class="normal">Let’s fit the <a id="_idIndexMarker145"/>new model with the extra feature using the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">X2 = X2.drop([<span class="hljs-string">'weight'</span>,<span class="hljs-string">'</span><span class="hljs-string">height'</span>], axis=<span class="hljs-number">1</span>)
X2_train, X2_test,__,_ = train_test_split(
  X2, y, test_size=<span class="hljs-number">0.15</span>, random_state=<span class="hljs-number">9</span>)
log_model2 = sm.Logit(y_train, sm.add_constant(X2_train))
log_result2 = log_model2.fit()
</code></pre>
    <p class="normal">Now, let’s see whether test case #2872 is in the positive decision region when comparing <code class="inlineCode">ap_hi</code> to <code class="inlineCode">bmi</code> if we keep <code class="inlineCode">age</code> constant at <code class="inlineCode">60</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">filler_feature_values2 = {
    <span class="hljs-string">"age"</span>: <span class="hljs-number">60</span>, <span class="hljs-string">"gender"</span>: <span class="hljs-number">1</span>, <span class="hljs-string">"ap_hi"</span>: <span class="hljs-number">110</span>,
    <span class="hljs-string">"ap_lo"</span>: <span class="hljs-number">70</span>, <span class="hljs-string">"cholesterol"</span>: <span class="hljs-number">1</span>, <span class="hljs-string">"gluc"</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">"smoke"</span>: <span class="hljs-number">0</span>, <span class="hljs-string">"alco"</span>:<span class="hljs-number">0</span>, <span class="hljs-string">"active"</span>:<span class="hljs-number">1</span>, <span class="hljs-string">"bmi"</span>:<span class="hljs-number">20</span> 
}
X2_highlight = np.reshape(
    np.concatenate(([<span class="hljs-number">1</span>],X2_test.iloc[<span class="hljs-number">2872</span>].to_numpy())), (<span class="hljs-number">1</span>, <span class="hljs-number">11</span>)
)
fig, ax = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>))
mldatasets.create_decision_plot(
    X2_test, y_test, log_result2,
    [<span class="hljs-string">"ap_hi"</span>, <span class="hljs-string">"bmi"</span>], <span class="hljs-literal">None</span>, X2_highlight,
    filler_feature_values2, ax=ax)
plt.show()
</code></pre>
    <p class="normal">The preceding code plots decision regions in <em class="italic">Figure 2.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_02_05.png" alt="Chart, scatter chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.5: The decision regions for ap_hi and bmi, with test case #2872</p>
    <p class="normal"><em class="italic">Figure 2.5</em> shows<a id="_idIndexMarker146"/> that controlling for <code class="inlineCode">age</code>, <code class="inlineCode">ap_hi</code>, and <code class="inlineCode">bmi</code> can help explain the positive prediction for CVD because the circle is in the positive decision region. Please note that there are some likely anomalous <code class="inlineCode">bmi</code> outliers (the highest BMI ever recorded was 204), so there are probably some incorrect weights or heights in the dataset.</p>
    <div class="note">
      <p class="normal">WHAT’S THE PROBLEM WITH OUTLIERS?</p>
      <p class="normal">Outliers can be <strong class="keyWord">influential</strong> or <strong class="keyWord">high leverage</strong> and, therefore, affect the model when trained with these included. Even if they don’t, they can make interpretation more difficult. If they are <strong class="keyWord">anomalous</strong>, then you should remove them, as we did with blood pressure at the beginning of this chapter. And sometimes, they can hide in plain sight because they are only perceived as <em class="italic">anomalous</em> in the context of other features. In any case, there are practical reasons why outliers are problematic, such as making plots like the preceding one “zoom out” to be able to fit them while not letting you appreciate the decision boundary where it matters. And there are also more profound reasons, such as losing trust in the data, thereby tainting trust in the models that were trained on that data, or making the model perform worse. This sort of problem is to be expected with real-world data. Even though we haven’t done it in this chapter for the sake of expediency, it’s essential to begin every project by thoroughly exploring the data, treating missing values and outliers, and doing other data housekeeping tasks.</p>
    </div>
    <h2 id="_idParaDest-52" class="heading-2">Interactivity</h2>
    <p class="normal">When we <a id="_idIndexMarker147"/>created <code class="inlineCode">bmi</code>, we didn’t only linearize a non-linear relationship, but we also created interactions <a id="_idIndexMarker148"/>between two features. <code class="inlineCode">bmi</code> is, therefore, an <strong class="keyWord">interaction feature</strong>, but this was informed by domain knowledge. However, many model classes do this automatically by permutating all kinds of operations between features. After all, features have <em class="italic">latent</em> relationships between one another, much like <code class="inlineCode">height</code> and <code class="inlineCode">width</code>, and <code class="inlineCode">ap_hi</code> and <code class="inlineCode">ap_lo</code>. Therefore, automating the process of looking for them is not always a bad thing. In fact, it can even be absolutely necessary. This is the case for many deep learning problems where the data is unstructured and, therefore, part of the task of training the model is looking for the latent relationships to make sense of it.</p>
    <p class="normal">However, for structured data, even though interactions can be significant for model performance, they can hurt interpretability by adding potentially unnecessary complexity to the model and also <a id="_idIndexMarker149"/>finding<a id="_idIndexMarker150"/> latent relationships that <em class="italic">don’t mean anything</em> (which is called a <strong class="keyWord">spurious relationship</strong> or <strong class="keyWord">correlation</strong>).</p>
    <h2 id="_idParaDest-53" class="heading-2">Non-monotonicity</h2>
    <p class="normal">Often, a variable<a id="_idIndexMarker151"/> has a meaningful and consistent relationship between a feature and the target variable. So, we know that as <code class="inlineCode">age</code> increases, the risk of CVD (<code class="inlineCode">cardio</code>) must increase. There is no point at which you reach a certain age and this risk drops. Maybe the risk slows down, but it does not drop. We call <a id="_idIndexMarker152"/>this <strong class="keyWord">monotonicity</strong>, and functions that are <em class="italic">monotonic</em> are either always increasing or decreasing throughout their entire domain.</p>
    <p class="normal">Please note that <strong class="keyWord">all</strong> linear relationships are monotonic, but not all monotonic relationships are necessarily linear. This is because they don’t have to be a straight line. A common problem in machine learning is that a model doesn’t know about a monotonic relationship that we expect because of our domain expertise. Then, because of noise and omissions in the data, the model is trained in such a way in which there are ups and downs where you don’t expect them.</p>
    <p class="normal">Let’s propose a<a id="_idIndexMarker153"/> hypothetical example. Let’s imagine that due to a lack of availability of data for 57–60-year-olds, and because the few cases we did have for this range were negative for CVD, the model could learn that this is where you would expect a drop in CVD risk. Some model classes are inherently monotonic, such as logistic regression, so they can’t have this problem, but many others do. We will examine this in more detail in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_02_06.png" alt="Figure 2.6 – A partial dependence plot between a target variable (yhat) and a predictor with monotonic and non-monotonic models "/></figure>
    <p class="packt_figref">Figure 2.6: A partial dependence plot between a target variable (yhat) and a predictor with monotonic and non-monotonic models</p>
    <p class="normal"><em class="italic">Figure 2.6</em> is what is called <a id="_idIndexMarker154"/>a <strong class="keyWord">Partial Dependence Plot</strong> (<strong class="keyWord">PDP</strong>), from an unrelated example. PDPs are a concept<a id="_idIndexMarker155"/> we will study in further detail in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>, but what is important to grasp from it is that the prediction <code class="inlineCode">yhat</code> is supposed to decrease as the feature <code class="inlineCode">quantity_indexes_for_real_gdp_by_state</code> increases. As you can tell by the lines, in the monotonic model, it consistently decreases, but in the non-monotonic one, it has jagged peaks as it decreases, and then increases at the very end.</p>
    <h1 id="_idParaDest-54" class="heading-1">Mission accomplished</h1>
    <p class="normal">The first part of the mission was to understand risk factors for cardiovascular disease, and you’ve determined that the top four risk factors are systolic blood pressure (<code class="inlineCode">ap_hi</code>), <code class="inlineCode">age</code>, <code class="inlineCode">cholesterol</code>, and <code class="inlineCode">weight</code> according to the logistic regression model, of which only <code class="inlineCode">age</code> is non-modifiable. However, you also realized that systolic blood pressure (<code class="inlineCode">ap_hi</code>) is not as meaningful on its own since it relies on diastolic blood pressure (<code class="inlineCode">ap_lo</code>) for interpretation. The same goes for <code class="inlineCode">weight</code> and <code class="inlineCode">height</code>. We learned that the interaction of features plays a crucial role in interpretation, and so does their relationship with each other and the target variable, whether linear or monotonic. Furthermore, the data is only a representation of the truth, which can be wrong. After all, we found <em class="italic">anomalies</em> that, left unchecked, can bias our model.</p>
    <p class="normal">Another source of bias is how the data was collected. After all, you can wonder why the model’s top features were all objective and examination features. Why isn’t smoking or drinking a larger factor? To verify whether there<a id="_idIndexMarker156"/> was <em class="italic">sample</em> <em class="italic">bias</em> involved, you would have to compare with other more trustworthy datasets to check whether your dataset is underrepresenting drinkers and smokers. Or maybe the bias was introduced by the question that asked whether they smoked now, and not whether they had ever smoked for an extended period.</p>
    <p class="normal">Another type of bias that we could <a id="_idIndexMarker157"/>address is <em class="italic">exclusion bias</em>—our data might be missing information that explains the truth that the model is trying to depict. For instance, we know through medical research that blood pressure issues such as isolated systolic hypertension, which increases CVD risk, are caused by underlying conditions such as diabetes, hyperthyroidism, arterial stiffness, and obesity, to name a few. The only one of these conditions that we can derive from the data is obesity and not the other ones. If we want to be able to interpret a model’s predictions well, we need to have all relevant features. Otherwise, there will be gaps we cannot explain. Maybe once we add them, they won’t make much of a difference, but that’s what the methods we will learn in <em class="chapterRef">Chapter 10</em>, <em class="italic">Feature Selection and Engineering for Interpretability</em>, are for.</p>
    <p class="normal">The second part of the mission was to be able to interpret individual model predictions. We can do this well enough by plotting decision regions. It’s a simple method, but it has many limitations, especially in situations where there are more than a handful of features, and they tend to interact a lot with each other. <em class="chapterRef">Chapter 5</em>, <em class="italic">Local Model-Agnostic Interpretation Methods</em>, and <em class="chapterRef">Chapter 6</em>, <em class="italic">Anchors and Counterfactual Explanations</em>, will cover local interpretation methods in more detail. However, the decision region plot method helps illustrate many of the concepts surrounding decision boundaries, which we will discuss in those chapters.</p>
    <h1 id="_idParaDest-55" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we covered two model interpretation methods: feature importance and decision boundaries. We also learned about model interpretation method types and scopes and the three elements that impact interpretability in machine learning. We will keep mentioning these fundamental concepts in subsequent chapters. For a machine learning practitioner, it is paramount to be able to spot them so that we can know what tools to leverage to overcome interpretation challenges. In the next chapter, we will dive deeper into this topic.</p>
    <h1 id="_idParaDest-56" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Molnar, Christoph. <em class="italic">Interpretable Machine Learning. A Guide for Making Black Box Models Explainable</em>. 2019: <a href="https://christophm.github.io/interpretable-ml-book/"><span class="url">https://christophm.github.io/interpretable-ml-book/</span></a></li>
      <li class="bulletList"><em class="italic">Mlextend Documentation. Plotting Decision Regions</em>: <a href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/"><span class="url">http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/</span></a></li>
    </ul>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask the author questions, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="Chapter_2.xhtml"><span class="url">https://packt.link/inml</span></a></p>
    <p class="normal"><img src="../Images/QR_Code107161072033138125.png" alt="" role="presentation"/></p>
  </div>
</body></html>