<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Creating a Physics Simulation Based on a Pen and Paper Sketch</h1>
                </header>
            
            <article>
                
<div class="packt_quote"><span>"James Bond lives in a nightmarish world where laws are written at the point of a gun."</span><br/>
<span>                                                                    – Yuri Zhukov, Pravda, 30 September 1965</span></div>
<div class="packt_quote"><span>"Just a moment. Three measures of Gordon's, one of vodka, half a measure of Kina Lillet. Shake it very well until it's ice-cold, then add a large thin slice of lemon peel. Got it?"</span><br/>
<span>                                                            – Casino Royale, Chapter 7, Rouge et Noir (1953)</span></div>
<p>James Bond is a precise man. Like a physicist, he seems to see order in a world where others see chaos. Another mission, another romance, another shaken drink, another crashing car or helicopter or skier, and another gunshot, do not change the way the world works<span>—</span>the way the Cold War works. He seems to take comfort in this consistency.</p>
<p>A psychologist might say that Bond is reenacting an unhappy childhood, which the novels reveal to us in brief glimpses. The boy lacked a permanent home. His father was an international arms dealer for the Vickers company, so the family moved often for work. When James was 11, his parents died in a mountain climbing accident<span>, </span><span>the first of many dramatic, untimely deaths in the Bond saga. An aunt in Kent took in the orphaned James, but the next year he was sent to boarding at Eton College. There, the lonesome boy became infatuated with a maid, got into trouble over it, and was expelled, the first of his many short-lived and fraught romances. Next, he was sent even further from family, to Fettes College in Scotland. The pattern of displacement and trouble was set. By 16, he was trying to live the life of a playboy in Paris. By 20, he was a dropout from the University of Geneva and he was off to join the Royal Navy at the height of the Second World War.</span></p>
<p>Amid all that upheaval, Bond did manage to learn a thing or two. He is clever<span>—</span>not just with his eyebrow-raising witty remarks, but also with his fast solutions to puzzles that involve mechanics, kinematics, or physics. He is never caught flat-footed (although he is sometimes caught in other ways).</p>
<p>The moral of the story is that a secret agent must practice his physics, even under the most trying of circumstances. An app can help with that.</p>
<p>When I think about problems of geometry or physics, I like to draw them with a pen and paper. However, I also like to see animations. Our app, <kbd>Rollingball</kbd>, will allow us to combine these two media. It will use computer vision to detect simple geometric shapes that the user can draw on paper. Then, based on the detected shapes, the app will create a physics simulation that the user can watch. The user can also influence the simulation by tilting the device to alter the simulated direction of gravity. The experience is like designing and playing with one's own version of a ball-in-a-maze puzzle, a fine toy for aspiring secret agents.</p>
<p>Building games is fun<span>, </span>but it is not all fun and games! We have a new list of skills to master in this chapter:</p>
<ul>
<li>Detecting linear and circular edges with the Hough transform</li>
<li>Using OpenCV in the Unity game engine</li>
<li>Building a Unity game for Android</li>
<li>Converting coordinates from OpenCV's space to Unity's space and creating three-dimensional objects in Unity based on our detection results in OpenCV</li>
<li>Customizing the appearance and physics behavior of three-dimensional objects in Unity, using shaders, materials, and physics materials</li>
<li>Drawing lines and rectangles using OpenGL calls from Unity</li>
</ul>
<p>With these goals in mind, let's get ready to play ball!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter's project has the following software dependencies:</p>
<ul>
<li>Unity—a cross-platform game engine that supports Windows and Mac as development platforms. Development on Linux is not supported in this chapter.</li>
<li>OpenCV for Unity.</li>
<li>Android SDK, which comes with Android Studio.</li>
</ul>
<p>Where not otherwise noted, setup instructions are covered in <a href="e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml">Chapter 1</a>,<a href="e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml"/> <em>Preparing for the Mission</em>. You might want to build and run the project in <a href="61bdd7aa-b605-4061-8bfe-71084c7c7104.xhtml">Chapter 4</a>, <em>Controlling a Phone App with Your Suave Gestures</em>, to ensure that Android SDK is properly set up as part of Android Studio. Setup instructions for OpenCV for Unity are covered in the current chapter, in the section <em>Setting up OpenCV for Unity</em>. Always refer to the setup instructions for any version requirements. Instructions for building and running Unity projects are covered in the current chapter.</p>
<p class="mce-root"><span>The completed project from this chapter can be found in this book's GitHub repository, at </span><a href="https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition">https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition</a><span>, in the </span><kbd>Chapter006</kbd><span> folder.</span> The repository doesn't contain the OpenCV for Unity plugin, which must be licensed and added to the project, as described in the <em>Setting up OpenCV for Unity</em> section in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Planning the Rollingball app</h1>
                </header>
            
            <article>
                
<p><kbd>Rollingball</kbd> will be a mobile app. We will develop it in the Unity game engine by using a third-party plugin called <strong>OpenCV for Unity</strong>. The app will be compatible with both Android and iOS. Our build instructions will focus on Android, but we will also provide a few notes for readers who are experienced with the iOS build process (on Mac).</p>
<div class="packt_infobox">For instructions on setting up Unity and finding relevant documentation and tutorials, please refer back to the <em>Setting up Unity and OpenCV</em> section in <a href="e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml">Chapter 1</a>, <em><span class="ChapterrefPACKT">Preparing for the Mission</span></em>. At the time of writing this book, Unity's officially supported development environments are Windows and Mac, although there is ongoing beta development toward Linux support.</div>
<p>Using the mobile device's camera, <kbd>Rollingball</kbd> will scan two types of primitive shapes—circles and lines. The user will start by drawing any combination of these primitive shapes, or by setting up linear or circular objects on a plain background. For example, refer to the following image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/06aa0190-2658-4f7c-9eea-abf14afcfaef.jpg" style="width:8.92em;height:11.75em;"/></p>
<p>Here, we have several circles drawn on a paper napkin. Our detector will work best with outlines, rather than solid circles, and particularly with smoothly drawn outlines, rather than lumpy or broken outlines. For this image, our detector will work best on the two rightmost circles. We also have a pen with edges that look like straight lines against the background of the paper. Our detector will work well with these linear edges.</p>
<p><kbd>Rollingball</kbd> is a simple app in which the user primarily interacts with one Android activity or one iOS view controller. A live video feed fills most of the background. When circles or lines are detected, they are highlighted in red, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/22b94b14-9bb8-4ad2-b8e4-50ae5c9e93b7.png" style="width:14.83em;height:26.33em;"/></p>
<p>Note that some linear edges are detected multiple times. Lighting effects and discontinuities in the pen's color create ambiguities about where its edges are located.</p>
<p>The user can press a button to start the physics simulation. During the simulation, the video pauses, the detector stops running, and the red-highlighted areas are replaced with cyan balls and lines. The lines are stationary, but the balls fall freely and may bounce off each other and off the lines. Real-world gravity, as measured by the mobile device's gravity sensor, is used to control the simulated direction of gravity. However, the simulation is two-dimensional, and gravity is flattened so that it points toward an edge of the screen. The following screenshot shows the simulated balls after they have fallen partway down the page, bounced apart, and rolled along the lines:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/15deca3d-40ac-4389-85f1-c7d84167a3d4.png" style="width:15.58em;height:27.92em;"/></p>
<p>The user can press the button again to clear all the simulated objects and resume the live video and detection. The cycle can continue indefinitely, with the user choosing to simulate different drawings or different views of the same drawing.</p>
<p>Now, let's consider the techniques for detecting circles and lines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detecting circles and lines</h1>
                </header>
            
            <article>
                
<p>From <kbd>The Living Headlights</kbd> (our project in <a href="b4619968-1f90-45f0-8a77-3505624bc0c0.xhtml">Chapter 5</a>,<em><span class="ChapterrefPACKT"> Equipping Your Car with a Rearview Camera and Hazard Detection</span></em>), we are already familiar with one technique for detecting circles. We treated the problem as a special case of blob detection, and we used an OpenCV class, <kbd>SimpleBlobDetector</kbd>, which allows us to specify many detection criteria, such as a blob's size, color, and circularity (or non-circularity, that is, linearity).</p>
<p>A <strong>blob</strong> is a shape filled with a solid (or nearly solid) color. This definition implies that many circular or linear objects are not detectable as blobs. In the following screenshot, we can see a sunlit desk with a china teapot, china bowl, and pewter bowl:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0b3b59eb-5cfc-4c51-8188-4198a7036671.jpg" style="width:38.08em;height:25.25em;"/></p>
<p>The bowls and the lid of the teapot have approximately circular outlines in this top-down view. However, they are unlikely to pass detection as blobs, because the interior of each shape is multicolored, especially in uneven light.</p>
<p>Blob detection starts with a simple threshold filter (marking bright regions as white and dark regions as black); a more general approach to shape detection should start with an edge-finding filter (marking edge regions as white and interior regions as black) and then a thresholding process. We define an edge as the discontinuity between neighboring regions of different brightness. Thus, an edge pixel has darker neighbors on one side and brighter neighbors on the opposite side. An edge-finding filter subtracts neighbor values from one side and adds them from the opposite side, in order to measure how strongly a pixel exhibits this edge-like contrast in a given direction. To achieve a measurement that is independent of edges' directions, we can apply multiple filters (each oriented for edges of a different direction) and treat each filter's output as a dimension of a vector whose magnitude represents the overall <strong>edginess</strong> of the pixel. A set of such measurements for all pixels is sometimes called the <strong>derivative</strong> of the image. Having computed the image's derivative, we select a threshold value based on the minimum contrast that we require in an edge. A high threshold accepts only high-contrast edges, while a lower threshold also accepts lower-contrast edges.</p>
<p>A popular edge-finding technique is the <strong>Canny algorithm</strong>. OpenCV's implementation, the <kbd>Imgproc.Canny</kbd> function, performs both filtering and thresholding. As arguments, it takes a grayscale image, an output image, a low threshold value, and a high threshold value. The low threshold should accept all pixels that might be part of a good edge. The high threshold should only accept pixels that are definitely part of a good edge. From the set whose members might be edge pixels, the Canny algorithm only accepts the members that connect to definite edge pixels. The double criteria help to ensure that we can accept thin extremities of a major edge, while rejecting edges that are altogether faint. For example, a pen stroke or the curb of a road extending into the distance may be a major edge with thin extremities.</p>
<p>Having identified edge pixels, we can count how many of them are intersected by a given primitive shape. The greater the number of intersections, the more confident we can be that the given primitive shape correctly represents an edge in the image. Each intersection is called a <strong>vote</strong>, and a shape needs a specified number of votes to be accepted as a real edge's shape. Out of all possible primitive shapes (of a given kind) in the image, we consider an evenly spaced, representative sample. We do so by specifying a step size for the shapes' geometric parameters. (For example, a line's parameters are a point and angle, while a circle's parameters are a center point and radius.) This sample of possible shapes is called a <strong>grid</strong>, the individual shapes in it are called <strong>cells</strong>, and votes are said to be cast in cells. This process (tallying the matches between actual edge pixels and a sample of possible shapes) is the core of a technique called the <strong>Hough transform</strong>, which has various specializations, such as <strong>Hough line detection</strong> and <strong>Hough circle detection</strong>.</p>
<p>Hough line detection has two implementations in OpenCV—<kbd>Imgproc.HoughLines</kbd>, which is based on the original Hough transform, and <kbd>Imgproc.HoughLinesP</kbd>, which is based on a probabilistic variant of the Hough transform. <kbd>Imgproc.HoughLines</kbd> does an exhaustive count of intersections for all possible lines for a given pair of step sizes, in pixels and in radians. <kbd>Imgproc.HoughLinesP</kbd> is usually faster (particularly in images with a few long line segments), as it takes possible lines in a random order and discards some of the possible lines after finding a good line in a region. <kbd>Imgproc.HoughLines</kbd> expresses each line as a distance from the origin and an angle, whereas <kbd>Imgproc.HoughLinesP</kbd> expresses each line as two points<span>, </span>the endpoints of a detected segment of the line<span>, </span><span>which is a more useful representation, since it gives us the option to treat the detection results as line segments, rather than indefinitely long lines. For both functions, the arguments include the image (which should be preprocessed with Canny, or a similar algorithm), the step sizes in pixels and radians, and the minimum number of intersections required to accept a line. The arguments to</span> <kbd>Imgproc.HoughLinesP</kbd> <span>also include a minimum length between endpoints and a maximum gap, where a gap consists of non-edge pixels between edge pixels that intersect the line.</span></p>
<p>Hough circle detection has one implementation in OpenCV, <kbd>Imgproc.HoughCircles</kbd>, which is based on a variant of the Hough transform that makes use of gradient information at edges. This function's arguments include the image (not preprocessed with Canny or a similar algorithm, as <span><kbd>Imgproc.HoughCircles</kbd> applies the Canny algorithm internally</span>), a downsampling factor (which acts somewhat like a blur factor to smooth the edges of potential circles), a minimum distance between detected circles' centers, a Canny edge detection threshold, a minimum number of intersections required to accept a circle, and minimum and maximum radii. The specified Canny threshold is the upper threshold; internally, the lower threshold is hardcoded as half of the upper threshold.</p>
<div class="packt_infobox">For more details on the Canny algorithm, the Hough transform, and OpenCV's implementations of them, refer to <em><span class="ChapterrefPACKT">Chapter 7, Extracting Lines, Contours, and Components,</span></em> in Robert Laganière's book, <em>OpenCV 3 Computer Vision Application Programming Cookbook</em> (<em>Packt Publishing, 2017</em>).</div>
<p>Despite using a more efficient algorithm than the original Hough transform, <kbd>Imgproc.HoughCircles</kbd> is a computationally expensive function. Nonetheless, we use it in <kbd>Rollingball</kbd>, since many of today's mobile devices can handle the cost. For low-powered devices, such as Raspberry Pi, we would consider blob detection as a cheaper alternative. <span><kbd>Imgproc.HoughCircles</kbd> tends to work best with outlines of circles, whereas blob detection only works on solid circles.</span> For line detection, we use the <kbd>Imgproc.HoughLinesP</kbd> function, which is not as expensive as OpenCV's other Hough detectors.</p>
<p>Having chosen the algorithms and their OpenCV implementations, let's set up the plugin that will let us easily access this functionality in Unity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up OpenCV for Unity</h1>
                </header>
            
            <article>
                
<p>Unity provides a cross-platform framework for scripting games in C#. However, it also supports platform-specific plugins in languages such as C, C++, Objective-C (for Mac and iOS), and Java (for Android). Developers may publish these plugins (and other assets) on the Unity Asset Store. Many published plugins represent a large amount of high-quality work, and buying one may be more economical than writing your own.</p>
<p>OpenCV for Unity, by ENOX SOFTWARE (<a href="https://enoxsoftware.com">https://enoxsoftware.com</a>), is a $95 plugin (at the time of writing this book). It offers a C# API that is closely based on OpenCV's official Java (Android) bindings. However, the plugin wraps OpenCV's C++ libraries and is compatible with Android, iOS, Windows Phone, Windows, Mac, Linux, and WebGL. It is reliable in my experience, and it saves us a lot of work that we would otherwise put into custom C++ code and C# wrappers. Moreover, it comes with several valuable samples.</p>
<div class="packt_infobox">OpenCV for Unity is not the only set of third-party C# bindings for OpenCV. Alternatives include OpenCvSharp (<span class="URLPACKT"><a href="https://github.com/shimat/opencvsharp">https://github.com/shimat/opencvsharp</a></span>) and Emgu CV (<span class="URLPACKT"><a href="http://www.emgu.com">http://www.emgu.com</a></span>). However, in this book, we use OpenCV for Unity because it offers easy integration with Unity and it tends to be updated quickly when new OpenCV versions are released.</div>
<p>Let's go shopping. Open Unity and create a new project. From the menu bar, select <span class="packt_screen">Window </span><span class="packt_screen">| </span><span class="packt_screen">Asset Store</span>. If you haven't already created a Unity account, follow the prompts to create one. Once you have logged into the store, you should see the <span class="packt_screen">Asset Store</span> window. Enter <kbd><span class="KeyPACKT">OpenCV for Unity</span></kbd> in the search bar in the upper-right corner. Click on the <span class="packt_screen">OpenCV for Unity</span> link among the search results. You should see something similar to the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e9246272-9d84-4bb3-984f-3315282a08cd.png" style="width:51.67em;height:37.42em;"/></p>
<p>Click on the <span class="packt_screen">Add to Cart</span> button and complete the transaction, as directed. Click on the <span class="packt_screen">Download</span> button and wait for the download to complete. Click on the <span class="packt_screen">Import</span> button. You should now see the <span class="packt_screen">Import Unity Package</span> window, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cedde273-d2e9-4bfa-9451-73a684b3cd2c.png" style="width:40.58em;height:50.58em;"/></p>
<p>This is a list of all of the files in the bundle that we just purchased. Ensure that all of their checkboxes are checked, and then click on the <span class="packt_screen">Import</span> button. Soon, you should see all of the files in the <span class="packt_screen">Project</span> pane of the Unity editor.</p>
<p>The bundle includes further setup instructions and helpful links in the <kbd>OpenCVForUnity/ReadMe.pdf</kbd> file. Read the <kbd>ReadMe!</kbd> note that contains useful instructions for iOS if you wish to build for that platform.</p>
<div class="packt_infobox">Throughout this chapter, paths are relative to the project's <kbd>Assets</kbd> folder, unless otherwise specified.</div>
<p>Next, let's try the samples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring and building the Unity project</h1>
                </header>
            
            <article>
                
<p>Unity supports many target platforms. Switching to a new target is easy, as long as our plugins support it. We just need to set a few build configuration values, some of which are shared across multiple targets, and some of which are platform-specific.</p>
<p>From the menu bar, select <span class="packt_screen">Unity <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Preferences...</span>, which should bring up the <span class="packt_screen">Preferences</span><strong> </strong>window. Click on the <span class="packt_screen">External Tools</span> tab and set <span class="packt_screen">Android SDK</span> to be the base path to your Android SDK installation. Normally, for an Android Studio environment, the path to the SDK is <kbd>C:\Users\username\AppData\Local\Android\sdk\</kbd> on Windows or <kbd>Users/&lt;your_username&gt;/Library/Android/sdk/</kbd> on Mac. Now, the window should look similar to the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2a2b0a88-878e-4354-977b-c4ee096370af.png" style="width:57.50em;height:50.00em;"/></p>
<p>Now, from the menu bar, select <span class="packt_screen">File <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Build Settings</span>. The <span class="packt_screen">Build Settings</span> window should appear. Drag all of the sample scene files, such as <kbd>OpenCVForUnity/Examples/OpenCVForUnityExample.unity</kbd> and <kbd>OpenCVForUnity/Examples/Advanced/ComicFilterExample/ComicFilterExample.unity</kbd>, from the <span class="packt_screen">Project</span> pane to the <span class="packt_screen">Scenes In Build</span> list in the <span class="packt_screen">Build Settings</span> window. The first scene in the list is the start-up scene. Ensure that <kbd>OpenCVForUnityExample</kbd> is the first in the list. (Drag and drop the list items to reorder them.) Also, ensure that all of the scenes' checkboxes are checked. Click on the <span class="packt_screen">Android</span> platform, and then the <span class="packt_screen">Switch Platform</span> button. The window should now look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/52291210-4d8e-41d0-8e2a-cb3c9db41479.png" style="width:57.50em;height:58.00em;"/></p>
<p>Click on the <span class="packt_screen">Player Settings...</span> button. A list of settings should appear in the <span class="packt_screen">Inspector</span> pane of the Unity editor. Fill in a <span class="packt_screen">Company Name</span>, such as <kbd><span class="KeyPACKT">Nummist Media Corporation Limited</span></kbd>, and a <span class="packt_screen">Product Name</span>, such as <kbd><span class="KeyPACKT">Rollingball</span></kbd>. Optionally, select a <span class="packt_screen">Default Icon</span> (which must be an image file that you have added somewhere in the <span class="packt_screen">Project</span> pane). Click on <span class="packt_screen">Resolution and Presentation</span> to expand it, and then, for <span class="packt_screen">Default Orientation</span>, select <span class="packt_screen">Portrait</span>. So far, the <span class="packt_screen">PlayerSettings</span> options should look similar to the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1a93fd9b-ea54-4d00-8a0b-1c64814e7991.png" style="width:42.50em;height:53.83em;"/></p>
<p>Click on <span class="packt_screen">Other Settings</span> to expand it, and then fill out a <span class="packt_screen">Bundle Identifier</span> with something like <kbd><span class="KeyPACKT">com.nummist.rollingball</span></kbd>. Now, we have finished with the <span class="packt_screen">PlayerSettings</span> options.</p>
<p>Ensure that an Android device is plugged in and that USB debugging is enabled on the device. Go back to the <span class="packt_screen">Build Settings</span> window and click on <span class="packt_screen">Build and Run</span>. Specify a path for the build. It is good practice to separate the build path from the Unity project folder, just as you would normally separate builds from source code. Once the build has begun, a progress bar should appear. Watch the <span class="packt_screen">Console</span> pane of the Unity editor to be sure that no build errors occur. When the build has finished, it is copied onto the Android device, and then it runs.</p>
<p>Enjoy the OpenCV for Unity samples! If you like, browse their source code and scenes in the Unity editor.</p>
<p>Next, we have our own scene to build!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the Rollingball scene in Unity</h1>
                </header>
            
            <article>
                
<p>Let's create a directory, <kbd>Rollingball</kbd>, to contain our application-specific code and assets. Right-click in the <span class="packt_screen">Project</span> pane and choose <span class="packt_screen">Create <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Folder</span> from the context menu. Rename the new folder <kbd><span class="KeyPACKT">Rollingball</span></kbd>. Create a subfolder, <kbd>Rollingball/Scenes</kbd>, in a similar way.</p>
<p>From the menu bar, select <span class="packt_screen">File <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">New Scene</span> and then <span class="packt_screen">File <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Save As...</span>. Save the scene as <kbd>Rollingball/Scenes/Rollingball.unity</kbd>.</p>
<p>By default, our newly created scene only contains a camera (<span>that is, the virtual world's camera, not a capture device) and a directional light. The light will illuminate the balls and lines in our physics simulation. We are going to add three more objects, in the following way:</span></p>
<ol>
<li>From the menu bar, select <span class="packt_screen">GameObject <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">3D Object <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Quad</span>. An object called <kbd>Quad</kbd> should appear in the <span class="packt_screen">Hierarchy</span> pane. Rename <kbd>Quad</kbd> to <kbd><span class="KeyPACKT">VideoRenderer</span></kbd>. This object is going to represent the live video feed.</li>
<li>From the menu bar, select <span class="packt_screen">GameObject <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Create Empty</span>. An object called <kbd>GameObject</kbd> should appear in the <span class="packt_screen">Hierarchy</span> pane. Rename <kbd>GameObject</kbd> to <kbd><span class="KeyPACKT">QuitOnAndroidBack</span></kbd>. Later, it will hold a script component that responds to the standard back button on Android.</li>
</ol>
<div class="packt_infobox">Objects in the <span class="packt_screen">Hierarchy</span> are called <strong>game objects</strong>, and the sections that are visible in their <span class="packt_screen">Inspector</span> panes are called <strong>components</strong>.</div>
<p>Drag the <span class="packt_screen">Main Camera</span> onto the <span class="packt_screen">VideoRenderer</span> to make the former a child of the latter. A child moves, rotates, and scales up or down when its parent does. The relevance is that we want our camera to maintain a predictable relationship with the live video background.</p>
<div class="packt_infobox">Parent-child relationships in the <span class="packt_screen">Hierarchy</span> do not represent object-oriented inheritance; in other words, a child does not have an <strong>is</strong> <strong>a</strong> relationship with its parent. Rather, a parent who has a one-to-many <strong>has a </strong>relationship with its children.</div>
<p>With the new objects created and the <span class="packt_screen">Main Camera</span> reparented, the <span class="packt_screen">Hierarchy</span> should look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0d1bae8f-c080-4dfc-89f2-cdb71e85aa4b.png" style="width:17.33em;height:10.67em;"/></p>
<p>The <span class="packt_screen">VideoRenderer</span> and <span class="packt_screen">Main Camera</span> will be configured in code, based on the properties of the mobile device's video camera. However, let's set some reasonable defaults. Select the <span class="packt_screen">VideoRenderer</span> in the <span class="packt_screen">Hierarchy</span>, and then, in the <span class="packt_screen">Inspector</span> pane, edit its <span class="packt_screen">Transform</span> properties to match the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cbaee7bf-a058-4e31-a297-3ace58ef5cf3.png" style="width:31.83em;height:14.50em;"/></p>
<p>Similarly, select <span class="packt_screen">Main Camera</span> and edit its <span class="packt_screen">Transform</span> and <span class="packt_screen">Camera</span> properties to match the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/090ddbd6-3ecf-44c1-b4e5-b04cc88540e5.png" style="width:31.08em;height:36.58em;"/></p>
<p>Note that we have configured an orthographic projection, meaning that the objects' pixel sizes are constant, regardless of their distance from the camera. This configuration is appropriate for a two-dimensional game or simulation, such as <kbd>Rollingball</kbd>.</p>
<p>These four objects are the foundation of our scene. The rest of the project involves attaching custom properties to these objects and using C# scripts to control them and create new objects around them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating Unity assets and adding them to the scene</h1>
                </header>
            
            <article>
                
<p>The custom properties and behaviors in a Unity project are defined through various types of files that are generically called <strong>assets</strong>. Our project has four remaining questions and requirements that we must address by creating and configuring assets:</p>
<ul>
<li>What is the appearance of the surfaces in the scene—namely, the video feed, the detected circles and lines, and the simulated balls and lines? We need to write <em>shader</em> code and create <em>Material</em> configurations to define the appearance of these surfaces.</li>
<li>How bouncy are the balls? We need to create a <em>Physics Material</em> configuration to answer this all-important question.</li>
<li>What objects represent a simulated ball and a simulated line? We need to create and configure <em>Prefab</em> objects that the simulation can instantiate.</li>
<li>How does it all behave? We need to write Unity <em>scripts</em>—specifically, code that subclasses a Unity class called <kbd>MonoBehaviour</kbd>—in order to control objects in the scene at various stages in their life cycles.</li>
</ul>
<p>The following subsections will tackle these requirements one by one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing shaders and creating materials</h1>
                </header>
            
            <article>
                
<p>A <strong>shader</strong> is a set of functions that run on the GPU. Although such functions can be applied to general-purpose computing, typically, they are used for graphics rendering—that is, to define the color of output pixels on the screen based on input that describes the lighting, geometry, surface texture, and perhaps other variables, such as time. Unity comes with many shaders for common styles of three-dimensional and two-dimensional rendering. We can also write our own shaders.</p>
<div class="packt_infobox">For in-depth tutorials on shader scripting in Unity, see the <em>Unity 2018 Shaders and Effects Cookbook,</em> <em>by John P. Doran and Alan Zucconi</em> (<em>Packt Publishing, 2018</em>).</div>
<p>Let's create a folder, <kbd>Rollingball/Shaders</kbd>, and then create a shader in it (by clicking on <span class="packt_screen">Create <span><span><span class="packt_screen">|</span></span></span></span><span class="packt_screen">Shader <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Standard Surface Shader</span> in the <span class="packt_screen">Project</span> pane's context menu). Rename the shader <kbd>DrawSolidColor</kbd>. Double-click on it to edit it, and replace the contents with the following code:</p>
<pre>Shader "Draw/Solid Color" {<br/>  Properties {<br/>    _Color ("Main Color", Color) = (1.0, 1.0, 1.0, 1.0)<br/>  }<br/>  SubShader {<br/>    Pass { Color [_Color] }<br/>  }<br/>}</pre>
<p>This humble shader has one parameter—a color. The shader renders pixels in this color regardless of conditions, such as lighting. For the purposes of the <span class="packt_screen">Inspector</span> GUI, the shader's name is <span class="packt_screen">Draw <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Solid Color</span>, and its parameter's name is <span class="packt_screen">Main Color</span>.</p>
<p>A material has a shader and a set of parameter values for the shader. The same shader may be used by multiple materials, which may use different parameter values. Let's create a material that draws solid red. We will use this material to highlight detected circles and lines.</p>
<p>Create a new folder, <kbd>Rollingball/Materials</kbd>, and then create a material in it (by clicking on <span class="packt_screen">Create <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Material</span> in the context menu). Rename the material <kbd><span class="KeyPACKT">DrawSolidRed</span></kbd>. Select it, and in the <span class="packt_screen">Inspector</span>, set its shader to <span class="packt_screen">Draw | </span><span class="packt_screen">Solid Color</span> and its <span class="packt_screen">Main Color</span> to the RGBA value for red (<kbd>255</kbd>, <kbd>0</kbd>, <kbd>0</kbd>, <kbd>255</kbd>). The <span class="packt_screen">Inspector</span> should now look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/296677c6-e4bd-4767-a931-726f5d53c06c.png" style="width:25.25em;height:11.75em;"/></p>
<p>We are going to create two more materials using shaders that come with Unity. First, create a material, name it <kbd>Cyan</kbd>, and configure it so that its shader is <span class="packt_screen">Legacy Shaders <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Diffuse</span> and its <span class="packt_screen">Main Color</span> is cyan (<kbd>0</kbd>, <kbd>255</kbd>, <kbd>255</kbd>, <kbd>255</kbd>). Leave the <span class="packt_screen">Base (RBG)</span> texture as <span class="packt_screen">None</span>. We will apply this material to the simulated balls and lines. Its <span class="packt_screen">Inspector</span> should look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cc6d7f0e-ff56-4b3f-9b80-5734b44d366e.png" style="width:27.42em;height:20.75em;"/></p>
<p>Now, create a material named <span class="packt_screen">Video</span> and configure it so that its shader is <span class="packt_screen">Unlit <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Texture</span>. Leave the <span class="packt_screen">Base (RBG)</span> texture as <span class="packt_screen">None</span>. Later, through code, we will assign the video texture to this material. Drag the <span class="packt_screen">Video</span> material (from the <span class="packt_screen">Project</span> pane) to <span class="packt_screen">VideoRenderer</span> (in the <span class="packt_screen">Hierarchy</span> pane) in order to assign the material to the quad. Select <span class="packt_screen">VideoRenderer</span> and confirm that its <span class="packt_screen">Inspector</span> includes the following items:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bad6bd55-589a-42f9-bf97-b00382c86d8f.png" style="width:26.58em;height:17.08em;"/></p>
<p>We will assign the remaining materials once we have created prefabs and scripts.</p>
<p>Now that we have made materials for rendering, let's look at the analogous concept of physics materials.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating physics materials</h1>
                </header>
            
            <article>
                
<p>Although Unity's rendering pipeline can run custom functions that we write in shaders, its physics pipeline runs fixed functions. Nonetheless, we can configure the parameters of those functions through physics materials.</p>
<div class="packt_infobox">Unity's physics engine is based on NVIDIA PhysX. PhysX supports acceleration through CUDA on NVIDIA GeForce GPUs. However, on typical mobile devices, the physics calculations will run on the CPU.</div>
<p>Let's create a folder, <kbd>Rollingball/Physics Materials</kbd>, and in it, create a physics material (by clicking on <span class="packt_screen">Create <span><span><span class="packt_screen">| </span></span></span></span><span class="packt_screen">Physics Material</span> in the context menu). Rename the physics material <kbd><span class="KeyPACKT">Bouncy</span></kbd>. Select it, and note that it has the following properties in the <span class="packt_screen">Inspector</span>:</p>
<ul>
<li><strong>Dynamic Friction</strong>: This is the ratio between the force that presses two objects together (for example, gravity) and the frictional force that resists continued motion along the surface.</li>
<li><strong>Static Friction</strong>: This is the ratio between the force pressing two objects together (for example, gravity) and the frictional force that resists initial motion along the surface. Refer to Wikipedia (<span class="URLPACKT"><a href="https://en.wikipedia.org/wiki/Friction#Approximate_coefficients_of_friction">https://en.wikipedia.org/wiki/Friction#Approximate_coefficients_of_friction</a></span>) for sample values. For static friction, a value of 0.04 is like Teflon on Teflon, a value of 1.0 is like rubber on concrete, and a value of 1.05 is like copper on cast iron.</li>
<li><strong>Bounciness</strong>: This is the proportion of an object's kinetic energy that it retains when it bounces off another surface. Here, a value of <kbd>0</kbd> means that the object doesn't bounce. A value of <kbd>1</kbd> means that it bounces without a loss of energy. A value greater than 1 means that it (unrealistically) gains energy when it bounces.</li>
<li><strong>Friction Combine</strong>: When objects collide, which object's friction value affects this object? The options are <span class="packt_screen">Average</span>, <span class="packt_screen">Minimum</span>, <span class="packt_screen">Multiply</span>, and <span class="packt_screen">Maximum</span>.</li>
<li><strong>Bounce Combine</strong>: When objects collide, which object's bounciness value affects this object? The options are <span class="packt_screen">Average</span>, <span class="packt_screen">Minimum</span>, <span class="packt_screen">Multiply</span>, and <span class="packt_screen">Maximum</span>.</li>
</ul>
<div class="packt_tip"><span class="packt_screen">Careful! Are those physics materials explosive?</span><br/>
A physics simulation is said to explode when the values grow continually and overflow the system's floating-point numeric limits. For example, if a collision's combined bounciness is greater than <kbd>1</kbd> and the collision occurs repeatedly, then over time, the forces tend toward infinity. Kaboom! We broke the physics engine.<br/>
<br/>
Even without weird physics materials, numeric problems arise in scenes of an extremely large or small scale. For example, consider a multiplayer game that uses input from the <strong>Global Positioning System</strong><span> (</span><strong>GPS</strong><span>) so</span> that objects in a Unity scene are positioned according to the players' real-world longitude and latitude. The physics simulation cannot handle a human-sized object in this scene, because the object and the forces acting on it are so small that they vanish inside the margin of floating-point error! This is a case where the simulation implodes (rather than explodes).<strong><br/></strong></div>
<p>Let's set the <span class="packt_screen">Bounciness</span> to <kbd>1</kbd> (very bouncy!) and leave the other values at their defaults. Later, you can adjust everything to your tastes, if you wish. The <span class="packt_screen">Inspector</span> should look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b0fe9302-677d-4cf1-80bb-24e60d39a813.png" style="width:26.67em;height:14.33em;"/></p>
<p>Our simulated lines will use default physics parameters, so they don't need a physics material.</p>
<p>Now that we have our rendering materials and physics materials, let's create prefabs for an entire simulated ball and an entire simulated line.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating prefabs</h1>
                </header>
            
            <article>
                
<p>A <strong>prefab</strong> is an object that isn't a part of a scene itself, but is designed to be copied into scenes during editing or at runtime. It can be copied many times to make many objects in the scene. At runtime, the copies have no special connection to the prefab or each other, and all copies can behave independently. Although the role of a prefab is sometimes likened to the role of a class, a prefab is not a type.</p>
<p>Even though prefabs are not part of a scene, they are created and typically edited through a scene. Let's create a sphere in the scene by selecting <span class="packt_screen">GameObject </span><span class="packt_screen">| </span><span class="packt_screen">3D Object </span><span class="packt_screen">| </span><span class="packt_screen">Sphere</span> from the menu bar. An object named <strong><span class="packt_screen">Sphere</span></strong> should appear in the <span class="packt_screen">Hierarchy</span>. Rename it <kbd>SimulatedCircle</kbd>. Drag each of the following assets from the <span class="packt_screen">Project</span> pane onto <kbd>SimulatedCircle</kbd> in the <span class="packt_screen">Hierarchy</span>:</p>
<ul>
<li><strong><span class="packt_screen">Cyan</span></strong> (in <kbd>Rollingball/Materials</kbd>)</li>
<li><strong><span class="packt_screen">Bouncy</span></strong> (in <kbd>Rollingball/PhysicsMaterials</kbd>)</li>
</ul>
<p>Now, select <span class="packt_screen">SimulatedCircle</span>. In the <span class="packt_screen">Inspector</span>, click on <span class="packt_screen">Add Component</span> and select <span class="packt_screen">Physics </span><span class="packt_screen">| </span><span class="packt_screen">Rigidbody</span>. A <span class="packt_screen">Rigidbody</span> section should appear in the <span class="packt_screen">Inspector</span>. In this section, expand the <span class="packt_screen">Constraints</span> field and check <span class="packt_screen">Freeze Position </span><span class="packt_screen">| </span><span class="packt_screen">Z</span>. The effect of this change is to constrain the sphere's motion to two dimensions. Confirm that the <span class="packt_screen">Inspector</span> looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bf5b268b-0e8a-496c-b5ab-dfcb42e8810d.png" style="width:22.33em;height:56.75em;"/></p>
<p class="mce-root"/>
<p>Create a folder, <kbd>Rollingball/Prefabs</kbd>, and drag <kbd>SimulatedCircle</kbd> from the <span class="packt_screen">Hierarchy</span> into the folder in the <span class="packt_screen">Project</span> pane. A prefab, also named <kbd>SimulatedCircle</kbd>, should appear in the folder. Meanwhile, the name of the <kbd>SimulatedCircle</kbd> object in the <span class="packt_screen">Hierarchy</span> should turn blue to indicate that the object has a prefab connection. Changes to the object in the scene may be applied back to the prefab by clicking on the <span class="packt_screen">Apply</span> button in the scene object's <span class="packt_screen">Inspector</span>. Conversely, changes to the prefab (at edit time, not at runtime) are automatically applied to instances in scenes, except for properties in which an instance has unapplied changes.</p>
<p>Now, let's follow similar steps to create a prefab of a simulated line. Create a cube in the scene by selecting <span class="packt_screen">GameObject </span><span class="packt_screen">| </span><span class="packt_screen">3D Object </span><span class="packt_screen">| </span><span class="packt_screen">Cube</span> from the menu bar. An object named <span class="packt_screen">Cube</span> should appear in the <span class="packt_screen">Hierarchy</span>. Rename it <kbd><span class="KeyPACKT">SimulatedLine</span></kbd>. Drag <span class="packt_screen">Cyan</span> from the <span class="packt_screen">Project</span> pane onto <span class="packt_screen">SimulatedLine</span> in the <span class="packt_screen">Hierarchy</span>. Select <span class="packt_screen">SimulatedLine</span>, add a <span class="packt_screen">Rigidbody</span> component, and, in the <span class="packt_screen">Rigidbody</span> section of its <span class="packt_screen">Inspector</span>, check <span class="packt_screen">Is Kinematic</span>, which means that the object is not moved by the physics simulation (even though it is a part of the simulation for the purpose of other objects colliding with it). Recall that we want the lines to be stationary. They are just obstacles for the falling balls. The <span class="packt_screen">Inspector</span> should now look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3deaf9e6-9d68-4cd7-9796-6ab38e5a9496.png" style="width:22.42em;height:56.67em;"/></p>
<p>Let's clean up our scene by deleting the instances of the prefabs from the <span class="packt_screen">Hierarchy</span> so that we don't have any circles or lines in the scene when it opens. (However, we want to keep the prefabs themselves in the <span class="packt_screen">Project</span> so that we can instantiate them later through our scripts.) Now, let's turn our attention to the writing of scripts, which, among other things, are able to copy prefabs at runtime.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing our first Unity script</h1>
                </header>
            
            <article>
                
<p>As we mentioned earlier, a Unity script is a subclass of <kbd>MonoBehaviour</kbd>. A <kbd>MonoBehaviour</kbd> object can obtain references to objects in the <span class="packt_screen">Hierarchy</span> and components that we attach to these objects in the <span class="packt_screen">Inspector</span>. A <kbd>MonoBehaviour</kbd> object also has its own <span class="packt_screen">Inspector</span>, where we can assign additional references, including references to <span class="packt_screen">Project</span> assets, such as prefabs. At runtime, Unity sends messages to all <kbd>MonoBehaviour</kbd> objects when certain events occur. A subclass of <kbd>MonoBehaviour</kbd> may implement callbacks for any of these messages. <kbd>MonoBehaviour</kbd> supports more than 60 standard message callbacks. Here are some examples:</p>
<ul>
<li><kbd>Awake</kbd>: This is called during initialization.</li>
<li><kbd>Start</kbd>: This is called after <kbd>Awake</kbd>, but before the first call to <kbd>Update</kbd>.</li>
<li><kbd>Update</kbd>: This is called every frame.</li>
<li><kbd>OnGUI</kbd>: This is called when the GUI overlay is ready for rendering instructions and GUI events are ready to be handled.</li>
<li><kbd>OnPostRender</kbd>: This is called after the scene is rendered. This is an appropriate callback in which to implement post-processing effects.</li>
<li><kbd>OnDestroy</kbd>: This is called when this instance of the script is about to be destroyed. For example, this happens when the scene is about to end.</li>
</ul>
<div class="packt_infobox">For more information on the standard message callbacks and the arguments that some callbacks' implementations may optionally take, see the official documentation at <span class="URLPACKT"><a href="http://docs.unity3d.com/ScriptReference/MonoBehaviour.html">http://docs.unity3d.com/ScriptReference/MonoBehaviour.html</a></span>. Also, note that we can send custom messages to all <kbd>MonoBehaviour</kbd> objects by using the <kbd>SendMessage</kbd> method.</div>
<p>Implementations of these and Unity's other callbacks may be <kbd>private</kbd>, <kbd>protected</kbd>, or <kbd>public</kbd>. Unity calls them regardless of the protection level.</p>
<p>To summarize, then, scripts are the glue—the game logic—connecting runtime events to various objects that we see in the <span class="packt_screen">Project</span>, <span class="packt_screen">Hierarchy</span>, and <span class="packt_screen">Inspector</span>.</p>
<p>Let's create a folder, <kbd>Rollingball/Scripts</kbd>, and in that, create a script (by clicking on <span class="packt_screen">Create </span><span class="packt_screen">| </span><span class="packt_screen">C# Script</span> in the context menu). Rename the script <kbd>QuitOnAndroidBack</kbd> and double-click on it to edit it. Replace its contents with the following code:</p>
<pre>using UnityEngine;<br/><br/><br/>namespace com.nummist.rollingball {<br/><br/>    public sealed class QuitOnAndroidBack : MonoBehaviour {<br/><br/>        void Start() {<br/>            // Show the standard Android navigation bar.<br/>            Screen.fullScreen = false;<br/>        }<br/><br/>        void Update() {<br/>            if (Input.GetKeyUp(KeyCode.Escape)) {<br/>                Application.Quit();<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>We are using a namespace, <kbd>com.nummist.rollingball</kbd>, to keep our code organized and to avoid potential conflicts between our type names and type names in other parties' code. Namespaces in C# are like packages in Java. Our class is called <kbd>QuitOnAndroidBack</kbd>. It extends Unity's <kbd>MonoBehaviour</kbd> class. We use the <kbd>sealed</kbd> modifier (similar to Java's <kbd>final</kbd> modifier) to indicate that we don't intend to create subclasses of <kbd>QuitOnAndroidBack</kbd>.</p>
<div class="packt_infobox">Note that <kbd>MonoBehaviour</kbd> uses the UK English spelling of behavior.</div>
<p>Thanks to Unity's callback system, the script's <kbd>Start</kbd> method is called after the object is initialized—in this case, at the start of the scene. Our <kbd>Start</kbd> method ensures that the standard Android navigation bar is visible. After <kbd>Start</kbd>, the script's <kbd>Update</kbd> method gets called every frame. It checks whether the user has pressed a key (or button) that is mapped to the <kbd>Escape</kbd> keycode. On Android, the standard back button is mapped to <kbd>Escape</kbd>. When the key (or button) is pressed, the application quits.</p>
<p>Save the script and drag it from the <strong><span class="packt_screen">Project</span></strong> pane to the <kbd>QuitOnAndroidBack</kbd> object in the <span class="packt_screen">Hierarchy</span>. Click on the <kbd>QuitOnAndroidBack</kbd> object and confirm that its <span class="packt_screen">Inspector</span> looks like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2a6142e4-2d6c-46e6-8e48-b82452922c60.png" style="width:23.75em;height:15.17em;"/></p>
<p>That was an easy script, right? The next one is a bit trickier<span>, </span><span>but more fun, because it handles everything</span> except <span>quitting.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the main Rollingball script</h1>
                </header>
            
            <article>
                
<p>Let's create a folder, <kbd>Rollingball/Scripts</kbd>, and in it, create a script (by clicking on <span class="packt_screen">Create </span><span class="packt_screen">| </span><span class="packt_screen">C# Script</span> in the context menu). Rename the script <kbd><span class="KeyPACKT">DetectAndSimulate</span></kbd> and double-click on it to edit it. Delete its default contents and begin the code with the following <kbd>import</kbd> statements:</p>
<pre>using UnityEngine;<br/>using System.Collections;<br/>using System.Collections.Generic;<br/>using System.IO;<br/><br/>using OpenCVForUnity.CoreModule;<br/>using OpenCVForUnity.ImgprocModule;<br/>using OpenCVForUnity.UnityUtils;</pre>
<p>Next, let's declare our namespace and class with the following code:</p>
<pre>namespace com.nummist.rollingball {<br/><br/>    [RequireComponent (typeof(Camera))]<br/>    public sealed class DetectAndSimulate : MonoBehaviour {</pre>
<p>Note that the class has an attribute, <kbd>[RequireComponent (typeof(Camera))]</kbd>, which means that the script can only be attached to a game object that has a camera (not a video camera, but rather, a game-world camera representing the player's virtual eye in the scene). We specify this requirement because we are going to highlight the detected shapes through an implementation of the standard <kbd>OnPostRender</kbd> callback, and this callback only gets called for scripts attached to a game object with a camera.</p>
<p><kbd>DetectAndSimulate</kbd> needs to store representations of circles and lines in both two-dimensional screen space and three-dimensional world space. Coordinates in screen space (that is, on the user's screen) are measured in pixels, with the screen's top-left pixel pixel being the origin. Coordinates in world space (that is, in the game scene where we recently positioned <kbd>VideoRenderer</kbd> and <kbd>Main Camera</kbd>) are measured in arbitrary units with an arbitrary origin. The representations of circles and lines don't need to be visible to any other class in our application, so it is appropriate to define their types as private inner structs. Our <kbd>Circle</kbd> type stores two-dimensional coordinates representing the circle's center in screen space, a float representing its radius in screen space, and three-dimensional coordinates representing the circle's center in world space. A constructor accepts all of these values as arguments. Here is the <kbd>Circle</kbd> implementation:</p>
<pre>        struct Circle {<br/><br/>            public Vector2 screenPosition;<br/>            public float screenDiameter;<br/>            public Vector3 worldPosition;<br/><br/>            public Circle(Vector2 screenPosition,<br/>                          float screenDiameter,<br/>                          Vector3 worldPosition) {<br/>                this.screenPosition = screenPosition;<br/>                this.screenDiameter = screenDiameter;<br/>                this.worldPosition = worldPosition;<br/>            }<br/>        }</pre>
<p>We define another inner struct, <kbd>Line</kbd>, to store two sets of two-dimensional coordinates representing endpoints in screen space and two sets of three-dimensional coordinates representing the same endpoints in world space. A constructor accepts all of these values as arguments. Here is the implementation of <kbd>Line</kbd>:</p>
<pre>        struct Line {<br/>            <br/>            public Vector2 screenPoint0;<br/>            public Vector2 screenPoint1;<br/>            public Vector3 worldPoint0;<br/>            public Vector3 worldPoint1;<br/>            <br/>            public Line(Vector2 screenPoint0,<br/>                        Vector2 screenPoint1,<br/>                        Vector3 worldPoint0,<br/>                        Vector3 worldPoint1) {<br/>                this.screenPoint0 = screenPoint0;<br/>                this.screenPoint1 = screenPoint1;<br/>                this.worldPoint0 = worldPoint0;<br/>                this.worldPoint1 = worldPoint1;<br/>            }<br/>        }</pre>
<p>Next, we define member variables that are editable in the <span class="packt_screen">Inspector</span>. Such a variable is marked with the <kbd>[SerializeField]</kbd> attribute, which means that Unity serializes the variable, despite it being non-public. (Alternatively, public variables are also editable in the <span class="packt_screen">Inspector</span>.) The following four variables describe our preferences for camera input, including the direction the camera faces, its resolution, and its frame rate:</p>
<pre>        [SerializeField] bool useFrontFacingCamera = false;<br/>        [SerializeField] int preferredCaptureWidth = 640;<br/>        [SerializeField] int preferredCaptureHeight = 480;<br/>        [SerializeField] int preferredFPS = 15;</pre>
<p>At runtime, the camera devices and modes available to us may differ from these preferences.</p>
<p>We will also make several more variables editable in the <span class="packt_screen">Inspector</span><span>—</span>namely, a reference to the video background's renderer, a reference to the material for highlighting detected shapes, a factor for adjusting the scale of the simulation's gravity, references to the simulated shapes' prefabs, and a font size for the button:</p>
<pre>        [SerializeField] Renderer videoRenderer;<br/><br/>        [SerializeField] Material drawPreviewMaterial;<br/><br/>        [SerializeField] float gravityScale = 8f;<br/><br/>        [SerializeField] GameObject simulatedCirclePrefab;<br/>        [SerializeField] GameObject simulatedLinePrefab;<br/><br/>        [SerializeField] int buttonFontSize = 24;</pre>
<p>We also have a number of member variables that don't need to be editable in the <span class="packt_screen">Inspector</span>. Among them are references to the game world's camera, a reference to the real-world camera's video texture, matrices to store images and intermediate processing results, and measurements relating to camera images, the screen, simulated objects, and the button:</p>
<pre>        Camera _camera;<br/><br/>        WebCamTexture webCamTexture;<br/>        Color32[] colors;<br/>        Mat rgbaMat;<br/>        Mat grayMat;<br/>        Mat cannyMat;<br/><br/>        float screenWidth;<br/>        float screenHeight;<br/>        float screenPixelsPerImagePixel;<br/>        float screenPixelsYOffset;<br/><br/>        float raycastDistance;<br/>        float lineThickness;<br/>        UnityEngine.Rect buttonRect;</pre>
<p>We store a matrix of Hough circle representations in OpenCV's format (which has image coordinates for a landscape image, in this case) and a list of circle representations in our own <kbd>Circle</kbd> format (which has screen coordinates for a portrait screen, as well as three-dimensional coordinates for a game world):</p>
<pre>        Mat houghCircles;<br/>        List&lt;Circle&gt; circles = new List&lt;Circle&gt;();</pre>
<p>Similarly, we store a matrix of Hough line representations in OpenCV's format and a list of line representations in our own <kbd>Line</kbd> format:</p>
<pre>        Mat houghLines;<br/>        List&lt;Line&gt; lines = new List&lt;Line&gt;();</pre>
<p>We hold a reference to the gyroscope input device and we store the magnitude of gravity to be used in our physics simulation:</p>
<pre>        Gyroscope gyro;<br/>        float gravityMagnitude;</pre>
<div>
<div class="packt_infobox">
<p>We (and the Unity API) are using the terms <strong>gyroscope</strong> and <strong>gyro</strong> loosely. We are referring to a fusion of motion sensors that may or may not include a real gyroscope. A gyroscope can be simulated, albeit poorly, by using other real sensors, such as an accelerometer or gravity sensor.</p>
<p><span>Unity provides a property,</span> <kbd>SystemInfo.supportsGyroscope</kbd><span>, to indicate whether the device has a real gyroscope. However, this information doesn't concern us. We just use Unity's</span> <kbd>Gyroscope.gravity</kbd> <span>property, which might be derived from a real gravity sensor or might be simulated by using other real sensors, such as an accelerometer and/or gyroscope. Unity Android apps are configured to require an accelerometer by default, so we can safely assume that at least a simulated gravity sensor is available.</span></p>
</div>
</div>
<p>We keep track of a list of simulated objects, and we provide a property, <kbd>simulating</kbd>, that is <kbd>true</kbd> when the list is non-empty:</p>
<pre>        List&lt;GameObject&gt; simulatedObjects =<br/>                new List&lt;GameObject&gt;();<br/>        bool simulating {<br/>            get {<br/>                return simulatedObjects.Count &gt; 0;<br/>            }<br/>        }</pre>
<p>Now, let's turn our attention to methods. We implement the standard <kbd>Start</kbd> callback. The implementation begins by getting a reference to the attached camera, initializing matrices, getting a reference to the gyro, and computing the magnitude of the game world's gravity, as seen in the following code:</p>
<pre>        void Start() {<br/><br/>            // Cache the reference to the game world's<br/>            // camera.<br/>            _camera = GetComponent&lt;Camera&gt;();<br/><br/>            houghCircles = new Mat();<br/>            houghLines = new Mat();<br/><br/>            gyro = Input.gyro;<br/>            gravityMagnitude = Physics.gravity.magnitude *<br/>                               gravityScale;</pre>
<div class="packt_infobox">The <kbd>MonoBehaviour</kbd> object provides getters for many components that may be attached to the same game object as the script. Such components would appear alongside the script in the<span> Inspector. </span>For example, the <kbd>camera</kbd> getter returns a <kbd>Camera</kbd> object (or <kbd>null</kbd>, if none are present). These getters are expensive because they use introspection. Thus, if you need to refer to a component repeatedly, it is more efficient to store the reference in a member variable by using a statement such as <kbd>_camera = camera;</kbd>.</div>
<p>You might be wondering why we initialize <kbd>Mat</kbd> objects in the <kbd>Start</kbd> method, instead of initializing them when we declare them or in a constructor for <kbd>DetectAndSimulate</kbd>. The reason is that the OpenCV libraries are not necessarily loaded until after scripts such as <kbd>DetectAndSimulate</kbd> have been constructed.</p>
<p>The implementation of <kbd>Start</kbd> proceeds by finding a camera that faces the required direction (either front or rear, depending on the value of the preceding <kbd>useFrontFacingCamera</kbd> field). If we are playing the scene in the Unity editor (in order to debug the script and scene during development), we hardcode the camera direction to be front-facing, in order to support typical webcams. If no suitable camera is found, the method returns early, as seen in the following code:</p>
<pre>#if UNITY_EDITOR<br/>            useFrontFacingCamera = true;<br/>#endif<br/><br/>            // Try to find a (physical) camera that faces<br/>            // the required direction.<br/>            WebCamDevice[] devices = WebCamTexture.devices;<br/>            int numDevices = devices.Length;<br/>            for (int i = 0; i &lt; numDevices; i++) {<br/>                WebCamDevice device = devices[i];<br/>                if (device.isFrontFacing ==<br/>                            useFrontFacingCamera) {<br/>                    string name = device.name;<br/>                    Debug.Log("Selecting camera with " +<br/>                              "index " + i + " and name " +<br/>                              name);<br/>                    webCamTexture = new WebCamTexture(<br/>                            name, preferredCaptureWidth,<br/>                            preferredCaptureHeight,<br/>                            preferredFPS);<br/>                    break;<br/>                }<br/>            }<br/><br/>            if (webCamTexture == null) {<br/>                // No camera faces the required direction.<br/>                // Give up.<br/>                Debug.LogError("No suitable camera found");<br/>                Destroy(this);<br/>                return;<br/>            }</pre>
<div class="packt_infobox">Throughout our implementation of <kbd>DetectAndSimulate</kbd>, when we encounter an unrecoverable runtime problem, we call <kbd>Destroy(this);</kbd>, thereby deleting the instance of the script and preventing further messages from reaching its callbacks.</div>
<p>The <kbd>Start</kbd> callback concludes by activating the camera and gyroscope (including the gravity sensor) and launching a helper coroutine called <kbd>Init</kbd>:</p>
<pre>            // Ask the camera to start capturing.<br/>            webCamTexture.Play();<br/><br/>            if (gyro != null) {<br/>                gyro.enabled = true;<br/>            }<br/><br/>            // Wait for the camera to start capturing.<br/>            // Then, initialize everything else.<br/>            StartCoroutine(Init());<br/>        }</pre>
<div class="packt_infobox">A <strong>coroutine</strong> is a method that doesn't necessarily run to completion in one frame. Rather, it can <kbd>yield</kbd> for one or more frames, in order to wait for a certain condition to be fulfilled or to make something happen after a defined delay. Note that a coroutine runs on the main thread.</div>
<p>Our <kbd>Init</kbd> coroutine begins by waiting for the camera to capture the first frame. Then, we determine the frame's dimensions and we create OpenCV matrices to match these dimensions. Here is the first part of the method's implementation:</p>
<pre>        IEnumerator Init() {<br/>            <br/>            // Wait for the camera to start capturing.<br/>            while (!webCamTexture.didUpdateThisFrame) {<br/>                yield return null;<br/>            }<br/>            <br/>            int captureWidth = webCamTexture.width;<br/>            int captureHeight = webCamTexture.height;<br/>            float captureDiagonal = Mathf.Sqrt(<br/>                    captureWidth * captureWidth +<br/>                    captureHeight * captureHeight);<br/>            Debug.Log("Started capturing frames at " +<br/>                      captureWidth + "x" + captureHeight);<br/>            <br/>            colors = new Color32[<br/>                    captureWidth * captureHeight];<br/>            <br/>            rgbaMat = new Mat(captureHeight, captureWidth,<br/>                              CvType.CV_8UC4);<br/>            grayMat = new Mat(captureHeight, captureWidth,<br/>                              CvType.CV_8UC1);<br/>            cannyMat = new Mat(captureHeight, captureWidth,<br/>                               CvType.CV_8UC1);</pre>
<p>The coroutine proceeds by configuring the game world's orthographic camera and video quad to match the capture resolution and to render the video texture:</p>
<pre>            transform.localPosition =<br/>                    new Vector3(0f, 0f, -captureWidth);<br/>            _camera.nearClipPlane = 1;<br/>            _camera.farClipPlane = captureWidth + 1;<br/>            _camera.orthographicSize =<br/>                    0.5f * captureDiagonal;<br/>            raycastDistance = 0.5f * captureWidth;<br/><br/>            Transform videoRendererTransform =<br/>                    videoRenderer.transform;<br/>            videoRendererTransform.localPosition =<br/>                    new Vector3(captureWidth / 2,<br/>                                -captureHeight / 2, 0f);<br/>            videoRendererTransform.localScale =<br/>                    new Vector3(captureWidth,<br/>                                captureHeight, 1f);<br/><br/>            videoRenderer.material.mainTexture =<br/>                    webCamTexture;</pre>
<p>The device's screen and captured camera images likely have different resolutions. Moreover, recall that our application is configured for portrait orientation (in <span class="packt_screen">PlayerSettings</span>). This orientation affects screen coordinates, but not the coordinates in camera images, which will remain in landscape orientation. Thus, we need to calculate the conversion factors between image coordinates and screen coordinates, as seen in the following code:</p>
<pre>            // Calculate the conversion factors between<br/>            // image and screen coordinates.<br/>            // Note that the image is landscape but the<br/>            // screen is portrait.<br/>            screenWidth = (float)Screen.width;<br/>            screenHeight = (float)Screen.height;<br/>            screenPixelsPerImagePixel =<br/>                    screenWidth / captureHeight;<br/>            screenPixelsYOffset =<br/>                    0.5f * (screenHeight - (screenWidth *<br/>                    captureWidth / captureHeight));</pre>
<p>Our conversions will be based on fitting the video background to the width of the portrait screen, while either letterboxing or cropping the video at the top and bottom, if necessary.</p>
<p>The thickness of the simulated lines and the dimensions of the button are based on the screen resolution, as seen in the following code, which concludes the <kbd>Init</kbd> coroutine:</p>
<pre>            lineThickness = 0.01f * screenWidth;<br/><br/>            buttonRect = new UnityEngine.Rect(<br/>                    0.4f * screenWidth,<br/>                    0.75f * screenHeight,<br/>                    0.2f * screenWidth,<br/>                    0.1f * screenHeight);<br/>        }</pre>
<p>We implement the standard <kbd>Update</kbd> callback by processing gravity sensor input and processing camera input, provided that certain conditions are met. At the beginning of the method, if OpenCV objects are not yet initialized, the method returns early. Otherwise, the game world's direction of gravity is updated based on the real-world direction of gravity, as detected by the device's gravity sensor. Here is the first part of the method's implementation:</p>
<pre>        void Update() {<br/><br/>            if (rgbaMat == null) {<br/>                // Initialization is not yet complete.<br/>                return;<br/>            }<br/><br/>            if (gyro != null) {<br/>                // Align the game-world gravity to real-world<br/>                // gravity.<br/>                Vector3 gravity = gyro.gravity;<br/>                gravity.z = 0f;<br/>                gravity = gravityMagnitude *<br/>                          gravity.normalized;<br/>                Physics.gravity = gravity;<br/>            }</pre>
<p>Next, if there is no new camera frame ready or if the simulation is currently running, the method returns early. Otherwise, we convert the frame into OpenCV's format, convert it into gray, find the edges, and call two helper methods, <kbd>UpdateCircles</kbd> and <kbd>UpdateLines</kbd>, to perform shape detection. Here is the relevant code, which concludes the <kbd>Update</kbd> method:</p>
<pre>            if (!webCamTexture.didUpdateThisFrame) {<br/>                // No new frame is ready.<br/>                return;<br/>            }<br/><br/>            if (simulating) {<br/>                // No new detection results are needed.<br/>                return;<br/>            }<br/><br/>            // Convert the RGBA image to OpenCV's format using<br/>            // a utility function from OpenCV for Unity.<br/>            Utils.webCamTextureToMat(webCamTexture,<br/>                                     rgbaMat, colors);<br/><br/>            // Convert the OpenCV image to gray and<br/>            // equalize it.<br/>            Imgproc.cvtColor(rgbaMat, grayMat,<br/>                             Imgproc.COLOR_RGBA2GRAY);<br/>            Imgproc.Canny(grayMat, cannyMat, 50.0, 200.0);<br/><br/>            UpdateCircles();<br/>            UpdateLines();<br/>        }</pre>
<p>Our <kbd>UpdateCircles</kbd> helper method begins by performing Hough circle detection. We are looking for circles at least <kbd>10.0</kbd> pixels apart, with a radius of at least <kbd>5.0</kbd> pixels, and at most, <kbd>60</kbd> pixels. We specify that internally, <kbd>HoughCircles</kbd> should use a Canny upper threshold of <kbd>200</kbd>, <span>down sample by a factor of <kbd>2</kbd>, </span>and require <kbd>150.0</kbd> intersections to accept a circle. We clear the list of any previously detected circles. Then, we iterate over the results of the Hough circle detection. Here is the opening of the method's implementation:</p>
<pre>        void UpdateCircles() {<br/><br/>            // Detect blobs.<br/>            Imgproc.HoughCircles(grayMat, houghCircles,<br/>                                 Imgproc.HOUGH_GRADIENT, 2.0,<br/>                                 10.0, 200.0, 150.0, 5, 60);<br/><br/>            //<br/>            // Calculate the circles' screen coordinates<br/>            // and world coordinates.<br/>            //<br/><br/>            // Clear the previous coordinates.<br/>            circles.Clear();<br/><br/>            // Count the elements in the matrix of Hough circles.<br/>            // Each circle should have 3 elements:<br/>            // { x, y, radius }<br/>            int numHoughCircleElems = houghCircles.cols() *<br/>                                      houghCircles.rows() *<br/>                                      houghCircles.channels();<br/><br/>            if (numHoughCircleElems == 0) {<br/>                return;<br/>            }<br/><br/>            // Convert the matrix of Hough circles to a 1D array:<br/>            // { x_0, y_0, radius_0, ..., x_n, y_n, radius_n }<br/>            float[] houghCirclesArray = new float[numHoughCircleElems];<br/>            houghCircles.get(0, 0, houghCirclesArray);<br/><br/>            // Iterate over the circles.<br/>            for (int i = 0; i &lt; numHoughCircleElems; i += 3) {</pre>
<p>We use a helper method, <kbd>ConvertToScreenPosition</kbd>, to convert each circle's center point from the image space to the screen space. We also convert its diameter:</p>
<pre>                // Convert circles' image coordinates to<br/>                // screen coordinates.<br/>                Vector2 screenPosition =<br/>                        ConvertToScreenPosition(<br/>                                houghCirclesArray[i],<br/>                                houghCirclesArray[i + 1]);<br/>                float screenDiameter =<br/>                        houghCirclesArray[i + 2] *<br/>                        screenPixelsPerImagePixel;</pre>
<p>We use another helper method, <kbd>ConvertToWorldPosition</kbd>, to convert the circle's center point from the screen space to the world space. We also convert its diameter. Having done our conversions, we instantiate a <kbd>Circle</kbd> and add it to the list. Here is the code, which completes the <kbd>UpdateCircles</kbd> method:</p>
<pre>                // Convert screen coordinates to world<br/>                // coordinates based on raycasting.<br/>                Vector3 worldPosition =<br/>                        ConvertToWorldPosition(<br/>                                screenPosition);<br/><br/>                Circle circle = new Circle(<br/>                        screenPosition, screenDiameter,<br/>                        worldPosition);<br/>                circles.Add(circle);<br/>            }<br/>        }</pre>
<p>Our <kbd>UpdateLines</kbd> helper method begins by performing probabilistic Hough line detection with step sizes of one pixel and one degree. For each line, we require at least <kbd>50</kbd> detected intersections with edge pixels, a length of at least <kbd>50</kbd> pixels, and no gaps of more than <kbd>10.0</kbd> pixels. We clear the list of any previously detected lines. Then, we iterate over the results of the Hough line detection. Here is the first part of the method's implementation:</p>
<pre>        void UpdateLines() {<br/><br/>            // Detect lines.<br/>            Imgproc.HoughLinesP(cannyMat, houghLines, 1.0,<br/>                                Mathf.PI / 180.0, 50,<br/>                                50.0, 10.0);<br/><br/>            //<br/>            // Calculate the lines' screen coordinates and<br/>            // world coordinates.<br/>            //<br/><br/>            // Clear the previous coordinates.<br/>            lines.Clear();<br/><br/>            // Count the elements in the matrix of Hough lines.<br/>            // Each line should have 4 elements:<br/>            // { x_start, y_start, x_end, y_end }<br/>            int numHoughLineElems = houghLines.cols() *<br/>                                    houghLines.rows() *<br/>                                    houghLines.channels();<br/><br/>            if (numHoughLineElems == 0) {<br/>                return;<br/>            }<br/><br/>            // Convert the matrix of Hough circles to a 1D array:<br/>            // { x_start_0, y_start_0, x_end_0, y_end_0, ...,<br/>            //   x_start_n, y_start_n, x_end_n, y_end_n }<br/>            int[] houghLinesArray = new int[numHoughLineElems];<br/>            houghLines.get(0, 0, houghLinesArray);<br/><br/>            // Iterate over the lines.<br/>            for (int i = 0; i &lt; numHoughLineElems; i += 4) {</pre>
<p>We use our <kbd>ConvertToScreenPosition</kbd> helper method to convert each line's endpoints from the image space to the screen space:</p>
<pre>                // Convert lines' image coordinates to<br/>                // screen coordinates.<br/>                Vector2 screenPoint0 =<br/>                        ConvertToScreenPosition(<br/>                                houghLinesArray[i],<br/>                                houghLinesArray[i + 1]);<br/>                Vector2 screenPoint1 =<br/>                        ConvertToScreenPosition(<br/>                                houghLinesArray[i + 2],<br/>                                houghLinesArray[i + 3]);</pre>
<p>Similarly, we use our <kbd>ConvertToWorldPosition</kbd> helper method to convert the line's endpoints from the screen space to the world space. Having done our conversions, we instantiate a <kbd>Line</kbd> and add it to the list. Here is the code, which completes the <kbd>UpdateLines</kbd> method:</p>
<pre>                // Convert screen coordinates to world<br/>                // coordinates based on raycasting.<br/>                Vector3 worldPoint0 =<br/>                        ConvertToWorldPosition(<br/>                                screenPoint0);<br/>                Vector3 worldPoint1 =<br/>                        ConvertToWorldPosition(<br/>                                screenPoint1);<br/><br/>                Line line = new Line(<br/>                        screenPoint0, screenPoint1,<br/>                        worldPoint0, worldPoint1);<br/>                lines.Add(line);<br/>            }<br/>        }</pre>
<p>Our <kbd>ConvertToScreenPosition</kbd> helper method takes into account the fact that our screen coordinates are in portrait format, whereas our image coordinates are in landscape format. The conversion from image space to screen space is implemented as follows:</p>
<pre>        Vector2 ConvertToScreenPosition(float imageX,<br/>                                        float imageY) {<br/>            float screenX = screenWidth - imageY *<br/>                            screenPixelsPerImagePixel;<br/>            float screenY = screenHeight - imageX *<br/>                            screenPixelsPerImagePixel -<br/>                            screenPixelsYOffset;<br/>            return new Vector2(screenX, screenY);<br/>        }</pre>
<p>Our <kbd>ConvertToWorldPosition</kbd> helper method uses Unity's built-in raycasting functionality and our specified target distance, <kbd>raycastDistance</kbd>, to convert the given two-dimensional screen coordinates into three-dimensional world coordinates:</p>
<pre>        Vector3 ConvertToWorldPosition(<br/>                Vector2 screenPosition) {<br/>            Ray ray = _camera.ScreenPointToRay(<br/>                    screenPosition);<br/>            return ray.GetPoint(raycastDistance);<br/>        }</pre>
<p>We implement the standard <kbd>OnPostRender</kbd> callback by checking whether any simulated balls or lines are present, and, if not, by calling a helper method, <kbd>DrawPreview</kbd>. Here is the code:</p>
<pre>        void OnPostRender() {<br/>            if (!simulating) {<br/>                DrawPreview();<br/>            }<br/>        }</pre>
<p>The <kbd>DrawPreview</kbd> helper method serves to show the positions and dimensions of detected circles and lines, if any. To avoid unnecessary draw calls, the method returns early if there are no objects to draw, as seen in the following code:</p>
<pre>        void DrawPreview() {<br/><br/>            // Draw 2D representations of the detected<br/>            // circles and lines, if any.<br/><br/>            int numCircles = circles.Count;<br/>            int numLines = lines.Count;<br/>            if (numCircles &lt; 1 &amp;&amp; numLines &lt; 1) {<br/>                return;<br/>            }</pre>
<p>Having determined that there are detected shapes to draw, the method proceeds by configuring the OpenGL context to draw in the screen space by using <kbd>drawPreviewMaterial</kbd>. This setup is seen in the following code:</p>
<pre>            GL.PushMatrix();<br/>            if (drawPreviewMaterial != null) {<br/>                drawPreviewMaterial.SetPass(0);<br/>            }<br/>            GL.LoadPixelMatrix();</pre>
<p>If there are any detected circles, we do one draw call to highlight them all. Specifically, we tell OpenGL to begin drawing quads, we feed it the screen coordinates of squares that approximate the circles, and then we tell it to stop drawing quads. Here is the code:</p>
<pre>            if (numCircles &gt; 0) {<br/>                // Draw the circles.<br/>                GL.Begin(GL.QUADS);<br/>                for (int i = 0; i &lt; numCircles; i++) {<br/>                    Circle circle = circles[i];<br/>                    float centerX =<br/>                            circle.screenPosition.x;<br/>                    float centerY =<br/>                            circle.screenPosition.y;<br/>                    float radius =<br/>                            0.5f * circle.screenDiameter;<br/>                    float minX = centerX - radius;<br/>                    float maxX = centerX + radius;<br/>                    float minY = centerY - radius;<br/>                    float maxY = centerY + radius;<br/>                    GL.Vertex3(minX, minY, 0f);<br/>                    GL.Vertex3(minX, maxY, 0f);<br/>                    GL.Vertex3(maxX, maxY, 0f);<br/>                    GL.Vertex3(maxX, minY, 0f);<br/>                }<br/>                GL.End();<br/>            }</pre>
<p>Similarly, if there are any detected lines, we do one draw call to highlight them all. Specifically, we tell OpenGL to begin drawing lines, we feed it the lines' screen coordinates, and then we tell it to stop drawing lines. Here is the code, which completes the <kbd>DrawPreview</kbd> method:</p>
<pre>            if (numLines &gt; 0) {<br/>                // Draw the lines.<br/>                GL.Begin(GL.LINES);<br/>                for (int i = 0; i &lt; numLines; i++) {<br/>                    Line line = lines[i];<br/>                    GL.Vertex(line.screenPoint0);<br/>                    GL.Vertex(line.screenPoint1);<br/>                }<br/>                GL.End();<br/>            }<br/><br/>            GL.PopMatrix();<br/>        }</pre>
<p>We implement the standard <kbd>OnGUI</kbd> callback by drawing a button. Depending on whether simulated balls and lines are already present, the button says either <span class="packt_screen">Stop Simulation</span> or <span class="packt_screen">Start Simulation</span>. (However, if there are no simulated balls or lines, and there are also no detected balls or lines, then the button is not shown at all.) When the button is clicked, a helper method is called (either <kbd>StopSimulation</kbd> or <kbd>StartSimulation</kbd>). Here is the code for <kbd>OnGUI</kbd>:</p>
<pre>        void OnGUI() {<br/>            GUI.skin.button.fontSize = buttonFontSize;<br/>            if (simulating) {<br/>                if (GUI.Button(buttonRect,<br/>                               "Stop Simulation")) {<br/>                    StopSimulation();<br/>                }<br/>            } else if (circles.Count &gt; 0 || lines.Count &gt; 0) {<br/>                if (GUI.Button(buttonRect,<br/>                               "Start Simulation")) {<br/>                    StartSimulation();<br/>                }<br/>            }<br/>        }</pre>
<p>The <kbd>StartSimulation</kbd> helper method begins by pausing the video feed and placing copies of the <kbd>simulatedCirclePrefab</kbd> atop the detected circles. Each instance is scaled to match a detected circle's diameter. Here is the first part of the method:</p>
<pre>        void StartSimulation() {<br/><br/>            // Freeze the video background<br/>            webCamTexture.Pause();<br/><br/>            // Create the circles' representation in the<br/>            // physics simulation.<br/>            int numCircles = circles.Count;<br/>            for (int i = 0; i &lt; numCircles; i++) {<br/>                Circle circle = circles[i];<br/>                GameObject simulatedCircle =<br/>                        (GameObject)Instantiate(<br/>                                simulatedCirclePrefab);<br/>                Transform simulatedCircleTransform =<br/>                        simulatedCircle.transform;<br/>                simulatedCircleTransform.position =<br/>                        circle.worldPosition;<br/>                simulatedCircleTransform.localScale =<br/>                        circle.screenDiameter *<br/>                        Vector3.one;<br/>                simulatedObjects.Add(simulatedCircle);<br/>            }</pre>
<p>The method finishes by placing copies of <kbd>simulatedLinePrefab</kbd> atop the detected lines. Each instance is scaled to match a detected line's length. Here is the rest of the method:</p>
<pre>            // Create the lines' representation in the<br/>            // physics simulation.<br/>            int numLines = lines.Count;<br/>            for (int i = 0; i &lt; numLines; i++) {<br/>                Line line = lines[i];<br/>                GameObject simulatedLine =<br/>                        (GameObject)Instantiate(<br/>                                simulatedLinePrefab);<br/>                Transform simulatedLineTransform =<br/>                        simulatedLine.transform;<br/>                float angle = -Vector2.Angle(<br/>                        Vector2.right, line.screenPoint1 -<br/>                                line.screenPoint0);<br/>                Vector3 worldPoint0 = line.worldPoint0;<br/>                Vector3 worldPoint1 = line.worldPoint1;<br/>                simulatedLineTransform.position =<br/>                        0.5f * (worldPoint0 + worldPoint1);<br/>                simulatedLineTransform.eulerAngles =<br/>                        new Vector3(0f, 0f, angle);<br/>                simulatedLineTransform.localScale =<br/>                        new Vector3(<br/>                                Vector3.Distance(<br/>                                        worldPoint0,<br/>                                        worldPoint1),<br/>                                lineThickness,<br/>                                lineThickness);<br/>                simulatedObjects.Add(simulatedLine);<br/>            }<br/>        }</pre>
<p>The <kbd>StopSimulation</kbd> helper method simply serves to resume the video feed, delete all simulated balls and lines, and clear the list that contained these simulated objects. With the list empty, the conditions for the detectors to run (in the <kbd>Update</kbd> method) are fulfilled again. <kbd>StopSimulation</kbd> is implemented like this:</p>
<pre>        void StopSimulation() {<br/><br/>            // Unfreeze the video background.<br/>            webCamTexture.Play();<br/><br/>            // Destroy all objects in the physics simulation.<br/>            int numSimulatedObjects =<br/>                    simulatedObjects.Count;<br/>            for (int i = 0; i &lt; numSimulatedObjects; i++) {<br/>                GameObject simulatedObject =<br/>                        simulatedObjects[i];<br/>                Destroy(simulatedObject);<br/>            }<br/>            simulatedObjects.Clear();<br/>        }</pre>
<p>When the script's instance is destroyed (at the end of the scene), we ensure that the webcam and gyroscope are released, as seen in the following code:</p>
<pre>        void OnDestroy() {<br/>            if (webCamTexture != null) {<br/>                webCamTexture.Stop();<br/>            }<br/>            if (gyro != null) {<br/>                gyro.enabled = false;<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>Save the script and drag it from the <span class="packt_screen">Project</span> pane to the <span class="packt_screen">Main Camera</span> object in the <span class="packt_screen">Hierarchy</span>. Click on the <span class="packt_screen">Main Camera</span> object, and, in the <span class="packt_screen">Detect And Simulate (Script)</span> section of its <span class="packt_screen">Inspector</span>, drag the following objects to the following fields:</p>
<ul>
<li>Drag <kbd>VideoRenderer</kbd> (from the <span class="packt_screen">Hierarchy</span>) to the <span class="packt_screen">Video Renderer</span> field (in the <span class="packt_screen">Inspector</span>)</li>
<li>Drag <kbd>DrawSolidRed</kbd> (from <kbd>Rollingball/Materials</kbd> in the <span class="packt_screen">Project</span> pane) to the <span class="packt_screen">Draw Preview Material</span> field (in the <span class="packt_screen">Inspector</span>)</li>
<li>Drag <kbd>SimulatedCircle</kbd> (from <kbd>Rollingball/Prefabs</kbd> in the <span class="packt_screen">Project</span> pane) to the <span class="packt_screen">Simulated Circle Prefab</span> field (in the <span class="packt_screen">Inspector</span>)</li>
<li>Drag <kbd>SimulatedLine</kbd> (from <kbd>Rollingball/Prefabs</kbd> in the <span class="packt_screen">Project</span> pane) to the <span class="packt_screen">Simulated Line Prefab</span> field (in the <span class="packt_screen">Inspector</span>)</li>
</ul>
<p>After these changes, the script's section in the <span class="packt_screen">Inspector</span> should look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bcfcbe15-3f81-4b7a-8c3a-cc50e7236ee6.png" style="width:22.58em;height:17.83em;"/></p>
<p>Our main scene is complete! Now, we need a simple launcher scene that is responsible for obtaining the user's permission to access the camera and for launching the main scene.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the launcher scene in Unity</h1>
                </header>
            
            <article>
                
<p>Our <kbd>Rollingball</kbd> scene, and specifically the <kbd>DetectAndSimulate</kbd> script, attempts to access a camera through Unity's <kbd>WebCamDevice</kbd> and <kbd>WebCamTexture</kbd> classes. Unity is somewhat smart about camera permissions on Android. At the start of the <kbd>Rollingball</kbd> scene (or any scene that requires camera access), Unity will automatically check whether the user has granted permission for camera access; if not, Unity will request permission. Unfortunately, this automatic request comes too late for <kbd>DetectAndSimulate</kbd> to properly access the camera in its <kbd>Start</kbd> and <kbd>Init</kbd> methods. To avoid this kind of problem, it is better to write a launcher scene with a script that explicitly requests camera access.</p>
<p>Create a new scene and save it as <kbd>Launcher</kbd> in the <kbd>Rollingball/Scenes</kbd> folder. Delete the <span class="packt_screen">Directional Light</span> from the scene. Add an empty object and name it <kbd>Launcher</kbd>. Now, the scene's <span class="packt_screen">Hierarchy</span> should look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/82455710-cbb2-4a73-b14b-6a3caf0fbb74.png" style="width:16.17em;height:8.25em;"/></p>
<p>Edit the <span class="packt_screen">Main Camera</span> in the <span class="packt_screen">Inspector</span> to give it a solid black background, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fde1c045-a75c-4933-b25a-5e2e9371555f.png" style="width:28.58em;height:35.50em;"/></p>
<p>In <span><kbd>Rollingball/Scripts</kbd>, create a new script, rename it <kbd>Launcher</kbd>, edit it, and replace its contents with the following code</span>:</p>
<pre>using UnityEngine;<br/>using UnityEngine.SceneManagement;<br/><br/>#if PLATFORM_ANDROID<br/>using UnityEngine.Android;<br/>#endif<br/><br/><br/>namespace com.nummist.rollingball {<br/><br/>    public class Launcher : MonoBehaviour {<br/><br/>        void Start() {<br/><br/>#if PLATFORM_ANDROID<br/>            if (!Permission.HasUserAuthorizedPermission(<br/>                    Permission.Camera))<br/>            {<br/>                // Ask the user's permission for camera access.<br/>                Permission.RequestUserPermission(Permission.Camera);<br/>            }<br/>#endif<br/><br/>            SceneManager.LoadScene("Rollingball");<br/>        }<br/>    }<br/>}</pre>
<p>Upon <kbd>Start</kbd>, this script checks whether the user has already granted permission to access the camera. If not, the script requests permission by showing a standard Android permission request dialog. The <kbd>Start</kbd> method finishes by loading the <kbd>Rollingball</kbd> scene that we created previously.</p>
<p><span>Save the script and drag it from the </span><span class="packt_screen">Project</span><span> pane to the </span><kbd>Launcher</kbd><span> object in the </span><span class="packt_screen">Hierarchy</span><span>. Click on the </span><kbd>Launcher</kbd> <span>object and confirm that its </span><span class="packt_screen">Inspector</span> <span>looks like the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e13afcc0-0c96-4270-949f-7be5ebd10dd5.png" style="width:26.33em;height:11.92em;"/></p>
<p><span>Our launcher scene is complete. All that remains is to configure, build, and test our project.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tidying up and testing</h1>
                </header>
            
            <article>
                
<p>Let's return to the <span class="packt_screen">Build Settings</span> window (<span class="packt_screen">File | </span><span class="packt_screen">Build Settings...</span>). We no longer want the OpenCV for Unity demos in our build. Remove them by either unchecking them or selecting and deleting them (<em><span class="KeyPACKT">Delete</span></em> on Windows or <em><span class="KeyPACKT">Cmd</span></em> + <em><span class="KeyPACKT">Del</span></em> on Mac). Then, add the <kbd>Launcher</kbd> and <kbd>Rollingball</kbd> scenes by dragging them from the <span class="packt_screen">Project</span> pane into the <span class="packt_screen">Scenes In Build</span> list. When you are finished, the <span class="packt_screen">Build Settings</span> window should look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/49fa420d-dffc-41e7-bd5e-0c81926a40c8.png" style="width:55.42em;height:52.50em;"/></p>
<p><span>Click on the</span> <span class="packt_screen">Build and Run</span> <span>button, overwrite any previous builds, and let the good times roll!</span></p>
<div class="packt_infobox">If you are building for iOS, remember to follow the additional instructions in <kbd>OpenCVForUnity/ReadMe.pdf</kbd>. Particularly, ensure that the project's <span class="packt_screen">Camera Usage Description</span> is set to a helpful descriptive string, for example <kbd>Rollingball</kbd> uses the camera to detect circles and lines (obviously!) and that the <span class="packt_screen">Target minimum iOS Version</span> is set to <kbd>8.0</kbd>.</div>
<p>Test the app by drawing and scanning dots and lines of various sizes, with various styles of pen strokes. Also, try scanning some things that are not drawings. Feel free to go back to the code, edit the detectors' parameters, rebuild, and see how the sensitivity has changed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter has really rounded out our experience and has drawn a line under our accomplishments. You have learned how to detect primitive shapes by using the Hough transform. We have also used OpenCV and Unity together to turn a pen and paper drawing into a physics toy. We have even surpassed the things that Q can make a pen do!</p>
<p>Still, a secret agent cannot solve all problems with ink and paper alone. Next, we will take off our reading glasses, put down our physics simulations, and consider ways of deconstructing real motion in the world around us. Prepare to look through the kaleidoscope of the frequency domain!</p>


            </article>

            
        </section>
    </body></html>