<html><head></head><body>
		<div id="_idContainer119">
			<h1 class="chapter-number"><a id="_idTextAnchor143"/>12</h1>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor144"/>Designing Machine Learning Pipelines (MLOps) and  Their Testing</h1>
			<p>MLOps, short for machine learning (ML) operations, is a set of practices and techniques aimed at streamlining the deployment, management, and monitoring of ML models in production environments. It borrows concepts from the DevOps (development and operations) approach, adapting them to the unique challenges posed <span class="No-Break">by ML.</span></p>
			<p>The main goal of MLOps is to bridge the gap between data science and operations teams, fostering collaboration and ensuring that ML projects can be effectively and reliably deployed at scale. MLOps helps to automate and optimize the entire ML life cycle, from model development to deployment and maintenance, thus improving the efficiency and effectiveness of ML systems <span class="No-Break">in production.</span></p>
			<p>In this chapter, we learn how ML systems are designed and operated in practice. The chapter shows how pipelines are turned into a software system, with a focus on testing ML pipelines and their deployment at <span class="No-Break">Hugging Face.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>What ML <span class="No-Break">pipelines are</span></li>
				<li>ML pipelines – how to use ML in the system <span class="No-Break">in practice</span></li>
				<li>Raw <span class="No-Break">data-based pipelines</span></li>
				<li><span class="No-Break">Feature-based pipelines</span></li>
				<li>Testing of <span class="No-Break">ML pipelines</span></li>
				<li>Monitoring ML systems <span class="No-Break">at runtime</span></li>
			</ul>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor145"/>What ML pipelines are</h1>
			<p>Undoubtedly, in recent years, the field of ML has witnessed remarkable advancements, revolutionizing industries and empowering innovative applications. As the demand for more sophisticated and accurate models grows, so does the complexity of developing and deploying them <a id="_idIndexMarker465"/>effectively. The industrial introduction of ML systems called for more rigorous testing and validation of these ML-based systems. In response to these challenges, the concept of ML pipelines has emerged as a crucial framework to streamline the entire ML development process, from data preprocessing and feature engineering to model training and deployment. This chapter explores the applications of MLOps in<a id="_idIndexMarker466"/> the context of both cutting-edge <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) models such as <strong class="bold">Generative Pre-trained Transformer</strong> (<strong class="bold">GPT</strong>) and traditional <a id="_idIndexMarker467"/>classical <span class="No-Break">ML models.</span></p>
			<p>We begin by exploring the underlying concepts of ML pipelines, stressing their importance in organizing the ML workflow and promoting collaboration between data scientists and engineers. We synthesize a lot of knowledge presented in the previous chapters – data quality assessment, model inference, <span class="No-Break">and monitoring.</span></p>
			<p>Next, we discuss the unique characteristics and considerations involved in building pipelines for GPT models and similar, leveraging their pre-trained nature to tackle a wide range of language tasks. We explore the intricacies of fine-tuning GPT models on domain-specific data and the challenges of incorporating them into <span class="No-Break">production systems.</span></p>
			<p>Following the exploration of GPT pipelines, we shift our focus to classical ML models, examining the feature engineering process and its role in extracting relevant information from raw data. We delve into the diverse landscape of traditional ML algorithms, understanding when to use each approach, and their trade-offs in <span class="No-Break">different scenarios.</span></p>
			<p>Finally, we show how to test ML pipelines, and we emphasize the significance of model evaluation and validation in assessing performance and ensuring robustness in production environments. Additionally, we examine strategies for model monitoring and maintenance, safeguarding against concept drift and guaranteeing continuous <span class="No-Break">performance improvement.</span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor146"/>ML pipelines</h2>
			<p>An ML pipeline is a systematic and automated process that organizes the various stages of an ML workflow. It encompasses the steps involved in preparing data, training an ML model, evaluating its performance, and deploying it for use in real-world applications. The primary goal of an ML<a id="_idIndexMarker468"/> pipeline is to streamline the end-to-end ML process, making it more efficient, reproducible, <span class="No-Break">and scalable.</span></p>
			<p>An ML pipeline typically consists of the following <span class="No-Break">essential components:</span></p>
			<ul>
				<li><strong class="bold">Data collection, preprocessing, and wrangling</strong>: In this initial stage, relevant data is gathered from various sources and prepared for model training. Data preprocessing<a id="_idIndexMarker469"/> involves cleaning, transforming, and normalizing the data to ensure it is in a suitable format for the <span class="No-Break">ML algorithm.</span></li>
				<li><strong class="bold">Feature engineering and selection</strong>: Feature engineering involves selecting and creating relevant features (input variables) from the raw data that will help the model learn patterns and make accurate predictions. Proper feature selection is crucial in improving model performance and reducing <span class="No-Break">computational overhead.</span></li>
				<li><strong class="bold">Model selection and training</strong>: In this stage, one or more ML algorithms are chosen, and the model is trained on the prepared data. Model training involves learning underlying patterns and relationships in the data to make predictions <span class="No-Break">or classifications.</span></li>
				<li><strong class="bold">Model evaluation and validation</strong>: The trained model is evaluated using metrics such as accuracy, precision, recall, F1-score, and so on, to assess its performance on unseen data. Cross-validation techniques are often used to ensure the model’s <span class="No-Break">generalization capability.</span></li>
				<li><strong class="bold">Hyperparameter tuning</strong>: Many ML algorithms have hyperparameters, which are adjustable parameters that control the model’s behavior. Hyperparameter tuning involves finding the optimal values for these parameters to improve the <span class="No-Break">model’s performance.</span></li>
				<li><strong class="bold">Model deployment</strong>: Once the model has been trained and validated, it is deployed into a production environment, where it can make predictions on new, unseen data. Model deployment may involve integrating the model into existing applications <span class="No-Break">or systems.</span></li>
				<li><strong class="bold">Model monitoring and maintenance</strong>: After deployment, the model’s performance is continuously monitored to detect any issues or drift in performance. Regular maintenance may involve retraining the model with new data to ensure it <a id="_idIndexMarker470"/>remains accurate and up <span class="No-Break">to date.</span></li>
			</ul>
			<p>An ML pipeline provides a structured framework for managing the complexity of ML projects, enabling data scientists and engineers to collaborate more effectively and ensuring that models can be developed and deployed reliably and efficiently. It promotes reproducibility, scalability, and ease of experimentation, facilitating the development of high-quality ML solutions. <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.1</em> shows a conceptual model of an ML pipeline, which we introduced in <a href="B19548_02.xhtml#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer110">
					<img alt="Figure 12.1 – ML pipeline: a conceptual overview" src="image/B19548_12_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – ML pipeline: a conceptual overview</p>
			<p>We covered the elements of the blue-shaded elements in the previous chapters, and here, we focus mostly on the parts that are not covered yet. However, before we dive into the technical elements of this pipeline, let us introduce the concept <span class="No-Break">of MLOps.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor147"/>Elements of MLOps</h2>
			<p>As the main goal of MLOps is to bridge<a id="_idIndexMarker471"/> the gap between data science and operations teams, MLOps automates and optimizes the entire ML life cycle, from model development to deployment and maintenance, thus improving the efficiency and effectiveness of ML systems <span class="No-Break">in production.</span></p>
			<p>Key components and practices in <span class="No-Break">MLOps include:</span></p>
			<ul>
				<li><strong class="bold">Version control</strong>: Applying <strong class="bold">version control systems</strong> (<strong class="bold">VCSs</strong>) such as Git to manage and track changes<a id="_idIndexMarker472"/> in ML code, datasets, and model versions. This enables easy collaboration, reproducibility, and tracking of <span class="No-Break">model improvements.</span></li>
				<li><strong class="bold">Continuous integration and continuous deployment (CI/CD)</strong>: Leveraging CI/CD pipelines to <a id="_idIndexMarker473"/>automate the testing, integration, and deployment of ML models. This helps ensure that changes to the code base are seamlessly deployed to production while maintaining <span class="No-Break">high-quality standards.</span></li>
				<li><strong class="bold">Model packaging</strong>: Creating standardized, reproducible, and shareable containers or packages for ML <a id="_idIndexMarker474"/>models, making it easier to deploy them across different <span class="No-Break">environments consistently.</span></li>
				<li><strong class="bold">Model monitoring</strong>: Implementing monitoring and logging solutions to keep track of the model’s performance and behavior in real time. This helps detect issues early and ensure the model’s <span class="No-Break">ongoing reliability.</span></li>
				<li><strong class="bold">Scalability and infrastructure management</strong>: Designing and managing the underlying infrastructure to support the demands of the ML models in production, ensuring they can handle increased workloads and <span class="No-Break">scale efficiently.</span></li>
				<li><strong class="bold">Model governance and compliance</strong>: Implementing processes and tools to ensure compliance with legal and ethical requirements, privacy regulations, and company policies when deploying and using <span class="No-Break">ML models.</span></li>
				<li><strong class="bold">Collaboration and communication</strong>: Facilitating effective communication and collaboration between data scientists, engineers, and other stakeholders involved in the ML <span class="No-Break">deployment process.</span></li>
			</ul>
			<p>By adopting MLOps principles, organizations can accelerate the development and deployment of ML models while maintaining their reliability and effectiveness in real-world applications. It also helps reduce the risk of deployment failures and promotes a culture of collaboration and continuous improvement within data science and <span class="No-Break">operations teams.</span></p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor148"/>ML pipelines – how to use ML in the system in practice</h1>
			<p>Training and validating ML models <a id="_idIndexMarker475"/>on a local platform is the beginning of the process of using an ML pipeline. After all, it would be of limited use if we had to retrain the ML models on every computer from <span class="No-Break">our customers.</span></p>
			<p>Therefore, we often deploy ML models to a model repository. There are a few popular ones, but the one that is used by the largest community is the HuggingFace repository. In that repository, we can deploy both the models and datasets and even create spaces where the models can be used for experiments without the need to download them. Let us deploy the model trained in <a href="B19548_11.xhtml#_idTextAnchor132"><span class="No-Break"><em class="italic">Chapter 11</em></span></a> to that repository. For that, we need to have an account at huggingface.com, and then we <span class="No-Break">can start.</span></p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor149"/>Deploying models to HuggingFace</h2>
			<p>First, we need to create a<a id="_idIndexMarker476"/> new model using the <strong class="bold">New</strong> button<a id="_idIndexMarker477"/> on the main page, as in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer111">
					<img alt="Figure 12.2 – New button to create a model" src="image/B19548_12_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – New button to create a model</p>
			<p>Then, we fill in information about our model to create space for it. <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.3</em> presents a screenshot of this process. In the form, we fill in the name of the model, whether it should be private or public, and we choose a license for it. In this example, we go with the MIT License, which is very permissive and allows everyone to use, reuse, and redistribute the model as long as they include the MIT License text along <span class="No-Break">with it:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer112">
					<img alt="Figure 12.3 – Model metadata card" src="image/B19548_12_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Model metadata card</p>
			<p>Once the model has been created, we get a <a id="_idIndexMarker478"/>space where we can start deploying the model. The empty space<a id="_idIndexMarker479"/> looks like the one in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer113">
					<img alt="Figure 12.4 – Empty model space" src="image/B19548_12_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Empty model space</p>
			<p>The top menu contains four options, but the first two are the most important ones – <strong class="bold">Model card</strong> and <strong class="bold">Files and versions</strong>. The model card is a short description of the model. It can contain any kind of information, but the most common information is how to use the model. We follow this convention and prepare the model card as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer114">
					<img alt="Figure 12.5 – The beginning of the model card for our wolfBERTa model" src="image/B19548_12_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – The beginning of the model card for our wolfBERTa model</p>
			<p class="callout-heading">Best practice #60</p>
			<p class="callout">The model card should contain information about how the model was trained, how to use it, which tasks it supports, and how to reference <span class="No-Break">the model.</span></p>
			<p>Since HuggingFace is a community, it is<a id="_idIndexMarker480"/> important to properly document models created and provide information on how the models were trained and what they can do. Therefore, my best <a id="_idIndexMarker481"/>practice is to include all that information in the model card. Many models include also information about how to contact the authors and whether the models had been pre-trained before they <span class="No-Break">were trained.</span></p>
			<p>Once the model card is ready, we can move to the <strong class="bold">Files and versions</strong> section of the model space. In that space, we can see files that have been created so far (that is, <strong class="source-inline">Readme.txt</strong> – the model card), and we can add actual model files (see <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer115">
					<img alt="Figure 12.6 – Files and versions of models; we can add a model by using the Add file button in the top right-hand corner" src="image/B19548_12_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Files and versions of models; we can add a model by using the Add file button in the top right-hand corner</p>
			<p>Once we click on the <strong class="bold">Add file</strong> button, we can add model files. We find the model files in the same<a id="_idIndexMarker482"/> repository that we used in <a href="B19548_11.xhtml#_idTextAnchor132"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, in the <strong class="source-inline">wolfBERTa</strong> subfolder. That folder contains the <span class="No-Break">following </span><span class="No-Break"><a id="_idIndexMarker483"/></span><span class="No-Break">files:</span></p>
			<pre class="console">
Mode                 LastWriteTime                     Name
------        --------------------        -----------------
d----l        2023-07-01     10:25        checkpoint-340000
d----l        2023-07-01     10:25        checkpoint-350000
-a---l        2023-06-27     21:30        config.json
-a---l        2023-06-27     17:55        merges.txt
-a---l        2023-06-27     21:30        pytorch_model.bin
-a---l        2023-06-27     21:30        training_args.bin
-a---l        2023-06-27     17:55        vocab.json</pre>			<p>The first two entries are the model checkpoints; that is, the versions of the model saved during our training process. These two folders are not important for the deployment, and therefore they will be ignored. The rest of the files should be copied to the newly created model repository <span class="No-Break">at HuggingFace.</span></p>
			<p>The model, after uploading, should look something like the one presented in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer116">
					<img alt="Figure 12.7 – Model uploaded to the HuggingFace repository" src="image/B19548_12_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Model uploaded to the HuggingFace repository</p>
			<p>After this, the model is ready <a id="_idIndexMarker484"/>to be used by the community. What we can also do is create an inference API for the community to quickly test our models. It is provided to us automatically once<a id="_idIndexMarker485"/> we go back to the <strong class="bold">Model card</strong> menu, under the <strong class="bold">Hosted inference API</strong> section (right-hand side of <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer117">
					<img alt="Figure 12.8 – Hosted inference API provided automatically for our model" src="image/B19548_12_8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – Hosted inference API provided automatically for our model</p>
			<p>When we input <strong class="source-inline">int HTTP_get(&lt;mask&gt;)</strong>, we ask the model to provide the input parameter for that function. The results show that the most probable token is <strong class="source-inline">void</strong> and the second in line is the <strong class="source-inline">int</strong> token. Both are relevant as they are types used in parameters, but they are probably not going to make this program compile, so we would need to develop a loop that would predict more than just one token for the program. It probably needs a bit more training <span class="No-Break">as well.</span></p>
			<p>Now, we have a fully <a id="_idIndexMarker486"/>deployed model that can be used in <a id="_idIndexMarker487"/>other applications without <span class="No-Break">much hassle.</span></p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor150"/>Downloading models from HuggingFace</h2>
			<p>We have already seen how to download a model from HuggingFace, but for the sake of completeness, let’s see how this is done for the <strong class="source-inline">wolfBERTa</strong> model. Essentially, we follow the model card and use<a id="_idIndexMarker488"/> the following Python <a id="_idIndexMarker489"/><span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
from transformers import pipeline
unmasker = pipeline('fill-mask', model='mstaron/wolfBERTa')
unmasker("Hello I'm a &lt;mask&gt; model.")</pre>			<p>This code fragment downloads the model and uses an <strong class="source-inline">unmasker</strong> interface to make an inference using the <strong class="source-inline">fill-mask</strong> pipeline. The pipeline allows you to input a sentence with a <strong class="source-inline">&lt;mask&gt;</strong> masked token, and the model will attempt to predict the most suitable word to fill in the masked position. The three lines of this code fragment do <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">from transformers import pipeline</strong>: This line imports the pipeline function from the <strong class="source-inline">transformer</strong>s library. The pipeline function simplifies the process of using pre-trained models<a id="_idIndexMarker490"/> for various <strong class="bold">natural language processing</strong> (<span class="No-Break"><strong class="bold">NLP</strong></span><span class="No-Break">) tasks.</span></li>
				<li><strong class="source-inline">unmasker = pipeline('fill-mask', model='mstaron/wolfBERTa')</strong>: This line creates a new pipeline named <strong class="source-inline">unmasker</strong> for the task. The pipeline will use the pre-trained <span class="No-Break"><strong class="source-inline">wolfBERTa</strong></span><span class="No-Break"> model.</span></li>
				<li><strong class="source-inline">unmasker("Hello I'm a &lt;mask&gt; model.")</strong>: This line utilizes the <strong class="source-inline">unmasker</strong> pipeline to predict the word that best fits the masked position in the given sentence. The <strong class="source-inline">&lt;mask&gt;</strong> token indicates the position where the model should try to fill in <span class="No-Break">a word.</span></li>
			</ul>
			<p>When this line is executed, the pipeline will call the <strong class="source-inline">wolfBERTa</strong> model, which will make predictions based on the provided sentence. The model will predict the word that it finds to best complete the <a id="_idIndexMarker491"/>sentence in the position of the <strong class="source-inline">&lt;</strong><span class="No-Break"><strong class="source-inline">mask&gt;</strong></span><span class="No-Break"> token.</span></p>
			<p>One can use other models in a very similar way. The main advantage of a community model hub such as HuggingFace is that it provides a great way to uniformly manage models and pipelines and<a id="_idIndexMarker492"/> allows us to quickly exchange models in <span class="No-Break">software products.</span></p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor151"/>Raw data-based pipelines</h1>
			<p>Creating a full pipeline can be a daunting task and requires creating customized tools for all models and all kinds of data. It allows us to optimize how we use the models, but it requires a lot of<a id="_idIndexMarker493"/> effort. The main rationale behind pipelines is that they link two areas of ML – the model and its computational capabilities with the task and the data from the domain. Luckily for us, the main model hubs such as HuggingFace have an API that provides ML pipelines automatically. Pipelines in HuggingFace are related to the model and provided by the framework based on the model’s architecture, input, <span class="No-Break">and output.</span></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor152"/>Pipelines for NLP-related tasks</h2>
			<p>Text classification is a pipeline designed to classify text input into predefined categories or classes. It’s particularly <a id="_idIndexMarker494"/>useful for tasks such as <strong class="bold">sentiment analysis</strong> (<strong class="bold">SA</strong>), topic <a id="_idIndexMarker495"/>categorization, spam detection, intent recognition, and so on. The pipeline typically employs pre-trained models fine-tuned on specific datasets for different classification tasks. We have seen similar capabilities in <em class="italic">Part I</em> of this book when we used ML for SA of <span class="No-Break">code reviews.</span></p>
			<p>An example is presented in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
from transformers import pipeline
# Load the text classification pipeline
classifier = pipeline("text-classification")
# Classify a sample text
result = classifier("This movie is amazing and highly recommended!")
print(result)</pre>			<p>The code fragment shows that there are essentially two lines of code (in boldface) that we need to instantiate the pipeline, as we’ve also <span class="No-Break">seen before.</span></p>
			<p>Text generation is another pipeline that allows the generating of text using pre-trained language models, such as GPT-3, based on a provided prompt or seed text. It’s capable of generating human-like text for various <a id="_idIndexMarker496"/>applications, such as chatbots, creative writing, <strong class="bold">question answering</strong> (<strong class="bold">QA</strong>), <span class="No-Break">and more.</span></p>
			<p>An example is presented<a id="_idIndexMarker497"/> in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
from transformers import pipeline
# Load the text generation pipeline
generator = pipeline("text-generation")
# Generate text based on a prompt
prompt = "In a galaxy far, far away… "
result = generator(prompt, max_length=50, num_return_sequences=3)
for output in result:
    print(output['generated_text'])</pre>			<p>Summarization is a pipeline designed to summarize longer texts into shorter, coherent summaries. It utilizes transformer-based models that have been trained on large datasets with a focus on the summarization task.  The pipeline is exemplified in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
from transformers import pipeline
# Load the summarization pipeline
summarizer = pipeline("summarization")
# Summarize a long article
article = """
In a groundbreaking discovery, scientists have found a new species of dinosaur in South America. The dinosaur, named "Titanus maximus," is estimated to have been the largest terrestrial creature to ever walk the Earth. It belonged to the sauropod group of dinosaurs, known for their long necks and tails. The discovery sheds new light on the diversity of dinosaurs that once inhabited our planet.
"""
result = summarizer(article, max_length=100, min_length=30, do_sample=False)
print(result[0]['summary_text'])</pre>			<p>There are more pipelines<a id="_idIndexMarker498"/> in the HuggingFace <strong class="source-inline">transformers</strong> API, so I encourage you to take a look at these pipelines. However, my best practice related to pipelines <span class="No-Break">is this:</span></p>
			<p class="callout-heading">Best practice #61</p>
			<p class="callout">Experiment with different models to find the <span class="No-Break">best pipeline.</span></p>
			<p>Since the API provides the same pipeline for similar models, changing the model or its version is quite simple. Therefore, we can create a product based on a model that has similar (but not the same) capabilities as the model that we use and simultaneously train <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor153"/>Pipelines for images</h2>
			<p>Pipelines for image processing are designed specifically for tasks related to image processing. The HuggingFace hub contains several of these pipelines, with the following ones being the <span class="No-Break">most popular.</span></p>
			<p>Image classification is designed <a id="_idIndexMarker499"/>specifically to classify an image to a specific class. It is the same kind of task as is probably the most widely known – classifying an image to be “cat”, “dog”, or “car”. The following code example (from the HuggingFace tutorial) shows the usage of an image <span class="No-Break">classification pipeline:</span></p>
			<pre class="source-code">
from transformers import pipeline
# first, create an instance of the image classification pipeline for the selected model
classifier = pipeline(model="microsoft/beit-base-patch16-224-pt22k-ft22k")
# now, use the pipeline to classify an image
classifier("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")</pre>			<p>The preceding code fragment shows that an image classification pipeline is created equally easily (if not easier) as pipelines for text <span class="No-Break">analysis tasks.</span></p>
			<p>An image segmentation pipeline is used when we want to add a so-called semantic map to an image (see <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer118">
					<img alt="Figure 12.9 – Semantic map of an image, the same as we saw in Chapter 3" src="image/B19548_12_9.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Semantic map of an image, the same as we saw in <a href="B19548_03.xhtml#_idTextAnchor038"><em class="italic">Chapter 3</em></a></p>
			<p>An example code fragment that <a id="_idIndexMarker500"/>contains such a pipeline is presented next (again, from the <span class="No-Break">HuggingFace tutorial):</span></p>
			<pre class="source-code">
from transformers import pipeline
segmenter = pipeline(model="facebook/detr-resnet-50-panoptic")
segments = segmenter("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
segments[0]["label"]</pre>			<p>The preceding code fragment creates an image segmentation pipeline, uses it, and stores the results in a <strong class="source-inline">segments</strong> list. The last line of the list prints the label of the first segment. Using the <strong class="source-inline">segments[0]["mask"].size</strong> statement, we can receive the size of the image map <span class="No-Break">in pixels.</span></p>
			<p>An object detection pipeline is used for tasks that require the recognition of objects of a predefined class in the image. We have seen an example of this task in <a href="B19548_03.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> already. The code for this kind of pipeline looks very similar to the <span class="No-Break">previous ones:</span></p>
			<pre class="source-code">
from transformers import pipeline
detector = pipeline(model="facebook/detr-resnet-50")
detector("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")</pre>			<p>Executing this code creates a list of bounding boxes of objects detected in the image, together with its bounding <a id="_idIndexMarker501"/>boxes. My best practices related to the use of pipelines for images are the same as for <span class="No-Break">language tasks.</span></p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor154"/>Feature-based pipelines</h1>
			<p>Feature-based pipelines do not have specific classes because they are much lower level. They are the <strong class="source-inline">model.fit()</strong> and <strong class="source-inline">model.predict()</strong> statements from the standard Python ML implementation. These pipelines require software developers to prepare the data manually and also to take care of the results manually; that is, by implementing preprocessing steps<a id="_idIndexMarker502"/> such as converting data to tables using one-hot encoding and post-processing steps such as converting the data into a <span class="No-Break">human-readable output.</span></p>
			<p>An example of this kind of pipeline was the prediction of defects that we have seen in the previous parts of the book; therefore, they do not need to <span class="No-Break">be repeated.</span></p>
			<p>What is important, however, is that all pipelines are the way that link the ML domain with the software engineering domain. The first activity that I do after developing a pipeline is to <span class="No-Break">test it.</span></p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor155"/>Testing of ML pipelines</h1>
			<p>Testing of ML pipelines is <a id="_idIndexMarker503"/>done at multiple levels, starting with unit tests and moving up toward integration (component) tests and then to system and acceptance tests. In these tests, two elements are important – the model itself and the data (for the model and <span class="No-Break">the oracle).</span></p>
			<p>Although we can use the unit test framework included in Python, I strongly recommend using the Pytest framework instead, due to its simplicity and flexibility. We can install this framework by simply using <span class="No-Break">this command:</span></p>
			<pre class="console">
&gt;&gt; pip install pytest</pre>			<p>That will download and install the <span class="No-Break">required packages.</span></p>
			<p class="callout-heading"> Best practice #62</p>
			<p class="callout">Use a professional testing framework such <span class="No-Break">as Pytest.</span></p>
			<p>Using a professional framework provides us with the compatibility required by MLOps principles. We can share our models, data, source code, and all other elements without the need for cumbersome setup and installation of the frameworks themselves. For Python, I recommend <a id="_idIndexMarker504"/>using the Pytest framework as it is well known, widely used, and supported by a <span class="No-Break">large community.</span></p>
			<p>Here is a code fragment that downloads a model and prepares it for <span class="No-Break">being tested:</span></p>
			<pre class="source-code">
# import json to be able to read the embedding vector for the test
import json
# import the model via the huggingface library
from transformers import AutoTokenizer, AutoModelForMaskedLM
# load the tokenizer and the model for the pretrained SingBERTa
tokenizer = AutoTokenizer.from_pretrained('mstaron/SingBERTa')
# load the model
model = AutoModelForMaskedLM.from_pretrained("mstaron/SingBERTa")
# import the feature extraction pipeline
from transformers import pipeline
# create the pipeline, which will extract the embedding vectors
# the models are already pre-defined, so we do not need to train anything here
features = pipeline(
    "feature-extraction",
    model=model,
    tokenizer=tokenizer,
    return_tensor = False
)</pre>			<p>This code snippet is used to load <a id="_idIndexMarker505"/>and set up a pre-trained language model, specifically the <strong class="source-inline">SingBERTa</strong> model, using the Hugging Face <strong class="source-inline">transformers</strong> library. It contains the <span class="No-Break">following elements:</span></p>
			<ol>
				<li>Import the necessary modules from the <span class="No-Break"><strong class="source-inline">transformers</strong></span><span class="No-Break"> library:</span><ol><li class="Alphabets"><strong class="source-inline">AutoTokenizer</strong>: This class is used to automatically select the appropriate tokenizer for the <span class="No-Break">pre-trained model.</span></li><li class="Alphabets"><strong class="source-inline">AutoModelForMaskedLM</strong>: This class is used to automatically select the appropriate<a id="_idIndexMarker506"/> model for <strong class="bold">masked language modeling</strong> (<span class="No-Break"><strong class="bold">MLM</strong></span><span class="No-Break">) tasks.</span></li></ol></li>
				<li>Load the tokenizer and model for the pre-trained <span class="No-Break"><strong class="source-inline">SingBERTa</strong></span><span class="No-Break"> model:</span><ol><li class="Alphabets"><strong class="source-inline">tokenizer = AutoTokenizer.from_pretrained('mstaron/SingBERTa')</strong>: This line loads the tokenizer for the pre-trained <strong class="source-inline">SingBERTa</strong> model from the Hugging Face <span class="No-Break">model hub.</span></li><li class="Alphabets"><strong class="source-inline">model = AutoModelForMaskedLM.from_pretrained("mstaron/SingBERTa")</strong>: This line loads the pre-trained <span class="No-Break"><strong class="source-inline">SingBERTa</strong></span><span class="No-Break"> model.</span></li></ol></li>
				<li>Import the feature <span class="No-Break">extraction pipeline:</span><ol><li class="Alphabets"><strong class="source-inline">from transformers import pipeline</strong>: This line imports the pipeline class from the <strong class="source-inline">transformers</strong> library, which allows us to easily create pipelines for various <span class="No-Break">NLP tasks.</span></li></ol></li>
				<li>Create a feature <a id="_idIndexMarker507"/><span class="No-Break">extraction pipeline:</span><ol><li class="Alphabets"><strong class="source-inline">features = pipeline("feature-extraction", model=model, tokenizer=tokenizer, return_tensor=False)</strong>: This line creates a pipeline for feature extraction. The pipeline uses the pre-trained model and tokenizer loaded earlier to extract embedding vectors from the input text. The <strong class="source-inline">return_tensor=False</strong> argument ensures that the output will be in a non-tensor format (likely NumPy arrays or <span class="No-Break">Python lists).</span></li></ol></li>
			</ol>
			<p>With this setup, you can now use the <strong class="source-inline">features</strong> pipeline to extract embedding vectors from text input using the pre-trained <strong class="source-inline">SingBERTa</strong> model without the need for any additional training. We’ve seen this model being used before, so here, let us focus on its testing. The following code fragment is a test case to check that the model has been downloaded correctly and that it is ready to <span class="No-Break">be used:</span></p>
			<pre class="source-code">
def test_features():
    # get the embeddings of the word "Test"
    lstFeatures = features("Test")
    # read the oracle from the json file
    with open('test.json', 'r') as f:
        lstEmbeddings = json.load(f)
    # assert the embeddings and the oracle are the same
    assert lstFeatures[0][0] == lstEmbeddings</pre>			<p>This code fragment defines a <strong class="source-inline">test_features()</strong> test function. The purpose of this function is to test the correctness of the feature extraction pipeline created in the previous code snippet by comparing the embeddings of the word <strong class="source-inline">"Test"</strong> obtained from the pipeline to the expected<a id="_idIndexMarker508"/> embeddings stored in a JSON file named <strong class="source-inline">'test.json'</strong>. The content of that file is our oracle, and it is a large vector of numbers that we use to compare to the actual <span class="No-Break">model output:</span></p>
			<ul>
				<li><strong class="source-inline">lstFeatures = features("Test")</strong>: This line uses the previously defined <strong class="source-inline">features</strong> pipeline to extract embeddings for the word <strong class="source-inline">"Test"</strong>. The <strong class="source-inline">features</strong> pipeline was created using the pre-trained <strong class="source-inline">SingBERTa</strong> model and tokenizer. The pipeline takes the input <strong class="source-inline">"Test"</strong>, processes it through the tokenizer, passes it through the model, and returns embedding vectors <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">lstFeatures</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">with open('test.json', 'r') as f:</strong>: This line opens the <strong class="source-inline">'test.json'</strong> file in read mode using a context manager (<span class="No-Break"><strong class="source-inline">with</strong></span><span class="No-Break"> statement).</span></li>
				<li><strong class="source-inline">lstEmbeddings = json.load(f)</strong>: This line reads the contents of the <strong class="source-inline">'test.json'</strong> file and loads its content into the <strong class="source-inline">lstEmbeddings</strong> variable. The JSON file should contain a list of embedding vectors representing the expected embeddings for the <span class="No-Break">word </span><span class="No-Break"><strong class="source-inline">"Test"</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">assert lstFeatures[0][0] == lstEmbeddings</strong>: This line performs an assertion to check if the embedding vector obtained from the pipeline (<strong class="source-inline">lstFeatures[0][0]</strong>) is equal to the expected embedding vector (oracle) from the JSON file (<strong class="source-inline">lstEmbeddings</strong>). A comparison is made by checking whether the elements at the same position in both lists are <span class="No-Break">the same.</span></li>
			</ul>
			<p>If the assertion is <strong class="source-inline">true</strong> (that is, the pipeline’s extracted embedding vector is the same as the expected vector from the JSON file), the test will pass without any output. However, if the assertion is <strong class="source-inline">false</strong> (that is, the embeddings do not match), the test framework (Pytest) marks this test case <span class="No-Break">as failed.</span></p>
			<p>In order to execute the tests, we can write the following statement in the same directory as <span class="No-Break">our project:</span></p>
			<pre class="console">
&gt;&gt; pytest</pre>			<p>In our case, this results in the following output (redacted <span class="No-Break">for brevity):</span></p>
			<pre class="console">
=================== test session starts ===================
platform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.2.0
rootdir: C:\machine_learning_best_practices\chapter_12
plugins: anyio-3.7.0
collected 1 item
chapter_12_download_model_test.py .                 [100%]
====================== 1 passed in 4.17s ==================</pre>			<p>This fragment shows that the framework found one test case (<strong class="source-inline">collected 1 item</strong>) and that it executed it. It also<a id="_idIndexMarker509"/> says that the test case passed in <span class="No-Break">4.17 seconds.</span></p>
			<p>Therefore, here comes my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #63</p>
			<p class="callout">Set up your test infrastructure based on your <span class="No-Break">training data.</span></p>
			<p>Since the models are inherently probabilistic, it is best to test the models based on the training data. Here, I do not mean that we test the performance in the sense of ML, like accuracy. I mean that we test that the models actually work. By using the same data as we used for the training, we can check whether the models’ inference is correct for the data that we used before. Therefore, I mean this as testing in the software engineering sense of <span class="No-Break">this term.</span></p>
			<p>Now, analogous to the language models presented previously, we can use a similar approach to test a classical ML model. It’s sometimes called a zero-table test. In this test, we use simple data with one data point only to test that the model’s predictions are correct. Here is how <a id="_idIndexMarker510"/>we set up such <span class="No-Break">a test:</span></p>
			<pre class="source-code">
# import the libraries pandas and joblib
import pandas as pd
import joblib
# load the model
model = joblib.load('./chapter_12_decision_tree_model.joblib')
# load the data that we used for training
dfDataAnt13 = pd.read_excel('./chapter_12.xlsx',
                            sheet_name='ant_1_3',
                            index_col=0)</pre>			<p>This fragment of the code uses the <strong class="source-inline">joblib</strong> library to load an ML model. In this case, it is a model that we used in <a href="B19548_10.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic">Chapter 10</em></span></a> when we trained a classical ML model. It is a decision <span class="No-Break">tree model.</span></p>
			<p>Then, the program reads the same dataset that we used for training the model so that the format of the data is exactly the same. In this case, we can expect the same results that we used for the training dataset. For more complex models, we can create such a table by making one inference directly after the model has been trained and before it <span class="No-Break">was saved.</span></p>
			<p>Now, we can define three test cases in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# test that the model is not null
# which means that it actually exists
def test_model_not_null():
    assert model is not None
# test that the model predicts class 1 correctly
# here correctly means that it predicts the same way as when it was trained
def test_model_predicts_class_correctly():
    X = dfDataAnt13.drop(['Defect'], axis=1)
    assert model.predict(X)[0] == 1
# test that the model predicts class 0 correctly
# here correctly means that it predicts the same way as when it was trained
def test_model_predicts_class_0_correctly():
    X = dfDataAnt13.drop(['Defect'], axis=1)
    assert model.predict(X)[1] == 0</pre>			<p>The first test function (<strong class="source-inline">test_model_not_null</strong>) checks if the <strong class="source-inline">model</strong> variable, which is expected to hold the trained ML model, is not <strong class="source-inline">null</strong>. If the model is <strong class="source-inline">null</strong> (that is, it does not exist), the <strong class="source-inline">assert</strong> statement will raise an exception, indicating that the test <span class="No-Break">has failed.</span></p>
			<p>The second test function (<strong class="source-inline">test_model_predicts_class_correctly</strong>) checks whether the <a id="_idIndexMarker511"/>model predicts class 1 correctly for the given dataset. It does so by doing <span class="No-Break">the following:</span></p>
			<ul>
				<li>Preparing the <strong class="source-inline">X</strong> input features by dropping the <strong class="source-inline">'Defect'</strong> column from the <strong class="source-inline">dfDataAnt13</strong> DataFrame, assuming that <strong class="source-inline">'Defect'</strong> is the target column (<span class="No-Break">class label).</span></li>
				<li>Using the trained model (<strong class="source-inline">model.predict(X)</strong>) to make predictions on the <strong class="source-inline">X</strong> <span class="No-Break">input features.</span></li>
				<li>Asserting that the first prediction (<strong class="source-inline">model.predict(X)[0]</strong>) should be equal to 1 (class 1). If the model predicts class 1 correctly, the test passes; otherwise, it raises an exception, indicating a <span class="No-Break">test failure.</span></li>
			</ul>
			<p>The third test case (<strong class="source-inline">test_model_predicts_class_0_correctly</strong>) checks whether the model predicts class 0 correctly for the given dataset. It follows a similar process as the <span class="No-Break">previous test:</span></p>
			<ul>
				<li>Preparing the <strong class="source-inline">X</strong> input features by dropping the <strong class="source-inline">'Defect'</strong> column from the <span class="No-Break"><strong class="source-inline">dfDataAnt13</strong></span><span class="No-Break"> DataFrame.</span></li>
				<li>Using the trained model (<strong class="source-inline">model.predict(X)</strong>) to make predictions on the <strong class="source-inline">X</strong> <span class="No-Break">input features.</span></li>
				<li>Asserting that the second prediction (<strong class="source-inline">model.predict(X)[1]</strong>) should be equal to 0 (class 0). If the model predicts class 0 correctly, the test passes; otherwise, it raises an exception, indicating a <span class="No-Break">test failure.</span></li>
			</ul>
			<p>These tests verify the<a id="_idIndexMarker512"/> integrity and correctness of the trained model and ensure it performs as expected on the given dataset. The output from executing the tests is shown <span class="No-Break">as follows:</span></p>
			<pre class="console">
=============== test session starts =======================
platform win32 -- Python 3.11.4, pytest-7.4.0, pluggy-1.2.0
rootdir: C:\machine_learning_best_practices\chapter_12
plugins: anyio-3.7.0
collected 4 items
chapter_12_classical_ml_test.py                      [ 75%]
chapter_12_download_model_test.py                    [100%]
================ 4 passed in 12.76s =======================</pre>			<p>The Pytest framework found all of our tests and showed that three (out of four) are in the <strong class="source-inline">chapter_12_classical_ml_test.py</strong> file and one is in the <span class="No-Break"><strong class="source-inline">chapter_12_downloaded_model_test.py</strong></span><span class="No-Break"> file.</span></p>
			<p>My next best practice is, <span class="No-Break">therefore, this:</span></p>
			<p class="callout-heading">Best practice #64</p>
			<p class="callout">Treat models as units and prepare unit tests for <span class="No-Break">them accordingly.</span></p>
			<p>I recommend treating ML <a id="_idIndexMarker513"/>models as units (the same as modules) and using unit testing practices for them. This helps to reduce the effects of the probabilistic nature of the models and provides us with the possibility to check whether the model works correctly. It helps to debug the entire software <span class="No-Break">system afterward.</span></p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor156"/>Monitoring ML systems at runtime</h1>
			<p>Monitoring pipelines in production is a critical aspect of MLOps to ensure the performance, reliability, and accuracy of deployed ML models. This includes <span class="No-Break">several practices.</span></p>
			<p>The first practice is<a id="_idIndexMarker514"/> logging and collecting metrics. This activity includes instrumenting the ML code with logging statements to capture relevant information during model training and inference. Key metrics to monitor are model accuracy, data drift, latency, and throughput. Popular logging and monitoring frameworks include <a id="_idIndexMarker515"/>Prometheus, Grafana, and <strong class="bold">Elasticsearch, Logstash, and </strong><span class="No-Break"><strong class="bold">Kibana</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ELK</strong></span><span class="No-Break">).</span></p>
			<p>The second one is alerting, which is a setup of alerts based on predefined thresholds for key metrics. This helps in proactively identifying issues or anomalies in the production pipeline. When an alert is triggered, the appropriate team members can be notified to investigate and address the <span class="No-Break">problem promptly.</span></p>
			<p>Data drift detection is the third activity, which includes monitoring the distribution of incoming data to identify data drift. Data drift refers to changes in data distribution over time, which can impact <span class="No-Break">model performance.</span></p>
			<p>The fourth activity is performance monitoring, where the MLOps team continuously tracks the performance of the deployed model. They measure inference times, prediction accuracy, and other relevant metrics, and they monitor for performance degradation, which might occur due to changes in data, infrastructure, <span class="No-Break">or dependencies.</span></p>
			<p>In addition to these four main activities, an MLOps team has also the <span class="No-Break">following responsibilities:</span></p>
			<ul>
				<li><strong class="bold">Error analysis</strong>: Using tools to analyze and log errors encountered during inference and understanding the nature of errors can help improve the model or identify issues in the data <span class="No-Break">or system.</span></li>
				<li><strong class="bold">Model versioning</strong>: Keep track of model versions and their performance over time, and (if needed) roll back to previous versions if issues arise with the <span class="No-Break">latest deployment.</span></li>
				<li><strong class="bold">Environment monitoring</strong>: Monitoring the infrastructure and environment where the model is deployed with KPIs such as CPU/memory utilization, and network traffic and <a id="_idIndexMarker516"/>looking for <span class="No-Break">performance bottlenecks.</span></li>
				<li><strong class="bold">Security and compliance</strong>: Ensuring that the deployed models adhere to security and compliance standards as well as monitor access logs and any <span class="No-Break">suspicious activities.</span></li>
				<li><strong class="bold">User feedback</strong>: Collecting, analyzing, and incorporating user feedback into the monitoring and inference process. MLOps solicits feedback from end users to understand the model’s performance from a <span class="No-Break">real-world perspective.</span></li>
			</ul>
			<p>By monitoring pipelines effectively, MLOps can quickly respond to any issues that arise, deliver better user experiences, and maintain the overall health of your ML systems. However, monitoring all of the aforementioned aspects is rather effort-intensive, and not all MLOps teams have the resources to do that. Therefore, my last best practice in this chapter <span class="No-Break">is this:</span></p>
			<p class="callout-heading">Best practice #65</p>
			<p class="callout">Identify key aspects of the ML deployment and monitor these <span class="No-Break">aspects accordingly.</span></p>
			<p>Although this sounds straightforward, it is not always easy to identify key aspects. I usually start by prioritizing the monitoring of the infrastructure and logging and collecting metrics. Monitoring of the infrastructure is important as any kind of problems quickly propagate to customers and result in losing credibility and even business. Monitoring metrics and logging gives a great insight into the operation of ML systems and prevents a lot of problems with the production of <span class="No-Break">ML systems.</span></p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor157"/>Summary</h1>
			<p>Constructing ML pipelines concludes the part of the book that focuses on the core technical aspects of ML. Pipelines are important for ensuring that the ML models are used according to best practices in <span class="No-Break">software engineering.</span></p>
			<p>However, ML pipelines are still not a complete ML system. They can only provide inference of the data and provide an output. For the pipelines to function effectively, they need to be connected to other parts of the system such as the user interface and storage. That is the content of the <span class="No-Break">next chapter.</span></p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor158"/>References</h1>
			<ul>
				<li><em class="italic">A. Lima</em>, <em class="italic">L. Monteiro</em>, and <em class="italic">A.P. Furtado</em>, <em class="italic">MLOps: Practices, Maturity Models, Roles, Tools, and Challenges-A Systematic Literature Review</em>. <em class="italic">ICEIS (1), 2022: </em><span class="No-Break"><em class="italic">p. 308-320</em></span><span class="No-Break">.</span></li>
				<li><em class="italic">John, M.M.</em>, <em class="italic">Olsson, H.H.</em>, and<em class="italic"> Bosch, J.</em>, <em class="italic">Towards MLOps: A framework and maturity model</em>. In <em class="italic">2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)</em>. <span class="No-Break"><em class="italic">2021</em></span><span class="No-Break">. </span><span class="No-Break"><em class="italic">IEEE</em></span><span class="No-Break">.</span></li>
				<li><em class="italic">Staron, M. et al.</em>, <em class="italic">Industrial experiences from evolving measurement systems into self‐healing systems for improved availability</em>. <em class="italic">Software: Practice and Experience</em>, <em class="italic">2018</em>. <em class="italic">48(3): </em><span class="No-Break"><em class="italic">p. 719-739</em></span><span class="No-Break">.</span></li>
			</ul>
		</div>
	</body></html>