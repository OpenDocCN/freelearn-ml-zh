- en: Feature Improvement - Cleaning Datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征改进 - 清洗数据集
- en: In the last two chapters, we have gone from talking about a basic understanding
    of feature engineering and how it can be used to enhance our machine learning
    pipelines to getting our hands dirty with datasets and evaluating and understanding
    the different types of data that we can encounter in the wild.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们已从谈论特征工程的基本理解及其如何用于增强我们的机器学习流程，过渡到实际操作数据集，评估和理解在野外可能遇到的不同类型的数据。
- en: In this chapter, we will be using what we learned and taking things a step further
    and begin to change the datasets that we work with. Specifically, we will be starting
    to *clean* and *augment* our datasets. By cleaning, we will generally be referring
    to the process of altering columns and rows already given to us. By augmenting,
    we will generally refer to the processes of removing columns and adding columns
    to datasets. As always, our goal in all of these processes is to enhance our machine
    learning pipelines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将运用所学知识，更进一步，开始改变我们使用的数据集。具体来说，我们将开始对数据集进行*清洗*和*增强*。通过清洗，我们通常指的是改变已经给定的列和行。通过增强，我们通常指的是从数据集中移除列和添加列的过程。正如往常一样，我们所有这些过程中的目标是提升我们的机器学习流程。
- en: 'In the following chapters, we will be:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将进行以下操作：
- en: Identifying missing values in data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别数据中的缺失值
- en: Removing harmful data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除有害数据
- en: Imputing (filling in) these missing values
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充（补充）这些缺失值
- en: Normalizing/standardizing data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化/标准化数据
- en: Constructing brand new features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建全新的特征
- en: Selecting (removing) features manually and automatically
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动和自动选择（移除）特征
- en: Using mathematical matrix computations to transform datasets to different dimensions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数学矩阵计算将数据集转换为不同的维度
- en: These methods will help us develop a better sense of which features are important
    within our data. In this chapter, we will be diving deeper into the first four
    methods, and leave the other three for future chapters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法将帮助我们更好地了解数据中哪些特征是重要的。在本章中，我们将深入探讨前四种方法，并将其他三种方法留待未来章节讨论。
- en: Identifying missing values in data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别数据中的缺失值
- en: Our first method of identifying missing values is to give us a better understanding
    of how to work with real-world data. Often, data can have missing values due to
    a variety of reasons, for example with survey data, some observations may not
    have been recorded. It is important for us to analyze our data, and get a sense
    of what the missing values are so we can decide how we want to handle missing
    values for our machine learning. To start, let's dive into a dataset that we will
    be interested in for the duration of this chapter, the `Pima Indian Diabetes Prediction`
    dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们识别缺失值的第一种方法是为了更好地理解如何处理现实世界的数据。通常，数据可能由于各种原因而存在缺失值，例如在调查数据中，一些观察结果可能没有被记录。对我们来说，分析数据并了解缺失值是什么，以便我们决定如何处理机器学习中的缺失值是很重要的。首先，让我们深入研究一个在本章期间我们将感兴趣的数据集，即`皮马印第安糖尿病预测`数据集。
- en: The Pima Indian Diabetes Prediction dataset
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 皮马印第安糖尿病预测数据集
- en: 'This dataset is available on the UCI Machine Learning Repository at:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可在以下UCI机器学习仓库中找到：
- en: '[https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes).'
- en: From the main website, we can learn a few things about this publicly available
    dataset. We have nine columns and 768 instances (rows). The dataset is primarily
    used for predicting the onset of diabetes within five years in females of Pima
    Indian heritage over the age of 21 given medical details about their bodies.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从主网站上，我们可以了解一些关于这个公开数据集的信息。我们共有九列和768个实例（行）。这个数据集主要用于预测21岁以上的皮马印第安女性在五年内是否会患上糖尿病，前提是提供她们的医疗详细信息。
- en: 'The dataset is meant to correspond with a binary (2-class) classification machine
    learning problem. Namely, the answer to the question, *will this person develop
    diabetes within five years?* The column names are provided as follows (in order):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集旨在对应一个二元（双分类）机器学习问题。即，回答问题：*这个人五年内会患上糖尿病吗？* 列名如下（按顺序）：
- en: Number of times pregnant
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 怀孕次数
- en: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 口服葡萄糖耐量测试2小时后的血浆葡萄糖浓度
- en: Diastolic blood pressure (mm Hg)
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 舒张压（毫米汞柱）
- en: Triceps skinfold thickness (mm)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 三角肌皮肤褶皱厚度（毫米）
- en: 2-Hour serum insulin measurement (mu U/ml)
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2小时血清胰岛素测量（微U/ml）
- en: Body mass index (weight in kg/(height in m)²)
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 体重指数（千克/（米）²）
- en: Diabetes pedigree function
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 糖尿病家系函数
- en: Age (years)
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 年龄（年）
- en: Class variable (zero or one)
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类变量（零或一）
- en: The goal of the dataset is to be able to predict the final column of `class`
    variable, which predicts if the patient has developed diabetes, using the other
    eight features as inputs to a machine learning function.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的目标是能够预测`class`变量的最后一列，该变量预测患者是否患有糖尿病，使用其他八个特征作为机器学习函数的输入。
- en: 'There are two very important reasons we will be working with this dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个数据集有两个非常重要的原因：
- en: We will have to work with missing values
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将不得不处理缺失值
- en: All of the features we will be working with will be quantitative
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用的所有特征都将是有量的
- en: The first point makes more sense for now as a reason, because the point of this
    chapter is to deal with missing values. As far as only choosing to work with quantitative
    data, this will only be the case for this chapter. We do not have enough tools
    to deal with missing values in categorical columns. In the next chapter, when
    we talk about feature construction, we will deal with this procedure.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目前来说，第一个点作为一个原因更有意义，因为本章的目的是处理缺失值。如果我们只选择处理定量数据，这种情况将仅限于本章。我们没有足够的工具来处理分类列中的缺失值。在下一章，当我们讨论特征构建时，我们将处理这个流程。
- en: The exploratory data analysis (EDA)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）
- en: 'To identify our missing values we will begin with an EDA of our dataset. We
    will be using some useful python packages, pandas and numpy, to store our data
    and make some simple calculations as well as some popular visualization tools
    to see what the distribution of our data looks like. Let''s begin and dive into
    some code. First, we will do some imports:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别我们的缺失值，我们将从我们的数据集的EDA开始。我们将使用一些有用的Python包，如pandas和numpy，来存储我们的数据并进行一些简单的计算，以及一些流行的可视化工具来查看我们的数据分布情况。让我们开始并深入一些代码。首先，我们将进行一些导入：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will import our tabular data through a CSV, as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过CSV导入我们的表格数据，如下所示：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `head` method allows us to see the first few rows in our dataset. The output
    is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`head`方法允许我们查看数据集的前几行。输出如下：'
- en: '|  | **6** | **148** | **72** | **35** | **0** | **33.6** | **0.627** | **50**
    | **1** |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | **6** | **148** | **72** | **35** | **0** | **33.6** | **0.627** | **50**
    | **1** |'
- en: '| **0** | 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 |'
- en: '| **1** | 8 | 183 | 64 | 0 | 0 | 23.3 | 0.627 | 32 | 1 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 8 | 183 | 64 | 0 | 0 | 23.3 | 0.627 | 32 | 1 |'
- en: '| **2** | 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 |'
- en: '| **3** | 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 |'
- en: '| **4** | 5 | 116 | 74 | 0 | 0 | 25.6 | 0.201 | 30 | 0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 5 | 116 | 74 | 0 | 0 | 25.6 | 0.201 | 30 | 0 |'
- en: 'Something''s not right here, there''s no column names. The CSV must not have
    the names for the columns built into the file. No matter, we can use the data
    source''s website to fill this in, as shown in the following code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有些不对劲，没有列名。CSV文件中可能没有将列名嵌入到文件中。没关系，我们可以使用数据源的网站来填写这些信息，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, using the `head` method again, we can see our columns with the appropriate
    headers. The output of the preceding code is as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，再次使用`head`方法，我们可以看到带有适当标题的列。前面代码的输出如下：
- en: '|  | **times_pregnant** | **plasma_glucose_concentration** | **diastolic_blood_pressure**
    | **triceps_thickness** | **serum_insulin** | **bmi** | **pedigree_function**
    | **age** | **onset_diabetes** |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | **怀孕次数** | **血浆葡萄糖浓度** | **舒张压** | **三头肌厚度** | **血清胰岛素** | **BMI** | **家系函数**
    | **年龄** | **糖尿病发病时间** |'
- en: '| **0** | 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 |'
- en: '| **1** | 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 |'
- en: '| **2** | 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 |'
- en: '| **3** | 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 |'
- en: '| **4** | 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 |'
- en: 'Much better, now we can use the column names to do some basic stats, selecting,
    and visualizations. Let''s first get our null accuracy as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，现在我们可以使用列名来进行一些基本的统计、选择和可视化。让我们首先获取我们的空值准确率如下：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If our eventual goal is to exploit patterns in our data in order to predict
    the onset of diabetes, let us try to visualize some of the differences between
    those that developed diabetes and those that did not. Our hope is that the histogram
    will reveal some sort of pattern, or obvious difference in values between the
    classes of prediction:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的最终目标是利用数据中的模式来预测糖尿病的发病，让我们尝试可视化那些患病和未患病的人之间的差异。我们的希望是直方图会揭示某种模式，或者在预测类别的值之间有明显的差异：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/408febaa-777c-4619-a4ca-76cb171ced80.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/408febaa-777c-4619-a4ca-76cb171ced80.png)'
- en: 'It seems that this histogram is showing us a pretty big difference between
    `plasma_glucose_concentration` between the two prediction classes. Let''s show
    the same histogram style for multiple columns as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个直方图似乎在向我们展示两个预测类别之间`血浆葡萄糖浓度`的很大差异。让我们以相同的直方图风格展示多个列，如下所示：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the preceding code will give us the following three histograms.
    The first one is show us the distributions of **bmi** for the two class variables
    (non-diabetes and diabetes):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出将给我们以下三个直方图。第一个直方图展示了两个类别变量（非糖尿病和糖尿病）的**BMI**分布：
- en: '![](img/b05cd7b9-0005-471d-84da-5e8d3d8320e3.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b05cd7b9-0005-471d-84da-5e8d3d8320e3.png)'
- en: 'The next histogram to appear will shows us again contrastingly different distributions
    between a feature across our two class variables. This time we are looking at
    **diastolic_blood_pressure**:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个出现的直方图将再次向我们展示两个类别变量之间在某个特征上的对比性不同的分布。这次我们正在查看**舒张压**：
- en: '![](img/1eb28f27-66a3-4265-af6d-13dbbfef4a20.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1eb28f27-66a3-4265-af6d-13dbbfef4a20.png)'
- en: 'The final graph will show **plasma_glucose_concentration** differences between
    our two class variables:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个图表将展示两个类别变量之间的**血浆葡萄糖浓度**差异：
- en: '![](img/a5887f0e-8069-41af-9e70-884e0be4a426.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a5887f0e-8069-41af-9e70-884e0be4a426.png)'
- en: 'We can definitely see some major differences simply by looking at just a few
    histograms. For example, there seems to be a large jump in `plasma_glucose_concentration`
    for those who will eventually develop diabetes. To solidify this, perhaps we can
    visualize a linear correlation matrix in an attempt to quantify the relationship
    between these variables. We will use the visualization tool, seaborn, which we
    imported at the beginning of this chapter for our correlation matrix as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 只需通过查看几个直方图，我们就可以明显地看到一些主要差异。例如，对于最终会患上糖尿病的人来说，`血浆葡萄糖浓度`似乎有一个很大的跳跃。为了巩固这一点，也许我们可以通过可视化线性相关矩阵来尝试量化这些变量之间的关系。我们将使用在章节开头导入的可视化工具seaborn来创建以下的相关矩阵：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Following is the correlation matrix of our dataset. This is showing us the
    correlation amongst the different columns in our `Pima` dataset. The output is
    as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们数据集的相关矩阵。这显示了`Pima`数据集中不同列之间的相关性。输出如下：
- en: '![](img/d4f2b0c8-3429-4b62-9ccc-039296923e95.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d4f2b0c8-3429-4b62-9ccc-039296923e95.png)'
- en: 'This correlation matrix is showing a strong correlation between `plasma_glucose_concentration` and `onset_diabetes`.
    Let''s take a further look at the numerical correlations for the `onset_diabetes` column,
    with the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个相关矩阵显示了`血浆葡萄糖浓度`和`发病糖尿病`之间强烈的关联。让我们进一步查看`发病糖尿病`列的数值相关性，以下代码：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will explore the powers of correlation in a later [Chapter 4](430d621e-7ce6-48c0-9990-869e82a0d0c6.xhtml), *Feature
    Construction*, but for now we are using **exploratory data analysis** (**EDA**)
    to hint at the fact that the `plasma_glucose_concentration` column will be an
    important factor in our prediction of the onset of diabetes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第4章](430d621e-7ce6-48c0-9990-869e82a0d0c6.xhtml)中探索相关性的力量，*特征构建*，但现在我们正在使用**探索性数据分析**（**EDA**）来暗示`血浆葡萄糖浓度`这一列将是我们预测糖尿病发病的重要因素。
- en: 'Moving on to more important matters at hand, let''s see if we are missing any
    values in our dataset by invoking the built-in `isnull()` method of the pandas
    DataFrame:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注更重要的任务，通过调用pandas DataFrame的内置`isnull()`方法来查看我们的数据集中是否有缺失值：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Great! We don''t have any missing values. Let''s go on to do some more EDA,
    first using the `shape` method to see the number of rows and columns we are working
    with:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们没有缺失值。让我们继续进行更多的EDA，首先使用`shape`方法查看我们正在处理的行数和列数：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Confirming we have `9` columns (including our response variable) and `768`
    data observations (rows). Now, let''s take a peak at the percentage of patients
    who developed diabetes, using the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 确认我们有`9`列（包括我们的响应变量）和`768`个数据观测值（行）。现在，让我们看一下患糖尿病的患者的百分比，使用以下代码：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This shows us that `65%` of the patients did not develop diabetes, while about
    35% did. We can use a nifty built-in method of a pandas DataFrame called `describe`
    to look at some basic descriptive statistics:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明65%的患者没有患上糖尿病，而大约35%的患者患上了。我们可以使用pandas DataFrame的一个内置方法`describe`来查看一些基本描述性统计信息：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We get the output as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '|  | **times_pregnant** | **plasma_glucose _concentration** | **diastolic_
    blood_pressure** | **triceps _thickness** | **serum _insulin** | **bmi** | **pedigree
    _function** | **age** | **onset _diabetes** |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | **times_pregnant** | **plasma_glucose _concentration** | **diastolic_
    blood_pressure** | **triceps _thickness** | **serum _insulin** | **bmi** | **pedigree
    _function** | **age** | **onset _diabetes** |'
- en: '| **count** | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000
    | 768.000000 | 768.000000 | 768.000000 | 768.000000 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **count** | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000
    | 768.000000 | 768.000000 | 768.000000 | 768.000000 |'
- en: '| **mean** | 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578
    | 0.471876 | 33.240885 | 0.348958 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| **mean** | 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578
    | 0.471876 | 33.240885 | 0.348958 |'
- en: '| **std** | 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160
    | 0.331329 | 11.760232 | 0.476951 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **std** | 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160
    | 0.331329 | 11.760232 | 0.476951 |'
- en: '| **min** | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000
    | 0.078000 | 21.000000 | 0.000000 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **min** | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000
    | 0.078000 | 21.000000 | 0.000000 |'
- en: '| **25%** | 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000
    | 0.243750 | 24.000000 | 0.000000 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| **25%** | 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000
    | 0.243750 | 24.000000 | 0.000000 |'
- en: '| **50%** | 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000
    | 0.372500 | 29.000000 | 0.000000 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **50%** | 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000
    | 0.372500 | 29.000000 | 0.000000 |'
- en: '| **75%** | 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000
    | 0.626250 | 41.000000 | 1.000000 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **75%** | 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000
    | 0.626250 | 41.000000 | 1.000000 |'
- en: '| **max** | 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 |
    67.100000 | 2.420000 | 81.000000 | 1.000000 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **max** | 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 |
    67.100000 | 2.420000 | 81.000000 | 1.000000 |'
- en: 'This shows us quite quickly some basic stats such as mean, standard deviation,
    and some different percentile measurements of our data. But, notice that the minimum
    value of the `BMI` column is `0`. That is medically impossible; there must be
    a reason for this to happen. Perhaps the number zero has been encoded as a missing
    value instead of the None value or a missing cell. Upon closer inspection, we
    see that the value 0 appears as a minimum value for the following columns:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这很快地显示了一些基本统计信息，如平均值、标准差和一些不同的百分位数测量值。但请注意，`BMI`列的最小值是`0`。这在医学上是不可能的；这肯定有原因。也许数字零被编码为缺失值，而不是None值或缺失单元格。经过仔细检查，我们发现以下列的最小值出现了0：
- en: '`times_pregnant`'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`times_pregnant`'
- en: '`plasma_glucose_concentration`'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plasma_glucose_concentration`'
- en: '`diastolic_blood_pressure`'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diastolic_blood_pressure`'
- en: '`triceps_thickness`'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`triceps_thickness`'
- en: '`serum_insulin`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`serum_insulin`'
- en: '`bmi`'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bmi`'
- en: '`onset_diabetes`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onset_diabetes`'
- en: 'Because zero is a class for `onset_diabetes` and 0 is actually a viable number
    for `times_pregnant`, we may conclude that the number 0 is encoding missing values
    for:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 因为0是`onset_diabetes`的一个类别，而0实际上对于`times_pregnant`来说是一个有效的数字，所以我们可能得出结论，数字0用于编码以下变量的缺失值：
- en: '`plasma_glucose_concentration`'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plasma_glucose_concentration`'
- en: '`diastolic_blood_pressure`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diastolic_blood_pressure`'
- en: '`triceps_thickness`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`triceps_thickness`'
- en: '`serum_insulin`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`serum_insulin`'
- en: '`bmi`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bmi`'
- en: So, we actually do having missing values! It was obviously not luck that we
    happened upon the zeros as missing values, we knew it beforehand. As a data scientist,
    you must be ever vigilant and make sure that you know as much about the dataset
    as possible in order to find missing values encoded as other symbols. Be sure
    to read any and all documentation that comes with open datasets in case they mention
    any missing values.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们实际上确实有缺失值！显然，我们偶然发现零作为缺失值并不是运气，我们事先就知道。作为一名数据科学家，你必须始终保持警惕，并确保尽可能多地了解数据集，以便找到以其他符号编码的缺失值。务必阅读任何与公开数据集一起提供的所有文档，以防它们提到任何缺失值。
- en: 'If no documentation is available, some common values used instead of missing
    values are:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有可用的文档，一些常用的值被用来代替缺失值：
- en: '**0** (for numerical values)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0**（用于数值变量）'
- en: '**unknown** or **Unknown** (for categorical variables)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unknown**或**Unknown**（用于分类变量）'
- en: '**?** (for categorical variables)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**?**（用于分类变量）'
- en: So, we have five columns in which missing values exist, so now we get to talk
    about how to deal with them, in depth.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有五个存在缺失值的列，现在我们可以深入讨论如何处理它们。
- en: Dealing with missing values in a dataset
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据集中的缺失值
- en: When working with data, one of the most common issues a data scientist will
    run into is the problem of missing data. Most commonly, this refers to empty cells
    (row/column intersections) where the data just was not acquired for whatever reason.
    This can become a problem for many reasons; notably, when applying learning algorithms
    to data with missing values, most (not all) algorithms are not able to cope with
    missing values.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据时，数据科学家最常见的问题之一是缺失数据的问题。最常见的是指由于某种原因数据未采集的空单元格（行/列交叉点）。这可能会成为许多问题的原因；值得注意的是，当将学习算法应用于带有缺失值的数据时，大多数（不是所有）算法无法处理缺失值。
- en: 'For this reason, data scientists and machine learning engineers have many tricks
    and tips on how to deal with this problem. Although there are many variations
    of methodologies, the two major ways in which we can deal with missing data are:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据科学家和机器学习工程师有很多技巧和建议来处理这个问题。尽管有许多方法变体，但我们处理缺失数据的主要两种方式是：
- en: Remove rows with missing values in them
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除包含缺失值的行
- en: Impute (fill in) missing values
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充（补全）缺失值
- en: Each method will **clean** our dataset to a point where a learning algorithm
    can handle it, but each method will have its pros and cons.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法都会将我们的数据集**清理**到学习算法可以处理的程度，但每种方法都会有其优缺点。
- en: 'First off, before we go too far, let''s get rid of the zeros and replace them
    all with the value `None` in Python. This way, our `fillna` and `dropna` methods
    will work correctly. We could manually replace all zeros with None, each column
    at a time, like so:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在我们走得太远之前，让我们去掉所有的零，并在Python中将它们全部替换为值`None`。这样，我们的`fillna`和`dropna`方法将正常工作。我们可以手动逐列将所有零替换为`None`，如下所示：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We could repeat this procedure for every column with incorrectly labeled missing
    values, or we could use a `for` loop and a built-in `replace` method to speed
    things up, as shown in the following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为每个带有错误标记缺失值的列重复此过程，或者我们可以使用`for`循环和内置的`replace`方法来加快速度，如下面的代码所示：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'So, now if we try to count the number of missing values using the `isnull`
    method, we should start to see missing values being counted as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在如果我们尝试使用`isnull`方法来计算缺失值的数量，我们应该开始看到缺失值被计数如下：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, looking at the first few rows of our dataset, we get the output as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，查看数据集的前几行，我们得到以下输出：
- en: '|  | **times_pregnant** | **plasma_glucose_concentration** | **diastolic_blood_pressure**
    | **triceps_thickness** | **serum_insulin** | **bmi** | **pedigree_function**
    | **age** | **onset_diabetes** |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | **times_pregnant** | **plasma_glucose_concentration** | **diastolic_blood_pressure**
    | **triceps_thickness** | **serum_insulin** | **bmi** | **pedigree_function**
    | **age** | **onset_diabetes** |'
- en: '| **0** | 6 | 148 | 72 | 35 | NaN | 33.6 | 0.627 | 50 | 1 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 6 | 148 | 72 | 35 | NaN | 33.6 | 0.627 | 50 | 1 |'
- en: '| **1** | 1 | 85 | 66 | 29 | NaN | 26.6 | 0.351 | 31 | 0 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 1 | 85 | 66 | 29 | NaN | 26.6 | 0.351 | 31 | 0 |'
- en: '| **2** | 8 | 183 | 64 | None | NaN | 23.3 | 0.672 | 32 | 1 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 8 | 183 | 64 | None | NaN | 23.3 | 0.672 | 32 | 1 |'
- en: '| **3** | 1 | 89 | 66 | 23 | NaN | 28.1 | 0.167 | 21 | 0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 1 | 89 | 66 | 23 | NaN | 28.1 | 0.167 | 21 | 0 |'
- en: '| **4** | 0 | 137 | 40 | 35 | NaN | 43.1 | 2.288 | 33 | 1 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 0 | 137 | 40 | 35 | NaN | 43.1 | 2.288 | 33 | 1 |'
- en: OK, this is starting to make much more sense. We can now see that five columns
    have missing values, and the degree to which data is missing is staggering. Some
    columns, such as `plasma_glucose_concentration`, are only missing five values,
    but look at `serum_insulin`; that column is missing almost half of its values.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这开始变得更有意义。我们现在可以看到有五个列存在缺失值，数据缺失的程度令人震惊。一些列，如`plasma_glucose_concentration`，只缺失了五个值，但看看`serum_insulin`列；该列几乎缺失了其一半的值。
- en: 'Now that we have missing values properly injected into our dataset instead
    of the `0` placeholders that the dataset originally came with, our exploratory
    data analysis will be more accurate:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据集中的缺失值正确地注入，而不是使用数据集最初带的`0`占位符，我们的探索性数据分析将更加准确：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding code produces the following output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '|  | **times_pregnant** | **serum_insulin** | **pedigree_function** | **age**
    | **onset_diabetes** |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | **times_pregnant** | **serum_insulin** | **pedigree_function** | **age**
    | **onset_diabetes** |'
- en: '| **count** | 768.000000 | 394.000000 | 768.000000 | 768.000000 | 768.000000
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **计数** | 768.000000 | 394.000000 | 768.000000 | 768.000000 | 768.000000 |'
- en: '| **mean** | 3.845052 | 155.548223 | 0.471876 | 33.240885 | 0.348958 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **均值** | 3.845052 | 155.548223 | 0.471876 | 33.240885 | 0.348958 |'
- en: '| **std** | 3.369578 | 118.775855 | 0.331329 | 11.760232 | 0.476951 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| **标准差** | 3.369578 | 118.775855 | 0.331329 | 11.760232 | 0.476951 |'
- en: '| **min** | 0.000000 | 14.000000 | 0.078000 | 21.000000 | 0.000000 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| **最小值** | 0.000000 | 14.000000 | 0.078000 | 21.000000 | 0.000000 |'
- en: '| **25%** | 1.000000 | 76.250000 | 0.243750 | 24.000000 | 0.000000 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| **25%** | 1.000000 | 76.250000 | 0.243750 | 24.000000 | 0.000000 |'
- en: '| **50%** | 3.000000 | 125.000000 | 0.372500 | 29.000000 | 0.000000 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **50%** | 3.000000 | 125.000000 | 0.372500 | 29.000000 | 0.000000 |'
- en: '| **75%** | 6.000000 | 190.000000 | 0.626250 | 41.000000 | 1.000000 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| **75%** | 6.000000 | 190.000000 | 0.626250 | 41.000000 | 1.000000 |'
- en: '| **max** | 17.000000 | 846.000000 | 2.420000 | 81.000000 | 1.000000 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| **最大值** | 17.000000 | 846.000000 | 2.420000 | 81.000000 | 1.000000 |'
- en: 'Notice that the `describe` method doesn''t include columns with missing values,
    which while not ideal, doesn''t mean that we cannot obtain them by computing the
    mean and standard deviation of the specific columns, like so:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到`describe`方法不包括缺失值的列，虽然这不是理想的情况，但这并不意味着我们不能通过计算特定列的均值和标准差来获得它们，如下所示：
- en: '[PRE16]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Let us move on to our two ways of dealing with missing data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论处理缺失数据的两种方法。
- en: Removing harmful rows of data
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除有害数据行
- en: 'Probably the most common and easiest of our two options for dealing with missing
    data is to simply remove the observations that have any missing values. By doing
    so, we will be left with only the **complete** data points with all data filled
    in. We can obtain a new DataFrame by invoking the `dropna` method in pandas, as
    shown in the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据最常见且最简单的方法可能是简单地删除任何缺失值的观测值。这样做，我们将只剩下所有数据都已填满的**完整**数据点。我们可以通过在pandas中调用`dropna`方法来获得一个新的DataFrame，如下所示：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, of course, the obvious problem here is that we lost a few rows. To check
    how many exactly, use the following code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当然，这里明显的问题是我们丢失了一些行。为了检查具体丢失了多少行，请使用以下代码：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Wow! We lost about 51% of the rows from the original dataset, and if we think
    about this from a machine learning perspective, even though now we have clean
    data with everything filled in, we aren't really learning as much as we possibly
    could be by ignoring over half of the data's observations. That's like a doctor
    trying to understand how heart attacks happen, ignoring over half of their patients
    coming in for check-ups.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们从原始数据集中丢失了大约51%的行，如果我们从机器学习的角度来看，尽管现在我们有干净的数据，所有数据都已填满，但我们通过忽略超过一半的数据观测值，实际上并没有学到尽可能多的东西。这就像一位医生试图了解心脏病发作的原因，却忽略了超过一半前来检查的患者。
- en: 'Let''s perform some more EDA on the dataset and compare the statistics about
    the data from before and after dropping the missing-values rows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对数据集进行更多的探索性数据分析（EDA），并比较删除缺失值行前后的数据统计信息：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, let''s look at the same split after we dropped the rows, using the following
    code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下代码查看删除行后的相同拆分：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It seems that the binary response stayed relatively the same during the drastic
    transformation of our dataset. Let''s take a look at the *shape* of our data by
    comparing the average values of columns before and after the transformation, using
    the `pima.mean` function, as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，在数据集的剧烈变化过程中，二进制响应保持相对稳定。让我们通过比较变换前后列的平均值来查看我们数据的*形状*，如下使用`pima.mean`函数：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And now for the same averages after dropping the rows, using the `pima_dropped.mean()` function,
    as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`pima_dropped.mean()`函数查看删除行后的相同平均值：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To get a better look at how these numbers changed, let''s create a new chart
    that visualizes the percentages changed on average for each column. First, let''s
    create a table of the percent changes in the average values of each column, as
    shown in the following code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地查看这些数字的变化，让我们创建一个新的图表来可视化每个列平均百分比的变化。首先，让我们创建一个表格，显示每个列平均值的百分比变化，如下所示：
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And now let''s visualize these changes as a bar chart, using the following
    code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用以下代码将这些变化可视化成条形图：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding code produces the following output:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码产生以下输出：
- en: '![](img/512ce682-ec85-47c9-af15-0620dc6cec7d.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/512ce682-ec85-47c9-af15-0620dc6cec7d.png)'
- en: We can see that the number of `times_pregnant` variable average fell 14% after
    dropping missing values, which is a big change! The `pedigree_function` also rose
    11%, another big leap. We can see how dropping rows (observations) severely affects
    the shape of the data and we should try to retain as much data as possible. Before
    moving on to the next method of dealing with missing values, let's introduce some
    actual machine learning into the mix.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在删除缺失值后，`times_pregnant`变量的平均值下降了14%，这是一个很大的变化！`pedigree_function`也上升了11%，另一个大跳跃。我们可以看到删除行（观测值）如何严重影响数据的形状，我们应该尽量保留尽可能多的数据。在继续到处理缺失值的下一个方法之前，让我们引入一些实际的机器学习。
- en: 'The following code block (which we will go over line by line in a moment) will
    become a very familiar code block in this book. It describes and achieves a single
    fitting of a machine learning model over a variety of parameters in the hope of
    obtaining the best possible model, given the features at hand:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块（我们将在稍后逐行分析）将成为本书中一个非常熟悉的代码块。它描述并实现了一个机器学习模型在多种参数上的单次拟合，目的是在给定的特征下获得最佳模型：
- en: '[PRE25]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'OK, let''s go through this line by line. First, we have two new import statements:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们逐行分析。首先，我们有两条新的导入语句：
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We will be utilizing scikit-learn''s **K-Nearest Neighbors** (**KNN**) classification
    model, as well as a grid search module that will automatically find the best combo
    of parameters (using brute force) for the KNN model that best fits our data with
    respect to cross-validated accuracy. Next, let''s take our dropped dataset (with
    the missing-valued rows removed) and create an `X` and a `y` variable for our
    predictive model. Let''s start with our `X` (our feature matrix):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用scikit-learn的**K-Nearest Neighbors**（KNN）分类模型，以及一个网格搜索模块，该模块将自动找到最适合我们的数据并具有最佳交叉验证准确率的KNN模型的最佳参数组合（使用暴力搜索）。接下来，让我们取我们的删除数据集（已删除缺失值行）并为我们预测模型创建一个`X`和`y`变量。让我们从我们的`X`（我们的特征矩阵）开始：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Ouch, it''s already obvious that there''s a major problem with this approach.
    Our machine learning algorithm is going to be fitting and learning from far fewer
    data observations than which we started with. Let''s now create our `y` (response
    series):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，这个方法的问题已经很明显了。我们的机器学习算法将要拟合和学习的观测数据比我们开始时使用的要少得多。现在让我们创建我们的`y`（响应序列）：
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now that we have our `X` and our `y` variable, we can introduce the variables
    and instances we need to successfully run a **grid search**. We will set the number
    of `params` to try at seven to keep things simple in this chapter. For every data
    cleaning and feature engineering method we try (dropping rows, filling in data),
    we will try to fit the best KNN as having somewhere between one and seven neighbors
    complexity. We can set this model up as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了`X`和`y`变量，我们可以引入我们需要成功运行**网格搜索**的变量和实例。我们将尝试的`params`数量设置为七个，以使本章的内容简单。对于我们尝试的每一种数据清理和特征工程方法（删除行，填充数据），我们将尝试将最佳KNN拟合到具有一到七个邻居复杂度的某个地方。我们可以这样设置这个模型：
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we will instantiate a grid search module, as shown in the following code,
    and fit it to our feature matrix and response variable. Once we do so, we will
    print out the best accuracy as well as the best parameter used to learn:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实例化一个网格搜索模块，如下面的代码所示，并将其拟合到我们的特征矩阵和响应变量。一旦这样做，我们将打印出最佳准确率以及用于学习的最佳参数：
- en: '[PRE30]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So, it seems that using seven neighbors as its parameter, our KNN model was
    able to achieve a `74.4%` accuracy (better than our null accuracy of around 65%),
    but keep in mind that it is only learning from 49% of the original data, so who
    knows how it could have done on the rest of the data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，似乎使用七个邻居作为其参数，我们的KNN模型能够达到74.4%的准确率（比我们大约65%的零准确率要好），但请记住，它只从原始数据的49%中学习，那么谁知道它在剩余的数据上会表现如何。
- en: This is our first real look into using machine learning in this book. We will
    be assuming that the reader does have basic familiarity with machine learning
    as well as statistical procedures such as cross-validation.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书中我们第一次真正探讨使用机器学习。我们假设读者对机器学习和诸如交叉验证之类的统计过程有基本的了解。
- en: It's probably pretty clear that while dropping the *dirty* rows may not exactly
    be feature engineering, it is still a data cleaning technique we can utilize to
    help sanitize our machine learning pipeline inputs. Let's try for a slightly harder
    method.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，虽然删除*脏*行可能并不完全等同于特征工程，但它仍然是一种我们可以利用的数据清洗技术，有助于净化我们的机器学习管道输入。让我们尝试一个稍微复杂一些的方法。
- en: Imputing the missing values in data
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在数据中填充缺失值
- en: 'Imputing is the more involved method of dealing with missing values. By *imputing, *we
    refer to the act of filling in missing data values with numerical quantities that
    are somehow ascertained from existing knowledge/data. We have a few options on
    how we can fill in these missing values, the most common of them being filling
    in missing values with the average value for the rest of the column, as shown
    in the following code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 填充是处理缺失值的更复杂的方法。通过*填充*，我们指的是用从现有知识/数据中确定的数值填充缺失数据值的行为。我们有几种方法可以填充这些缺失值，其中最常见的是用该列其余部分的平均值填充缺失值，如下面的代码所示：
- en: '[PRE31]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s look at the five rows where `plasma_glucose_concentration` is missing:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`plasma_glucose_concentration`缺失的五行：
- en: '[PRE32]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let''s use the built-in `fillna` method to replace all of the `None` values
    with the mean value of the rest of the `plasma_glucose_concentration` column:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用内置的`fillna`方法将所有`None`值替换为`plasma_glucose_concentration`列其余部分的平均值：
- en: '[PRE33]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'And if we check out the column, we should see that the `None` values have been
    replaced by `121.68`, the mean value we obtained earlier for this column:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查该列，我们应该看到`None`值已被替换为之前为此列获得的平均值`121.68`：
- en: '[PRE34]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Great! But this can be cumbersome. Let''s use a module in the scikit-learn
    preprocessing class (the documentation can be found at [http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing))
    called the `Imputer` (aptly named). We can import it as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！但这可能有点麻烦。让我们使用scikit-learn预处理类中的一个模块（文档可以在[http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)找到）称为`Imputer`（恰如其名）。我们可以如下导入它：
- en: '[PRE35]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As with most scikit-learn modules, we have a few new parameters to play with,
    but I will focus on the major one, called the `strategy`. We can define how we
    want to impute values into our dataset by setting this parameter. For quantitative
    values, we can use the built-in mean and median strategies to fill in values with
    either quantity. To use the `Imputer`, we must first instantiate the object, as
    shown in the following code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数scikit-learn模块一样，我们有一些新的参数可以调整，但我将重点介绍其中一个，称为`strategy`。我们可以通过设置此参数来定义如何将值填充到我们的数据集中。对于定量值，我们可以使用内置的均值和中值策略来填充值。要使用`Imputer`，我们必须首先实例化对象，如下面的代码所示：
- en: '[PRE36]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we can call the `fit_transform` method to create a new object, as shown
    in the following code:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以调用`fit_transform`方法来创建一个新的对象，如下面的代码所示：
- en: '[PRE37]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We do have a small issue to deal with. The output of the Imputer is not a pandas
    DataFrame, but rather the output is of type **NumPy** array:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实有一个小问题需要处理。`Imputer`的输出不是一个pandas DataFrame，而是输出类型为**NumPy**数组：
- en: '[PRE38]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This can be easily dealt with, as we could just cast the array as a DataFrame,
    as shown in the following code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易处理，因为我们只需将数组转换为DataFrame，如下面的代码所示：
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s take a look at our new DataFrame:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的新DataFrame：
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The preceding code produces the following output:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '|  | **times_pregnant** | **plasma_glucose_concentration** | **diastolic_blood_pressure**
    | **triceps_thickness** | **serum_insulin** | **bmi** | **pedigree_function**
    | **age** | **onset_diabetes** |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | **怀孕次数** | **血浆葡萄糖浓度** | **舒张压** | **三头肌厚度** | **血清胰岛素** | **BMI** | **谱系功能**
    | **年龄** | **糖尿病发病时间** |'
- en: '| 0 | 6.0 | 148.0 | 72.0 | 35.00000 | 155.548223 | 33.6 | 0.627 | 50.0 | 1.0
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 6.0 | 148.0 | 72.0 | 35.00000 | 155.548223 | 33.6 | 0.627 | 50.0 | 1.0
    |'
- en: '| 1 | 1.0 | 85.0 | 66.0 | 29.00000 | 155.548223 | 26.6 | 0.351 | 31.0 | 0.0
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.0 | 85.0 | 66.0 | 29.00000 | 155.548223 | 26.6 | 0.351 | 31.0 | 0.0
    |'
- en: '| 2 | 8.0 | 183.0 | 64.0 | 29.15342 | 155.548223 | 23.3 | 0.672 | 32.0 | 1.0
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8.0 | 183.0 | 64.0 | 29.15342 | 155.548223 | 23.3 | 0.672 | 32.0 | 1.0
    |'
- en: '| 3 | 1.0 | 89.0 | 66.0 | 23.00000 | 94.000000 | 28.1 | 0.167 | 21.0 | 0.0
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.0 | 89.0 | 66.0 | 23.00000 | 94.000000 | 28.1 | 0.167 | 21.0 | 0.0
    |'
- en: '| 4 | 0.0 | 137.0 | 40.0 | 35.00000 | 168.000000 | 43.1 | 2.288 | 33.0 | 1.0
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.0 | 137.0 | 40.0 | 35.00000 | 168.000000 | 43.1 | 2.288 | 33.0 | 1.0
    |'
- en: 'Let''s check in on our `plasma_glucose_concentration` column to make sure that
    the values are still filled in with the same mean we calculated manually earlier:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As a final check, our imputed DataFrame should have no missing values, as shown
    in the following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Great! The `Imputer` helps a great deal with the menial task of imputing data
    values into missing slots. Let''s try imputing a few types of values and seeings
    its effect on our KNN model for classification. Let''s first try an even simpler
    imputing method. Let''s re-fill in the empty values with zeros:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: If we had left the values as `0`, our accuracy would have been lower than dropping
    the rows with missing values. Our goal now is to obtain a machine learning pipeline
    that can learn from all `768` rows, but can perform better than the model that
    learned from only 392 rows. This means that the accuracy to beat is 0.745, or
    74.5%.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Imputing values in a machine learning pipeline
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we wish to transfer the `Imputer` over to a production-ready machine learning
    pipeline, we will need to talk briefly about the topic of pipelines.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines in machine learning
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about *pipelines* in machine learning, we are usually talking about
    the fact that data is only passed through a learning algorithm raw, but also through
    a variety of preprocessing steps and even multiple learning algorithms before
    the final output is interpreted. Because it is so common to have several steps
    and transformation and prediction within a single machine learning pipeline, scikit-learn
    has a built-in module for building these pipelines.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines are especially important because it is actually *improper* to not
    use a pipeline when imputing values using the `Imputer` class. This is because
    the goal of the learning algorithm is to generalize the patterns in the training
    set in order to apply those patterns to the testing set. If we impute values for
    the entire dataset before splitting the set and applying learning algorithms,
    then we are cheating and our models are not actually learning any patterns. To
    visualize this concept, let's take a single train test split, a potential one
    of many during a cross-validation training phase.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a copy of a single column of the `Pima` dataset in order to emphasize
    our point a bit more drastically, and also import a single train test split module
    from scikit-learn:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let''s take a single split. But before doing so, we will impute the average
    value of `X` in the entire dataset, using the following code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, let''s fit a KNN model to the training and testing sets:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note that we aren't implementing any grid searching here, just a plain fit.
    We see that our model boasts a 66% accuracy rate (not great, but that's not the
    point). The important thing to note here is that both the training and the testing
    set of `X` were imputed using the mean of the entire `X` matrix. This is in direct
    violation of a core tenet of the machine learning procedure. We cannot assume
    that we know the mean of the entire dataset when predicting the test set's response
    values. Simply put, our KNN model is using information gained from the testing
    set to fit to the training set. This is a big red flag.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: For more information on pipelines and why we need to use them, check out *The
    Principles of Data Science* (available from Packt Publishing) at [https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s do it properly by first taking the mean of the training set and
    then using the mean of the training set to fill in values of the testing set.
    Again, this procedure tests the model''s ability to use the average value of training
    data to predict unseen test cases:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, instead of taking the mean of the entire `X` matrix, we will properly
    only do so for the training set and use that value to fill in missing cells in **both**
    the training and test set:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Finally, let''s score a KNN model on the *same* dataset, but imputed correctly,
    as shown in the following code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This is of course a much lower accuracy, but at least it is a more honest representation
    of the model''s ability to learn from the training set''s features and apply what
    it learned to unseen and withheld testing data. Scikit-learn''s pipelines make
    this entire process much easier by giving structure and order to the steps of
    our machine learning pipelines. Let''s take a look at a code block of how to use
    the scikit-learn `Pipeline` with the `Imputer`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'A few new things to note. First off, our `Pipeline` has two steps:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: An `Imputer` with `strategy= mean`
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A classifier of type KNN
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Secondly, we had to redefine our `param` dict for the grid search as we have
    to specify exactly to which step of the pipeline the `n_neighbors` parameter belongs:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Other than that, everything is normal and proper. The `Pipeline` class will
    handle most of the procedure for us. It will handle properly imputing values from
    several training sets and using them to fill in missing values in the test set,
    properly testing the KNN''s ability to generalize patterns in the data and finally
    outputting the best performing model, having an accuracy of 73%, just beneath
    our goal to beat of .745\. Now that we have this syntax down, let''s try the entire
    procedure again, but with a slight modification, as shown in the following code:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Here, the only difference is that our pipeline will try a different strategy
    of imputing **median**, wherein the missing values will be filled in the median
    of the remaining values. It is important to reiterate that our accuracies may
    be lower than the model's fit on the dropped rows, but they were made on more
    than twice the size of the dataset with missing values! And they were still better
    than leaving them all at 0, as the data was originally presented to us.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a minute to recap the scores we have gotten so far using our proper
    pipelines:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pipeline description** | **# rows model learned from** | **Cross-validated
    accuracy** |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| **drop missing-valued rows** | **392** | **.74489** |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| Impute values with 0 | 768 | .7304 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| Impute values with mean of column | 768 | .7318 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| Impute values with median of column | 768 | .7357 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: If we go by accuracy alone, it appears the best procedure is to drop the missing-values
    rows. Perhaps using the `Pipeline` and `Imputer` features alone in scikit-learn
    is not enough. We still would like to see comparable (if not better) performance
    coming from all 768 rows if possible. In order to achieve this, let's try introducing
    a brand new feature engineering trick, standardization and normalization.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Standardization and normalization
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up until now, we have dealt with identifying the types of data as well as the
    ways data can be missing and finally, the ways we can fill in missing data. Now,
    let''s talk about how we can manipulate our data (and our features) in order to
    enhance our machine pipelines further. So far, we have tried four different ways
    of manipulating our dataset, and the best cross-validated accuracy we have achieved
    with a KNN model is .745\. If we look back at some of the EDA we have previously
    done, we will notice something about our features:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, let''s use a standard histogram to see the distribution across all nine columns,
    as follows, specifying a figure size:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding code produces the following output:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a54e3566-6330-47f0-be03-ef5f2629f983.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'Nice, but notice anything off? Every single column has a vastly different mean,
    min, max, and standard deviation. This is also obvious through the describe method,
    using the following code:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output is as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **times _pregnant** | **plasma _glucose'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: _concentration** | **diastolic_ blood_pressure** | **triceps _thickness** |
    **serum _insulin** | **bmi** | **pedigree _function** | **age** | **onset _diabetes**
    |
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '| **count** | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000
    | 768.000000 | 768.000000 | 768.000000 | 768.000000 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| **mean** | 3.845052 | 121.686763 | 72.405184 | 29.153420 | 155.548223 | 32.457464
    | 0.471876 | 33.240885 | 0.348958 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| **std** | 3.369578 | 30.435949 | 12.096346 | 8.790942 | 85.021108 | 6.875151
    | 0.331329 | 11.760232 | 0.476951 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| **min** | 0.000000 | 44.000000 | 24.000000 | 7.000000 | 14.000000 | 18.200000
    | 0.078000 | 21.000000 | 0.000000 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| **25%** | 1.000000 | 99.750000 | 64.000000 | 25.000000 | 121.500000 | 27.500000
    | 0.243750 | 24.000000 | 0.000000 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| **50%** | 3.000000 | 117.000000 | 72.202592 | 29.153420 | 155.548223 | 32.400000
    | 0.372500 | 29.000000 | 0.000000 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| **75%** | 6.000000 | 140.250000 | 80.000000 | 32.000000 | 155.548223 | 36.600000
    | 0.626250 | 41.000000 | 1.000000 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| **max** | 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 |
    67.100000 | 2.420000 | 81.000000 | 1.000000 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: 'But why does this matter? Well, some machine learning models rely on learning
    methods that are affected greatly by the *scale* of the data, meaning that if
    we have a column such as `diastolic_blood_pressure` that lives between 24 and
    122, and an age column between 21 and 81, then our learning algorithms will not
    learn optimally. To really see the differences in scales, let''s invoke two optional
    parameters in the histogram method, `sharex` and `sharey`, so that we can see
    each graph on the same scale as every other graph, using the following code:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The preceding code produces the following output:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19ac589d-4953-4cba-9484-ba41aec2a6ab.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: It is quite clear that our data all lives on vastly different scales. Data engineers
    have options on how to deal with this problem in our machine learning pipelines
    that are under a family of operations called **normalization**. Normalization
    operations are meant to align and transform both columns and rows to a consistent
    set of rules. For example, a common form of normalization is to transform all
    quantitative columns to be between a consistent and static range of values (for
    example all values must be between 0 and 1). We may also impose mathematical rules
    such as, *all columns must have the same mean and standard deviation* so that
    they appear nicely on the same histogram (unlike the pima histograms we computed
    recently). Normalization techniques are meant to *level the playing field* of
    data by ensuring that all rows and columns are treated equally under the eyes
    of machine learning.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'We will focus on three methods of data normalization:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Z-score standardization
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Min-max scaling
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Row normalization
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two deal specifically with altering features in place, while the third
    option actually manipulates the rows of the data, but is still just as pertinent
    as the first two.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Z-score standardization
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most common of the normalization techniques, **z-score standardization**, utilizes
    a very simple statistical idea of a z-score. The output of a z-score normalization
    are features that are re-scaled to have a mean of zero and a standard deviation
    of one. By doing this, by re-scaling our features to have a uniform mean and variance
    (square of standard deviation), then we allow models such as KNN to learn optimally
    and not skew towards larger scaled features. The formula is simple: for every
    column, we replace the cells with the following value:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '*z = (x - μ) / σ*'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '*z* is our new value (z-score)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is the previous value of the cell'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*μ* is the mean of the column'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*σ* is the standard deviation of the columns'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see an example by scaling the `plasma_glucose_concentration` column
    in our dataset:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'And now let''s manually compute z-scores for every value in our column, using
    the following code:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We see that every single value in the column will be replaced, and also notice
    how now some of them are negative. This is because the resulting values represent
    a *distance* from the mean. So, if a value originally was below the mean of the
    column, the resulting z-score will be negative. Of course, in scikit-learn, we
    have built-in objects to help us out, as shown in the following code:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let''s try it out, shown as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The preceding code produces the following output:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d15f3d1-4137-4f8e-b6db-b0a921abdca1.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see the distribution of the column before doing anything. Now,
    let''s apply a z-score scaling, as shown in the following code:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We can see that after we apply our scaler to the column, or mean drops to zero
    and our standard deviation is one. Furthermore, if we take a look at the distribution
    of values across our recently scaled data:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output is as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ba0f92b-7fc8-46f6-92e2-848bc109359b.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
- en: 'We will notice that our *x* axis is now much more constrained, while our *y *axis
    is unchanged. Also note that the shape of the data is unchanged entirely. Let''s
    take a look at the histograms of our DataFrame after we apply a z-score transformation
    on every single column. When we do this, the `StandardScaler` will compute a mean
    and standard deviation for every column separately:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The preceding code produces the following output:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e944967-c803-4a98-b357-ad8766bcdc18.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: 'Notice that our *x* axes are all now much more constrained across the entire
    dataset. Let''s now plug a `StandardScaler` into our machine learning pipeline
    from before:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Note a few things here. We included a new set of parameters to grid search,
    namely the strategy of imputing missing values. Now, I am looking for the best
    combination of strategy and number of neighbors in our KNN attached to a z-score
    scaling and our result is .742, which so far is the closest score we have gotten
    to our goal of beating .745, and this pipeline is learning from all 768 rows.
    Let's now look at another method of column normalization.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: The min-max scaling method
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Min-max scaling **is similar to z-score normalization in that it will replace
    every value in a column with a new value using a formula. In this case, that formula
    is:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '*m = (x -x[min]) / (x[max] -x[min])*'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '*m* is our new value'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is the original cell value'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x[min]* is the minimum value of the column'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x[max]* is the maximum value of the column'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using this formula, we will see that the values of each column will now be
    between zero and one. Let''s take a look at an example using a built-in scikit-learn
    module:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Here is the output of our `describe` method:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **times _pregnant** | **plasma _glucose'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: _concentration** | **diastolic _blood
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: _pressure** | **triceps _thickness** | **serum _insulin** | **bmi** | **pedigree
    _function** | **age** | **onset _diabetes** |
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '| **count** | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000
    | 768.000000 | 768.000000 | 768.000000 | 768.000000 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| **mean** | 0.226180 | 0.501205 | 0.493930 | 0.240798 | 0.170130 | 0.291564
    | 0.168179 | 0.204015 | 0.348958 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '| **std** | 0.198210 | 0.196361 | 0.123432 | 0.095554 | 0.102189 | 0.140596
    | 0.141473 | 0.196004 | 0.476951 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '| **min** | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000
    | 0.000000 | 0.000000 | 0.000000 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| **25%** | 0.058824 | 0.359677 | 0.408163 | 0.195652 | 0.129207 | 0.190184
    | 0.070773 | 0.050000 | 0.000000 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '| **50%** | 0.176471 | 0.470968 | 0.491863 | 0.240798 | 0.170130 | 0.290389
    | 0.125747 | 0.133333 | 0.000000 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| **75%** | 0.352941 | 0.620968 | 0.571429 | 0.271739 | 0.170130 | 0.376278
    | 0.234095 | 0.333333 | 1.000000 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| **max** | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000
    | 1.000000 | 1.000000 | 1.000000 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: 'Notice how the `min` are all zeros and the `max` values are all ones. Note
    further that the standard deviations are now all very very small, a side effect
    of this type of scaling. This can hurt some models as it takes away weight from
    outliers. Let''s plug our new normalization technique into our pipeline:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Woah, this is the best accuracy we've gotten so far working with the missing
    data and using all of the 768 original rows in the dataset. It seems as though
    the min-max scaling is helping our KNN a great deal! Wonderful; let's try a third
    type of normalization, and this time let's move away from normalizing columns
    and onto normalizing rows instead.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: The row normalization method
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our final normalization method works row-wise instead of column-wise. Instead
    of calculating statistics on each column, mean, min, max, and so on, the row normalization
    technique will ensure that each row of data has a *unit norm*, meaning that each
    row will be the same vector length. Imagine if each row of data belonged to an
    n-dimensional space; each one would have a vector norm, or length. Another way
    to put it is if we consider every row to be a vector in space:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '*x = (x[1], x[2], ..., x[n])*'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'Where 1, 2, ..., n in the case of Pima would be 8, 1 for each feature (not
    including the response), the norm would be calculated as:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '*||x|| = √(x[1]^(2 + )x[2]^(2 +) ... + x[n]²)*'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: This is called the **L-2 Norm**. Other types of norms exist, but we will not
    get into that in this text. Instead, we are concerned with making sure that every
    single row has the same norm. This comes in handy, especially when working with
    text data or clustering algorithms.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'Before doing anything, let''s see the average norm of our mean-imputed matrix,
    using the following code:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now, let''s bring in our row-normalizer, as shown in the following code:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'After normalizing, we see that every single row has a norm of one now. Let''s
    see how this method fares in our pipeline:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Ouch, not great, but worth a try. Now that we have seen three different methods
    of data normalization, let's put it all together and see how we did on this dataset.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many learning algorithms that are affected by the scale of data.
    Here is a list of some popular learning algorithms that are affected by the scale
    of data:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: KNN-due to its reliance on the Euclidean Distance
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Means Clustering - same reasoning as KNN
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression, SVM, neural networks—if you are using gradient descent
    to learn weights
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis—eigen vectors will be skewed towards larger columns
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After dealing with a variety of problems with our dataset, from identifying
    missing values hidden as zeros, imputing missing values, and normalizing data
    at different scales, it''s time to put all of our scores together into a single
    table and see what combination of feature engineering did the best:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pipeline description** | **# rows model learned from** | **Cross-validated
    accuracy** |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| Drop missing-valued rows | 392 | .7449 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| Impute values with 0 | 768 | .7304 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| Impute values with mean of column | 768 | .7318 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| Impute values with median of column | 768 | .7357 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| Z-score normalization with median imputing | 768 | .7422 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| **Min-max normalization with mean imputing** | **768** | **.7461** |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| Row-normalization with mean imputing | 768 | .6823 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: It seems as though we were finally able to get a better accuracy by applying
    mean imputing and min-max normalization to our dataset and still use all `768`
    available rows. Great!
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature improvement is about recognizing areas of issue and improvement in our
    data and figuring out which cleaning methods will be the most effective. Our main
    takeaway should be to look at data with the eyes of a data scientist. Instead
    of immediately dropping rows/columns with problems, we should think about the
    best ways of fixing these problems. More often than not, our machine learning
    performance will thank us in the end.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: This chapter contains several ways of dealing with issues with our quantitative
    columns. The next chapter will deal with the imputing of categorical columns,
    as well as how to introduce brand new features into the mix from existing features.
    We will be working with scikit-learn pipelines with a mix of numerical and categorical
    columns to really expand the types of data we can work with.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
