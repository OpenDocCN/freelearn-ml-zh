- en: Feature Improvement - Cleaning Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last two chapters, we have gone from talking about a basic understanding
    of feature engineering and how it can be used to enhance our machine learning
    pipelines to getting our hands dirty with datasets and evaluating and understanding
    the different types of data that we can encounter in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be using what we learned and taking things a step further
    and begin to change the datasets that we work with. Specifically, we will be starting
    to *clean* and *augment* our datasets. By cleaning, we will generally be referring
    to the process of altering columns and rows already given to us. By augmenting,
    we will generally refer to the processes of removing columns and adding columns
    to datasets. As always, our goal in all of these processes is to enhance our machine
    learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following chapters, we will be:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying missing values in data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing harmful data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputing (filling in) these missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing/standardizing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing brand new features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting (removing) features manually and automatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using mathematical matrix computations to transform datasets to different dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These methods will help us develop a better sense of which features are important
    within our data. In this chapter, we will be diving deeper into the first four
    methods, and leave the other three for future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying missing values in data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first method of identifying missing values is to give us a better understanding
    of how to work with real-world data. Often, data can have missing values due to
    a variety of reasons, for example with survey data, some observations may not
    have been recorded. It is important for us to analyze our data, and get a sense
    of what the missing values are so we can decide how we want to handle missing
    values for our machine learning. To start, let's dive into a dataset that we will
    be interested in for the duration of this chapter, the `Pima Indian Diabetes Prediction`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The Pima Indian Diabetes Prediction dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This dataset is available on the UCI Machine Learning Repository at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes).'
  prefs: []
  type: TYPE_NORMAL
- en: From the main website, we can learn a few things about this publicly available
    dataset. We have nine columns and 768 instances (rows). The dataset is primarily
    used for predicting the onset of diabetes within five years in females of Pima
    Indian heritage over the age of 21 given medical details about their bodies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is meant to correspond with a binary (2-class) classification machine
    learning problem. Namely, the answer to the question, *will this person develop
    diabetes within five years?* The column names are provided as follows (in order):'
  prefs: []
  type: TYPE_NORMAL
- en: Number of times pregnant
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diastolic blood pressure (mm Hg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Triceps skinfold thickness (mm)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2-Hour serum insulin measurement (mu U/ml)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Body mass index (weight in kg/(height in m)²)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diabetes pedigree function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Age (years)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Class variable (zero or one)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The goal of the dataset is to be able to predict the final column of `class`
    variable, which predicts if the patient has developed diabetes, using the other
    eight features as inputs to a machine learning function.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two very important reasons we will be working with this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: We will have to work with missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the features we will be working with will be quantitative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first point makes more sense for now as a reason, because the point of this
    chapter is to deal with missing values. As far as only choosing to work with quantitative
    data, this will only be the case for this chapter. We do not have enough tools
    to deal with missing values in categorical columns. In the next chapter, when
    we talk about feature construction, we will deal with this procedure.
  prefs: []
  type: TYPE_NORMAL
- en: The exploratory data analysis (EDA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To identify our missing values we will begin with an EDA of our dataset. We
    will be using some useful python packages, pandas and numpy, to store our data
    and make some simple calculations as well as some popular visualization tools
    to see what the distribution of our data looks like. Let''s begin and dive into
    some code. First, we will do some imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will import our tabular data through a CSV, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `head` method allows us to see the first few rows in our dataset. The output
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **6** | **148** | **72** | **35** | **0** | **33.6** | **0.627** | **50**
    | **1** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 8 | 183 | 64 | 0 | 0 | 23.3 | 0.627 | 32 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 5 | 116 | 74 | 0 | 0 | 25.6 | 0.201 | 30 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Something''s not right here, there''s no column names. The CSV must not have
    the names for the columns built into the file. No matter, we can use the data
    source''s website to fill this in, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, using the `head` method again, we can see our columns with the appropriate
    headers. The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **times_pregnant** | **plasma_glucose_concentration** | **diastolic_blood_pressure**
    | **triceps_thickness** | **serum_insulin** | **bmi** | **pedigree_function**
    | **age** | **onset_diabetes** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Much better, now we can use the column names to do some basic stats, selecting,
    and visualizations. Let''s first get our null accuracy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If our eventual goal is to exploit patterns in our data in order to predict
    the onset of diabetes, let us try to visualize some of the differences between
    those that developed diabetes and those that did not. Our hope is that the histogram
    will reveal some sort of pattern, or obvious difference in values between the
    classes of prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/408febaa-777c-4619-a4ca-76cb171ced80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It seems that this histogram is showing us a pretty big difference between
    `plasma_glucose_concentration` between the two prediction classes. Let''s show
    the same histogram style for multiple columns as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code will give us the following three histograms.
    The first one is show us the distributions of **bmi** for the two class variables
    (non-diabetes and diabetes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b05cd7b9-0005-471d-84da-5e8d3d8320e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next histogram to appear will shows us again contrastingly different distributions
    between a feature across our two class variables. This time we are looking at
    **diastolic_blood_pressure**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1eb28f27-66a3-4265-af6d-13dbbfef4a20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final graph will show **plasma_glucose_concentration** differences between
    our two class variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5887f0e-8069-41af-9e70-884e0be4a426.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can definitely see some major differences simply by looking at just a few
    histograms. For example, there seems to be a large jump in `plasma_glucose_concentration`
    for those who will eventually develop diabetes. To solidify this, perhaps we can
    visualize a linear correlation matrix in an attempt to quantify the relationship
    between these variables. We will use the visualization tool, seaborn, which we
    imported at the beginning of this chapter for our correlation matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the correlation matrix of our dataset. This is showing us the
    correlation amongst the different columns in our `Pima` dataset. The output is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4f2b0c8-3429-4b62-9ccc-039296923e95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This correlation matrix is showing a strong correlation between `plasma_glucose_concentration` and `onset_diabetes`.
    Let''s take a further look at the numerical correlations for the `onset_diabetes` column,
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will explore the powers of correlation in a later [Chapter 4](430d621e-7ce6-48c0-9990-869e82a0d0c6.xhtml), *Feature
    Construction*, but for now we are using **exploratory data analysis** (**EDA**)
    to hint at the fact that the `plasma_glucose_concentration` column will be an
    important factor in our prediction of the onset of diabetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on to more important matters at hand, let''s see if we are missing any
    values in our dataset by invoking the built-in `isnull()` method of the pandas
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! We don''t have any missing values. Let''s go on to do some more EDA,
    first using the `shape` method to see the number of rows and columns we are working
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirming we have `9` columns (including our response variable) and `768`
    data observations (rows). Now, let''s take a peak at the percentage of patients
    who developed diabetes, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us that `65%` of the patients did not develop diabetes, while about
    35% did. We can use a nifty built-in method of a pandas DataFrame called `describe`
    to look at some basic descriptive statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **times_pregnant** | **plasma_glucose _concentration** | **diastolic_
    blood_pressure** | **triceps _thickness** | **serum _insulin** | **bmi** | **pedigree
    _function** | **age** | **onset _diabetes** |'
  prefs: []
  type: TYPE_TB
- en: '| **count** | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000
    | 768.000000 | 768.000000 | 768.000000 | 768.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **mean** | 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578
    | 0.471876 | 33.240885 | 0.348958 |'
  prefs: []
  type: TYPE_TB
- en: '| **std** | 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160
    | 0.331329 | 11.760232 | 0.476951 |'
  prefs: []
  type: TYPE_TB
- en: '| **min** | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000
    | 0.078000 | 21.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **25%** | 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000
    | 0.243750 | 24.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **50%** | 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000
    | 0.372500 | 29.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **75%** | 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000
    | 0.626250 | 41.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **max** | 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 |
    67.100000 | 2.420000 | 81.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: 'This shows us quite quickly some basic stats such as mean, standard deviation,
    and some different percentile measurements of our data. But, notice that the minimum
    value of the `BMI` column is `0`. That is medically impossible; there must be
    a reason for this to happen. Perhaps the number zero has been encoded as a missing
    value instead of the None value or a missing cell. Upon closer inspection, we
    see that the value 0 appears as a minimum value for the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`times_pregnant`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plasma_glucose_concentration`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`diastolic_blood_pressure`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`triceps_thickness`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`serum_insulin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bmi`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onset_diabetes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Because zero is a class for `onset_diabetes` and 0 is actually a viable number
    for `times_pregnant`, we may conclude that the number 0 is encoding missing values
    for:'
  prefs: []
  type: TYPE_NORMAL
- en: '`plasma_glucose_concentration`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`diastolic_blood_pressure`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`triceps_thickness`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`serum_insulin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bmi`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we actually do having missing values! It was obviously not luck that we
    happened upon the zeros as missing values, we knew it beforehand. As a data scientist,
    you must be ever vigilant and make sure that you know as much about the dataset
    as possible in order to find missing values encoded as other symbols. Be sure
    to read any and all documentation that comes with open datasets in case they mention
    any missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'If no documentation is available, some common values used instead of missing
    values are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**0** (for numerical values)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unknown** or **Unknown** (for categorical variables)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**?** (for categorical variables)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we have five columns in which missing values exist, so now we get to talk
    about how to deal with them, in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with missing values in a dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with data, one of the most common issues a data scientist will
    run into is the problem of missing data. Most commonly, this refers to empty cells
    (row/column intersections) where the data just was not acquired for whatever reason.
    This can become a problem for many reasons; notably, when applying learning algorithms
    to data with missing values, most (not all) algorithms are not able to cope with
    missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, data scientists and machine learning engineers have many tricks
    and tips on how to deal with this problem. Although there are many variations
    of methodologies, the two major ways in which we can deal with missing data are:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove rows with missing values in them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impute (fill in) missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each method will **clean** our dataset to a point where a learning algorithm
    can handle it, but each method will have its pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: 'First off, before we go too far, let''s get rid of the zeros and replace them
    all with the value `None` in Python. This way, our `fillna` and `dropna` methods
    will work correctly. We could manually replace all zeros with None, each column
    at a time, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We could repeat this procedure for every column with incorrectly labeled missing
    values, or we could use a `for` loop and a built-in `replace` method to speed
    things up, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now if we try to count the number of missing values using the `isnull`
    method, we should start to see missing values being counted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, looking at the first few rows of our dataset, we get the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **times_pregnant** | **plasma_glucose_concentration** | **diastolic_blood_pressure**
    | **triceps_thickness** | **serum_insulin** | **bmi** | **pedigree_function**
    | **age** | **onset_diabetes** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 6 | 148 | 72 | 35 | NaN | 33.6 | 0.627 | 50 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 1 | 85 | 66 | 29 | NaN | 26.6 | 0.351 | 31 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 8 | 183 | 64 | None | NaN | 23.3 | 0.672 | 32 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 1 | 89 | 66 | 23 | NaN | 28.1 | 0.167 | 21 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 0 | 137 | 40 | 35 | NaN | 43.1 | 2.288 | 33 | 1 |'
  prefs: []
  type: TYPE_TB
- en: OK, this is starting to make much more sense. We can now see that five columns
    have missing values, and the degree to which data is missing is staggering. Some
    columns, such as `plasma_glucose_concentration`, are only missing five values,
    but look at `serum_insulin`; that column is missing almost half of its values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have missing values properly injected into our dataset instead
    of the `0` placeholders that the dataset originally came with, our exploratory
    data analysis will be more accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **times_pregnant** | **serum_insulin** | **pedigree_function** | **age**
    | **onset_diabetes** |'
  prefs: []
  type: TYPE_TB
- en: '| **count** | 768.000000 | 394.000000 | 768.000000 | 768.000000 | 768.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| **mean** | 3.845052 | 155.548223 | 0.471876 | 33.240885 | 0.348958 |'
  prefs: []
  type: TYPE_TB
- en: '| **std** | 3.369578 | 118.775855 | 0.331329 | 11.760232 | 0.476951 |'
  prefs: []
  type: TYPE_TB
- en: '| **min** | 0.000000 | 14.000000 | 0.078000 | 21.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **25%** | 1.000000 | 76.250000 | 0.243750 | 24.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **50%** | 3.000000 | 125.000000 | 0.372500 | 29.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **75%** | 6.000000 | 190.000000 | 0.626250 | 41.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **max** | 17.000000 | 846.000000 | 2.420000 | 81.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: 'Notice that the `describe` method doesn''t include columns with missing values,
    which while not ideal, doesn''t mean that we cannot obtain them by computing the
    mean and standard deviation of the specific columns, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Let us move on to our two ways of dealing with missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Removing harmful rows of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Probably the most common and easiest of our two options for dealing with missing
    data is to simply remove the observations that have any missing values. By doing
    so, we will be left with only the **complete** data points with all data filled
    in. We can obtain a new DataFrame by invoking the `dropna` method in pandas, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, of course, the obvious problem here is that we lost a few rows. To check
    how many exactly, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Wow! We lost about 51% of the rows from the original dataset, and if we think
    about this from a machine learning perspective, even though now we have clean
    data with everything filled in, we aren't really learning as much as we possibly
    could be by ignoring over half of the data's observations. That's like a doctor
    trying to understand how heart attacks happen, ignoring over half of their patients
    coming in for check-ups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform some more EDA on the dataset and compare the statistics about
    the data from before and after dropping the missing-values rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the same split after we dropped the rows, using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that the binary response stayed relatively the same during the drastic
    transformation of our dataset. Let''s take a look at the *shape* of our data by
    comparing the average values of columns before and after the transformation, using
    the `pima.mean` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And now for the same averages after dropping the rows, using the `pima_dropped.mean()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a better look at how these numbers changed, let''s create a new chart
    that visualizes the percentages changed on average for each column. First, let''s
    create a table of the percent changes in the average values of each column, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And now let''s visualize these changes as a bar chart, using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/512ce682-ec85-47c9-af15-0620dc6cec7d.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the number of `times_pregnant` variable average fell 14% after
    dropping missing values, which is a big change! The `pedigree_function` also rose
    11%, another big leap. We can see how dropping rows (observations) severely affects
    the shape of the data and we should try to retain as much data as possible. Before
    moving on to the next method of dealing with missing values, let's introduce some
    actual machine learning into the mix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block (which we will go over line by line in a moment) will
    become a very familiar code block in this book. It describes and achieves a single
    fitting of a machine learning model over a variety of parameters in the hope of
    obtaining the best possible model, given the features at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'OK, let''s go through this line by line. First, we have two new import statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be utilizing scikit-learn''s **K-Nearest Neighbors** (**KNN**) classification
    model, as well as a grid search module that will automatically find the best combo
    of parameters (using brute force) for the KNN model that best fits our data with
    respect to cross-validated accuracy. Next, let''s take our dropped dataset (with
    the missing-valued rows removed) and create an `X` and a `y` variable for our
    predictive model. Let''s start with our `X` (our feature matrix):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Ouch, it''s already obvious that there''s a major problem with this approach.
    Our machine learning algorithm is going to be fitting and learning from far fewer
    data observations than which we started with. Let''s now create our `y` (response
    series):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our `X` and our `y` variable, we can introduce the variables
    and instances we need to successfully run a **grid search**. We will set the number
    of `params` to try at seven to keep things simple in this chapter. For every data
    cleaning and feature engineering method we try (dropping rows, filling in data),
    we will try to fit the best KNN as having somewhere between one and seven neighbors
    complexity. We can set this model up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will instantiate a grid search module, as shown in the following code,
    and fit it to our feature matrix and response variable. Once we do so, we will
    print out the best accuracy as well as the best parameter used to learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: So, it seems that using seven neighbors as its parameter, our KNN model was
    able to achieve a `74.4%` accuracy (better than our null accuracy of around 65%),
    but keep in mind that it is only learning from 49% of the original data, so who
    knows how it could have done on the rest of the data.
  prefs: []
  type: TYPE_NORMAL
- en: This is our first real look into using machine learning in this book. We will
    be assuming that the reader does have basic familiarity with machine learning
    as well as statistical procedures such as cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: It's probably pretty clear that while dropping the *dirty* rows may not exactly
    be feature engineering, it is still a data cleaning technique we can utilize to
    help sanitize our machine learning pipeline inputs. Let's try for a slightly harder
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Imputing the missing values in data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imputing is the more involved method of dealing with missing values. By *imputing, *we
    refer to the act of filling in missing data values with numerical quantities that
    are somehow ascertained from existing knowledge/data. We have a few options on
    how we can fill in these missing values, the most common of them being filling
    in missing values with the average value for the rest of the column, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the five rows where `plasma_glucose_concentration` is missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use the built-in `fillna` method to replace all of the `None` values
    with the mean value of the rest of the `plasma_glucose_concentration` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'And if we check out the column, we should see that the `None` values have been
    replaced by `121.68`, the mean value we obtained earlier for this column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! But this can be cumbersome. Let''s use a module in the scikit-learn
    preprocessing class (the documentation can be found at [http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing))
    called the `Imputer` (aptly named). We can import it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As with most scikit-learn modules, we have a few new parameters to play with,
    but I will focus on the major one, called the `strategy`. We can define how we
    want to impute values into our dataset by setting this parameter. For quantitative
    values, we can use the built-in mean and median strategies to fill in values with
    either quantity. To use the `Imputer`, we must first instantiate the object, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can call the `fit_transform` method to create a new object, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We do have a small issue to deal with. The output of the Imputer is not a pandas
    DataFrame, but rather the output is of type **NumPy** array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be easily dealt with, as we could just cast the array as a DataFrame,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at our new DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **times_pregnant** | **plasma_glucose_concentration** | **diastolic_blood_pressure**
    | **triceps_thickness** | **serum_insulin** | **bmi** | **pedigree_function**
    | **age** | **onset_diabetes** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 6.0 | 148.0 | 72.0 | 35.00000 | 155.548223 | 33.6 | 0.627 | 50.0 | 1.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.0 | 85.0 | 66.0 | 29.00000 | 155.548223 | 26.6 | 0.351 | 31.0 | 0.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 8.0 | 183.0 | 64.0 | 29.15342 | 155.548223 | 23.3 | 0.672 | 32.0 | 1.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.0 | 89.0 | 66.0 | 23.00000 | 94.000000 | 28.1 | 0.167 | 21.0 | 0.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.0 | 137.0 | 40.0 | 35.00000 | 168.000000 | 43.1 | 2.288 | 33.0 | 1.0
    |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s check in on our `plasma_glucose_concentration` column to make sure that
    the values are still filled in with the same mean we calculated manually earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'As a final check, our imputed DataFrame should have no missing values, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! The `Imputer` helps a great deal with the menial task of imputing data
    values into missing slots. Let''s try imputing a few types of values and seeings
    its effect on our KNN model for classification. Let''s first try an even simpler
    imputing method. Let''s re-fill in the empty values with zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: If we had left the values as `0`, our accuracy would have been lower than dropping
    the rows with missing values. Our goal now is to obtain a machine learning pipeline
    that can learn from all `768` rows, but can perform better than the model that
    learned from only 392 rows. This means that the accuracy to beat is 0.745, or
    74.5%.
  prefs: []
  type: TYPE_NORMAL
- en: Imputing values in a machine learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we wish to transfer the `Imputer` over to a production-ready machine learning
    pipeline, we will need to talk briefly about the topic of pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about *pipelines* in machine learning, we are usually talking about
    the fact that data is only passed through a learning algorithm raw, but also through
    a variety of preprocessing steps and even multiple learning algorithms before
    the final output is interpreted. Because it is so common to have several steps
    and transformation and prediction within a single machine learning pipeline, scikit-learn
    has a built-in module for building these pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines are especially important because it is actually *improper* to not
    use a pipeline when imputing values using the `Imputer` class. This is because
    the goal of the learning algorithm is to generalize the patterns in the training
    set in order to apply those patterns to the testing set. If we impute values for
    the entire dataset before splitting the set and applying learning algorithms,
    then we are cheating and our models are not actually learning any patterns. To
    visualize this concept, let's take a single train test split, a potential one
    of many during a cross-validation training phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a copy of a single column of the `Pima` dataset in order to emphasize
    our point a bit more drastically, and also import a single train test split module
    from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s take a single split. But before doing so, we will impute the average
    value of `X` in the entire dataset, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s fit a KNN model to the training and testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note that we aren't implementing any grid searching here, just a plain fit.
    We see that our model boasts a 66% accuracy rate (not great, but that's not the
    point). The important thing to note here is that both the training and the testing
    set of `X` were imputed using the mean of the entire `X` matrix. This is in direct
    violation of a core tenet of the machine learning procedure. We cannot assume
    that we know the mean of the entire dataset when predicting the test set's response
    values. Simply put, our KNN model is using information gained from the testing
    set to fit to the training set. This is a big red flag.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on pipelines and why we need to use them, check out *The
    Principles of Data Science* (available from Packt Publishing) at [https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s do it properly by first taking the mean of the training set and
    then using the mean of the training set to fill in values of the testing set.
    Again, this procedure tests the model''s ability to use the average value of training
    data to predict unseen test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, instead of taking the mean of the entire `X` matrix, we will properly
    only do so for the training set and use that value to fill in missing cells in **both**
    the training and test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s score a KNN model on the *same* dataset, but imputed correctly,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This is of course a much lower accuracy, but at least it is a more honest representation
    of the model''s ability to learn from the training set''s features and apply what
    it learned to unseen and withheld testing data. Scikit-learn''s pipelines make
    this entire process much easier by giving structure and order to the steps of
    our machine learning pipelines. Let''s take a look at a code block of how to use
    the scikit-learn `Pipeline` with the `Imputer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'A few new things to note. First off, our `Pipeline` has two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: An `Imputer` with `strategy= mean`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A classifier of type KNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Secondly, we had to redefine our `param` dict for the grid search as we have
    to specify exactly to which step of the pipeline the `n_neighbors` parameter belongs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Other than that, everything is normal and proper. The `Pipeline` class will
    handle most of the procedure for us. It will handle properly imputing values from
    several training sets and using them to fill in missing values in the test set,
    properly testing the KNN''s ability to generalize patterns in the data and finally
    outputting the best performing model, having an accuracy of 73%, just beneath
    our goal to beat of .745\. Now that we have this syntax down, let''s try the entire
    procedure again, but with a slight modification, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Here, the only difference is that our pipeline will try a different strategy
    of imputing **median**, wherein the missing values will be filled in the median
    of the remaining values. It is important to reiterate that our accuracies may
    be lower than the model's fit on the dropped rows, but they were made on more
    than twice the size of the dataset with missing values! And they were still better
    than leaving them all at 0, as the data was originally presented to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a minute to recap the scores we have gotten so far using our proper
    pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pipeline description** | **# rows model learned from** | **Cross-validated
    accuracy** |'
  prefs: []
  type: TYPE_TB
- en: '| **drop missing-valued rows** | **392** | **.74489** |'
  prefs: []
  type: TYPE_TB
- en: '| Impute values with 0 | 768 | .7304 |'
  prefs: []
  type: TYPE_TB
- en: '| Impute values with mean of column | 768 | .7318 |'
  prefs: []
  type: TYPE_TB
- en: '| Impute values with median of column | 768 | .7357 |'
  prefs: []
  type: TYPE_TB
- en: If we go by accuracy alone, it appears the best procedure is to drop the missing-values
    rows. Perhaps using the `Pipeline` and `Imputer` features alone in scikit-learn
    is not enough. We still would like to see comparable (if not better) performance
    coming from all 768 rows if possible. In order to achieve this, let's try introducing
    a brand new feature engineering trick, standardization and normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Standardization and normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up until now, we have dealt with identifying the types of data as well as the
    ways data can be missing and finally, the ways we can fill in missing data. Now,
    let''s talk about how we can manipulate our data (and our features) in order to
    enhance our machine pipelines further. So far, we have tried four different ways
    of manipulating our dataset, and the best cross-validated accuracy we have achieved
    with a KNN model is .745\. If we look back at some of the EDA we have previously
    done, we will notice something about our features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use a standard histogram to see the distribution across all nine columns,
    as follows, specifying a figure size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a54e3566-6330-47f0-be03-ef5f2629f983.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Nice, but notice anything off? Every single column has a vastly different mean,
    min, max, and standard deviation. This is also obvious through the describe method,
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **times _pregnant** | **plasma _glucose'
  prefs: []
  type: TYPE_NORMAL
- en: _concentration** | **diastolic_ blood_pressure** | **triceps _thickness** |
    **serum _insulin** | **bmi** | **pedigree _function** | **age** | **onset _diabetes**
    |
  prefs: []
  type: TYPE_NORMAL
- en: '| **count** | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000
    | 768.000000 | 768.000000 | 768.000000 | 768.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **mean** | 3.845052 | 121.686763 | 72.405184 | 29.153420 | 155.548223 | 32.457464
    | 0.471876 | 33.240885 | 0.348958 |'
  prefs: []
  type: TYPE_TB
- en: '| **std** | 3.369578 | 30.435949 | 12.096346 | 8.790942 | 85.021108 | 6.875151
    | 0.331329 | 11.760232 | 0.476951 |'
  prefs: []
  type: TYPE_TB
- en: '| **min** | 0.000000 | 44.000000 | 24.000000 | 7.000000 | 14.000000 | 18.200000
    | 0.078000 | 21.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **25%** | 1.000000 | 99.750000 | 64.000000 | 25.000000 | 121.500000 | 27.500000
    | 0.243750 | 24.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **50%** | 3.000000 | 117.000000 | 72.202592 | 29.153420 | 155.548223 | 32.400000
    | 0.372500 | 29.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **75%** | 6.000000 | 140.250000 | 80.000000 | 32.000000 | 155.548223 | 36.600000
    | 0.626250 | 41.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **max** | 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 |
    67.100000 | 2.420000 | 81.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: 'But why does this matter? Well, some machine learning models rely on learning
    methods that are affected greatly by the *scale* of the data, meaning that if
    we have a column such as `diastolic_blood_pressure` that lives between 24 and
    122, and an age column between 21 and 81, then our learning algorithms will not
    learn optimally. To really see the differences in scales, let''s invoke two optional
    parameters in the histogram method, `sharex` and `sharey`, so that we can see
    each graph on the same scale as every other graph, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19ac589d-4953-4cba-9484-ba41aec2a6ab.png)'
  prefs: []
  type: TYPE_IMG
- en: It is quite clear that our data all lives on vastly different scales. Data engineers
    have options on how to deal with this problem in our machine learning pipelines
    that are under a family of operations called **normalization**. Normalization
    operations are meant to align and transform both columns and rows to a consistent
    set of rules. For example, a common form of normalization is to transform all
    quantitative columns to be between a consistent and static range of values (for
    example all values must be between 0 and 1). We may also impose mathematical rules
    such as, *all columns must have the same mean and standard deviation* so that
    they appear nicely on the same histogram (unlike the pima histograms we computed
    recently). Normalization techniques are meant to *level the playing field* of
    data by ensuring that all rows and columns are treated equally under the eyes
    of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will focus on three methods of data normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: Z-score standardization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Min-max scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Row normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two deal specifically with altering features in place, while the third
    option actually manipulates the rows of the data, but is still just as pertinent
    as the first two.
  prefs: []
  type: TYPE_NORMAL
- en: Z-score standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most common of the normalization techniques, **z-score standardization**, utilizes
    a very simple statistical idea of a z-score. The output of a z-score normalization
    are features that are re-scaled to have a mean of zero and a standard deviation
    of one. By doing this, by re-scaling our features to have a uniform mean and variance
    (square of standard deviation), then we allow models such as KNN to learn optimally
    and not skew towards larger scaled features. The formula is simple: for every
    column, we replace the cells with the following value:'
  prefs: []
  type: TYPE_NORMAL
- en: '*z = (x - μ) / σ*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*z* is our new value (z-score)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is the previous value of the cell'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*μ* is the mean of the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*σ* is the standard deviation of the columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see an example by scaling the `plasma_glucose_concentration` column
    in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'And now let''s manually compute z-scores for every value in our column, using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that every single value in the column will be replaced, and also notice
    how now some of them are negative. This is because the resulting values represent
    a *distance* from the mean. So, if a value originally was below the mean of the
    column, the resulting z-score will be negative. Of course, in scikit-learn, we
    have built-in objects to help us out, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try it out, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d15f3d1-4137-4f8e-b6db-b0a921abdca1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see the distribution of the column before doing anything. Now,
    let''s apply a z-score scaling, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that after we apply our scaler to the column, or mean drops to zero
    and our standard deviation is one. Furthermore, if we take a look at the distribution
    of values across our recently scaled data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ba0f92b-7fc8-46f6-92e2-848bc109359b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will notice that our *x* axis is now much more constrained, while our *y *axis
    is unchanged. Also note that the shape of the data is unchanged entirely. Let''s
    take a look at the histograms of our DataFrame after we apply a z-score transformation
    on every single column. When we do this, the `StandardScaler` will compute a mean
    and standard deviation for every column separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e944967-c803-4a98-b357-ad8766bcdc18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that our *x* axes are all now much more constrained across the entire
    dataset. Let''s now plug a `StandardScaler` into our machine learning pipeline
    from before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Note a few things here. We included a new set of parameters to grid search,
    namely the strategy of imputing missing values. Now, I am looking for the best
    combination of strategy and number of neighbors in our KNN attached to a z-score
    scaling and our result is .742, which so far is the closest score we have gotten
    to our goal of beating .745, and this pipeline is learning from all 768 rows.
    Let's now look at another method of column normalization.
  prefs: []
  type: TYPE_NORMAL
- en: The min-max scaling method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Min-max scaling **is similar to z-score normalization in that it will replace
    every value in a column with a new value using a formula. In this case, that formula
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*m = (x -x[min]) / (x[max] -x[min])*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*m* is our new value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is the original cell value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x[min]* is the minimum value of the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x[max]* is the maximum value of the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using this formula, we will see that the values of each column will now be
    between zero and one. Let''s take a look at an example using a built-in scikit-learn
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of our `describe` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **times _pregnant** | **plasma _glucose'
  prefs: []
  type: TYPE_NORMAL
- en: _concentration** | **diastolic _blood
  prefs: []
  type: TYPE_NORMAL
- en: _pressure** | **triceps _thickness** | **serum _insulin** | **bmi** | **pedigree
    _function** | **age** | **onset _diabetes** |
  prefs: []
  type: TYPE_NORMAL
- en: '| **count** | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000
    | 768.000000 | 768.000000 | 768.000000 | 768.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **mean** | 0.226180 | 0.501205 | 0.493930 | 0.240798 | 0.170130 | 0.291564
    | 0.168179 | 0.204015 | 0.348958 |'
  prefs: []
  type: TYPE_TB
- en: '| **std** | 0.198210 | 0.196361 | 0.123432 | 0.095554 | 0.102189 | 0.140596
    | 0.141473 | 0.196004 | 0.476951 |'
  prefs: []
  type: TYPE_TB
- en: '| **min** | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000
    | 0.000000 | 0.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **25%** | 0.058824 | 0.359677 | 0.408163 | 0.195652 | 0.129207 | 0.190184
    | 0.070773 | 0.050000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **50%** | 0.176471 | 0.470968 | 0.491863 | 0.240798 | 0.170130 | 0.290389
    | 0.125747 | 0.133333 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **75%** | 0.352941 | 0.620968 | 0.571429 | 0.271739 | 0.170130 | 0.376278
    | 0.234095 | 0.333333 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| **max** | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000
    | 1.000000 | 1.000000 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: 'Notice how the `min` are all zeros and the `max` values are all ones. Note
    further that the standard deviations are now all very very small, a side effect
    of this type of scaling. This can hurt some models as it takes away weight from
    outliers. Let''s plug our new normalization technique into our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Woah, this is the best accuracy we've gotten so far working with the missing
    data and using all of the 768 original rows in the dataset. It seems as though
    the min-max scaling is helping our KNN a great deal! Wonderful; let's try a third
    type of normalization, and this time let's move away from normalizing columns
    and onto normalizing rows instead.
  prefs: []
  type: TYPE_NORMAL
- en: The row normalization method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our final normalization method works row-wise instead of column-wise. Instead
    of calculating statistics on each column, mean, min, max, and so on, the row normalization
    technique will ensure that each row of data has a *unit norm*, meaning that each
    row will be the same vector length. Imagine if each row of data belonged to an
    n-dimensional space; each one would have a vector norm, or length. Another way
    to put it is if we consider every row to be a vector in space:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x = (x[1], x[2], ..., x[n])*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where 1, 2, ..., n in the case of Pima would be 8, 1 for each feature (not
    including the response), the norm would be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*||x|| = √(x[1]^(2 + )x[2]^(2 +) ... + x[n]²)*'
  prefs: []
  type: TYPE_NORMAL
- en: This is called the **L-2 Norm**. Other types of norms exist, but we will not
    get into that in this text. Instead, we are concerned with making sure that every
    single row has the same norm. This comes in handy, especially when working with
    text data or clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before doing anything, let''s see the average norm of our mean-imputed matrix,
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s bring in our row-normalizer, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'After normalizing, we see that every single row has a norm of one now. Let''s
    see how this method fares in our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Ouch, not great, but worth a try. Now that we have seen three different methods
    of data normalization, let's put it all together and see how we did on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many learning algorithms that are affected by the scale of data.
    Here is a list of some popular learning algorithms that are affected by the scale
    of data:'
  prefs: []
  type: TYPE_NORMAL
- en: KNN-due to its reliance on the Euclidean Distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Means Clustering - same reasoning as KNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression, SVM, neural networks—if you are using gradient descent
    to learn weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis—eigen vectors will be skewed towards larger columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After dealing with a variety of problems with our dataset, from identifying
    missing values hidden as zeros, imputing missing values, and normalizing data
    at different scales, it''s time to put all of our scores together into a single
    table and see what combination of feature engineering did the best:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pipeline description** | **# rows model learned from** | **Cross-validated
    accuracy** |'
  prefs: []
  type: TYPE_TB
- en: '| Drop missing-valued rows | 392 | .7449 |'
  prefs: []
  type: TYPE_TB
- en: '| Impute values with 0 | 768 | .7304 |'
  prefs: []
  type: TYPE_TB
- en: '| Impute values with mean of column | 768 | .7318 |'
  prefs: []
  type: TYPE_TB
- en: '| Impute values with median of column | 768 | .7357 |'
  prefs: []
  type: TYPE_TB
- en: '| Z-score normalization with median imputing | 768 | .7422 |'
  prefs: []
  type: TYPE_TB
- en: '| **Min-max normalization with mean imputing** | **768** | **.7461** |'
  prefs: []
  type: TYPE_TB
- en: '| Row-normalization with mean imputing | 768 | .6823 |'
  prefs: []
  type: TYPE_TB
- en: It seems as though we were finally able to get a better accuracy by applying
    mean imputing and min-max normalization to our dataset and still use all `768`
    available rows. Great!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature improvement is about recognizing areas of issue and improvement in our
    data and figuring out which cleaning methods will be the most effective. Our main
    takeaway should be to look at data with the eyes of a data scientist. Instead
    of immediately dropping rows/columns with problems, we should think about the
    best ways of fixing these problems. More often than not, our machine learning
    performance will thank us in the end.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter contains several ways of dealing with issues with our quantitative
    columns. The next chapter will deal with the imputing of categorical columns,
    as well as how to introduce brand new features into the mix from existing features.
    We will be working with scikit-learn pipelines with a mix of numerical and categorical
    columns to really expand the types of data we can work with.
  prefs: []
  type: TYPE_NORMAL
