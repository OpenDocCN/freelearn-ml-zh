<html><head></head><body><div class="chapter" title="Chapter&#xA0;13.&#xA0;Case Study Exercise II"><div class="titlepage"><div><div><h1 class="title"><a id="ch13"/>Chapter 13. Case Study Exercise II</h1></div></div></div><p>You already solved a real-world classification problem in the previous chapter. In this chapter, we will present a new problem in the form of another case study exercise and we will solve it with a very simple solution. This exercise represents a regression problem.</p><p>Like in the previous chapter, it won't present a step-by-step guide with all the details; however, it will provide pointers so that you can solve the problem. This chapter assumes that you have successfully completed all the previous chapters, or already know about Azure ML.</p><div class="section" title="Problem definition and scope"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec78"/>Problem definition and scope</h1></div></div></div><p>The problem is taken from one of the <span class="strong"><strong>Kaggle</strong></span> machine learning competitions, where they exposed the datasets with a bit of information and asked the contestants to make a prediction from a test dataset using machine learning techniques. It is about the <span class="strong"><strong>Africa Soil Property Prediction Challenge</strong></span>. The training dataset contains different measurements <a id="id429" class="indexterm"/>of the soil and it is expected that the contestants are able to predict values for five properties of the soil (target variables): <span class="strong"><strong>SOC</strong></span>, <span class="strong"><strong>pH</strong></span>, <span class="strong"><strong>Ca</strong></span>, <span class="strong"><strong>P</strong></span>, and <span class="strong"><strong>Sand</strong></span>. Full details regarding this can be found at <a class="ulink" href="https://www.kaggle.com/c/afsis-soil-properties/">https://www.kaggle.com/c/afsis-soil-properties/</a>.</p><p>For the purpose of <a id="id430" class="indexterm"/>this case study, we will choose just one target variable named P and ignore the others. Once you are able to predict one target variable, you can follow the same approach and predict others. So, you should try predicting all the variables by yourself.</p></div></div>
<div class="section" title="The dataset"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec79"/>The dataset</h1></div></div></div><p>You can download <a id="id431" class="indexterm"/>the dataset and find the description at <a class="ulink" href="https://www.kaggle.com/c/afsis-soil-properties/data">https://www.kaggle.com/c/afsis-soil-properties/data</a>.</p><p>The dataset has been explained in the following term list, as found at the preceding web link:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>PIDN</strong></span>: This is the <a id="id432" class="indexterm"/>unique soil sample identifier.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>SOC</strong></span>: This refers <a id="id433" class="indexterm"/>to soil organic carbon.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>pH</strong></span>: These <a id="id434" class="indexterm"/>are the pH values.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Ca</strong></span>: This is the <a id="id435" class="indexterm"/>Mehlich-3 extractable calcium.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>P</strong></span>: This is the <a id="id436" class="indexterm"/>Mehlich-3 extractable phosphorus.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Sand</strong></span>: This is the <a id="id437" class="indexterm"/>sand content.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>m7497.96 - m599.76</strong></span>: There are 3,578 mid-infrared absorbance measurements. For example, the "m7497.96" column is the absorbance at wavenumber 7497.96 cm-1. We suggest you remove spectra CO2 bands, which <a id="id438" class="indexterm"/>are in the region m2379.76 to m2352.76, but you do not have to.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Depth</strong></span>: This is the depth of the soil sample (this has two categories: "<span class="strong"><strong>Topsoil</strong></span>" and "<span class="strong"><strong>Subsoil</strong></span>"). They have also included some potential spatial predictors from remote sensing data sources. Short variable descriptions of different terms are <a id="id439" class="indexterm"/>provided below and additional descriptions can be found in AfSIS data. The data has been mean centered and scaled.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>BSA</strong></span>: These are the average long-term <span class="strong"><strong>Black Sky Albedo</strong></span> measurements from the <span class="strong"><strong>MODIS</strong></span> <a id="id440" class="indexterm"/>satellite images (here, BSAN = near-infrared, BSAS = shortwave, and BSAV = visible).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>CTI</strong></span>: This <a id="id441" class="indexterm"/>refers to the <span class="strong"><strong>Compound Topographic Index</strong></span> calculated from the <span class="strong"><strong>Shuttle Radar Topography Mission</strong></span> elevation data.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>ELEV</strong></span>: This refers to the <a id="id442" class="indexterm"/>Shuttle Radar Topography Mission elevation data.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>EVI</strong></span>: This is the average <a id="id443" class="indexterm"/>long-term <span class="strong"><strong>Enhanced Vegetation Index</strong></span> from the MODIS satellite images.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>LST</strong></span>: This is the average long-term <span class="strong"><strong>Land Surface Temperatures</strong></span> from the MODIS satellite <a id="id444" class="indexterm"/>images (here, LSTD = day time temperature and LSTN = night time temperature).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Ref</strong></span>: This refers to the <a id="id445" class="indexterm"/>average long-term <span class="strong"><strong>Reflectance</strong></span> measurements from the MODIS satellite images (here, Ref1 = blue, Ref2 = red, Ref3 = near-infrared, and Ref7 = mid-infrared).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Reli</strong></span>: This is the <a id="id446" class="indexterm"/>topographic <span class="strong"><strong>Relief</strong></span> calculated from the Shuttle Radar Topography mission elevation data.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>TMAP and TMFI</strong></span>: These <a id="id447" class="indexterm"/>refer to the average long-term <span class="strong"><strong>Tropical Rainfall Monitoring Mission</strong></span> data (here, TMAP = mean annual precipitation and TMFI = modified fournier index).</li></ul></div><p>Download the training dataset (<a class="ulink" href="https://www.kaggle.com/c/afsis-soil-properties/download/train.zip">https://www.kaggle.com/c/afsis-soil-properties/download/train.zip</a>). Note that you may have to create an account to download the dataset. Upload the dataset to ML Studio (for this, refer to <a class="link" href="ch04.html" title="Chapter 4. Getting Data in and out of ML Studio">Chapter 4</a>, <span class="emphasis"><em>Getting Data in and out of ML Studio</em></span>, to find details on how to upload a dataset from your local machine to ML Studio).</p></div>
<div class="section" title="Data exploration and preparation"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec80"/>Data exploration and preparation</h1></div></div></div><p>Create a new <a id="id448" class="indexterm"/>experiment in ML Studio. Drag the uploaded dataset to the canvas and <a id="id449" class="indexterm"/>visualize it. As you can see, it has 1157 rows and 3600 columns. Usually, the data exposed in a Kaggle competition is already cleaned, which saves you the effort of data cleansing, such as dealing with missing values. In ML Studio, you can't see all the columns and rows. There are 3,578 columns that have mid-infrared absorbance measurements and these entire column names start with the letter 'm'. You may like to separate them out. To do so, you can use an <span class="strong"><strong>Execute Python Script</strong></span> module with the following code, where the inline comments explain the lines of code. For this, refer to <a class="link" href="ch10.html" title="Chapter 10. Extensibility with R and Python">Chapter 10</a>, <span class="emphasis"><em>Extensibility with R and Python</em></span>, to find the details on how to integrate a Python/R script inside ML Studio:</p><div class="informalexample"><pre class="programlisting">def azureml_main(dataframe1 = None, dataframe2 = None):
    #Get all the columns
    cols = dataframe1.columns.tolist()
    #Select columns with name starting with letter 'm'
    dataframe1=dataframe1[[col for col in cols if col.startswith('m')]]
    #Return the modified dataset
    return dataframe1</pre></div><p>The model in progress may appear as follows:</p><div class="mediaobject"><img src="graphics/0792EN_13_02.jpg" alt="Data exploration and preparation"/></div><p>Alternatively, you can also use an <span class="strong"><strong>Execute R Script</strong></span> module with R code to achieve the same.</p><p>These extracted 3,578 columns are almost impossible to visualize and will take a long time to process in a model, especially when you use the <span class="strong"><strong>Sweep Parameters</strong></span> module. It would be worthwhile condensing them into a few lines, so they are easier to process. The <span class="strong"><strong>Principal Component </strong></span><a id="id450" class="indexterm"/>
<span class="strong"><strong>Analysis</strong></span> module would be of great help, as it would extract a <a id="id451" class="indexterm"/>given number of the most relevant features from the given features. The <a id="id452" class="indexterm"/>
<span class="strong"><strong>Principal Component Analysis</strong></span> (PCA) is a popular technique that takes a feature set and computes a new feature set with reduced dimensionality or a lesser number of features or components; with most of the information contained in the original feature set. The <span class="strong"><strong>Principal Component Analysis</strong></span> module present in ML Studio takes <span class="strong"><strong>Number of dimensions to reduce to</strong></span> as input, where you can specify the desired, low number of features.</p><p>You may try <span class="strong"><strong>10</strong></span> components (for the <span class="strong"><strong>Number of dimensions to reduce to</strong></span> option) as its parameter, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/0792EN_13_06.jpg" alt="Data exploration and preparation"/></div><p>You may use another <span class="strong"><strong>Execute Python Script</strong></span> module or <span class="strong"><strong>Execute R Script</strong></span> to extract other relevant columns, which are all the columns excluding those that start with 'm' and other target variables (because we are only interested in P). You may also like to exclude PIDN, which is the unique soil sample identifier. Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_13_03.jpg" alt="Data exploration and preparation"/></div><p>The Python  code for <a id="id453" class="indexterm"/>the same is as follows:</p><div class="informalexample"><pre class="programlisting">def azureml_main(dataframe1 = None, dataframe2 = None):
    #Get all the columns 
    cols = dataframe1.columns.tolist()
    #Select columns with name starting with letter 'm'
    mCols=dataframe1[[col for col in cols if col.startswith('m')]]
    #List of other columns to be excluded 
    exCols=['PIDN', 'Ca',  'pH', 'SOC', 'Sand']
    #Drop all the columns with name starting with letter 'm'
    dataframe1.drop(mCols, axis=1, inplace=True)
    #Drop other columns - 'PIDN', 'Ca',  'pH', 'SOC', 'Sand'
    dataframe1.drop(exCols,axis=1, inplace=True)
    #Return the modified dataset 
    return dataframe1</pre></div><p>Use the two sets of extracted <a id="id454" class="indexterm"/>columns to combine, and make, one dataset using the <span class="strong"><strong>Add Columns</strong></span> module. By now, you should have a reduced feature set, but you still have to find the most relevant ones. Unnecessary data or noise may reduce the predictive power of a model, so should be excluded. The <span class="strong"><strong>Filter Based Feature Selection</strong></span> module can identify the most important features in a dataset. You may try the same with a different number of desired features as parameters, and evaluate the performance of the overall model.</p><div class="mediaobject"><img src="graphics/0792EN_13_04.jpg" alt="Data exploration and preparation"/></div><p>Before you proceed in <a id="id455" class="indexterm"/>building the model, you need to prepare train, validate, and test <a id="id456" class="indexterm"/>dataset. Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_13_05.jpg" alt="Data exploration and preparation"/></div></div>
<div class="section" title="Model development"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec81"/>Model development</h1></div></div></div><p>After preparing <a id="id457" class="indexterm"/>your data, you are not sure which model or regression algorithm would perform well for the problem at hand. Because the target variable P is continuous, you know that it's a regression problem. So, it would be worthwhile trying different algorithms and choosing the best one. You may use the <span class="strong"><strong>Sweep Parameters</strong></span> module to obtain the optimum parameters for the algorithm. You need to pass three inputs to the <span class="strong"><strong>Sweep Parameters</strong></span> module: the untrained algorithm, training dataset, and validation dataset. Use the <span class="strong"><strong>Score</strong></span> modules to score the test data. Use an <span class="strong"><strong>Evaluate</strong></span> module to compare the two models with the scored data.</p><p>You should try <a id="id458" class="indexterm"/>different algorithms to choose the best one. The following figure is just for your reference, which shows four algorithms.</p><div class="mediaobject"><img src="graphics/0792EN_13_01_.jpg" alt="Model development"/></div><p>Run the model and <a id="id459" class="indexterm"/>find out which algorithm performs the best for you.</p></div>
<div class="section" title="Model deployment"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec82"/>Model deployment</h1></div></div></div><p>After you are happy with a <a id="id460" class="indexterm"/>particular model, save it as a trained model and then prepare an experiment for a web service and proceed to deploy the model. Refer to <a class="link" href="ch11.html" title="Chapter 11. Publishing a Model as a Web Service">Chapter 11</a>, <span class="emphasis"><em>Publishing a Model as a Web Service</em></span>, to find the details on how to deploy a model to the staging environment and test it visually in ML Studio.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec83"/>Summary</h1></div></div></div><p>In this last chapter, you solved another real-world problem. You started with understanding the problem and then, acquired the necessary data. After initial data exploration, you realized that the data has a large number of columns, so you used Python script modules to first split the data into two sets of features, and then used the PCA algorithm to get a reduced set of features. Then, you used the <span class="strong"><strong>Filter Based Feature Selection</strong></span> module, which can identify most of the important features from the reduced dataset. To select the right model, you tried different algorithms and trained them with optimum parameters using the <span class="strong"><strong>Sweep Parameters</strong></span> modules. Finally, you selected the model and proceeded to publish it as a web service.</p></div></body></html>