- en: Object Segmentation Using CNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN进行对象分割
- en: 'Throughout the chapters in this book, we have seen various machine learning
    models, each progressively increasing their perceptual abilities. By this, I mean
    that we were first introduced to a model capable of classifying a single object
    present in an image. Then came a model that was able to classify not only multiple
    objects but also their corresponding bounding boxes. In this chapter, we continue
    this progression by introducing semantic segmentation, in other words, being able
    to assign each pixel to a specific class, as shown in the following figure:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的各个章节中，我们看到了各种机器学习模型，每个模型都逐渐增强了它们的感知能力。我的意思是，我们首先接触到的模型能够对图像中存在的单个对象进行分类。然后是能够不仅对多个对象进行分类，还能对它们相应的边界框进行分类的模型。在本章中，我们通过引入语义分割继续这一进程，也就是说，能够将每个像素分配到特定的类别，如下面的图所示：
- en: '![](img/bd0261d3-7c65-446d-8af5-1071f134fef6.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bd0261d3-7c65-446d-8af5-1071f134fef6.png)'
- en: Source: http://cocodataset.org/#explore
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：http://cocodataset.org/#explore
- en: This allows for a greater understanding of the scene and, therefore, opportunities
    for more intelligible interfaces and services. But this is not the main focus
    of this chapter. In this chapter, we will use semantic segmentation to create
    an image effects application as a way to demonstrate imperfect predictions. We'll
    be using this to motivate a discussion on one of the most important aspects of
    designing and building machine learning (or artificial intelligence) interfaces—dealing
    with probabilistic, or imperfect, outcomes from models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得对场景的理解更加深入，因此，为更易于理解的界面和服务提供了机会。但这并不是本章的重点。在本章中，我们将使用语义分割来创建一个图像效果应用程序，以此展示不完美的预测。我们将利用这一点来激发对设计构建机器学习（或人工智能）界面最重要的一个方面——处理模型的不确定或错误结果的讨论。
- en: 'By the end of this chapter, you will have:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将：
- en: An understanding semantic of segmentation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对语义分割的理解
- en: Built an intuitive understanding of how it is achieved (learned)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立了对如何实现（学习）的直观理解
- en: Learned how it can be applied in a novel way for real life applications by building
    an action shot photo effects application
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过构建动作镜头照片效果应用程序，学习了如何以新颖的方式应用于现实生活应用
- en: Gained appreciation and awareness for dealing with probability outcomes from
    machine learning models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对处理机器学习模型产生的概率结果有了欣赏和认识
- en: Let's begin by better understanding what semantic segmentation is and get an
    intuitive understanding of how it is achieved.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先更好地理解语义分割是什么，以及它是如何实现的直观理解。
- en: Classifying pixels
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 像素分类
- en: 'As we have already discussed, the desired output of a model performing semantic
    segmentation is an image with each of its pixels assigned a label of its most
    likely class (or even a specific instance of a class). Throughout this book, we
    have also seen that layers of a deep neural network learn features that are activated
    when a corresponding input that satisfies the particular feature is detected. We
    can visualize these activations using a technique called **class activation maps**
    (**CAMs**). The output produces a heatmap of class activations over the input
    image; the heatmap consists of a matrix of scores associated with a specific class,
    essentially giving us a spatial map of how intensely the input region activates
    a specified class. The following figure shows an output of a CAM visualization
    for the class cat. Here, you can see that the heatmap portrays what the model
    considers important features (and therefore regions) for this class:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所讨论的，执行语义分割的模型所期望的输出是一个图像，其中每个像素都被分配了其最可能的类别标签（甚至是一个特定类别的实例）。在整个本书中，我们也看到了深度神经网络层学习在检测到满足特定特征的相应输入时被激活的特征。我们可以使用称为**类别激活图**（**CAMs**）的技术来可视化这些激活。输出产生了一个输入图像上类别激活的热图；热图由与特定类别相关的分数矩阵组成，本质上为我们提供了一个空间图，显示了输入区域如何强烈地激活指定的类别。以下图显示了为猫类别的一个CAM可视化输出。在这里，你可以看到热图描绘了模型认为对这个类别重要的特征（以及区域）：
- en: '![](img/6d4c72c3-ac9e-430e-af95-f5ff4c284d3b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6d4c72c3-ac9e-430e-af95-f5ff4c284d3b.png)'
- en: 'The preceding figure was produced using the implementation described in the
    paper *Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization*
    by R. Selvaraju. The approach is to take the output feature map of a convolutional
    layer and weigh every channel in that feature map by the gradient of the class.
    For more details of how it works, please refer to the original paper: [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图是使用R. Selvaraju在论文《Grad-CAM：通过基于梯度的定位从深度网络中获取视觉解释》中描述的实现生成的。方法是取卷积层的输出特征图，并按类别的梯度为该特征图中的每个通道加权。有关其工作原理的更多详细信息，请参阅原始论文：[https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)。
- en: Early attempts of semantic segmentation were made using slightly adapted classification
    models such as VGG and Alexnet, but they only produced coarse approximations.
    This can be seen in the preceding figure and is largely due to the network using
    repetitive pooling layers, which results in loss of spatial information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 早期对语义分割的尝试使用了略微修改的分类模型，如VGG和Alexnet，但它们只能产生粗略的近似。这可以从前面的图中看出，这主要是因为网络使用了重复的池化层，这导致了空间信息的丢失。
- en: 'U-Net is one architecture that addresses this; it consists of an **encoder**
    and **decoder**, with the addition of **shortcuts** between the two to preserve
    spatial information. Released in 2015 by *O. Ronneberger*, *P. Fischer*, and *T.
    Brox* for biomedical image segmentation, it has since become one of the go-to
    architectures for segmentation due to its effectiveness (it can be trained on
    a small dataset) and performance. The following figure shows the modified U-Net
    we will be using in this chapter:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net是解决这一问题的架构之一；它由一个**编码器**和一个**解码器**组成，并在两者之间添加了**捷径**以保留空间信息。由O. Ronneberger、P.
    Fischer和T. Brox于2015年发布，用于生物医学图像分割，由于其有效性和性能，它已成为分割的首选架构之一。以下图显示了本章我们将使用的修改后的U-Net：
- en: '![](img/dc50598c-dd4e-480f-be82-5bff251864ed.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dc50598c-dd4e-480f-be82-5bff251864ed.png)'
- en: U-Net is one of many architectures for semantic segmentation. Sasank Chilamkurthy's
    post *A 2017 Guide to Semantic Segmentation with Deep Learning* provides a great
    overview and comparison of the most popular architectures, available at [http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review](http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review).
    For further details on U-Net, please refer to the original paper mentioned earlier.
    It is available at [https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net是用于语义分割的众多架构之一。Sasank Chilamkurthy的文章《2017年深度学习语义分割指南》提供了一个关于最流行架构的精彩概述和比较，可在[http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review](http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review)找到。有关U-Net的更多详细信息，请参阅前面提到的原始论文。它可在[https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf)找到。
- en: On the left in the preceding figure, we have the full network used in this chapter's
    project, and on the right we have an extract of blocks used in the encoder and
    decoder parts of the network. As a reminder, the focus of this book is on applying
    machine learning rather than the details of the models themselves. So for this
    reason, we won't be delving into the details, but there are a few interesting
    and useful things worth pointing out.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中左侧，我们有本章项目使用的完整网络，而在右侧，我们有网络编码器和解码器部分使用的块提取。作为提醒，本书的重点是应用机器学习，而不是模型本身的细节。因此，我们不会深入探讨细节，但有一些有趣且有用的事情值得指出。
- en: The first is the general structure of the network; it consists of an encoder and
    decoder.The encoder's role is to capture context. The decoder's task is to use
    this context and features from the corresponding shortcuts to project its understanding
    onto pixel space, to get a dense and precise classification. It's a common practice
    to bootstrap the encoder using an architecture and weights from a trained classification
    model, such as VGG16\. This not only speeds up training but also is likely to
    increase performance as it brings with it a depth (pun intended) of understanding
    of images it has been trained on, which is typically from a larger dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是网络的一般结构；它由一个编码器和一个解码器组成。编码器的角色是捕捉上下文。解码器的任务是使用这个上下文和来自相应快捷方式的特征，将其理解投影到像素空间，以获得密集和精确的分类。通常的做法是使用训练好的分类模型（如VGG16）的架构和权重来启动编码器。这不仅加快了训练速度，而且很可能会提高性能，因为它带来了对训练过的图像的深度（有意为之）理解，这通常来自更大的数据集。
- en: Another point worth highlighting is those shortcuts between the encoder and
    decoder. As mentioned previously, they are used to preserve spatial information
    outputted from convolutional layers from each encoding block before being lost
    when its downsampled using max pooling. This information is used to assist the
    model in precise localization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得强调的点是在编码器和解码器之间的快捷方式。如前所述，它们用于在最大池化下采样之前保留从每个编码块输出的卷积层的空间信息。这些信息用于帮助模型进行精确定位。
- en: It's the first time in this book that we have seen an upsampling layer. As the
    name implies, it's a technique that upsamples your image (or feature maps) to
    a higher resolution. One of the easiest ways is to use the same techniques we
    use with image upsampling, that is, rescaling the input to a desired size and
    calculating the values at each point using an interpolation method, such as bilinear
    interpolation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这本书中第一次看到上采样层。正如其名所示，这是一种将你的图像（或特征图）上采样到更高分辨率的技巧。其中一种最简单的方法是使用我们用于图像上采样的相同技术，即，将输入缩放到期望的大小，并使用插值方法（如双线性插值）计算每个点的值。
- en: 'Lastly, I wanted to bring to your attention the input and outputs of the model.
    The model is expecting a 448 x 448 color image as its input and outputs a 448
    x 448 x 1 (single channel) matrix. If you inspect the architecture, you will notice
    that the last layer is a sigmoid activation, where a sigmoid function is typically
    used for binary classification, which is precisely what we are doing here. Typically,
    you would perform multi-class classification for semantic segmentation tasks,
    in which case you would replace the sigmoid activation with a softmax activation.
    An example commonly used when introducing semantic segmentation is scene understanding
    for self-driving cars. The following is an example of a labeled scene from Cambridge
    University''s Motion-based Segmentation and Recognition Dataset where each color
    represents a different class:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想引起你的注意，即模型的输入和输出。模型期望输入一个448 x 448的彩色图像，并输出一个448 x 448 x 1（单通道）矩阵。如果你检查架构，你会注意到最后一层是sigmoid激活，其中sigmoid函数通常用于二元分类，这正是我们在这里所做的事情。通常，你会对语义分割任务执行多类分类，在这种情况下，你会将sigmoid激活替换为softmax激活。在介绍语义分割时常用到的例子是自动驾驶汽车的场景理解。以下是从剑桥大学基于运动的分割和识别数据集中标记的场景示例，其中每种颜色代表一个不同的类别：
- en: '![](img/269955f2-4095-4b7a-8b25-b96fe313555a.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/269955f2-4095-4b7a-8b25-b96fe313555a.png)'
- en: Source: http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/
- en: But in this example, a binary classifier is sufficient, which will become apparent
    as we go into the details of the project. However, I wanted to highlight it here
    as the architecture will scale to multi-class classification by simply swapping
    the last layer with a softmax activation and changing the loss function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这个例子中，一个二元分类器就足够了，这一点在我们深入了解项目细节时会变得明显。然而，我想在这里强调它，因为架构只需通过将最后一层与softmax激活交换并更改损失函数，就可以扩展到多类分类。
- en: You have thus seen the architecture we will be using in this chapter. Let's
    now look at how we will use it and the data used to train the model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你已经看到了我们将在这章中使用的架构。现在让我们看看我们将如何使用它以及用于训练模型的数据。
- en: Data to drive the desired effect – action shots
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 驱动所需效果的数据——动作镜头
- en: 'Now would be a good time to introduce the photo effect we want to create in
    this chapter. The effect, as I know it, is called an **action shot.** It''s essentially
    a still photograph that shows someone (or something) in motion, probably best
    illustrated with an image - like the one shown here:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是介绍本章中想要创建的摄影效果的好时机。据我所知，这种效果被称为**动作快照**。它本质上是一张静态照片，展示了某人（或某物）在运动中，可能最好用一张图片来展示——就像这里展示的这张一样：
- en: '![](img/3cc3c762-cb35-4a7c-8882-06f407243dee.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3cc3c762-cb35-4a7c-8882-06f407243dee.png)'
- en: As previously mentioned, the model we used in this chapter performs binary (or
    single-class) classification. This simplification, using a binary classifier instead
    of a multi-class classifier, has been driven by the intended use that is just
    segmenting people from the background. Similar to any software project, you should
    strive for simplicity where you can.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，本章中使用的模型执行的是二元（或单类别）分类。使用二元分类器而不是多类别分类器的这种简化，是由仅从背景中分割人物的预期用途所驱动的。与任何软件项目一样，您应该努力在可能的情况下保持简单。
- en: 'To extract people, we need a model to learn how to recognize people and their
    associated pixels. For this, we need a dataset consisting of images of people
    and corresponding images with those pixels of the persons labeled—and lots of
    them. Unlike datasets for classification, datasets for object segmentation are
    not so common nor as vast. This is understandable given the additional effort
    that would be required to label such a dataset. Some common datasets for object
    segmentation, and ones that are considered for this chapter, include:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取人物，我们需要一个模型来学习如何识别人物及其相关的像素。为此，我们需要一个包含人物图像及其相应像素标记图像的数据集——而且数量要很多。与用于分类的数据集相比，用于对象分割的数据集并不常见，也不那么庞大。考虑到标记此类数据集所需的额外努力，这是可以理解的。一些常见的对象分割数据集，以及在本章中考虑的数据集包括：
- en: '**PASCAL VOC**: A dataset with 9,993 labeled images across 20 classes. You
    can find the dataset at [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PASCAL VOC**：一个包含20个类别、9,993个标记图像的数据集。您可以在[http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html)找到该数据集。'
- en: '**Labeled Faces in the Wild (LFW) from University of Massachusetts Amherst**: A
    dataset comprising 2,927 faces. Each has the hair, skin, and background labeled
    (three classes). You can find the dataset at [http://vis-www.cs.umass.edu/lfw/part_labels/](http://vis-www.cs.umass.edu/lfw/part_labels/).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来自马萨诸塞大学阿默斯特分校的Labeled Faces in the Wild (LFW)**：一个包含2,927个面部图像的数据集。每个图像都有头发、皮肤和背景的标记（三个类别）。您可以在[http://vis-www.cs.umass.edu/lfw/part_labels/](http://vis-www.cs.umass.edu/lfw/part_labels/)找到该数据集。'
- en: '**Common Objects in Context (COCO) dataset**: A popular dataset for all things
    related to computer vision, including segmentation. Its segmented datasets comprise
    approximately 200,000 labeled images across 80 classes. It''s the dataset that
    was used and which we will be briefly exploring in this section. You can find
    the dataset at [http://cocodataset.org/#home](http://cocodataset.org/#home).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Common Objects in Context (COCO)数据集**：一个涵盖所有与计算机视觉相关内容的流行数据集，包括分割。其分割数据集包含大约80个类别、200,000个标记图像。这是我们在本节中将使用并简要探讨的数据集。您可以在[http://cocodataset.org/#home](http://cocodataset.org/#home)找到该数据集。'
- en: Not considered for this project but good to be aware of is the **Cambridge-driving
    Labeled Video Database** (**CamVid**) from Cambridge University. As is clear from
    the name, the dataset is made up of frames from a video feed from a car camera—ideal
    for anyone interested in training their own self-driving car. You can find the
    dataset at [http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/).
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然本项目没有考虑，但值得了解的是剑桥大学的**Cambridge-driving Labeled Video Database**（**CamVid**）。从其名称可以看出，该数据集由汽车摄像头的视频流帧组成——对于任何想要训练自己的自动驾驶汽车的人来说都是理想的。您可以在[http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)找到该数据集。
- en: Listing the datasets here is possibly superfluous, but semantic segmentation
    is such an exciting opportunity with huge potential that I hope listing these
    here will encourage you to explore and experiment with new applications of it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里列出数据集可能是多余的，但语义分割是一个如此激动人心的机会，具有巨大的潜力，我希望在这里列出这些数据集能鼓励您探索和实验其新的应用。
- en: 'Luckily for us, COCO''s 13+ GB dataset contains many labeled images of people
    and a convenient API to make finding relevant images easy. For this chapter, COCO''s
    API was used to find all images including people. Then, these were filtered further,
    only keeping those that contained either one or two people and whose area covered
    between 20% and 70% of the image, discarding those images where the person was
    too small or too large. For each of these images, the contours of each of the
    persons were fetched and then used to create a binary mask, which then became
    our labels for our training. The following figure illustrates this process for
    a single image:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，COCO的13+ GB数据集包含许多标记的人物图像，并有一个方便的API来轻松查找相关图像。对于本章，使用了COCO的API来查找所有包含人物的图像。然后，进一步过滤这些图像，只保留那些包含一个或两个人物且其面积占图像20%到70%的图像，丢弃那些人物太小或太大的图像。对于这些图像中的每一个，都获取了每个人的轮廓，然后用于创建二值掩码，这成为了我们训练的标签。以下图示了单个图像的此过程：
- en: '![](img/130f0640-fac9-4810-8b88-a875cd0b11a4.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/130f0640-fac9-4810-8b88-a875cd0b11a4.png）'
- en: 'Source: The COCO dataset (http://cocodataset.org)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：COCO数据集（http://cocodataset.org）
- en: After training on 8,689 images over 40 epochs, an **Intersection over Union**
    (**IoU**) coefficient (also known as the **dice coefficient**) of 0.8192 was achieved
    on the validation data (approximately 300).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 经过在8,689张图像上训练40个epoch后，在验证数据上（大约300个）实现了0.8192的**交并比**（**IoU**）（也称为**dice系数**）。
- en: Hopefully, IoU sounds familiar as it was what we used back in [Chapter 5](6365f272-41e9-4511-a564-dc0f8db5d3ca.xhtml),
    *Locating Objects in the World*. As a reminder, IoU is an evaluation metric used
    to measure how well two bounding boxes overlap each other. A perfect overlap,
    where both bounding boxes overlap each other perfectly, would return 1.0 (which
    is why the loss is negated for training).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 希望IoU这个术语听起来很熟悉，因为它在[第5章](6365f272-41e9-4511-a564-dc0f8db5d3ca.xhtml)“在世界上定位物体”中我们已经使用过了。作为提醒，IoU是一个用于衡量两个边界框重叠程度的评估指标。完美的重叠，即两个边界框完美重叠，会返回1.0（这也是为什么训练时损失被取反的原因）。
- en: 'In the following image, we get to see what this looks like, starting with random
    examples from the validation set. Then, some are manually searched for, like the
    ones that portray actions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图像中，我们可以看到这看起来是什么样子，从验证集中的随机例子开始。然后，手动搜索一些，比如描绘动作的例子：
- en: '![](img/d0b5ea40-c7b0-473c-a4ac-2ee4b6e66bbf.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d0b5ea40-c7b0-473c-a4ac-2ee4b6e66bbf.png)'
- en: 'Source: The COCO dataset (http://cocodataset.org)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：COCO数据集（http://cocodataset.org）
- en: 'And here are some examples of action images where the model was able to sufficiently segment
    the person from the image:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些动作图像的例子，其中模型能够足够地分割出图像中的人物：
- en: '![](img/3eea43d5-3b5b-4f33-b880-7de6224dc15f.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3eea43d5-3b5b-4f33-b880-7de6224dc15f.png)'
- en: 'Finally, here are some, out of many, examples of action images where the model
    was less successful:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里有一些，许多中的几个，动作图像的例子，其中模型不太成功：
- en: '![](img/991af62f-8fb1-48af-99b2-ccb419db803b.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/991af62f-8fb1-48af-99b2-ccb419db803b.png)'
- en: We have covered the model and training data and examined the outputs of the
    model. It's now time to turn our attention to the application in this chapter,
    which we will begin working on in the next section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了模型和训练数据，并检查了模型的输出。现在是时候将我们的注意力转向本章的应用程序了，我们将在下一节开始着手处理。
- en: Building the photo effects application
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建照片效果应用程序
- en: In this section, we will be looking briefly at the application and highlighting
    some of the interesting pieces of the code, omitting most of it as it has already
    been discussed in previous chapters. As mentioned in the introduction, this example
    is to provide a case study for a later section, where we will discuss some broad
    strategies to use when building intelligent interfaces and services.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要地查看应用程序，并突出一些有趣的代码片段，省略大部分内容，因为它们已经在之前的章节中讨论过了。如引言中所述，这个例子是为了提供一个案例研究，以便在后面的章节中讨论构建智能界面和服务时可以使用的广泛策略。
- en: If you haven't already, pull down the latest code from the accompanying repository
    at [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the `Chapter9/Start/` directory and open the project
    `ActionShot.xcodeproj`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有做，请从随附的仓库[https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)下载最新的代码。下载完成后，导航到`Chapter9/Start/`目录并打开项目`ActionShot.xcodeproj`。
- en: 'As mentioned in the previous section, the example for this chapter is an photo
    effects application. In it, the user is able to take an *action shot*, have the
    application extract each person from the frames, and compose them onto the final
    frame, as illustrated in the following figure:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，本章的示例是一个照片效果应用。在其中，用户可以拍摄一个*动作快照*，应用会从帧中提取每个人，并将他们合成到最终帧中，如图所示：
- en: '![](img/c6ce6ad3-e903-40ec-b1db-8b989b4294f9.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c6ce6ad3-e903-40ec-b1db-8b989b4294f9.png)'
- en: The application consists of two view controllers; one is responsible for capturing
    the frames and the other for presenting the composite image. The workhorse for
    the processing, once again, has been delegated to the `ImageProcessor` class and
    it is the perspective from which we will be reviewing this project.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用由两个视图控制器组成；一个负责捕获帧，另一个负责展示合成图像。处理工作再次委托给了`ImageProcessor`类，这也是我们将从该角度审查这个项目的视角。
- en: '`ImageProcessor` acts as both the sink and processor; by sink I refer to it
    being the class that is passed captured frames from the camera, using the `CameraViewController`,
    and holding them in memory for processing. Let''s see what the code for this looks
    like; select `ImageProcessor.swift` from the left panel to bring the source code
    into focus. Let''s see what exists; initially paying particular attention to the
    properties and methods responsible for handling received frames and then move
    on to their processing.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageProcessor`既是接收器也是处理器；通过接收器我指的是它是一个从`CameraViewController`接收捕获的帧并将其保存在内存中以供处理的类。让我们看看代码的样子；从左侧面板选择`ImageProcessor.swift`将源代码聚焦。让我们看看有什么存在；最初特别关注处理接收到的帧的属性和方法，然后继续到它们的处理。'
- en: 'At the top of the file, you will notice that a protocol has been declared,
    which is implemented by the `EffectViewController`; it is used to broadcast the
    progress of the tasks:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件顶部，你会注意到已经声明了一个协议，该协议由`EffectViewController`实现；它用于广播任务的进度：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first callback, `onImageProcessorFinishedProcessingFrame`, is used to notify
    the delegate of frame-by-frame processing progress while the other, `onImageProcessorFinishedComposition`,
    is used to notify the delegate once the final image has be created. These discrete
    callbacks are intentionally split as the processing has been broken down into
    segmentation and composition. Segmentation is responsible for segmenting each
    of the frames using our model, and composition is responsible for generating the
    final image using the processed (segmented) frames. This structure is also mimicked
    in the layout of the class, with the class broken down into four parts and the
    flow we will follow in this section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个回调`onImageProcessorFinishedProcessingFrame`用于通知代理帧逐帧处理的进度，而另一个回调`onImageProcessorFinishedComposition`则用于在最终图像创建完成后通知代理。这些离散的回调故意分开，因为处理已经被分解为分割和合成。分割负责使用我们的模型分割每一帧，而合成则负责使用处理过的（分割的）帧生成最终图像。这种结构也在类的布局中得到了体现，类被分解为四个部分，本节我们将遵循的流程。
- en: 'The first part declares all the variables. The second implements the properties
    and methods responsible for retrieving the frames while they''re being captured.
    The third contains all the methods for processing the frames, whereby the delegate
    is notified using the `onImageProcessorFinishedProcessingFrame` callback. The
    final part, and the one we will focus on the most, contains the methods responsible
    for generating the final image, that is, it composites the frames. Let''s peek
    at the first part to get a sense of what variables are available, which are shown
    in the following code snippet:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分声明了所有变量。第二部分实现了在捕获过程中检索帧的属性和方法。第三部分包含处理帧的所有方法，通过`onImageProcessorFinishedProcessingFrame`回调通知代理。最后一部分，我们将最关注的部分，包含生成最终图像的方法，即合成帧。让我们先看看第一部分，以了解可用的变量，如下面的代码片段所示：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Nothing extraordinary. We first declare a property that wraps our model in an
    instance of `VNCoreMLModel` so that we can take advantage of the Vision framework's
    preprocessing functionality. We then declare a series of variables to deal with
    storing the frames and handling the processing; we make use of an `NSLock` instance
    to avoid different threads reading stale property values.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么特别之处。我们首先声明一个属性，它将我们的模型包装在一个`VNCoreMLModel`实例中，这样我们就可以利用Vision框架的预处理功能。然后，我们声明一系列变量来处理存储帧和处理；我们使用一个`NSLock`实例来避免不同的线程读取过时的属性值。
- en: 'The following code snippet, and part of the `ImageProcessor` class, includes
    variables and methods for handling retrieving and releasing the captured frames:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段和`ImageProcessor`类的一部分包括处理检索和释放捕获帧的变量和方法：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Although fairly verbose, it should all be self-explanatory; probably the only
    method worth outlying is the method `addFrame`, which is called each time a frame
    is captured by the camera. To give some bearing of how everything is tied together,
    the following diagram illustrates the general flow whilst capturing frames:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然相当冗长，但应该都是不言自明的；可能唯一值得特别指出的是`addFrame`方法，该方法在相机捕获每个帧时都会被调用。为了说明一切是如何联系在一起的，以下图表展示了捕获帧时的总体流程：
- en: '![](img/a160b4da-708f-4a9c-a037-8009ce7a17ac.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a160b4da-708f-4a9c-a037-8009ce7a17ac.png)'
- en: 'The details of the flow are covered in the following points:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 流程的细节将在以下几点中介绍：
- en: Although capturing of the frames is persistent throughout the lifetime of the
    `CameraViewController`, they are only passed to the `ImageProcessor` once flagged
    once the user taps (and holds) their finger on the action button
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管在`CameraViewController`的生命周期内持续捕获帧，但只有在用户点击（并按住）动作按钮后，这些帧才会被传递给`ImageProcessor`
- en: During this time, each frame that is captured (at the throttled rate—currently
    10 frames per second) is passed to the `CameraViewController`
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这段时间内，每个捕获的帧（以限制的速率—目前为每秒10帧）都会传递给`CameraViewController`
- en: This subsequently passes it to the `ImageProcessor` using the `addFrame` method
    shown earlier
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它使用前面显示的`addFrame`方法将帧传递给`ImageProcessor`
- en: The capturing stops when the user lifts their finger from the action button
    and, once finished, the `EffectsViewController` is instantiated and presented,
    along with passing it a reference to the `ImageProcessor` with the reference to
    the captured frames
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当用户从动作按钮抬起手指时，捕获停止，完成后，将实例化并展示`EffectsViewController`，同时传递一个包含捕获帧引用的`ImageProcessor`引用
- en: 'The next part of the `ImageProcessor` class is responsible for processing each
    of these images; this is initiated using the `processFrames` method, which is
    called by the `EffectsViewController` once it has loaded. This part has a lot
    more code, but most of it should be familiar to you as it''s the boilerplate code
    we''ve used in many of the projects during the course of this book. Let''s start
    by inspecting the `processFrames` method, as shown in the following snippet:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageProcessor`类的下一部分负责处理这些图像中的每一个；这是通过`processFrames`方法启动的，该方法由`EffectsViewController`在加载后调用。这部分有更多的代码，但其中大部分应该对你来说都很熟悉，因为这是我们在这本书的许多项目中使用的样板代码。让我们首先检查以下片段中的`processFrames`方法：'
- en: All of the remaining code is assumed to be inside the `ImageProcessor` class
    for the rest of this chapter unless stated otherwise; that is, the class and class
    extension declaration will be omitted to make the code easier to read.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分，除非另有说明，所有剩余的代码都假定为`ImageProcessor`类内部；也就是说，将省略类和类扩展声明，以便使代码更容易阅读。
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This method simply dispatches the method call `processingNextFrame` to the
    background thread. This is mandatory when performing inference with Core ML and
    also a good practice when performing compute-intensive tasks to avoid locking
    up the user interface. Let''s continue the trail by inspecting the `processingNextFrame`
    method along with the method responsible for returning an instance of a `VNCoreMLRequest`,
    which is shown in the following code snippet:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法简单地调度`processingNextFrame`方法到后台线程。当使用Core ML进行推理时这是强制性的，而且在执行计算密集型任务时也是良好的实践，以避免锁定用户界面。让我们继续追踪，通过检查`processingNextFrame`方法以及负责返回`VNCoreMLRequest`实例的方法，后者在以下代码片段中显示：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We start off by setting the property `isProcessingImage` to `true` and checking
    that we have a frame to process, otherwise exiting early from the method.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将属性`isProcessingImage`设置为`true`，并检查我们是否有帧要处理，否则提前退出方法。
- en: The following might seem a little counter-intuitive (because it is); we have
    seen from previous chapters that `VNCoreMLRequest` handles the preprocessing task
    of resizing the cropping of our images. So, why are we doing it manually here?
    The reason has more to do with keeping the code simpler and meeting publishing
    deadlines. In this example, the final image is composited using the resized frames
    to avoid scaling and offsetting the output from the model, which I'll leave as
    an exercise for you. So here, we are performing that operation and persisting
    the result in the array `processedImages` to be used in the final stage. Finally,
    we execute the request, passing in the image, which calls our method `processRequest`
    once finished, passing in the results from the model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容可能看起来有些反直觉（因为确实如此）；从前几章中我们了解到`VNCoreMLRequest`处理了图像裁剪的预处理任务。那么，为什么我们在这里要手动处理呢？原因更多在于使代码更简洁和满足出版截止日期。在这个例子中，最终图像是通过调整大小的帧合成的，以避免对模型输出的缩放和偏移，这留作你的练习。因此，在这里，我们执行这个操作，并将结果持久化存储在`processedImages`数组中，以便在最终阶段使用。最后，我们执行请求，传入图像，一旦完成，调用我们的`processRequest`方法，传入模型的结果。
- en: 'Continuing on our trail, we will now inspect the `processRequest` method; as
    this method is quite long, we will break it down into chunks, working top to bottom:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的追踪，现在我们将检查`processRequest`方法；由于这个方法相当长，我们将将其分解成块，从上到下工作：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We start off by getting the latest counts, which will be broadcast to the delegate
    when this method finishes or fails. Talking of which, the following block verifies
    that a result was returned of type `[VNPixelBufferObservation]`, otherwise notifying
    the delegate and returning, as shown in the following snippet:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先获取最新的计数，这些计数将在此方法完成或失败时广播给代理。说到这一点，以下代码块验证是否返回了类型为`[VNPixelBufferObservation]`的结果，否则通知代理并返回，如下面的代码片段所示：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With reference to our result (`CVBufferPixel`), our next task is to create
    an instance of `CIImage`, passing in the buffer and requesting the color space
    to be grayscale to ensure that a single channel image is created. Then, we will
    be adding it to our `processedMasks` array, shown in the following snippet:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参考我们的结果（`CVBufferPixel`），我们的下一个任务是创建一个`CIImage`实例，传入缓冲区并请求将颜色空间设置为灰度，以确保创建单通道图像。然后，我们将将其添加到下面的代码片段中`processedMasks`数组：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Only two more things left to do! We notify the delegate that we have finished
    a frame and proceed to process the next frame, if available:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 只剩下两件事要做！我们通知代理我们已经完成了一帧，并继续处理下一帧（如果有的话）：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This concludes the third part of our `ImageProcessor`; at this point, we have
    two arrays containing the resized captured frames and the segmented images from
    our model. Before moving on to the final part of this class, let''s get a bird''s-eye
    view of what we just did, illustrated in this flow diagram:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们`ImageProcessor`的第三部分结束；到目前为止，我们有两个数组，包含调整大小的捕获帧和模型生成的分割图像。在继续到这个类的最后部分之前，让我们从鸟瞰图的角度回顾一下我们刚才所做的工作，如图流图所示：
- en: '![](img/e5a57ffb-0718-4d56-837f-d809613819f8.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e5a57ffb-0718-4d56-837f-d809613819f8.png)'
- en: 'The details of the flow are shown in the following points:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 流程的细节如下所示：
- en: As mentioned in the preceding diagram, processing is initiated once the `EffectsViewController` is
    loaded, which kicks off the background thread to process each of the captured
    frames
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前图所示，一旦加载了`EffectsViewController`，就会启动处理过程，这会启动后台线程来处理捕获到的每一帧
- en: Each frame is first resized and cropped to match the output of the model
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每一帧首先被调整大小和裁剪，以匹配模型的输出
- en: Then, it is added to the `processedFrames` array and passed to our model for
    interference (segmentation)
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它被添加到`processedFrames`数组中，并传递给我们的模型进行干扰（分割）
- en: Once the model returns with the result, we instantiate a single color instance
    of `CIImage`
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型返回结果，我们实例化一个单色的`CIImage`实例
- en: This instance is stored in the array `processedMasks` and the delegate is notified
    of the progress
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此实例存储在`processedMasks`数组中，并通知代理进度
- en: What happens when all frames have been processed? This is what we plan on answering
    in the next part, where we will discuss the details of how to create the effect.
    To start with, let's discuss how the process is initiated.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有帧都处理完毕时会发生什么？这是我们计划在下一部分回答的，我们将讨论创建效果的详细方法。首先，让我们讨论这个过程是如何开始的。
- en: 'Once the delegate (`EffectsViewController`) receives a callback, using `onImageProcessorFinishedProcessingFrame`,
    where all of the frames have been processed, it calls the `compositeFrames` method from
    the ImageProcessor to start the process of creating the effect. Let''s review
    this and the existing code within this part of the `ImageProcessor` class:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代理（`EffectsViewController`）收到回调，使用`onImageProcessorFinishedProcessingFrame`，其中所有帧都已处理，它就调用ImageProcessor中的`compositeFrames`方法来开始创建效果的过程。让我们回顾一下这部分`ImageProcessor`类中的现有代码：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: I have bolded the important/interesting parts, essentially the parts we will
    be implementing, but before writing any more code, let's review what we currently
    have (in terms of processed images) and an approach to creating our effect.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经将重要的/有趣的部分加粗，本质上是我们将要实现的部分，但在编写更多代码之前，让我们回顾一下我们目前拥有的内容（就处理过的图像而言）以及创建我们效果的方法。
- en: 'At this stage, we have an array of `processedFrames` that contains the resized
    and cropped versions of the captured images, and we have another array, `processedMasks`,
    containing the single-channel images from our segmentation model. Examples of
    these are shown in the following figure:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们有一个`processedFrames`数组，其中包含捕获图像的调整大小和裁剪版本，还有一个名为`processedMasks`的数组，包含来自我们的分割模型的单通道图像。以下图示了这些示例：
- en: '![](img/cb47ab2e-1b0b-46c4-a923-a9aa455d214e.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cb47ab2e-1b0b-46c4-a923-a9aa455d214e.png)'
- en: If we were to composite each of the frames as they are, we would end up with
    a lot of unwanted artifacts and excessive overlapping. One approach could be to
    adjust the frames that have been processed (and possibly captured), that is, skip
    every *n* frames to spread out the frames. The problem with this approach is that
    it assumes all subjects will be moving at the same speed; to account for this,
    you would need to expose this tuning to the user for manual adjustment (which
    is an reasonable approach). The approach we will take here will be to extract
    the bounding box for each of the frames, and using the displacement and relative
    overlap of these to determine when to insert a frame and when to skip a frame.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们像现在这样合成每个帧，我们最终会得到很多不需要的伪影和过多的重叠。一种方法可能是调整已经处理（以及可能捕获）的帧，即跳过每*n*帧以分散帧。这种方法的问题在于它假设所有主题将以相同的速度移动；为了解决这个问题，你需要将这种调整暴露给用户进行手动调整（这是一种合理的方法）。我们将采取的方法是提取每个帧的边界框，并使用这些帧的位移和相对重叠来确定何时插入帧以及何时跳过帧。
- en: 'To calculate the bounding box, we simply scan each line from each of the edges
    of the image, that is, from **top to bottom**, to determine the top of the object.
    Then, we do it **bottom to top** to determine the bottom of the object. Similarly,
    we do it on the horizontal axis, illustrated in the following figure:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算边界框，我们只需扫描图像的每条边上的每一行，即从**顶部到底部**，以确定物体的顶部。然后，我们**从底部到顶部**做同样的事情，以确定物体的底部。同样，我们在水平轴上做同样的事情，以下图示了这一点：
- en: '![](img/97e10b70-a9aa-4fe0-9eec-e4d1fa82e31f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/97e10b70-a9aa-4fe0-9eec-e4d1fa82e31f.png)'
- en: 'Even with bounding boxes, we still need to determine how far the object should
    move before inserting a frame. To determine this, we first determine the dominant
    direction, which is calculated by finding the direction between the first and
    last frames of the segmented object. This is then used to determine what axis
    to compare displacement on; that is, if the dominant direction is in the horizontal
    axis (as shown in the preceding figure), then we measure the displacement across
    the *x *axis, ignoring the *y *axis. We then simply measure the distance between
    the frames against some predetermined threshold to decide whether to composite
    the frame or ignore it. This is illustrated in the following figure:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有边界框，我们仍然需要确定在插入帧之前物体应该移动多远。为了确定这一点，我们首先确定主导方向，这是通过找到分割物体的第一帧和最后一帧之间的方向来计算的。然后，这被用来确定比较位移的轴；也就是说，如果主导方向在水平轴上（如前图所示），那么我们测量沿*x*轴的位移，忽略*y*轴。然后，我们简单地测量帧之间的距离与一些预定的阈值，以决定是否合成帧或忽略它。以下图示了这一点：
- en: '![](img/c90dd24b-e51f-4311-b6ad-b04afca724b6.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c90dd24b-e51f-4311-b6ad-b04afca724b6.png)'
- en: 'Let''s see what this looks like in code, starting from determining the dominant
    direction. Add the following code to the `getDominantDirection` method:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码中的样子，从确定主导方向开始。将以下代码添加到`getDominantDirection`方法中：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As described earlier, we first find the bounding boxes of the start and end
    of our sequence of frames, and use their centers to calculate the dominate direction.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们首先找到我们序列帧的开始和结束的边界框，并使用它们的中心来计算主导方向。
- en: The implementation of the `CIImage` method `getContentBoundingBox` is omitted
    here, but it can be found in the accompanying the source code within the `CIImage+Extension.swift` file.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里省略了`CIImage`方法的`getContentBoundingBox`实现，但可以在`CIImage+Extension.swift`文件中找到相应的源代码。
- en: 'Armed with the dominant direction, we can now proceed with determining what
    frames to include and what frames to ignore. We will implement this in the method
    `getIndiciesOfBestFrames` of the `ImageProcessor` class, which iterates over all
    frames, measuring the overlap and ignoring those that don''t meet a specific threshold.
    The method returns an array of indices that satisfy this threshold to be composited
    onto the final image. Add the following code to the `getIndiciesOfBestFrames`
    method:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有了主导方向，我们现在可以继续确定要包含哪些帧以及要忽略哪些帧。我们将在`ImageProcessor`类的`getIndiciesOfBestFrames`方法中实现这一点，该方法遍历所有帧，测量重叠并忽略那些不满足特定阈值的帧。该方法返回一个索引数组，这些索引满足这个阈值，可以合成到最终图像上。将以下代码添加到`getIndiciesOfBestFrames`方法中：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We begin by getting the dominant direction, as discussed earlier, and then
    proceed to iterate through our sequence of frames in reverse order (reverse as
    it is assumed that the user''s heroshot is the last frame). With each frame, we
    obtain the bounding box, and if it''s the first frame to be checked, we assign
    it to the variable `previousBoundingBox`. This will be used to compare subsequent
    bounding boxes (and updated to the latest included frame). If `previousBoundingBox`
    is not null, then we calculate the displacement between the two based on the dominant
    direction, as shown in the following snippet:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先获取主导方向，如前所述，然后按逆序（因为假设用户的heroshot是最后一帧）遍历我们的帧序列。对于每个帧，我们获取边界框，如果它是第一个要检查的帧，我们将其分配给变量`previousBoundingBox`。这将用于比较后续的边界框（并更新为最新的包含帧）。如果`previousBoundingBox`不为空，则根据主导方向计算两者之间的位移，如下面的代码片段所示：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We then calculate the minimum length needed to separate the two objects, which
    is calculated by the combined size of the relative axis divided by 2\. This gives
    us a distance of half of the combined frame, as shown in the following snippet:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算分离两个对象所需的最小长度，这是通过相对轴的合并大小除以2来计算的。这给我们一个距离，是合并帧的一半，如下面的代码片段所示：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We then compare the distance with the bounds along with a threshold and proceed
    to add the frame to the current index if the distance satisfies this threshold:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将距离与边界以及阈值进行比较，如果距离满足这个阈值，就继续将帧添加到当前索引中：
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Returning to the `compositeFrames` method, we are now ready to composite the
    selected frames. To achieve this, we will leverage `CoreImages` filters; but before
    doing so, let's quickly review what it is exactly that we want to achieve.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到`compositeFrames`方法，我们现在准备合成选定的帧。为了实现这一点，我们将利用`CoreImages`过滤器；但在这样做之前，让我们快速回顾一下我们到底想实现什么。
- en: 'For each selected (processed) image and mask pair, we want to clip out the
    image and overlay it onto the final image. To improve the effect, we will apply
    a progressively increasing alpha so that frames closer to the final frame will
    have an opacity closer to 1.0 while the frames further away will be progressively
    transparent; this will give us a faded trailing effect. This process is summarized
    in the following figure:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个选定的（处理过的）图像和掩码对，我们希望裁剪出图像并将其叠加到最终图像上。为了提高效果，我们将应用逐渐增加的alpha值，使得接近最终帧的帧将具有接近1.0的不透明度，而远离最终帧的帧将逐渐变得透明；这将给我们一个渐变的拖尾效果。这个过程在以下图中进行了总结：
- en: '![](img/9a61021e-de1e-4cd3-8f29-eb8ab88899df.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a61021e-de1e-4cd3-8f29-eb8ab88899df.png)'
- en: 'Let''s turn this into code by first implementing the filter. As shown earlier,
    we will be passing the kernel the output image, the overlay and its corresponding
    mask, and an alpha. Near the top of the `ImageProcessor` class, add the following
    code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过首先实现过滤器将这个想法转化为代码。如前所述，我们将传递内核输出图像、叠加图像及其相应的掩码和一个alpha值。在`ImageProcessor`类的顶部附近，添加以下代码：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Previously, we have implemented the `CIColorKernel`, which is responsible for
    compositing all of our frames onto the final image as discussed. We start by testing
    the mask''s value, and if it is 1.0, we assign the strength 1.0 (meaning we want
    to replace the color at that location of the final image with that of the overlay).
    Otherwise, we assign 0, ignoring it. Then, we multiply the strength with the blend
    argument passed to our kernel. Finally, we calculate and return the final color
    with the statement `vec4(image.rgb * (1.0-overlayStrength), 1.0) + vec4(overlay.rgb
    * (overlayStrength), 1.0)`. With our filter now implemented, let''s return the `compositeFrames`
    method and put it to use. Within `compositeFrames`, replace the comment `// TODO
    Composite final image using segments from intermediate frames` with the following
    code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Most of this should be self-explanatory; we start by calculating an alpha stride
    that will be used to progressively increase opacity as we get closer to the final
    frame. We then iterate through all the selected frames, applying the filter we
    just implemented in the preceding snippet, compositing our final image.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'With that done, we have now finished this method and the coding for this chapter.
    Well done! It''s time to test it out; build and run the project to see your hard
    work in action. The following is a result from a weekend park visit:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4caad02-de67-4097-bdf1-90c31c307d45.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: Before wrapping up this chapter, let's briefly discuss some strategies when
    working with machine learning models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Working with probabilistic results
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As alluded to at the beginning of this chapter and seen firsthand in the previous
    section, working with machine learning models requires a set of new techniques
    and strategies to deal with uncertainty. The approach taken will be domain-specific,
    but there are some broad strategies that are worth keeping in mind, and that's
    what we will cover in this section in the context of the example project of this
    chapter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Improving the model
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first is improving the model. Of course, there may be limitations depending
    on the source of the model and dataset, but it's important to be able to understand
    ways in which the model can be improved as its output directly correlates to the
    quality of the user experience.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: In the context of this project, we can augment the model using an existing pre-trained
    image classifier as the encoder, as mentioned earlier. This not only fast-tracks
    training, providing more opportunities to iterate, but also is likely to improve
    performance by having the model transfer existing knowledge from a more comprehensive
    dataset.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Another is tuning the dataset that the model was trained on. A simple, and
    relevant, example of how the model can be improved can be seen by any image in
    which the user is holding an object (which has been labeled). An example of this
    can be seen in the following figure, in which the guitar is cropped from the person:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f869a3f5-a10a-40ed-bc75-1b1eeeb40f81.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f869a3f5-a10a-40ed-bc75-1b1eeeb40f81.png)'
- en: How you deal with this is dependent on the desired characteristics of your model.
    In the context of the application presented in this chapter, it would make sense
    to either perform multi-class classification, including objects normally held
    by people, or including them in the mask.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何处理这个问题取决于你模型期望的特性。在本章所展示的应用程序背景下，进行多类分类，包括人们通常持有的物体，或者将它们包含在掩码中，是有意义的。
- en: Another common technique is **data augmentation**. This is where you artificially
    adjust the image (input) to increase variance in your dataset, or even adjust
    it to make it more aligned with the data for your particular use case. Some example
    augmentations include blurring (useful when dealing with fast moving objects),
    rotation, adding random noise, color adjustment - essentially any image manipulation
    effect that introduces nuances that you are likely to get in the real world.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的技术是**数据增强**。这是指你人工调整图像（输入），以增加数据集的方差，或者甚至调整它以使其更符合你特定用例的数据。一些增强的例子包括模糊（在处理快速移动的物体时很有用）、旋转、添加随机噪声、颜色调整——本质上任何在现实世界中可能遇到的图像处理效果。
- en: Of course, there are many more techniques and tools to improve the model and
    data; here, our intention is just to highlight the main areas rather than delve
    into the details.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有许多其他技术和工具可以改进模型和数据；在这里，我们的意图只是强调主要领域，而不是深入细节。
- en: Designing in constraints
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在约束中设计
- en: This is somewhat unavoidable and how we design intelligent interfaces is still
    in its infancy. That is, how much transparency do you expose to the user? And
    how do you effectively assist them in building a useful mental model of your system
    without being distracting or losing the convenience of using the model in the
    first place? But here, I am simply referring to designing constraints within the
    experience to increase the chances of the model's success. A great, although slightly
    unrelated, example of this is household robots and the dishwasher. Despite its
    non-robotic characteristics, the faithful dishwasher can be considered a first
    generation robot for household tasks, like Rosie from Jetsons. Unlike Rosie, however,
    we had not been able to get the dishwasher to adapt to our environment. So, we
    adapted the environment for the dishwasher, that is, we encapsulated it in a box
    environment rather than using the existing kitchen sink we're accustomed to.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这在某种程度上是不可避免的，而且我们设计智能界面的方式仍处于初级阶段。也就是说，你向用户暴露多少透明度？你如何有效地帮助他们构建一个有用的心理模型，而不会分散他们的注意力或失去最初使用模型时的便利性？但在这里，我仅仅是指在设计体验中的约束条件，以提高模型成功的可能性。一个很好的例子，尽管与主题稍有不相关，就是家用机器人和洗碗机。尽管它没有机器人特性，但忠诚的洗碗机可以被认为是第一代用于家庭任务的机器人，就像《杰森一家》中的Rosie。然而，与Rosie不同的是，我们还没有能够让洗碗机适应我们的环境。因此，我们为洗碗机调整了环境，也就是说，我们将其封装在一个箱子环境中，而不是使用我们习惯的现有厨房水槽。
- en: 'One simple approach is making the user aware of how to achieve the best results;
    in this example, it could be as simple as asking them to use a wall as their background.
    These hints can be delivered before use or delivered when there are signs of poor
    performance (or both). One approach for automatically detecting poor performance
    would be to measure the bounding box and its center of mass, comparing it with
    the expected center of mass, as illustrated in the following figure:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是让用户意识到如何获得最佳结果；在这个例子中，这可能仅仅是通过要求他们使用墙壁作为背景。这些提示可以在使用前提供，也可以在出现性能不佳的迹象时提供（或者两者都有）。自动检测性能不佳的一种方法是对边界框及其质心进行测量，并将其与预期的质心进行比较，如图所示：
- en: '![](img/0710dcbb-d0fd-4cb3-8899-36bd5b0f93df.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0710dcbb-d0fd-4cb3-8899-36bd5b0f93df.png)'
- en: 'Which brings us nicely to the next strategy: embedding heuristics.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这就自然引出了下一个策略：嵌入启发式方法。
- en: Embedding heuristics
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入启发式方法
- en: Heuristics is essentially codifying rules you have in your head to solve a particular
    task, typically implemented using a function that returns a score. This is used
    to rank a set of alternatives. In the previous section, *Designing in constraints*,
    we saw how we could use the center of mass and the bounding box to determine how
    well distributed the pixels are for a segmented image. This in turn could be used
    to rank each frame, by favoring those with a center of mass near the center of
    the bounding box. We also implemented a type of heuristic in the application when
    determining which frames to keep and which ones to ignore by measuring the overlap.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式方法本质上是在你的脑海中编码规则以解决特定任务，通常通过一个返回分数的函数来实现。这用于对一组替代方案进行排序。在上一节“在设计约束中”，我们看到了如何使用质心和边界框来确定分割图像中像素的分布情况。这反过来可以用来对每个帧进行排序，优先考虑那些质心靠近边界框中心的帧。我们还实现了一种启发式方法，在确定保留哪些帧和忽略哪些帧时，通过测量重叠来实现。
- en: Heuristics can be a powerful ally, but be careful to ensure that the heuristics
    you derive can generalize well to your problem, just as you would expect a good
    model to. Also, be mindful of the additional computational cost incurred from
    using them.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式方法可以是一个强大的盟友，但请注意确保你推导出的启发式方法能够很好地泛化到你的问题上，就像你期望一个好的模型那样。此外，还要注意使用它们所带来的额外计算成本。
- en: Post-processing and ensemble techniques
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后处理和集成技术
- en: Techniques from image processing, computer vision, and computer graphics can
    be borrowed to improve the quality of the output as well as detection. As an example,
    a typical image processing task is performing the morphology operations of opening
    and closing. This combination is commonly used to remove noise and fill in small
    holes in a binary image. Another useful post-processing task we could borrow from computer
    vision is watersheds, a segmentation technique that treats the image as a topographical
    map, where the intensity of change defines the ridges and the boundary of the
    fill (or segmentation).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 可以借鉴图像处理、计算机视觉和计算机图形学中的技术来提高输出质量以及检测能力。例如，一个典型的图像处理任务是执行开闭形态学操作。这种组合通常用于去除二值图像中的噪声和填充小孔。另一个我们可以从计算机视觉中借鉴的有用的后处理任务是流域，这是一种将图像视为地形图的分割技术，其中变化的强度定义了脊和填充（或分割）的边界。
- en: Another tool to use for post-processing is another model. You're familiar with
    YOLO for object detection. We can apply it to obtain its predicted boundaries
    of the object, which we can then use to refine our segmentation. Another model,
    and one being adopted for this task, is **conditional random fields** (**CRF**),
    which is capable of smoothing out the edges of our mask.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用于后处理的工具是另一个模型。你对YOLO（用于对象检测）很熟悉。我们可以将其应用于获取对象的预测边界，然后我们可以使用这些边界来细化我们的分割。另一个模型，也是用于此任务的模型，是**条件随机字段**（**CRF**），它能够平滑我们的掩模边缘。
- en: There are a vast number of techniques available from the fields of image processing,
    computer vision, and computer graphics, and I strongly encourage you to explore
    each area to build up your tool set.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像处理、计算机视觉和计算机图形学等领域有大量的技术可供选择，我强烈建议你探索每个领域以构建你的工具集。
- en: If you are new to computer vision, then I recommend the books *Computer Vision
    and Image Processing* by T. Morris and *Algorithms for Image Processing and Computer
    Vision* by J. Parker for a pragmatic introduction to the field.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚开始接触计算机视觉，我推荐T. Morris的《计算机视觉与图像处理》和J. Parker的《图像处理和计算机视觉算法》这两本书，作为对这个领域的实用介绍。
- en: Human assistance
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工辅助
- en: 'Sometimes it''s unavoidable or even desirable to include the human in tuning
    the output from the model. In these instances, the model is used to assist the
    user rather than completely automating the task. A few approaches that could be
    employed for the project in this chapter include the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 有时包含人类在调整模型输出中是不可避免的，甚至可能是期望的。在这些情况下，模型被用来辅助用户而不是完全自动化任务。本章项目中可能采用的一些方法包括以下内容：
- en: Provide an intermediate step where the user can tidy up the masks. By this,
    I mean allowing the user to erase parts of the mask that have been incorrectly
    classified or are unwanted by the user.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供一个中间步骤，让用户可以整理掩模。我的意思是允许用户擦除被错误分类或用户不想要的掩模部分。
- en: Present the user with a series of frames and have them select the frames for
    composition.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向用户展示一系列帧，并让他们选择用于合成的帧。
- en: Present the user with variations of the final composited image and have them
    select the one with the most appeal.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another related concept is introducing human-in-the-loop machine learning. This
    has a human intervening when the model is not confident in its prediction, and
    it passes the responsibility over to the user for classification and/or correction.
    The amendments from the user are then stored and used for training the model to
    improve performance. In this example, we could let the user (or crowd-source this
    task) segment the image and use this data when re-training the model. Eventually,
    given sufficient data, the model will improve its performance relevant to the
    context it is being used in.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: I hope this section highlighted the importance of handling uncertainty when
    working with machine models and provided enough of a springboard so that you can
    approach designing intelligent applications from the perspectives outlined here.
    Let's now conclude this chapter by reviewing what we have covered.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the concept of semantic segmentation, an approach
    that gives our applications increased perceptual understanding of our photos and
    videos. It works by training a model to assign each pixel to a specific class.
    One popular architecture for this is U-Net, which achieves high-precision localization
    by preserving spatial information, by bridging the convolutional layers. We then
    reviewed the data used for training along with some example outputs of the model,
    including examples that highlight the limitations of the model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: We then saw how this model could be used by creating an image effects application,
    where the segmented images were used to clip people from a series of frames and
    composite them together to create an action shot. But this is just one example
    of how semantic segmentation can be applied; it's frequently used in domains such
    as robotics, security surveillance, and quality assurance in factories, to name
    a few. How else it can be applied is up to you.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: In the final section, we spent some time discussing strategies when dealing
    with models, specifically, their probabilistic (or level of uncertainty) outputs,
    to improve user experience.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: This is the last of our examples of applying machine learning. In the next,
    and last, chapter, we'll shift gears and provide a primer into building your own
    models with the help of Create ML. Let's get started.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
