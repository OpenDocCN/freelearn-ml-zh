- en: Object Segmentation Using CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout the chapters in this book, we have seen various machine learning
    models, each progressively increasing their perceptual abilities. By this, I mean
    that we were first introduced to a model capable of classifying a single object
    present in an image. Then came a model that was able to classify not only multiple
    objects but also their corresponding bounding boxes. In this chapter, we continue
    this progression by introducing semantic segmentation, in other words, being able
    to assign each pixel to a specific class, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd0261d3-7c65-446d-8af5-1071f134fef6.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: http://cocodataset.org/#explore
  prefs: []
  type: TYPE_NORMAL
- en: This allows for a greater understanding of the scene and, therefore, opportunities
    for more intelligible interfaces and services. But this is not the main focus
    of this chapter. In this chapter, we will use semantic segmentation to create
    an image effects application as a way to demonstrate imperfect predictions. We'll
    be using this to motivate a discussion on one of the most important aspects of
    designing and building machine learning (or artificial intelligence) interfaces—dealing
    with probabilistic, or imperfect, outcomes from models.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will have:'
  prefs: []
  type: TYPE_NORMAL
- en: An understanding semantic of segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built an intuitive understanding of how it is achieved (learned)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learned how it can be applied in a novel way for real life applications by building
    an action shot photo effects application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gained appreciation and awareness for dealing with probability outcomes from
    machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin by better understanding what semantic segmentation is and get an
    intuitive understanding of how it is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying pixels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have already discussed, the desired output of a model performing semantic
    segmentation is an image with each of its pixels assigned a label of its most
    likely class (or even a specific instance of a class). Throughout this book, we
    have also seen that layers of a deep neural network learn features that are activated
    when a corresponding input that satisfies the particular feature is detected. We
    can visualize these activations using a technique called **class activation maps**
    (**CAMs**). The output produces a heatmap of class activations over the input
    image; the heatmap consists of a matrix of scores associated with a specific class,
    essentially giving us a spatial map of how intensely the input region activates
    a specified class. The following figure shows an output of a CAM visualization
    for the class cat. Here, you can see that the heatmap portrays what the model
    considers important features (and therefore regions) for this class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d4c72c3-ac9e-430e-af95-f5ff4c284d3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding figure was produced using the implementation described in the
    paper *Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization*
    by R. Selvaraju. The approach is to take the output feature map of a convolutional
    layer and weigh every channel in that feature map by the gradient of the class.
    For more details of how it works, please refer to the original paper: [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391).'
  prefs: []
  type: TYPE_NORMAL
- en: Early attempts of semantic segmentation were made using slightly adapted classification
    models such as VGG and Alexnet, but they only produced coarse approximations.
    This can be seen in the preceding figure and is largely due to the network using
    repetitive pooling layers, which results in loss of spatial information.
  prefs: []
  type: TYPE_NORMAL
- en: 'U-Net is one architecture that addresses this; it consists of an **encoder**
    and **decoder**, with the addition of **shortcuts** between the two to preserve
    spatial information. Released in 2015 by *O. Ronneberger*, *P. Fischer*, and *T.
    Brox* for biomedical image segmentation, it has since become one of the go-to
    architectures for segmentation due to its effectiveness (it can be trained on
    a small dataset) and performance. The following figure shows the modified U-Net
    we will be using in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc50598c-dd4e-480f-be82-5bff251864ed.png)'
  prefs: []
  type: TYPE_IMG
- en: U-Net is one of many architectures for semantic segmentation. Sasank Chilamkurthy's
    post *A 2017 Guide to Semantic Segmentation with Deep Learning* provides a great
    overview and comparison of the most popular architectures, available at [http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review](http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review).
    For further details on U-Net, please refer to the original paper mentioned earlier.
    It is available at [https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: On the left in the preceding figure, we have the full network used in this chapter's
    project, and on the right we have an extract of blocks used in the encoder and
    decoder parts of the network. As a reminder, the focus of this book is on applying
    machine learning rather than the details of the models themselves. So for this
    reason, we won't be delving into the details, but there are a few interesting
    and useful things worth pointing out.
  prefs: []
  type: TYPE_NORMAL
- en: The first is the general structure of the network; it consists of an encoder and
    decoder.The encoder's role is to capture context. The decoder's task is to use
    this context and features from the corresponding shortcuts to project its understanding
    onto pixel space, to get a dense and precise classification. It's a common practice
    to bootstrap the encoder using an architecture and weights from a trained classification
    model, such as VGG16\. This not only speeds up training but also is likely to
    increase performance as it brings with it a depth (pun intended) of understanding
    of images it has been trained on, which is typically from a larger dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Another point worth highlighting is those shortcuts between the encoder and
    decoder. As mentioned previously, they are used to preserve spatial information
    outputted from convolutional layers from each encoding block before being lost
    when its downsampled using max pooling. This information is used to assist the
    model in precise localization.
  prefs: []
  type: TYPE_NORMAL
- en: It's the first time in this book that we have seen an upsampling layer. As the
    name implies, it's a technique that upsamples your image (or feature maps) to
    a higher resolution. One of the easiest ways is to use the same techniques we
    use with image upsampling, that is, rescaling the input to a desired size and
    calculating the values at each point using an interpolation method, such as bilinear
    interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, I wanted to bring to your attention the input and outputs of the model.
    The model is expecting a 448 x 448 color image as its input and outputs a 448
    x 448 x 1 (single channel) matrix. If you inspect the architecture, you will notice
    that the last layer is a sigmoid activation, where a sigmoid function is typically
    used for binary classification, which is precisely what we are doing here. Typically,
    you would perform multi-class classification for semantic segmentation tasks,
    in which case you would replace the sigmoid activation with a softmax activation.
    An example commonly used when introducing semantic segmentation is scene understanding
    for self-driving cars. The following is an example of a labeled scene from Cambridge
    University''s Motion-based Segmentation and Recognition Dataset where each color
    represents a different class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/269955f2-4095-4b7a-8b25-b96fe313555a.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/
  prefs: []
  type: TYPE_NORMAL
- en: But in this example, a binary classifier is sufficient, which will become apparent
    as we go into the details of the project. However, I wanted to highlight it here
    as the architecture will scale to multi-class classification by simply swapping
    the last layer with a softmax activation and changing the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: You have thus seen the architecture we will be using in this chapter. Let's
    now look at how we will use it and the data used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Data to drive the desired effect – action shots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now would be a good time to introduce the photo effect we want to create in
    this chapter. The effect, as I know it, is called an **action shot.** It''s essentially
    a still photograph that shows someone (or something) in motion, probably best
    illustrated with an image - like the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cc3c762-cb35-4a7c-8882-06f407243dee.png)'
  prefs: []
  type: TYPE_IMG
- en: As previously mentioned, the model we used in this chapter performs binary (or
    single-class) classification. This simplification, using a binary classifier instead
    of a multi-class classifier, has been driven by the intended use that is just
    segmenting people from the background. Similar to any software project, you should
    strive for simplicity where you can.
  prefs: []
  type: TYPE_NORMAL
- en: 'To extract people, we need a model to learn how to recognize people and their
    associated pixels. For this, we need a dataset consisting of images of people
    and corresponding images with those pixels of the persons labeled—and lots of
    them. Unlike datasets for classification, datasets for object segmentation are
    not so common nor as vast. This is understandable given the additional effort
    that would be required to label such a dataset. Some common datasets for object
    segmentation, and ones that are considered for this chapter, include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PASCAL VOC**: A dataset with 9,993 labeled images across 20 classes. You
    can find the dataset at [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labeled Faces in the Wild (LFW) from University of Massachusetts Amherst**: A
    dataset comprising 2,927 faces. Each has the hair, skin, and background labeled
    (three classes). You can find the dataset at [http://vis-www.cs.umass.edu/lfw/part_labels/](http://vis-www.cs.umass.edu/lfw/part_labels/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Common Objects in Context (COCO) dataset**: A popular dataset for all things
    related to computer vision, including segmentation. Its segmented datasets comprise
    approximately 200,000 labeled images across 80 classes. It''s the dataset that
    was used and which we will be briefly exploring in this section. You can find
    the dataset at [http://cocodataset.org/#home](http://cocodataset.org/#home).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not considered for this project but good to be aware of is the **Cambridge-driving
    Labeled Video Database** (**CamVid**) from Cambridge University. As is clear from
    the name, the dataset is made up of frames from a video feed from a car camera—ideal
    for anyone interested in training their own self-driving car. You can find the
    dataset at [http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing the datasets here is possibly superfluous, but semantic segmentation
    is such an exciting opportunity with huge potential that I hope listing these
    here will encourage you to explore and experiment with new applications of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily for us, COCO''s 13+ GB dataset contains many labeled images of people
    and a convenient API to make finding relevant images easy. For this chapter, COCO''s
    API was used to find all images including people. Then, these were filtered further,
    only keeping those that contained either one or two people and whose area covered
    between 20% and 70% of the image, discarding those images where the person was
    too small or too large. For each of these images, the contours of each of the
    persons were fetched and then used to create a binary mask, which then became
    our labels for our training. The following figure illustrates this process for
    a single image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/130f0640-fac9-4810-8b88-a875cd0b11a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: The COCO dataset (http://cocodataset.org)'
  prefs: []
  type: TYPE_NORMAL
- en: After training on 8,689 images over 40 epochs, an **Intersection over Union**
    (**IoU**) coefficient (also known as the **dice coefficient**) of 0.8192 was achieved
    on the validation data (approximately 300).
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, IoU sounds familiar as it was what we used back in [Chapter 5](6365f272-41e9-4511-a564-dc0f8db5d3ca.xhtml),
    *Locating Objects in the World*. As a reminder, IoU is an evaluation metric used
    to measure how well two bounding boxes overlap each other. A perfect overlap,
    where both bounding boxes overlap each other perfectly, would return 1.0 (which
    is why the loss is negated for training).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following image, we get to see what this looks like, starting with random
    examples from the validation set. Then, some are manually searched for, like the
    ones that portray actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0b5ea40-c7b0-473c-a4ac-2ee4b6e66bbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: The COCO dataset (http://cocodataset.org)'
  prefs: []
  type: TYPE_NORMAL
- en: 'And here are some examples of action images where the model was able to sufficiently segment
    the person from the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3eea43d5-3b5b-4f33-b880-7de6224dc15f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, here are some, out of many, examples of action images where the model
    was less successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/991af62f-8fb1-48af-99b2-ccb419db803b.png)'
  prefs: []
  type: TYPE_IMG
- en: We have covered the model and training data and examined the outputs of the
    model. It's now time to turn our attention to the application in this chapter,
    which we will begin working on in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building the photo effects application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be looking briefly at the application and highlighting
    some of the interesting pieces of the code, omitting most of it as it has already
    been discussed in previous chapters. As mentioned in the introduction, this example
    is to provide a case study for a later section, where we will discuss some broad
    strategies to use when building intelligent interfaces and services.
  prefs: []
  type: TYPE_NORMAL
- en: If you haven't already, pull down the latest code from the accompanying repository
    at [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the `Chapter9/Start/` directory and open the project
    `ActionShot.xcodeproj`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the previous section, the example for this chapter is an photo
    effects application. In it, the user is able to take an *action shot*, have the
    application extract each person from the frames, and compose them onto the final
    frame, as illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6ce6ad3-e903-40ec-b1db-8b989b4294f9.png)'
  prefs: []
  type: TYPE_IMG
- en: The application consists of two view controllers; one is responsible for capturing
    the frames and the other for presenting the composite image. The workhorse for
    the processing, once again, has been delegated to the `ImageProcessor` class and
    it is the perspective from which we will be reviewing this project.
  prefs: []
  type: TYPE_NORMAL
- en: '`ImageProcessor` acts as both the sink and processor; by sink I refer to it
    being the class that is passed captured frames from the camera, using the `CameraViewController`,
    and holding them in memory for processing. Let''s see what the code for this looks
    like; select `ImageProcessor.swift` from the left panel to bring the source code
    into focus. Let''s see what exists; initially paying particular attention to the
    properties and methods responsible for handling received frames and then move
    on to their processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top of the file, you will notice that a protocol has been declared,
    which is implemented by the `EffectViewController`; it is used to broadcast the
    progress of the tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first callback, `onImageProcessorFinishedProcessingFrame`, is used to notify
    the delegate of frame-by-frame processing progress while the other, `onImageProcessorFinishedComposition`,
    is used to notify the delegate once the final image has be created. These discrete
    callbacks are intentionally split as the processing has been broken down into
    segmentation and composition. Segmentation is responsible for segmenting each
    of the frames using our model, and composition is responsible for generating the
    final image using the processed (segmented) frames. This structure is also mimicked
    in the layout of the class, with the class broken down into four parts and the
    flow we will follow in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part declares all the variables. The second implements the properties
    and methods responsible for retrieving the frames while they''re being captured.
    The third contains all the methods for processing the frames, whereby the delegate
    is notified using the `onImageProcessorFinishedProcessingFrame` callback. The
    final part, and the one we will focus on the most, contains the methods responsible
    for generating the final image, that is, it composites the frames. Let''s peek
    at the first part to get a sense of what variables are available, which are shown
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Nothing extraordinary. We first declare a property that wraps our model in an
    instance of `VNCoreMLModel` so that we can take advantage of the Vision framework's
    preprocessing functionality. We then declare a series of variables to deal with
    storing the frames and handling the processing; we make use of an `NSLock` instance
    to avoid different threads reading stale property values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet, and part of the `ImageProcessor` class, includes
    variables and methods for handling retrieving and releasing the captured frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Although fairly verbose, it should all be self-explanatory; probably the only
    method worth outlying is the method `addFrame`, which is called each time a frame
    is captured by the camera. To give some bearing of how everything is tied together,
    the following diagram illustrates the general flow whilst capturing frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a160b4da-708f-4a9c-a037-8009ce7a17ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The details of the flow are covered in the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Although capturing of the frames is persistent throughout the lifetime of the
    `CameraViewController`, they are only passed to the `ImageProcessor` once flagged
    once the user taps (and holds) their finger on the action button
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During this time, each frame that is captured (at the throttled rate—currently
    10 frames per second) is passed to the `CameraViewController`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This subsequently passes it to the `ImageProcessor` using the `addFrame` method
    shown earlier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The capturing stops when the user lifts their finger from the action button
    and, once finished, the `EffectsViewController` is instantiated and presented,
    along with passing it a reference to the `ImageProcessor` with the reference to
    the captured frames
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next part of the `ImageProcessor` class is responsible for processing each
    of these images; this is initiated using the `processFrames` method, which is
    called by the `EffectsViewController` once it has loaded. This part has a lot
    more code, but most of it should be familiar to you as it''s the boilerplate code
    we''ve used in many of the projects during the course of this book. Let''s start
    by inspecting the `processFrames` method, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: All of the remaining code is assumed to be inside the `ImageProcessor` class
    for the rest of this chapter unless stated otherwise; that is, the class and class
    extension declaration will be omitted to make the code easier to read.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This method simply dispatches the method call `processingNextFrame` to the
    background thread. This is mandatory when performing inference with Core ML and
    also a good practice when performing compute-intensive tasks to avoid locking
    up the user interface. Let''s continue the trail by inspecting the `processingNextFrame`
    method along with the method responsible for returning an instance of a `VNCoreMLRequest`,
    which is shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We start off by setting the property `isProcessingImage` to `true` and checking
    that we have a frame to process, otherwise exiting early from the method.
  prefs: []
  type: TYPE_NORMAL
- en: The following might seem a little counter-intuitive (because it is); we have
    seen from previous chapters that `VNCoreMLRequest` handles the preprocessing task
    of resizing the cropping of our images. So, why are we doing it manually here?
    The reason has more to do with keeping the code simpler and meeting publishing
    deadlines. In this example, the final image is composited using the resized frames
    to avoid scaling and offsetting the output from the model, which I'll leave as
    an exercise for you. So here, we are performing that operation and persisting
    the result in the array `processedImages` to be used in the final stage. Finally,
    we execute the request, passing in the image, which calls our method `processRequest`
    once finished, passing in the results from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing on our trail, we will now inspect the `processRequest` method; as
    this method is quite long, we will break it down into chunks, working top to bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We start off by getting the latest counts, which will be broadcast to the delegate
    when this method finishes or fails. Talking of which, the following block verifies
    that a result was returned of type `[VNPixelBufferObservation]`, otherwise notifying
    the delegate and returning, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'With reference to our result (`CVBufferPixel`), our next task is to create
    an instance of `CIImage`, passing in the buffer and requesting the color space
    to be grayscale to ensure that a single channel image is created. Then, we will
    be adding it to our `processedMasks` array, shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Only two more things left to do! We notify the delegate that we have finished
    a frame and proceed to process the next frame, if available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This concludes the third part of our `ImageProcessor`; at this point, we have
    two arrays containing the resized captured frames and the segmented images from
    our model. Before moving on to the final part of this class, let''s get a bird''s-eye
    view of what we just did, illustrated in this flow diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5a57ffb-0718-4d56-837f-d809613819f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The details of the flow are shown in the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the preceding diagram, processing is initiated once the `EffectsViewController` is
    loaded, which kicks off the background thread to process each of the captured
    frames
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each frame is first resized and cropped to match the output of the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it is added to the `processedFrames` array and passed to our model for
    interference (segmentation)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model returns with the result, we instantiate a single color instance
    of `CIImage`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This instance is stored in the array `processedMasks` and the delegate is notified
    of the progress
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens when all frames have been processed? This is what we plan on answering
    in the next part, where we will discuss the details of how to create the effect.
    To start with, let's discuss how the process is initiated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the delegate (`EffectsViewController`) receives a callback, using `onImageProcessorFinishedProcessingFrame`,
    where all of the frames have been processed, it calls the `compositeFrames` method from
    the ImageProcessor to start the process of creating the effect. Let''s review
    this and the existing code within this part of the `ImageProcessor` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: I have bolded the important/interesting parts, essentially the parts we will
    be implementing, but before writing any more code, let's review what we currently
    have (in terms of processed images) and an approach to creating our effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this stage, we have an array of `processedFrames` that contains the resized
    and cropped versions of the captured images, and we have another array, `processedMasks`,
    containing the single-channel images from our segmentation model. Examples of
    these are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb47ab2e-1b0b-46c4-a923-a9aa455d214e.png)'
  prefs: []
  type: TYPE_IMG
- en: If we were to composite each of the frames as they are, we would end up with
    a lot of unwanted artifacts and excessive overlapping. One approach could be to
    adjust the frames that have been processed (and possibly captured), that is, skip
    every *n* frames to spread out the frames. The problem with this approach is that
    it assumes all subjects will be moving at the same speed; to account for this,
    you would need to expose this tuning to the user for manual adjustment (which
    is an reasonable approach). The approach we will take here will be to extract
    the bounding box for each of the frames, and using the displacement and relative
    overlap of these to determine when to insert a frame and when to skip a frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the bounding box, we simply scan each line from each of the edges
    of the image, that is, from **top to bottom**, to determine the top of the object.
    Then, we do it **bottom to top** to determine the bottom of the object. Similarly,
    we do it on the horizontal axis, illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97e10b70-a9aa-4fe0-9eec-e4d1fa82e31f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Even with bounding boxes, we still need to determine how far the object should
    move before inserting a frame. To determine this, we first determine the dominant
    direction, which is calculated by finding the direction between the first and
    last frames of the segmented object. This is then used to determine what axis
    to compare displacement on; that is, if the dominant direction is in the horizontal
    axis (as shown in the preceding figure), then we measure the displacement across
    the *x *axis, ignoring the *y *axis. We then simply measure the distance between
    the frames against some predetermined threshold to decide whether to composite
    the frame or ignore it. This is illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c90dd24b-e51f-4311-b6ad-b04afca724b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see what this looks like in code, starting from determining the dominant
    direction. Add the following code to the `getDominantDirection` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As described earlier, we first find the bounding boxes of the start and end
    of our sequence of frames, and use their centers to calculate the dominate direction.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of the `CIImage` method `getContentBoundingBox` is omitted
    here, but it can be found in the accompanying the source code within the `CIImage+Extension.swift` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Armed with the dominant direction, we can now proceed with determining what
    frames to include and what frames to ignore. We will implement this in the method
    `getIndiciesOfBestFrames` of the `ImageProcessor` class, which iterates over all
    frames, measuring the overlap and ignoring those that don''t meet a specific threshold.
    The method returns an array of indices that satisfy this threshold to be composited
    onto the final image. Add the following code to the `getIndiciesOfBestFrames`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin by getting the dominant direction, as discussed earlier, and then
    proceed to iterate through our sequence of frames in reverse order (reverse as
    it is assumed that the user''s heroshot is the last frame). With each frame, we
    obtain the bounding box, and if it''s the first frame to be checked, we assign
    it to the variable `previousBoundingBox`. This will be used to compare subsequent
    bounding boxes (and updated to the latest included frame). If `previousBoundingBox`
    is not null, then we calculate the displacement between the two based on the dominant
    direction, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We then calculate the minimum length needed to separate the two objects, which
    is calculated by the combined size of the relative axis divided by 2\. This gives
    us a distance of half of the combined frame, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We then compare the distance with the bounds along with a threshold and proceed
    to add the frame to the current index if the distance satisfies this threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Returning to the `compositeFrames` method, we are now ready to composite the
    selected frames. To achieve this, we will leverage `CoreImages` filters; but before
    doing so, let's quickly review what it is exactly that we want to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each selected (processed) image and mask pair, we want to clip out the
    image and overlay it onto the final image. To improve the effect, we will apply
    a progressively increasing alpha so that frames closer to the final frame will
    have an opacity closer to 1.0 while the frames further away will be progressively
    transparent; this will give us a faded trailing effect. This process is summarized
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a61021e-de1e-4cd3-8f29-eb8ab88899df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s turn this into code by first implementing the filter. As shown earlier,
    we will be passing the kernel the output image, the overlay and its corresponding
    mask, and an alpha. Near the top of the `ImageProcessor` class, add the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Previously, we have implemented the `CIColorKernel`, which is responsible for
    compositing all of our frames onto the final image as discussed. We start by testing
    the mask''s value, and if it is 1.0, we assign the strength 1.0 (meaning we want
    to replace the color at that location of the final image with that of the overlay).
    Otherwise, we assign 0, ignoring it. Then, we multiply the strength with the blend
    argument passed to our kernel. Finally, we calculate and return the final color
    with the statement `vec4(image.rgb * (1.0-overlayStrength), 1.0) + vec4(overlay.rgb
    * (overlayStrength), 1.0)`. With our filter now implemented, let''s return the `compositeFrames`
    method and put it to use. Within `compositeFrames`, replace the comment `// TODO
    Composite final image using segments from intermediate frames` with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Most of this should be self-explanatory; we start by calculating an alpha stride
    that will be used to progressively increase opacity as we get closer to the final
    frame. We then iterate through all the selected frames, applying the filter we
    just implemented in the preceding snippet, compositing our final image.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that done, we have now finished this method and the coding for this chapter.
    Well done! It''s time to test it out; build and run the project to see your hard
    work in action. The following is a result from a weekend park visit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4caad02-de67-4097-bdf1-90c31c307d45.png)'
  prefs: []
  type: TYPE_IMG
- en: Before wrapping up this chapter, let's briefly discuss some strategies when
    working with machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Working with probabilistic results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As alluded to at the beginning of this chapter and seen firsthand in the previous
    section, working with machine learning models requires a set of new techniques
    and strategies to deal with uncertainty. The approach taken will be domain-specific,
    but there are some broad strategies that are worth keeping in mind, and that's
    what we will cover in this section in the context of the example project of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first is improving the model. Of course, there may be limitations depending
    on the source of the model and dataset, but it's important to be able to understand
    ways in which the model can be improved as its output directly correlates to the
    quality of the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of this project, we can augment the model using an existing pre-trained
    image classifier as the encoder, as mentioned earlier. This not only fast-tracks
    training, providing more opportunities to iterate, but also is likely to improve
    performance by having the model transfer existing knowledge from a more comprehensive
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another is tuning the dataset that the model was trained on. A simple, and
    relevant, example of how the model can be improved can be seen by any image in
    which the user is holding an object (which has been labeled). An example of this
    can be seen in the following figure, in which the guitar is cropped from the person:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f869a3f5-a10a-40ed-bc75-1b1eeeb40f81.png)'
  prefs: []
  type: TYPE_IMG
- en: How you deal with this is dependent on the desired characteristics of your model.
    In the context of the application presented in this chapter, it would make sense
    to either perform multi-class classification, including objects normally held
    by people, or including them in the mask.
  prefs: []
  type: TYPE_NORMAL
- en: Another common technique is **data augmentation**. This is where you artificially
    adjust the image (input) to increase variance in your dataset, or even adjust
    it to make it more aligned with the data for your particular use case. Some example
    augmentations include blurring (useful when dealing with fast moving objects),
    rotation, adding random noise, color adjustment - essentially any image manipulation
    effect that introduces nuances that you are likely to get in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are many more techniques and tools to improve the model and
    data; here, our intention is just to highlight the main areas rather than delve
    into the details.
  prefs: []
  type: TYPE_NORMAL
- en: Designing in constraints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is somewhat unavoidable and how we design intelligent interfaces is still
    in its infancy. That is, how much transparency do you expose to the user? And
    how do you effectively assist them in building a useful mental model of your system
    without being distracting or losing the convenience of using the model in the
    first place? But here, I am simply referring to designing constraints within the
    experience to increase the chances of the model's success. A great, although slightly
    unrelated, example of this is household robots and the dishwasher. Despite its
    non-robotic characteristics, the faithful dishwasher can be considered a first
    generation robot for household tasks, like Rosie from Jetsons. Unlike Rosie, however,
    we had not been able to get the dishwasher to adapt to our environment. So, we
    adapted the environment for the dishwasher, that is, we encapsulated it in a box
    environment rather than using the existing kitchen sink we're accustomed to.
  prefs: []
  type: TYPE_NORMAL
- en: 'One simple approach is making the user aware of how to achieve the best results;
    in this example, it could be as simple as asking them to use a wall as their background.
    These hints can be delivered before use or delivered when there are signs of poor
    performance (or both). One approach for automatically detecting poor performance
    would be to measure the bounding box and its center of mass, comparing it with
    the expected center of mass, as illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0710dcbb-d0fd-4cb3-8899-36bd5b0f93df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Which brings us nicely to the next strategy: embedding heuristics.'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding heuristics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Heuristics is essentially codifying rules you have in your head to solve a particular
    task, typically implemented using a function that returns a score. This is used
    to rank a set of alternatives. In the previous section, *Designing in constraints*,
    we saw how we could use the center of mass and the bounding box to determine how
    well distributed the pixels are for a segmented image. This in turn could be used
    to rank each frame, by favoring those with a center of mass near the center of
    the bounding box. We also implemented a type of heuristic in the application when
    determining which frames to keep and which ones to ignore by measuring the overlap.
  prefs: []
  type: TYPE_NORMAL
- en: Heuristics can be a powerful ally, but be careful to ensure that the heuristics
    you derive can generalize well to your problem, just as you would expect a good
    model to. Also, be mindful of the additional computational cost incurred from
    using them.
  prefs: []
  type: TYPE_NORMAL
- en: Post-processing and ensemble techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Techniques from image processing, computer vision, and computer graphics can
    be borrowed to improve the quality of the output as well as detection. As an example,
    a typical image processing task is performing the morphology operations of opening
    and closing. This combination is commonly used to remove noise and fill in small
    holes in a binary image. Another useful post-processing task we could borrow from computer
    vision is watersheds, a segmentation technique that treats the image as a topographical
    map, where the intensity of change defines the ridges and the boundary of the
    fill (or segmentation).
  prefs: []
  type: TYPE_NORMAL
- en: Another tool to use for post-processing is another model. You're familiar with
    YOLO for object detection. We can apply it to obtain its predicted boundaries
    of the object, which we can then use to refine our segmentation. Another model,
    and one being adopted for this task, is **conditional random fields** (**CRF**),
    which is capable of smoothing out the edges of our mask.
  prefs: []
  type: TYPE_NORMAL
- en: There are a vast number of techniques available from the fields of image processing,
    computer vision, and computer graphics, and I strongly encourage you to explore
    each area to build up your tool set.
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to computer vision, then I recommend the books *Computer Vision
    and Image Processing* by T. Morris and *Algorithms for Image Processing and Computer
    Vision* by J. Parker for a pragmatic introduction to the field.
  prefs: []
  type: TYPE_NORMAL
- en: Human assistance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes it''s unavoidable or even desirable to include the human in tuning
    the output from the model. In these instances, the model is used to assist the
    user rather than completely automating the task. A few approaches that could be
    employed for the project in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide an intermediate step where the user can tidy up the masks. By this,
    I mean allowing the user to erase parts of the mask that have been incorrectly
    classified or are unwanted by the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Present the user with a series of frames and have them select the frames for
    composition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Present the user with variations of the final composited image and have them
    select the one with the most appeal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another related concept is introducing human-in-the-loop machine learning. This
    has a human intervening when the model is not confident in its prediction, and
    it passes the responsibility over to the user for classification and/or correction.
    The amendments from the user are then stored and used for training the model to
    improve performance. In this example, we could let the user (or crowd-source this
    task) segment the image and use this data when re-training the model. Eventually,
    given sufficient data, the model will improve its performance relevant to the
    context it is being used in.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this section highlighted the importance of handling uncertainty when
    working with machine models and provided enough of a springboard so that you can
    approach designing intelligent applications from the perspectives outlined here.
    Let's now conclude this chapter by reviewing what we have covered.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the concept of semantic segmentation, an approach
    that gives our applications increased perceptual understanding of our photos and
    videos. It works by training a model to assign each pixel to a specific class.
    One popular architecture for this is U-Net, which achieves high-precision localization
    by preserving spatial information, by bridging the convolutional layers. We then
    reviewed the data used for training along with some example outputs of the model,
    including examples that highlight the limitations of the model.
  prefs: []
  type: TYPE_NORMAL
- en: We then saw how this model could be used by creating an image effects application,
    where the segmented images were used to clip people from a series of frames and
    composite them together to create an action shot. But this is just one example
    of how semantic segmentation can be applied; it's frequently used in domains such
    as robotics, security surveillance, and quality assurance in factories, to name
    a few. How else it can be applied is up to you.
  prefs: []
  type: TYPE_NORMAL
- en: In the final section, we spent some time discussing strategies when dealing
    with models, specifically, their probabilistic (or level of uncertainty) outputs,
    to improve user experience.
  prefs: []
  type: TYPE_NORMAL
- en: This is the last of our examples of applying machine learning. In the next,
    and last, chapter, we'll shift gears and provide a primer into building your own
    models with the help of Create ML. Let's get started.
  prefs: []
  type: TYPE_NORMAL
