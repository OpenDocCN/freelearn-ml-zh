- en: '*Chapter 7*: Neural Network Classifier with TPOT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you''ll learn how to build your deep learning classifier in
    an automated fashion – by using the TPOT library. It''s assumed that you know
    the basics of artificial neural networks, so terms such as *neurons*, *layers*,
    *activation functions*, and *learning rates* should sound familiar. If you don''t
    know how to explain these terms simply, please revisit [*Chapter 6*](B16954_06_Final_SK_ePub.xhtml#_idTextAnchor073),
    *Getting Started with Deep Learning: Crash Course in Neural Networks*.'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, you'll learn how easy it is to build a simple classifier
    based on neural networks and how you can tweak the neural network so that it better
    suits your needs and the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring options for training neural network classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a neural network classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You don''t need any prior hands-on experience with deep learning and neural
    networks. A knowledge of some basics concepts and terminology is a must, however.
    If you''re entirely new to the subject, please revisit [*Chapter 6*](B16954_06_Final_SK_ePub.xhtml#_idTextAnchor073),
    *Getting Started with Deep Learning: Crash Course in Neural Networks*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the source code and dataset for this chapter here: [https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter07](https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter07).'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no reason to go wild with the dataset. Just because we can train neural
    network models with TPOT doesn't mean we should spend 50+ pages exploring and
    transforming needlessly complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For that reason, you''ll use a scikit-learn built-in dataset throughout the
    chapter – the Breast cancer dataset. This dataset doesn''t have to be downloaded
    from the web as it comes built-in with scikit-learn. Let''s start by loading and
    exploring it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, you''ll need to load in a couple of libraries. We''re importing NumPy,
    pandas, Matplotlib, and Seaborn for easy data analysis and visualization. Also,
    we''re importing the `load_breast_cancer` function from the `sklearn.datasets`
    module. That''s the function that will load in the dataset. Finally, the `rcParams`
    module is imported from Matplotlib to make default styling a bit easier on the
    eyes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now use the `load_breast_cancer` function to load in the dataset. The
    function returns a dictionary, so we can use the `keys()` method to print the
    keys:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Dictionary keys of the Breast cancer dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.1 – Dictionary keys of the Breast cancer dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can now use this dictionary to extract attributes of interest. What is essential
    for now are the `data` and `target` keys. You can store their values to separate
    variables and then construct a data frame object from them. Working with raw values
    is possible, but a pandas data frame data structure will allow easier data manipulation,
    transformation, and exploration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s how you can transform these to a pandas data frame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following table:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Sample of eights rows from the Breast cancer dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_2.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.2 – Sample of eights rows from the Breast cancer dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first thing you always want to do, analysis-wise, is to check for missing
    data. Pandas has an `isnull()` method built in, which returns Booleans for every
    value in the dataset. You can then call the `sum()` method on top of these results
    to get the count of missing values per column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Missing value count per column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_3.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.3 – Missing value count per column
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, there are no missing values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next thing to do in the exploratory phase is to get familiar with your dataset.
    Data visualization techniques can provide an excellent way of doing so.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, you can declare a function called `make_count_chart()` that takes
    any categorical attribute and visualizes its distribution. Here''s what the code
    for this function could look like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now use the following code snippet to visualize the target variable
    to find out how many instances were benign and how many were malignant:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Number of malignant and benign cases'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_4.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.4 – Number of malignant and benign cases
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, there's a decent amount more of malignant cases, so the classes
    aren't perfectly balanced. Class imbalance can lead to highly accurate but unusable
    models. Just imagine you are classifying a rare event. In every 10,000 transactions,
    only one is classified as an anomaly. Clearly, the machine learning model doesn't
    have much chance to learn what makes an anomaly so different from the rest.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Furthermore, always predicting that the transaction is normal leads to a 99.99%
    accurate model. State-of-the-art accuracy, for certain, but the model is unusable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are numerous techniques for dealing with imbalanced datasets, but they
    are beyond the scope for this book.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next stop – correlation analysis. The aim of this step is to take a glimpse
    at which feature(s) have the biggest effect on the target variables. In other
    words, we want to establish how correlated a change in direction in a feature
    is with the target class. Visualizing an entire correlation matrix on a dataset
    of 30+ columns isn't the best idea because it would require a figure too large
    to fit comfortably on a single page. Instead, we can calculate the correlation
    of the feature with the target variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s how you can do this for the `mean area` feature – by calling the `corrcoeff()`
    method from NumPy:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Correlation coefficient between a single feature and the target
    variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_5.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please note the `[:-1]` at the beginning of the loop. Since the target variable
    is the last column, we can use the aforementioned slicing technique to exclude
    the target variable from the correlation calculation. The correlation coefficient
    between the target variable and the non-target variable would be 1, which is not
    particularly useful to us.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can now use the following code to make a horizontal bar chart of the correlations
    with the target variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Feature correlation with the target variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_6.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.6 – Feature correlation with the target variable
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, most of the features have a high negative correlation with the
    target variable. Negative correlation means that one variable increases as the
    other one decreases. In our case, a decrease in the number of features leads to
    an increase in the target variable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You could also visualize the distribution of every numeric column with respect
    to the target variable value. To be more precise, this means two separate histograms
    are drawn on a single chart, and each histogram shows the distribution only for
    the respective target value's subset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, this means that one histogram will show the distribution of malignant
    and the other of benign instances, for each variable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The code snippet you're about to see declares a `draw_histogram()` function
    that goes over every column in a dataset, makes a histogram with respect to the
    distinct classes in the target variable, and appends this histogram to a figure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once all of the histograms are appended, the figure is displayed to the user.
    The user also has to specify how many rows and columns they want, which gives
    a bit of extra freedom when designing visualizations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here is the code snippet for drawing this histogram grid:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will be a pretty large data visualization, containing 9 rows and 4 columns.
    The last row will have only 2 histograms, as there are 30 continuous variables
    in total.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The results are shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Histogram for every continuous variable'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_07_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – Histogram for every continuous variable
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there is a distinct separation most of the time, so our model
    shouldn't have too much trouble making decent separations between the classes.
  prefs: []
  type: TYPE_NORMAL
- en: And that's all we'll do with regard to the exploratory data analysis. You can,
    and are encouraged to, do more, especially with custom and more complex datasets.
    The next section will introduce you to the options you have for training automated
    neural network classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring options for training neural network classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have a lot of options when training neural network models with TPOT. The
    whole neural network story is still new and experimental with TPOT, requiring
    a bit more manual work than regular scikit-learn estimators.
  prefs: []
  type: TYPE_NORMAL
- en: By default, TPOT won't use the neural network models unless you explicitly specify
    that it has to. This specification is done by selecting an adequate configuration
    dictionary that includes one or more neural network estimators (you can also write
    these manually).
  prefs: []
  type: TYPE_NORMAL
- en: 'The more convenient option is to import configuration dictionaries from the
    `tpot/config/classifier_nn.py` file. This file contains two PyTorch classifier
    configurations, as visible in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – TPOT PyTorch classifier configurations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_07_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – TPOT PyTorch classifier configurations
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding diagram, you can see that TPOT can currently handle two
    different types of classifiers based on deep learning libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression: shown in `tpot.builtins.PytorchLRClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multi-layer perceptron: shown in `tpot.builtins.PytorchMLPClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can either import this file or write the configurations manually. In addition,
    you can also specify your own configuration dictionaries, which somehow modify
    the existing ones. For example, you can use this code to use a PyTorch-based logistic
    regression estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Custom configurations will be discussed later in the chapter when we start to
    implement neural network classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: You should keep in mind that training neural network classifiers with TPOT is
    an expensive task and will typically take much more time to train than scikit-learn
    estimators. As a rule of thumb, you should expect the training time to be several
    orders of magnitude slower with neural networks. This is because neural network
    architectures can have millions of trainable and adjustable parameters, and finding
    the correct value for all of them takes time.
  prefs: []
  type: TYPE_NORMAL
- en: Having that in mind, you should always consider simpler options first, as TPOT
    is highly likely to give you an excellent-performing pipeline on the default scikit-learn
    estimators.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will continue with the training of neural network classifiers
    right where the previous section stopped and will show you how different training
    configurations can be used to train your models.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up to this point, we''ve loaded in the dataset and undertaken a basic exploratory
    data analysis. This section of the chapter will focus on training models through
    different configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: Before we can move on to model training, we need to split our dataset into training
    and testing subsets. Doing so will allow us to have a sample of the data never
    seen by the model, and which can later be used for evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet will split the data in a 75:25 ratio:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can begin with training next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As always, let's start simply by training a baseline model. This will serve
    as a minimum viable performance that the neural network classifier has to outperform.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The simplest binary classification algorithm is logistic regression. The following
    code snippet imports it from scikit-learn, alongside some evaluation metrics,
    such as a confusion matrix and an accuracy score. Furthermore, the snippet instantiates
    the model, trains it, makes a prediction on the holdout set, and prints the confusion
    matrix and the accuracy score.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The code snippet is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Confusion matrix and accuracy of the baseline model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_9.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.9 – Confusion matrix and accuracy of the baseline model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We now know that the baseline model is 96.5% accurate, making 4 false positives
    and 1 false negative. Next, we'll train an automated neural network classifier
    with TPOT and see how the results compare.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As mentioned before, training a neural network classifier with TPOT is a heavy
    task. For that reason, you might be better off switching to a free GPU Cloud environment,
    such as *Google Colab*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will ensure faster training time, but also you won''t melt your PC. Once
    there, you can use the following snippet to train the PyTorch-based logistic regression
    model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will train the model for two generations. You''ll see various outputs
    during training, such as the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.10 – TPOT neural network training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.12 – TPOT PyTorch logistic regression best pipeline'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Confusion matrix and accuracy score of a PyTorch logistic regression
    model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.13 – Confusion matrix and accuracy score of a PyTorch logistic regression
    model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, two generations weren't enough to produce a better-than-baseline
    model. Let's see whether using a multi-layer perceptron model could help.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We're still in the Google Colab environment, as training on your own PC is significantly
    slower (depending on your configuration). The idea now is to use the multi-layer
    perceptron model instead of logistic regression and see how the change in the
    model could affect performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To begin, you''ll have to make a change to the `template` parameter of `TPOTClassifier`,
    as shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can see, we''re now using `PytorchMLPClassifier` instead of `PytorchLRClassifier`.
    To begin the optimization process, simply call the `fit()` method with the training
    data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As with the logistic regression algorithm, you''ll also see the progress bar
    during the optimization process:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.14 – TPOT multi-layer perceptron training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.16 – TPOT PyTorch multi-layer perceptron best pipeline'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Confusion matrix and accuracy score of a PyTorch multi-layer
    perceptron model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_07_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.17 – Confusion matrix and accuracy score of a PyTorch multi-layer perceptron
    model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, two generations still weren't enough to produce a better-than-baseline
    model, but the MLP model outperformed the logistic regression one. Let's now see
    whether using a custom training configuration could push the accuracy even higher.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, let's see how you can specify possible hyperparameter values for either
    logistic regression or multi-layer perceptron models. All you have to do is specify
    a custom configuration dictionary, which holds the hyperparameters you want to
    test for (such as learning rate, batch size, and number of epochs), and assign
    values to those hyperparameters in the form of a list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s an example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now use this `custom_config` dictionary when training models. Here
    is an example training snippet based on a multi-layer perceptron model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can see, only the `config_dict` parameter has changed. Once the training
    process has started, you''ll see a progress bar similar to this one in the notebook:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.18 – TPOT custom tuning with neural networks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_07_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.18 – TPOT custom tuning with neural networks
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the training process is complete, you should see something along the following
    lines in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – TPOT multi-layer perceptron classifier with custom hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_07_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.19 – TPOT multi-layer perceptron classifier with custom hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: 'And that''s all there is to it! Just to verify, you can examine the best-fitted
    pipeline by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – TPOT best-fitted pipeline for a model with custom hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_07_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.20 – TPOT best-fitted pipeline for a model with custom hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, all of the hyperparameter values are within the specified range,
    which indicates that the custom model was trained successfully.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the model training portion of this section and this section in
    general. What follows is a brief summary of everything we have learned thus far,
    and a brief introduction to everything that will follow in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was quite intensive in terms of hands-on examples and demonstrations.
    You've hopefully managed to learn how to train automated classification pipelines
    with TPOT and what you can tweak during the process.
  prefs: []
  type: TYPE_NORMAL
- en: You should now be capable of training any kind of automated machine learning
    model with TPOT, whether we're talking about regression, classification, standard
    classifiers, or neural network classifiers. There is good news, as this was the
    last chapter with TPOT examples.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, [*Chapter 8*](B16954_08_Final_SK_ePub.xhtml#_idTextAnchor093),
    *TPOT Model Deployment*, you'll learn how to wrap the predictive functionality
    of your models inside a REST API, which will then be tested and deployed both
    locally and to the cloud. You'll also learn how to communicate with the API once
    it's deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the previous chapter, [*Chapter 9*](B16954_09_Final_SK_ePub.xhtml#_idTextAnchor102),
    *Using the Deployed TPOT Model in Production*, you'll learn how to develop something
    useful with the deployed APIs. To be more precise, you'll learn how to make predictions
    in the Notebook environment by making REST calls to the deployed API, and you'll
    learn how to develop a simple GUI application that makes your model presentable
    to the end user.
  prefs: []
  type: TYPE_NORMAL
- en: As always, feel free to study TPOT in more depth, but by now, you're well ahead
    of the majority, and you're ready to make machine learning useful. See you there!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which two algorithms are available in TPOT with regard to neural networks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Approximately how many times are neural network classifiers slower to train
    than the default, scikit-learn ones?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List and briefly explain the different hyperparameters available when training
    models with TPOT and neural networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you specify a custom range of hyperparameter values when training custom
    neural network models with TPOT? If so, how?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you find the best-fitted pipeline after the model has finished training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages of using a GPU runtime such as Google Colab when training
    neural network models with TPOT?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe why a single neuron in the multi-layer perceptron model can be thought
    of as logistic regression.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
