- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Examples and Case Studies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter embarks on a journey into the realm of machine learning, exploring
    practical applications and real-world examples that demonstrate its power and
    potential. In the previous chapters, we have learned all the essential skills
    required to build a good machine learning solution. In this chapter, we will utilize
    all the knowledge gained and build the following examples from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer churn example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will create a linear regression model to predict the value
    of a house in the California area. Letâ€™s begin by getting familiar with the dataset.
    We will use a common California house values dataset. This is a collection of
    data related to residential real estate properties in various regions of California,
    USA. It is commonly used in machine learning and data analysis tasks for predicting
    house prices based on various features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset we will use contains the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`medianIncome`: The median income of households in a specific block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`housingMedianAge`: The median age of houses in a block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`totalRooms`: The total number of rooms in the houses in a block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`totalBedrooms`: The total number of bedrooms in the houses in a block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`population`: The total population of the block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`households`: The total number of households (a group of people residing within
    a home unit) within a block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latitude`: The latitude of the geographical location of the house.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`longitude`: The longitude of the geographical location of the house.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`medianHouseValue`: The median value of houses in the block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`oceanProximity`: Categorical description of the distance to the ocean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a sample of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: Sample data from a California housing dataset](img/B19863_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Sample data from a California housing dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The example dataset can be found in the GitHub repository of this book. A good
    place to find other datasets is, for example, [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in our machine learning project is to define a question we want
    to answer using our model. In this case, we are using a rather simple historical
    dataset and therefore the framework used is modified a bit. Letâ€™s determine the
    following characteristics to start:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trigger**: A new house data is inserted into the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target**: The value of the house in US dollars'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Features**: Latitude, longitude, median age, total rooms, total bedrooms,
    population, households, median income, ocean proximity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning question**: Predicting what will the house value be in the
    California area?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To begin our actual work, letâ€™s first upload `housing_test.csv` and `housing_train.csv`
    into our Qlik cloud tenant. These files can be found in the GitHub repository
    of this book. As you can see, the dataset is already split into train and test
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In a normal machine learning project, we would need to take care of encoding
    the categorical fields, handling null values, scaling and so on, but in our case,
    Qlik AutoML takes care of all these steps. Our next task is to create a new machine
    learning experiment (**Add New** ðŸ¡ª **New** **ML Experiment**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Give a name to your experiment, define a space you want to use, and press `housing_train.csv`,
    which we uploaded earlier. You should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2: Housing prices experiment â€“ target selection](img/B19863_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Housing prices experiment â€“ target selection'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will select our target variable. We can also select the features to
    be used in our experiment. Select `median_house_value` as the target and all other
    fields should be automatically selected to be included in our experiment. You
    should see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3: Target and features selected](img/B19863_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Target and features selected'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous image, we have also marked the feature type of `total_bedrooms`
    with a red square. Qlik has recognized this field as a string and forms a categorical
    feature by default. Change that to `total_bedrooms`, we can select **Run Experiment**
    from the bottom right corner. After a while, you should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4: Housing prices experiment â€“ first results](img/B19863_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Housing prices experiment â€“ first results'
  prefs: []
  type: TYPE_NORMAL
- en: When looking at the SHAP diagrams for our first version of the experiment, we
    can see that the `median_income` field has a rather high correlation with the
    predicted house values. Letâ€™s try to configure a second version of our experiment
    without that field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select `median_income` as in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5: Features reconfiguration](img/B19863_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Features reconfiguration'
  prefs: []
  type: TYPE_NORMAL
- en: Select `total_rooms` is the most determining feature, but our R2 score has also
    dropped. In this case, we will go with the first version of our experiment since
    it gave us better accuracy. You can try to configure multiple versions and experiment
    with the models to get a better model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the list at the top part of the screen, scroll down until you see the top-performing
    model of the first run and select it. Your screen should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6: The model selected for deployment.](img/B19863_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: The model selected for deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Deploy** from the bottom right corner. Enter a name and define the
    space for your newly deployed model. Make sure that **Enable real-time API access**
    is selected and press **Deploy**. Our model is now deployed and ready for use.
  prefs: []
  type: TYPE_NORMAL
- en: As we have learned in previous chapters, the deployed model itself provides
    information about the required schema, algorithm deployed, and some metadata from
    the experiment. You can open the deployed model and have a closer look at this
    information at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Our next task to get the predicted results into a finalized application is to
    create a new Qlik analytics application.
  prefs: []
  type: TYPE_NORMAL
- en: For the data in our application, we will import `housing_test.csv`. After that,
    we will create a new data connection for the Qlik AutoML model that we deployed
    in the earlier step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Qlik AutoML connection under `Id`. The connector settings should
    look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7: Connector settings](img/B19863_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Connector settings'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test the connection and save it after that. Next, we will select the data to
    load. In the `housing_test.csv` and select `housing_predictions` to be included.
    You should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8: The Select data to load window.](img/B19863_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: The Select data to load window.'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Insert script** and load the data into the application. You are now
    ready to create the actual dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you run into problems during the application creation, there is a sample
    application in the Github repository of this book for your reference.
  prefs: []
  type: TYPE_NORMAL
- en: We will not cover the dashboard creation part in detail to give you a chance
    to play with the different visualization options. The following image represents
    a sample dashboard that shows predicted house values on a map using a gradient
    color scheme, a histogram to show the distribution of prices, and a waterfall
    diagram to visualize SHAP values. Use the skills acquired from previous chapters
    and create a dashboard of your own to visualize the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9: Housing prices â€“ sample dashboard](img/B19863_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Housing prices â€“ sample dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: You can create multiple dashboards and try to cross-reference the data from
    multiple models if you re-run the experiment with different parameters. Try to
    play with the different settings and graph types to find an effective visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have implemented a linear regression example, it is time to move
    on to another example with a slightly more complex scenario. We will investigate
    the customer churn example next.
  prefs: []
  type: TYPE_NORMAL
- en: Customer churn example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our second example, we will create a binary model to predict customer churn
    for a bank. We are going to use a dataset that contains the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`customer_id`: A unique identifier for each customer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`credit_score`: A numerical representation of a customerâ€™s creditworthiness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`country`: The country where the customer resides.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gender`: The gender of the customer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age`: The age of the customer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tenure`: The duration of the customerâ€™s relationship with the company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`balance`: The current balance in the customerâ€™s account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`products_number`: The number of products the customer has brought from the
    company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`credit_card`: A binary indicator showing whether the customer holds a credit
    card with the company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`active_member`: A binary indicator indicating whether the customer is currently
    an active member of the company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`estimated_salary`: An approximate estimation of the customerâ€™s salary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`churn`: A binary indicator showing whether the customer has churned (`1`)
    or not (`0`). Churning refers to customers who have ended their relationship with
    the company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a sample of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10: Customer churn â€“ sample data](img/B19863_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Customer churn â€“ sample data'
  prefs: []
  type: TYPE_NORMAL
- en: To start with our machine learning project, we will first upload `Bank Customer
    Churn Prediction.csv` into Qlik Cloud. This data file can be found in the GitHub
    repository of this book. We also must split the dataset. Qlik AutoML splits the
    dataset during the experiment phase, but we will also create separate training
    and test datasets to get a better understanding of the modelâ€™s performance. To
    split the dataset, we will use the training and test data in a ratio of 70:30.
  prefs: []
  type: TYPE_NORMAL
- en: 'Splitting the dataset can be done in Qlik. To do that, we will first create
    a new analytics application and name it `Churn Data Prep`. Next, create a data
    connection to the location that contains the previously uploaded `Bank Customer
    Churn Prediction.csv` file. You can use the following code to do the splitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code reads `Bank Customer Churn.csv` using the file data connection
    and adds a row number to each row. Then, two subsets (training and test) are created
    using this row number and are stored in QVD files. The `Row number` field is dropped
    before storing because we donâ€™t need it in our machine learning project. As a
    result of the script, the new data files (`banking_churn_train.qvd` and `banking_churn_test.qvd`)
    are stored in the same location as the original data file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will start to investigate the data to form a machine learning question.
    Data exploration can be done in the same Qlik application where you split the
    data. Remember to drop the test and train tables before continuing with the visualizations.
    An example of an analysis view is represented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11: Customer churn example - Initial analysis](img/B19863_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Customer churn example - Initial analysis'
  prefs: []
  type: TYPE_NORMAL
- en: If we plot the values using a histogram, we can see that `credit_score` and
    `age` follow a normal distribution. If we look at the balance, there are lot of
    zero values, but the rest of the data is normally distributed. If we look at the
    tenure, there are only a few customers with 1 year of tenure and 95 of those have
    been churned. This is important information when defining our prediction window.
    We can also see that there are no issues in the data. You can play with different
    visualizations when getting familiar with the data. An example can be found in
    the GitHub repository of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'After investigating the data, we can use the following framework to form our
    machine learning question:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event trigger**: When a new customer subscribes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target**: When a customer leaves the company services (churn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Binary outcome: Yes or No'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The horizon is based on the average churned customer tenure length (around five
    years)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`active_member`, `age`, `balance`, `country`, `credit_card`, `credit_score`,
    `estimated_salary`, `gender`, `products_number`, and `tenure`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction point**: One year after subscription'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning question**: After one year of activity as a customer, will
    the customer churn during the first five years?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By defining our model using the framework, we have defined that after a new
    customer has signed, we will collect data during the first year and then predict
    whether the customer will churn during the first five years. We can re-calculate
    the predictions periodically after the initial results when we get new data (for
    example, every six months after the initial results). Since we had a minimal number
    of customers that churned during the first year, our data accumulation window
    (the time between the event trigger and the prediction point) is not too long.
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s now create the actual machine learning experiment using our training
    dataset. Start by creating a new experiment and select the correct dataset (`banking_churn_train.qvd`).
    Select `churn` as the target and all the other fields except `customer_id` as
    features. The following figure represents the first experiment setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12: Experiment setup](img/B19863_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: Experiment setup'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that Qlik AutoML recognized the categorical features and will automatically
    apply one-hot encoding to these fields. You can now run the experiment. After
    the first run, the experiment returns the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13: First results from the experiment](img/B19863_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.13: First results from the experiment'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see from the confusion matrix, the rate of false negatives is quite
    large and our ROC curve indicates that our model is not performing well. From
    the permutation and SHAP importance graphs we can see that `age` and `products_number`
    correlate highly with the result. Letâ€™s try to make another run without these
    variables and see if we will get more accurate results. Select `age` and `products_number`
    and press **Run v2**. You should see the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14: Results after modification](img/B19863_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.14: Results after modification'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the accuracy of the model dropped so our changes were not beneficial.
    After a few iterations, we will find the best combination of features. After that,
    it is possible to finetune the model even more by enabling hyperparameter optimization
    from the settings and defining a time window for that. You can try different combinations
    and investigate the model performance. Once you are done, configure the new version
    and select the following fields: `age`, `products_number`, `active_member`, `gender`,
    `balance`, and `country`. Also, enable hyperparameter optimization and set the
    window to one hour. You should see the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15: Results from the optimized experiment](img/B19863_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.15: Results from the optimized experiment'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the accuracy of our final model is 84.4% and the F1 score is
    0.623\. The model is not a top performer but will give us relatively good results.
    Select the top-performing model and press **Deploy**. This will create a machine
    learning deployment for us. You can open the deployed model. Verify that you see
    the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16: Banking churn schema](img/B19863_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.16: Banking churn schema'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create a new analytics application and load `banking_churn_test.qvd`
    into it as a data table. You should do this in script view. We should also create
    a data connection to our newly deployed model. To create the correct data connection,
    select Qlik AutoML from the connection list and select the model from the dropdown
    menu. Give a name to the returned table and select SHAP -values and errors to
    be included. Type `customer_id` into **Association Field**. Your settings should
    look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17: Connection settings](img/B19863_10_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.17: Connection settings'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, add the predictions to the script. Enter the name of the `banking_churn_test`
    table into the **Resident Table** field and select the result set. Select **Insert
    script**. Your code should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will load the test dataset in the preceding code, and then call the model
    endpoint through the data connector. After you complete the script, select **Load
    data**. Open **Data model viewer** and verify that you can see two tables connected.
    Next, we will focus on creating the actual application. You should try different
    visualization types and connect to the model using the **server-side extension**
    (**SSE**) syntax. An example dashboard may look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.18: Churn analysis example](img/B19863_10_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.18: Churn analysis example'
  prefs: []
  type: TYPE_NORMAL
- en: We will not cover the creation of every visualization in this chapter since
    you should experiment with the data and visualizations. A sample application is
    provided as part of the materials in the GitHub repository. We have now successfully
    implemented two different machine learning solutions with different use cases
    and studied how to form a machine learning question to be answered. We have also
    learned how to optimize and finetune a model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we utilized the skills learned during the previous chapters
    by implementing two different use cases. In our first example, we studied the
    data of houses in California and created a model to predict their prices based
    on house-related variables. We created an application to utilize our model and
    learned about the iterations and how to interpret the experiment results.
  prefs: []
  type: TYPE_NORMAL
- en: In our second example, we learned how to form a customer churn model and utilize
    it in multiple ways. We also learned how to create different datasets from our
    original data file and how to form a machine learning question using a framework.
    We visualized the results using native visualizations in Qlik Sense.
  prefs: []
  type: TYPE_NORMAL
- en: In our next and last chapter, we will look into the future. We will investigate
    current trends in machine learning and artificial intelligence and try to predict
    how these might evolve in the future. We will also investigate megatrends and
    get familiar with the characteristics of a megatrend. We will also think about
    the evaluation of possible megatrends. Understanding megatrends is a crucial skill
    in being able to compete and evolve.
  prefs: []
  type: TYPE_NORMAL
