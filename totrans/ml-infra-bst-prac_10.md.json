["```py\n# create the feature extractor, i.e., BOW vectorizer\n# please note the argument - max_features\n# this argument says that we only want three features\n# this will illustrate that we can get problems - e.g. noise\n# when using too few features\nvectorizer = CountVectorizer(max_features = 3)\n```", "```py\nfrom tokenizers import BertWordPieceTokenizer\n# initialize the actual tokenizer\ntokenizer = BertWordPieceTokenizer(\n    clean_text=True,\n    handle_chinese_chars=False,\n    strip_accents=False,\n    lowercase=True\n)\n```", "```py\n# and train the tokenizer based on the text\ntokenizer.train(files=paths,\n                vocab_size=30_000,\n                min_frequency=1,\n                limit_alphabet=1000,\n                wordpieces_prefix='##',\n                special_tokens=['[PAD', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\n```", "```py\n'##ll': 183,\n'disable': 326,\n'al': 263,\n'##cket': 90,\n'##s': 65,\n'computed': 484\n```", "```py\nstrCProgram = '''\nint main(int argc, void **argc)\n{\n  printf(\"%s\", \"Hello World\\n\");\n  return 0;\n}\n'''\n# now, let's see how the tokenizer works\n# we invoke it based on the program above\ntokenizedText = tokenizer.encode(strCProgram)\ntokenizedText.tokens\n```", "```py\n'in', '##t', 'ma', '##in', '(', 'in', '##t', 'a', '##r', '##g'\n```", "```py\n110, 57, 272, 104, 10, 110, 57, 30, 61, 63\n```", "```py\n# in this example we use the tokenizers\n# from the HuggingFace library\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\n# we instantiate the tokenizer\ntokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n```", "```py\nfrom tokenizers.trainers import BpeTrainer\n# here we instantiate the trainer\n# which is a specific class that will manage\n# the training process of the tokenizer\ntrainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\",\n                     \"[SEP]\", \"[PAD]\", \"[MASK]\"])\nfrom tokenizers.pre_tokenizers import Whitespace\ntokenizer.pre_tokenizer = Whitespace()\n# now, we need to prepare a dataset\n# in our case, let's just read a dataset that is a code of a program\n# in this example, I use the file from an open-source component - Azure NetX\n# the actual part is not that important, as long as we have a set of\n# tokens that we want to analyze\npaths = ['/content/drive/MyDrive/ds/cs_dos/nx_icmp_checksum_compute.c']\n# finally, we are ready to train the tokenizer\ntokenizer.train(paths, trainer)\n```", "```py\n'only': 565, 'he': 87, 'RTOS': 416, 'DE': 266, 'CH': 154, 'a': 54, 'ps': 534, 'will': 372, 'NX_SHIFT_BY': 311, 'O': 42,\n```", "```py\nimport sentencepiece as spm\n# this statement trains the tokenizer\nspm.SentencePieceTrainer.train('--input=\"/content/drive/MyDrive/ds/cs_dos/nx_icmp_checksum_compute.c\" --model_prefix=m --vocab_size=200')\n# makes segmenter instance and\n# loads the model file (m.model)\nsp = spm.SentencePieceProcessor()\nsp.load('m.model')\n```", "```py\nstrCProgram = '''\nint main(int argc, void **argc)\n{\n  printf(\"%s\", \"Hello World\\n\");\n  return 0;\n}\n'''\nprint(sp.encode_as_pieces(strCProgram))\n```", "```py\n'▁in', 't', '▁', 'm', 'a', 'in', '(', 'in', 't', '▁a'\n```", "```py\nfrom gensim.models import word2vec\n# now, we need to prepare a dataset\n# in our case, let's just read a dataset that is a code of a program\n# in this example, I use the file from an open source component - Azure NetX\n# the actual part is not that important, as long as we have a set of\n# tokens that we want to analyze\npath = '/content/drive/MyDrive/ds/cs_dos/nx_icmp_checksum_compute.c'\n# read all lines into an array\nwith open(path, 'r') as r:\n  lines = r.readlines()\n# and see how many lines we got\nprint(f'The file (and thus our corpus) contains {len(lines)} lines')\n```", "```py\n# we need to pass splitted sentences to the model\ntokenized_sentences = [sentence.split() for sentence in lines]\nmodel = word2vec.Word2Vec(tokenized_sentences,\n                          vector_size=10,\n                          window=1,\n                          min_count=0,\n                          workers=4)\n```", "```py\n'*/': 0, '/*': 1, 'the': 2, '=': 3, 'checksum': 4, '->': 5, 'packet': 6, 'if': 7, 'of': 8, '/**********************************************************************/': 9,\n```", "```py\n('NX_LOWER_16_MASK;', 0.8372778296470642),\n('Mask', 0.8019374012947083),\n('DESCRIPTION', 0.7171915173530579),\n```", "```py\n('again', 0.24998697638511658),\n('word', 0.21356187760829926),\n('05-19-2020', 0.21174617111682892),\n('*current_packet;', 0.2079058289527893),\n```", "```py\nfrom gensim.models import FastText\n# create the instance of the model\nmodel = FastText(vector_size=4,\n                 window=3,\n                 min_count=1)\n# build a vocabulary\nmodel.build_vocab(corpus_iterable=tokenized_sentences)\n# and train the model\nmodel.train(corpus_iterable=tokenized_sentences,\n            total_examples=len(tokenized_sentences),\n            epochs=10)\n```", "```py\n('void', 0.5913326740264893),\n('int', 0.43626993894577026),\n('{', 0.2602742612361908),\n```"]