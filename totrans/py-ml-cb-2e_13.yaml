- en: Deep Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a single layer neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a deep neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a vector quantizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a recurrent neural network for sequential data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the characters in an OCR database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an optical character recognizer using neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing optimization algorithms in ANN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To address the recipes in this chapter, you need the following files (available
    on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '`perceptron.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`single_layer.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_single_layer.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deep_neural_network.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vector_quantization.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_vq.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recurrent_network.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`visualize_characters.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ocr.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IrisClassifier.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our brain is really good at identifying and recognizing things. We want machines
    to be able to do the same. A neural network is a framework that is modeled after
    the human brain to simulate our learning processes. Neural networks are designed
    to learn from data and recognize the underlying patterns. As with all learning
    algorithms, neural networks deal with numbers. Therefore, if we want to achieve
    any real-world task involving images, text, sensors, and so on, we have to convert
    them into a numerical format before we feed them into a neural network. We can
    use a neural network for classification, clustering, generation, and many other
    related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network consists of layers of **neurons**. These neurons are modeled
    after the biological neurons in the human brain. Each layer is basically a set
    of independent neurons that are connected to the neurons on adjacent layers. The
    input layer corresponds to the input data that we provide, and the output layer
    consists of the output that we desire. All the layers in between are called **hidden
    layers**. If we design a neural network with more hidden layers, then we give
    it more freedom to train itself with higher accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we want the neural network to classify data, based on our needs.
    For a neural network to work accordingly, we need to provide labeled training
    data. The neural network will then train itself by optimizing the `cost` function.
    This `cost` function is the error between actual labels and the predicted labels
    from the neural network. We keep iterating until the error goes below a certain
    threshold.
  prefs: []
  type: TYPE_NORMAL
- en: What exactly are *deep* neural networks? Deep neural networks are neural networks
    that consist of many hidden layers. In general, this falls under the realm of
    deep learning. This is a field that is dedicated to the study of these neural
    networks, composed of multiple layers that are used across many verticals.
  prefs: []
  type: TYPE_NORMAL
- en: Building a perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start our neural network adventure with a **perceptron**. A perceptron
    is a single neuron that performs all the computations. It is a very simple model,
    but it forms the basis of building up complex neural networks. The following is
    what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19e75f7c-8bf7-484a-94c4-0cc7e2addc9e.png)'
  prefs: []
  type: TYPE_IMG
- en: The neuron combines inputs using different weights, and it then adds a bias
    value to compute the output. It's a simple linear equation relating input values
    to the output of the perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a library called `neurolab` to define a perceptron
    with two inputs. Before you proceed, make sure that you install it. You can find
    the installation instructions at [https://pythonhosted.org/neurolab/install.html](https://pythonhosted.org/neurolab/install.html).
    Let's go ahead and look at how to design and develop this neural network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `perceptron.py` file that is provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Define some input data and its corresponding labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot this data to see where the datapoints are located:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a `perceptron` with two inputs. This function also needs us to
    specify the minimum and maximum values in the input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s train the `perceptron` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the results, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see two diagrams. The first diagram displays
    the input data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7821b056-2d66-47fa-9a10-3a9a0414c7cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second diagram displays the training error progress, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87192fdb-445b-4215-bc1f-fb57351dfd96.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used a single neuron that performs all the computations.
    To train the `perceptron`, the following parameters are set. The number of epochs
    specifies the number of complete passes through our training dataset. The `show` parameter
    specifies how frequently we want to display the progress. The `lr` parameter specifies
    the learning rate of the `perceptron`. It is the step size for when the algorithm
    searches through the parameter space. If this is large, then the algorithm may
    move faster, but it might miss the optimum value. If this is small, then the algorithm
    will hit the optimum value, but it will be slow. So, it's a trade-off; hence,
    we choose a value of `0.01`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can understand a `perceptron` concept as anything that takes multiple inputs
    and produces one output. This is the simplest form of a neural network. The `perceptron`
    concept was suggested by Frank Rosenblatt in 1958 as an object with an input and
    output layer and a learning rule targeted at minimizing errors. This learning
    function called **error backpropagation** changes connective weights (synapses)
    relying on the actual output of the network, with respect to a given input, as
    the difference between the actual output and the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `neurolab` library: [https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Refer to A Basic Introduction to Neural Networks* (from the University of
    Wisconsin-Madison): [http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html](http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a single layer neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, *Building a perceptron*, we learned how to create a
    `perceptron`; now let''s create a single layer neural network. A single layer
    neural network consists of multiple neurons in a single layer. Overall, we will
    have an input layer, a hidden layer, and an output layer, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6f756e8-e5a6-459a-b929-cba0f2cd0c76.png)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to create a single layer neural network using
    the `neurolab` library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a single layer neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `single_layer.py` file that is provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the data in the `data_single_layer.txt` file. Let''s load this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s extract the minimum and maximum values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a single layer neural network with two neurons in the hidden
    layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the neural network for 50 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the results, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s test the neural network on new test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see two diagrams. The first diagram displays
    the input data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dbcff17-c510-4d99-97a2-c1992e4688e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second diagram displays the training error progress, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec7f2ad0-2672-48db-8566-34e3e3d0a4e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will see the following printed on your Terminal, indicating where the input
    test points belong:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can verify that the outputs are correct based on our labels.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A single layer neural network has the following architecture: the inputs form
    the input layer, the middle layer that performs the processing is called the hidden
    layer, and the outputs form the output layer. The hidden layer can convert the
    input to the desired output. Understanding the hidden layer requires knowledge
    of weights, bias, and activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Weights are vital to convert an input so it impacts the output; they are numerical
    parameters that monitor how all of the neurons affect the others. The related
    concept resembles the slope in linear regression, where a weight is multiplied
    to the input to add up and form the output.
  prefs: []
  type: TYPE_NORMAL
- en: Bias is similar to the intercept added to a linear equation. It is also an additional
    parameter that is used to regulate the output along with the weighted sum of the
    inputs to the neuron.
  prefs: []
  type: TYPE_NORMAL
- en: An activation function is a mathematical function that converts the input to
    an output and determines the total signal a neuron receives. Without activation
    functions, neural networks would behave like linear functions.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `neurolab` library: [https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Refer to Introduction to Neural Networks* (from Yale University): [http://euler.stat.yale.edu/~tba3/stat665/lectures/lec12/lecture12.pdf](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec12/lecture12.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a deep neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to build a **deep neural network**. A deep neural network
    consists of an input layer, many hidden layers, and an output layer. This looks
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5cf925a1-60cd-4cab-8bbc-6db301b0df88.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts a multilayer neural network with one input layer,
    one hidden layer, and one output layer. In a deep neural network, there are many
    hidden layers between the input and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will build a deep neural network. Deep learning forms an
    advanced neural network with numerous hidden layers. Deep learning is a vast subject
    and is an important concept in building AI. In this recipe, we will use generated
    training data and define a multilayer neural network with two hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a deep neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `deep_neural_network.py` file that is provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define parameters to generate some training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This training data will consist of a function we define that will transform
    the values. We expect the neural network to learn this on its own, based on the
    input and output values that we provide:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a deep neural network with two hidden layers, where each hidden layer
    consists of 10 neurons and the output layer consists of one neuron:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the training algorithm to gradient descent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the output for the training inputs to see the performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the training error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a set of new inputs and run the neural network on them to see
    how it performs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see three diagrams. The first diagram displays
    the input data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0972568-3b7d-43f1-a2c5-b7aa5b1187e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second diagram displays the training error progress, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69be89c5-c8d1-4152-b4e1-893706177089.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The third diagram displays the output of the neural network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ed36633-eb8a-4f62-a725-8c21677a2025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will see the following on your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use generated training data to train a multilayer deep
    neural network with two hidden layers. To train the model, the gradient descent algorithm was
    used. **Gradient descent** is an iterative approach used for error correction
    in any learning model. A gradient descent approach is the process of iterating
    updating weights and biases with the error times derivative of the activation
    function (backpropagation). In this approach, the steepest descent step size is
    substituted by a similar size from the previous step. Gradient is the slope of
    the curve, as it is the derivative of the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of finding the gradient descent at every step is to find the global
    cost minimum, where the error is the lowest. And this is where the model has a
    good fit for the data, and predictions are more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `neurolab` library: [https://pythonhosted.org/neurolab/lib.html](https://pythonhosted.org/neurolab/lib.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some notes on gradient descent (by Marc Toussaint from Stuttgart University):
    [https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf](https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a vector quantizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can use neural networks for vector quantization as well. **Vector quantization**
    is the *N*-dimensional version of rounding off. This is very commonly used across
    multiple areas in computer vision, NLP, and machine learning in general.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous recipes, we have already addressed **v****ector quantization **concepts: *Compressing
    an image using vector quantization* and *Creating features using visual Codebook
    and vector quantization*. In this recipe, we will define a neural network with
    two layers—10 neurons in input layer and 4 neurons in the output layer. Then we
    will use this network to divide the space into four regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting, you need to make a change to fix a library bug. You need to
    open the following file: `neurolab | net.py`. Then find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace the preceding line with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to create a vector quantizer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `vector_quantization.py` file that is provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the input data from the `data_vq.txt` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a **learning vector quantization** (**LVQ**) neural network with two
    layers. The array in the last parameter specifies the percentage weightage for
    each output (they should add up to 1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the LVQ neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a grid of values for testing and visualization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the network on this grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the four classes in our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the grids for all these classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following diagram, where the space is
    divided into regions. Each region corresponds to a bucket in the list of vector-quantized
    regions in the space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3f4e00d-48d5-4e10-a4c7-b1fe5d5bf5dc.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we defined a neural network with two layers: 10 neurons in
    the input layer and 4 neurons in the output layer. This neural network was first
    trained and then used to divide the space into four regions. Each region corresponds
    to a bucket in the list of vector-quantized regions in the space.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Vector quantization** is based on the division of a large set of points (vectors)
    into groups that show the same number of points closer to them. Each group is
    identified by its centroid point, as is the case with most clustering algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `neurolab` library: [https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a recurrent neural network for sequential data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks are really good at analyzing sequential and time-series
    data. A **recurrent neural network** (**RNN**) is a neural model in which a bidirectional
    flow of information is present. In other words, while the propagation of signals
    in feedforward networks takes place only in a continuous manner, going from inputs
    to outputs, recurrent networks are different. In them, this propagation can also
    occur from a neural layer following a previous one, or between neurons belonging
    to the same layer, and even between a neuron and itself.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we deal with sequential and time-series data, we cannot just extend generic
    models. The temporal dependencies in the data are really important, and we need
    to account for this in our models. Let's build a recurrent neural network using
    the `neurolab` library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a recurrent neural network for sequential data analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `recurrent_network.``py` file that is provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to create a waveform, based on input parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Create different amplitudes for each interval to create a random waveform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Combine the arrays to create output arrays. This data corresponds to the input
    and the amplitude corresponds to the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to draw the output after passing the data through the trained
    neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `main` function and start by creating sample data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a recurrent neural network with two layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the initialized functions for each layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the recurrent neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the output from the network for the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the training error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a waveform of random length and see whether the network can predict
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Create another waveform with a shorter length and see whether the network can
    predict it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see two diagrams. The first diagram displays
    training errors and the performance on the training data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc6e8533-b424-477c-8ca6-8d3db31e3c76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second diagram displays how a trained recurrent neural net performs on
    sequences of arbitrary lengths, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0e5d044-b84a-4cb4-8ce1-1383b289a4bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will see the following on your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, first, we created an artificial signal with waveform characteristics,
    that is, a curve showing the shape of a wave at a given time. Then we built a
    recurrent neural network to see whether the network could predict a waveform of
    random length.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent networks are distinguished from feedforward networks thanks to the
    feedback loop linked to their past decisions, thus accepting their output momentarily
    as inputs. This feature can be emphasized by saying that recurrent networks have
    memory. There is information in the sequence, and it is used perform the tasks
    that feedforward networks cannot.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of `neurolab` library: [https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Recurrent Neural Networks* (from Yale University): [http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/lecture21.pdf](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/lecture21.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the characters in an OCR database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now look at how to use neural networks to perform **optical character
    recognition **(**OCR**). This refers to the process of identifying handwritten
    characters in images. We have always been particularly sensitive to the problem
    of the automatic recognition of writing in order to achieve a simpler interaction
    between humans and machines. Especially in the last few years, this problem has
    been subject to interesting developments and more and more efficient solutions
    thanks to a very strong economic interest and an ever-greater capacity to process
    data of modern computers. In particular, some countries, such as Japan, and Asian
    countries in general, are investing heavily, in terms of research and financial
    resources, to make state-of-the-art OCR.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will display the handwritten digits contained in a dataset. We
    will use the dataset available at [http://ai.stanford.edu/~btaskar/ocr](http://ai.stanford.edu/~btaskar/ocr).
    The default file name after downloading is `letter.data`. To start with, let's
    see how to interact with the data and visualize it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to visualize the characters in an OCR database:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `visualize_characters.``py` file that is provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the input file name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Define visualization parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep looping through the file until the user presses the *Escape* key. Split
    the line into tab-separated characters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the array into the required shape, resize it, and display it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'If the user presses *Escape*, break the loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: If you run this code, you will see a window displaying characters.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we showed the handwritten figures contained in a dataset. To
    do this, the following tasks are performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Load input data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define visualization parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loop until you encounter the *Escape* key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Approaches to the OCR problem are basically of two types: one is based on pattern
    matching or on the comparison of a model and the other is based on structural
    analysis. Often, these two techniques are used in combination, and provide remarkable
    results in terms of recognition and speed.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `OpenCV` library: [https://opencv.org/](https://opencv.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an optical character recognizer using neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to interact with data, let's build a neural network-based
    OCR system. The operations of classification and indexing the images are based
    on the automatic analysis of the image content, which constitutes the main application
    field of the imaging analysis. The objective of an automatic image recognition
    system consists in the description, through mathematical models and computer implementations,
    the content of an image, all the while trying, as far as possible, to respect
    the principles of the human visual system.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will build a neural network-based OCR system.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build an optical character recognizer using neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `ocr.``py` file that is provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the input filename:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'When we work with neural networks that deal with large amounts of data, it
    takes a lot of time to train. To demonstrate how to build this system, we will
    take only `20` datapoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: If you look at the data, you will see that there are seven distinct characters
    in the
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'first 20 lines. Let''s define them:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use 90% of the data for training and the remaining 10% for testing.
    Define the training and testing parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The starting and ending indices in each line of the dataset file are specified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Add an error check to see whether the characters are in our list of labels
    (if the label is not in our ground truth labels, skip it):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the label, and append it to the main list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the character, and append it to the main list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Exit the loop once we have enough data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert this data into NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the number of dimensions in our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the neural network until `10,000` epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the output for test inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following on your Terminal at the end
    of training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the neural network is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we used a neural network to recognize the handwritten digits.
    To do this, the following tasks are performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading and manipulating input data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting data and labels into NumPy arrays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting the number of dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and training the neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the output for test inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **handwriting recognition** (**HWR**) refers to the ability of a computer
    to receive and interpret as text intelligible handwritten input from sources such
    as paper documents, photographs, and touchscreens. Written text can be detected
    on a piece of paper via optical scanning (OCR) or **intelligent word recognition**.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `neurolab` library: [https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Optical character recognition* (from Wikipedia): [https://en.wikipedia.org/wiki/Optical_character_recognition](https://en.wikipedia.org/wiki/Optical_character_recognition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Handwriting recognition* (from Wikipedia): [https://en.wikipedia.org/wiki/Handwriting_recognition](https://en.wikipedia.org/wiki/Handwriting_recognition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing optimization algorithms in ANN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have built several neural networks and obtained satisfactory overall
    performances. We have evaluated the model''s performance using the `loss` function,
    which is a mathematical way to measure how wrong our predictions are. To improve
    the performance of a model based on neural networks, during the training process,
    weights are modified to try to minimize the `loss` function and make our predictions
    as correct as possible. To do this, optimizers are used: they are algorithms that
    regulate the parameters of the model, updating it in relation to what is returned
    by the `loss` function. In practice, optimizers shape the model in its most accurate
    form possible by overcoming weights: The `loss` function tells the optimizer when
    it is moving in the right or wrong direction.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will build a neural network using the Keras library and
    improve the performance of the model by adopting several optimizers. To do this,
    the `iris` dataset will be used. I''m referring to the **Iris flower dataset**,
    a multivariate dataset introduced by the British statistician and biologist Ronald
    Fisher in his 1936 paper: *The use of multiple measurements in taxonomic problems
    as an example of linear discriminant analysis*.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to implement optimization algorithms in ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `IrisClassifier.py` file that is provided to you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the data from the `sklearn` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide the data into an input and target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: For the target, the data was converted to a single column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s encode the class labels as `One Hot Encode`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data for training and testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Three layers will be added: an input layer, a hidden layer, and an output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'The following three arguments are passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`optimizer=''SGD''`: Stochastic gradient descent optimizer. Includes support
    for momentum, learning rate decay, and Nesterov momentum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss=''categorical_crossentropy''`: We have used the `categorical_crossentropy`
    argument here. When using `categorical_crossentropy`, your targets should be in
    categorical format (we have three classes; the target for each sample must be
    a three-dimensional vector that is all-zeros except for a 1 at the index corresponding
    to the class of the sample).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metrics=[''accuracy'']`: A `metric` is a function that is used to evaluate
    the'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: performance of your model during training and testing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, test the model using unseen data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see what happens if we use a different optimizer. To do this, just
    change the optimizer parameter in the compile method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: The `adam` optimizer is an algorithm for the first-order, gradient-based optimization
    of stochastic objective functions, based on adaptive estimates of lower-order
    moments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we said in the *Building a deep neural network* recipe, gradient descent
    is an iterative approach used for error correction in any learning model. Gradient
    descent approach is the process of iterating the update of weights and biases
    with the error times derivative of the activation function (backpropagation).
    In this approach, the steepest descent step size is substituted by a similar size
    from the previous step. The gradient is the slope of the curve, as it is the derivative
    of the activation function. The SGD optimizer is based on this approach.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimization problems are usually so complex that it is not possible to determine
    a solution analytically. Complexity is determined primarily by the number of variables
    and constraints, which define the size of the problem, and then by the possible
    presence of non-linear functions. An analytical solution is only possible in the
    case of a few variables and extremely simple functions. In practice, to solve
    an optimization problem, it is necessary to resort to an iterative algorithm,
    that is, to a calculation program that, given a current approximation of the solution,
    determines, with an appropriate sequence of operations, a new approximation. Starting
    from an initial approximation, a succession is thus determined.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the Keras optimizer: [https://keras.io/optimizers/](https://keras.io/optimizers/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Optimization for Deep Neural Networks* (from The University of Chicago):
    [https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf](https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
