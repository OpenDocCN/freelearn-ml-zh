- en: Deep Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下食谱：
- en: Building a perceptron
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建感知器
- en: Building a single layer neural network
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建单层神经网络
- en: Building a deep neural network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度神经网络构建深度神经网络
- en: Creating a vector quantizer
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建矢量量化器
- en: Building a recurrent neural network for sequential data analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建用于序列数据分析的循环神经网络
- en: Visualizing the characters in an OCR database
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化OCR数据库中的字符
- en: Building an optical character recognizer using neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络构建光学字符识别器
- en: Implementing optimization algorithms in ANN
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在人工神经网络中实现优化算法
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To address the recipes in this chapter, you need the following files (available
    on GitHub):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理本章中的食谱，你需要以下文件（可在GitHub上获取）：
- en: '`perceptron.py`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perceptron.py`'
- en: '`single_layer.py`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`single_layer.py`'
- en: '`data_single_layer.txt`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_single_layer.txt`'
- en: '`deep_neural_network.py`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deep_neural_network.py`'
- en: '`vector_quantization.py`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector_quantization.py`'
- en: '`data_vq.txt`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_vq.txt`'
- en: '`recurrent_network.py`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`recurrent_network.py`'
- en: '`visualize_characters.py`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`visualize_characters.py`'
- en: '`ocr.py`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ocr.py`'
- en: '`IrisClassifier.py`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IrisClassifier.py`'
- en: Introduction
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Our brain is really good at identifying and recognizing things. We want machines
    to be able to do the same. A neural network is a framework that is modeled after
    the human brain to simulate our learning processes. Neural networks are designed
    to learn from data and recognize the underlying patterns. As with all learning
    algorithms, neural networks deal with numbers. Therefore, if we want to achieve
    any real-world task involving images, text, sensors, and so on, we have to convert
    them into a numerical format before we feed them into a neural network. We can
    use a neural network for classification, clustering, generation, and many other
    related tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑非常擅长识别和识别事物。我们希望机器能够做到同样的。神经网络是一个模仿人类大脑以模拟我们的学习过程的框架。神经网络被设计用来从数据中学习并识别潜在的模式。与所有学习算法一样，神经网络处理数字。因此，如果我们想要实现任何涉及图像、文本、传感器等现实世界任务，我们必须在将它们输入神经网络之前将它们转换为数值格式。我们可以使用神经网络进行分类、聚类、生成和其他许多相关任务。
- en: A neural network consists of layers of **neurons**. These neurons are modeled
    after the biological neurons in the human brain. Each layer is basically a set
    of independent neurons that are connected to the neurons on adjacent layers. The
    input layer corresponds to the input data that we provide, and the output layer
    consists of the output that we desire. All the layers in between are called **hidden
    layers**. If we design a neural network with more hidden layers, then we give
    it more freedom to train itself with higher accuracy.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由**神经元**层组成。这些神经元模仿人类大脑中的生物神经元。每一层基本上是一组独立的神经元，它们与相邻层的神经元相连。输入层对应于我们提供的输入数据，输出层包含我们期望的输出。所有介于输入层和输出层之间的层都称为**隐藏层**。如果我们设计一个具有更多隐藏层的神经网络，那么我们给它更多的自由度，以更高的精度进行自我训练。
- en: Let's say that we want the neural network to classify data, based on our needs.
    For a neural network to work accordingly, we need to provide labeled training
    data. The neural network will then train itself by optimizing the `cost` function.
    This `cost` function is the error between actual labels and the predicted labels
    from the neural network. We keep iterating until the error goes below a certain
    threshold.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望神经网络根据我们的需求对数据进行分类。为了使神经网络按预期工作，我们需要提供标记的训练数据。然后神经网络将通过优化`成本`函数来自我训练。这个`成本`函数是实际标签与神经网络预测标签之间的误差。我们持续迭代，直到误差低于某个阈值。
- en: What exactly are *deep* neural networks? Deep neural networks are neural networks
    that consist of many hidden layers. In general, this falls under the realm of
    deep learning. This is a field that is dedicated to the study of these neural
    networks, composed of multiple layers that are used across many verticals.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络究竟是什么？深度神经网络是由许多隐藏层组成的神经网络。通常，这属于深度学习的范畴。这是一个致力于研究这些由多层组成的神经网络的领域，这些网络被用于许多垂直领域。
- en: Building a perceptron
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建感知器
- en: 'Let''s start our neural network adventure with a **perceptron**. A perceptron
    is a single neuron that performs all the computations. It is a very simple model,
    but it forms the basis of building up complex neural networks. The following is
    what it looks like:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个**感知器**开始我们的神经网络冒险。感知器是一个执行所有计算的单一神经元。这是一个非常简单的模型，但它是构建复杂神经网络的基石。以下是其外观：
- en: '![](img/19e75f7c-8bf7-484a-94c4-0cc7e2addc9e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19e75f7c-8bf7-484a-94c4-0cc7e2addc9e.png)'
- en: The neuron combines inputs using different weights, and it then adds a bias
    value to compute the output. It's a simple linear equation relating input values
    to the output of the perceptron.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元使用不同的权重组合输入，然后添加一个偏置值来计算输出。这是一个简单的线性方程，将输入值与感知器的输出联系起来。
- en: Getting ready
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use a library called `neurolab` to define a perceptron
    with two inputs. Before you proceed, make sure that you install it. You can find
    the installation instructions at [https://pythonhosted.org/neurolab/install.html](https://pythonhosted.org/neurolab/install.html).
    Let's go ahead and look at how to design and develop this neural network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用一个名为`neurolab`的库来定义一个具有两个输入的感知器。在继续之前，请确保您已安装它。您可以在[https://pythonhosted.org/neurolab/install.html](https://pythonhosted.org/neurolab/install.html)找到安装说明。让我们继续看看如何设计和开发这个神经网络。
- en: How to do it...
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s see how to build a perceptron:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何构建一个感知器：
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `perceptron.py` file that is provided to you):'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '创建一个新的Python文件，并导入以下包（完整的代码在提供的`perceptron.py`文件中）： '
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define some input data and its corresponding labels:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一些输入数据和相应的标签：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s plot this data to see where the datapoints are located:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制这些数据以查看数据点的位置：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s define a `perceptron` with two inputs. This function also needs us to
    specify the minimum and maximum values in the input data:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个具有两个输入的`perceptron`。此函数还需要我们指定输入数据中的最小值和最大值：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s train the `perceptron` model:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们训练`perceptron`模型：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s plot the results, as follows:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制结果，如下所示：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you run this code, you will see two diagrams. The first diagram displays
    the input data, as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行此代码，您将看到两个图表。第一个图表显示了输入数据，如下所示：
- en: '![](img/7821b056-2d66-47fa-9a10-3a9a0414c7cb.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7821b056-2d66-47fa-9a10-3a9a0414c7cb.png)'
- en: 'The second diagram displays the training error progress, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图表显示了训练错误进度，如下所示：
- en: '![](img/87192fdb-445b-4215-bc1f-fb57351dfd96.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87192fdb-445b-4215-bc1f-fb57351dfd96.png)'
- en: How it works...
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we used a single neuron that performs all the computations.
    To train the `perceptron`, the following parameters are set. The number of epochs
    specifies the number of complete passes through our training dataset. The `show` parameter
    specifies how frequently we want to display the progress. The `lr` parameter specifies
    the learning rate of the `perceptron`. It is the step size for when the algorithm
    searches through the parameter space. If this is large, then the algorithm may
    move faster, but it might miss the optimum value. If this is small, then the algorithm
    will hit the optimum value, but it will be slow. So, it's a trade-off; hence,
    we choose a value of `0.01`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用了一个执行所有计算的单一神经元。为了训练`perceptron`，以下参数被设置。`epochs`的数量指定了通过我们的训练数据集的完整遍历次数。`show`参数指定了我们希望显示进度的频率。`lr`参数指定了`perceptron`的学习率。它是算法在参数空间中搜索时的步长大小。如果这个值很大，那么算法可能会更快地移动，但可能会错过最优值。如果这个值很小，那么算法将击中最优值，但会较慢。因此，这是一个权衡；因此，我们选择`0.01`的值。
- en: There's more…
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: We can understand a `perceptron` concept as anything that takes multiple inputs
    and produces one output. This is the simplest form of a neural network. The `perceptron`
    concept was suggested by Frank Rosenblatt in 1958 as an object with an input and
    output layer and a learning rule targeted at minimizing errors. This learning
    function called **error backpropagation** changes connective weights (synapses)
    relying on the actual output of the network, with respect to a given input, as
    the difference between the actual output and the desired output.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`perceptron`的概念理解为任何接受多个输入并产生一个输出的东西。这是神经网络的最简单形式。1958年，Frank Rosenblatt提出了`perceptron`的概念，它是一个具有输入层和输出层以及旨在最小化错误的训练规则的物体。这个称为**误差反向传播**的学习函数根据网络的实际输出与给定输入之间的差异，依赖于网络的实际输出和期望输出，来改变连接权重（突触）。
- en: See also
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Refer to the official documentation of the `neurolab` library: [https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考`neurolab`库的官方文档：[https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
- en: '*Refer to A Basic Introduction to Neural Networks* (from the University of
    Wisconsin-Madison): [http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html](http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*参考《神经网络基础入门》（来自威斯康星大学麦迪逊分校）：[http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html](http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html)'
- en: Building a single layer neural network
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建单层神经网络
- en: 'In the previous recipe, *Building a perceptron*, we learned how to create a
    `perceptron`; now let''s create a single layer neural network. A single layer
    neural network consists of multiple neurons in a single layer. Overall, we will
    have an input layer, a hidden layer, and an output layer, as shown in the following
    diagram:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的配方中，*构建感知器*，我们学习了如何创建`感知器`；现在让我们创建一个单层神经网络。单层神经网络由单层中的多个神经元组成。总体而言，我们将有一个输入层、一个隐藏层和一个输出层，如下面的图所示：
- en: '![](img/a6f756e8-e5a6-459a-b929-cba0f2cd0c76.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a6f756e8-e5a6-459a-b929-cba0f2cd0c76.png)'
- en: Getting ready
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will learn how to create a single layer neural network using
    the `neurolab` library.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将学习如何使用`neurolab`库创建单层神经网络。
- en: How to do it...
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s see how to build a single layer neural network:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何构建一个单层神经网络：
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `single_layer.py` file that is provided to you):'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整的代码在提供的`single_layer.py`文件中给出）：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will use the data in the `data_single_layer.txt` file. Let''s load this:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`data_single_layer.txt`文件中的数据。让我们加载这个文件：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s plot the input data:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制输入数据：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s extract the minimum and maximum values:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们提取最小值和最大值：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s define a single layer neural network with two neurons in the hidden
    layer:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个具有隐藏层中两个神经元的单层神经网络：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Train the neural network for 50 epochs:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练神经网络50个周期：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Plot the results, as follows:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式绘制结果：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s test the neural network on new test data:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在新的测试数据上测试神经网络：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you run this code, you will see two diagrams. The first diagram displays
    the input data, as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行此代码，你将看到两个图表。第一个图表显示输入数据，如下所示：
- en: '![](img/8dbcff17-c510-4d99-97a2-c1992e4688e2.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8dbcff17-c510-4d99-97a2-c1992e4688e2.png)'
- en: 'The second diagram displays the training error progress, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图表显示训练错误进度，如下所示：
- en: '![](img/ec7f2ad0-2672-48db-8566-34e3e3d0a4e9.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ec7f2ad0-2672-48db-8566-34e3e3d0a4e9.png)'
- en: 'You will see the following printed on your Terminal, indicating where the input
    test points belong:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在你的终端上看到以下打印信息，指示输入测试点属于何处：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can verify that the outputs are correct based on our labels.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据我们的标签验证输出是否正确。
- en: How it works...
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'A single layer neural network has the following architecture: the inputs form
    the input layer, the middle layer that performs the processing is called the hidden
    layer, and the outputs form the output layer. The hidden layer can convert the
    input to the desired output. Understanding the hidden layer requires knowledge
    of weights, bias, and activation functions.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 单层神经网络具有以下架构：输入形成输入层，执行处理的中间层称为隐藏层，输出形成输出层。隐藏层可以将输入转换为所需的输出。理解隐藏层需要了解权重、偏置和激活函数。
- en: There's more…
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Weights are vital to convert an input so it impacts the output; they are numerical
    parameters that monitor how all of the neurons affect the others. The related
    concept resembles the slope in linear regression, where a weight is multiplied
    to the input to add up and form the output.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 权重对于将输入转换为影响输出的因素至关重要；它们是监控所有神经元如何影响彼此的数值参数。相关的概念类似于线性回归中的斜率，其中权重乘以输入以相加并形成输出。
- en: Bias is similar to the intercept added to a linear equation. It is also an additional
    parameter that is used to regulate the output along with the weighted sum of the
    inputs to the neuron.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置类似于添加到线性方程中的截距。它也是一个用于调节输出以及神经元输入加权总和的附加参数。
- en: An activation function is a mathematical function that converts the input to
    an output and determines the total signal a neuron receives. Without activation
    functions, neural networks would behave like linear functions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是一个数学函数，它将输入转换为输出并确定神经元接收的总信号。没有激活函数，神经网络将表现得像线性函数。
- en: See also
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Refer to the official documentation of the `neurolab` library: [https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考官方的`neurolab`库文档：[https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
- en: '*Refer to Introduction to Neural Networks* (from Yale University): [http://euler.stat.yale.edu/~tba3/stat665/lectures/lec12/lecture12.pdf](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec12/lecture12.pdf)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*参考《神经网络入门》*（来自耶鲁大学）：[http://euler.stat.yale.edu/~tba3/stat665/lectures/lec12/lecture12.pdf](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec12/lecture12.pdf)'
- en: Building a deep neural network
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度神经网络
- en: 'We are now ready to build a **deep neural network**. A deep neural network
    consists of an input layer, many hidden layers, and an output layer. This looks
    like the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好构建一个**深度神经网络**。深度神经网络由一个输入层、多个隐藏层和一个输出层组成。这看起来如下所示：
- en: '![](img/5cf925a1-60cd-4cab-8bbc-6db301b0df88.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5cf925a1-60cd-4cab-8bbc-6db301b0df88.png)'
- en: The preceding diagram depicts a multilayer neural network with one input layer,
    one hidden layer, and one output layer. In a deep neural network, there are many
    hidden layers between the input and output layers.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表描述了一个具有一个输入层、一个隐藏层和一个输出层的多层神经网络。在深度神经网络中，输入层和输出层之间有许多隐藏层。
- en: Getting ready
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will build a deep neural network. Deep learning forms an
    advanced neural network with numerous hidden layers. Deep learning is a vast subject
    and is an important concept in building AI. In this recipe, we will use generated
    training data and define a multilayer neural network with two hidden layers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将构建一个深度神经网络。深度学习形成了一个具有众多隐藏层的先进神经网络。深度学习是一个庞大的主题，并且在构建人工智能中是一个重要的概念。在这个菜谱中，我们将使用生成的训练数据和定义一个具有两个隐藏层的多层神经网络。
- en: How to do it...
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s see how to build a deep neural network:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何构建一个深度神经网络：
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `deep_neural_network.py` file that is provided to you):'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整的代码在提供的`deep_neural_network.py`文件中）：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s define parameters to generate some training data:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义参数以生成一些训练数据：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This training data will consist of a function we define that will transform
    the values. We expect the neural network to learn this on its own, based on the
    input and output values that we provide:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这组训练数据将包括我们定义的函数，该函数将转换值。我们期望神经网络能够根据我们提供的输入和输出值自行学习：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Reshape the arrays:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新塑形数组：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Plot the input data:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制输入数据：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define a deep neural network with two hidden layers, where each hidden layer
    consists of 10 neurons and the output layer consists of one neuron:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个具有两个隐藏层的深度神经网络，其中每个隐藏层包含10个神经元，输出层包含一个神经元：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Set the training algorithm to gradient descent:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练算法设置为梯度下降：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Train the network:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练网络：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Predict the output for the training inputs to see the performance:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测训练输入的输出以查看性能：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Plot the training error:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制训练错误：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s create a set of new inputs and run the neural network on them to see
    how it performs:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一组新的输入并运行神经网络以查看其性能：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Plot the outputs:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制输出：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If you run this code, you will see three diagrams. The first diagram displays
    the input data, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行此代码，你将看到三个图表。第一个图表显示了输入数据，如下所示：
- en: '![](img/a0972568-3b7d-43f1-a2c5-b7aa5b1187e0.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a0972568-3b7d-43f1-a2c5-b7aa5b1187e0.png)'
- en: 'The second diagram displays the training error progress, as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图表显示了训练错误进度，如下所示：
- en: '![](img/69be89c5-c8d1-4152-b4e1-893706177089.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/69be89c5-c8d1-4152-b4e1-893706177089.png)'
- en: 'The third diagram displays the output of the neural network, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个图表显示了神经网络的输出，如下所示：
- en: '![](img/4ed36633-eb8a-4f62-a725-8c21677a2025.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4ed36633-eb8a-4f62-a725-8c21677a2025.png)'
- en: 'You will see the following on your Terminal:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在你的终端上看到以下内容：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How it works...
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we will use generated training data to train a multilayer deep
    neural network with two hidden layers. To train the model, the gradient descent algorithm was
    used. **Gradient descent** is an iterative approach used for error correction
    in any learning model. A gradient descent approach is the process of iterating
    updating weights and biases with the error times derivative of the activation
    function (backpropagation). In this approach, the steepest descent step size is
    substituted by a similar size from the previous step. Gradient is the slope of
    the curve, as it is the derivative of the activation function.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用生成的训练数据来训练一个具有两个隐藏层的多层深度神经网络。为了训练模型，使用了梯度下降算法。**梯度下降**是一种迭代方法，用于任何学习模型的错误校正。梯度下降方法是一个迭代过程，通过更新权重和偏差的误差乘以激活函数的导数（反向传播）。在这个方法中，最陡下降步长被替换为上一步的类似大小。梯度是曲线的斜率，因为它就是激活函数的导数。
- en: There's more…
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: The objective of finding the gradient descent at every step is to find the global
    cost minimum, where the error is the lowest. And this is where the model has a
    good fit for the data, and predictions are more accurate.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步寻找梯度下降的目标是找到全局成本最小值，其中错误最低。这正是模型与数据拟合良好，预测更准确的地方。
- en: See also
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: 'Refer to the official documentation of the `neurolab` library: [https://pythonhosted.org/neurolab/lib.html](https://pythonhosted.org/neurolab/lib.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考官方的`neurolab`库文档：[https://pythonhosted.org/neurolab/lib.html](https://pythonhosted.org/neurolab/lib.html)
- en: 'Some notes on gradient descent (by Marc Toussaint from Stuttgart University):
    [https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf](https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于梯度下降的一些笔记（由斯图加特大学的Marc Toussaint提供）：[https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf](https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf)
- en: Creating a vector quantizer
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个向量量化器
- en: You can use neural networks for vector quantization as well. **Vector quantization**
    is the *N*-dimensional version of rounding off. This is very commonly used across
    multiple areas in computer vision, NLP, and machine learning in general.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用神经网络进行向量量化。**向量量化**是**N**维度的舍入。这在计算机视觉、NLP和机器学习等多个领域都非常常用。
- en: Getting ready
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In previous recipes, we have already addressed **v****ector quantization **concepts: *Compressing
    an image using vector quantization* and *Creating features using visual Codebook
    and vector quantization*. In this recipe, we will define a neural network with
    two layers—10 neurons in input layer and 4 neurons in the output layer. Then we
    will use this network to divide the space into four regions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的菜谱中，我们已经讨论了**向量量化**的概念：*使用向量量化压缩图像*和*使用视觉代码簿和向量量化创建特征*。在这个菜谱中，我们将定义一个具有两层的人工神经网络——输入层有10个神经元，输出层有4个神经元。然后我们将使用这个网络将空间划分为四个区域。
- en: 'Before starting, you need to make a change to fix a library bug. You need to
    open the following file: `neurolab | net.py`. Then find the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，你需要进行更改以修复库中的错误。你需要打开以下文件：`neurolab | net.py`。然后找到以下内容：
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Replace the preceding line with the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 将前面的行替换为以下内容：
- en: '[PRE29]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How to do it...
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s see how to create a vector quantizer:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何创建一个向量量化器：
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `vector_quantization.py` file that is provided to you):'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件并导入以下包（完整的代码在提供的`vector_quantization.py`文件中给出）：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s load the input data from the `data_vq.txt` file:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载`data_vq.txt`文件中的输入数据：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define a **learning vector quantization** (**LVQ**) neural network with two
    layers. The array in the last parameter specifies the percentage weightage for
    each output (they should add up to 1):'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个**学习向量量化**（**LVQ**）神经网络，具有两层。最后一个参数中的数组指定了每个输出的百分比权重（它们应该加起来为1）：
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Train the LVQ neural network:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练LVQ神经网络：
- en: '[PRE33]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create a grid of values for testing and visualization:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个用于测试和可视化的值网格：
- en: '[PRE34]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Evaluate the network on this grid:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个网格上评估网络：
- en: '[PRE35]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define the four classes in our data:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们数据中的四个类别：
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Define the grids for all these classes:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义所有这些类别的网格：
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Plot the outputs:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制输出：
- en: '[PRE38]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If you run this code, you will see the following diagram, where the space is
    divided into regions. Each region corresponds to a bucket in the list of vector-quantized
    regions in the space:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这段代码，你会看到以下图表，其中空间被划分为区域。每个区域对应于空间中向量量化区域列表中的一个桶：
- en: '![](img/e3f4e00d-48d5-4e10-a4c7-b1fe5d5bf5dc.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e3f4e00d-48d5-4e10-a4c7-b1fe5d5bf5dc.png)'
- en: How it works...
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In this recipe, we defined a neural network with two layers: 10 neurons in
    the input layer and 4 neurons in the output layer. This neural network was first
    trained and then used to divide the space into four regions. Each region corresponds
    to a bucket in the list of vector-quantized regions in the space.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们定义了一个具有两层的人工神经网络：输入层有10个神经元，输出层有4个神经元。这个神经网络首先被训练，然后用来将空间划分为四个区域。每个区域对应于空间中向量量化区域列表中的一个桶。
- en: There's more…
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: '**Vector quantization** is based on the division of a large set of points (vectors)
    into groups that show the same number of points closer to them. Each group is
    identified by its centroid point, as is the case with most clustering algorithms.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量量化**基于将大量点（向量）划分为显示相同数量点更接近它们的组。每个组由其质心点标识，这与大多数聚类算法类似。'
- en: See also
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Refer to the official documentation of the `neurolab` library: [https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参考`neurolab`库的官方文档：[https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
- en: Building a recurrent neural network for sequential data analysis
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建用于序列数据分析的循环神经网络
- en: Recurrent neural networks are really good at analyzing sequential and time-series
    data. A **recurrent neural network** (**RNN**) is a neural model in which a bidirectional
    flow of information is present. In other words, while the propagation of signals
    in feedforward networks takes place only in a continuous manner, going from inputs
    to outputs, recurrent networks are different. In them, this propagation can also
    occur from a neural layer following a previous one, or between neurons belonging
    to the same layer, and even between a neuron and itself.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络在分析序列和时间序列数据方面非常出色。**循环神经网络**（**RNN**）是一种信息双向流动的神经网络模型。换句话说，在前馈网络中，信号的传播仅以连续的方式进行，从输入到输出，而循环网络则不同。在它们中，这种传播也可以发生在前一个神经层之后的神经层之间，或者在同一层内的神经元之间，甚至是一个神经元与其自身之间。
- en: Getting ready
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: When we deal with sequential and time-series data, we cannot just extend generic
    models. The temporal dependencies in the data are really important, and we need
    to account for this in our models. Let's build a recurrent neural network using
    the `neurolab` library.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理序列和时间序列数据时，我们不能仅仅扩展通用模型。数据中的时间依赖性非常重要，我们需要在我们的模型中考虑这一点。让我们使用`neurolab`库构建一个循环神经网络。
- en: How to do it...
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s see how to build a recurrent neural network for sequential data analysis:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何构建用于序列数据分析的循环神经网络：
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `recurrent_network.``py` file that is provided to you):'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整的代码在提供的`recurrent_network.py`文件中，请查阅）：
- en: '[PRE39]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define a function to create a waveform, based on input parameters:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个基于输入参数创建波形的函数：
- en: '[PRE40]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create different amplitudes for each interval to create a random waveform:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个区间创建不同的振幅以创建随机波形：
- en: '[PRE41]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Combine the arrays to create output arrays. This data corresponds to the input
    and the amplitude corresponds to the labels:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数组组合以创建输出数组。这些数据对应于输入，振幅对应于标签：
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define a function to draw the output after passing the data through the trained
    neural network:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于在数据通过训练好的神经网络后绘制输出：
- en: '[PRE43]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the `main` function and start by creating sample data:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`main`函数，并开始创建样本数据：
- en: '[PRE44]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Create a recurrent neural network with two layers:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有两层结构的循环神经网络：
- en: '[PRE45]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Set the initialized functions for each layer:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每一层设置初始化函数：
- en: '[PRE46]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Train the recurrent neural network:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练循环神经网络：
- en: '[PRE47]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Compute the output from the network for the training data:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网络中计算训练数据的输出：
- en: '[PRE48]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Plot the training error:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制训练误差图：
- en: '[PRE49]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Plot the results:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制结果：
- en: '[PRE50]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Create a waveform of random length and see whether the network can predict
    it:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个随机长度的波形，并查看网络能否预测它：
- en: '[PRE51]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Create another waveform with a shorter length and see whether the network can
    predict it:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建另一个较短长度的波形，并查看网络能否预测它：
- en: '[PRE52]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'If you run this code, you will see two diagrams. The first diagram displays
    training errors and the performance on the training data, as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行此代码，您将看到两个图表。第一个图表显示了训练误差和训练数据的性能，如下所示：
- en: '![](img/bc6e8533-b424-477c-8ca6-8d3db31e3c76.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc6e8533-b424-477c-8ca6-8d3db31e3c76.png)'
- en: 'The second diagram displays how a trained recurrent neural net performs on
    sequences of arbitrary lengths, as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图显示了训练好的循环神经网络在任意长度序列上的表现，如下所示：
- en: '![](img/a0e5d044-b84a-4cb4-8ce1-1383b289a4bf.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0e5d044-b84a-4cb4-8ce1-1383b289a4bf.png)'
- en: 'You will see the following on your Terminal:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的终端上，您将看到以下内容：
- en: '[PRE53]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: How it works...
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, first, we created an artificial signal with waveform characteristics,
    that is, a curve showing the shape of a wave at a given time. Then we built a
    recurrent neural network to see whether the network could predict a waveform of
    random length.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，首先，我们创建了一个具有波形特性的合成信号，即表示给定时间波形形状的曲线。然后我们构建了一个循环神经网络，以查看网络能否预测随机长度的波形。
- en: There's more…
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Recurrent networks are distinguished from feedforward networks thanks to the
    feedback loop linked to their past decisions, thus accepting their output momentarily
    as inputs. This feature can be emphasized by saying that recurrent networks have
    memory. There is information in the sequence, and it is used perform the tasks
    that feedforward networks cannot.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 递归网络与前馈网络的区别在于它们与过去决策相关的反馈循环，因此将它们的输出暂时作为输入接受。可以说，递归网络具有记忆功能。序列中存在信息，并且这些信息被用来执行前馈网络无法执行的任务。
- en: See also
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Refer to the official documentation of `neurolab` library: [https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考官方`neurolab`库文档：[https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
- en: 'Refer to *Recurrent Neural Networks* (from Yale University): [http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/lecture21.pdf](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/lecture21.pdf)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考耶鲁大学的*递归神经网络*（[http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/lecture21.pdf](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec21/lecture21.pdf)）
- en: Visualizing the characters in an OCR database
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化OCR数据库中的字符
- en: We will now look at how to use neural networks to perform **optical character
    recognition **(**OCR**). This refers to the process of identifying handwritten
    characters in images. We have always been particularly sensitive to the problem
    of the automatic recognition of writing in order to achieve a simpler interaction
    between humans and machines. Especially in the last few years, this problem has
    been subject to interesting developments and more and more efficient solutions
    thanks to a very strong economic interest and an ever-greater capacity to process
    data of modern computers. In particular, some countries, such as Japan, and Asian
    countries in general, are investing heavily, in terms of research and financial
    resources, to make state-of-the-art OCR.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨如何使用神经网络进行**光学字符识别（OCR**）。这指的是在图像中识别手写字符的过程。我们一直特别关注自动识别书写的问题，以便实现人与机器之间更简单的交互。特别是在过去几年里，由于强大的经济利益和现代计算机处理数据能力的不断提高，这个问题已经得到了有趣的发展，并且越来越高效的解决方案。特别是，一些国家，如日本，以及亚洲国家，在研究和财务资源方面投入了大量资金，以实现最先进的OCR。
- en: Getting ready
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will display the handwritten digits contained in a dataset. We
    will use the dataset available at [http://ai.stanford.edu/~btaskar/ocr](http://ai.stanford.edu/~btaskar/ocr).
    The default file name after downloading is `letter.data`. To start with, let's
    see how to interact with the data and visualize it.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将显示数据集中包含的手写数字。我们将使用在[http://ai.stanford.edu/~btaskar/ocr](http://ai.stanford.edu/~btaskar/ocr)可用的数据集。下载后的默认文件名是`letter.data`。首先，让我们看看如何与数据交互并可视化它。
- en: How to do it...
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s see how to visualize the characters in an OCR database:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何可视化OCR数据库中的字符：
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `visualize_characters.``py` file that is provided to you):'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整的代码在提供的`visualize_characters.py`文件中给出）：
- en: '[PRE54]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Define the input file name:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入文件名：
- en: '[PRE55]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Define visualization parameters:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义可视化参数：
- en: '[PRE56]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Keep looping through the file until the user presses the *Escape* key. Split
    the line into tab-separated characters:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持循环读取文件，直到用户按下*Escape*键。将行分割为制表符分隔的字符：
- en: '[PRE57]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Reshape the array into the required shape, resize it, and display it:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数组重塑为所需的形状，调整大小，并显示：
- en: '[PRE58]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'If the user presses *Escape*, break the loop:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果用户按下*Escape*，则中断循环：
- en: '[PRE59]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: If you run this code, you will see a window displaying characters.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果运行此代码，你将看到一个显示字符的窗口。
- en: How it works...
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'In this recipe, we showed the handwritten figures contained in a dataset. To
    do this, the following tasks are performed:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了数据集中包含的手写数字。为此，执行以下任务：
- en: Load input data
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载输入数据
- en: Define visualization parameters
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义可视化参数
- en: Loop until you encounter the *Escape* key
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环直到遇到*Escape*键
- en: There's more…
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Approaches to the OCR problem are basically of two types: one is based on pattern
    matching or on the comparison of a model and the other is based on structural
    analysis. Often, these two techniques are used in combination, and provide remarkable
    results in terms of recognition and speed.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: OCR问题的方法基本上有两种类型：一种是基于模式匹配或模型比较，另一种是基于结构分析。通常，这两种技术会结合使用，并在识别和速度方面提供显著的结果。
- en: See also
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: 'Refer to the official documentation of the `OpenCV` library: [https://opencv.org/](https://opencv.org/)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考官方的`OpenCV`库文档：[https://opencv.org/](https://opencv.org/)
- en: Building an optical character recognizer using neural networks
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络构建光学字符识别器
- en: Now that we know how to interact with data, let's build a neural network-based
    OCR system. The operations of classification and indexing the images are based
    on the automatic analysis of the image content, which constitutes the main application
    field of the imaging analysis. The objective of an automatic image recognition
    system consists in the description, through mathematical models and computer implementations,
    the content of an image, all the while trying, as far as possible, to respect
    the principles of the human visual system.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何与数据交互，让我们构建一个基于神经网络的OCR系统。图像分类和索引的操作基于对图像内容的自动分析，这构成了图像分析的主要应用领域。自动图像识别系统的目标是，通过数学模型和计算机实现，描述图像的内容，同时尽可能遵守人类视觉系统的原则。
- en: Getting ready
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will build a neural network-based OCR system.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将构建一个基于神经网络的OCR系统。
- en: How to do it...
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s see how to build an optical character recognizer using neural networks:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用神经网络构建光学字符识别器：
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `ocr.``py` file that is provided to you):'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整的代码在提供的`ocr.py`文件中给出）：
- en: '[PRE60]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Define the input filename:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入文件名：
- en: '[PRE61]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'When we work with neural networks that deal with large amounts of data, it
    takes a lot of time to train. To demonstrate how to build this system, we will
    take only `20` datapoints:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们处理涉及大量数据的神经网络时，训练需要花费很多时间。为了演示如何构建这个系统，我们将只使用`20`个数据点：
- en: '[PRE62]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: If you look at the data, you will see that there are seven distinct characters
    in the
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你查看数据，你会看到有七个不同的字符在
- en: 'first 20 lines. Let''s define them:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前二十行。让我们定义它们：
- en: '[PRE63]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We will use 90% of the data for training and the remaining 10% for testing.
    Define the training and testing parameters:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用90%的数据进行训练，剩余的10%用于测试。定义训练和测试参数：
- en: '[PRE64]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The starting and ending indices in each line of the dataset file are specified:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集文件中每行的起始和结束索引被指定：
- en: '[PRE65]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Create the dataset:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据集：
- en: '[PRE66]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Add an error check to see whether the characters are in our list of labels
    (if the label is not in our ground truth labels, skip it):'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个错误检查来查看字符是否在我们的标签列表中（如果标签不在我们的地面真实标签中，则跳过它）：
- en: '[PRE67]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Extract the label, and append it to the main list:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取标签，并将其追加到主列表中：
- en: '[PRE68]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Extract the character, and append it to the main list:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取字符，并将其追加到主列表中：
- en: '[PRE69]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Exit the loop once we have enough data:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有足够的数据，就退出循环：
- en: '[PRE70]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Convert this data into NumPy arrays:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此数据转换为NumPy数组：
- en: '[PRE71]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Extract the number of dimensions in our data:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取我们数据中的维数：
- en: '[PRE72]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Train the neural network until `10,000` epochs:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练神经网络直到`10,000`个epoch：
- en: '[PRE73]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Predict the output for test inputs:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测测试输入的输出：
- en: '[PRE74]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'If you run this code, you will see the following on your Terminal at the end
    of training:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你运行此代码，你将在训练结束时在你的终端上看到以下内容：
- en: '[PRE75]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output of the neural network is shown as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的输出如下所示：
- en: '[PRE76]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: How it works...
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In this recipe, we used a neural network to recognize the handwritten digits.
    To do this, the following tasks are performed:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用神经网络来识别手写数字。为此，执行以下任务：
- en: Loading and manipulating input data
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载和处理输入数据
- en: Creating the dataset
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据集
- en: Converting data and labels into NumPy arrays
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据和标签转换为NumPy数组
- en: Extracting the number of dimensions
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取维数
- en: Creating and training the neural network
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和训练神经网络
- en: Predicting the output for test inputs
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测测试输入的输出
- en: There's more…
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: The term **handwriting recognition** (**HWR**) refers to the ability of a computer
    to receive and interpret as text intelligible handwritten input from sources such
    as paper documents, photographs, and touchscreens. Written text can be detected
    on a piece of paper via optical scanning (OCR) or **intelligent word recognition**.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**手写识别**（**HWR**）指的是计算机接收并解释来自纸张文件、照片和触摸屏等来源的作为文本的清晰手写输入的能力。书写文本可以通过光学扫描（OCR）或**智能文字识别**在纸张上检测到。
- en: See also
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Refer to the official documentation of the `neurolab` library: [https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考官方的`neurolab`库文档：[https://pythonhosted.org/neurolab/](https://pythonhosted.org/neurolab/)
- en: 'Refer to *Optical character recognition* (from Wikipedia): [https://en.wikipedia.org/wiki/Optical_character_recognition](https://en.wikipedia.org/wiki/Optical_character_recognition)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考*光学字符识别*（来自维基百科）：[https://en.wikipedia.org/wiki/Optical_character_recognition](https://en.wikipedia.org/wiki/Optical_character_recognition)
- en: 'Refer to *Handwriting recognition* (from Wikipedia): [https://en.wikipedia.org/wiki/Handwriting_recognition](https://en.wikipedia.org/wiki/Handwriting_recognition)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考*手写识别*（来自维基百科）：[https://en.wikipedia.org/wiki/Handwriting_recognition](https://en.wikipedia.org/wiki/Handwriting_recognition)
- en: Implementing optimization algorithms in ANN
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在人工神经网络中实现优化算法
- en: 'So far, we have built several neural networks and obtained satisfactory overall
    performances. We have evaluated the model''s performance using the `loss` function,
    which is a mathematical way to measure how wrong our predictions are. To improve
    the performance of a model based on neural networks, during the training process,
    weights are modified to try to minimize the `loss` function and make our predictions
    as correct as possible. To do this, optimizers are used: they are algorithms that
    regulate the parameters of the model, updating it in relation to what is returned
    by the `loss` function. In practice, optimizers shape the model in its most accurate
    form possible by overcoming weights: The `loss` function tells the optimizer when
    it is moving in the right or wrong direction.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经构建了几个神经网络并获得了令人满意的总体性能。我们使用`loss`函数来评估模型的性能，这是一种数学方法，用来衡量我们的预测有多错误。为了提高基于神经网络的模型性能，在训练过程中，通过修改权重来尝试最小化`loss`函数，使我们的预测尽可能正确。为此，使用优化器：它们是调节模型参数的算法，根据`loss`函数返回的结果来更新模型。在实践中，优化器通过克服权重来以尽可能准确的形式塑造模型：`loss`函数告诉优化器它是在正确的还是错误的方向上移动。
- en: Getting ready
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we will build a neural network using the Keras library and
    improve the performance of the model by adopting several optimizers. To do this,
    the `iris` dataset will be used. I''m referring to the **Iris flower dataset**,
    a multivariate dataset introduced by the British statistician and biologist Ronald
    Fisher in his 1936 paper: *The use of multiple measurements in taxonomic problems
    as an example of linear discriminant analysis*.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用Keras库构建一个神经网络，并通过采用几个优化器来提高模型的性能。为此，将使用`iris`数据集。我指的是**鸢尾花数据集**，这是一个由英国统计学家和生物学家罗纳德·费希尔在1936年的论文《*The
    use of multiple measurements in taxonomic problems as an example of linear discriminant
    analysis*》中引入的多变量数据集。
- en: How to do it...
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s see how to implement optimization algorithms in ANN:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在人工神经网络中实现优化算法：
- en: 'Create a new Python file, and import the following packages (the full code
    is given in the `IrisClassifier.py` file that is provided to you):'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '创建一个新的Python文件，并导入以下包（完整的代码在提供的`IrisClassifier.py`文件中）： '
- en: '[PRE77]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Import the data from the `sklearn` dataset:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`sklearn`数据集导入数据：
- en: '[PRE78]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Divide the data into an input and target:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为输入和目标：
- en: '[PRE79]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: For the target, the data was converted to a single column.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对于目标，数据被转换为单列。
- en: 'Let''s encode the class labels as `One Hot Encode`:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将类别标签编码为`One Hot Encode`：
- en: '[PRE80]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Split the data for training and testing:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练和测试：
- en: '[PRE81]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Let''s build the model:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们构建模型：
- en: '[PRE82]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Three layers will be added: an input layer, a hidden layer, and an output layer.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将添加三个层：输入层、隐藏层和输出层。
- en: '[PRE83]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Let''s compile the model:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编译模型：
- en: '[PRE84]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The following three arguments are passed:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 以下三个参数被传递：
- en: '`optimizer=''SGD''`: Stochastic gradient descent optimizer. Includes support
    for momentum, learning rate decay, and Nesterov momentum.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer=''SGD''`：随机梯度下降优化器。包括对动量、学习率衰减和Nesterov动量的支持。'
- en: '`loss=''categorical_crossentropy''`: We have used the `categorical_crossentropy`
    argument here. When using `categorical_crossentropy`, your targets should be in
    categorical format (we have three classes; the target for each sample must be
    a three-dimensional vector that is all-zeros except for a 1 at the index corresponding
    to the class of the sample).'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss=''categorical_crossentropy''`：我们在这里使用了`categorical_crossentropy`参数。当使用`categorical_crossentropy`时，你的目标应该以分类格式（我们有三个类别；每个样本的目标必须是一个三维向量，除了对应于样本类别的索引处的1之外，其余都是0）。'
- en: '`metrics=[''accuracy'']`: A `metric` is a function that is used to evaluate
    the'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics=[''accuracy'']`：`metric`是一个用于评估的函数'
- en: performance of your model during training and testing.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练和测试期间评估你的模型性能。
- en: 'Let''s train the model:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们训练模型：
- en: '[PRE85]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Finally, test the model using unseen data:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用未见过的数据测试模型：
- en: '[PRE86]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The following results are returned:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE87]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now let''s see what happens if we use a different optimizer. To do this, just
    change the optimizer parameter in the compile method, as follows:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看看如果我们使用不同的优化器会发生什么。为此，只需在编译方法中更改优化器参数，如下所示：
- en: '[PRE88]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: The `adam` optimizer is an algorithm for the first-order, gradient-based optimization
    of stochastic objective functions, based on adaptive estimates of lower-order
    moments.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '`adam`优化器是一种基于一阶、梯度优化随机目标函数的算法，它基于低阶矩的自适应估计。'
- en: 'The following results are returned:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE89]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: How it works...
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As we said in the *Building a deep neural network* recipe, gradient descent
    is an iterative approach used for error correction in any learning model. Gradient
    descent approach is the process of iterating the update of weights and biases
    with the error times derivative of the activation function (backpropagation).
    In this approach, the steepest descent step size is substituted by a similar size
    from the previous step. The gradient is the slope of the curve, as it is the derivative
    of the activation function. The SGD optimizer is based on this approach.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*构建深度神经网络*配方中所说的，梯度下降是一种用于任何学习模型中错误校正的迭代方法。梯度下降方法是通过迭代更新权重和偏置与误差乘以激活函数的导数（反向传播）的过程。在此方法中，最陡下降步长被替换为上一步的类似大小。梯度是曲线的斜率，正如它是激活函数的导数一样。 SGD优化器基于这种方法。
- en: There's more…
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Optimization problems are usually so complex that it is not possible to determine
    a solution analytically. Complexity is determined primarily by the number of variables
    and constraints, which define the size of the problem, and then by the possible
    presence of non-linear functions. An analytical solution is only possible in the
    case of a few variables and extremely simple functions. In practice, to solve
    an optimization problem, it is necessary to resort to an iterative algorithm,
    that is, to a calculation program that, given a current approximation of the solution,
    determines, with an appropriate sequence of operations, a new approximation. Starting
    from an initial approximation, a succession is thus determined.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 优化问题通常非常复杂，以至于无法通过解析方法确定解决方案。复杂性主要取决于变量的数量和约束条件，这些定义了问题的大小，然后是可能存在的非线性函数。只有当变量数量很少且函数极其简单时，才能找到解析解。在实践中，为了解决优化问题，必须求助于迭代算法，即给定当前解的近似值，通过适当的操作序列确定新的近似值。因此，从初始近似值开始，确定了一系列近似值。
- en: See also
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Refer to the official documentation of the Keras optimizer: [https://keras.io/optimizers/](https://keras.io/optimizers/)
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '参考Keras优化器的官方文档: [https://keras.io/optimizers/](https://keras.io/optimizers/)'
- en: 'Refer to *Optimization for Deep Neural Networks* (from The University of Chicago):
    [https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf](https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf)'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '参考来自芝加哥大学的《*深度神经网络优化*》: [https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf](https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf)'
