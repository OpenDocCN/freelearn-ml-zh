<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Neural Networks - MNIST Handwriting Recognition</h1>
                </header>
            
            <article>
                
<p>Imagine, you were a postal worker. Your would be job to deliver letters. Most of the time, the recipient's name and address would be printed and quite legible, and your job becomes quite easy. But come Thanksgiving and Christmas, the number of envelopes with handwritten addresses increases as people give their personal touches and flourishes. And, to be frank, some people (me included) just have terrible handwriting.</p>
<p>Blame it on schools for no longer emphasizing cursive handwriting if you must, but the problem <span>remains: handwriting is hard to read and interpret.</span> God forbid you have to deliver a letter penned by a doctor (good luck doing that!).</p>
<p>Imagine, instead, if you had built a machine learning system that allows you to read handwriting. That's what we will be doing this chapter and the next; we will be building a<span> </span>type of machine-learning algorithm<span> </span>known as an artificial neural network, and in the next chapter, we will be expanding on the concept with deep learning.</p>
<p>In this chapter, we will learn the basics of neural networks, see how it's inspired by biological neurons, find a better way of representing them, and finally apply neural networks on handwriting to recognize digits.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">A neural network</h1>
                </header>
            
            <article>
                
<p>The term <strong>neural network</strong> can mean one of two things in modern parlance. The first refers to a network of neurons found in your brain. These neurons form specific networks and pathways and are vital to you understanding this very sentence. The second meaning of the term refers to an artificial neural network; that is, things we build in software to emulate a neural network in the brain.</p>
<p>This, of course, has led to very many unfortunate comparisons between a biological neural network and an artificial neural network. To understand why, we must start at the beginning.</p>
<div class="packt_infobox">From here on, I shall spell<span> </span><strong>neuron</strong><span> </span>with a British spelling denoting a real neuron cell, while the American spelling,<span> </span><strong>neuron,</strong><span> </span>will be reserved for the artificial variant.</div>
<p>This following diagram is of a neuron:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-452 image-border" src="Images/6ccfd1f8-ed10-48a7-beb1-2df575822988.png" style="width:41.00em;height:19.25em;" width="1028" height="484"/></p>
<p>In general, a neuron typically consists of the soma (the general body of the cell that contains its nucleus), an optional axon covered in a kind of fatty tissue known as <strong>myelin</strong>, and dendrites. The latter two components ( the axon and dendrites) are particularly interesting because together they form a structure known as a synapse. Specifically, it's the end of an axon the terminal) that forms such synapses.</p>
<p>The vast majority of synapses in mammalian brains are between axon terminals and dendrites. The typical flow of signals (chemical or electrical impulses) goes from one neuron, travels along the axon, and<span> </span>deposits its signal onto the next.</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-462 image-border" src="Images/fee48211-f139-4b97-9a5b-3ef35750c1fe.png" style="width:30.75em;height:28.17em;" width="577" height="528"/></p>
<p><span class="fontstyle0">In the image above, </span>we have three neurons, labelled A, B, and C. Imagine A receives a signal from an external source (like your eyes). It receives a signal that is strong enough that it passes the signal down the axon, which touches the dendrites of B via a synapse. B receives the signal and decides it doesn't warrant passing along the signal to C, so nothing goes down the axon of B.</p>
<p>And so we will now explore<span> </span>how you might emulate this.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Emulating a neural network</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Let's simplify the preceding diagram of the neural network:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-166 image-border" src="Images/affc082b-26f6-48c2-b27c-7a5950eab893.png" style="width:27.83em;height:22.08em;" width="1038" height="825"/></div>
<p>We'll have a circle represent the body of the neuron, and we'll call it the <strong>neuron</strong>. The "dendrites" of the neuron receive inputs from other neurons (unshown) and add up all the inputs. Each input represents an input from another neuron; so, if you see three inputs, it means that this neuron is connected to three other neurons.</p>
<p>If the sum of the inputs exceeds a threshold value, then we can say the neuron "fires" or is activated. This simulates the activation potential of an actual neuron. For simplicity, let's say if it fires, then the output will be 1; otherwise, it will be 0. Here is a good emulation of it in Go code:</p>
<pre>func neuron(threshold int, inputs ...int) int {<br/>  var total int<br/>  for _, in := range inputs {<br/>    total += in<br/>  }<br/>  if total &gt; threshold {<br/>    return 1<br/>  }<br/>  return 0<br/>}</pre>
<p>This is generally known as a <strong>perceptron</strong>, and it's a faithful emulation of how neurons work, if your knowledge of how neurons work is stuck in the 1940s and 1950s.</p>
<p>Here is a rather interesting anecdote: As I was writing this section, King Princess' 1950 started playing in the background and I thought it would be rather apt to imagine ourselves in the 1950s, developing the perceptron. There remains a problem: the artificial network we emulated so far cannot learn! It is programmed to do whatever the inputs tell it to do.</p>
<p>What does it mean for an artificial neural network "to learn" exactly? There's an idea that arose in neuroscience in the 1950s, called the <strong>Hebbian Rule</strong>, which can be briefly summed up as: <em>Neurons that fire together grow together</em>. This gives rise to an idea that some synapses are thicker; hence,they have stronger connections, and other synapses are thinner; hence, they  have weaker connections.</p>
<p>To emulate this, we would need to introduce the concept of a weighted value, the weight of which corresponds to the strength of the input from another neuron. Here's a good approximation of this idea:</p>
<pre>func neuron(threshold, weights, inputs []int) int {<br/>  if len(weights) != len(inputs) {<br/>    panic("Expected length of weights to be the same as the length of inputs")<br/>  }<br/>  var total int <br/>  for i, in := range inputs {<br/>    total += weights[i]*in<br/>  }<br/>  if total &gt; threshold {<br/>    return 1<br/>  }<br/>  return 0<br/>}</pre>
<p>At this point, if you are familiar with linear algebra, you might think to yourself that<span> </span><kbd><span class="VerbatimChar">total</span></kbd><span> </span>is essentially a vector product. You would be absolutely correct. Additionally, if the threshold is<span> </span><span class="VerbatimChar">0</span>, then you have simply applied a <kbd>heaviside</kbd> step function:</p>
<pre>func heaviside(a float64) float64 {<br/>  if a &gt;= 0 {<br/>    return 1<br/>  }<br/>  return 0<br/>}</pre>
<p>In other words, we can summarize a single neuron in the following way:</p>
<pre>func neuron(weights, inputs []float64) float64 {<br/>  return heaviside(vectorDot(weights, inputs))<br/>}</pre>
<p>Note in the last two examples, I switched over from<span> </span><kbd><span class="VerbatimChar">int</span></kbd><span> </span>to a more canonical<span> </span><kbd><span class="VerbatimChar">float64</span></kbd>. The point remains the same: a single neuron is simply a function applied to a vector product.</p>
<p>A single neuron does not do much. But stack a bunch of them together and arrange them by layers like so, a<span>nd then suddenly they start to do more</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-167 image-border" src="Images/357ea737-22c2-41a6-93c6-7bf87ee3c5c5.png" style="width:22.58em;height:25.92em;" width="621" height="712"/></div>
<p>Now we come to the part that requires a conceptual leap: if a neuron is essentially just a vector product, <em>stacking</em> the neurons simply makes it a matrix!</p>
<p>Given an image can be represented as a flat slice of<span> </span><kbd><span class="VerbatimChar">float64</span></kbd><span> </span>, the<span> </span><kbd><span class="VerbatimChar">vectorDot</span></kbd><span> </span>function is replaced with<span> </span><kbd><span class="VerbatimChar">matVecMul</span></kbd><span>, </span>which is a function that multiplies a matrix and vector to return a vector. We can write a function representing the neural layer like so:</p>
<pre>func affine(weights [][]float64, inputs []float64) []float64 {<br/>  return activation(matVecMul(weights, inputs))<br/>}</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear algebra 101</h1>
                </header>
            
            <article>
                
<p>I want to take a detour to talk about linear algebra. It's featured quite a bit so far in this book, although it was scarcely mentioned by name. In fact linear algebra underlies every chapter we've done so far.</p>
<p>Imagine you have two equations:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7132731e-26cc-4ef8-bb5e-5042f0bfa77b.png" style="width:9.25em;height:3.33em;" width="1280" height="460"/></p>
<p>Let's say<span> </span><img class="fm-editor-equation" src="Images/08f454a0-6b2a-4c0b-88b5-cd421a39f6bb.png" style="width:1.25em;height:1.08em;" width="180" height="160"/> and <img class="fm-editor-equation" src="Images/8f907ffd-9c6a-4dc8-9f15-16fd50bfe601.png" style="width:1.33em;height:1.17em;" width="180" height="160"/> is <img class="fm-editor-equation" src="Images/2795f1bc-6f85-449c-a35c-4eeb72ace9c3.png" style="width:0.67em;height:1.25em;" width="90" height="160"/> and <img class="fm-editor-equation" src="Images/92bc7148-9b76-4912-b340-d2b1bb340fd8.png" style="width:0.67em;height:1.17em;" width="90" height="160"/>, respectively. We can now write the following equations as such:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7e57f3c1-9194-4c7e-99fd-bf1aea04ff75.png" style="width:8.83em;height:3.42em;" width="1190" height="460"/></p>
<p>And we can solve it using basic algebra (please do work it out on your own):<span> </span><img class="fm-editor-equation" src="Images/6a2280ac-9c8d-464b-8d66-b85679e23414.png" style="width:3.92em;height:1.33em;" width="530" height="180"/> and<span> </span><img class="fm-editor-equation" src="Images/bd4f7b98-14cc-46c9-a203-c9a3433c88c8.png" style="width:3.75em;height:1.25em;" width="530" height="180"/>.</p>
<p>What if you have three, four, or five simultaneous equations? It starts to get cumbersome to calculate these values. Instead, we invented a new notation: the matrix notation, which will allow us to solve simultaneous equations faster.</p>
<p>It had been used for about 100 years without a name (it was first termed "matrix" by James Sylvester) and formal rules were being used until Arthur Cayley formalized the rules in 1858. Nonetheless, the idea of grouping together parts of an equation into a bunch had been long used.</p>
<p>We start by "factoring" out the equations into their parts:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5a2fcb74-6071-44a3-92d4-ead6ddc3d1b6.png" style="width:14.42em;height:4.08em;" width="1930" height="550"/></p>
<p>The horizontal line indicates that it's two different equations, not that they are ratios. Of course, we realize that we've been making too many repetitions so we simplify the matrix of <img class="fm-editor-equation" src="Images/42c27d9e-dee6-4c3f-88cb-2c7b7a5d15c7.png" style="width:1.58em;height:1.17em;" width="190" height="140"/> and <img class="fm-editor-equation" src="Images/9900269b-ee42-4ed5-b00f-83d1626aa677.png" style="width:1.58em;height:1.17em;" width="190" height="140"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/fc172162-8e7b-4190-878a-134847ec9ca1.png" style="width:15.25em;height:3.50em;" width="2070" height="480"/></p>
<p>Here, you can see that<span> </span><img class="fm-editor-equation" src="Images/fd1741f7-280b-4b26-b879-2f85d5566a96.png" style="width:1.58em;height:1.17em;" width="190" height="140"/> and<span> </span><img class="fm-editor-equation" src="Images/39e7e985-0ee3-413e-9821-7a30732beda3.png" style="width:1.58em;height:1.17em;" width="190" height="140"/> is only ever written once. It's rather unneat to write it the way we just wrote it, so instead we write it like so to be neater:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/aa7b5549-dad6-4e96-bbf6-f70d9b3aa10c.png" style="width:13.25em;height:3.58em;" width="1780" height="480"/></p>
<p>Not only do we write it like so, we give specific rule on how to read this notation: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/43c4d6a6-d9c2-4fa6-accd-860c1c436f07.png" style="width:11.33em;height:11.17em;" width="1730" height="1700"/></p>
<p>We should give the matrices names so we can refer to them later on:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/62d17701-c859-467c-ad8c-2596e593f698.png" style="width:6.00em;height:10.33em;" width="1160" height="1960"/></p>
<div class="packt_infobox">The bold indicates that the variable holds multiple values. An uppercase indicates a matrix (<img class="fm-editor-equation" src="Images/a768d36c-8050-4467-bb39-65ba0d19dd87.png" style="width:1.17em;height:0.92em;" width="220" height="160"/>), and lowercase indicates a vector (<span> </span><img class="fm-editor-equation" src="Images/babf308e-bdf6-464d-9744-e4c275f220ff.png" style="width:0.75em;height:0.83em;" width="110" height="120"/> and<span> </span><img class="fm-editor-equation" src="Images/42393974-379f-47dc-9f4c-b991cd3bcaff.png" style="width:0.67em;height:1.00em;" width="110" height="160"/>. This is to distinguish it from scalar variables (variables that only hold one value), which are typically written without boldface (for example,<span> </span><img class="fm-editor-equation" src="Images/c248220b-8bfa-49d8-bb7f-bf56e777ef95.png" style="width:0.92em;height:0.92em;" width="110" height="110"/> and<span> </span><img class="fm-editor-equation" src="Images/a5e79da2-e584-41e7-a42c-fddcd77de875.png" style="width:0.75em;height:1.33em;" width="90" height="160"/>).</div>
<p>To solve the equations, the solution is simply this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2223ba92-4d76-4dea-a40c-60ce62525d48.png" style="width:5.75em;height:1.50em;" width="880" height="230"/></p>
<p>The<span> </span><img class="fm-editor-equation" src="Images/bb1d7cdb-9e10-4977-9680-9a41e8341f59.png" style="width:1.58em;height:1.50em;" width="190" height="180"/> superscript indicates an inverse is to be taken. This is rather consistent with normal algebra.</p>
<p>Consider a problem<span> </span><img class="fm-editor-equation" src="Images/d594d7b5-c706-4002-8aea-bc9f6c475e27.png" style="width:4.25em;height:1.17em;" width="580" height="160"/> where you are asked to solve for<span> </span><img class="fm-editor-equation" src="Images/4606db1b-66e9-4465-a4a3-7ea25d8a7d28.png" style="width:0.92em;height:0.92em;" width="110" height="110"/>. The solution is simply<span> </span><img class="fm-editor-equation" src="Images/84bc32bb-efd6-4cc3-bd10-034f7a25b4c4.png" style="width:3.83em;height:2.58em;" width="550" height="370"/>. Or we can rewrite it as a series of multiplications as<img class="fm-editor-equation" src="Images/2877c3bd-3326-4c07-8d7e-f1ebddc3d4da.png" style="width:5.67em;height:2.67em;" width="870" height="410"/>. And what do we know about fractions where one is the numerator? They can simply be written as a power to the -1. Hence, we arrive at this solution equation: <img class="fm-editor-equation" src="Images/3287306b-cdd3-47c6-979e-a3a489b222bd.png" style="width:6.42em;height:1.92em;" width="770" height="230"/></p>
<p>Now if you squint very carefully, the scalar version of the equation looks very much like the matrix notation version of the equation.</p>
<p>How to calculate the inverse of a matrix is not what this book aims to do. Instead, I encourage you to pick up a linear algebra text book. I highly recommend Sheldon Axler's <em>Linear Algebra Done Right</em> (Springer Books).</p>
<p>To recap, here are the main points:</p>
<ul>
<li>Matrix multiplication and notation were invented to solve simultaneous equations.</li>
<li>To solve the simultaneous equation, we treat the equation as though the variables were scalar variables and use inverses.</li>
</ul>
<p>Now comes the interesting part. Using the same two equations, we will turn the question around. What if we knew what<span> </span><img class="fm-editor-equation" src="Images/711bc8fc-a9f0-415b-8a8f-b44628ae2ae0.png" style="width:1.58em;height:1.17em;" width="190" height="140"/> and<span> </span><img class="fm-editor-equation" src="Images/a93465f3-1ea7-4725-bd05-ab0aa6dabfbd.png" style="width:1.58em;height:1.17em;" width="190" height="140"/> is instead? The equations would now look something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/64ca7973-411f-4c88-af64-a1df61a8a353.png" style="width:11.08em;height:3.00em;" width="1700" height="460"/></p>
<p>Writing it in matrix form, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/993b3a8b-ad23-404d-a5d5-171116a8911a.png" style="width:12.83em;height:3.33em;" width="1850" height="480"/></p>
<p>Careful readers would have caught an error by now: there are <em>four</em> variables (<img class="fm-editor-equation" src="Images/9eca9944-2c2a-4642-a5c0-a79f3c6aa4c2.png" style="width:1.83em;height:1.25em;" width="220" height="150"/>, <img class="fm-editor-equation" src="Images/8e698660-7b72-497f-a335-8e797a420e28.png" style="width:1.83em;height:1.25em;" width="220" height="150"/>, <img class="fm-editor-equation" src="Images/9a666a8f-89b7-4038-b0e9-37dcf01f05ed.png" style="width:1.83em;height:1.25em;" width="220" height="150"/>, and <img class="fm-editor-equation" src="Images/ddf46523-0dcc-437d-89d2-0d66c71ed1e0.png" style="width:1.83em;height:1.25em;" width="220" height="150"/>), but only <em>two</em> equations. From high-school math, we learn that you can't solve a system of equations where there are fewer equations than there are variables!</p>
<p>The thing is, your high school math teacher kind of lied to you. It is sort of possible to solve this, and you've already done so yourself in <a href="3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml">Chapter 2</a>, <em>Linear Regression - House Price Prediction</em>.<em><br/></em></p>
<p>In fact, most machine learning problems can be re-expressed in linear algebra, specifically of this form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0736a77e-0079-4872-8275-c5b4c2de61b1.png" style="width:16.83em;height:1.50em;" width="2270" height="200"/></p>
<p>And this in my opinion, is the right way to think about artificial neural networks: a series of mathematical functions, not an analogue of biological neurons. We will explore this a bit more in the next chapter. In fact, this understanding is vital to the understanding of deep learning and why it works.</p>
<p>For now, it suffices to follow on with the more common notion that an artificial neural network is similar in actions to a biologically inspired neural network.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exploring activation functions</h1>
                </header>
            
            <article>
                
<p>The thing about linear algebra is, it's linear. It is useful when the change of the output is proportional to the change in input. The real world is full of non-linear functions and equations. Solving non-linear equation is hard with a capital H. But we've got a trick. We can take a linear equation, and then add a non-linearity to it. This way, the function becomes non-linear!</p>
<p>Following from this view, you can view an artificial neural network as a generic version of all the previous chapters we've gone through so far.</p>
<p>Throughout the history of artificial neural networks, the community has favored particular activation functions in a fashionable way. In the early days, the Heaviside function was favored. Gradually, the community moved toward favoring differentiable, continuous functions, such as sigmoid and tanh. But lately, the pendulum of fashion has swung back toward the harder, seemingly discontinuous functions. The key is that we've learned new tricks on how to differentiate functions, such as the <strong>rectified linear unit</strong> (<strong>ReLu</strong>).</p>
<p><span>Here are some of the more popular activation functions over time:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-453 image-border" src="Images/bacc949b-f005-4866-b8f2-c595cafa4c97.png" style="width:29.00em;height:23.92em;" width="586" height="483"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-458 image-border" src="Images/de354d5c-72cd-4045-8481-84ba7681540d.png" style="width:28.67em;height:21.50em;" width="650" height="488"/>'</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-459 image-border" src="Images/ce2830b2-db5e-4508-aa69-2f478de83a22.png" style="width:29.50em;height:19.67em;" width="640" height="427"/></p>
<p class="mce-root">One thing to note about these is that these functions are all nonlinear and they all have a hard limit on the y axis.</p>
<p class="mce-root">The vertical ranges of the activation functions are limited, but the horizontal ranges are not. We can use biases to adjust how our activation functions look.</p>
<p>It should be noted that biases can be zero. It also means that we can omit biases. Most of the time, for more complex projects, this is fine, though adding biases will add to the accuracy of the neural network.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Learning</h1>
                </header>
            
            <article>
                
<p>I want you to think about how you learn. Not the learning styles, mind; no, I want you to give your learning process a long hard deep thought. Think of the various ways you learn. Maybe you've touched a stove while it's hot once. Or if you ever learned a new language, maybe you started out by memorizing phrases before becoming fluent. Think about all the chapters that had preceded this. What do they have in common?</p>
<p>In broad strokes, learning is done by means of corrections. If you touched a stove while it's hot, you made a mistake. The correction is to never touch a stove when it's hot ever again. You've learned how not to touch the stove while it's hot.</p>
<p>Similarly, the way a neural network learns is by means of correction. If we want to train a machine to learn to classify handwriting, we would need to provide some sample images, and tell the machine which are the correct labels. If the machine predicted the labels wrongly, we need to tell it to change something in the neural network and try again.</p>
<p>What can be changed? The weights of course. The inputs can't be changed; they're inputs. But we can always try different weights. Hence, the process of learning can be broken down into two steps:</p>
<ul>
<li>Telling the neural network that it is wrong when it made a mistake.</li>
<li>Updating the weights so that the next try will yield a better result.</li>
</ul>
<p>When broken down like this, we have a good idea of how to proceed next. One way would be a binary determination mechanism: if the neural network predicted the correct answer, don't update the weights. If it's wrong, update the weights.</p>
<p>How to update the weights, then? Well, one way would be to completely replace the weight matrix with new values and try again. Since the weight matrix is filled from values pulled from a random distribution, the new weight matrix would be a new random matrix.</p>
<p>It should be quite obvious that these two methods, when combined, would take a very very long time before the neural network learns anything; it's as if we're simply guessing our way into the correct weight matrices.</p>
<p>Instead, modern neural networks use the concept of<span> </span><strong>backpropagation</strong><span> </span>to tell the neural network that it's made a mistake, and some form of<span> </span><strong>gradient descent</strong><span> </span>to update the weights.</p>
<p>The specifics of backpropagation and gradient descent are outside the scope of this chapter (and book). I'll, however, briefly run through the big ideas by sharing a story. I was having lunch with a couple of friends who also work in machine learning and that lunch ended with us arguing. This was because I had casually mentioned that backpropagation was "discovered", as opposed to "invented". My friends were adamant that backpropagation was invented, not discovered. My reasoning was simple: Mathematics is "discovered" if multiple people stumble upon it with the same formulation. Mathematics is "invented" if there were no parallel discovery of it.</p>
<p>Backpropagation, in various forms, has been constantly rediscovered over time. The first time backpropagation was discovered was in the invention of linear regression. I should note that it was a very specific form of backpropagation specific to linear regression: the sum of squared errors can be propagated back to its inputs by differentiating the result of the sum of squared errors with regard to the inputs.</p>
<p>We start with a cost. Remember how we have to tell the neural network that it's made a mistake. We do so by telling the neural network the cost of making a prediction. This is called a cost function. We can define a cost so that when the neural network makes a correct prediction, the cost is low, and when the neural network makes a wrong prediction, the cost is high.</p>
<p>Imagine for now, that the cost function is<span> </span><img class="fm-editor-equation" src="Images/29e4aff9-eb8a-4967-8815-49f749fe2f4f.png" style="width:6.33em;height:1.58em;" width="760" height="190"/>. How do you know at what values of<span> </span><img class="fm-editor-equation" src="Images/68ac22cd-9ee1-4dbc-ad85-7c9461a01602.png" style="width:1.33em;height:1.33em;" width="160" height="160"/> the cost will be lowest? From high- school math, we know that the solution is to differentiate <img class="fm-editor-equation" src="Images/3ac8cdda-cb4c-49f4-9fca-cb77ddb71c50.png" style="width:2.75em;height:1.25em;" width="330" height="150"/><span> </span>with regard to<span> </span><img class="fm-editor-equation" src="Images/0676e8ac-1a8f-4534-b806-dcfb07833e16.png" style="width:0.92em;height:0.92em;" width="110" height="110"/> and solve for the solution when it's 0:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b2bc33e9-f947-41d0-9f7b-681e8fc3ef30.png" style="width:6.92em;height:3.42em;" width="830" height="410"/></div>
<p>Backpropagation takes the same cue. In short, backpropagtion is just a bunch of partial differentiations with regard to the weights. The main difference between our toy example and real backpropagation is that the derivation of our expression is easy to solve. For more complex mathematical expressions, it can be computationally too expensive to compute the solution. Instead, we rely on gradient descent to find the answer.</p>
<p>Gradient descent assumes we start our<span> </span><span class="VerbatimChar">x</span><span> </span>somewhere and we update the<span> </span><span class="VerbatimChar">x</span><span> </span>iteratively toward the lowest cost. In each iteration, we update the weights. The simplest form of gradient descent is to add the gradient of the weights to the weights themselves.</p>
<p>The key takeaway is the powerful notion that you can tell the inputs that an error has occurred by performing differentiation of the function and finding a point at which the derivatives are at its minimum.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The project</h1>
                </header>
            
            <article>
                
<p>The project we're embarking on is the one as mentioned in the opening paragraphs. The dataset which we are going to classify is a collection of handwritten numbers originally collected by the National Institute of Standards and Technology and later modified by Yann LeCun's team. Our goal is to classify the handwritten numbers as either one of 0, 1, 2... 9.</p>
<p>We're going to build a basic neural network with the understanding that neural networks are applied linear algebra, and we'll be using Gorgonia for this and the next chapter.</p>
<p>To install Gorgonia, simply run<span> </span><kbd><span class="VerbatimChar">go get -u gorgonia.org/gorgonia</span></kbd><span> </span>and<span> </span><span class="VerbatimChar"><kbd>go get -u gorgonia.org/tensor</kbd>.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gorgonia</h1>
                </header>
            
            <article>
                
<p>Gorgonia is a library that facilitates efficient mathematical operations for the purposes of building deep neural networks. It operates on the fundamental understanding that neural networks are mathematical expressions. As such it is quite easy to build neural networks using Gorgonia.</p>
<p>A note on the chapters: Because Gorgonia is a relatively huge library, parts of this chapter will elide over some things about Gorgonia but will be expanded upon in the next chapter, as well as another Packt book,  <em>Hands On Deep Learning in Go</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting the data</h1>
                </header>
            
            <article>
                
<p>The data for the MNIST data can be found in the repository for this chapter. In its original form, it's not in a standard image format. So, we will need to parse the data into an acceptable format.</p>
<p>The dataset comes in two parts: labels and images. So here are a couple of functions, designed to read and parse the MNIST file:</p>
<pre>// Image holds the pixel intensities of an image.<br/>// 255 is foreground (black), 0 is background (white).<br/>type RawImage []byte<br/><br/>// Label is a digit label in 0 to 9<br/>type Label uint8<br/><br/><br/>const numLabels = 10<br/>const pixelRange = 255<br/><br/>const (<br/>  imageMagic = 0x00000803<br/>  labelMagic = 0x00000801<br/>  Width = 28<br/>  Height = 28<br/>)<br/><br/>func readLabelFile(r io.Reader, e error) (labels []Label, err error) {<br/>  if e != nil {<br/>    return nil, e<br/>  }<br/><br/>  var magic, n int32<br/>  if err = binary.Read(r, binary.BigEndian, &amp;magic); err != nil {<br/>    return nil, err<br/>  }<br/>  if magic != labelMagic {<br/>    return nil, os.ErrInvalid<br/>  }<br/>  if err = binary.Read(r, binary.BigEndian, &amp;n); err != nil {<br/>    return nil, err<br/>  }<br/>  labels = make([]Label, n)<br/>  for i := 0; i &lt; int(n); i++ {<br/>    var l Label<br/>    if err := binary.Read(r, binary.BigEndian, &amp;l); err != nil {<br/>      return nil, err<br/>    }<br/>    labels[i] = l<br/>  }<br/>  return labels, nil<br/>}<br/><br/>func readImageFile(r io.Reader, e error) (imgs []RawImage, err error) {<br/>  if e != nil {<br/>    return nil, e<br/>  }<br/><br/>  var magic, n, nrow, ncol int32<br/>  if err = binary.Read(r, binary.BigEndian, &amp;magic); err != nil {<br/>    return nil, err<br/>  }<br/>  if magic != imageMagic {<br/>    return nil, err /*os.ErrInvalid*/<br/>  }<br/>  if err = binary.Read(r, binary.BigEndian, &amp;n); err != nil {<br/>    return nil, err<br/>  }<br/>  if err = binary.Read(r, binary.BigEndian, &amp;nrow); err != nil {<br/>    return nil, err<br/>  }<br/>  if err = binary.Read(r, binary.BigEndian, &amp;ncol); err != nil {<br/>    return nil, err<br/>  }<br/>  imgs = make([]RawImage, n)<br/>  m := int(nrow * ncol)<br/>  for i := 0; i &lt; int(n); i++ {<br/>    imgs[i] = make(RawImage, m)<br/>    m_, err := io.ReadFull(r, imgs[i])<br/>    if err != nil {<br/>      return nil, err<br/>    }<br/>    if m_ != int(m) {<br/>      return nil, os.ErrInvalid<br/>    }<br/>  }<br/>  return imgs, nil<br/>}</pre>
<p>First, the functions read the file from a<span> </span><kbd><span class="VerbatimChar">io.Reader</span></kbd><span> </span>and reading a set of<span> </span><span class="VerbatimChar"><kbd>int32</kbd>s</span>. These are the metadata of the file. The first<span> </span><kbd><span class="VerbatimChar">int32</span></kbd><span> </span>is a magic number that is used to indicate if a file is a labels file or a file of images.<span> </span><kbd><span class="VerbatimChar">n</span></kbd><span> </span>indicates the number of images or labels the file contains.<span> </span><kbd><span class="VerbatimChar">nrow</span></kbd><span> </span>and<span> </span><kbd><span class="VerbatimChar">ncol</span></kbd><span> </span>are metadata that exists in the file, and indicates how many rows/columns there are in each image.</p>
<p>Zooming into the<span> </span><kbd><span class="VerbatimChar">readImageFile</span></kbd><span> </span>function, we can see that after all the metadata has been read, we know to create a<span> </span><kbd><span class="VerbatimChar">[]RawImage</span></kbd><span> </span>of size<span> </span><kbd><span class="VerbatimChar">n</span></kbd>. The image format used in the MNIST dataset is essentially a slice of 784 bytes (28 columns and 28 rows). Each byte therefore represents a pixel in the image. The value of each byte represents how bright the pixel is, ranging from 0 to 255:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-454 image-border" src="Images/6c023a57-bc75-44bd-9a8b-8783b8f4e46e.png" style="width:13.25em;height:13.42em;" width="127" height="129"/></p>
<p>The preceding image is an example of an MNIST image blown up. At the top-left corner, the index of the pixel in a flat slice is 0. At the top right corner, the index of the pixel in a flat slice is 27. At the bottom-left corner, the index of the pixel in a flat slice is 755. And, finally, at the bottom-right corner, the index is 727.  This is an important concept to keep in mind: A 2D image can be represented as a 1D slice.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Acceptable format</h1>
                </header>
            
            <article>
                
<p>What is an acceptable format to represent the image? A slice of bytes is useful for reading and displaying the image, but it's not particularly useful for doing any machine learning. Rather, we should want to represent the image as a slice of floating points. So, here's a function to convert a byte into a<span> </span><kbd><span class="VerbatimChar">float64</span></kbd>:</p>
<pre>func pixelWeight(px byte) float64 {<br/>  retVal := float64(px)/pixelRange*0.9 + 0.1<br/>  if retVal == 1.0 {<br/>    return 0.999<br/>  }<br/>  return retVal<br/>}</pre>
<p>This is essentially a scaling function that scales from 0-255 to between 0.0 and 1.0. There is an additional check; if the value is 1.0, we return 0.999 instead of 1. This is mainly due to the fact that when values are 1.0, numerical instability tends to happen, as mathematical functions tend to act weirdly. So instead, replace 1.0 with values that are very close to 1.</p>
<p>So now, we can make a<span> </span><kbd><span class="VerbatimChar">RawImage</span></kbd><span> </span>into a<span> </span><kbd><span class="VerbatimChar">[]float64</span></kbd>. And because we have<span> </span><kbd>N</kbd><span> </span>images in the form of<span> </span><kbd><span class="VerbatimChar">[]RawImage</span></kbd>, we can make it into a<span> </span><kbd><span class="VerbatimChar">[][]float64</span></kbd>, or a matrix.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">From images to a matrix</h1>
                </header>
            
            <article>
                
<p>So far we've established that we can convert a list of images in a special format in to a slice of slices of<span> </span><kbd><span class="VerbatimChar">float64</span></kbd>. Recall from earlier, that when you stack neurons together they form a matrix, and the activation of a neural layer is simply a matrix-vector multiplication. And when the inputs are stacked together, it's simply matrix-matrix multiplication.</p>
<p>We technically can build a neural network with just<span> </span><kbd><span class="VerbatimChar">[][]float64</span></kbd>. But the end result will be quite slow. Collectively as a species, we have had approximately 40 years of developing algorithms for efficient linear algebra operations, such as matrix multiplication and matrix-vector multiplication. This collection of algorithms are generally known as BLAS (Basic Linear Algebra Subprograms).</p>
<p>We have been, up to this point in the book, using libraries built on top of a library that provide BLAS functions, namely  Gonum's BLAS library. If you had been following the book up to this point, you would have it installed already. Otherwise, run<span> </span><kbd><span class="VerbatimChar">go get -u gonum.org/v1/gonum/...</span></kbd>, which would install the entire suite of Gonum libraries.</p>
<p>Because of the way BLAS works in general, we need a better way of representing matrices than<span> </span><kbd><span class="VerbatimChar">[][]float64</span></kbd>. Here we have two options:</p>
<ul>
<li>Gonum's<span> </span><kbd><span class="VerbatimChar">mat</span></kbd><span> </span>library</li>
<li>Gorgonia's<span> </span><kbd><span class="VerbatimChar">tensor</span></kbd><span> </span>library</li>
</ul>
<p><span>Why Gorgonia's </span><kbd><span class="VerbatimChar">tensor</span></kbd><span>? The reason for </span><kbd><span class="VerbatimChar">tensor</span></kbd><span> </span><span>is quite simple. It plays well with Gorgonia itself, which requires multidimensional arrays. Gonum's </span><kbd><span class="VerbatimChar">mat</span></kbd><span> only takes up to two dimensions, while in the next chapter we'll see a use of four-dimensional arrays. </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">What is a tensor?</h1>
                </header>
            
            <article>
                
<p><span>Fundamentally tensors are very much like vectors. The idea is stolen from physics. Imagine pushing a box on a two-dimensional plane. If you push the box with a force of 1 Newton along the </span><em>x</em><span> axis, there is no force applied to the </span><em>y</em><span> axis. You would write the vector as such:</span><span> </span><kbd>[1, 0]</kbd><span>. If the box were moving along the </span><em>x</em><span> axis with at a speed of 10 km/h and along the </span><em>y</em><span> axis with a speed of 2 km/h, you would write the vector as such:</span><span> </span><kbd>[10, 2]</kbd><span>. Note that they are unitless: the first example was a vector of Newtons, the second example was a vector with km/h as its units.</span></p>
<p><span>In short, it is a representation of something (a force, a speed, or anything with magnitude and direction) applied to a direction. From this idea, computer science co-opted the name vector. But in Go, they're called a <strong>slice</strong>.</span></p>
<div class="packt_infobox"><span>So what is a tensor? Eliding a lot of the details but without a loss of generality, a tensor is like a vector. Except multidimensional. Imagine if you were to describe two speeds along the plane (imagine a silly putty being stretched in two directions at different speeds):</span><span> </span><kbd>[1, 0]</kbd><span> </span><span>and</span><span> </span><kbd>[10, 2]</kbd><span>. You would write it as such:</span><br/>
<kbd>⎡ 1 0⎤</kbd><br/>
<kbd>⎣10 2⎦</kbd></div>
<p><span>This is also called a matrix (when it's two-dimensional). It's called a 3-Tensor when it's three-dimensional, 4-Tensor when its four-dimensional, and so on and so forth. Note that if you have a third speed (that is, the silly putty being stretched in a third direction), you wouldn't have a 3-Tensor. Instead you'd still have  a matrix, with three rows.</span></p>
<p><span>To visualize a 3-Tensor while building on the previous example, imagine if you will, that the two directions that the silly putty was being pulled at was a slice in time. Then imagine another slice in time where the same silly putty is pulled in two directions again. So now you'd have two matrices. A 3-Tensor is what happens when you imagine stacking these matrices together.</span></p>
<p>To convert a<span> </span><kbd><span class="VerbatimChar">[]RawImage</span></kbd><span> </span>to a<span> </span><kbd><span class="VerbatimChar">tensor.Tensor</span></kbd>, the code is as follows:</p>
<pre>func prepareX(M []RawImage) (retVal tensor.Tensor) {<br/>  rows := len(M)<br/>  cols := len(M[0])<br/><br/>  b := make([]float64, 0, rows*cols)<br/>  for i := 0; i &lt; rows; i++ {<br/>    for j := 0; j &lt; len(M[i]); j++ {<br/>      b = append(b, pixelWeight(M[i][j]))<br/>    }<br/>  }<br/>  return tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(b))<br/>}</pre>
<p>Gorgonia may be a bit confusing to beginners. So let me explain the code line by line. But first, you must be aware that like Gonum matrices, Gorgonia tensors, no matter how many dimensions, are also internally represented as a flat slice. Gorgonia tensors are a little more flexible in the sense that they can take more than a flat slice of<span> </span><kbd><span class="VerbatimChar">float64</span></kbd><span> (</span> it takes slices of other types too). This is called the backing slice or array. This is one of the fundamental reasons why performing linear algebra operations is more efficient in Gonum and Gorgonia than using plain<span> </span><kbd><span class="VerbatimChar">[][]float64</span></kbd>.</p>
<p><kbd><span class="VerbatimChar">rows := len(M)</span></kbd><span> </span>and<span> </span><kbd><span class="VerbatimChar">cols := len(M[0])</span></kbd><span> </span>are pretty self explanatory. We want to know the rows (that is, number of images) and columns (the number of pixels in the image).</p>
<p><kbd><span class="VerbatimChar">b := make([]float64, 0, rows*cols)</span></kbd><span> </span>creates the backing array with a capacity of<span> </span><kbd><span class="VerbatimChar">rows * cols</span></kbd>. This backing array is called a backing<span> </span><em>array</em><span> </span>because throughout the lifetime of<span> </span><kbd><span class="VerbatimChar">b</span></kbd>, the size will not change. Here we start with a length of <kbd>0</kbd> because we want to use the<span> </span><kbd><span class="VerbatimChar">append</span></kbd><span> </span>function later on.</p>
<div class="packt_tip"><kbd><span class="VerbatimChar">a := make([]T, 0, capacity)</span></kbd><span> </span>is a good pattern to use to pre-allocate a slice. Consider a snippet that looks like this:<br/>
<kbd><span class="VerbatimChar">a := make([]int, 0)</span></kbd><br/>
<kbd><span class="VerbatimChar">    for i := 0; i &lt; 10; i++ {</span></kbd><br/>
<kbd><span class="VerbatimChar">        a = append(a, i)</span></kbd><br/>
<kbd><span class="VerbatimChar">    }<br/></span></kbd><br/>
During the first call to<span> </span><span class="VerbatimChar">append</span>, the Go runtime will look at the capacity of<span> </span><kbd><span class="VerbatimChar">a</span></kbd>, and find it's <kbd>0</kbd>. So it will allocate some memory to create a slice of size 1. Then on the second call to<span> </span><span class="VerbatimChar">append</span>, the Go runtime will look at the capacity of<span> </span><kbd><span class="VerbatimChar">a</span></kbd><span> </span>and find that it's <kbd>1</kbd>, which is insufficient. So it will allocate twice the current capacity of the slice. On the fourth iteration, it will find the capacity of<span> </span><kbd><span class="VerbatimChar">a</span></kbd><span> is </span>insufficient for appending and once again allocates twice the current capacity of the slice.<br/>
<br/>
The thing about allocation is that it is an expensive operation. Occasionally the Go runtime may not only have to allocate memory, but copy the memory to a new location. This adds to the cost of appending to a slice.<br/>
<br/>
So instead, if we know the capacity of the slice upfront, it's best to allocate all of it in one shot. We can specify the length, but it's often a cause of indexing errors. So my recommendation is to allocate with the capacity and a length of <kbd>0</kbd>. That way, you can safely use<span> </span><span class="VerbatimChar">append</span><span> </span>without having to worry about indexing errors.</div>
<p>After creating a backing slice, we simply populate the backing slice with the values of the pixel, converted to a<span> </span><kbd><span class="VerbatimChar">float64</span></kbd><span> </span>using the<span> </span><kbd><span class="VerbatimChar">pixelWeight</span></kbd><span> </span>function that we described earlier.</p>
<p>Finally, we call<span> </span><kbd><span class="VerbatimChar">tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(b))</span></kbd><span>, </span>which returns a<span> </span><kbd><span class="VerbatimChar">*tensor.Dense</span></kbd>. The<span> </span><kbd><span class="VerbatimChar">tensor.WithShape(rows, cols)</span></kbd><span> </span>construction option creates a<span> </span><kbd><span class="VerbatimChar">*tensor.Dense</span></kbd><span> </span>with the specified shape while<span> </span><kbd><span class="VerbatimChar">tensor.WithBacking(b)</span></kbd><span> </span>simply uses the already pre-allocated and pre-filled<span> </span><kbd><span class="VerbatimChar">b</span></kbd><span> </span>as a backing slice.</p>
<p>The<span> </span><kbd>tensor</kbd><span> </span>library will simply reuse the entire backing array so that fewer allocations are made. What this means is  you have to be careful when handling<span> </span><kbd>b</kbd>. Modifying the contents of<span> </span><kbd>b</kbd><span> </span>afterward will change the content in the<span> </span><kbd>tensor.Dense</kbd><span> </span>as well. Given that<span> </span><kbd>b</kbd><span> </span>was created in the<span> </span><kbd>prepareX</kbd><span> </span>function, once the function has returned, there's no way to modify the contents of<span> </span><kbd>b</kbd>. This is a good way to prevent accidental modification.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">From labels to one-hot vectors</h1>
                </header>
            
            <article>
                
<p>Recall that neural networks built in Gorgonia only take<span> </span><kbd><span class="VerbatimChar">tensor.Tensor</span></kbd>s as inputs. Therefore, the labels will also have to be converted into<span> </span><kbd><span class="VerbatimChar">tensor.Tensor</span></kbd>. The function is quite similar to<span> </span><kbd><span class="VerbatimChar">prepareX</span></kbd>:</p>
<pre>func prepareY(N []Label) (retVal tensor.Tensor) {<br/>  rows := len(N)<br/>  cols := 10<br/><br/>  b := make([]float64, 0, rows*cols)<br/>  for i := 0; i &lt; rows; i++ {<br/>    for j := 0; j &lt; 10; j++ {<br/>      if j == int(N[i]) {<br/>        b = append(b, 1)<br/>      } else {<br/>        b = append(b, 0)<br/>      }<br/>    }<br/>  }<br/>  return tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(b))<br/>}<br/><br/></pre>
<p>What we're building here is a matrix with <em>N</em> rows and ten columns. The specifics of why we build a matrix of <kbd>(N,10)</kbd> will be explored in the next chapter, but for now let's zoom into an imaginary row. Imagine the first label,<span> </span><kbd>(<span class="VerbatimChar">int(N[i])</span>)</kbd><span>, </span>is<span> </span><kbd><span class="VerbatimChar">7</span></kbd>. The row will look like this:</p>
<p><kbd>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</kbd></p>
<p>This is called a one-hot vector encoding. It will be useful to us later, and will expanded upon in the next chapter.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Visualization</h1>
                </header>
            
            <article>
                
<p>It's also useful to have visualization when we are dealing with image data. Earlier we had converted our image pixels from a<span> </span><kbd><span class="VerbatimChar">byte</span></kbd><span> </span>to a<span> </span><kbd><span class="VerbatimChar">float64</span></kbd><span> </span>using<span> </span><kbd><span class="VerbatimChar">pixelWeight</span></kbd>. It'd be instructive to also have the reverse function:</p>
<pre>func reversePixelWeight(px float64) byte {<br/>  return byte(((px - 0.001) / 0.999) * pixelRange)<br/>}</pre>
<p>Here's how to visualize 100 of the images:</p>
<pre>// visualize visualizes the first N images given a data tensor that is made up of float64s.<br/>// It's arranged into (rows, 10) image.<br/>// Row counts are calculated by dividing N by 10 - we only ever want 10 columns.<br/>// For simplicity's sake, we will truncate any remainders.<br/>func visualize(data tensor.Tensor, rows, cols int, filename string) (err error) {<br/>  N := rows * cols<br/><br/>  sliced := data<br/>  if N &gt; 1 {<br/>    sliced, err = data.Slice(makeRS(0, N), nil) // data[0:N, :] in python<br/>    if err != nil {<br/>      return err<br/>    }<br/>  }<br/><br/>  if err = sliced.Reshape(rows, cols, 28, 28); err != nil {<br/>    return err<br/>  }<br/><br/>  imCols := 28 * cols<br/>  imRows := 28 * rows<br/>  rect := image.Rect(0, 0, imCols, imRows)<br/>  canvas := image.NewGray(rect)<br/><br/>  for i := 0; i &lt; cols; i++ {<br/>    for j := 0; j &lt; rows; j++ {<br/>      var patch tensor.Tensor<br/>      if patch, err = sliced.Slice(makeRS(i, i+1), makeRS(j, j+1)); err != nil {<br/>        return err<br/>      }<br/><br/>      patchData := patch.Data().([]float64)<br/>      for k, px := range patchData {<br/>        x := j*28 + k%28<br/>        y := i*28 + k/28<br/>        c := color.Gray{reversePixelWeight(px)}<br/>        canvas.Set(x, y, c)<br/>      }<br/>    }<br/>  }<br/><br/>  var f io.WriteCloser<br/>  if f, err = os.Create(filename); err != nil {<br/>    return err<br/>  }<br/><br/>  if err = png.Encode(f, canvas); err != nil {<br/>    f.Close()<br/>    return err<br/>  }<br/><br/>  if err = f.Close(); err != nil {<br/>    return err<br/>  }<br/>  return nil<br/>}</pre>
<p>The dataset is a huge slice of images. We need to figure out how many we want first;  hence,<span> </span><kbd><span class="VerbatimChar">N := rows * cols</span></kbd>. Having the number we want, we then slice using<span> </span><kbd><span class="VerbatimChar">data.Slice(makeRS(0, N), nil)</span></kbd><span>, </span>which slices the tensor along the first axis. The sliced tensor is then reshaped into a four-dimensional array with<span> </span><kbd><span class="VerbatimChar">sliced.Reshape(rows, cols, 28,28)</span></kbd>. The way you can think about it is to have a stacked rows and columns of 28x28 images.</p>
<div class="packt_infobox"><strong>A primer on slicing</strong><br/>
<br/>
A<span> </span><kbd>*tensor.Dense</kbd><span> </span>acts very much like a standard Go slice; just as you can slice<span> </span><kbd>a[0:2]</kbd>, you can do the same with Gorgonia's tensors. The<span> </span><kbd>.Slice()</kbd><span> </span>method for all tensors accepts a<span> </span><kbd>tensor.Slice</kbd><span> </span>descriptor, defined as: <br/>
<br/>
<kbd>type Slice interface {</kbd><br/>
<kbd>    Start() int</kbd><br/>
<kbd>    End() int</kbd><br/>
<kbd>    Step() int</kbd><br/>
<kbd>}</kbd><br/>
<br/>
As such, we would have to make our own data type that fulfills the<span> </span><kbd>Slice</kbd><span> </span>interface. It's defined in the <kbd>utils.go</kbd> file of this project.<span> </span><kbd>makeRS(0, N)</kbd><span> </span>simply reads as if we were doing<span> </span><kbd>data[0:N]</kbd>. Details and reasoning for this API can be found on the Gorgonia tensor Godoc page.</div>
<p>Then a grayscale image is created using the built-in<span> </span><span class="VerbatimChar">image</span><span> </span>package:<span> </span><kbd><span class="VerbatimChar">canvas := image.NewGray(rect)</span></kbd>. A<span> </span><kbd><span class="VerbatimChar">image.Gray</span></kbd><span> </span>is essentially a slice of bytes and each byte is a pixel. What we need to do next is to fill up the pixels. Quite simply, we simply loop through the columns and rows in each patch, and we fill it up with the correct value extracted from the tensor. The<span> </span><span class="VerbatimChar"><kbd>reversePixelWeight</kbd><span> </span>function</span><span> </span>is used to convert the float into a byte, which is then converted into a<span> </span><kbd><span class="VerbatimChar">color.Gray</span></kbd>. The pixel in the canvas is then set using<span> </span><kbd><span class="VerbatimChar">canvas.Set(x, y, c)</span></kbd>.</p>
<p>Following that, the canvas is encoded as a PNG. <em>Et voilà</em>, our visualization is done!</p>
<p>Now Calling<span> the </span><span class="VerbatimChar">visualize</span><span> </span>in the main function as such:</p>
<pre>func main() {<br/>  imgs, err := readImageFile(os.Open("train-images-idx3-ubyte"))<br/>  if err != nil {<br/>    log.Fatal(err)<br/>  }<br/>  log.Printf("len imgs %d", len(imgs))<br/><br/>  data := prepareX(imgs)<br/>  visualize(data, 100, "image.png")<br/>}</pre>
<p>This yields the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-455 image-border" src="Images/0efea962-a169-49a2-a75e-a1c147214e9e.jpg" style="width:20.50em;height:20.50em;" width="202" height="202"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preprocessing</h1>
                </header>
            
            <article>
                
<p>What we are going to do next is to "whiten" our data using a <strong>Zero Phase Component Analysis</strong> (<strong>ZCA</strong>). The definitions of ZCA is beyond the scope of this chapter, but briefly, ZCA is very much like <strong>Principal Component Analysis</strong> (<strong>PCA</strong>). In our 784-pixel slice, there is a high probability that the pixels are correlated with one another. What PCA does is it finds the set of pixels that are uncorrelated with one another. It does this by looking at all the images at once and figuring out how each column correlates with one another:</p>
<pre>func zca(data tensor.Tensor) (retVal tensor.Tensor, err error) {<br/>  var dataᵀ, data2, sigma tensor.Tensor<br/>  data2 = data.Clone().(tensor.Tensor)<br/><br/>  if err := minusMean(data2); err != nil {<br/>    return nil, err<br/>  }<br/>  if dataᵀ, err = tensor.T(data2); err != nil {<br/>    return nil, err<br/>  }<br/><br/>  if sigma, err = tensor.MatMul(dataᵀ, data2); err != nil {<br/>    return nil, err<br/>  }<br/><br/>  cols := sigma.Shape()[1]<br/>  if _, err = tensor.Div(sigma, float64(cols-1), tensor.UseUnsafe()); err != nil {<br/>    return nil, err<br/>  }<br/><br/>  s, u, _, err := sigma.(*tensor.Dense).SVD(true, true)<br/>  if err != nil {<br/>    return nil, err<br/>  }<br/><br/>  var diag, uᵀ, tmp tensor.Tensor<br/>  if diag, err = s.Apply(invSqrt(0.1), tensor.UseUnsafe()); err != nil {<br/>    return nil, err<br/>  }<br/>  diag = tensor.New(tensor.AsDenseDiag(diag))<br/><br/>  if uᵀ, err = tensor.T(u); err != nil {<br/>    return nil, err<br/>  }<br/><br/>  if tmp, err = tensor.MatMul(u, diag); err != nil {<br/>    return nil, err<br/>  }<br/><br/>  if tmp, err = tensor.MatMul(tmp, uᵀ); err != nil {<br/>    return nil, err<br/>  }<br/><br/>  if err = tmp.T(); err != nil {<br/>    return nil, err<br/>  }<br/><br/>  return tensor.MatMul(data, tmp)<br/>}<br/><br/>func invSqrt(epsilon float64) func(float64) float64 {<br/>  return func(a float64) float64 {<br/>    return 1 / math.Sqrt(a+epsilon)<br/>  }<br/>}</pre>
<p>This is a pretty large chunk of code. Let's go through the code. But first, let's understand the key ideas behind ZCA before going through the code that implements it..</p>
<p>First, recall what PCA does: it finds the set of inputs (columns and pixels, to be used interchangeably) that are least correlated with one another. What ZCA does is then to take the principal components found and multiply them by the inputs to transform the inputs so that they become less correlated with one another.</p>
<p>First, we want to subtract the row mean. To do that, we first make a clone of the data (we'll see why later), then subtract the mean with this function:</p>
<pre>func minusMean(a tensor.Tensor) error {<br/>  nat, err := native.MatrixF64(a.(*tensor.Dense))<br/>  if err != nil {<br/>    return err<br/>  }<br/>  for _, row := range nat {<br/>    mean := avg(row)<br/>    vecf64.Trans(row, -mean)<br/>  }<br/><br/>  rows, cols := a.Shape()[0], a.Shape()[1]<br/><br/>  mean := make([]float64, cols)<br/>  for j := 0; j &lt; cols; j++ {<br/>    var colMean float64<br/>    for i := 0; i &lt; rows; i++ {<br/>      colMean += nat[i][j]<br/>    }<br/>    colMean /= float64(rows)<br/>    mean[j] = colMean<br/>  }<br/><br/>  for _, row := range nat {<br/>    vecf64.Sub(row, mean)<br/>  }<br/><br/>  return nil<br/>}</pre>
<p>After all the preceding spiel about efficiency of a flat slice versus a<span> </span><kbd><span class="VerbatimChar">[][]float64</span></kbd>, what I am going to suggest next is going to sound counter-intuitive. But please bear with me.<span> </span><kbd><span class="VerbatimChar">native.MatrixF64</span><span> </span></kbd>takes a<span> </span><span class="VerbatimChar"><kbd>*tensor.Dense</kbd></span><span> </span>and returns a<span> </span><kbd><span class="VerbatimChar">[][]float64</span></kbd>, which we call<span> </span><kbd><span class="VerbatimChar">nat</span></kbd>.<span> </span><kbd><span class="VerbatimChar">nat</span></kbd><span> </span>shares the same allocation as the tensor<span> </span><kbd><span class="VerbatimChar">a</span></kbd>. No extra allocations are made, and any modification made to<span> </span><kbd><span class="VerbatimChar">nat</span></kbd><span> </span>will show up in<span> </span><kbd><span class="VerbatimChar">a</span></kbd>. In this scenario, we should treat<span> </span><kbd><span class="VerbatimChar">[][]float64</span></kbd><span> </span>as an easy way to iterate through the values in the tensor. This can be seen here:</p>
<pre>  for j := 0; j &lt; cols; j++ {<br/>    var colMean float64<br/>    for i := 0; i &lt; rows; i++ {<br/>      colMean += nat[i][j]<br/>    }<br/>    colMean /= float64(rows)<br/>    mean[j] = colMean<br/>  }</pre>
<p>Like in the<span> </span><kbd><span class="VerbatimChar">visualize</span></kbd><span> </span>function, we first iterate through the columns, albeit for a different purpose. We want to find the mean of each column. We then store the mean of each column in the<span> </span><span class="VerbatimChar">mean</span><span> </span>variable. This allows us to subtract the column mean:</p>
<pre>  for _, row := range nat {<br/>    vecf64.Sub(row, mean)<br/>  }</pre>
<p>This block of code uses the<span> </span><kbd><span class="VerbatimChar">vecf64</span></kbd><span> </span>package that comes with Gorgonia to subtract a slice from another slice, element-wise. It's rather the same as the following:</p>
<pre>  for _, row := range nat {<br/>    for j := range row {<br/>      row[j] -= mean[j]<br/>    }<br/>  }</pre>
<p>The only real reason to use<span> </span><kbd><span class="VerbatimChar">vecf64</span></kbd><span> </span>is that it's optimized to perform the operation with SIMD instructions: instead of doing<span> </span><kbd><span class="VerbatimChar">row[j] -= mean[j]</span></kbd><span> </span>one at a time, it performs<span> </span><kbd><span class="VerbatimChar">row[j] -= mean[j]</span></kbd>,<span> </span><kbd><span class="VerbatimChar">row[j+1] -= mean[j+1]</span></kbd>,<span> </span><kbd><span class="VerbatimChar">row[j+2] -= mean[j+2]</span></kbd>, and <kbd><span class="VerbatimChar">row[j+3] -= mean[j+3]</span></kbd><span> </span>simultaneously.</p>
<p>After we've subtracted the mean, we find its transpose and make a copy of it:</p>
<pre>  if dataᵀ, err = tensor.T(data2); err != nil {<br/>    return nil, err<br/>  }</pre>
<p>Typically, you would find the transpose of a<span> </span><kbd><span class="VerbatimChar">tensor.Tensor</span></kbd><span> </span>by using something like<span> </span><kbd><span class="VerbatimChar">data2.T()</span></kbd>. But this does not return a copy of it. Instead, the <kbd><span class="VerbatimChar">tensor.T</span></kbd>  function clones the data structure, then performs a transposition on it. The reason for that? We're about to use both the tranpose and<span> </span><kbd><span class="VerbatimChar">data2</span></kbd><span> </span>to find<span> </span><kbd><span class="VerbatimChar">Sigma</span></kbd><span> </span>(more on matrix multiplication will be expounded in the next chapter):</p>
<pre>  var sigma tensor.Tensor<br/>  if sigma, err = tensor.MatMul(dataᵀ, data2); err != nil {<br/>    return nil, err<br/>  }</pre>
<p>After we have found<span> </span><kbd><span class="VerbatimChar">sigma</span></kbd>, we divide it by the number of columns-1. This provides an unbiased estimator. The<span> </span><kbd><span class="VerbatimChar">tensor.UseUnsafe</span></kbd><span> </span>option is used to indicate that the result should be stored back into the<span> </span><kbd><span class="VerbatimChar">sigma</span></kbd><span> </span>tensor:</p>
<pre>  cols := sigma.Shape()[1]<br/>  if _, err = tensor.Div(sigma, float64(cols-1), tensor.UseUnsafe()); err != nil {<br/>    return nil, err<br/>  }</pre>
<p>All this is done so that we can perform an SVD on<span> </span><kbd><span class="VerbatimChar">sigma</span></kbd>:</p>
<pre>  s, u, _, err := sigma.(*tensor.Dense).SVD(true, true)<br/>  if err != nil {<br/>    return nil, err<br/>  }</pre>
<p>Singular Value Decomposition, if you are not familiar with it, is a method among many that breaks down a matrix into its constituents. Why would you want to do so? For one, it makes parts of calculations of some things easier. What it does is to factorize<span> </span> <img class="fm-editor-equation" src="Images/e8e5b0cf-7095-4d02-bcea-adf293b141b7.png" style="width:1.08em;height:1.17em;" width="160" height="170"/>, a (M, N) matrix into a (M, N) matrix called<span> <img class="fm-editor-equation" src="Images/b3336713-7358-4b4f-81ea-59bac8d93aa8.png" style="width:0.75em;height:1.08em;" width="120" height="170"/></span>, a (M,M) matrix called<span> <img class="fm-editor-equation" src="Images/2ef3a6de-2db6-47a8-9325-d4ba67b13ab6.png" style="width:0.92em;height:1.00em;" width="160" height="160"/></span>, and a (N, N) matrix called<span> <img class="fm-editor-equation" src="Images/40514d0a-31c6-4a02-b0c3-5836f4d76358.png" style="width:0.92em;height:0.92em;" width="160" height="160"/></span>. To reconstruct A, the formula is simply:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/958010d7-bc97-44b5-9afb-6f620e3879f6.png" style="width:6.50em;height:1.33em;" width="970" height="200"/></p>
<p>The decomposed parts will then be used. In our case, we're not particularly interested about the right singular values <img class="fm-editor-equation" src="Images/8cbd12c1-85b2-42bc-894d-c37cfdbe30e8.png" style="width:1.08em;height:1.25em;" width="160" height="180"/>, so we'll ignore it for now. The decomposed parts are simply used to transform the images, which can be found in the tailend of the function body.</p>
<p>After preprocessing, we can once more visualize the first 100 or so images:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-456 image-border" src="Images/882ed7d0-2a36-45fd-a66b-45c9c5c04b30.png" style="width:31.75em;height:31.83em;" width="280" height="280"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a neural network</h1>
                </header>
            
            <article>
                
<p>Finally, let's build a neural network! We'll be building a simple three-layer neural network with one hidden layer. A three-layer neural network has two weight matrices, so we can define the neural network as such:</p>
<pre>type NN struct {<br/>  hidden, final *tensor.Dense<br/>  b0, b1 float64<br/>}</pre>
<p><kbd><span class="VerbatimChar">hidden</span></kbd><span> </span>represents the weight matrix between the input layer and hidden layer, while<span> </span><kbd><span class="VerbatimChar">final</span></kbd><span> </span>represents the weight matrix between the hidden layer and the final layer.</p>
<p>This is a graphical representation of our<span> </span><span class="VerbatimChar">*NN</span><span> </span>data structure:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/4bba69b0-285c-4dc9-befe-501938ee971c.png" style="width:22.75em;height:26.17em;" width="621" height="712"/></p>
<p>The input layer is the slice of 784<span> </span><kbd><span class="VerbatimChar">float64</span></kbd> which is then fed forward (that is, a matrix multiplication followed by an activation function) to form the hidden layer. The hidden layer is then fed forward to form the final layer. The final layer is a vector of ten<span> </span><span class="VerbatimChar"><kbd>float64</kbd></span>, which is exactly the one-hot encoding that we discussed earlier. You can think of them as pseud-probabilities,  because the values don't exactly sum up to 1.</p>
<p>A key thing to note:<span> </span><kbd><span class="VerbatimChar">b0</span></kbd><span> </span>and<span> </span><kbd><span class="VerbatimChar">b1</span></kbd><span> </span>are bias values for the hidden layer and the final layer, respectively. They are not actually used mainly due to the mess;  it's quite difficult to get the correct differentiation. A challenge for the reader is to later incorporate the use of<span> </span><kbd><span class="VerbatimChar">b0</span></kbd><span> </span>and<span> </span><kbd><span class="VerbatimChar">b1</span></kbd>.</p>
<p>And to create a new neural network, we have the <kbd>New</kbd> function:</p>
<pre>func New(input, hidden, output int) (retVal *NN) {<br/>  r := make([]float64, hidden*input)<br/>  r2 := make([]float64, hidden*output)<br/>  fillRandom(r, float64(len(r)))<br/>  fillRandom(r2, float64(len(r2)))<br/>  hiddenT := tensor.New(tensor.WithShape(hidden, input), tensor.WithBacking(r))<br/>  finalT := tensor.New(tensor.WithShape(output, hidden), tensor.WithBacking(r2))<br/>  return &amp;NN{<br/>    hidden: hiddenT,<br/>    final: finalT,<br/>  }<br/>}</pre>
<p>The<span> </span><kbd><span class="VerbatimChar">fillRandom</span></kbd><span> </span>function fills a<span> </span><kbd><span class="VerbatimChar">[]float64</span></kbd><span> </span>with random values. In our case, we fill it up from random values drawn from a uniform distribution. Here, we use the<span> </span><kbd><span class="VerbatimChar">distuv</span></kbd><span> </span>package from Gonum:</p>
<pre>func fillRandom(a []float64, v float64) {<br/>  dist := distuv.Uniform{<br/>    Min: -1 / math.Sqrt(v),<br/>    Max: 1 / math.Sqrt(v),<br/>  }<br/>  for i := range a {<br/>    a[i] = dist.Rand()<br/>  }<br/>}</pre>
<p>After the slices<span> </span><kbd><span class="VerbatimChar">r</span></kbd><span> </span>and<span> </span><kbd><span class="VerbatimChar">r2</span></kbd><span> </span>have been filled, the tensors<span> </span><kbd><span class="VerbatimChar">hiddenT</span></kbd><span> </span>and<span> </span><kbd><span class="VerbatimChar">finalT</span></kbd><span> </span>are created, and the<span> </span><kbd><span class="VerbatimChar">*NN</span></kbd><span> </span>is returned.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feed forward</h1>
                </header>
            
            <article>
                
<p>Now that we have a conceptual idea of how the neural network works, let's write the forward propagation function. We'll call it<span> </span><kbd><span class="VerbatimChar">Predict</span></kbd><span> </span>because, well, to predict, you merely need to run the function forward:</p>
<pre>func (nn *NN) Predict(a tensor.Tensor) (int, error) {<br/>  if a.Dims() != 1 {<br/>    return -1, errors.New("Expected a vector")<br/>  }<br/><br/>  var m maybe<br/>  hidden := m.do(func() (tensor.Tensor, error) { return nn.hidden.MatVecMul(a) })<br/>  act0 := m.do(func() (tensor.Tensor, error) { return hidden.Apply(sigmoid, tensor.UseUnsafe()) })<br/><br/>  final := m.do(func() (tensor.Tensor, error) { return tensor.MatVecMul(nn.final, act0) })<br/>  pred := m.do(func() (tensor.Tensor, error) { return final.Apply(sigmoid, tensor.UseUnsafe()) })<br/><br/>  if m.err != nil {<br/>    return -1, m.err<br/>  }<br/>  return argmax(pred.Data().([]float64)), nil<br/>}</pre>
<p>This is fairly straightforward, except for a few control structures. I should first explain that the API of the<span> </span><span class="VerbatimChar">tensor</span><span> </span>package is quite expressive in the sense in that it allows the user multiple ways of doing the same thing, albeit with different type signatures. Briefly, the patterns are the following:</p>
<ul>
<li><kbd><span class="VerbatimChar">tensor.BINARYOPERATION(a, b tensor.Tensor, opts ...tensor.FuncOpt) (tensor.Tensor, error)</span></kbd></li>
<li><kbd><span class="VerbatimChar">tensor.UNARYOPERATION(a tensor.Tensor, opts ...tensor.FuncOpt)(tensor.Tensor, error)</span></kbd></li>
<li><kbd><span class="VerbatimChar">(a *tensor.Dense) BINARYOPERATION (b *tensor.Dense, opts ...tensor.FuncOpt) (*tensor.Dense, error)</span></kbd></li>
<li><kbd><span class="VerbatimChar">(a *tensor.Dense) UNARYOPERATION(opts ...tensor.FuncOpt) (*tensor.Dense, error)</span></kbd></li>
</ul>
<p>Key things to note are package level operations (<kbd><span class="VerbatimChar">tensor.Add</span></kbd>,<span> </span><kbd><span class="VerbatimChar">tensor.Sub</span></kbd><span> , </span>and so on) take one or more<span> </span><kbd><span class="VerbatimChar">tensor.Tensor</span></kbd>s and return a<span> </span><kbd><span class="VerbatimChar">tensor.Tensor</span></kbd><span> </span>and an<span> </span><kbd><span class="VerbatimChar">error</span></kbd>. There are multiple things that fulfill a<span> </span><kbd><span class="VerbatimChar">tensor.Tensor</span></kbd><span> </span>interface, and the<span> </span><span class="VerbatimChar">tensor</span><span> </span>package provides two structural types that fulfill the interface:</p>
<ul>
<li><kbd><span class="VerbatimChar">*tensor.Dense</span></kbd>: A representation of of a densely packed tensor</li>
<li><kbd><span class="VerbatimChar">*tensor.CS</span></kbd>: A memory-efficient representation of a sparsely packed tensor with the data arranged in compressed sparse columns/row format</li>
</ul>
<p>For the most part, the most commonly used type of<span> </span><kbd><span class="VerbatimChar">tensor.Tensor</span></kbd><span> </span>is the<span> </span><kbd><span class="VerbatimChar">*tensor.Dense</span></kbd><span> </span>type. The<span> </span><kbd><span class="VerbatimChar">*tensor.CS</span></kbd><span> </span>data structure is only used for very specific memory-constrained optimizations for specific algorithms. We shan't talk more about the<span> </span><kbd>*<span class="VerbatimChar">tensor.CS</span></kbd><span> </span>type in this chapter.</p>
<p>In addition to the package level operations, each specific type also has methods that they implement.<span> </span><kbd><span class="VerbatimChar">*tensor.Dense</span></kbd>'s methods (<kbd><span class="VerbatimChar">.Add(...)</span></kbd>,<span> </span><kbd><span class="VerbatimChar">.Sub(...)</span></kbd>, and so on) take one or more<span> </span><kbd><span class="VerbatimChar">*tensor.Dense</span></kbd><span> </span>and return<span> </span><kbd><span class="VerbatimChar">*tensor.Dense</span></kbd><span> </span>and an<span> </span><span class="VerbatimChar">error</span>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Handling errors with maybe</h1>
                </header>
            
            <article>
                
<p>With that quick introduction out of the way, we can now talk about the<span> </span><kbd><span class="VerbatimChar">maybe</span></kbd><span> </span>type.</p>
<p>One of the things you may have already noticed is that almost all the operations return an error. Indeed, there are very few functions and methods that do not return an error. The logic behind this is simple: most of the errors are actually recoverable and have suitable recovery strategies.</p>
<p>However, for this project, we have one error recovery strategy: bubble up the error to the<span> </span><kbd><span class="VerbatimChar">main</span></kbd><span> </span>function, where a<span> </span><kbd><span class="VerbatimChar">log.Fatal</span><span> </span></kbd>will be called and the error will be inspected for debugging.</p>
<p>So, I defined<span> </span><kbd><span class="VerbatimChar">maybe</span></kbd><span> </span>as follows:</p>
<pre>type maybe struct {<br/>  err error<br/>}<br/><br/>func (m *maybe) do(fn func() (tensor.Tensor, error)) tensor.Tensor {<br/>  if m.err != nil {<br/>    return nil<br/>  }<br/><br/>  var retVal tensor.Tensor<br/>  if retVal, m.err = fn(); m.err == nil {<br/>    return retVal<br/>  }<br/>  m.err = errors.WithStack(m.err)<br/>  return nil<br/>}</pre>
<p>This way, it is able to handle any function as long as it's wrapped within a closure.</p>
<p>Why do this? I personally do not enjoy this structure. I taught it to a few students of mine as a cool trick, and since then they claimed that the resulting code was more understandable than having blocks of:</p>
<pre>if foo, err := bar(); err != nil {<br/>  return err<br/>}</pre>
<p>I can definitely empathize with this view. It is most useful in my opinion in the prototyping phase, especially when it is not clear yet when and where to handle the error (in our case, return early). Leaving the returning of an error until the end of the function can be useful. In production code though, I would prefer to be as explicit as possible about error-handling strategies.</p>
<p>This can be further augmented by abstracting common function calls into methods. For example, we see this line,<span> </span><kbd><span class="VerbatimChar">m.do(func() (tensor.Tensor, error) { return hidden.Apply(sigmoid, tensor.UseUnsafe()) })</span><span> </span></kbd>, twice in the preceding snippet. If we want to prioritize understandability while leaving the structure mostly intact, we could abstract it away by creating a new method:</p>
<pre>func (m *maybe) sigmoid(a tensor.Tensor) (retVal tensor.Tensor){<br/>  if m.err != nil {<br/>    return nil<br/>  }<br/>  if retVal, m.err = a.Apply(sigmoid); m.err == nil {<br/>    return retVal<br/>  }<br/>  m.err = errors.WithStack(m.err)<br/>  return nil<br/>}</pre>
<p>And we would just call<span> </span><kbd><span class="VerbatimChar">m.sigmoid(hidden)</span></kbd><span> </span>instead. This is one of the many error-handling strategies that programmers can employ to help them. Remember, you're a programmer; you are allowed and even expected to program your way out!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Explaining the feed forward function</h1>
                </header>
            
            <article>
                
<p>With all that done, let's walk through the the feed forward function, line by line.</p>
<p>First, recall from the section, <em>Emulating a neural network</em>, that we can define a neural network as follows:</p>
<pre>func affine(weights [][]float64, inputs []float64) []float64 {<br/>  return activation(matVecMul(weights, inputs))<br/>}</pre>
<p>We do the first matrix multiplication as part of calculating the first hidden layer:<span> </span><kbd><span class="VerbatimChar">hidden := m.do(func() (tensor.Tensor, error) { return nn.hidden.MatVecMul(a)) })</span></kbd>.<span> </span><kbd><span class="VerbatimChar">MatVecMul</span></kbd><span> </span>is used because we're multiplying a matrix by a vector.</p>
<p>Then we perform the second part of calculating a layer:<span> </span><kbd><span class="VerbatimChar">act0 := m.do(func() (tensor.Tensor, error) { return hidden.Apply(sigmoid, tensor.UseUnsafe()) })</span></kbd>. Once again, the<span> </span><kbd><span class="VerbatimChar">tensor.UseUnsafe()</span></kbd><span> </span>function option is used to tell the function to not allocate a new<span> </span><span class="VerbatimChar">tensor</span>. <em>Voila</em>! We've successfully calculated the first layer.</p>
<p>The same two steps are repeated for the final layer, and we get a one-hot-ish vector. Do note that for the first step, I used<span> </span><kbd><span class="VerbatimChar">tensor.MatVecMul(nn.final, act0)</span></kbd><span> </span>instead of<span> </span><kbd><span class="VerbatimChar">nn.final.MatVecMul(act0)</span></kbd>. This was done to show that both functions are indeed the same, and they just take different types (the method takes a concrete type while the package function takes an abstract data type). They are otherwise identical in function.</p>
<div class="packt_infobox"> Notice how the<span> </span><kbd><span class="VerbatimChar">affine</span></kbd><span> </span>function is quite easy to read, whereas the other functions are quite difficult to read? Read through the section about<span> </span><kbd><span class="VerbatimChar">maybe</span></kbd><span> </span>and see if you can come up with a way to write it in such a way that it reads more like<span> </span><span class="VerbatimChar">affine</span>.<br/>
Is there a way to abstract the function into a function like<span> </span><kbd><span class="VerbatimChar">affine</span></kbd><span> </span>so that you could just call a single function and not repeat yourself?</div>
<p>Before we return the result, we need to perform a check to see if anything in the prece-ing steps have errored. Think about what are the errors that could happen. They would, in my experience, predominantly be shape related errors. In this specific project, a shape error should be considered a failure, so we return a nil result and the error.</p>
<p>The reason why we would have to check for errors at this point is because we are about to use<span> </span><kbd><span class="VerbatimChar">pred</span></kbd>. If<span> </span><kbd><span class="VerbatimChar">pred</span></kbd><span> </span>is<span> </span><span class="VerbatimChar">nil</span><span> </span>(which it would be if an error had occurred earlier), trying to access the<span> </span><kbd><span class="VerbatimChar">.Data()</span></kbd><span> </span>function would cause a panic.</p>
<p>Anyway, after the check, we call the<span> </span><kbd><span class="VerbatimChar">.Data()</span></kbd><span> </span>method, which returns the raw data as a flat slice. It's an<span> </span><kbd><span class="VerbatimChar">interface{}</span></kbd><span> </span>type though, so we would have to convert it back to a<span> </span><kbd><span class="VerbatimChar">[]float64</span></kbd><span> </span>before inspecting the data further. Because the result is a vector, it is no different in data layout from a<span> </span><kbd><span class="VerbatimChar">[]float64</span></kbd>, so we can directly call<span> </span><kbd><span class="VerbatimChar">argmax</span></kbd><span> </span>on it.</p>
<p><kbd><span class="VerbatimChar">argmax</span></kbd><span> </span>simply returns the index of the greatest value in the slice. It's defined thus:</p>
<pre>func affine(weights [][]float64, inputs []float64) []float64 {<br/>  return activation(matVecMul(weights, inputs))<br/>}</pre>
<p>And thus, we have managed to write a feed forward function for our neural network.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Costs</h1>
                </header>
            
            <article>
                
<p>Having written a fairly straightforward feed forward function, let's now look at how to make the neural network learn.</p>
<p>Recall that we said earlier that a neural network learns when you tell it that it's made a mistake? More technically, we ask the question: what kind of cost function can we use so that it is able to convey to the neural network accurately about what the true value is.</p>
<p>The cost function we want to use for this project is the sum of squared errors. What is an error? Well, an error is simply the difference between the real value and the predicted value. Does this mean that if the real value is<span> </span><kbd><span class="VerbatimChar">7</span></kbd>, and the neural network predicted<span> </span><kbd><span class="VerbatimChar">2</span></kbd>, the cost would just be<span> </span><span class="VerbatimChar"><kbd>7</kbd>-<kbd>2</kbd></span><span> </span>? No. This is because we should not treat the labels as numbers. They are labels.</p>
<p>So what do we subtract? Recall the one-hot vector that we created earlier? If we peek inside the<span> </span><kbd><span class="VerbatimChar">Predict</span></kbd><span> </span>function, we can see that<span> </span><kbd><span class="VerbatimChar">pred</span></kbd>, the result of the final activation is a slice of ten<span> </span><kbd><span class="VerbatimChar">float64</span></kbd>s. That's what we're going to subtract. Because both are slices of ten<span> </span><span class="VerbatimChar"><kbd>float64</kbd></span>s, we would have to subtract them element-wise.</p>
<p>Merely subtracting the slices would not be useful; the results may be negative. Imagine if you were tasked to find the lowest possible costs for a product. If someone came up to you and told you that their product costs negative amounts and that they would pay you to use it, would you not use it? So to prevent that, we take the square of the errors.</p>
<p>To calculate the sum of squared errors, we simply square the result. Because we're training the neural network one image at a time, the sum is simply the squared errors of that one image.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Backpropagation</h1>
                </header>
            
            <article>
                
<p>The section on costs is a little sparse for good reason. Furthermore, there is a twist: we're not going to entirely calculate the full cost function, mainly because we don't need to for this specific case. Costs are heavily tied to the notion of backpropagation. Now we're going to do some mathematical trickery.</p>
<p>Recall that our cost was the sum of squared errors. We can write it like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6f4c7e6a-9d36-4667-a1a1-31446b34ff28.png" style="width:12.33em;height:2.00em;" width="1480" height="240"/></p>
<p>Now what I am about to describe can sound very much like cheating, but it's a valid strategy. The derivative with regard to<span> </span><span class="VerbatimChar">prediction</span><span> </span>is this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b363491e-6a94-4a6b-93cf-32484d500d7e.png" style="width:14.00em;height:3.75em;" width="1680" height="450"/></p>
<p>To make things a bit easier on ourselves, let's redefine the cost as this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/61dca636-79b3-48f2-8399-a2ddcc7fd9bb.png" style="width:13.67em;height:3.33em;" width="1640" height="400"/></p>
<p>It doesn't make a difference to the process of finding the lowest cost. Think about it; imagine a highest cost and a lowest cost. The difference between them if there is a<span> </span><img class="fm-editor-equation" src="Images/2a654360-2167-43c0-b2bb-1c1fee72bfcc.png" style="width:0.83em;height:2.17em;" width="180" height="460"/> multiplier in front of them does not change the fact that the lowest cost is still lower than the highest cost. Take some time to work this out on your own to convince yourself that having a constant multiplier doesn't change the process.</p>
<p>The derivative of a <kbd>sigmoid</kbd> function is:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/175fea51-70db-4c72-9023-0335d29534c5.png" style="width:16.25em;height:3.58em;" width="1950" height="430"/></p>
<p>From there, we can work out the derivation of the cost function with regard to the weights matrix. How to work out the full backpropagation will be explained in the next chapter. For now, here is the code:</p>
<pre>  // backpropagation<br/>  outputErrors := m.do(func() (tensor.Tensor, error) { return tensor.Sub(y, pred) })<br/>  cost = sum(outputErrors.Data().([]float64))<br/>  <br/>  hidErrs := m.do(func() (tensor.Tensor, error) {<br/>    if err := nn.final.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer nn.final.UT()<br/>    return tensor.MatMul(nn.final, outputErrors)<br/>  })<br/><br/>  if m.err != nil {<br/>    return 0, m.err<br/>  }<br/><br/>  dpred := m.do(func() (tensor.Tensor, error) { return pred.Apply(dsigmoid, tensor.UseUnsafe()) })<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Mul(pred, outputErrors, tensor.UseUnsafe()) })<br/>  // m.do(func() (tensor.Tensor, error) { err := act0.T(); return act0, err })<br/>  dpred_dfinal := m.do(func() (tensor.Tensor, error) {<br/>    if err := act0.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer act0.UT()<br/>    return tensor.MatMul(outputErrors, act0)<br/>  })<br/><br/>  dact0 := m.do(func() (tensor.Tensor, error) { return act0.Apply(dsigmoid) })<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Mul(hidErrs, dact0, tensor.UseUnsafe()) })<br/>  m.do(func() (tensor.Tensor, error) { err := hidErrs.Reshape(hidErrs.Shape()[0], 1); return hidErrs, err })<br/>  // m.do(func() (tensor.Tensor, error) { err := x.T(); return x, err })<br/>  dcost_dhidden := m.do(func() (tensor.Tensor, error) {<br/>    if err := x.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer x.UT()<br/>    return tensor.MatMul(hidErrs, x)<br/>  })</pre>
<p>And there we have it, the derivatives of the cost with regard to the inputs matrices.</p>
<p>The thing to do with the derivatives is to use them as gradients to update the input matrices. To do that, use a simple gradient descent algorithm; we simply add the gradient to the values itself. But we don't want to add the full value of the gradient. If we do that and our starting value is very close to the minima, we'd overshoot it. So we need to multiply the gradients by some small value, known as the learn rate:</p>
<pre>  // gradient update<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Mul(dcost_dfinal, learnRate, tensor.UseUnsafe()) })<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Mul(dcost_dhidden, learnRate, tensor.UseUnsafe()) })<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Add(nn.final, dcost_dfinal, tensor.UseUnsafe()) })<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Add(nn.hidden, dcost_dhidden, tensor.UseUnsafe()) })</pre>
<p>And this is the training function in full:</p>
<pre>// X is the image, Y is a one hot vector<br/>func (nn *NN) Train(x, y tensor.Tensor, learnRate float64) (cost float64, err error) {<br/>  // predict<br/>  var m maybe<br/>  m.do(func() (tensor.Tensor, error) { err := x.Reshape(x.Shape()[0], 1); return x, err })<br/>  m.do(func() (tensor.Tensor, error) { err := y.Reshape(10, 1); return y, err })<br/><br/>  hidden := m.do(func() (tensor.Tensor, error) { return tensor.MatMul(nn.hidden, x) })<br/>  act0 := m.do(func() (tensor.Tensor, error) { return hidden.Apply(sigmoid, tensor.UseUnsafe()) })<br/><br/>  final := m.do(func() (tensor.Tensor, error) { return tensor.MatMul(nn.final, act0) })<br/>  pred := m.do(func() (tensor.Tensor, error) { return final.Apply(sigmoid, tensor.UseUnsafe()) })<br/>  // log.Printf("pred %v, correct %v", argmax(pred.Data().([]float64)), argmax(y.Data().([]float64)))<br/><br/>  // backpropagation.<br/>  outputErrors := m.do(func() (tensor.Tensor, error) { return tensor.Sub(y, pred) })<br/>  cost = sum(outputErrors.Data().([]float64))<br/><br/>  hidErrs := m.do(func() (tensor.Tensor, error) {<br/>    if err := nn.final.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer nn.final.UT()<br/>    return tensor.MatMul(nn.final, outputErrors)<br/>  })<br/><br/>  if m.err != nil {<br/>    return 0, m.err<br/>  }<br/><br/>  dpred := m.do(func() (tensor.Tensor, error) { return pred.Apply(dsigmoid, tensor.UseUnsafe()) })<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Mul(pred, outputErrors, tensor.UseUnsafe()) })<br/>  // m.do(func() (tensor.Tensor, error) { err := act0.T(); return act0, err })<br/>  dpred_dfinal := m.do(func() (tensor.Tensor, error) {<br/>    if err := act0.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer act0.UT()<br/>    return tensor.MatMul(outputErrors, act0)<br/>  })<br/><br/>  dact0 := m.do(func() (tensor.Tensor, error) { return act0.Apply(dsigmoid) })<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Mul(hidErrs, dact0, tensor.UseUnsafe()) })<br/>  m.do(func() (tensor.Tensor, error) { err := hidErrs.Reshape(hidErrs.Shape()[0], 1); return hidErrs, err })<br/>  // m.do(func() (tensor.Tensor, error) { err := x.T(); return x, err })<br/>  dcost_dhidden := m.do(func() (tensor.Tensor, error) {<br/>    if err := x.T(); err != nil {<br/>      return nil, err<br/>    }<br/>    defer x.UT()<br/>    return tensor.MatMul(hidErrs, x)<br/>  })<br/><br/>  // gradient update<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Mul(dcost_dfinal, learnRate, tensor.UseUnsafe()) })<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Mul(dcost_dhidden, learnRate, tensor.UseUnsafe()) })<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Add(nn.final, dcost_dfinal, tensor.UseUnsafe()) })<br/>  m.do(func() (tensor.Tensor, error) { return tensor.Add(nn.hidden, dcost_dhidden, tensor.UseUnsafe()) })<br/>  return cost, m.err</pre>
<p>There are several observations to be made:</p>
<ul>
<li>You may note that parts of the body of the<span> </span><kbd><span class="VerbatimChar">Predict</span></kbd><span> </span>method are repeated at the top of the<span> </span><kbd><span class="VerbatimChar">Train</span></kbd><span> </span>method</li>
<li>The<span> </span><span><kbd><span class="VerbatimChar">tensor.UseUnsafe()</span></kbd></span><span> </span><span>function option is used a lot</span></li>
</ul>
<p>This is going to be a pain point when we start scaling up into deeper networks. As such, in the next chapter, we will explore the possible solutions to these problems.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training the neural network</h1>
                </header>
            
            <article>
                
<p>Our main looks like this so far:</p>
<pre>func main() {<br/>  imgs, err := readImageFile(os.Open("train-images-idx3-ubyte"))<br/>  if err != nil {<br/>    log.Fatal(err)<br/>  }<br/>  labels, err := readLabelFile(os.Open("train-labels-idx1-ubyte"))<br/>  if err != nil {<br/>    log.Fatal(err)<br/>  }<br/><br/>  log.Printf("len imgs %d", len(imgs))<br/>  data := prepareX(imgs)<br/>  lbl := prepareY(labels)<br/>  visualize(data, 10, 10, "image.png")<br/><br/>  data2, err := zca(data)<br/>  if err != nil {<br/>    log.Fatal(err)<br/>  }<br/>  visualize(data2, 10, 10, "image2.png")<br/><br/>  nat, err := native.MatrixF64(data2.(*tensor.Dense))<br/>  if err != nil {<br/>    log.Fatal(err)<br/>  }<br/><br/>  log.Printf("Start Training")<br/>  nn := New(784, 100, 10)<br/>  costs := make([]float64, 0, data2.Shape()[0])<br/>  for e := 0; e &lt; 5; e++ {<br/>    data2Shape := data2.Shape()<br/>    var oneimg, onelabel tensor.Tensor<br/>    for i := 0; i &lt; data2Shape[0]; i++ {<br/>      if oneimg, err = data2.Slice(makeRS(i, i+1)); err != nil {<br/>        log.Fatalf("Unable to slice one image %d", i)<br/>      }<br/>      if onelabel, err = lbl.Slice(makeRS(i, i+1)); err != nil {<br/>        log.Fatalf("Unable to slice one label %d", i)<br/>      }<br/>      var cost float64<br/>      if cost, err = nn.Train(oneimg, onelabel, 0.1); err != nil {<br/>        log.Fatalf("Training error: %+v", err)<br/>      }<br/>      costs = append(costs, cost)<br/>    }<br/>    log.Printf("%d\t%v", e, avg(costs))<br/>    shuffleX(nat)<br/>    costs = costs[:0]<br/>  }<br/>  log.Printf("End training")<br/>}</pre>
<p>Here are the steps in brief:</p>
<ol>
<li>Load image files.</li>
<li>Load label files.</li>
<li>Convert image files into<span> </span><kbd><span class="VerbatimChar">*tensor.Dense.</span></kbd></li>
<li>Convert label files into<span> </span><kbd><span class="VerbatimChar">*tensor.Dense.</span></kbd></li>
<li>Visualize 100 of the images.</li>
<li>Perform ZCA whitening on the images.</li>
<li>Visualize the whitened images.</li>
<li>Create a native iterator for the dataset.</li>
<li>Create the neural network with a 100 unit hidden layer.</li>
<li>Create a slice of the costs. This is so we can keep track of the average cost over time.</li>
<li>Within each epoch, slice the input into single image slices.</li>
<li>Within each epoch, slice the output labels into single slices.</li>
<li>Within each epoch, call<span> </span><kbd><span class="VerbatimChar">nn.Train()</span></kbd><span> </span>with a learn rate of<span> </span><kbd><span class="VerbatimChar">0.1</span></kbd><span> </span>and use the sliced single image and single labels as a training example.</li>
<li>Train for five epochs.</li>
</ol>
<p>How would we know that the neural network has learned well? One way is to monitor the costs. If the neural network is learning, the average costs over time will drop. There may be bumps, of course, but the overall big picture should be that the cost does not end up higher than when the program first runs.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cross-validation</h1>
                </header>
            
            <article>
                
<p>Another way we could test how well the neural network is learning is to cross-validate. The neural network could learn very well on the training data, in essence, memorizing which collections of pixels will result in a particular label. However, to check that the machine learning algorithm generalizes well, we need to show the neural network some data it's never seen before.</p>
<p>Here's the code to do so:</p>
<pre>  log.Printf("Start testing")<br/>  testImgs, err := readImageFile(os.Open("t10k-images.idx3-ubyte"))<br/>  if err != nil {<br/>    log.Fatal(err)<br/>  }<br/><br/>  testlabels, err := readLabelFile(os.Open("t10k-labels.idx1-ubyte"))<br/>  if err != nil {<br/>    log.Fatal(err)<br/>  }<br/><br/>  testData := prepareX(testImgs)<br/>  testLbl := prepareY(testlabels)<br/>  shape := testData.Shape()<br/>  testData2, err := zca(testData)<br/>  if err != nil {<br/>    log.Fatal(err)<br/>  }<br/><br/>  visualize(testData, 10, 10, "testData.png")<br/>  visualize(testData2, 10, 10, "testData2.png")<br/><br/>  var correct, total float64<br/>  var oneimg, onelabel tensor.Tensor<br/>  var predicted, errcount int<br/>  for i := 0; i &lt; shape[0]; i++ {<br/>    if oneimg, err = testData.Slice(makeRS(i, i+1)); err != nil {<br/>      log.Fatalf("Unable to slice one image %d", i)<br/>    }<br/>    if onelabel, err = testLbl.Slice(makeRS(i, i+1)); err != nil {<br/>      log.Fatalf("Unable to slice one label %d", i)<br/>    }<br/>    if predicted, err = nn.Predict(oneimg); err != nil {<br/>      log.Fatalf("Failed to predict %d", i)<br/>    }<br/><br/>    label := argmax(onelabel.Data().([]float64))<br/>    if predicted == label {<br/>      correct++<br/>    } else if errcount &lt; 5 {<br/>      visualize(oneimg, 1, 1, fmt.Sprintf("%d_%d_%d.png", i, label, predicted))<br/>      errcount++<br/>    }<br/>    total++<br/>  }<br/>  fmt.Printf("Correct/Totals: %v/%v = %1.3f\n", correct, total, correct/total)</pre>
<p>Note that the code is largely the same as the code before in the <kbd>main</kbd> function. The exception is that instead of calling<span> </span><kbd><span class="VerbatimChar">nn.Train</span></kbd>, we call<span> </span><kbd><span class="VerbatimChar">nn.Predict</span></kbd>. Then we check to see whether the label is the same as what we predicted.</p>
<p>Here are the tweakable parameters:</p>
<p>After running (it takes 6.5 minutes), and tweaking various parameters, I ran the code and got the following results:</p>
<pre><span class="VerbatimChar">$ go build . -o chapter7</span><br/> <span class="VerbatimChar">$ ./chapter7</span><br/> <span class="VerbatimChar">Corerct/Totals: 9719/10000 = 0.972</span></pre>
<p>A simple three-layer neural network leads to a 97% accuracy! This is, of course, not close to state of the art. We'll build one that goes up to 99.xx% in the next chapter, but requires a big shift of mindset.</p>
<div class="packt_infobox">Training a neural network takes time. It's often wise to want to save the result of the neural network. The<span> </span><kbd><span class="VerbatimChar">*tenso</span><span class="VerbatimChar">r.Dense</span><span> </span>type</kbd> implements<kbd><span> </span><span class="VerbatimChar">gob.GobEncoder</span></kbd><span> </span>and<span> </span><kbd><span class="VerbatimChar">gob.GobDecoder</span><span> </span></kbd>and to save the neural network to disk, simply save the weights (<kbd><span class="VerbatimChar">nn.hidden</span></kbd><span> </span>and<kbd><span> </span><span class="VerbatimChar">nn.final</span></kbd>). For an additional challenge, write a gob encoder for those weight matrices and save/load the functionality. </div>
<p>Furthermore, let's have a look at a few of the things that was wrongly classified. In the preceding code, this snippet writes out five wrong predictions:</p>
<pre>    if predicted == label {<br/>      correct++<br/>    } else if errcount &lt; 5 {<br/>      visualize(oneimg, 1, 1, fmt.Sprintf("%d_%d_%d.png", i, label, predicted))<br/>      errcount++<br/>    }</pre>
<p>And here they are:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/fd98a558-7acd-4604-a721-9051bf245c7d.png" width="28" height="28"/> <img src="Images/4013c335-7f1a-48da-91d0-b51ce57444f9.png" width="28" height="28"/> <img src="Images/a046a761-ede0-40d8-ba20-dd8e66e956de.png" width="28" height="28"/></p>
<p>In the first image, the neural network classified it as a <kbd>0</kbd>, while the true value is <kbd>6</kbd>. As you can see, it is an easy mistake to make. The second image shows a <kbd>2</kbd>, and the neural network classified it as a <kbd>4</kbd>. You may be inclined to think that looks a bit like a <kbd>4</kbd>. And, lastly, if you are an American reader, the chances are you have been exposed to the Palmer handwriting method. If so, I'll bet that you might classify the last picture as a 7, instead of a <kbd>2</kbd>, which is exactly what the neural network predicts. Unfortunately, the real label is that it's a <kbd>2</kbd>. Some people just have terrible handwriting.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we've learned how to write a simple neural network with one hidden layer that performs remarkably well. Along the way, we've learned how to perform ZCA whitening so that the data can be cleaned. There are some difficulties with this model, of course; you'd have to pre-calculate the derivatives by hand before you coded it.</p>
<p>The key takeaway point is that a simple neural network can do a lot! While this version of the code is very Gorgonia's<span> </span><span class="VerbatimChar">tensor-</span>centric, the principles are exactly the same, even if using Gonum's<span> </span><span class="VerbatimChar">mat</span>. In fact, Gorgonia's<span> </span><span class="VerbatimChar">tensor</span><span> </span>uses Gonum's awesome matrix multiplication library underneath.</p>
<p>In the next chapter, we will revisit the notion of a neural network on the same dataset to get a 99% accuracy, but our mindsets of how to approach a neural network will have to change. I would advise re-reading the section on linear algebra to get a stronger grasp on things.</p>


            </article>

            
        </section>
    </div>



  </body></html>