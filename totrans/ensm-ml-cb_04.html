<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Statistical and Machine Learning Algorithms</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft3">In this chapter, we will cover the following recipes:</p>
<ul class="calibre10">
<li class="calibre11">Multiple linear regression</li>
<li class="calibre11">Logistic regression</li>
<li class="calibre11">Naive Bayes</li>
<li class="calibre11">Decision trees</li>
<li class="calibre11">Support vector machines</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">The technical requirements for this chapter remain the same as those we detailed in <a href="2dbc8735-7eef-42ba-8b19-5644632c3197.xhtml" class="calibre9">Chapter 1</a>, <em class="calibre13">Get Closer to Your Data</em>.</span></p>
<p class="calibre2">Visit the GitHub repository to get the dataset and the code. These are arranged by chapter and by the name of the topic. For the linear regression dataset and code, for example, visit <kbd class="calibre12">.../Chapter 3/Linear regression</kbd>.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiple linear regression</h1>
                </header>
            
            <article>
                
<p class="calibre2">Multiple linear regression is a technique used to train a linear model, that assumes that there are linear relationships between multiple predictor variables (<img class="fm-editor-equation12" src="assets/a873ef1f-dd8a-4450-8afa-5e5c3c4b2dc5.png"/>) and a continuous target variable (<img class="fm-editor-equation13" src="assets/9d832a44-5813-46fb-9d89-681fc4ed016a.png"/>). The general equation for a multiple linear regression with m predictor variables is as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation14" src="assets/088198d7-1233-4571-9fcc-61c6dba157ad.png"/></p>
<p class="CDPAlignLeft3">                                                                        <img class="fm-editor-equation15" src="assets/19a78dc6-d3ee-4492-b89e-6d42851d79b0.png"/></p>
<p class="calibre2"><span class="calibre5">Training a linear regression model involves estimating the values of the coefficients for each of the predictor variables denoted by the </span>letter <img class="fm-editor-equation16" src="assets/a343305a-25c0-4a07-8783-5045b92efe76.png"/>. In the preceding equation, <img class="fm-editor-equation17" src="assets/9c4fefc7-e6fe-4dd2-a801-a379f072ae3c.png"/> denotes an error term, which is normally distributed, and has zero mean and constant variance. This is represented as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation18" src="assets/8302e115-092f-45d0-b4be-8689554c67a9.png"/></p>
<p class="calibre2">Various techniques can be used to build a linear regression model. The most frequently used is the <strong class="calibre4">ordinary least square</strong> (<strong class="calibre4">OLS</strong>) estimate. The OLS method is used to produce a linear regression line that seeks to minimize the sum of the squared error. <span class="calibre5">The error </span><span class="calibre5">is the distance from an actual data point to the regression line. The sum of the squared error measures the aggregate of the squared difference between the training instances, which are each of our data points, and the values predicted by the regression line. This can be represented as follows:</span></p>
<p class="CDPAlignCenter"><img class="fm-editor-equation19" src="assets/20b55abb-3b52-41db-8c2e-e1d7b36c8d0c.png"/></p>
<p class="calibre2"><span class="calibre5"><span class="calibre5">In the preceding equation, </span></span><img class="fm-editor-equation13" src="assets/bcc37182-bc5c-4ce3-9f45-8e124cd79d81.png"/> is the actual training instance and <img class="fm-editor-equation20" src="assets/5d58def7-dceb-4c83-8add-15c9237d55b0.png"/> is the value predicted by the regression line. </p>
<p class="calibre2">In the context of machine learning, gradient descent is a common technique that can be used to optimize the coefficients of predictor variables by minimizing the training error of the model through multiple iterations. <span class="calibre5">Gradient descent starts by initializing the coefficients to zero. Then, the coefficients are updated with the intention of minimizing the error. Updating the coefficients is an iterative process and is performed until a minimum squared error is achieved. </span></p>
<p class="calibre2">In the gradient descent technique, a hyperparameter called the <strong class="calibre4">learning rate</strong>, denoted<br class="calibre6"/>
by <img class="fm-editor-equation21" src="assets/6ae71158-dd81-4087-b5ce-5d8e76ce02c4.png"/> is provided to the algorithm. <span class="calibre5">This parameter determines how fast the algorithm moves toward the optimal value of the coefficients. If <img class="fm-editor-equation21" src="assets/556b7d9a-e0a6-4d3f-af81-cda64717e7ed.png"/> is very large, the algorithm might skip the optimal solution. If it is too small, however, the algorithm might have too many iterations to converge to the optimum coefficient values. For this reason, it is important to use the right value for <img class="fm-editor-equation21" src="assets/f2895db5-0f99-4a3e-801f-5b9594ecf991.png"/>.</span></p>
<p class="calibre2">In this recipe, we will use the gradient descent method to train our linear regression model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <a href="2dbc8735-7eef-42ba-8b19-5644632c3197.xhtml" class="calibre9">Chapter 1</a>, <em class="calibre13">Get Closer To Your Data</em>, we took the <kbd class="calibre12">HousePrices.csv</kbd> file and looked at how to manipulate and prepare our data. We also analyzed and treated the missing values in the dataset. We will now use this final dataset for our model-building exercise, using linear regression:</p>
<p class="calibre2">In the following code block, we will start by importing the required libraries:</p>
<pre class="calibre18"># import os for operating system dependent functionalities<br class="title-page-name"/>import os<br class="title-page-name"/><br class="title-page-name"/># import other required libraries<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import seaborn as sns <br class="title-page-name"/>import matplotlib.pyplot as plt</pre>
<p class="calibre2">We set our working directory with the <kbd class="calibre12">os.chdir()</kbd> command:</p>
<pre class="calibre18"># Set your working directory according to your requirement<br class="title-page-name"/>os.chdir(".../Chapter 4/Linear Regression")<br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2">Let's read our data. We prefix the DataFrame name with <kbd class="calibre12">df_</kbd> so that we can understand it easily:</p>
<pre class="calibre18">df_housingdata = pd.read_csv("Final_HousePrices.csv")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">Let's move on to building our model. </span>We will start by identifying our numerical and categorical variables. We study the correlations using the correlation matrix and the correlation plots.</p>
<ol class="calibre14">
<li class="calibre11">First, we'll take a look at the variables and the variable types:</li>
</ol>
<pre class="calibre18"># See the variables and their data types<br class="title-page-name"/>df_housingdata.dtypes</pre>
<ol start="2" class="calibre14">
<li class="calibre11">We'll then look at the correlation matrix. The <kbd class="calibre12">corr()</kbd> method <span>computes the pairwise correlation of columns:</span></li>
</ol>
<pre class="calibre18"># We pass 'pearson' as the method for calculating our correlation<br class="title-page-name"/>df_housingdata.corr(method='pearson')</pre>
<ol start="3" class="calibre14">
<li class="calibre11">Besides this, we'd also like to study the correlation between the predictor variables and the response variable:</li>
</ol>
<pre class="calibre18"># we store the correlation matrix output in a variable<br class="title-page-name"/>pearson = df_housingdata.corr(method='pearson')<br class="title-page-name"/><br class="title-page-name"/># assume target attr is the last, then remove corr with itself<br class="title-page-name"/>corr_with_target = pearson.iloc[-1][:-1]<br class="title-page-name"/><br class="title-page-name"/># attributes sorted from the most predictive<br class="title-page-name"/>corr_with_target.sort_values(ascending=False)</pre>
<div class="packttip">We may also want to sort our correlation by absolute values. In order to do this, we can use the following command: <kbd class="calibre19">corr_with_target[abs(corr_with_target).argsort()[::-1]]</kbd></div>
<ol start="4" class="calibre14">
<li class="calibre11">We can look at the correlation plot using the <kbd class="calibre12">heatmap()</kbd> function from the <kbd class="calibre12">seaborn</kbd> package:</li>
</ol>
<pre class="calibre18">f, ax = plt.subplots(figsize=(11, 11))<br class="title-page-name"/><br class="title-page-name"/># Generate a mask for the upper triangle<br class="title-page-name"/># np.zeros_like - Return an array of zeros with the same shape and type as a given array<br class="title-page-name"/># In this case we pass the correlation matrix<br class="title-page-name"/># we create a variable “mask” which is a 14 X 14 numpy array<br class="title-page-name"/><br class="title-page-name"/>mask = np.zeros_like(pearson, dtype=np.bool)<br class="title-page-name"/>tt = np.triu_indices_from(mask)<br class="title-page-name"/><br class="title-page-name"/># We create a tuple with triu_indices_from() by passing the “mask” array<br class="title-page-name"/># k is used to offset diagonal<br class="title-page-name"/># with k=0, we offset all diagnoals<br class="title-page-name"/># If we put k=13, means we offset 14-13=1 diagonal<br class="title-page-name"/><br class="title-page-name"/># triu_indices_from() Return the indices for the upper-triangle of arr.<br class="title-page-name"/>mask[np.triu_indices_from(mask, k=0)] = True<br class="title-page-name"/><br class="title-page-name"/># First 2 param - anchor hues for negative and positive extents of the map.<br class="title-page-name"/># 3rd param - Anchor saturation for both extents of the map<br class="title-page-name"/># If true, return a matplotlib colormap object rather than a list of colors.<br class="title-page-name"/><br class="title-page-name"/>cmap = sns.diverging_palette(10, 129, s=50, as_cmap=True)<br class="title-page-name"/><br class="title-page-name"/># Adjust size of the legend bar with cbar_kws={“shrink”: 0.5}<br class="title-page-name"/># cmap=“YlGnBu” gives the color from Yellow-Green-Blue palette<br class="title-page-name"/><br class="title-page-name"/>sns.heatmap(pearson, mask=mask, cmap="YlGnBu", vmax=.3, center=0,<br class="title-page-name"/>           square=True, linewidths=.1, cbar_kws={"shrink": 0.5})</pre>
<p class="calibre20">The <span class="calibre5">following screenshot is the correlation plot</span>. Note that we have removed the upper triangle of the heatmap using the <kbd class="calibre12">np.zeros_like()</kbd> and <kbd class="calibre12">np.triu_indices_from()</kbd> functions:</p>
<p class="CDPAlignCenter"><img class="aligncenter39" src="assets/6b5aa08a-231d-4df4-b201-8b520478b0ee.png"/></p>
<p class="calibre20">Let's explore our data by visualizing other variables.</p>
<ol start="5" class="calibre14">
<li class="calibre11">We can look at the distribution of our target variable, <kbd class="calibre12">SalePrice</kbd>, using a histogram with a kernel density estimator as follows:</li>
</ol>
<pre class="calibre18"># Setting the plot size<br class="title-page-name"/>plt.figure(figsize=(8, 8))<br class="title-page-name"/><br class="title-page-name"/>sns.distplot(df_housingdata['SalePrice'], bins=50, kde=True)</pre>
<p class="calibre20">The following screenshot gives us the <span class="calibre5">distribution plot </span><span class="calibre5">for the </span><kbd class="calibre12">SalePrice</kbd> <span class="calibre5">variable:</span></p>
<p class="CDPAlignCenter"><img class="aligncenter40" src="assets/e333a93b-7cdc-4307-955a-02f3ee7132e4.png"/></p>
<div class="packtinfobox">In statistics, <strong class="calibre1">kernel density estimation</strong> (<strong class="calibre1">KDE</strong>) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are based on a finite data sample. KDE is a technique that provides you with a smooth curve given a set of data. It can be handy if you want to visualize the shape of some data, as a kind of continuous replacement for the discrete values plotted in a histogram.</div>
<ol start="6" class="calibre14">
<li class="calibre11">We can also use <kbd class="calibre12">JointGrid()</kbd> from our <kbd class="calibre12">seaborn</kbd> package to plot a combination of plots:</li>
</ol>
<pre class="calibre18">from scipy import stats<br class="title-page-name"/>g = sns.JointGrid(df_housingdata['YearBuilt'], df_housingdata['SalePrice'])<br class="title-page-name"/>g = g.plot(sns.regplot, sns.distplot)<br class="title-page-name"/>g = g.annotate(stats.pearsonr)</pre>
<p class="calibre20">With the preceding code, we are able to plot the scatter plot for <span class="calibre5">GarageArea</span> and <span class="calibre5">SalePrice</span>, while also plotting the histogram for each of these variables on each axis:</p>
<p class="CDPAlignCenter"><img class="aligncenter41" src="assets/d410aa20-1b45-465d-83a8-bcddeded8f74.png"/></p>
<ol start="7" class="calibre14">
<li class="calibre11">Let's now scale our numeric variables using min-max normalization. To do this, we first need to select only the numeric variables from our dataset:</li>
</ol>
<pre class="calibre18"># create a variable to hold the names of the data types viz int16, in32 and so on<br class="title-page-name"/>num_cols = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']<br class="title-page-name"/><br class="title-page-name"/># Filter out variables with numeric data types<br class="title-page-name"/>df_numcols_only = df_housingdata.select_dtypes(include=num_cols)</pre>
<ol start="8" class="calibre14">
<li class="calibre11">We will now apply the min-max scaling to our numeric variables:</li>
</ol>
<pre class="calibre18"># Importing MinMaxScaler and initializing it<br class="title-page-name"/>from sklearn.preprocessing import MinMaxScaler<br class="title-page-name"/>min_max=MinMaxScaler()<br class="title-page-name"/><br class="title-page-name"/># Scaling down the numeric variables<br class="title-page-name"/># We exclude SalePrice using iloc() on df_numcols_only DataFrame<br class="title-page-name"/>df_housingdata_numcols=pd.DataFrame(min_max.fit_transform(df_numcols_only.iloc[:,0:36]), columns=df_numcols_only.iloc[:,0:36].columns.tolist())</pre>
<p class="calibre20">In the following table, we can see that our numeric variables have been scaled down:</p>
<p class="CDPAlignCenter"><img class="aligncenter42" src="assets/c2ba2525-15ca-4bca-8972-48bff32dddfe.png"/></p>
<ol start="9" class="calibre14">
<li class="calibre11">Now, we will perform one-hot encoding on our categorical variables:</li>
</ol>
<pre class="calibre18"># We exclude all numeric columns<br class="title-page-name"/>df_housingdata_catcol = df_housingdata.select_dtypes(exclude=num_cols)<br class="title-page-name"/><br class="title-page-name"/># Steps to one-hot encoding:<br class="title-page-name"/># We iterate through each categorical column name<br class="title-page-name"/># Create encoded variables for each categorical columns<br class="title-page-name"/># Concatenate the encoded variables to the DataFrame<br class="title-page-name"/># Remove the original categorical variable<br class="title-page-name"/>for col in df_housingdata_catcol.columns.values:<br class="title-page-name"/>   one_hot_encoded_variables = pd.get_dummies(df_housingdata_catcol[col],prefix=col)<br class="title-page-name"/>   df_housingdata_catcol = pd.concat([df_housingdata_catcol,one_hot_encoded_variables],axis=1)<br class="title-page-name"/>   df_housingdata_catcol.drop([col],axis=1, inplace=True)</pre>
<ol start="10" class="calibre14">
<li class="calibre11">We have now created a DataFrame with only numeric variables that have been scaled. We have also created a <span>DataFrame </span>with only categorical variables that have been encoded. Let's combine the two <span>DataFrame</span>s into a single <span>DataFrame</span>:</li>
</ol>
<pre class="calibre18">df_housedata = pd.concat([df_housingdata_numcols, df_housingdata_catcol], axis=1)</pre>
<ol start="11" class="calibre14">
<li class="calibre11">We can then concatenate the <kbd class="calibre12">SalePrice</kbd> variable to our <kbd class="calibre12">df_housedata</kbd> <span>DataFrame</span>:</li>
</ol>
<pre class="calibre18"># Concatenate SalePrice to the final DataFrame<br class="title-page-name"/>df_housedata_final = pd.concat([df_housedata, df_numcols_only.iloc[:,36]], axis=1)</pre>
<ol start="12" class="calibre14">
<li class="calibre11">We can create our training and testing datasets using the <kbd class="calibre12">train_test_split</kbd> class from <kbd class="calibre12">sklearn.model_selection</kbd>:</li>
</ol>
<pre class="calibre18"># Create feature and response variable set<br class="title-page-name"/># We create train &amp; test sample from our dataset<br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/><br class="title-page-name"/># create feature &amp; response variables<br class="title-page-name"/>X = df_housedata_final.iloc[:,0:302]<br class="title-page-name"/>Y = df_housedata_final['SalePrice']<br class="title-page-name"/><br class="title-page-name"/># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = \<br class="title-page-name"/>train_test_split(X, Y, test_size=0.30, random_state=1)</pre>
<ol start="13" class="calibre14">
<li class="calibre11">We can now use <kbd class="calibre12">SGDRegressor()</kbd> to build a linear model. We fit this linear model by minimizing the regularized empirical loss with SGD:</li>
</ol>
<pre class="calibre18">import numpy as np<br class="title-page-name"/>from sklearn.linear_model import SGDRegressor<br class="title-page-name"/><br class="title-page-name"/>lin_model = SGDRegressor()<br class="title-page-name"/><br class="title-page-name"/># We fit our model with train data<br class="title-page-name"/>lin_model.fit(X_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/># We use predict() to predict our values<br class="title-page-name"/>lin_model_predictions = lin_model.predict(X_test)<br class="title-page-name"/><br class="title-page-name"/># We check the coefficient of determination with score()<br class="title-page-name"/>print(lin_model.score(X_test, Y_test))<br class="title-page-name"/><br class="title-page-name"/># We can also check the coefficient of determination with r2_score() from sklearn.metrics<br class="title-page-name"/>from sklearn.metrics import r2_score<br class="title-page-name"/>print(r2_score(Y_test, lin_model_predictions))</pre>
<p class="calibre20">By running the preceding code, we find out that the coefficient of determination is roughly 0.81.</p>
<div class="packttip">Note that <kbd class="calibre19">r2_score()</kbd> takes two arguments. <span>The first argument should be the true values, not the predicted values, otherwise, it would return an incorrect result.</span></div>
<ol start="14" class="calibre14">
<li class="calibre11">We check the <strong class="calibre1">root mean square error</strong> (<strong class="calibre1">RMSE</strong>) on the test data:</li>
</ol>
<pre class="calibre18">from sklearn.metrics import mean_squared_error<br class="title-page-name"/>mse = mean_squared_error(Y_test, lin_model_predictions)<br class="title-page-name"/>rmse = np.sqrt(mse)<br class="title-page-name"/>print(rmse)</pre>
<p class="calibre20">Running the preceding code provides output to the effect that the RMSE equals 36459.44.</p>
<ol start="15" class="calibre14">
<li class="calibre11">We now plot the actual and predicted values using <kbd class="calibre12">matplotlib.pyplot</kbd>:</li>
</ol>
<pre class="calibre18">plt.figure(figsize=(8, 8))<br class="title-page-name"/>plt.scatter(Y_test, lin_model_predictions)<br class="title-page-name"/>plt.xlabel('Actual Median value of house prices ($1000s)')<br class="title-page-name"/>plt.ylabel('Predicted Median value of house prices ($1000s)')<br class="title-page-name"/>plt.tight_layout()</pre>
<p class="calibre2">The resulting plot with our actual values and the predicted values will look as follows:</p>
<p class="CDPAlignCenter"><img class="aligncenter43" src="assets/11ce1f99-0212-467f-aa91-f014135dccc1.png"/></p>
<p class="calibre2">Because the chart shows most values in approximately a 45-degree diagonal line, our predicted values are quite close to the actual values, apart from a few.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">In <em class="calibre13">Step 1</em>, we looked at</span> the variable types. We saw that the dataset had both numeric and non-numeric variables. In <em class="calibre13">Step 2</em>, we used the <kbd class="calibre12">Pearson</kbd> method to calculate the pairwise correlation among all the numeric variables. After that, in <em class="calibre13">Step 3</em>, <span class="calibre5">we saw how all of the predictor variables are related to the target variable. </span>We also looked at how to sort correlation coefficients by their absolute values. </p>
<p class="calibre2">In <em class="calibre13">Step 4</em>, we painted a heatmap to visualize the correlation between the variables. Then, we introduced two functions from the NumPy library: <span class="calibre5"><kbd class="calibre12">zeros_like()</kbd> and </span><span class="calibre5"><kbd class="calibre12">triu_indices_from()</kbd>. The <kbd class="calibre12">zeros_like()</kbd> function takes the correlation matrix as an input and returns an array of zeros with the same shape and type as the given array. <kbd class="calibre12">triu_indices_from()</kbd> returns the indices for the upper triangle of the array. We used these two functions to mask the upper triangular part of the correlation plot. We called the <kbd class="calibre12">heatmap()</kbd> function from the <kbd class="calibre12">seaborn</kbd> library to paint a correlation heat map and passed our correlation matrix to it. We also set the color of the matrix using <kbd class="calibre12">cmap="YlGnBu"</kbd> and the size of the legend bar using </span><span class="calibre5"><kbd class="calibre12">cbar_kws={"shrink": 0.5}</kbd>.</span></p>
<div class="packtinfobox"><kbd class="calibre19">numpy.tril_indices_from()</kbd> <span>returns the indices for the lower triangle of the array. </span></div>
<p class="calibre2">In <em class="calibre13">Step 5</em>, we looked at the distribution of the target variable,<span class="calibre5"> </span><kbd class="calibre12">SalePrice</kbd>. In <em class="calibre13">Step 6</em>, we used<span class="calibre5"> </span><kbd class="calibre12">JointGrid()</kbd><span class="calibre5"> </span>from the <kbd class="calibre12">seaborn</kbd> library to show how it is possible to plot a scatter plot for two numeric variables with a regression line, along with plotting the distribution of both variables on the axis in the same chart. In <em class="calibre13">Steps 7</em> and <em class="calibre13">8</em>, we selected only the numeric variables and scaled the variables using min-max normalization. <span class="calibre5">This scales the values to a numeric range of data between 0 and 1. This is also called feature scaling, and is performed using the following formula:</span></p>
<p class="CDPAlignCenter"><img class="fm-editor-equation22" src="assets/b3749017-4989-4796-a144-a72591fca108.png"/></p>
<p class="calibre2">In <em class="calibre13">Step 9</em>, <em class="calibre13">Step<span class="calibre5"> </span></em><em class="calibre13">10</em>, and <em class="calibre13">Step<span class="calibre5"> </span></em><em class="calibre13">11</em>, we performed one-hot encoding on the categorical variables and added the encoded variables to the <span class="calibre5">DataFrame</span>. We also dropped the original categorical variables. In <em class="calibre13">Step 12</em>, we split our dataset into a training set and a testing set. In <em class="calibre13">Step 13</em>, we built our linear regression model using <kbd class="calibre12">SGDRegressor()</kbd> and printed the coefficient of determination. Finally, in <em class="calibre13">Step 14</em>, we plotted the predicted and actual values to see how well our model performed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2">Consider a linear regression model, given the following hypothesis function:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation23" src="assets/7a6e344d-3744-49ef-8abb-d5b3b90d5c73.png"/></p>
<p class="calibre2">In this case, the cost function for <img class="fm-editor-equation16" src="assets/dbdd5107-ab3c-4439-9eb4-d37c9f398178.png"/> is the <strong class="calibre4">mean squared error</strong> (<strong class="calibre4">MSE</strong>).</p>
<p class="calibre2">The formula is as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation24" src="assets/3437f7b5-298e-43d2-aadb-27b6cbd92af3.png"/></p>
<p class="calibre2"><span class="calibre5"><span class="calibre5">In this formula, </span></span><img class="fm-editor-equation25" src="assets/330bdebb-32ae-4643-bb20-9c1536e89174.png"/> represents the number of training instances. <img class="fm-editor-equation26" src="assets/cdaa24f4-3130-4e53-9433-6ed55b2a2c5e.png"/> and <img class="fm-editor-equation27" src="assets/6bbb6455-36e9-4530-8abd-4cf149a36eb3.png"/> are the input vector and the target vector for the i<sup class="calibre30">th</sup> training instance respectively, while <img class="fm-editor-equation16" src="assets/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png"/> represents the parameters or coefficients for each input variable. <img class="fm-editor-equation28" src="assets/fde84455-2d8b-45cf-91cf-7e050c0b2754.png"/> is the predicted value for the i<sup class="calibre30">th</sup> training instance using the <span class="calibre5"> </span><img class="fm-editor-equation29" src="assets/8a9e74ee-919c-441b-a52a-8fdf77b8d324.png"/> <span class="calibre5">parameters. The </span><span class="calibre5">MSE is always non-negative and the closer it gets to zero, the better.</span></p>
<p class="calibre2">The MSE is higher when the model performs poorly on the training data. The objective of the learning algorithm, therefore, is to find the value of <strong class="calibre4"><img class="fm-editor-equation16" src="assets/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png"/> </strong>such that the MSE is minimized. This can be represented as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation30" src="assets/2ce4ff82-022a-4132-853b-1f5ed9d6bc39.png"/></p>
<p class="calibre2">The stochastic gradient descent method finds the values of <img class="fm-editor-equation16" src="assets/8a9e74ee-919c-441b-a52a-8fdf77b8d324.png"/> that minimize the cost function. In order to minimize the cost function, it keeps changing the <span class="calibre5"><img class="fm-editor-equation16" src="assets/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png"/></span><span class="calibre5"> </span><span class="calibre5">parameters </span><span class="calibre5">by calculating the slope of the derivative of the cost function. It starts by initializing the</span> <strong class="calibre4"><img class="fm-editor-equation16" src="assets/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png"/> </strong><span class="calibre5">parameters </span><span class="calibre5">to zero. </span>The <strong class="calibre4"><img class="fm-editor-equation16" src="assets/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png"/> </strong><span class="calibre5">para</span><span class="calibre5">meters </span>are updated at each step of the gradient descent:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation31" src="assets/0922d000-fa2c-4c1e-b2c0-ce4a5835d302.png"/></p>
<div class="packtinfobox">The number of updates required for the algorithm to converge will increase with the increase in the training data. However, as the training data gets larger and larger, it is quite possible for the algorithm to converge much before every instance in the training data is learnt. In other words, the increase in the training data size need not increase the training time needed to train the best possible model where the test error is at its least.</div>
<p class="CDPAlignLeft3"><span class="calibre5">Every training instance will modify <strong class="calibre4"><img class="fm-editor-equation16" src="assets/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png"/></strong>. The algorithm averages these <strong class="calibre4"><img class="fm-editor-equation16" src="assets/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png"/> </strong>values to calculate the final <strong class="calibre4"><img class="fm-editor-equation16" src="assets/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png"/>.</strong></span></p>
<p class="CDPAlignLeft3"><img class="fm-editor-equation21" src="assets/bf8d8412-f33e-49a7-a87f-4bd993db62d7.png"/> is the learning rate, which tells the algorithm <span class="calibre5">how rapidly to move toward the minimum. A large <img class="fm-editor-equation21" src="assets/ad3709fe-d549-4f9c-8f87-c34446880063.png"/> might miss the minimum error, while a small <img class="fm-editor-equation21" src="assets/374cefed-6b08-4a0c-966c-6c701c4ee21a.png"/> might take a longer time for the algorithm to run.</span></p>
<p class="calibre2">In the preceding section, we used a <kbd class="calibre12">SGDRegressor()</kbd> function, but we opted for the default values of the hyperparameters. We are now going to change <img class="fm-editor-equation21" src="assets/374cefed-6b08-4a0c-966c-6c701c4ee21a.png"/> to 0.0000001 and the <kbd class="calibre12">max_iter</kbd> value to 2000:</p>
<pre class="calibre15">lin_model = SGDRegressor(alpha=0.0000001, max_iter=2000)</pre>
<div class="packtinfobox"><kbd class="calibre19">max_iter</kbd> is an integer value that tells the algorithm the maximum number of passes it can make over the training data. This is also known as the number of epochs.</div>
<p class="calibre2">In our case, the preceding code gives the result that the RMSE drops from 36,459 to 31,222 and the coefficient of determination improved from 0.81 to 0.86. These results will vary for every iteration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">The scikit-learn documentation on regression metrics: <a href="https://bit.ly/2D6Wn8s" class="calibre9">https://bit.ly/2D6Wn8s</a></li>
<li class="calibre11">The scikit-learn guide to density estimation: <a href="https://bit.ly/2RlnlMj" class="calibre9">https://bit.ly/2RlnlMj</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous section, we noted that linear regression is a good choice when the target variable is continuous. We're now going to move on to look at a binomial logistic regression model, which can predict the probability that an observation falls into one of two categories of a dichotomous target variable based on one or more predictor variables. A binomial logistic regression is often referred to as logistic regression.</p>
<p class="calibre2">Logistic regression is similar to linear regression, except that the dependent variable is measured on a dichotomous scale. Logistic regression allows us to model a relationship between multiple predictor variables and a dichotomous target variable. However, unlike linear regression, in the case of logistic regression, the linear function is used as an input to another function, such as <img class="fm-editor-equation12" src="assets/0abef8f3-a09d-48ce-bed0-de61f72b600d.png"/>:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation32" src="assets/c667dd69-61a9-421f-b5b8-f1f0d4b797e7.png"/></p>
<p class="calibre2">Here, <img class="fm-editor-equation12" src="assets/0abef8f3-a09d-48ce-bed0-de61f72b600d.png"/> is the sigmoid or logistic function. The sigmoid function is given as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation33" src="assets/9a62d322-fb87-4998-b8ae-ab4d8a51b87d.png"/></p>
<p class="calibre2">The following <span class="calibre5">graph</span><span class="calibre5"> represents a sigmoid curve in which the </span><span class="calibre5">values of the </span><span class="calibre5">y-axis lie between 0 and 1. It crosses the axis at 0.5. :</span></p>
<p class="CDPAlignCenter"><img class="aligncenter44" src="assets/309120d7-1ddd-42ee-a168-2a77cbdcc19f.png"/></p>
<p class="calibre2">The output, which lies between 0 and 1, is the probability of the positive class. We can interpret the output of our hypothesis function as positive if the value returned is <img class="fm-editor-equation34" src="assets/5ca126ce-5e50-4933-a3e3-d96ecc68495f.png"/> 0.5. Otherwise, we interpret it as negative. </p>
<p class="calibre2">In the case of logistic regression, we use a cost function known as cross-entropy. This takes the following form for binary classification:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation35" src="assets/d56c0b91-cce1-442c-a47c-855294fd4253.png"/></p>
<p class="calibre2">For <em class="calibre13">y=1</em> and <em class="calibre13">y=0</em>, we get the following results:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation36" src="assets/cf567852-7a51-4677-b73c-589245f03199.png"/></p>
<p class="calibre2"><span class="calibre5">Cross-entropy increases as the predicted probability diverges from the actual label. A higher divergence results in a higher cross-entropy value. </span>In the case of linear regression, we saw that <span class="calibre5">we can minimize the cost </span><span class="calibre5">using gradient descent. In the case of logistic regression, we can also use gradient descent to update the coefficients and minimize the cost function. </span></p>
<p class="calibre2">In this recipe, we will use the <kbd class="calibre12">SGDClassfier()</kbd> implementation of scikit-learn. <span class="calibre5"><kbd class="calibre12">SGDClassifier()</kbd> implements regularized linear models with stochastic gradient descent, which, for large datasets, is much faster than gradient descent. This is because gradient descent considers the whole training dataset, while stochastic gradient descent </span><span class="calibre5">only</span><span class="calibre5"> </span><span class="calibre5">considers one random point while updating the weights.</span></p>
<div class="packtinfobox">By default, <kbd class="calibre19">SGDClassifier</kbd> might not perform as well as logistic regression. It is likely to require hyperparameter tuning.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we're going to use a dataset that contains information on default payments, demographics, credit data, <span class="calibre5">payment</span> history, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. This dataset is taken from the UCI ML repository and is available at GitHub:</p>
<p class="calibre2">We will start by importing the required libraries:</p>
<pre class="calibre18"># import os for operating system dependent functionalities<br class="title-page-name"/>import os<br class="title-page-name"/><br class="title-page-name"/># import other required libraries<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>from sklearn.preprocessing import StandardScaler<br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/>from sklearn.linear_model import SGDClassifier<br class="title-page-name"/>from sklearn.metrics import roc_curve<br class="title-page-name"/>from sklearn.metrics import auc<br class="title-page-name"/>import matplotlib.pyplot as plt</pre>
<p class="calibre2">We set our working directory with the <kbd class="calibre12">os.chdir()</kbd> command:</p>
<pre class="calibre18"># Set your working directory according to your requirement<br class="title-page-name"/>os.chdir(".../Chapter 4/Logistic Regression")<br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2">Let's read our data. We will prefix the name of the DataFrame with <kbd class="calibre12">df_</kbd> to make it easier to read:</p>
<pre class="calibre18">df_creditdata = pd.read_csv("UCI_Credit_Card.csv")</pre>
<p class="calibre2">We will now move on to look at building our model using <kbd class="calibre12">SGDClassifier()</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's start by looking at the variables and data types:</p>
<ol class="calibre14">
<li class="calibre11">First, we're going to take a look at our dataset using the <kbd class="calibre12">read_csv()</kbd> function:</li>
</ol>
<pre class="calibre18">print(df_creditdata.shape)<br class="title-page-name"/>print(df_creditdata.head())</pre>
<ol start="2" class="calibre14">
<li class="calibre11">We will take a look at the datatypes using <kbd class="calibre12">dtypes</kbd>:</li>
</ol>
<pre class="calibre18">df_creditdata.dtypes</pre>
<ol start="3" class="calibre14">
<li class="calibre11">We will drop the ID column as we do not need this here:</li>
</ol>
<pre class="calibre18">df_creditdata.drop(["ID"],axis=1,inplace=True)</pre>
<ol start="4" class="calibre14">
<li class="calibre11">In the previous section, we saw how to explore correlations among the variables. We will skip this here, but readers are advised to check for correlation as multicollinearity might have an impact on the model.</li>
<li class="calibre11">However, we will check if there are any null values, as follows:</li>
</ol>
<pre class="calibre18">df_creditdata.isnull().sum()</pre>
<ol start="6" class="calibre14">
<li class="calibre11">We will then separate the predictor and response variables. We will also split our training and testing data:</li>
</ol>
<pre class="calibre18"># split features &amp; response variable<br class="title-page-name"/>X = df_creditdata.iloc[:,0:23]<br class="title-page-name"/>Y = df_creditdata['default.payment.next.month']<br class="title-page-name"/><br class="title-page-name"/># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=1)</pre>
<ol start="7" class="calibre14">
<li class="calibre11">We standardize our predictor variables using <kbd class="calibre12">StandardScaler()</kbd>:</li>
</ol>
<pre class="calibre18">scaler = StandardScaler().fit(X_train)<br class="title-page-name"/>X_train = scaler.transform(X_train)<br class="title-page-name"/>X_test = scaler.transform(X_test)</pre>
<ol start="8" class="calibre14">
<li class="calibre11">We then move our model using <kbd class="calibre12">SGDClassifier()</kbd>:</li>
</ol>
<pre class="calibre18"># We create an instance of SGDClassifier()<br class="title-page-name"/>logistic_model = SGDClassifier(alpha=0.000001, loss=‘log’, max_iter=100000, penalty=‘l2’)<br class="title-page-name"/><br class="title-page-name"/># We fit our model to the data<br class="title-page-name"/>fitted_model = logistic_model.fit(X_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/># We use predict_proba() to predict the probabilities<br class="title-page-name"/>predictedvalues = fitted_model.predict_proba(X_test)<br class="title-page-name"/><br class="title-page-name"/># We print the probabilities to take a glance<br class="title-page-name"/>predictedvalues</pre>
<ol start="9" class="calibre14">
<li class="calibre11">We separate out the probabilities of one class. In this case, we will look at class 1:</li>
</ol>
<pre class="calibre18"># We take the predicted values of class 1<br class="title-page-name"/>Y_predicted = predictedvalues[:, 1]<br class="title-page-name"/><br class="title-page-name"/># We check to see if the right values have been considered from the predicted values<br class="title-page-name"/>print(Y_predicted)</pre>
<ol start="10" class="calibre14">
<li class="calibre11">We check the accuracy of our model on the training data:</li>
</ol>
<pre class="calibre18"># Check for accuracy<br class="title-page-name"/>logistic_model.score(X_test,Y_test)</pre>
<ol start="11" class="calibre14">
<li class="calibre11">We can then see the <strong class="calibre1">area under curve</strong> (<strong class="calibre1">AUC</strong>) value of the <strong class="calibre1">receiver operating characteristic</strong> (<strong class="calibre1">ROC</strong>) curve:</li>
</ol>
<pre class="calibre18"># We use roc_curve() to generate fpr &amp; tpr values<br class="title-page-name"/>fpr, tpr, thresholds = roc_curve(Y_test, Y_predicted)<br class="title-page-name"/><br class="title-page-name"/># we pass the fpr &amp; tpr values to auc() to calculate the area under curve<br class="title-page-name"/>roc_auc = auc(fpr, tpr)<br class="title-page-name"/>print(roc_auc)</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="12" class="calibre14">
<li class="calibre11">We plot our ROC curve as follows:</li>
</ol>
<pre class="calibre18">plt.figure()<br class="title-page-name"/>plt.plot(fpr,tpr, color=‘orange’, lw=2, label=‘ROC curve (area under curve = %0.2f)’ % roc_auc)<br class="title-page-name"/>plt.plot([0, 1], [0, 1], color=‘darkgrey’, lw=2, linestyle=‘--’)<br class="title-page-name"/>plt.xlim([0.0, 1.0])<br class="title-page-name"/>plt.ylim([0.0, 1.0])<br class="title-page-name"/>plt.xlabel(‘False Positive Rate (1-Specificity)’)<br class="title-page-name"/>plt.ylabel(‘True Positive Rate (Sensitivity)’)<br class="title-page-name"/>plt.title(‘ROC Curve’)<br class="title-page-name"/>plt.legend(loc=“upper left”)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre20">The following graph shows the ROC curve with the AUC value annotated on it:</p>
<p class="CDPAlignCenter"><img class="aligncenter45" src="assets/1e8dc280-5e3d-4290-9b75-67691fa3be5e.png"/></p>
<p class="calibre2">The model can be improved by tuning the hyperparameters. It can also be improved through feature selection.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em>, we looked at the dimensions of our dataset. In <em class="calibre13">Step 2</em>, we took a glimpse at the datatypes of the variables and noticed that all our variables were numeric in nature. In <em class="calibre13">Step 3</em>, we dropped the ID column since it is of no use for our exercise. We skipped looking at the correlations between the variables, but it is recommended that the reader adds this step in order to fully understand and analyze the data.</p>
<p class="calibre2">In <em class="calibre13">Step 4</em>, we moved on to check whether we had any missing values in our dataset. We noticed that our dataset had no missing values in this case. In <em class="calibre13">Step 5</em>, we separated the predictor and response variable and also split our dataset into a training dataset, which was 70% of the data, and a testing dataset, which was 30% of the data. In <em class="calibre13">Step 6,</em> we used <kbd class="calibre12">StandardScaler()</kbd> from <kbd class="calibre12">sklearn.metrics</kbd> to standardize our predictor variables in both the training and testing datasets.</p>
<p class="calibre2">After that, in <em class="calibre13">Step 7</em>, we used <kbd class="calibre12">SGDClassifier()</kbd> from <kbd class="calibre12">sklearn.linear_model</kbd> to build our logistic regression model using the stochastic gradient descent method. We set our hyperparameters, such as alpha, loss, <kbd class="calibre12">max_iter</kbd>, and penalty. We set <kbd class="calibre12">loss='log'</kbd> in order to use the SGDClassifier for logistic regression. We used <kbd class="calibre12">predict_proba()</kbd> to predict the probabilities for our test observations, which provided us with the probabilities of both classes for all the test observations.</p>
<div class="packttip">With <kbd class="calibre19">loss</kbd> set to <kbd class="calibre19">hinge</kbd>, <kbd class="calibre19">SGDClassifier()</kbd> provides a linear SVM. (We will cover SVMs in the upcoming section). The loss can be set to other values, such as <kbd class="calibre19">squared_hinge</kbd>, which is the same as <kbd class="calibre19">hinge</kbd> but is quadratically penalized.</div>
<p class="calibre2">In <em class="calibre13">Steps 8</em> and <em class="calibre13">9</em>, we filtered out the probabilities for class 1 and looked at our model score. In <em class="calibre13">Steps 10</em> and <em class="calibre13">11</em>, we looked at the AUC value and <span class="calibre5">plotted our ROC curve. </span>We will explore more about hyperparameter tuning for each technique in upcoming sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">You might have noticed that, in <em class="calibre23">Step 7</em>, we used a hyperparameter penalty of <kbd class="calibre12">l2</kbd>. The penalty is the regularization term and <kbd class="calibre12">l2</kbd> is the default value. The hyperparameter penalty can also be set to <kbd class="calibre12">l1</kbd>; however, that may lead to a sparse solution, pushing most coefficients to zero. <span>More information about this topic can be found at the following link: <a href="https://bit.ly/2RjbSwM" class="calibre9">https://bit.ly/2RjbSwM</a></span></li>
<li class="calibre11">The scikit-learn guide to classification metrics: <a href="https://bit.ly/2NUJl2C" class="calibre9">https://bit.ly/2NUJl2C</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes</h1>
                </header>
            
            <article>
                
<p class="calibre2">The <strong class="calibre4">Naive Bayes algorithm</strong> is a probabilistic learning method. It is known as <strong class="calibre4">Naive</strong> because it assumes that all events in this word are independent, which is <span class="calibre5">actually</span><span class="calibre5"> quite </span><span class="calibre5">rare. However, in spite of this assumption, the Naive Bayesian algorithm has proven over time to provide great performance in terms of its prediction accuracy.</span></p>
<p class="calibre2">The Bayesian probability theory is based on the principle that the estimated likelihood of an event or a potential outcome should be based on the evidence at hand across multiple trials. Bayes’ theorem provides a way to calculate the probability of a given class, given some knowledge about prior observations.</p>
<p class="calibre2">This can be written as follows:</p>
<p class="CDPAlignCenter"><img src="assets/4c581d7c-8c29-4748-ba3b-99eb7954db06.png" class="calibre31"/></p>
<p class="calibre2">The different elements of this theorem can be explained as follows:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">p(class|observation)</strong>: This is the probability that the class holds given the observation.</li>
<li class="calibre11"><strong class="calibre1">P(observation)</strong>: This is the prior probability that the training data is observed.</li>
<li class="calibre11"><strong class="calibre1">p(class)</strong>: This is the prior probability of the class.</li>
<li class="calibre11"><strong class="calibre1">p(observation|class)</strong>: This is the probability of the observations given that the class holds.</li>
</ul>
<p class="calibre2">In other words, if <em class="calibre13">H</em> is the space for the possible hypothesis, the most probable hypothesis, class<img class="fm-editor-equation37" src="assets/0a15aa65-15d1-4624-9cfb-78a865c25282.png"/>H, is the one that maximizes <em class="calibre13">p(class|observation)</em>.</p>
<p class="calibre2">Given a new observation with attributes<img class="fm-editor-equation38" src="assets/a99e07ac-c800-4c4d-b285-7a4dabea94b8.png"/>, the Bayes algorithm classifies it as the most probable value:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation39" src="assets/773ce01b-85c7-4960-93eb-a4bbfbde8b68.png"/></p>
<p class="calibre2">Given the conditional independence assumption, we have the following:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation40" src="assets/0396de06-03a5-4e98-8aea-570307939933.png"/></p>
<p class="calibre2"/>
<p class="calibre2">The prediction of the Naive Bayesian Classifier is as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation41" src="assets/479f2a2b-86d2-4cb3-9d50-2bae310f93de.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">A Naive Bayes classifier is one of the most basic algorithms that can be applied in text classification problems.</p>
<p class="calibre2">In this recipe, we will use the <kbd class="calibre12">spam.csv</kbd> file, which can be downloaded from the GitHub.</p>
<p class="calibre2">This <kbd class="calibre12">spam.csv</kbd> dataset has two columns. One column holds messages and the other column holds the message <span class="calibre5">type</span>, which states whether it is a spam message or a ham message. We will apply the Naive Bayes technique to predict whether a message is likely to be spam or ham.</p>
<p class="calibre2">We will start by importing the required libraries:</p>
<pre class="calibre18"># import os for operating system dependent functionalities<br class="title-page-name"/>import os<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>import seaborn as sns<br class="title-page-name"/><br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/>from sklearn.feature_extraction.text import CountVectorizer<br class="title-page-name"/>from sklearn.naive_bayes import MultinomialNB</pre>
<p class="calibre2">We set your working directory with the <kbd class="calibre12">os.chdir()</kbd> command:</p>
<pre class="calibre18">os.chdir(".../Chapter 4/Naive Bayes")<br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2">Let's read our data. As we did in the previous sections, we will prefix the name of the DataFrame with <kbd class="calibre12">df_</kbd> so that we can read it easily:</p>
<pre class="calibre18">df_messages = pd.read_csv('spam.csv', encoding='latin-1', \<br class="title-page-name"/>                          sep=',', names=['labels','message'])</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">Let's now move on to look at how to build our model.</span></p>
<ol class="calibre14">
<li class="calibre11">After reading the data, we use the <kbd class="calibre12">head()</kbd> function to take a look it:</li>
</ol>
<pre class="calibre18"> df_messages.head(3)</pre>
<p class="calibre20"><span class="calibre5">In the following screenshot, we can see that there are two columns: <span class="calibre5">labels</span> and <span class="calibre5">message</span>. </span>The output is as follows:</p>
<p class="CDPAlignCenter"><img class="aligncenter46" src="assets/bb8ebbd7-0a60-4695-b9e9-c45f8d9c1e33.png"/></p>
<ol start="2" class="calibre14">
<li class="calibre11">We then use the <kbd class="calibre12">describe()</kbd> function to look at a few metrics in each of the columns:</li>
</ol>
<pre class="calibre18">df_messages.describe()</pre>
<p class="calibre20">This gives us the following metrics:</p>
<p class="CDPAlignCenter"><img class="aligncenter47" src="assets/da9a2829-efe3-493d-931c-789a261bb4e4.png"/></p>
<div class="packtinfobox">For the object datatype, the result of <kbd class="calibre19">describe()</kbd> will provide <kbd class="calibre19">metrics</kbd>, <kbd class="calibre19">count</kbd>, <kbd class="calibre19">unique</kbd>, <kbd class="calibre19">top</kbd>, and <kbd class="calibre19">freq</kbd>. <kbd class="calibre19">top</kbd> refers to the most common value, while <kbd class="calibre19">freq</kbd> is the frequency of this value.</div>
<ol start="3" class="calibre14">
<li class="calibre11">We can also take a look at the metrics by message type, as follows:</li>
</ol>
<pre class="calibre18">df_messages.groupby('labels').describe()</pre>
<p class="calibre20">With the preceding command, we see the count, number of unique values, and frequency for each class of the target variable:</p>
<p class="CDPAlignCenter"><img class="aligncenter48" src="assets/70ed29ff-2391-4a27-9fa3-5787c278c298.png"/></p>
<ol start="4" class="calibre14">
<li class="calibre11">To analyze our dataset even further, let's take a look at the word count and the character count for each message:</li>
</ol>
<pre class="calibre18">df_messages['word_count'] = df_messages['message'].apply(lambda x: len(str(x).split(" ")))<br class="title-page-name"/>df_messages['character_count'] = df_messages['message'].str.len() <br class="title-page-name"/><br class="title-page-name"/>df_messages[['message','word_count', 'character_count']].head()</pre>
<div class="packtinfobox">The <kbd class="calibre19">lambda</kbd> function is used to create small, anonymous functions in Python. A <kbd class="calibre19">lambda</kbd> function can take any number of arguments, but can only <span>have one expression. This function is passed as a parameter to other functions, such as <kbd class="calibre19">map</kbd>, <kbd class="calibre19">apply</kbd>, <kbd class="calibre19">reduce</kbd>, or <kbd class="calibre19">filter</kbd>.</span></div>
<p class="calibre20">The output of the preceding code will look as follows:</p>
<p class="CDPAlignCenter"><img class="aligncenter49" src="assets/c4816fd7-3a96-4529-9c72-baaa7691844d.png"/></p>
<ol start="5" class="calibre14">
<li class="calibre11"><span>In this case, </span><kbd class="calibre12">labels</kbd><span> is our target variable. We have two classes: <kbd class="calibre12">spam</kbd> and <kbd class="calibre12">ham</kbd>. </span>We can see the distribution of spam and ham messages using a bar plot:</li>
</ol>
<pre class="calibre18">labels_count = pd.DataFrame(df_messages.groupby('labels')['message'].count())<br class="title-page-name"/>labels_count.reset_index(inplace = True)<br class="title-page-name"/>plt.figure(figsize=(4,4))<br class="title-page-name"/>sns.barplot(labels_count['labels'], labels_count['message'])<br class="title-page-name"/>plt.ylabel('Frequency', fontsize=12)<br class="title-page-name"/>plt.xlabel('Labels', fontsize=12)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre20">The following is the output of the preceding code:</p>
<p class="CDPAlignCenter"><img class="aligncenter50" src="assets/05b69426-36e2-4535-8c53-5c6ffc1df48f.png"/></p>
<ol start="6" class="calibre14">
<li class="calibre11">In the following code block, we will label <kbd class="calibre12">spam</kbd> as <kbd class="calibre12">1</kbd>, and <kbd class="calibre12">ham</kbd> as <kbd class="calibre12">0</kbd>:</li>
</ol>
<pre class="calibre18"># create a variable that holds a key-value pair for ham and spam<br class="title-page-name"/>class_labels = {"ham":0,"spam":1}<br class="title-page-name"/><br class="title-page-name"/># use the class_labels variable with map()<br class="title-page-name"/>df_messages['labels']=df_messages['labels'].map(class_labels)<br class="title-page-name"/>df_messages.head()</pre>
<p class="calibre20">Notice that, in the following screenshot, under the <kbd class="calibre12">labels</kbd> variable, all ham and spam messages are now labelled as 0 and 1 respectively:</p>
<p class="CDPAlignCenter"><img class="aligncenter51" src="assets/524cff97-dbb5-4df5-973d-6c638d867809.png"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="7" class="calibre14">
<li class="calibre11">We will now split our data into training and testing samples:</li>
</ol>
<pre class="calibre18"># Split your data into train &amp; test set<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = train_test_split(df_messages[‘message’],\<br class="title-page-name"/>                                 df_messages[‘labels’],test_s=0.2,random_state=1)</pre>
<ol start="8" class="calibre14">
<li class="calibre11">We need to convert<span> the collection of messages to a matrix of token counts</span>. This can be done using <kbd class="calibre12">CountVectorizer()</kbd>:</li>
</ol>
<pre class="calibre18"># Creating an instance of the CountVectorizer class<br class="title-page-name"/># If ‘english’, a built-in stop word list for English is used.<br class="title-page-name"/># There are known issues with ‘english’ and you should consider an alternative<br class="title-page-name"/>vectorizer = CountVectorizer(lowercase=True, stop_words=‘english’, analyzer=‘word’)<br class="title-page-name"/><br class="title-page-name"/># Learn a vocabulary from one or more message using the fit_transform() function<br class="title-page-name"/>vect_train = vectorizer.fit_transform(X_train)</pre>
<ol start="9" class="calibre14">
<li class="calibre11">We proceed to build our model with the Naive Bayes algorithm:</li>
</ol>
<pre class="calibre18"># Create an instance of MultinomialNB()<br class="title-page-name"/>model_nb = MultinomialNB()<br class="title-page-name"/><br class="title-page-name"/># Fit your data to the model<br class="title-page-name"/>model_nb.fit(vect_train,Y_train)<br class="title-page-name"/><br class="title-page-name"/># Use predict() to predict target class<br class="title-page-name"/>predict_train = model_nb.predict(vect_train)</pre>
<ol start="10" class="calibre14">
<li class="calibre11">We load the required libraries for the evaluation metrics, as follows:</li>
</ol>
<pre class="calibre18">from sklearn.metrics import accuracy_score<br class="title-page-name"/>from sklearn.metrics import precision_score<br class="title-page-name"/>from sklearn.metrics import recall_score<br class="title-page-name"/>from sklearn.metrics import f1_score</pre>
<ol start="11" class="calibre14">
<li class="calibre11">We now check our accuracy by evaluating the model with the training data:</li>
</ol>
<pre class="calibre18"># Calculate Train Accuracy<br class="title-page-name"/>print(‘Accuracy score: {}’.format(accuracy_score(Y_train, predict_train)))<br class="title-page-name"/><br class="title-page-name"/># Calculate other metrics on your train results<br class="title-page-name"/>print(‘Precision score: {}’.format(precision_score(Y_train, predict_train)))<br class="title-page-name"/>print(‘Recall score: {}’.format(recall_score(Y_train, predict_train)))<br class="title-page-name"/>print(‘F1 score: {}’.format(f1_score(Y_train, predict_train)))</pre>
<p class="calibre20">The output of this is as follows:</p>
<p class="CDPAlignCenter"><img class="aligncenter52" src="assets/04a4ca07-c4d9-4266-8681-0165d9e1f05b.png"/></p>
<ol start="12" class="calibre14">
<li class="calibre11">Now we check the accuracy of our test data by evaluating the model with the unseen test data:</li>
</ol>
<pre class="calibre18"># We apply the model into our test data<br class="title-page-name"/>vect_test = vectorizer.transform(X_test)<br class="title-page-name"/>prediction = model_nb.predict(vect_test)<br class="title-page-name"/><br class="title-page-name"/># Calculate Test Accuracy<br class="title-page-name"/>print(‘Accuracy score: {}’.format(accuracy_score(Y_test, prediction)))<br class="title-page-name"/><br class="title-page-name"/># Calculate other metrics on your test data<br class="title-page-name"/>print(‘Precision score: {}’.format(precision_score(Y_test, prediction)))<br class="title-page-name"/>print(‘Recall score: {}’.format(recall_score(Y_test, prediction)))<br class="title-page-name"/>print(‘F1 score: {}’.format(f1_score(Y_test, prediction)))</pre>
<p class="calibre2">With the preceding code block, we print performance metrics as follows:</p>
<p class="CDPAlignCenter"><img class="aligncenter53" src="assets/e4ba0b37-2483-4806-99a9-5da4a042a38e.png"/></p>
<div class="packtinfobox">These results may vary with different samples and hyperparameters.</div>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1,</em> we looked at our dataset. In <em class="calibre13"><span class="calibre5">Step </span>2</em> and <em class="calibre13"><span class="calibre5">Step </span></em><em class="calibre13">3</em>, we looked at the statistics for the <kbd class="calibre12">ham</kbd> and spam class labels. In <em class="calibre13"><span class="calibre5">Step </span>4</em>, we extended our analysis by looking at the word count and the character count for each of the messages in our dataset. In <em class="calibre13"><span class="calibre5">Step </span>5</em>, we saw the distribution of our target variables (ham and spam), while in <em class="calibre13"><span class="calibre5">Step</span> 6</em> we encoded our class labels for the target variable with the numbers <kbd class="calibre12">1</kbd> and <kbd class="calibre12">0</kbd>. In S<em class="calibre13">tep</em> 7, we split our dataset into training and testing samples. In <em class="calibre13"><span class="calibre5">Step </span>8</em>, we used <kbd class="calibre12">CountVectorizer()</kbd> from <kbd class="calibre12">sklearn.feature_extraction.text</kbd> to convert the collection of messages to a matrix of token counts. </p>
<div class="packtinfobox">
<p class="calibre2">If you do not provide a dictionary in advance and do not use an analyzer that does some kind of feature selection, then the number of features will be equal to the vocabulary size found by analyzing the data. For more information on this, see the following: <a href="https://bit.ly/1pBh3T1" class="calibre21">https://bit.ly/1pBh3T1</a>.</p>
</div>
<p class="calibre2">In <em class="calibre13">Step 9</em> and <em class="calibre13">Step<span class="calibre5"> </span></em><em class="calibre13">10</em>, we built our model and imported the required classes from <kbd class="calibre12">sklearn.metrics</kbd> to measure the various scores respectively. In <em class="calibre13"><span class="calibre5">Step </span>11</em> and <em class="calibre13">12</em>, we checked the accuracy of our training and testing datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2">The Naive Bayes algorithm comes in multiple variations. These include <span class="calibre5">the Multivariate Bernoulli Naive Bayes,</span> Multinomial Naive Bayes, and Gaussian Multinomial Naive Bayes algorithms. These variations can be applied to solve different problems.</p>
<ul class="calibre10">
<li class="CDPAlignLeft4"><span><strong class="calibre1">Multivariate Bernoulli Naive Bayes</strong>: This algorithm is used when the feature vectors provide a binary representation of whether a word or feature occurs in the document or not. Every token in the feature vector of a document is associated with either the <kbd class="calibre12">1</kbd> or </span><kbd class="calibre12">0</kbd> values<span><span>. <kbd class="calibre12">1</kbd> represents a token in which the word occurs, and <kbd class="calibre12">0</kbd> represents a token in which the word does not occur. The Multivariate Bernoulli Naive Bayes algorithm can be used in situations in which the absence of a particular word matters, such as in the detection of spam content.</span></span></li>
<li class="calibre11"><strong class="calibre1">Multinomial Naive Bayes</strong>: This is used when multiple occurrences of words are to be considered in classification problems. <span>In this variation, text documents are characterized by the </span><span>frequency</span><span> of the </span><span>term, instead of binary values. Frequency is </span><span>a discrete count that </span><span>refers to how many times a given word or token appears in a document. The Multinomial Naive Bayes algorithm </span><span>can be used for topic modeling, which is a method for finding a group of words that best represent the key information in a corpus of documents.</span></li>
<li class="calibre11"><strong class="calibre1">Gaussian Multinomial Naive Bayes</strong>:<strong class="calibre1"> </strong>In scenarios where we have continuous features, <span>one way to deal with continuous data in Naive Bayes classifications is to discretize the features. </span>Alternatively, we can apply the Gaussian Multinomial Naive Bayes<strong class="calibre1"> </strong>algorithm.<em class="calibre23"><strong class="calibre1"> </strong></em>This assumes the features follow a normal distribution and uses a Gaussian kernel to calculate the class probabilities.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">In scikit-learn, <kbd class="calibre12">CountVectorizer()</kbd> <span>counts the number of times a word shows up in the document and uses that value as its weight. You can also use <kbd class="calibre12">TfidfVectorizer()</kbd></span>, <span>where the weight assigned to each token depends on both its frequency in a document and how often the term recurs in the entire corpus. You can find more on <kbd class="calibre12">TfidfVectorizer</kbd> at the following link: <a href="https://bit.ly/2sJCoVN" class="calibre9">https://bit.ly/2sJCoVN</a>.</span></li>
<li class="calibre11">The scikit-learn documentation on the Naive Bayes classifier for multivariate Bernoulli models: <a href="https://bit.ly/2y3fASv" class="calibre9">https://bit.ly/2y3fASv</a>.</li>
<li class="calibre11">The scikit-learn <span>documentation on</span> the Naive Bayes classifier for multinomial models: <a href="https://bit.ly/2P4Ohic" class="calibre9">https://bit.ly/2P4Ohic</a>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision trees</h1>
                </header>
            
            <article>
                
<p class="calibre2">Decision trees, <span class="calibre5">a</span><span class="calibre5"> non-parametric supervised learning method, </span><span class="calibre5">are popular algorithms used for predictive modeling.</span> <span class="calibre5">The most well-known decision tree algorithms include the <strong class="calibre4">iterative dichotomizer</strong> (<strong class="calibre4">ID3</strong>), C4.5, CART, and C5.0. </span><span class="calibre5">ID3 is </span><span class="calibre5">only</span><span class="calibre5"> </span><span class="calibre5">applicable for categorical features. C4.5 is an improvement on ID3 and has the ability to handle</span><span class="calibre5"> </span><span class="calibre5">missing values </span><span class="calibre5">and </span><span class="calibre5">continuous attributes.</span> <span class="calibre5">The tree-growing process involves finding the best split at each node using the information gain. However, the C4.5 algorithm converts a continuous attribute into a dichotomous categorical attribute by splitting at a suitable threshold value that can produce maximum information gain. </span></p>
<p class="calibre2">Leo Breiman, a distinguished statistician, introduced a decision tree algorithm called the <strong class="calibre4">Classification and Regression Tree</strong> (<strong class="calibre4">CART</strong>). CART, unlike ID3 and C4.5, can produce decision trees that can be used for both classification and regression problems. <span class="calibre5">This algorithm also forms the basis for the important </span><span class="calibre5">random forest</span><span class="calibre5"> algorithm.</span></p>
<p class="calibre2">Decision trees are built using recursive partitioning, which splits the data into subsets based on several dichotomous independent attributes. This recursive process may split the data multiple times until the splitting process terminates after a particular stopping criterion is reached. <span class="calibre5">The best split is the one that maximizes a splitting criterion. </span><span class="calibre5">For classification learning, </span><span class="calibre5">the techniques used as the splitting criterion are </span><span class="calibre5">entropy and information gain, the Gini index, and the gain ratio</span><span class="calibre5">. For regression tasks, however, standard deviation reduction is</span> <span class="calibre5">used. </span></p>
<p class="calibre2">The C4.5 and C5.0 algorithms use entropy (also known as <strong class="calibre4">Shannon entropy</strong>) and information gain to identify the optimal attributes and decide on the splitting criterion. Entropy is a probabilistic measure of uncertainty or randomness.</p>
<p class="calibre2">Mathematically, entropy can be expressed as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation42" src="assets/4048cca7-177c-491b-9c35-cfc9b0465f38.png"/></p>
<p class="calibre2">In the case of a two-class attribute, entropy can range from 0 to 1. For an n-class attribute, entropy can take values between 0 to <img class="fm-editor-equation43" src="assets/f16f2000-eeff-4579-9301-f9ea353aa172.png"/>. For a homogeneous variable, where there is just a single class, the entropy would be zero because the probability of that class being zero is 1 and <img class="fm-editor-equation44" src="assets/deeeb9d3-eb9f-4989-af93-113b21e38502.png"/> .</p>
<p class="calibre2"><span class="calibre5">To use entropy to identify the most identified attributes at which to split, the algorithm calculates the change in homogeneity that would result from the split at each possible attribute. This change is known as information gain. Constructing a decision tree is all about finding the attribute that returns the highest information gain. </span><span class="calibre5">This information gain is based on the decrease in entropy after a dataset is split at an attribute.</span></p>
<p class="calibre2"><span class="calibre5">Information gain is calculated as the difference between the entropy before the split and the entropy after the split:</span></p>
<p class="CDPAlignCenter"><img class="fm-editor-equation45" src="assets/95ffc038-e4e1-47f6-a3c9-e896b3a52ecb.png"/></p>
<p class="calibre2"/>
<p class="calibre2">The higher the information gain, the better a feature is. Information gain is calculated for all features. The algorithm chooses the feature with the highest information gain to create the root node. The information gain is calculated at each node to select the best feature for that node.</p>
<div class="packtinfobox">Information gain is also known as Kullback-Leibler divergence. This measures the difference between two probability distributions over the same variable. Put simply, if you have two probability distributions, the KL divergence measures the similarity of the two distributions. If the KL divergence is 0, the two distributions are equal.</div>
<p class="calibre2">The Gini index is a measure of the degree of impurity and can also be used <span class="calibre5">to identify the optimal attributes for the splitting criterion. It is calculated as follows:</span></p>
<p class="CDPAlignCenter"><span class="calibre5"><img class="fm-editor-equation46" src="assets/0e2db890-f8f0-407b-874a-b8ad80ec9f2d.png"/></span></p>
<p class="calibre2">In the preceding formula, <em class="calibre13">p</em> is the probability of a training instance belonging to a particular class. With regards to the Gini index, the lower the impurity, the better it is. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">To build our model with q decision tree algorithm, we will use the <kbd class="calibre12">backorders.csv</kbd> file, which can be downloaded from the following GitHub.</p>
<p class="calibre2">This dataset has 23 columns. The target variable is <kbd class="calibre12">went_on_backorder</kbd>. This identifies whether a product has gone on back order. The other 22 variables are the predictor variables. A description of the data is provided in the code that comes with this book:</p>
<p class="calibre2">We will start by importing the required libraries:</p>
<pre class="calibre18"># import os for operating system dependent functionalities<br class="title-page-name"/>import os<br class="title-page-name"/><br class="title-page-name"/># import other required libraries<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from sklearn.preprocessing import StandardScaler<br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/>from sklearn.metrics import accuracy_score<br class="title-page-name"/>from sklearn.metrics import confusion_matrix, roc_curve, auc<br class="title-page-name"/>import itertools<br class="title-page-name"/>from sklearn import tree<br class="title-page-name"/><br class="title-page-name"/>import seaborn as sns<br class="title-page-name"/>import matplotlib.pyplot as plt</pre>
<p class="calibre2">We set our working directory with the <kbd class="calibre12">os.chdir()</kbd> command:</p>
<pre class="calibre18"># Set your working directory according to your requirement<br class="title-page-name"/>os.chdir(".../Chapter 4/Decision Tree")<br class="title-page-name"/><br class="title-page-name"/># Check Working Directory <br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2">Let's read our data. As we have done previously, we are going to prefix the name of the DataFrame with <kbd class="calibre12">df_</kbd> to make it easier to understand:</p>
<pre class="calibre18">df_backorder = pd.read_csv("BackOrders.csv")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">Let's now move on to building our model:</span></p>
<ol class="calibre14">
<li class="calibre11">First, we want to look at the dimensions of the dataset and the data using the <kbd class="calibre12">shape</kbd> and <kbd class="calibre12">head()</kbd> functions. We also take a look at the statistics of the numeric variables using <kbd class="calibre12">describe()</kbd>:</li>
</ol>
<pre class="calibre18">df_backorder.shape<br class="title-page-name"/>df_backorder.head()<br class="title-page-name"/>df_backorder.describe()</pre>
<div class="packttip">If you get your output in scientific notation, you can change to view it in standard form instead by executing the following command: <kbd class="calibre19">pd.options.display.float_format = ‘{:.2f}’.format</kbd></div>
<ol start="2" class="calibre14">
<li class="calibre11">With <kbd class="calibre12">dtypes</kbd>, we get to see the data types of each of the variables:</li>
</ol>
<pre class="calibre18">df_backorder.dtypes</pre>
<ol start="3" class="calibre14">
<li class="calibre11">We can see that <kbd class="calibre12">sku</kbd> is an identifier and will be of no use to us for our model-building exercise. We will, therefore, drop <kbd class="calibre12">sku</kbd> from our DataFrame as follows:</li>
</ol>
<pre class="calibre18">df_backorder.drop('sku', axis=1, inplace=True)</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="4" class="calibre14">
<li class="calibre11">We can check whether there are any missing values with the <kbd class="calibre12">isnull().sum()</kbd> command:</li>
</ol>
<pre class="calibre18">df_backorder.isnull().sum()</pre>
<p class="calibre20">      We take a look at the following screenshot:</p>
<p class="CDPAlignCenter"><img class="aligncenter54" src="assets/5d2293ce-455b-424b-9f7b-3f62144d8dc5.png"/></p>
<p class="calibre2"/>
<ol start="5" class="calibre14">
<li class="calibre11">Since the number of missing values in the <kbd class="calibre12">lead_time</kbd> variable is about 5%, we will remove all the observations where <kbd class="calibre12">lead_time</kbd> is missing for our initial analysis:</li>
</ol>
<pre class="calibre18">df_backorder = df_backorder.dropna(axis=0)</pre>
<ol start="6" class="calibre14">
<li class="calibre11">We now need to encode our categorical variables. We select only the categorical variables and call <kbd class="calibre12">pd.get_dummies()</kbd> to dummy-code the non-numeric variables:</li>
</ol>
<pre class="calibre18">non_numeric_attributes = df_backorder.select_dtypes(include=['object']).columns<br class="title-page-name"/>df_backorder = pd.get_dummies(columns=non_numeric_attributes, data=df_backorder, prefix=non_numeric_attributes, prefix_sep="_",drop_first=True)<br class="title-page-name"/>df_backorder.dtypes</pre>
<p class="calibre20">With the preceding code, we get to see the datatypes. We notice that dummy-coded variables are all of the unsigned integer (<kbd class="calibre12">uint8</kbd>) type:</p>
<p class="CDPAlignCenter"><img class="aligncenter55" src="assets/6d4f6117-f192-4a9a-b747-39ae88f81f8c.png"/></p>
<ol start="7" class="calibre14">
<li class="calibre11">We will then look at our target variable distribution as follows:</li>
</ol>
<pre class="calibre18"># Target variable distribution<br class="title-page-name"/>pd.value_counts(df_backorder['went_on_backorder_Yes'].values)</pre>
<p class="calibre20">We can see that our data has a fairly balanced distribution, with approximately 81% of the observations belonging to class 0 and 19% belonging to class 1:</p>
<p class="CDPAlignCenter"><img class="aligncenter56" src="assets/383927b2-0b30-4b40-8d34-32912524e6f9.png"/></p>
<ol start="8" class="calibre14">
<li class="calibre11">We will now split our data into training and testing datasets:</li>
</ol>
<pre class="calibre18">#Performing train test split on the data<br class="title-page-name"/>X, Y = df_backorder.loc[:,df_backorder.columns!=‘went_on_backorder_Yes’].values, df_backorder.loc[:,‘went_on_backorder_Yes’].values<br class="title-page-name"/><br class="title-page-name"/># Split our dataset into train &amp; test set<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)</pre>
<ol start="9" class="calibre14">
<li class="calibre11">We will build our first model with <kbd class="calibre12">DecisionTreeClassifier()</kbd>:</li>
</ol>
<pre class="calibre18"># Create an instance of DecisionTreeClassifier()<br class="title-page-name"/>classifier = tree.DecisionTreeClassifier(random_state=1)<br class="title-page-name"/><br class="title-page-name"/># Fit our model to the data<br class="title-page-name"/>model_DT_Gini = classifier.fit(X_train, Y_train)<br class="title-page-name"/>model_DT_Gini</pre>
<p class="calibre20">With <kbd class="calibre12">model_DT_Gini</kbd>, we can see the default values of the hyperparameters that have been used:</p>
<p class="CDPAlignCenter"><img class="aligncenter57" src="assets/bb169144-daba-48b2-8cf7-9559c5d93ac6.png"/></p>
<ol start="10" class="calibre14">
<li class="calibre11">We can use the model to predict our class labels using both our training and our testing datasets:</li>
</ol>
<pre class="calibre18"># Predict with our test data<br class="title-page-name"/>test_predictedvalues = model_DT_Gini.predict(X_test)<br class="title-page-name"/><br class="title-page-name"/># Check accuracy<br class="title-page-name"/>acc = accuracy_score(Y_test, test_predictedvalues)<br class="title-page-name"/>print("Accuracy is", acc)<br class="title-page-name"/><br class="title-page-name"/># Check TP, TN, FP, FN<br class="title-page-name"/>tn, fp, fn, tp = confusion_matrix(Y_test, test_predictedvalues).ravel()<br class="title-page-name"/>print("TN:",tn, " FP:",fp, " FN:",fn, " TP:",tp)</pre>
<p class="calibre20">This gives us the accuracy along with the count of <strong class="calibre4">True Negative</strong> (<strong class="calibre4">TN</strong>), <strong class="calibre4">False Positive</strong> (<strong class="calibre4">FP</strong>), <strong class="calibre4">False Negative</strong> (<strong class="calibre4">FN</strong>), and <strong class="calibre4">True Positive</strong> (<strong class="calibre4">TP</strong>) values:</p>
<p class="CDPAlignCenter"><img class="aligncenter58" src="assets/d44ff442-1bd0-4719-9164-291d7426b989.png"/></p>
<ol start="11" class="calibre14">
<li class="calibre11">We will now use a <kbd class="calibre12"><span>plot_confusion_matrix</span></kbd> function to plot our confusion matrix. This function is taken from <a href="http://scikit-learn.org" class="calibre9">http://scikit-learn.org</a> and is readily available there, so we won't show this function here. It is, however, provided with the code in the book for your reference:</li>
</ol>
<pre class="calibre18">target_names = [ ‘No’, ‘Yes’]<br class="title-page-name"/><br class="title-page-name"/>#Pass Actual &amp; Predicted values to confusion_matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, test_predictedvalues)<br class="title-page-name"/><br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names, normalize=False)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre20">We can then see the amount of TNs, FPs, FNs, and TPs in our confusion matrix plot:</p>
<p class="CDPAlignCenter"><img class="aligncenter59" src="assets/1616f4ad-61b7-4a4e-af45-f6c8de1ea3b5.png"/></p>
<ol start="12" class="calibre14">
<li class="calibre11">We can change the hyperparameters to tune our model. We can also perform a grid search to find the hyperparameter values that supply optimum results. We can use the following code to set the hyperparameter values:</li>
</ol>
<pre class="calibre18"># set the parameters for grid search<br class="title-page-name"/>grid_search_parameters = {“criterion”: [“gini”, “entropy”],<br class="title-page-name"/>             “min_samples_split”: [2],<br class="title-page-name"/>             “max_depth”: [None, 2, 3],<br class="title-page-name"/>             “min_samples_leaf”: [1, 5],<br class="title-page-name"/>             “max_leaf_nodes”: [None],<br class="title-page-name"/>             }</pre>
<ol start="13" class="calibre14">
<li class="calibre11">We will use <kbd class="calibre12">GridSearchCV()</kbd> to grid search the parameters:</li>
</ol>
<pre class="calibre18">from sklearn.model_selection import GridSearchCV<br class="title-page-name"/><br class="title-page-name"/># Create an instance of DecisionTreeClassifier()<br class="title-page-name"/>classifier = tree.DecisionTreeClassifier()<br class="title-page-name"/><br class="title-page-name"/># Use GridSearchCV(), pass the values you have set for grid search<br class="title-page-name"/>model_DT_Grid = GridSearchCV(classifier, grid_search_parameters, cv=10)<br class="title-page-name"/>model_DT_Grid.fit(X_train, Y_train)</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="14" class="calibre14">
<li class="calibre11">After running the preceding command, we can see the best parameter values among those provided using <kbd class="calibre12">best_params_</kbd>: </li>
</ol>
<pre class="calibre18">model_DT_Grid.<span>best_params_ </span></pre>
<ol start="15" class="calibre14">
<li class="calibre11">You can use the model that is selected using the <kbd class="calibre12">GridSearchCV()</kbd> function:</li>
</ol>
<pre class="calibre18">test_predictedvalues = model_DT_Grid.predict(X_test)<br class="title-page-name"/><br class="title-page-name"/>cc = accuracy_score(Y_test, test_predictedvalues)<br class="title-page-name"/>print("Accuracy is", acc)<br class="title-page-name"/><br class="title-page-name"/>tn, fp, fn, tp = confusion_matrix(Y_test, test_predictedvalues).ravel()<br class="title-page-name"/>print("TN:",tn, " FP:",fp, " FN:",fn, " TP:",tp)<br class="title-page-name"/><br class="title-page-name"/>cm = confusion_matrix(Y_test, test_predictedvalues)<br class="title-page-name"/><br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names, normalize=False)<br class="title-page-name"/>plt.show()</pre>
<ol start="16" class="calibre14">
<li class="calibre11">In order to see the metrics per-label, we can also use the <kbd class="calibre12">classification_report</kbd>, as follows:</li>
</ol>
<pre class="calibre18">from sklearn.metrics import classification_report<br class="title-page-name"/><br class="title-page-name"/>target_names = [ 'No', 'Yes']<br class="title-page-name"/>print(classification_report(Y_test, test_predictedvalues, target_names=target_names))</pre>
<p class="calibre20">This step gives us the following output:</p>
<p class="CDPAlignCenter"><img class="aligncenter60" src="assets/5a15b729-30a8-4a17-b26d-81c41d1636f9.png"/></p>
<p class="calibre20">These results will vary depending on the samples used and the hyperparameter tuning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em>, we took a look at the dimensions of our dataset. We also saw the statistics of our numerical variables. In <em class="calibre13">Step 2</em>, we looked at the datatypes of each of our variables. In <em class="calibre13">Step 3</em>, we dropped the <kbd class="calibre12">sku</kbd> attribute, because it is an identifier that will be of no use to us for our model. In <em class="calibre13">Step 4</em>, we checked for missing values and noticed that the <kbd class="calibre12">lead_time</kbd> <span class="calibre5">attribute </span>had 3,403 missing values, which is roughly 5% of the total number of observations. In <em class="calibre13">Step 5</em>, we dropped the observations for which the <kbd class="calibre12">lead_time</kbd> had missing values. Note that there are various strategies to impute missing values, but we haven't considered these in this exercise.</p>
<p class="calibre2">In <em class="calibre13">Step 6</em>, we used <kbd class="calibre12">get_dummies()</kbd> from the pandas library with <kbd class="calibre12">drop_first=True</kbd> as one of the parameters to perform a k-1 dummy coding on the categorical variables. In <em class="calibre13">Step 7</em>, we took a look at the distribution of our target variable. We see the class labels, 0 and 1, are in the ratio of 19%<span class="calibre5">-</span><span class="calibre5">81%</span> approximately, which is not very well balanced. However, we had enough observations for both classes to proceed to our next steps. In <em class="calibre13">Step 8</em>, we separated our predictor and response variables. We also split our dataset to create a training dataset and a testing dataset. In <em class="calibre13">Step 9</em>, we used a <kbd class="calibre12">DecisionTreeClassifier()</kbd> to build our model. We noted the default hyperparameters values and noticed that, by default, <kbd class="calibre12"><span>DecisionTreeClassifier()</span></kbd> uses the Gini impurity measure as the splitting criterion. </p>
<p class="calibre2">In <em class="calibre13">Step 10</em>, we used the model to predict our test sample. We took a note of the overall accuracy and the amount of TP, TN, FP, and FN values that we achieved. In <em class="calibre13">Step 11</em>, we used <kbd class="calibre12">plot_confusion_matrix()</kbd> to plot these values in the form of a confusion matrix. Please note that <span class="calibre5"><kbd class="calibre12">plot_confusion_matrix()</kbd> is readily available at <a href="https://bit.ly/2MdyDU9" class="calibre9">https://bit.ly/2MdyDU9</a> and is also provided with the book in the code folder for this chapter.</span></p>
<p class="calibre2">We then looked at changing the hyperparameter values to fine-tune our model. We performed a grid search to find the optimum hyperparameter values. In <em class="calibre13">Step 12</em>, we defined the combination of values for our hyperparameters that we want to apply to our grid search algorithm. In <em class="calibre13">Step 13</em> and <em class="calibre13">14</em>, we used <kbd class="calibre12">GridSearchCV()</kbd> to look for the optimum hyperparameters. In <em class="calibre13">Step 15</em>, we used the model returned by the grid search to predict our test observations. Finally, in <em class="calibre13">Step 16</em>, we used <kbd class="calibre12">classification_report()</kbd> from <kbd class="calibre12">sklearn.metrics</kbd> to generate various scores including <kbd class="calibre12">precision</kbd>, <kbd class="calibre12">recall</kbd>, <kbd class="calibre12">f1-score</kbd>, and <kbd class="calibre12">support</kbd>. </p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2">Sometimes, a model can classify training data perfectly but faces difficulty when working with new data. This problem is known as <strong class="calibre4">overfitting</strong>. The model fails to generalize to the new test data. </p>
<p class="calibre2">We allow a recursive splitting process to repeat until we terminate the leaf node because we cannot split the data further. This model would fit the training data perfectly but leads to poor performance. For this reason, tree-based models are susceptible to overfitting. To overcome this, we need to control the depth of our decision tree.</p>
<p class="calibre2">There are multiple ways to avoid overfitting. One method is to terminate the growth before a perfect classification of the training data is made. The following approaches can be adopted to implement this stopping method:</p>
<ul class="calibre10">
<li class="calibre11">Stop when a tree reaches the maximum number of levels </li>
<li class="calibre11">Stop when a subset contains fewer than a defined number of training instances</li>
<li class="calibre11">Stop when the minimum information gain is reached</li>
</ul>
<p class="calibre2"><span class="calibre5">Another method is to allow the data to overfit, and then to prune the tree after it is constructed. This involves eliminating nodes that are not clearly relevant, which also minimizes the size of the decision tree.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">The scikit-learn documentation on the decision tree classifier: <a href="https://bit.ly/1Ymrzjw" class="calibre9">https://bit.ly/1Ymrzjw</a></li>
<li class="calibre11">The scikit-learn <span>documentation on</span> the decision tree regressor: <a href="https://bit.ly/2xMNSua" class="calibre9">https://bit.ly/2xMNSua</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector machines</h1>
                </header>
            
            <article>
                
<p class="calibre2">A <strong class="calibre4">support vector machine</strong> (<strong class="calibre4">SVM</strong>) is a popular machine learning algorithm for supervised learning. It can be used for both classification and regression problems. <span class="calibre5">In classification learning, SVM performs classifications by finding an optimal separating hyperplane that differentiates two classes of observations. If the data is linearly separable and one-dimensional, we may have a point that separates the data. In two-dimensional space, the data can be separated by a straight line, while a plane separates data in three-dimensional space. When we have more than three dimensions, this is called a hyperplane.</span></p>
<p class="calibre2"/>
<p class="calibre2">For a linear SVM, a dataset <em class="calibre13">X</em> with <em class="calibre13">n</em> feature vectors is represented as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation47" src="assets/5fb2c247-c973-48d5-a61c-081f8b4a7124.png"/></p>
<p class="calibre2">A bipolar target variable <em class="calibre13">Y</em> is written as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation48" src="assets/eb1770ec-6461-4385-8495-94d959b99152.png"/></p>
<p class="calibre2">The hyperplane is given by the following:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation49" src="assets/e6d2790e-d2d0-4fd6-a451-f9b13f70d3bd.png"/></p>
<p class="calibre2">For an SVM, the two classes are represented as -1 and +1 instead of 1 and 0. The hyperplane can, therefore, be written as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation50" src="assets/4d429da2-7ea8-41f7-9389-ddd5de0db021.png"/></p>
<p class="calibre2">To classify the data, we have the following two rules:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation51" src="assets/dafe97b7-53a1-4bfb-a6f6-c96829d976f3.png"/></p>
<p class="calibre2">However, it's quite possible that there are a lot of hyperplanes that correctly classify the training data. There might be infinite solutions of <em class="calibre13">w</em> and <em class="calibre13">b</em> that hold for the preceding rules. An algorithm such as a perceptron learning algorithm will just find any linear classifier. SVM, however, finds the optimal hyperplane, which <span class="calibre5">is at a maximum distance from any data point. </span>The further the data points lie from the hyperplane, the more confident we are that they have been correctly classified. We would therefore like the data points to be as far away from the hyperplane as possible, while still being able to classify them correctly. <span class="calibre5">The best hyperplane is the one that has the maximum margin between the two classes. This is known as the maximum-margin hyperplane. </span></p>
<p class="calibre2"><span class="calibre5">It's possible for SVM to choose </span><span class="calibre5">the most important vectors that define the </span><span class="calibre5">separation hyperplane from the training data. These are the data points that lie closest to the hyperplane and are known as support vectors. </span><span class="calibre5">Support vectors are the data points that are hardest to classify. </span><span class="calibre5">At the same time, these represent high-quality data. If you remove all the other data points and use only the support vectors, you can get back the exact decision hyperplane and the margin </span><span class="calibre5">using the same SVM model</span><span class="calibre5">. The number of data points does not really matter, just the support vectors.</span></p>
<p class="calibre2"/>
<p class="calibre2"><span class="calibre5">We normalize the weights <em class="calibre13">w</em> and <em class="calibre13">b</em> so that the support vectors satisfy the following condition:</span></p>
<p class="CDPAlignCenter"><img class="fm-editor-equation52" src="assets/b7b23e99-c87b-4838-b936-059d6ee20202.png"/></p>
<p class="calibre2">As a result, the classification rules change to the following:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation51" src="assets/fb754845-562f-4184-8d35-c979706a615c.png"/></p>
<p class="calibre2">The preceding equations can be combined and represented as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation53" src="assets/2fb465eb-6c11-4010-ada3-8a5e1df77834.png"/></p>
<p class="calibre2">The initial SVM algorithms could <span class="calibre5">only</span><span class="calibre5"> </span><span class="calibre5">be used in the case of linearly separable data. These are known as hard-margin SVMs. However, hard-margin SVMs can work only when the data is completely linearly separable and if doesn't have any noise. In the case of noise or outliers, a hard-margin SVM might fail.</span></p>
<p class="calibre2"><span class="calibre5">Vladimir Vapnik proposed soft-margin SVMs to deal with data that is non-linearly separable by using slack variables. Slack variables allows for errors to be made while fitting the model to the training dataset. In hard-margin classification, we will get a decision boundary with a small margin. In soft-margin classification, we will get a decision boundary with a larger margin:</span></p>
<p class="CDPAlignCenter"><img class="aligncenter61" src="assets/a46c8e11-1bfb-4664-98ab-008ff1d437df.png"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"><span class="calibre5">SVMs can also perform non-linear classification extremely well using something called a kernel trick. This refers to transformations in which the predictor variables are implicitly mapped to a higher-dimensional feature space. Popular kernel types include the following:</span></p>
<ul class="calibre10">
<li class="calibre11">Linear kernels</li>
<li class="calibre11"><span>Polynomial kernels</span></li>
<li class="calibre11"><span>Radial basis function (RBF) kernels</span></li>
<li class="calibre11"><span>Sigmoid kernels</span></li>
</ul>
<p class="calibre2"><span class="calibre5">Different kernel functions are available for various decision functions. We can add kernel functions together to achieve even more complex planes.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we are going to use the <kbd class="calibre12">bank.csv</kbd> file, which is based on bank marketing data and which you can download from GitHub. <span class="calibre5">This data is related to a Portuguese bank's direct marketing campaigns </span><span class="calibre5">that took place over phone calls. The goal is to predict whether the client will subscribe to a term deposit:</span></p>
<p class="calibre2">We will start by importing the required libraries:</p>
<pre class="calibre18"># import os for operating system dependent functionalities<br class="title-page-name"/>import os<br class="title-page-name"/><br class="title-page-name"/># import other required libraries<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from sklearn.svm import SVC<br class="title-page-name"/>from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc<br class="title-page-name"/>from sklearn.model_selection import train_test_split</pre>
<p class="calibre2"> We set our working directory with the <kbd class="calibre12">os.chdir()</kbd> command:</p>
<pre class="calibre18"># Set your working directory according to your requirement<br class="title-page-name"/>os.chdir(".../Chapter 4/Support Vector Machine")<br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2">Let's read our data. We will again prefix the name of the DataFrame with <kbd class="calibre12">df_</kbd> to make it easier to understand:</p>
<pre class="calibre18">df_bankdata = pd.read_csv("bank.csv")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we're going to look at checking null values, standardizing numeric values, and one-hot-encoding categorical variables:</p>
<ol class="calibre14">
<li class="calibre11">With the following command, we can see we that we have ten categorical variables and seven numerical variables in the dataset:</li>
</ol>
<pre class="calibre18">df_bankdata.dtypes</pre>
<ol start="2" class="calibre14">
<li class="calibre11">With the following command, we notice there are no missing values, so we can proceed with our next steps:</li>
</ol>
<pre class="calibre18">df_bankdata.isnull().sum()</pre>
<ol start="3" class="calibre14">
<li class="calibre11">We can check the class balance in our target variable as follows:</li>
</ol>
<pre class="calibre18">print("Total number of class labels: {}".format(df_bankdata.shape[0]))<br class="title-page-name"/>print("Number of people opted for Term Deposit: {}".format(df_bankdata[df_bankdata.y == 'yes'].shape[0]))<br class="title-page-name"/>print("Number of people not opted for Term Deposit: {}".format(df_bankdata[df_bankdata.y == 'no'].shape[0]))</pre>
<ol start="4" class="calibre14">
<li class="calibre11">We can convert our target class to the binary values 1 and 0 with the following command:</li>
</ol>
<pre class="calibre18">df_bankdata['y'] = (df_bankdata['y']=='yes').astype(int)</pre>
<ol start="5" class="calibre14">
<li class="calibre11">We can now perform one-hot encoding on our categorical variables. We only select variables that are categorical in nature. In the following code, we use <kbd class="calibre12">category_column_names</kbd> to provide the names of the non-numeric variables:</li>
</ol>
<pre class="calibre18"># Using select_dtypes() to select only the non-numerical type variable<br class="title-page-name"/>column_type = ['object']<br class="title-page-name"/>df_bank_data_category_cols = df_bankdata.select_dtypes(column_type)<br class="title-page-name"/><br class="title-page-name"/># This will give you the names of the non-numerical variables<br class="title-page-name"/>category_column_names = df_bank_data_category_cols.columns.values.tolist()<br class="title-page-name"/>category_column_names</pre>
<ol start="6" class="calibre14">
<li class="calibre11">We run a loop over each of the non-numerical variables to perform one-hot encoding on them and add them back to the DataFrame. We will also delete the original non-numerical variables after performing one-hot encoding:</li>
</ol>
<pre class="calibre18">for each_col in category_column_names:<br class="title-page-name"/>   dummy_var = pd.get_dummies(df_bank_data_category_cols[each_col], prefix=each_col)<br class="title-page-name"/>   df_joindata = df_bankdata.join(dummy_var)<br class="title-page-name"/>   df_joindata.drop([each_col], axis=1, inplace=True)<br class="title-page-name"/>   df_bankdata = df_joindata</pre>
<ol start="7" class="calibre14">
<li class="calibre11">We separate the predictor and response variables as follows:</li>
</ol>
<pre class="calibre18"># Separate features &amp; response variable<br class="title-page-name"/>X=df_bankdata.iloc[:, :-1]<br class="title-page-name"/>Y=df_bankdata['y']</pre>
<ol start="8" class="calibre14">
<li class="calibre11">We also split our data into training and testing datasets:</li>
</ol>
<pre class="calibre18">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)</pre>
<ol start="9" class="calibre14">
<li class="calibre11">We then build our first model using SVC with the default kernel, <strong class="calibre1">radial basis function</strong> (<strong class="calibre1">RBF</strong>):</li>
</ol>
<pre class="calibre18"># Note, you need not pass kernel='rbf' to the SVC() because its the default<br class="title-page-name"/>svc_model = SVC(kernel='rbf') <br class="title-page-name"/>svc_model.fit(X_train, Y_train)</pre>
<ol start="10" class="calibre14">
<li class="calibre11">We check our training and testing accuracy via the SVC model built with the RBF kernel:</li>
</ol>
<pre class="calibre18">train_predictedvalues=svc_model.predict(X_train)<br class="title-page-name"/>test_predictedvalues=svc_model.predict(X_test)<br class="title-page-name"/><br class="title-page-name"/>print('Train Accuracy Score:')<br class="title-page-name"/>print(metrics.accuracy_score(Y_train,train_predictedvalues))<br class="title-page-name"/><br class="title-page-name"/>print('Test Accuracy Score:')<br class="title-page-name"/>print(metrics.accuracy_score(Y_test,test_predictedvalues))</pre>
<p class="calibre20">We get the following output:</p>
<p class="CDPAlignCenter"><img class="aligncenter62" src="assets/e3b0f08d-0164-4b38-9bd4-f0013851e31e.png"/></p>
<ol start="11" class="calibre14">
<li class="calibre11">We can rebuild our SVC model with a polynomial kernel as follows:</li>
</ol>
<pre class="calibre18">svc_model =SVC(kernel='poly') <br class="title-page-name"/>svc_model.fit(X_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/>train_predictedvalues=svc_model.predict(X_train)<br class="title-page-name"/>test_predictedvalues=svc_model.predict(X_test)<br class="title-page-name"/><br class="title-page-name"/>print('Train Accuracy Score:')<br class="title-page-name"/>print(metrics.accuracy_score(Y_train,train_predictedvalues))<br class="title-page-name"/><br class="title-page-name"/>print('Test Accuracy Score:')<br class="title-page-name"/>print(metrics.accuracy_score(Y_test,test_predictedvalues))</pre>
<p class="calibre20">We get the following output with the polynomial kernel:</p>
<p class="CDPAlignCenter"><img class="aligncenter63" src="assets/b6429ace-6e67-4813-b1c9-d0fd31463280.png"/></p>
<ol start="12" class="calibre14">
<li class="calibre11">We can also build an SVC model with the linear kernel. Instead of <kbd class="calibre12">kernel='ploy'</kbd>, we can replace this with <kbd class="calibre12">kernel='linear'</kbd> in the preceding code:</li>
</ol>
<pre class="calibre18">svc_model =SVC(kernel='linear') </pre>
<p class="calibre20">With the linear kernel, we get the following accuracy:</p>
<p class="CDPAlignCenter"><img class="aligncenter64" src="assets/595facc0-1ed8-4282-a9cd-7d76ae51a0de.png"/></p>
<p class="calibre20"><span class="calibre5">Our results will vary depending on the </span>different types of kernel and other hyperparameter values used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em>, we looked at the data types of our variables. We noticed that we have ten categories and seven numerical variables. In <em class="calibre13">Step 2</em>, we checked for missing values and saw that there were no missing values in our dataset. In <em class="calibre13">Step 3</em>, we checked the class balance of our target variable and found out that it has the values of <kbd class="calibre12">yes</kbd> and <kbd class="calibre12">no</kbd>. In <em class="calibre13">Step 4</em>, we converted our target variable to 1 and 0 to represent <kbd class="calibre12">yes</kbd> and <kbd class="calibre12">no</kbd> respectively. In <em class="calibre13">Steps 5</em> and <em class="calibre13">6</em>, we performed one-hot encoding on the non-numerical variables.</p>
<p class="calibre2">In <em class="calibre13">Step 7</em>, we separate the predictor and response variables and in <em class="calibre13">Step 8</em>, we split our dataset into training and testing datasets. After that, in <em class="calibre13">Step 9</em>, we used <kbd class="calibre12">SVC()</kbd> from <kbd class="calibre12">sklearn.svm</kbd> with the default<span class="calibre5"> RBF</span> kernel to build our model. We applied it to our training and testing data to predict the class. In <em class="calibre13">Step 10</em>, we checked the accuracy of our training and testing data. In <em class="calibre13">Step 11</em>, we changed our hyperparameter to set the kernel to polynomial. We noticed that training accuracy remained more or less the same, but the test accuracy improved.</p>
<div class="packttip">With the polynomial kernel, the default degree is 3. You can change the polynomial degree to a higher degree and note of the change in the <span>model's</span> performance.</div>
<p class="calibre2">In <em class="calibre13">Step 12</em>, we changed the kernel to linear to see if the results improved compared to the polynomial kernel. We did not, however, see any significant improvement.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">In this exercise, we have seen how to use various kernels in our code. </span>Kernel functions must be symmetrical. Preferably, they should have a positive (semi) definite gram matrix. A gram matrix is the matrix of all the possible inner products of V, where V is the set of m vectors. For convenience, <span class="calibre5">we consider</span><span class="calibre5"> positive semi-definite and positive-definite functions indifferently. </span>In practice, a positive definiteness of kernel matrices ensures that kernel algorithms converge to a unique solution.</p>
<p class="calibre2"><span class="calibre5">A <strong class="calibre4">linear kernel</strong> is the simplest of all kernels available. It works well with text classification problems. </span></p>
<p class="calibre2">A linear kernel is presented as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation54" src="assets/523469ff-9905-47b2-b6c5-b7bafe00005c.png"/></p>
<p class="calibre2">Here, <strong class="calibre4">c</strong> is the constant term.</p>
<p class="calibre2"><span class="calibre5">A <strong class="calibre4">polynomial kernel</strong> has two parameters: a constant and the degree. A polynomial kernel with no constant and a degree of 1 is simply a linear kernel. As the degree of the polynomial kernel increases, the decision function becomes more complex. With higher degrees, it is possible to get good training accuracy, but the model might fail to generalize to unseen data, leading to overfitting. The polynomial kernel is represented as follows:</span></p>
<p class="CDPAlignCenter"><img class="fm-editor-equation55" src="assets/837f7b87-065c-485b-ab70-78c4b14d3931.png"/></p>
<p class="calibre2">Here, <img class="fm-editor-equation21" src="assets/cd03e4d1-7215-4c12-9417-06d231ce8fec.png"/> is the slope, d is the degree of the kernel, and c is the constant term.</p>
<p class="calibre2">The <strong class="calibre4">radial basis function kernel (RBF)</strong>, also known as the Gaussian kernel, is a more complicated kernel and can outperform polynomial kernels. The RBF kernel is given as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation56" src="assets/1a85ebd6-761c-4a94-a8dd-65bbadcb0b30.png"/></p>
<p class="calibre2">The <img class="fm-editor-equation57" src="assets/33e24c69-55f0-4345-a50f-b8e7491d809f.png"/><span class="calibre5"> </span><span class="calibre5">parameter </span><span class="calibre5">can be tuned to increase the performance of the kernel. This is important: w</span><span class="calibre5">ith an over-estimated </span><img class="fm-editor-equation57" src="assets/e1273c32-d0b3-4a18-9f6b-ea8aa5a8b505.png"/><span class="calibre5">, the kernel can lose its non-linear power and behave more linearly. On the other hand, if <img class="fm-editor-equation58" src="assets/e1273c32-d0b3-4a18-9f6b-ea8aa5a8b505.png"/> is underestimated, the decision function can be highly sensitive to noise in the training data.</span></p>
<p class="calibre2">Not all kernels are strictly positive-definite. The sigmoid kernel function, though is quite widely used, is not positive-definite. The sigmoid function is given as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation59" src="assets/cdaf800d-a3a4-489d-bf26-ff6a6a61d939.png"/></p>
<p class="calibre2">Here, <img class="fm-editor-equation21" src="assets/cd03e4d1-7215-4c12-9417-06d231ce8fec.png"/><span class="calibre5"> is the slope and</span> <strong class="calibre4">c</strong> is the constant term. Note that an SVM with a sigmoid kernel is the same as a two-layer perceptron neural network.</p>
<p class="calibre2"><span class="calibre5">Adding a kernel trick to an SVM model can give us new models. How do we choose which kernel to use? </span>The first approach is to try out the RBF kernel, since it works pretty well most of the time. However, it is a good idea to use other kernels and validate your results. Using the right kernel with the right dataset can help you build the best SVM models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">More on the positive definite matrix can be found here: <a href="https://bit.ly/2NnGeLK" class="calibre9">https://bit.ly/2NnGeLK</a>.</li>
<li class="calibre11">Positive definite kernels are a generalization of the positive definite matrix. You can find out more about this here: <a href="https://bit.ly/2NlsIs1" class="calibre9">https://bit.ly/2NlsIs1</a>.</li>
</ul>
<ul class="calibre10">
<li class="calibre11">The scikit-learn documentation on support vector regression: <a href="https://bit.ly/2OFZ8ix" class="calibre9">https://bit.ly/2OFZ8ix</a>.</li>
</ul>


            </article>

            
        </section>
    </body></html>