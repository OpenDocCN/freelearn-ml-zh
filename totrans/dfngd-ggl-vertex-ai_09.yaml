- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Optimizations – Hyperparameter Tuning and NAS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now become quite familiar with some of the Vertex AI offerings related
    to managing data, training no-code and low-code models, and launching large-scale
    custom model training jobs (with metadata tracking and monitoring capabilities).
    As ML practitioners, we know that it is highly unlikely that the first model we
    train would be the best model for a given use case and dataset. Thus, in order
    to find the best model (which is the most accurate and least biased), we often
    use different model optimization techniques. **Hyperparameter Tuning** (**HPT**)
    and **Neural Architecture Search** (**NAS**) are two such model optimization techniques.
    In this chapter, we will learn how to configure and launch model optimization
    experiments using Vertex AI on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will first learn about the importance of model optimization
    techniques such as HPT and then learn how to quickly set up and launch HPT jobs
    within Google Vertex AI. We will also understand how NAS works and how it is different
    from HPT. The topics covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: What is HPT and why is it important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up HPT jobs on Vertex AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is NAS and how is it different from HPT?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NAS on Vertex AI overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code examples shown in this chapter can be found in the following GitHub
    repo: [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter09](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter09)'
  prefs: []
  type: TYPE_NORMAL
- en: What is HPT and why is it important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hyperparameter tuning, or HPT for short, is a popular model optimization technique
    that is very commonly used across ML projects. In this section, we will learn
    about hyperparameters, the importance of tuning them, and different methods of
    finding the best hyperparameters for a machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: What are hyperparameters?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we train an ML system, we basically have three kinds of data – input data,
    model parameters, and model hyperparameters. Input data refers to our training
    or test data that is associated with the problem we are solving. Model parameters
    are the variables that we modify during the model training process and we try
    to adjust them to fit the training data. Model hyperparameters, on the other hand,
    are variables that govern the training process itself. These hyperparameters are
    fixed before we start to train our model. For example, learning rate, optimizer,
    batch size, number of hidden layers in a neural network, and the max depth in
    a tree-based algorithm are some examples of model hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Why HPT?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How well your machine learning model will perform largely depends upon the hyperparameters
    you choose before training. The values of hyperparameters can make all the difference
    to performance metrics (such as accuracy), training time, bias, fairness, and
    so on for your model. Hyperparameter tuning or HPT is a model optimization technique
    that chooses a set of optimal hyperparameters for a learning algorithm. The same
    ML algorithm can require totally different values of hyperparameters to generalize
    for different data patterns. There is an objective function associated with every
    HPT job that it tries to optimize (minimize or maximize) and it returns the values
    of hyperparameters that achieve that optimal value. This objective function can
    be similar to the model training objective (e.g., loss function) or it can be
    a completely new metric.
  prefs: []
  type: TYPE_NORMAL
- en: We run model optimization operations such as HPT or NAS when we are at a stage
    where our final model (i.e., XGBoost) is fixed and we have a fixed test set for
    which we want to optimize the hyperparameters of our chosen model. A typical HPT
    job runs multiple trials with different sets of hyperparameters and returns the
    hyperparameters that lead to the best trial. The best trial here represents the
    trial that optimizes the objective function associated with the HPT job.
  prefs: []
  type: TYPE_NORMAL
- en: Search algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While running HPT, we have to decide what kind of search algorithm we want
    to run over our hyperparameter space. There are multiple different kinds of search
    algorithms that we can choose from based on our needs. A few commonly used approaches
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss these approaches!
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The traditional way of performing HPT has been grid search, which is basically
    an exhaustive search over a manually specified search space. A grid search must
    be provided with a performance metric that it tries to calculate over all possible
    sets of hyperparameter combinations, measured over a hold-out validation set (or
    cross-validation on a training set). As it runs all possible combinations of provided
    hyperparameter ranges, it is important to set those ranges carefully and with
    discrete values. As grid search runs all the trials independently, it can be parallelized
    for faster outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Random search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of trying all combinations sequentially and exhaustively like grid search,
    random search selects hyperparameters randomly from the provided search space
    during each trial. As it chooses hyperparameter values randomly, it also generalizes
    to continuous spaces along with discrete spaces, as discussed above. Random search
    again is highly parallelizable as all the trials are independent. Despite its
    simplicity, random search is one of the most important baselines to test new optimization
    or search techniques against.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike grid search and random search, the Bayesian optimization method builds
    a probabilistic model of the function that maps hyperparameter values to the HPT
    objective function. Thus, at each new trial, it learns better about the direction
    it should take to find the optimal hyperparameters for the given objective function
    on a fixed validation set. It tries to balance exploration and exploitation and
    has been shown to obtain better results than the preceding techniques in fewer
    trials. But as it learns from the ongoing trials, it often runs the trials iteratively
    (thus it’s less parallelizable).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good understanding of HPT, let’s understand how to set up
    and launch HPT jobs on Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up HPT jobs on Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to set up HPT jobs with Vertex AI. We will
    use the same neural network model experiment from [*Chapter 7*](B17792_07.xhtml#_idTextAnchor093),
    *Training Fully Custom ML Models with Vertex AI*, and optimize its hyperparameters
    to get the best model settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create a new Jupyter Notebook in Vertex AI Workbench and
    import useful libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set up project configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize the Vertex AI SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to containerize the full training application code. We will
    put our full training code into a Python file, `task.py`, here. The `task.py`
    file should have an entire flow, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading and preparing the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model (running a trial with given hyperparameters as args)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving the model (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passing the training trial output to the `hypertune()` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The training script should have a list of hyperparameters that it wants to
    tune, defined as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we have other important hyperparameters, such as learning rate,
    batch size, loss function, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The script should have a function that loads and prepares the training and
    validation datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the validation and test data parts are loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function loads the already prepared dataset from GCS. We can refer
    to [*Chapter 7*](B17792_07.xhtml#_idTextAnchor093), *Training Fully Custom ML
    Models with Vertex AI*, to fully understand the data preparation part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the **TensorFlow** (**TF**) model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define the encoder part of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we will define two more encoder layers with an increasing number
    of filters, a kernel size of 3, and a stride of 2 so that we can compress the
    image into important features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the decoder part of the TF model within the same function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the decoder design is almost opposite to the encoder part. Here,
    we re-create the image from compressed features by using multiple layers of transpose
    convolutions and reducing the channels gradually to 3 to generate the final color
    image output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a `tanh` activation function to get the final colored output image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, add a function to build and compile the TF model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, add a `main` function that trains the model and provides the hyperparameter
    tuning metric value to the `hypertune()` function. In our case, we will be optimizing
    the loss over the validation dataset. See the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the configurations and load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s build the TF model and fit it on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `hypertune` to define and report hyperparameter tuning metrics to the HPT
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If we put this all into a single Python file, our `task.py` file should look
    something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Load all the dependencies for our task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse arguments where we define the hyperparameters used for tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a few more hyperparameter-related arguments for tuning the learning
    rate, batch size, and loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up configurations for training purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Define configuration settings for training distribution strategies based on
    the requirements – it can be a single, mirror, or multi-worker strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Load and prepare the training, validation, and test partitions of data from
    the GCS bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, load the validation and test partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the TF model architecture for converting a black-and-white image to
    a color image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the encoder part of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we will define two more encoder layers with an increasing number
    of filters, a kernel size of 3, and a stride of 2 so that we can compress the
    image into important features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the decoder part of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the decoder design is almost opposite to the encoder part. Here,
    we re-create the image from compressed features by using multiple layers of transpose
    convolutions and reduce the channels gradually to 3 to generate the final color
    image output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, generate the color image output by using the `tanh` activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function will build and compile the TF model for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s define the main function to start executing our training and tuning
    task. Here, the `num_replicas_in_sync` parameter defines how many training tasks
    are running in parallel on different workers in a multi-worker training strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the training and validation data to start training our TF model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, define the HPT metric with the help of the `hypertune` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a staging bucket in GCS that will be used for storing artifacts
    such as trial outcomes from our HPT job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to containerize the entire training code defined in the `task.py`
    file. The hyperparameter tuning job will use this container to launch different
    trials with different hyperparameters as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Our Dockerfile is ready – let’s build and push the Docker image to **Google
    Container** **Registry** (**GCR**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a container image ready with all the training code that we need.
    Let’s configure the HPT job.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the type of machine we want our trials to run on. The machine
    specification will depend upon the size of the model and training dataset. As
    this is a small experiment, we will use the `n1-standard-8` machine to run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note that, within the worker pool spec, we have also passed the path to the
    training image that we created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will define the parameter space that our job will use to find the
    best hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The parameter space should be carefully defined based on best practices and
    prior knowledge so that the HPT job doesn’t have to perform unnecessary trials
    over unimportant hyperparameter ranges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to define the metric specifications. In our case, as we are trying
    to optimize the validation loss value, we would like to minimize it. In the case
    of accuracy, we should have maximized our metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Vertex AI HPT jobs use the Bayesian optimization approach by default to find
    the best hyperparameters for our settings. We also have the option to use other
    optimization approaches. As Bayesian optimization works best for most cases, we
    will be using it in our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the custom job that will run our hyperparameter tuning trials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define the HPT job that will launch the trials using the preceding
    custom job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `max_trial_count` and `parallel_trial_count` parameters are important
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_trial_count`: You need to put an upper bound on the number of trials the
    service will run. More trials generally lead to better results, but there will
    be a point of diminishing returns, after which additional trials have little or
    no effect on the metric you’re trying to optimize. It is best practice to start
    with a smaller number of trials and get a sense of how impactful your chosen hyperparameters
    are before scaling up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parallel_trial_count`: If you use parallel trials, the service provisions
    multiple training processing clusters. Increasing the number of parallel trials
    reduces the amount of time the hyperparameter tuning job takes to run; however,
    it can reduce the effectiveness of the job overall. This is because the default
    tuning strategy uses the results of previous trials to inform the assignment of
    values in subsequent trials. If we keep the parallel trial count equal to the
    number of maximum trials, then all trials will start running in parallel, and
    we will end up running a “random parameter search” here as there will not be any
    scope of learning from the performance of previous trials.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we are all set, we can launch the HPT job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As soon as we launch the job, it provides us with a link to the Cloud console
    UI, where we can monitor the progress of our HPT trials and jobs. The Cloud console
    UI looks something similar to what’s shown in *Figure 9**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – HPT job monitoring within the Cloud console UI](img/B17792_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – HPT job monitoring within the Cloud console UI
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have successfully understood and launched an HPT job on Vertex AI,
    let’s jump to the next section and understand the NAS model optimization technique.
  prefs: []
  type: TYPE_NORMAL
- en: What is NAS and how is it different from HPT?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial Neural Networks** or **ANNs** are widely used today for solving
    complex ML problems. Most of the time, these network architectures are hand-designed
    by ML experts, which may not be optimal every time. **Neural Architecture Search**
    or **NAS** is a technique that automates the process of designing neural network
    architectures that usually outperform hand-designed networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Although both HPT and NAS are used as model optimization techniques, there are
    certain differences in how they both work. HPT assumes a given architecture and
    focuses on optimizing the hyperparameters that lead to the best model. HPT optimizes
    hyperparameters such as learning rate, optimizer, batch size, activation function,
    and so on. NAS, on the other hand, focuses on optimizing architecture-specific
    parameters (in a way, it automates the process of designing a neural network architecture).
    NAS optimizes parameters such as the number of layers, number of units, types
    of connections between layers, and so on. Using NAS, we can search for optimal
    neural architectures in terms of accuracy, latency, memory, a combination of these,
    or a custom metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'NAS usually works with a relatively larger search space than HPT and controls
    different aspects of network architectures. However, the underlying problem addressed
    is the same as HPT optimization. There are many NAS-based optimization approaches,
    but on a high level, any NAS approach has three main components, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Search space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s learn more about each of these components.
  prefs: []
  type: TYPE_NORMAL
- en: Search space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This component controls the set of possible neural architectures to consider.
    The search space is often problem-specific, as a vision-related problem would
    have the possibility of having **Convolutional Neural Network** (**CNN**) layers
    as well. However, the process of identifying the best architecture is automated
    by NAS. Carefully designing these search spaces still depends upon human expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This component decides how to navigate the search space to find the best possible
    architecture for a given application. Many different optimization methods have
    been applied to NAS, such as **reinforcement learning** (**RL**), Bayesian optimization,
    gradient-based optimization, evolutionary search, and so on. Each of these methods
    has its own way of evaluating the architectures, but the high-level goal is to
    focus on the area of the search space that provides better performance. This aspect
    of NAS is quite similar to HPT optimization methods.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The evaluation method is a component that is used for assessing the quality
    of architectures designed by the chosen optimization method. One simple way to
    evaluate neural architecture is to fully train it, but this method is quite computationally
    expensive. Alternatively, to make NAS more efficient, partial training and evaluation
    methods have been developed. In order to provide cheaper heuristic measures of
    neural network quality, some evaluation methods have been developed. These evaluation
    methods are quite specific to NAS and exploit the basic structure of a neural
    network to estimate the quality of a network. Some examples of these methods include
    weight-sharing, hypernetworks, network morphism, and so on. These NAS-specific
    evaluation methods are practically way cheaper than full training.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a good understanding of the NAS optimization method and how it works.
    Next, let’s explore the Vertex AI offering and its features for launching NAS
    on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: NAS on Vertex AI overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vertex AI NAS is an optimization technique that can be leveraged to find the
    best neural network architecture for a given ML use case. NAS-based optimization
    searches for the best network in terms of accuracy but can also be augmented with
    other constraints such as latency, memory, or a custom metric as per the requirements.
    In general, the search space of possible neural networks can be quite large and
    NAS may support a search space as large as 10^20\. In the past few years, NAS
    has been able to successfully generate some state-of-the-art computer vision network
    architectures, including NASNet, MNasNet, EfficientNet, SpineNet, NAS-FPN, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: It may seem complex, but NAS features are quite flexible and easy to use. A
    beginner can leverage prebuilt modules for search spaces, trainer scripts, and
    Jupyter notebooks to start exploring Vertex AI NAS on a custom dataset. If you
    are an expert, you could potentially develop custom trainer scripts, custom search
    spaces, custom evaluation methods, and even develop applications for non-vision-based
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vertex AI can be leveraged to explore the full set of NAS features for our
    customized architectures and use cases. Here is what Vertex AI provides us with
    to help in implementing NAS more conveniently:'
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI provides a NAS-specific language that can be leveraged to define a
    custom search space to try out the desired set of possible neural network architectures
    and integrate this space with our custom trainer scripts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-built state-of-the-art search spaces and a trainer that are ready to use
    and can run on a GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pre-defined NAS controller that samples our custom-defined search space to
    find the best neural network architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of prebuilt libraries and Docker images that can be leveraged to calculate
    latency, FLOPS (Floating-point operations per second), or memory usage on a custom
    hardware setting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud provides tutorials to explain the usage of NAS. It also provides
    examples and guidance for setting up NAS for PyTorch-based applications efficiently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-built tools to design proxy tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is library support that can be leveraged to report custom-defined metrics
    and perform analysis on them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Google Cloud console is very helpful in monitoring and managing NAS jobs.
    We also get some easy-to-use example notebooks to kick-start the search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Management of CPU/GPU resource usage on the basis of per project or per job,
    with the help of a prebuilt library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A NAS client to build Docker images, launch NAS jobs, and resume an old NAS
    search job that is Python-based.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer support is Google Cloud console UI-based.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These features can help us in setting up a custom NAS job without putting in
    too much effort. Now let’s discuss some of the best practices while working with
    NAS.
  prefs: []
  type: TYPE_NORMAL
- en: NAS best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The important thing to note here is that NAS is not an optimization method
    that we should apply to all our ML problems. There are certain things to keep
    in mind before deciding to run a NAS job for our use case. Some of these best
    practices are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: NAS is not meant for tuning the hyperparameters of a model. It only performs
    an architecture search and it is not advised to compare the results of these two
    methods. In some setups, HPT can be followed by NAS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NAS is not recommended for smaller or highly imbalanced datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NAS is expensive, so unless we can spend a few thousand dollars without extremely
    high expectations, it’s not meant for us.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should first try other traditional and conventional machine learning methods
    and techniques such as hyperparameter tuning. You should use neural architecture
    search only if you don’t see further gains with traditional methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up NAS jobs on Vertex AI is not very complex, thanks to the prebuilt
    assets and publicly released code examples. With these prebuilt features, examples,
    and best practices, we should be able to set up a custom NAS job that can help
    us find an optimal architecture to meet our project goals.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the importance of applying model optimization
    techniques to get the best performance for our application. We learned about two
    model optimization methods – HPT and NAS, with their similarities and differences.
    We also learned how to set up and launch large-scale HPT jobs on Vertex AI with
    code examples. Additionally, we discussed some best practices to get the best
    out of both HPT and NAS.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you should have a fair understanding of the term
    “model optimization” and its importance while developing ML applications. Additionally,
    you should now be confident about quickly setting up small to large-scale hyperparameter
    tuning experiments with the help of Vertex AI tooling on Google Cloud. You should
    also have a fair understanding of NAS, its differences from HPT, and the best
    practices for setting up a NAS job.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the importance and common methods of model optimization
    techniques, we are in good shape to develop high-quality models. Next, let’s learn
    about how to deploy these models so that they can be consumed by downstream applications.
  prefs: []
  type: TYPE_NORMAL
