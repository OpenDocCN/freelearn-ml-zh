<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Neural Networks</h1>
                </header>
            
            <article>
                
<p>As a developer, you have surely gained an interest in machine learning from looking at all the incredibly amazing applications that you see on your regular devices every day‚Äîautomatic speech translation, picture style transfer, the ability to generate new pictures from sample ones, and so on. Brace yourself... we are heading directly into the technology that has made all these things possible.</p>
<p>Linear and logistic models, such as the ones we've observed, have certain limitations in terms of the complexity of the training dataset they train a model on, <span>even when they are the basis of very articulated and efficient solutions</span>.</p>
<p>How complex does a model have to be to capture the style of writing of an author, the concept of an image of a cat versus an image of a dog, or the classification of a plant based on visual elements? These things require the summation of a huge mix of low-level and high-level details captured, in the case of our brain by specialized sets of neurons, and in computer science by neural models.</p>
<p>Contrary to what's expected, in this book I'll spare you the typical introduction to the nervous system, its capabilities, the number of neurons in the nervous system, its chemical properties, and so on. I find that these topics add an aura of impossibility to problems, but the models we are about to learn are simple math equations with computational counterparts, which we will try to emphasize to allow you, a person with algorithmic interests, to easily understand them.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>The history of neural models, including perceptron and ADALINE</li>
<li>Neural networks and the kinds of problems they solve</li>
<li>The multilayer perceptron</li>
<li>Implementing a simple neural layer to model a binary function</li>
</ul>
<p>This chapter will allow you to use the building blocks of most of the awesome machine learning applications you see daily. So, let's get started!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">History of neural models</h1>
                </header>
            
            <article>
                
<p>Neural models, in the sense of being disciplines that try to build representations of the internal workings of the brain, originated pretty distantly in the computer science timescale. They even date back to the time when the origins of modern computing were being invented, the mid-1940s.</p>
<p>At that time, the fields of neuroscience and computer science began to collaborate by researching ways of emulating the way the brain processes information, starting from its constituent unit‚Äîthe neuron.</p>
<p>The first mathematical method for representing the learning function of the human brain can be assigned to McCulloch and Pitts, in their 1943 paper <em>A Logical Calculus of Ideas Immanent in Nervous Activity:</em></p>
<div class="CDPAlignCenter CDPAlign"><img height="201" width="367" src="assets/0e0ee661-6f00-42e3-87a4-35139415faa8.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>McCulloch and Pitts model</span></div>
<p>This simple model was a basic but realistic model of a learning algorithm. You will be surprised by what happens if we use a linear function as a transfer function; this is a simple linear model, as were the ones we saw in the previous chapter.</p>
<div class="packt_infobox">You may have noted that we are now using the <kbd>w</kbd> letter to specify the parameters to be tuned by the model. Take this as the new standard from now on. Our old <kbd>Œ≤</kbd> parameters in linear regression models will now be <kbd>w</kbd>.</div>
<p>But the model hasn't determined a way to tune the parameters. Let's go forward to the 1950s and review the <strong>perceptron</strong> model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The perceptron model</h1>
                </header>
            
            <article>
                
<p>The perceptron model is one of the simplest ways to implement an artificial neuron. It was initially developed at the end of the 1950s, with the first hardware implementation being carried out in the 1960s. First, it was the name of a machine, and later became the name of an algorithm. Yes, perceptrons are not the strange entities we always thought they were, they're what you as a developer handle each day‚Äî algorithms!</p>
<p>Let's take a look at the following steps and learn how it works:</p>
<ol>
<li><span>Initialize the weights with a random (low value) distribution.</span></li>
<li><span>Select an input vector and present it to the network.</span></li>
<li><span>Compute the output</span> <em>y'</em> <span>of the network for the input vector specified and the values of the weights.</span></li>
</ol>
<p style="padding-left: 60px"><span>The function for a perceptron is as follows:</span></p>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img height="52" width="216" src="assets/7ef51780-af9a-41cd-8c1f-5749a0aa2e4e.png"/></div>
<ol start="4">
<li>If <em>y‚Äô ‚â† y</em>, modify all the connections,¬†<em>wi</em>, by adding the changes <em>Œîw =yxi</em>.</li>
<li>Return to <em>step 2</em>.</li>
</ol>
<p>It is basically an algorithm that learns a binary classification function and maps a real function to a single binary function.</p>
<p>Let's depict the new architecture of the perceptron and analyze the new scheme of the algorithm in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img height="137" width="224" src="assets/8dd545c6-e350-4130-a7e7-45da7a1065c6.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign">Perceptron model (With changes from previous model highlighted)</div>
<p>The perceptron builds on the ideas of its predecessors, but the novelty here is that we are adding a proper learning mechanism! In the following diagram, we have highlighted the new properties of the model‚Äîthe feedback loop, where we calculate the error of our outcomes, and the adjustment of weights‚Äîwith a predetermined formula:</p>
<div class="CDPAlignCenter CDPAlign"><img height="294" width="325" src="assets/db8ec5ff-a2b3-48bb-86ba-d7bb01b6c5b8.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Perceptron algorithm flowchart</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving our predictions ‚Äì the ADALINE algorithm</h1>
                </header>
            
            <article>
                
<p><span>ADALINE is another algorithm (yes, remember we are talking about algorithms) used to train neural networks. ADALINE is in some ways more advanced than the perceptron because it adds a new training method: gradient descent, which should be known to you by now. Additionally, it changes the point at which the error is measured before the activation output is applied to the summation of the weights:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="203" width="383" src="assets/48d0f5a9-2fb4-4a18-a130-ab8d98268c49.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Adaline model (With additions from Perceptron highlighted)</div>
<p>So, this is the standard way of representing the ADALINE algorithm in a structural way. As an algorithm consists of a series of steps, let's aggregate those in a more detailed way, with some additional mathematical details:</p>
<ol>
<li>Initialize the weights with a random (low value) distribution.</li>
<li>Select an input vector and present it to the network.</li>
<li>Compute the output <em>y'</em> of the network for the input vector specified and the values of the weights.</li>
<li>The output value that we will be taking will be the one after the summation:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><em><em>y=Œ£(xi * wi)</em></em></div>
<ol start="5">
<li>Compute the error, comparing the model output with the right label <em>o</em>:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><em>E=(o-y)<sup>2</sup></em></div>
<p style="padding-left: 90px" class="mce-root CDPAlignLeft CDPAlign">Does it look similar to something we have already seen? Yes! We are basically resolving a regression problem.</p>
<ol start="6">
<li class="mce-root">Adjust the weights with the following gradient descent recursion:</li>
</ol>
<div style="padding-left: 240px"><img height="24" width="140" src="assets/58540fe4-1f28-4b5a-b899-7b20601ad6da.png"/></div>
<ol start="7">
<li>Return to <em>step 2</em>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="358" width="368" src="assets/e8429de2-2994-466a-b964-e181702e4f94.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Adaline algorithm flowchart</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Similarities and differences between a perceptron and ADALINE</h1>
                </header>
            
            <article>
                
<p>We have covered a simplified explanation of the precursors of modern neural networks. As you can see, the elements of modern models were almost all laid out during the 1950s and the 1960s! Before continuing, let's try to compare the approaches:</p>
<ul>
<li><strong>Similarities</strong>:
<ul>
<li>They are both algorithms (it's important to stress that)</li>
<li>They are applied to single-layer neural models</li>
<li>They are classifiers for binary classification</li>
<li>Both have a linear decision boundary</li>
<li>Both can learn iteratively, sample by sample (the perceptron naturally, and ADALINE via stochastic gradient descent)</li>
<li>Both use a threshold function</li>
</ul>
</li>
<li><strong>Differences</strong>:
<ul>
<li>The perceptron uses the final class decision to train weights</li>
<li>ADALINE uses continuous predicted values (from the net input) to learn the model coefficients, and measures subtle changes in error with a continuous float point rank, not a Boolean or integer</li>
</ul>
</li>
</ul>
<p>Before we are done with our single-layer architectures and models, we will review some discoveries from the end of the 1960s that provoked quite a stir in the neural models community and were said to generate the first AI winter, or an abrupt collapse in the interest in machine learning research. Happily, some years after that, researchers found ways to overcome the limitations they faced, but this will be covered further on in the chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Limitations of early models</h1>
                </header>
            
            <article>
                
<p>The model itself now has most of the elements of any normal neural model, but it had its own problems. After some years of fast development, the publication of the book <span><em>P</em></span><em>erceptrons</em> by Minsky and Papert in 1969 prompted a stir in the field because of its main idea that perceptrons could only work on linearly separable problems, which were only a very tiny part of the problems that the practitioners thought were solvable. In a sense, the book suggested that perceptrons were verging on being useless, with the exception of the simplest classification tasks.</p>
<p>This new-found deficiency could be represented by the inability of the model to represent the XOR function, which is a <kbd>Boolean</kbd> function with an output of <kbd>1</kbd> when the inputs are different, and <kbd>0</kbd> when they are equal:</p>
<div class="CDPAlignCenter CDPAlign"><img height="163" width="291" src="assets/935f5509-f277-4e86-b870-241374cfc73f.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">The problem of modelling a XOR function. No single line will correctly separate the 0 and 1 values</div>
<p>As we can see in the preceding diagram, the main problem is that neither class (cross nor point) is linearly separable; that is, we can not separate the two with any linear function on the plane.</p>
<p><span>This detected problem caused a decrease in activity in the field that lasted for more or less five years, until the development of the backpropagation algorithm, which started in the mid 1970s.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Single and multilayer perceptrons</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Now we will discuss more contemporary times, building on the previous concepts to allow more complex elements of reality to be modeled.</span></p>
<div class="packt_infobox">In this section, we will directly study <strong>multilayer perceptrons</strong> (<strong>MLPs</strong>), which are the most common configuration used, and will consider a <strong>single-layer perceptron</strong> as a particular case of the former, highlighting the differences.</div>
<p><span>Single-layer and multilayer perceptrons were the most commonly used architecture during the 1970s and 1980s and provided huge advances in terms of the capabilities of neural systems. The main innovations they bring to the table are as follows:</span></p>
<ul>
<li>They are feedforward networks because the calculations, starting from the inputs, flow from layer to layer without any cycling (information never returns)</li>
<li>They use the backpropagation method to adjust their weights</li>
<li>The use of the <kbd>step</kbd> function as a transfer function is replaced by non-linear ones such as the sigmoid</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MLP origins</h1>
                </header>
            
            <article>
                
<p>After the power of single-unit neural models had been explored, an obvious step was to generate layers or sets of commonly connected units (we define a connection as the act of sending the output of one unit to be part of another unit's summation):</p>
<div class="CDPAlignCenter CDPAlign"><img height="284" width="355" src="assets/9fa32993-2a1f-4143-ad6d-86ff94158092.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Depiction of a simple multilayer feed forward Neural Network</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The feedforward mechanism</h1>
                </header>
            
            <article>
                
<p>In this phase of the operation of the network, the data will be input in the first layer and will flow from each unit to the corresponding units in the following layers. Then it will be summed and passed through in the hidden layers, and finally processed by the output layer. This process is totally unidirectional, so we are avoiding any recursive complications in the data flow.</p>
<div class="mce-root CDPAlignLeft CDPAlign">
<p>The feedforward mechanism, has in the MLP its counterpart in the training part of the modeling process, which will be in charge of improving the model's performance. The normally chosen algorithm is called <strong>backpropagation</strong>.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The chosen optimization algorithm ‚Äì backpropagation</h1>
                </header>
            
            <article>
                
<p>From the perceptron algorithm onward, every neural architecture has had a means to optimize its internal parameters based on the comparison of the ground truth with the model output. The common assumption was to take the derivative of the (then simple) model function and iteratively work towards the minimum value.</p>
<p>For complex multilayer networks, there is an additional overhead, which has to do with the fact that the output layer's output is the result of a long chain of functions compositions, where each layer's output is wrapped by the next one's transfer function. So, the derivative of the output will involve the derivative of an exceedingly complex function. In this case, the backpropagation method was proposed, with excellent results.</p>
<div class="packt_infobox">Backpropagation can be summarized as an algorithm used to calculate derivatives. The main attribute is that it is computationally efficient and works with complex functions. It is also a <span>generalization of the least mean squares algorithm in the linear perceptron, which we already knew!</span></div>
<p class="mce-root CDPAlignLeft CDPAlign">In the backpropagation algorithm, the responsibility of the error will be distributed among all the functions applied to the data in the whole architecture. So, the goal is to minimize the error, the gradient of the loss function, over a set of deeply compounded functions, which will again receive the help of the chain rule.</p>
<p>Now it's time to define our general algorithm for our modern version of a neural network in the following steps:</p>
<ol>
<li>Calculate the feedforward signals from the input to the output.</li>
<li>Calculate output error <em>E¬†</em>based on the prediction <em>a<sub>k</sub></em> and the target <em>t<sub>k</sub></em>.</li>
<li>Backpropagate the error signals by weighting them by the weights in the previous layers and the gradients of the associated activation functions.</li>
<li>Calculate the gradients <span>ùõø<em>E</em>/ùõø<em>Œ∏</em></span>¬†for the parameters based on the backpropagated error signal and the feedforward signals from the inputs.</li>
<li>Update the parameters using the calculated gradients¬† <em>Œ∏¬†‚Üê¬†Œ∏ -¬†Œ∑</em> ùõøE/ùõøŒ∏ <em>.</em></li>
</ol>
<p>Lets review this process now, in a graphical way:</p>
<div class="CDPAlignCenter CDPAlign"><img height="375" width="582" src="assets/6f3c94d8-c460-41e9-87c4-2af201ab0027.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Step by step representation of the Feed Forward¬† and Back Propagation training process</div>
<p>In the following diagram, we will represent the whole process in an algorithmic way. You can see the number of coincidences with the previous optimization methods, and the short number of computation blocks involved:</p>
<div class="CDPAlignCenter CDPAlign"><img height="427" width="245" src="assets/b02eecc6-ef04-42f1-be66-5a4a7daa093f.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Flowchart of the feedforward/backpropagation scheme</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of problem to be tackled</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Neural networks can be used for both regression problems and classification problems. The common architectural difference resides in the output layer: in order to be able to bring a real number-based result, no standardization function, such as sigmoid, should be applied. In this manner, we won't be changing the outcome of the variable to one of the many possible class values, getting a continuum of possible outcomes. Let's take a look at the following types of problems to be tackled:</p>
<ul>
<li><strong>Regression/function approximation problems:</strong>¬†This type of problem uses a min squared error function, a linear output activation, and sigmoidal¬†hidden activations. This will give us a real value for output.</li>
<li><strong>Classification problems (two classes, one output)</strong>: In this kind of problem we normally have a cross-entropy cost function, a sigmoid output, and hidden activations. The sigmoid function will give us the probability of occurrence or nonoccurrence of one of the classes.</li>
<li><strong>Classification problems (multiple-classes, one output per class)</strong>: In this kind of problem, we will have a cross-entropy¬†cost function with softmax outputs and sigmoid hidden activations, in order to have an output of the probabilities for any of the possible classes for a single input.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing a simple function with a single-layer perceptron</h1>
                </header>
            
            <article>
                
<p>Take a look at the following code snippet to implement a single function with a single-layer perceptron:</p>
<pre><span class="hljs-keyword">    import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">    import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
    plt.style.use(<span class="hljs-string">'fivethirtyeight'</span>)
<span class="hljs-keyword">    from</span> pprint <span class="hljs-keyword">import</span> pprint
    %matplotlib inline
<span class="hljs-keyword">    from</span> sklearn <span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">    import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining and graphing transfer function types</h1>
                </header>
            
            <article>
                
<p>The learning properties of a neural network would not be very good with just the help of a univariate linear classifier. Even some mildly complex problems in machine learning involve multiple non-linear variables, so many variants were developed as replacements for the transfer functions of the perceptron.</p>
<p>In order to represent non-linear models, a number of different non-linear functions can be used in the activation function. This implies changes in the way the neurons will react to changes in the input variables. In the following sections, we will define the main different transfer functions and define and represent them via code.</p>
<p>In this section, we will start using some <strong>object-oriented programming</strong> (<strong>OOP</strong>) techniques from Python to represent entities of the problem domain. This will allow us to represent concepts in a much clearer way in the examples.</p>
<p>Let's start by creating a <kbd>TransferFunction</kbd> class, which will contain the following two methods:</p>
<ul>
<li><kbd>getTransferFunction(x)</kbd>: This method will return an activation function determined by the class type</li>
<li><kbd>getTransferFunctionDerivative(x)</kbd>: This method will clearly return its derivative</li>
</ul>
<p>For both functions, the input will be a NumPy array and the function will be applied element by element, as follows:</p>
<pre><span class="hljs-class">    &gt;<span class="hljs-keyword">class</span> <span class="hljs-title">TransferFunction</span>:</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTransferFunction</span><span class="hljs-params">(x)</span>:</span>
            <span class="hljs-keyword">raise</span> NotImplementedError
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTransferFunctionDerivative</span><span class="hljs-params">(x)</span>:</span>
            <span class="hljs-keyword">raise</span> NotImplementedError</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Representing and understanding the transfer functions</h1>
                </header>
            
            <article>
                
<p>Let's take a look at the following code snippet to see how the transfer function works:</p>
<pre><span class="hljs-function"><span class="hljs-keyword">    def</span> <span class="hljs-title">graphTransferFunction</span><span class="hljs-params">(function)</span>:</span>
        x = np.arange(-<span class="hljs-number">2.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">0.01</span>)
        plt.figure(figsize=(<span class="hljs-number">18</span>,<span class="hljs-number">8</span>))
        ax=plt.subplot(<span class="hljs-number">121</span>)
        ax.set_title(function.__name__)
        plt.plot(x, function.getTransferFunction(x))

        ax=plt.subplot(<span class="hljs-number">122</span>)
        ax.set_title(<span class="hljs-string">'Derivative of '</span> + function.__name__)
        plt.plot(x, function.getTransferFunctionDerivative(x))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sigmoid or logistic function</h1>
                </header>
            
            <article>
                
<p>A sigmoid or logistic function is the canonical activation function and is well-suited for calculating probabilities in classification properties. Firstly, let's prepare a function that will be used to graph all the transfer functions with their derivatives, from a common range of <kbd>-2.0</kbd> to <kbd>2.0</kbd>, which will allow us to see the main characteristics of them around the <em>y</em> axis.<br/>
The classical formula for the sigmoid function is as follows:</p>
<pre><span class="hljs-class"><span class="hljs-keyword">    class</span> <span class="hljs-title">Sigmoid</span><span class="hljs-params">(TransferFunction)</span>:</span>  <span class="hljs-comment">#Squash 0,1</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTransferFunction</span><span class="hljs-params">(x)</span>:</span>
            <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTransferFunctionDerivative</span><span class="hljs-params">(x)</span>:</span>
        <span class="hljs-keyword">return</span> x*(<span class="hljs-number">1</span>-x)

    graphTransferFunction(Sigmoid)</pre>
<p>Take a look at the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="333" width="807" class="alignnone size-full wp-image-671 image-border" src="assets/05b6c520-45f2-464e-9f60-3cdc1612b2f6.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playing with the sigmoid</h1>
                </header>
            
            <article>
                
<p>Next, we will do an exercise to get an idea of how the sigmoid changes when multiplied by the weights and shifted by the bias to accommodate the final function towards its minimum. Let's then vary the possible parameters of a single sigmoid first and see it stretch and move:</p>
<pre>  ws=np.arange(-<span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.2</span>)
    bs=np.arange(-<span class="hljs-number">2.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">0.2</span>)
    xs=np.arange(-<span class="hljs-number">4.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">0.1</span>)
    plt.figure(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">10</span>))
    ax=plt.subplot(<span class="hljs-number">121</span>)
<span class="hljs-keyword">    for</span> i <span class="hljs-keyword">in</span> ws:
        plt.plot(xs,  Sigmoid.getTransferFunction(i *xs),label= str(i));
    ax.set_title(<span class="hljs-string">'Sigmoid variants in w'</span>)
    plt.legend(loc=<span class="hljs-string">'upper left'</span>);

    ax=plt.subplot(<span class="hljs-number">122</span>)
<span class="hljs-keyword">    for</span> i <span class="hljs-keyword">in</span> bs:
        plt.plot(xs,  Sigmoid.getTransferFunction(i +xs),label= str(i));
    ax.set_title(<span class="hljs-string">'Sigmoid variants in b'</span>)
    plt.legend(loc=<span class="hljs-string">'upper left'</span>);</pre>
<p>Take a look at the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="344" width="753" class="alignnone size-full wp-image-687 image-border" src="assets/21e172f3-3308-43d1-86cf-2ab429ddca97.png"/></div>
<p>Let's take a look at the following code snippet:</p>
<pre><span class="hljs-class"><span class="hljs-keyword">    class</span> <span class="hljs-title">Tanh</span><span class="hljs-params">(TransferFunction)</span>:</span>  <span class="hljs-comment">#Squash -1,1</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTransferFunction</span><span class="hljs-params">(x)</span>:</span>
            <span class="hljs-keyword">return</span> np.tanh(x)
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTransferFunctionDerivative</span><span class="hljs-params">(x)</span>:</span>
            <span class="hljs-keyword">return</span> np.power(np.tanh(x),<span class="hljs-number">2</span>)
    graphTransferFunction(Tanh)</pre>
<p>Lets take a look at the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="348" width="855" class="alignnone size-full wp-image-673 image-border" src="assets/79183576-593f-47a5-ab50-a0cc6d1b8637.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rectified linear unit or ReLU</h1>
                </header>
            
            <article>
                
<p><strong>ReLU</strong> is called a rectified linear unit, and one of its main advantages is that it is not affected by vanishing gradient problems, which generally consist of the first layers of a network tending to be values of zero, or a tiny epsilon:</p>
<pre><span class="hljs-class"><span class="hljs-keyword">    class</span> <span class="hljs-title">Relu</span><span class="hljs-params">(TransferFunction)</span>:</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTransferFunction</span><span class="hljs-params">(x)</span>:</span>
            <span class="hljs-keyword">return</span> x * (x&gt;<span class="hljs-number">0</span>)
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTransferFunctionDerivative</span><span class="hljs-params">(x)</span>:</span>
            <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> * (x&gt;<span class="hljs-number">0</span>)
    graphTransferFunction(Relu)</pre>
<p>Let's take a look at the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="371" width="902" class="alignnone size-full wp-image-674 image-border" src="assets/fe3f187c-9745-4477-b1dc-581078123250.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear transfer function</h1>
                </header>
            
            <article>
                
<p>Let's take a look at the following code snippet to understand the linear transfer function:</p>
<pre><span class="hljs-class"><span class="hljs-keyword">    class</span> <span class="hljs-title">Linear</span><span class="hljs-params">(TransferFunction)</span>:</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTransferFunction</span><span class="hljs-params">(x)</span>:</span>
            <span class="hljs-keyword">return</span> x
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTransferFunctionDerivative</span><span class="hljs-params">(x)</span>:</span>
            <span class="hljs-keyword">return</span> np.ones(len(x))
    graphTransferFunction(Linear)</pre>
<p>Let's take a look at the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="334" width="815" class="alignnone size-full wp-image-675 image-border" src="assets/a3137a1f-113f-4dd6-9482-0e0d3b2a2f3f.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining loss functions for neural networks</h1>
                </header>
            
            <article>
                
<p>As with every model in machine learning, we will explore the possible functions that we will use to determine how well our predictions and classification went.</p>
<p>The first type of distinction we will do is between the L1 and L2 error function types.</p>
<p>L1, also known as <strong>least absolute deviations</strong> (<strong>LAD</strong>) or <strong>least absolute errors</strong> (<strong>LAE</strong>), has very interesting properties, and it simply consists of the absolute difference between the final result of the model and the expected one, as follows:</p>
<div style="padding-left: 210px"><img height="39" width="131" src="assets/b812faa4-4083-4230-9a61-0f17931c0979.png"/></div>
<div style="padding-left: 210px"><img height="49" width="148" src="assets/9ce333ae-4b4d-49f1-9382-a44a174d8b71.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">L1 versus L2 properties</h1>
                </header>
            
            <article>
                
<p>Now it's time to do a head-to-head comparison between the two types of loss function:</p>
<ul>
<li><strong>Robustness</strong>: L1 is a more robust loss function, which can be expressed as the resistance of the function when being affected by outliers, which projects a quadratic function to very high values. Thus, in order to choose an L2 function, we should have very stringent data cleaning for it to be efficient.</li>
<li><strong>Stability</strong>: The stability property assesses how much the error curve jumps for a large¬†error value. L1 is more unstable, especially for non-normalized datasets (because numbers in the <kbd>[-1, 1]</kbd> range diminish when squared).</li>
<li><strong>Solution uniqueness</strong>: As can be inferred by its quadratic nature, the L2 function ensures that we will have a unique answer for our search for a minimum. L2 always has a unique solution, but L1 can have many solutions, due to the fact that we can find many paths with minimal length¬†for our models in the form of piecewise linear functions, compared to the single line distance in the case of L2.</li>
</ul>
<p>Regarding usage, the summation of the past properties allows us to use the L2 error type in normal cases, especially because of the solution uniqueness, which gives us the required certainty when starting to minimize error values. In the first example, we will start with a simpler L1 error function for educational purposes.</p>
<p>Let's explore these two approaches by graphing the error results for a sample L1 and L2 loss error function. In the next simple example, we will show you the very different nature of the two errors. In the first two examples, we have normalized the input between <kbd>-1</kbd> and <kbd>1</kbd> and then with values outside that range.</p>
<p>As you can see, from samples¬†<kbd>0</kbd> to <kbd>3</kbd>, the quadratic error increases steadily and continuously, but with non-normalized data it can explode, especially with outliers, as shown in the following code snippet:</p>
<pre>    sampley_=np.array([<span class="hljs-number">.1</span>,<span class="hljs-number">.2</span>,<span class="hljs-number">.3</span>,-<span class="hljs-number">.4</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>])
    sampley=np.array([<span class="hljs-number">.2</span>,-<span class="hljs-number">.2</span>,<span class="hljs-number">.6</span>,<span class="hljs-number">.10</span>, <span class="hljs-number">2</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, -<span class="hljs-number">1</span>])

    ax.set_title(<span class="hljs-string">'Sigmoid variants in b'</span>)
    plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))
    ax=plt.subplot()
    plt.plot(sampley_ - sampley, label=<span class="hljs-string">'L1'</span>)
    plt.plot(np.power((sampley_ - sampley),<span class="hljs-number">2</span>), label=<span class="hljs-string">"L2"</span>)
    ax.set_title(<span class="hljs-string">'L1 vs L2 initial comparison'</span>)
    plt.legend(loc=<span class="hljs-string">'best'</span>)
    plt.show()</pre>
<p>Let's take a look at the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="477" width="542" class="alignnone size-full wp-image-676 image-border" src="assets/1ff32091-af2b-40f5-9e24-283a72afb96f.png"/></div>
<p>Let's define the loss functions in the form of a <kbd>LossFunction</kbd> class and a <kbd>getLoss</kbd> method for the L1 and L2 loss function types, receiving two NumPy arrays as parameters, <kbd>y_</kbd>, or the estimated function value, and <kbd>y</kbd>, the expected value:</p>
<pre><span class="hljs-class"><span class="hljs-keyword">    class</span> <span class="hljs-title">LossFunction</span>:</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getLoss</span><span class="hljs-params">(y_ , y )</span>:</span>
            <span class="hljs-keyword">raise</span> NotImplementedError

<span class="hljs-class"><span class="hljs-keyword">    class</span> <span class="hljs-title">L1</span><span class="hljs-params">(LossFunction)</span>:</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getLoss</span><span class="hljs-params">(y_, y)</span>:</span>
            <span class="hljs-keyword">return</span> np.sum (y_ - y)

<span class="hljs-class"><span class="hljs-keyword">    class</span> <span class="hljs-title">L2</span><span class="hljs-params">(LossFunction)</span>:</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getLoss</span><span class="hljs-params">(y_, y)</span>:</span>
            <span class="hljs-keyword">return</span> np.sum (np.power((y_ - y),<span class="hljs-number">2</span>))</pre>
<p>Now it's time to define the goal function, which we will define as a simple <kbd>Boolean</kbd>. In order to allow faster convergence, it will have a direct relationship between the first input variable and the function's outcome:</p>
<pre><span class="hljs-comment">    # input dataset</span>
    X = np.array([  [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],
                    [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],
                    [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],
                    [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>] ])

<span class="hljs-comment">    # output dataset            </span>
    y = np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]]).T</pre>
<p>The first model we will use is a very minimal neural network with three cells and a weight for each one, without bias, in order to keep the model's complexity to a minimum:</p>
<pre class="mce-root">    # initialize weights randomly with mean 0
    W = 2*np.random.random((3,1)) - 1
    print (W)</pre>
<p class="mce-root">Take a look at the following output generated by running the preceding code:</p>
<pre>    [[ <span class="hljs-number">0.52014909</span>]
     [-<span class="hljs-number">0.25361738</span>]
     [ <span class="hljs-number">0.165037</span>  ]]</pre>
<p>Then we will define a set of variables to collect the model's error, the weights, and training results progression:</p>
<pre>    errorlist=np.empty(<span class="hljs-number">3</span>);
    weighthistory=np.array(<span class="hljs-number">0</span>)
    resultshistory=np.array(<span class="hljs-number">0</span>)</pre>
<p>Then it's time to do the iterative error minimization. In this case, it will consist of feeding the whole true table¬†<span>100 times¬†</span><span>via the weights and the neuron</span><span>'</span><span>s transfer function, adjusting the weights in the direction of the error.</span></p>
<p>Note that this model doesn't use a learning rate, so it should converge (or diverge) quickly:</p>
<pre><span class="hljs-keyword">    for</span> iter <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>):

        <span class="hljs-comment"># forward propagation</span>
        l0 = X
        l1 = Sigmoid.getTransferFunction(np.dot(l0,W))
        resultshistory = np.append(resultshistory , l1)

        <span class="hljs-comment"># Error calculation</span>
        l1_error = y - l1
        errorlist=np.append(errorlist, l1_error)

        <span class="hljs-comment"># Back propagation 1: Get the deltas</span>
        l1_delta = l1_error * Sigmoid.getTransferFunctionDerivative(l1)

        <span class="hljs-comment"># update weights</span>
        W += np.dot(l0.T,l1_delta)
        weighthistory=np.append(weighthistory,W)</pre>
<p>Let's simply review the last evaluation step by printing the output values at <kbd>l1</kbd>. Now we can see that we are reflecting quite literally the output of the original function:</p>
<pre class="mce-root"><span class="hljs-keyword">    print</span> (l1)</pre>
<p>Take a look at the following output, which is generated by running the preceding code:</p>
<pre>    [[ <span class="hljs-number">0.11510625</span>]
     [ <span class="hljs-number">0.08929355</span>]
     [ <span class="hljs-number">0.92890033</span>]
     [ <span class="hljs-number">0.90781468</span>]]</pre>
<p>To better understand the process, let's have a look at how the parameters change over time. First, let's graph the neuron weights. As you can see, they go from a random state to accepting the whole values of the first column (which is always right), going to almost <kbd>0</kbd> for the second column (which is right 50% of the time), and then going to <kbd>-2</kbd> for the third (mainly because it has to trigger <kbd>0</kbd> in the first two elements of the table):</p>
<pre>    plt.figure(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>))
<span class="hljs-keyword">    print</span> (W)
    plt.imshow(np.reshape(weighthistory[<span class="hljs-number">1</span>:],(-<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))[:<span class="hljs-number">40</span>], cmap=plt.cm.gray_r,     <br/>    interpolation=<span class="hljs-string">'nearest'</span>);</pre>
<p><span>Take a look at the following output, which is generated by running the preceding code:</span></p>
<pre class="mce-root">[[ <span class="hljs-number">4.62194116</span>]
 [-<span class="hljs-number">0.28222595</span>]
 [-<span class="hljs-number">2.04618725</span>]]</pre>
<p>Let's take a look at the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="357" width="714" class="alignnone size-full wp-image-678 image-border" src="assets/448f7444-c937-497c-a880-31f062970776.png"/></div>
<p>Let's also review how our solutions evolved (during the first 40 iterations) until we reached the last iteration; we can clearly see the convergence to the ideal values:</p>
<pre>    plt.figure(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>))
    plt.imshow(np.reshape(resultshistory[<span class="hljs-number">1</span>:], (-<span class="hljs-number">1</span>,<span class="hljs-number">4</span>))[:<span class="hljs-number">40</span>], cmap=plt.cm.gray_r,     <br/>    interpolation=<span class="hljs-string">'nearest'</span>);</pre>
<p><span>Let's take a look at the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="377" width="566" class="alignnone size-full wp-image-679 image-border" src="assets/450b72d6-0559-44d8-b979-24378328f577.png"/></div>
<p>We can see how the error evolves and tends to be zero through the different epochs. In this case, we can observe that it swings from negative to positive, which is possible because we first used an L1 error function:</p>
<pre>    plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))
    plt.plot(errorlist);</pre>
<p><span>Let's take a look at the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="428" width="501" class="alignnone size-full wp-image-680 image-border" src="assets/e6643314-6b1d-490c-937b-8c278c173ec0.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Depiction of the decreasing training error of our simple Neural Network</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we took a very important step towards solving complex problems together by means of implementing our first neural network. Now, the following architectures will have familiar elements, and we will be able to extrapolate the knowledge aquired on this chapter, to novel architectures.</p>
<p>In the next chapter, we will explore more complex models and problems, using more layers and special configurations, such as convolutional and dropout layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Refer to the following content:</span></p>
<ul>
<li class="mce-root">McCulloch, Warren S., and Walter Pitts<em>,. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics 5.4 (1943): 115-133.</em> Kleene, Stephen Cole. Representation of events in nerve nets and finite automata. No. RAND-RM-704. RAND PROJECT AIR FORCE SANTA MONICA CA, 1951.</li>
<li>Farley, B. W. A. C., and W. Clark<em>,</em> <em>Simulation of self-organizing systems by digital computer.</em> Transactions of the IRE Professional Group on Information Theory 4.4 (1954): 76-84.</li>
<li>Rosenblatt, Frank<em>, The perceptron: A probabilistic model for information storage and organization in the brain,</em> Psychological review 65.6 (1958): 386.Rosenblatt, Frank. x.</li>
<li>Principles of Neurodynamics: perceptrons and the Theory of Brain Mechanisms. Spartan Books, Washington DC, 1961</li>
<li>Werbos, P.J. (1975), <em>Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.</em></li>
<li>Preparata, Franco P., and Michael Ian Shamos<em>,. "Introduction." Computational Geometry. Springer New York, 1985. 1-35.</em></li>
<li>Rumelhart, David E., Geoffrey E. Hinton, and Ronald J<em>, Williams. Learning internal representations by error propagation. No. ICS-8506. California Univ San Diego La Jolla Inst for Cognitive Science, 1985.</em></li>
<li>Rumelhart, James L. McClelland, and the PDP research group. <em>Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundation. MIT Press, 1986.</em></li>
<li class="mce-root">Cybenko, G. 1989<em>. Approximation by superpositions of a sigmoidal function Mathematics of Control, Signals, and Systems, 2(4), 303‚Äì314.</em></li>
<li>Murtagh, Fionn<em>. Multilayer perceptrons for classification and regression. Neurocomputing 2.5 (1991): 183-197.</em></li>
<li>Schmidhuber, J√ºrgen<em>. Deep learning in neural networks: An overview. Neural networks 61 (2015): 85-117.</em></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>