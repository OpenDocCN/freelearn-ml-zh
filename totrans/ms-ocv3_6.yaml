- en: Face Recognition Using Eigenfaces or Fisherfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Face detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a machine-learning algorithm from collected faces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finishing touches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to face recognition and face detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Face recognition is the process of putting a label to a known face. Just like
    humans learn to recognize their family, friends, and celebrities just by seeing
    their face, there are many techniques for a computer to learn to recognize a known
    face. These generally involve four main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Face detection**: This is the process of locating a face region in an image
    (a large rectangle near the center of the following screenshot). This step does
    not care who the person is, just that it is a human face.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Face preprocessing**: This is the process of adjusting the face image to
    look more clear and similar to other faces (a small grayscale face in the top-center
    of the following screenshot).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Collecting and learning faces**: This is the process of saving many preprocessed
    faces (for each person that should be recognized), and then learning how to recognize
    them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Face recognition**: This is the process that checks which of the collected
    people are most similar to the face in the camera (a small rectangle on the top-right
    of the following screenshot).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the phrase **face recognition** is often used by the general public
    for finding positions of faces (that is, face detection, as described in step
    1), but this book will use the formal definition of face recognition referring
    to step 4 and face detection referring to *step 1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the final `WebcamFaceRec` project, including
    a small rectangle at the top-right corner highlighting the recognized person.
    Also notice the confidence bar that is next to the preprocessed face (a small
    face at the top-center of the rectangle marking the face), which in this case
    shows roughly 70 percent confidence that it has recognized the correct person:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The current face detection techniques are quite reliable in real-world conditions,
    whereas current face recognition techniques are much less reliable when used in
    real-world conditions. For example, it is easy to find research papers showing
    face recognition accuracy rates above 95 percent, but when testing those same
    algorithms yourself, you may often find that accuracy is lower than 50 percent.
    This comes from the fact that current face recognition techniques are very sensitive
    to exact conditions in the images, such as the type of lighting, direction of
    lighting and shadows, exact orientation of the face, expression of the face, and
    the current mood of the person. If they are all kept constant when training (collecting
    images) as well as when testing (from the camera image), then face recognition
    should work well, but if the person was standing to the left-hand side of the
    lights in a room when training, and then stood to the right-hand side while testing
    with the camera, it may give quite bad results. So the dataset used for training
    is very important.
  prefs: []
  type: TYPE_NORMAL
- en: Face preprocessing (*step 2*) aims to reduce these problems, such as by making
    sure the face always appears to have similar brightness and contrast, and perhaps
    making sure the features of the face will always be in the same position (such
    as aligning the eyes and/or nose to certain positions). A good face preprocessing
    stage will help improve the reliability of the whole face recognition system,
    so this chapter will place some emphasis on face preprocessing methods.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the big claims about face recognition for security in the media, it
    is unlikely that the current face recognition methods alone are reliable enough
    for any true security system, but they can be used for purposes that don't need
    high reliability, such as playing personalized music for different people entering
    a room or a robot that says your name when it sees you. There are also various
    practical extensions to face recognition, such as gender recognition, age recognition,
    and emotion recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 - face detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until the year 2000, there were many different techniques used for finding faces,
    but all of them were either very slow, very unreliable, or both. A major change
    came in 2001 when Viola and Jones invented the Haar-based cascade classifier for
    object detection, and in 2002 when it was improved by Lienhart and Maydt. The
    result is an object detector that is both fast (it can detect faces in real time
    on a typical desktop with a VGA webcam) and reliable (it detects approximately
    95 percent of frontal faces correctly). This object detector revolutionized the
    field of face recognition (as well as that of robotics and computer vision in
    general), as it finally allowed real-time face detection and face recognition,
    especially as Lienhart himself wrote the object detector that comes free with
    OpenCV! It works not only for frontal faces but also side-view faces (referred
    to as profile faces), eyes, mouths, noses, company logos, and many other objects.
  prefs: []
  type: TYPE_NORMAL
- en: This object detector was extended in OpenCV v2.0 to also use LBP features for
    detection based on work by Ahonen, Hadid, and Pietikäinen in 2006, as LBP-based
    detectors are potentially several times faster than Haar-based detectors, and
    don't have the licensing issues that many Haar detectors have.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea of the Haar-based face detector is that if you look at most frontal
    faces, the region with the eyes should be darker than the forehead and cheeks,
    and the region with the mouth should be darker than cheeks, and so on. It typically
    performs about 20 stages of comparisons like this to decide if it is a face or
    not, but it must do this at each possible position in the image and for each possible
    size of the face, so in fact it often does thousands of checks per image. The
    basic idea of the LBP-based face detector is similar to the Haar-based one, but
    it uses histograms of pixel intensity comparisons, such as edges, corners, and
    flat regions.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than have a person decide which comparisons would best define a face,
    both Haar- and LBP-based face detectors can be automatically trained to find faces
    from a large set of images, with the information stored as XML files to be used
    later. These cascade classifier detectors are typically trained using at least
    1,000 unique face images and 10,000 non-face images (for example, photos of trees,
    cars, and text), and the training process can take a long time even on a multi-core
    desktop (typically a few hours for LBP but 1week for Haar!). Luckily, OpenCV comes
    with some pretrained Haar and LBP detectors for you to use! In fact you can detect
    frontal faces, profile (side-view) faces, eyes, or noses just by loading different
    cascade classifier XML files to the object detector, and choose between the Haar
    or LBP detector, based on which XML file you choose.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing face detection using OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned previously, OpenCV v2.4 comes with various, pretrained XML detectors
    that you can use for different purposes. The following table lists some of the
    most popular XML files:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type of cascade classifier** | **XML filename** |'
  prefs: []
  type: TYPE_TB
- en: '| Face detector (default) | `haarcascade_frontalface_default.xml` |'
  prefs: []
  type: TYPE_TB
- en: '| Face detector (fast Haar) | `haarcascade_frontalface_alt2.xml` |'
  prefs: []
  type: TYPE_TB
- en: '| Face detector (fast LBP) | `lbpcascade_frontalface.xml` |'
  prefs: []
  type: TYPE_TB
- en: '| Profile (side-looking) face detector | `haarcascade_profileface.xml` |'
  prefs: []
  type: TYPE_TB
- en: '| Eye detector (separate for left and right) | `haarcascade_lefteye_2splits.xml`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mouth detector | `haarcascade_mcs_mouth.xml` |'
  prefs: []
  type: TYPE_TB
- en: '| Nose detector | `haarcascade_mcs_nose.xml` |'
  prefs: []
  type: TYPE_TB
- en: '| Whole person detector | `haarcascade_fullbody.xml` |'
  prefs: []
  type: TYPE_TB
- en: Haar-based detectors are stored in the `datahaarcascades` folder and LBP-based
    detectors are stored in the `datalbpcascades` folder of the OpenCV root folder,
    such as `C:opencvdatalbpcascades`.
  prefs: []
  type: TYPE_NORMAL
- en: For our face recognition project, we want to detect frontal faces, so let's
    use the LBP face detector because it is the fastest and doesn't have patent licensing
    issues. Note that this pretrained LBP face detector that comes with OpenCV v2.x
    is not tuned as well as the pretrained Haar face detectors, so if you want more
    reliable face detection then you may want to train your own LBP face detector
    or use a Haar face detector.
  prefs: []
  type: TYPE_NORMAL
- en: Loading a Haar or LBP detector for object or face detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform object or face detection, first you must load the pretrained XML
    file using OpenCV''s `CascadeClassifier` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This can load Haar or LBP detectors just by giving a different filename. A
    very common mistake when using this is to provide the wrong folder or filename,
    but depending on your build environment, the `load()` method will either return
    `false` or generate a C++ exception (and exit your program with an assert error).
    So it is best to surround the `load()` method with a `try... catch` block and
    display a nice error message to the user if something went wrong. Many beginners
    skip checking for errors, but it is crucial to show a help message to the user
    when something did not load correctly, otherwise you may spend a very long time
    debugging other parts of your code before eventually realizing something did not
    load. A simple error message can be displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Accessing the webcam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To grab frames from a computer's webcam or even from a video file, you can simply
    call the `VideoCapture::open()` function with the camera number or video filename,
    then grab the frames using the C++ stream operator, as mentioned in the section,*Accessing
    the webcam* in [Chapter 1](03913e76-ec18-4a31-875e-dfceca32d26f.xhtml), *Cartoonifier
    and Skin Changer for Raspberry Pi*.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting an object using the Haar or LBP Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have loaded the classifier (just once during initialization), we
    can use it to detect faces in each new camera frame. But first, we should do some
    initial processing of the camera image just for face detection, by performing
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Grayscale color conversion**: Face detection only works on grayscale images.
    So we should convert the color camera frame to grayscale.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Shrinking the camera image**: The speed of face detection depends on the
    size of the input image (it is very slow for large images but fast for small images),
    and yet detection is still fairly reliable even at low resolutions. So we should
    shrink the camera image to a more reasonable size (or use a large value for `minFeatureSize`
    in the detector, as explained shortly).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Histogram equalization**: Face detection is not as reliable in low-light
    conditions. So we should perform histogram equalization to improve the contrast
    and brightness.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Grayscale color conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can easily convert an RGB color image to grayscale using the `cvtColor()`
    function. But we should do this only if we know we have a color image (that is,
    it is not a grayscale camera), and we must specify the format of our input image
    (usually 3-channel BGR on desktop or 4-channel BGRA on mobile). So we should allow
    three different input color formats, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Shrinking the camera image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use the `resize()` function to shrink an image to a certain size or
    scale factor. Face detection usually works quite well for any image size greater
    than 240x240 pixels (unless you need to detect faces that are far away from the
    camera), because it will look for any faces larger than the `minFeatureSize` (typically
    20x20 pixels). So let''s shrink the camera image to be 320 pixels wide; it doesn''t
    matter if the input is a VGA webcam or a five mega pixel HD camera. It is also
    important to remember and enlarge the detection results, because if you detect
    faces in a shrunk image then the results will also be shrunk. Note that instead
    of shrinking the input image, you could use a large value for the `minFeatureSize` variable
    in the detector instead. We must also ensure the image does not become fatter
    or thinner. For example, a widescreen 800x400 image when shrunk to 300x200 would
    make a person look thin. So we must keep the aspect ratio (the ratio of width
    to height) of the output the same as the input. Let''s calculate how much to shrink
    the image width by, then apply the same scale factor to the height as well, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Histogram equalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can easily perform histogram equalization to improve the contrast and brightness
    of an image, using the `equalizeHist()` function. Sometimes this will make the
    image look strange, but in general it should improve the brightness and contrast
    and help face detection. The `equalizeHist()` function is used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Detecting the face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have converted the image to grayscale, shrunk the image, and equalized
    the histogram, we are ready to detect the faces using the `CascadeClassifier::detectMultiScale()`
    function! There are many parameters that we pass to this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`minFeatureSize`: This parameter determines the minimum face size that we care
    about, typically 20x20 or 30x30 pixels but this depends on your use case and image
    size. If you are performing face detection on a webcam or smartphone where the
    face will always be very close to the camera, you could enlarge this to 80 x 80
    to have much faster detections, or if you want to detect far away faces, such
    as on a beach with friends, then leave this as 20x20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`searchScaleFactor`: This parameter determines how many different sizes of
    faces to look for; typically it would be `1.1`, for good detection, or `1.2` for
    faster detection that does not find the face as often.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minNeighbors`: This parameter determines how sure the detector should be that
    it has detected a face, typically a value of `3` but you can set it higher if
    you want more reliable faces, even if many faces are not detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flags`: This parameter allows you to specify whether to look for all faces
    (default) or only look for the largest face (`CASCADE_FIND_BIGGEST_OBJECT`). If
    you only look for the largest face, it should run faster. There are several other
    parameters you can add to make the detection about 1% or 2% faster, such as `CASCADE_DO_ROUGH_SEARCH`
    or `CASCADE_SCALE_IMAGE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output of the `detectMultiScale()` function will be a `std::vector` of
    the `cv::Rect` type object. For example, if it detects two faces then it will
    store an array of two rectangles in the output. The `detectMultiScale()` function
    is used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can see if any faces were detected by looking at the number of elements stored
    in our vector of rectangles; that is, by using the `objects.size()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, if we gave a shrunken image to the face detector, the
    results will also be shrunk, so we need to enlarge them if we want to know the
    face regions for the original image. We also need to make sure faces on the border
    of the image stay completely within the image, as OpenCV will now raise an exception
    if this happens, as shown by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the preceding code will look for all faces in the image, but if you
    only care about one face, then you could change the flag variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `WebcamFaceRec` project includes a wrapper around OpenCV''s Haar or LBP
    detector, to make it easier to find a face or eye within an image. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a face rectangle, we can use it in many ways, such as to extract
    or crop the face image from the original image. The following code allows us to
    access the face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows the typical rectangular region given by the face
    detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Step 2 - face preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, face recognition is extremely vulnerable to changes in
    lighting conditions, face orientation, face expression, and so on, so it is very
    important to reduce these differences as much as possible. Otherwise the face
    recognition algorithm will often think there is more similarity between faces
    of two different people in the same conditions than between two faces of the same
    person.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest form of face preprocessing is just to apply histogram equalization
    using the `equalizeHist()` function, like we just did for face detection. This
    may be sufficient for some projects where the lighting and positional conditions
    won't change by much. But for reliability in real-world conditions, we need many
    sophisticated techniques, including facial feature detection (for example, detecting
    eyes, nose, mouth, and eyebrows). For simplicity, this chapter will just use eye
    detection and ignore other facial features such as the mouth and nose, which are
    less useful. The following image shows an enlarged view of a typical preprocessed
    face, using the techniques that will be covered in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Eye detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Eye detection can be very useful for face preprocessing, because for frontal
    faces you can always assume a person's eyes should be horizontal and on opposite
    locations of the face and should have a fairly standard position and size within
    a face, despite changes in facial expressions, lighting conditions, camera properties,
    distance to camera, and so on. It is also useful to discard false positives when
    the face detector says it has detected a face and it is actually something else.
    It is rare that the face detector and two eye detectors will all be fooled at
    the same time, so if you only process images with a detected face and two detected
    eyes then it will not have many false positives (but will also give fewer faces
    for processing, as the eye detector will not work as often as the face detector).
  prefs: []
  type: TYPE_NORMAL
- en: Some of the pretrained eye detectors that come with OpenCV v2.4 can detect an
    eye whether it is open or closed, whereas some of them can only detect open eyes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Eye detectors that detect open or closed eyes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`haarcascade_mcs_lefteye.xml` (and `haarcascade_mcs_righteye.xml`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`haarcascade_lefteye_2splits.xml` (and `haarcascade_righteye_2splits.xml`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eye detectors that detect open eyes only are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`haarcascade_eye.xml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`haarcascade_eye_tree_eyeglasses.xml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the open or closed eye detectors specify which eye they are trained on, you
    need to use a different detector for the left and the right eye, whereas the detectors
    for just open eyes can use the same detector for left or right eyes.
  prefs: []
  type: TYPE_NORMAL
- en: The detector `haarcascade_eye_tree_eyeglasses.xml` can detect the eyes if the
    person is wearing glasses, but is not reliable if they don't wear glasses.
  prefs: []
  type: TYPE_NORMAL
- en: If the XML filename says *left eye*, it means the actual left eye of the person,
    so in the camera image it would normally appear on the right-hand side of the
    face, not on the left-hand side!
  prefs: []
  type: TYPE_NORMAL
- en: The list of four eye detectors mentioned is ranked in approximate order from
    most reliable to least reliable, so if you know you don't need to find people
    with glasses then the first detector is probably the best choice.
  prefs: []
  type: TYPE_NORMAL
- en: Eye search regions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For eye detection, it is important to crop the input image to just show the
    approximate eye region, just like doing face detection and then cropping to just
    a small rectangle where the left eye should be (if you are using the left eye
    detector) and the same for the right rectangle for the right eye detector. If
    you just do eye detection on a whole face or whole photo then it will be much
    slower and less reliable. Different eye detectors are better suited to different
    regions of the face; for example, the `haarcascade_eye.xml` detector works best
    if it only searches in a very tight region around the actual eye, whereas the
    `haarcascade_mcs_lefteye.xml` and `haarcascade_lefteye_2splits.xml` detectors
    work best when there is a large region around the eye.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists some good search regions of the face for different
    eye detectors (when using the LBP face detector), using relative coordinates within
    the detected face rectangle:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cascade classifier** | **EYE_SX** | **EYE_SY** | **EYE_SW** | **EYE_SH**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `haarcascade_eye.xml` | 0.16 | 0.26 | 0.30 | 0.28 |'
  prefs: []
  type: TYPE_TB
- en: '| `haarcascade_mcs_lefteye.xml` | 0.10 | 0.19 | 0.40 | 0.36 |'
  prefs: []
  type: TYPE_TB
- en: '| `haarcascade_lefteye_2splits.xml` | 0.12 | 0.17 | 0.37 | 0.36 |'
  prefs: []
  type: TYPE_TB
- en: Here is the source code to extract the left-eye and right-eye regions from a
  prefs: []
  type: TYPE_NORMAL
- en: 'detected face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows the ideal search regions for the different eye detectors,
    where the `haarcascade_eye.xml` and `haarcascade_eye_tree_eyeglasses.xml` files
    are best with the small search region, while the `haarcascade_mcs_*eye.xml` and
    `haarcascade_*eye_2splits.xml` files are best with larger search regions. Note
    that the detected face rectangle is also shown, to give an idea of how large the
    eye search regions are compared to the detected face rectangle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When using the eye search regions given in the preceding table, here are the
    approximate detection properties of the different eye detectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cascade classifier** | **Reliability*** | **Speed**** | **Eyes found**
    | **Glasses** |'
  prefs: []
  type: TYPE_TB
- en: '| `haarcascade_mcs_lefteye.xml` | 80% | 18 msec | Open or closed | no |'
  prefs: []
  type: TYPE_TB
- en: '| `haarcascade_lefteye_2splits.xml` | 60% | 7 msec | Open or closed | no |'
  prefs: []
  type: TYPE_TB
- en: '| `haarcascade_eye.xml` | 40% | 5 msec | Open only | no |'
  prefs: []
  type: TYPE_TB
- en: '| `haarcascade_eye_tree_eyeglasses.xml` | 15% | 10 msec | Open only | yes |'
  prefs: []
  type: TYPE_TB
- en: '*** Reliability** values show how often both eyes will be detected after LBP
    frontal face detection when no eyeglasses are worn and both eyes are open. If
    eyes are closed then the reliability may drop, or if eyeglasses are worn then
    both reliability and speed will drop.'
  prefs: []
  type: TYPE_NORMAL
- en: '**** Speed** values are in milliseconds for images scaled to the size of 320x240
    pixels on an Intel Core i7 2.2 GHz (averaged across 1,000 photos). Speed is typically
    much faster when eyes are found than when eyes are not found, as it must scan
    the entire image, but the `haarcascade_mcs_lefteye.xml` is still much slower than
    the other eye detectors.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you shrink a photo to 320x240 pixels, perform a histogram equalization
    on it, use the LBP frontal face detector to get a face, then extract the
  prefs: []
  type: TYPE_NORMAL
- en: '*left-eye-region* and *right-eye-region* from the face using the `haarcascade_mcs_lefteye.xml`
    values, then perform a histogram equalization on each eye region. Then if you
    the `haarcascade_mcs_lefteye.xml` detector on the left eye (which is actually
    on the top-right side of your image) and use the `haarcascade_mcs_righteye.xml`
    detector on the right eye (the top-left part of your image), each eye detector
    should work in roughly 90 percent of photos with LBP-detected frontal faces. So
    if you want both eyes detected then it should work in roughly 80 percent'
  prefs: []
  type: TYPE_NORMAL
- en: of photos with LBP-detected frontal faces.
  prefs: []
  type: TYPE_NORMAL
- en: Note that while it is recommended to shrink the camera image before detecting
    faces, you should detect eyes at the full camera resolution because eyes will
    obviously be much smaller than faces, so you need as much resolution as you can
    get.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the table, it seems that when choosing an eye detector to use, you
    should decide whether you want to detect closed eyes or only open eyes. And remember
    that you can even use a one eye detector, and if it does not detect an eye then
    you can try with another one.
  prefs: []
  type: TYPE_NORMAL
- en: For many tasks, it is useful to detect eyes whether they are opened or closed,
    so if speed is not crucial, it is best to search with the `mcs_*eye` detector
    first, and if it fails then search with the `eye_2splits` detector.
  prefs: []
  type: TYPE_NORMAL
- en: But for face recognition, a person will appear quite different if their eyes
    are closed, so it is best to search with the plain `haarcascade_eye` detector
    first, and if it fails then search with the `haarcascade_eye_tree_eyeglasses`
    detector.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the same `detectLargestObject()` function we used for face detection
    to search for eyes, but instead of asking to shrink the images before eye detection,
    we specify the full eye region width to get a better eye detection. It is easy
    to search for the left eye using one detector, and if it fails then try another
    detector (same for right eye). The eye detection is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'With the face and both eyes detected, we''ll perform face preprocessing by
    combining:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Geometrical transformation and cropping**: This process would include scaling,
    rotating, and translating the images so that the eyes are aligned, followed by
    the removal of the forehead, chin, ears, and background from the face image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Separate histogram equalization for left and right sides**: This process
    standardizes the brightness and contrast on both the left- and right-hand sides
    of the face independently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smoothing**: This process reduces the image noise using a bilateral filter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elliptical mask**: The elliptical mask removes some remaining hair and background
    from the face image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image shows the face preprocessing steps 1 to 4 applied to a
    detected face. Notice how the final image has good brightness and contrast on
    both sides of the face, whereas the original does not:'
  prefs: []
  type: TYPE_NORMAL
- en: Geometrical transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important that the faces are all aligned together, otherwise the face-recognition
    algorithm might be comparing part of a nose with part of an eye, and so on. The
    output of face detection just seen will give aligned faces to some extent, but
    it is not very accurate (that is, the face rectangle will not always be starting
    from the same point on the forehead).
  prefs: []
  type: TYPE_NORMAL
- en: 'To have better alignment, we will use eye detection to align the face so the
    positions of the two detected eyes line up perfectly in the desired positions.
    We will do the geometrical transformation using the `warpAffine()` function, which
    is a single operation that will do four things:'
  prefs: []
  type: TYPE_NORMAL
- en: Rotate the face so that the two eyes are horizontal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale the face so that the distance between the two eyes is always the same
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translate the face so that the eyes are always centered horizontally and at
    a desired height
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crop the outer parts of the face, since we want to crop away the image background,
    hair, forehead, ears, and chin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Affine Warping takes an affine matrix that transforms the two detected eye
    locations to the two desired eye locations, and then crops to a desired size and
    position. To generate this affine matrix, we will get the center between the eyes,
    calculate the angle at which the two detected eyes appear, and look at their distance
    apart as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can transform the face (rotate, scale, and translate) to get the two
    detected eyes to be in the desired eye positions in an ideal face as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Separate histogram equalization for left and right sides
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In real-world conditions, it is common to have strong lighting on one half of
    the face and weak lighting on the other. This has an enormous effect on the face-recognition
    algorithm, as the left- and right-hand sides of the same face will seem like very
    different people. So we will perform histogram equalization separately on the
    left and right halves of the face, to have standardized brightness and contrast
    on each side of the face.
  prefs: []
  type: TYPE_NORMAL
- en: If we simply applied histogram equalization on the left half and then again
    on the right half, we would see a very distinct edge in the middle because the
    average brightness is likely to be different on the left and the right side, so
    to remove this edge, we will apply the two histogram equalizations gradually from
    the left-or right-hand side towards the center and mix it with a whole-face histogram
    equalization, so that the far left-hand side will use the left histogram equalization,
    the far right-hand side will use the right histogram equalization, and the center
    will use a smooth mix of the left or right value and the whole-face equalized
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows how the left-equalized, whole-equalized, and right-equalized
    images are blended together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_7_6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To perform this, we need copies of the whole face equalized as well as the
    left half equalized and the right half equalized, which is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we combine the three images together. As the images are small, we can easily
    access pixels directly using the `image.at<uchar>(y,x)` function even if it is
    slow; so let''s merge the three images by directly accessing pixels in the three
    input images and output images, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This separated histogram equalization should significantly help reduce the effect
    of different lighting on the left- and right-hand sides of the face, but we must
    understand that it won't completely remove the effect of one-sided lighting, since
    the face is a complex 3D shape with many shadows.
  prefs: []
  type: TYPE_NORMAL
- en: Smoothing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To reduce the effect of pixel noise, we will use a bilateral filter on the
    face, as a bilateral filter is very good at smoothing, most of an image while
    keeping edges sharp. Histogram equalization can significantly increase the pixel
    noise, so we will make the filter strength `20` to cover heavy pixel noise, but
    use a neighborhood of just two pixels as we want to heavily smooth the tiny pixel
    noise but not the large image regions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Elliptical mask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although we have already removed most of the image background and forehead
    and hair when we did the geometrical transformation, we can apply an elliptical
    mask to remove some of the corner region such as the neck, which might be in shadow
    from the face, particularly if the face is not looking perfectly straight towards
    the camera. To create the mask, we will draw a black-filled ellipse onto a white
    image. One ellipse to perform this has a horizontal radius of 0.5 (that is, it
    covers the face width perfectly), a vertical radius of 0.8 (as faces are usually
    taller than they are wide), and centered at the coordinates 0.5, 0.4, as shown
    in the following image, where the elliptical mask has removed some unwanted corners
    from the face:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_7_7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can apply the mask when calling the `cv::setTo()` function, which would
    normally set a whole image to a certain pixel value, but as we will give a mask
    image, it will only set some parts to the given pixel value. We will fill the
    image in gray so that it should have less contrast to the rest of the face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following enlarged image shows a sample result from all the face preprocessing
    stages. Notice it is much more consistent for face recognition at a different
    brightness, face rotations, angle from camera, backgrounds, positions of lights,
    and so on. This preprocessed face will be used as input to the face-recognition
    stages, both when collecting faces for training, and when trying to recognize
    input faces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_7_8.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 3 - Collecting faces and learning from them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Collecting faces can be just as simple as putting each newly preprocessed face
    into an array of preprocessed faces from the camera, as well as putting a label
    into an array (to specify which person the face was taken from). For example,
    you could use 10 preprocessed faces of the first person and 10 preprocessed faces
    of a second person, so the input to the face-recognition algorithm will be an
    array of 20 preprocessed faces and an array of 20 integers (where the first 10
    numbers are 0 and the next 10 numbers are 1).
  prefs: []
  type: TYPE_NORMAL
- en: The face-recognition algorithm will then learn how to distinguish between the
    faces of the different people. This is referred to as the training phase and the
    collected faces are referred to as the training set. After the face-recognition
    algorithm has finished training, you could then save the generated knowledge to
    a file or memory and later use it to recognize which person is seen in front of
    the camera. This is referred to as the testing phase. If you used it directly
    from a camera input then the preprocessed face would be referred to as the test
    image, and if you tested with many images (such as from a folder of image files),
    it would be referred to as the testing set.
  prefs: []
  type: TYPE_NORMAL
- en: It is important that you provide a good training set that covers the types of
    variations you expect to occur in your testing set. For example, if you will only
    test with faces that are looking perfectly straight ahead (such as ID photos),
    then you only need to provide training images with faces that are looking perfectly
    straight ahead. But if the person might be looking to the left or up, then you
    should make sure the training set will also include faces of that person doing
    this, otherwise the face-recognition algorithm will have trouble recognizing them,
    as their face will appear quite different. This also applies to other factors
    such as facial expression (for example, if the person is always smiling in the
    training set but not smiling in the testing set) or lighting direction (for example,
    a strong light is to the left-hand side in the training set but to the right-hand
    side in the testing set), then the face recognition algorithm will have difficulty
    recognizing them. The face preprocessing steps that we just saw will help reduce
    these issues, but it certainly won't remove these factors, particularly the direction
    in which the face is looking, as it has a large effect on the position of all
    elements in the face.
  prefs: []
  type: TYPE_NORMAL
- en: One way to obtain a good training set that will cover many different real-world
    conditions is for each person to rotate their head from looking left, to up, to
    right, to down, then looking directly straight. Then the person tilts their head
    sideways and then up and down, while also changing their facial expression, such
    as alternating between smiling, looking angry, and having a neutral face. If each
    person follows a routine such as this while collecting faces, then there is a
    much better chance of recognizing everyone in the real-world conditions.
  prefs: []
  type: TYPE_NORMAL
- en: For even better results, it should be performed again with one or two more locations
    or directions, such as by turning the camera around by 180 degrees and walking
    in the opposite direction of the camera and then repeating the whole routine,
    so that the training set would include many different lighting conditions.
  prefs: []
  type: TYPE_NORMAL
- en: So in general, having 100 training faces for each person is likely to give better
    results than having just 10 training faces for each person, but if all 100 faces
    look almost identical then it will still perform badly because it is more important
    that the training set has enough variety to cover the testing set, rather than
    to just have a large number of faces. So to make sure the faces in the training
    set are not all too similar, we should add a noticeable delay between each collected
    face. For example, if the camera is running at 30 frames per second, then it might
    collect 100 faces in just several seconds when the person has not had time to
    move around, so it is better to collect just one face per second, while the person
    moves their face around. Another simple method to improve the variation in the
    training set is to only collect a face if it is noticeably different from the
    previously collected face.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting preprocessed faces for training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make sure there is at least a 1 second gap between collecting new faces,
    we need to measure how much time has passed. This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To compare the similarity of two images, pixel by pixel, you can find the relative
    L2 error, which just involves subtracting one image from the other, summing the
    squared value of it, and then getting the square root of it. So if the person
    had not moved at all, subtracting the current face with the previous face should
    give a very low number at each pixel, but if they had just moved slightly in any
    direction, subtracting the pixels would give a large number and so the L2 error
    will be high. As the result is summed over all pixels, the value will depend on
    the image resolution. So to get the mean error, we should divide this value by
    the total number of pixels in the image. Let''s put this in a handy function,
    `getSimilarity()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This similarity will often be less than 0.2 if the image did not move much,
    and
  prefs: []
  type: TYPE_NORMAL
- en: higher than 0.4 if the image did move, so let's use 0.3 as our threshold for
    collecting
  prefs: []
  type: TYPE_NORMAL
- en: a new face.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many tricks we can play to obtain more training data, such as using
    mirrored faces, adding random noise, shifting the face by a few pixels, scaling
    the face by a percentage, or rotating the face by a few degrees (even though we
    specifically tried to remove these effects when preprocessing the face!). Let''s
    add mirrored faces to the training set, so that we have both a larger training
    set as well as a reduction in the problems of asymmetrical faces or if a user
    is always oriented slightly to the left or right during training but not testing.
    This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This will collect the `std::vector` arrays `preprocessedFaces` and `faceLabels`
    for a preprocessed face as well as the label or ID number of that person (assuming
    it is in the integer `m_selectedPerson` variable).
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it more obvious to the user that we have added their current face to
    the collection, you could provide a visual notification by either displaying a
    large white rectangle over the whole image or just displaying their face for just
    a fraction of a second so they realize a photo was taken. With OpenCV''s C++ interface,
    you can use the `+` overloaded `cv::Mat` operator to add a value to every pixel
    in the image and have it clipped to 255 (using `saturate_cast`, so it doesn''t
    overflow from white back to black!) Assuming `displayedFrame` will be a copy of
    the color camera frame that should be shown, insert this after the preceding code
    for face collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Training the face recognition system from collected faces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After you have collected enough faces for each person to recognize, you must
    train the system to learn the data using a machine-learning algorithm suited for
    face recognition. There are many different face-recognition algorithms in the
    literature, the simplest of which are Eigenfaces and Artificial Neural Networks.
    Eigenfaces tends to work better than ANNs, and despite its simplicity, it tends
    to work almost as well as many more complex face-recognition algorithms, so it
    has become very popular as the basic face-recognition algorithm for beginners
    as well as for new algorithms to be compared to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any reader who wishes to work further on face recognition is recommended to
    read the theory behind:'
  prefs: []
  type: TYPE_NORMAL
- en: Eigenfaces (also referred to as **Principal Component Analysis** (**PCA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fisherfaces (also referred to as **Linear Discriminant Analysis** (**LDA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other classic face recognition algorithms (many are available at [h t t p ://w
    w w . f a c e - r e c . o r g /a l g o r i t h m s /](http://www.face-rec.org/algorithms/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newer face recognition algorithms in recent Computer Vision research papers
    (such as CVPR and ICCV at [http://www.cvpapers.com/](http://www.cvpapers.com/)),
    as there are hundreds of face recognition papers published each year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, you don''t need to understand the theory of these algorithms in order
    to use them as shown in this book. Thanks to the OpenCV team and Philipp Wagner''s
    `libfacerec` contribution, OpenCV v2.4.1 provided `cv::Algo``rithm` as a simple
    and generic method to perform face recognition using one of several different
    algorithms (even selectable at runtime) without necessarily understanding how
    they are implemented. You can find the available algorithms in your version of
    OpenCV by using the `Algorithm::getList()` function, such as with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the three face-recognition algorithms available in OpenCV v2.4.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FaceRecognizer.Eigenfaces`: Eigenfaces, also referred to as PCA, first used
    by Turk and Pentland in 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FaceRecognizer.Fisherfaces`: Fisherfaces, also referred to as LDA, invented
    by Belhumeur, Hespanha, and Kriegman in 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FaceRecognizer.LBPH`: Local Binary Pattern Histograms, invented by Ahonen,
    Hadid, and Pietikäinen in 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information on these face-recognition algorithm implementations can be
    found with documentation, samples, and Python equivalents for each of them on
    Philipp Wagner's websites ([http://bytefish.de/blog](http://bytefish.de/blog)
    and [http://bytefish.de/dev/libfacerec/](http://bytefish.de/dev/libfacerec/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'These face recognition-algorithms are available through the `FaceRecognizer`
    class in OpenCV''s `contrib` module. Due to dynamic linking, it is possible that
    your program is linked to the `contrib` module but it is not actually loaded at
    runtime (if it was deemed as not required). So it is recommended to call the `cv::initModule_contrib()`
    function before trying to access the `FaceRecognizer` algorithms. This function
    is only available from OpenCV v2.4.1, so it also ensures that the face-recognition
    algorithms are at least available to you at compile time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To use one of the face-recognition algorithms, we must create a `FaceRecognizer`
    object using the `cv::Algorithm::create<FaceRecognizer>()` function. We pass the
    name of the face-recognition algorithm we want to use, as a string to this create
    function. This will give us access to that algorithm, if it is available in the
    OpenCV version. So it may be used as a runtime error check to ensure the user
    has OpenCV v2.4.1 or newer. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have loaded the `FaceRecognizer` algorithm, we simply call the `FaceRecognizer::train()`
    function with our collected face data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This one line of code will run the whole face recognition training algorithm
    that you selected (for example, Eigenfaces, Fisherfaces, or potentially other
    algorithms). If you have just a few people with less than 20 faces, then this
    training should return very quickly, but if you have many people with many faces,
    it is possible that `train()` function will take several seconds or even minutes
    to process all the data.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the learned knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it is not necessary, it is quite useful to view the internal data structures
    that the face-recognition algorithm generated when learning your training data,
    particularly if you understand the theory behind the algorithm you selected and
    want to verify if it worked or find out why it is not working as you hoped. The
    internal data structures can be different for different algorithms, but luckily
    they are the same for eigenfaces and fisherfaces, so let's just look at those
    two. They are both based on 1D eigenvector matrices that appear somewhat like
    faces when viewed as 2D images, therefore it is common to refer as eigenvectors
    as eigenfaces when using the **Eigenface** algorithm or as fisherfaces when using
    the **Fisherface** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, the basic principle of Eigenfaces is that it will calculate
    a set of special images (eigenfaces) and blending ratios (eigenvalues), which
    when combined in different ways can generate each of the images in the training
    set but can also be used to differentiate the many face images in the training
    set from each other. For example, if some of the faces in the training set had
    a moustache and some did not, then there would be at least one eigenface that
    shows a moustache, and so the training faces with a moustache would have a high
    blending ratio for that eigenface to show that it has a moustache, and the faces
    without a moustache would have a low blending ratio for that eigenvector. If the
    training set had five people with 20 faces for each person, then there would be
    100 eigenfaces and eigenvalues to differentiate the 100 total faces in the training
    set, and in fact these would be sorted so the first few eigenfaces and eigenvalues
    would be the most critical differentiators, and the last few eigenfaces and eigenvalues
    would just be random pixel noises that don't actually help to differentiate the
    data. So it is common practice to discard some of the last eigenfaces and just
    keep the first 50 or so eigenfaces.
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, the basic principle of Fisherfaces is that instead of calculating
    a special eigenvector and eigenvalue for each image in the training set, it only
    calculates one special eigenvector and eigenvalue for each person. So in the preceding
    example that has fivepeople with 20 faces for each person, the Eigenfaces algorithm
    would use 100 eigenfaces and eigenvalues whereas the Fisherfaces algorithm would
    use just five fisherfaces and eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the internal data structures of the Eigenfaces and Fisherfaces algorithms,
    we must use the `cv::Algorithm::get()` function to obtain them at runtime, as
    there is no access to them at compile time. The data structures are used internally
    as part of mathematical calculations rather than for image processing, so they
    are usually stored as floating-point numbers typically ranging between 0.0 and
    1.0, rather than 8-bit `uchar` pixels ranging from 0 to 255, similar to pixels
    in regular images. Also, they are often either a 1D row or column matrix or they
    make up one of the many 1D rows or columns of a larger matrix. So before you can
    display many of these internal data structures, you must reshape them to be the
    correct rectangular shape, and convert them to 8-bit `uchar` pixels between 0
    and 255\. As the matrix data might range from 0.0 to 1.0 or -1.0 to 1.0 or anything
    else, you can use the `cv::normalize()` function with the `cv::NORM_MINMAX` option
    to make sure it outputs data ranging between 0 and 255 no matter what the input
    range may be. Let''s create a function to perform this reshaping to a rectangle
    and conversion to 8-bit pixels for us as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To make it easier to debug OpenCV code and even more so, when internally debugging
    the `cv::Algorithm` data structure, we can use the `ImageUtils.cpp` and `ImageUtils.h`
    files to display information about a `cv::Mat` structure easily as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see something similar to the following printed to your console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This tells you that it is 640 elements wide and 480 high (that is, a 640 x 480
    image or a 480 x 640 matrix, depending on how you view it), with three channels
    per pixel that are 8-bits each (that is, a regular BGR image), and it shows the
    min and max value in the image for each of the color channels.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to print the actual contents of an image or matrix by using
    the `printMat()` function instead of the `printMatInfo()` function. This is quite
    handy for viewing matrices and multichannel-float matrices as these can be quite
    tricky to view for beginners.
  prefs: []
  type: TYPE_NORMAL
- en: The `ImageUtils` code is mostly for OpenCV's C interface, but is gradually including
    more of the C++ interface over time. The most recent version can be found at [http://shervinemami.info/openCV.html](http://shervinemami.info/openCV.html).
  prefs: []
  type: TYPE_NORMAL
- en: Average face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Both the Eigenfaces and Fisherfaces algorithms first calculate the average
    face that is the mathematical average of all the training images, so they can
    subtract the average image from each facial image to have better face recognition
    results. So let''s view the average face from our training set. The average face
    is named `mean` in the Eigenfaces and Fisherfaces implementations, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now see an average face image on your screen similar to the following
    (enlarged) image that is a combination of a man, a woman, and a baby. You should
    also see similar text to this shown on your console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The image would appear as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_7_9.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that `averageFace (row)` was a single row matrix of 64-bit floats, whereas
    `averageFace` is a rectangular image with 8-bit pixels covering the full range
  prefs: []
  type: TYPE_NORMAL
- en: from 0 to 255.
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvalues, Eigenfaces, and Fisherfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s view the actual component values in the eigenvalues (as text):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'For Eigenfaces, there is one eigenvalue for each face, so if we have three
    people with four faces each, we get a column vector with 12 eigenvalues sorted
    from best to worst as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For Fisherfaces, there is just one eigenvalue for each extra person, so if
    there are three people with four faces each, we just get a row vector with two
    eigenvalues as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the eigenvectors (as Eigenface or Fisherface images), we must extract
    them as columns from the big eigenvectors matrix. As data in OpenCV and C/C++
    is normally stored in matrices using row-major order, it means that to extract
    a column, we should use the `Mat::clone()` function to ensure the data will be
    continuous, otherwise we can''t reshape the data to a rectangle. Once we have
    a continuous column `Mat`, we can display the eigenvectors using the `getImageFrom1DFloatMat()`
    function just like we did for the average face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure displays eigenvectors as images. You can see that for
    three people with four faces, there are 12 Eigenfaces (left-hand side of the figure)
    or two Fisherfaces (right-hand side):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_7_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that both Eigenfaces and Fisherfaces seem to have the resemblance of
    some facial features but they don't really look like faces. This is simply because
    the average face was subtracted from them, so they just show the differences for
    each Eigenface from the average face. The numbering shows which Eigenface it is,
    because they are always ordered from the most significant Eigenface to the least
    significant Eigenface, and if you have 50 or more Eigenfaces then the later Eigenfaces
    will often just show random image noise and therefore should be discarded.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 - face recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have trained the Eigenfaces or Fisherfaces machine-learning algorithm
    with our set of training images and face labels, we are finally ready to figure
    out who a person is, just from a facial image! This last step is referred to as
    face recognition or face identification.
  prefs: []
  type: TYPE_NORMAL
- en: Face identification - recognizing people from their face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks to OpenCV's `FaceRecognizer` class, we can identify the person in a photo
    simply by calling the `FaceRecognizer::predict()` function on a facial image
  prefs: []
  type: TYPE_NORMAL
- en: 'as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This `identity` value will be the label number that we originally used when
    collecting faces for training. For example, 0 for the first person, 1 for the
    second person, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this identification is that it will always predict one of the
    given people, even if the input photo is of an unknown person or of a car. It
    would still tell you which person is the most likely person in that photo, so
    it can be difficult to trust the result! The solution is to obtain a confidence
    metric so we can judge how reliable the result is, and if it seems that the confidence
    is too low then we assume it is an unknown person.
  prefs: []
  type: TYPE_NORMAL
- en: Face verification - validating that it is the claimed person
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To confirm if the result of the prediction is reliable or whether it should
    be taken as an unknown person, we perform **face verification** (also referred
    to as **face authentication**), to obtain a confidence metric showing whether
    the single face image is similar to the claimed person (as opposed to face identification,
    which we just performed, comparing the single face image with many people).
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV's `FaceRecognizer` class can return a confidence metric when you call
    the `predict()` function but unfortunately the confidence metric is simply based
    on the distance in eigen-subspace, so it is not very reliable. The method we will
    use is to reconstruct the facial image using the *eigenvectors* and *eigenvalues*,
    and compare this reconstructed image with the input image. If the person had many
    of their faces included in the training set, then the reconstruction should work
    quite well from the learned eigenvectors and eigenvalues, but if the person did
    not have any faces in the training set (or did not have any that have similar
    lighting and facial expressions as the test image), then the reconstructed face
    will look very different from the input face, signaling that it is probably an
    unknown face.
  prefs: []
  type: TYPE_NORMAL
- en: Remember we said earlier that the Eigenfaces and Fisherfaces algorithms are
    based on the notion that an image can be roughly represented as a set of eigenvectors
    (special face images) and eigenvalues (blending ratios). So if we combine all
    the eigenvectors with the eigenvalues from one of the faces in the training set
    then we should obtain a fairly close replica of that original training image.
    The same applies with other images that are similar to the training set--if we
    combine the trained eigenvectors with the eigenvalues from a similar test image,
    we should be able to reconstruct an image that is somewhat a replica to the test
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, OpenCV''s `FaceRecognizer` class makes it quite easy to generate
    a reconstructed face from any input image, by using the `subspaceProject()` function
    to project onto the eigenspace and the `subspaceReconstruct()` function to go
    back from eigenspace to image space. The trick is that we need to convert it from
    a floating-point row matrix to a rectangular 8-bit image (like we did when displaying
    the average face and eigenfaces), but we don''t want to normalize the data, as
    it is already in the ideal scale to compare with the original image. If we normalized
    the data, it would have a different brightness and contrast from the input image,
    and it would become difficult to compare the image similarity just by using the
    L2 relative error. This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows two typical reconstructed faces. The face on the
    left-hand side was reconstructed well because it was from a known person, whereas
    the face on the right-hand side was reconstructed badly because it was from an
    unknown person or a known person but with unknown lighting conditions/facial expression/face
    direction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_7_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now calculate how similar this reconstructed face is to the input face
    by using the same `getSimilarity()` function we created previously for comparing
    two images, where a value less than 0.3 implies that the two images are very similar.
    For Eigenfaces, there is one eigenvector for each face, so reconstruction tends
    to work well and therefore we can typically use a threshold of 0.5, but Fisherfaces
    has just one eigenvector for each person, so reconstruction will not work as well
    and therefore it needs a higher threshold, say 0.7\. This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now you can just print the identity to the console, or use it for wherever your
    imagination takes you! Remember that this face-recognition method and this face-verification
    method are only reliable in the certain conditions that you train them for. So
    to obtain good recognition accuracy, you will need to ensure that the training
    set of each person covers the full range of lighting conditions, facial expressions,
    and angles that you expect to test with. The face preprocessing stage helped reduce
    some differences with lighting conditions and in-plane rotation (if the person
    tilts their head towards their left or right shoulder), but for other differences,
    such as out-of-plane rotation (if the person turns their head towards the left-hand
    side or right-hand side), it will only work if it is covered well in your training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Finishing touches - saving and loading files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You could potentially add a command-line based method that processes input files
    and saves them to the disk, or even perform face detection, face preprocessing
    and/or face recognition as a web service, and so on. For these types of projects,
    it is quite easy to add the desired functionality by using the `save` and `load`
    functions of the `FaceRecognizer` class. You may also want to save the trained
    data and then load it on the program's start up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Saving the trained model to an XML or YML file is very easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: You may also want to save the array of preprocessed faces and labels, if you
    want to add more data to the training set later.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is some sample code for loading the trained model from a
    file. Note that you must specify the face-recognition algorithm (for example, `FaceRecognizer.Eigenfaces`
    or `FaceRecognizer.Fisherfaces`) that was originally used to create the trained
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Finishing touches - making a nice and interactive GUI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the code given so far in this chapter is sufficient for a whole face recognition
    system, there still needs to be a way to put the data into the system and a way
    to use it. Many face recognition systems for research will choose the ideal input
    to be text files listing where the static image files are stored on the computer,
    as well as other important data such as the true name or identity of the person
    and perhaps true pixel coordinates of regions of the face (such as ground truth
    of where the face and eye centers actually are). This would either be collected
    manually or by another face recognition system.
  prefs: []
  type: TYPE_NORMAL
- en: The ideal output would then be a text file comparing the recognition results
    with the ground truth, so that statistics may be obtained for comparing the face
    recognition system with other face recognition systems.
  prefs: []
  type: TYPE_NORMAL
- en: However, as the face recognition system in this chapter is designed for learning
    as well as practical fun purposes, rather than competing with the latest research
    methods, it is useful to have an easy-to-use GUI that allows face collection,
    training, and testing, interactively from the webcam in real time. So this section
    will provide an interactive GUI providing these features. The reader is expected
    to either use this provided GUI that comes with this book, or to modify the GUI
    for their own purposes, or to ignore this GUI and design their own GUI to perform
    the face recognition techniques discussed so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we need the GUI to perform multiple tasks, let''s create a set of modes
    or states that the GUI will have, with buttons or mouse clicks for the user to
    change modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Startup**: This state loads and initializes the data and webcam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detection**: This state detects faces and shows them with preprocessing,
    until the user clicks on the Add Person button.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collection**: This state collects faces for the current person, until the
    user clicks anywhere in the window. This also shows the most recent face of each
    person. The user clicks either one of the existing people or the Add Person button,
    to collect faces for different people.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training**: In this state, the system is trained with the help of all the
    collected faces of all the collected people.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recognition**: This consists of highlighting the recognized person and showing
    a confidence meter. The user clicks either one of the people or the Add Person
    button, to return to mode 2 (*Collection*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To quit, the user can hit the *Esc* key in the window at any time. Let's also
    add a Delete All mode that restarts a new face recognition system, and a Debug
    button that toggles the display of extra debug info. We can create an enumerated
    `mode` variable to show the current mode.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing the GUI elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To display the current mode on the screen, let''s create a function to draw
    text easily. OpenCV comes with a `cv::putText()` function with several fonts and
    anti-aliasing, but it can be tricky to place the text in the correct location
    that you want. Luckily, there is also a `cv::getTextSize()` function to calculate
    the bounding box around the text, so we can create a wrapper function to make
    it easier to place text. We want to be able to place text along any edge of the
    window and make sure it is completely visible and also to allow placing multiple
    lines or words of text next to each other without overwriting each other. So here
    is a wrapper function to allow you to specify either left-justified or right-justified,
    as well as to specify top-justified or bottom-justified, and return the bounding
    box, so we can easily draw multiple lines of text on any corner or edge of the
    window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now to display the current mode on the GUI, as the background of the window
    will be the camera feed, it is quite possible that if we simply draw text over
    the camera feed; it might be the same color as the camera background! So let''s
    just draw a black shadow of text that is just 1 pixel apart from the foreground
    text we want to draw. Let''s also draw a line of helpful text below it, so the
    user knows the steps to follow. Here is an example of how to draw some text using
    the `drawString()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following partial screenshot shows the mode and info at the bottom of the
    GUI window, overlaid on top of the camera image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_07_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We mentioned that we want a few GUI buttons, so let''s create a function to
    draw a GUI button easily as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now we create several clickable GUI buttons using the `drawButton()` function,
  prefs: []
  type: TYPE_NORMAL
- en: which will always be shown at the top-left of the GUI, as shown in the following
  prefs: []
  type: TYPE_NORMAL
- en: 'partial screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we mentioned, the GUI program has some modes that it switches between (as
    a finite state machine), beginning with the Startup mode. We will store the current
    mode as the `m_mode` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Startup mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the Startup mode, we just need to load the XML detector files to detect
    the face and eyes and initialize the webcam, which we''ve already covered. Let''s
    also create a main GUI window with a mouse callback function that OpenCV will
    call whenever the user moves or clicks their mouse in our window. It may also
    be desirable to set the camera resolution to something reasonable; for example,
    640x480, if the camera supports it. This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Detection mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Detection mode, we want to continuously detect faces and eyes, draw rectangles
    or circles around them to show the detection result, and show the current preprocessed
    face. In fact, we will want these to be displayed no matter which mode we are
    in. The only thing special about the Detection mode is that it will change to
    the next mode (*Collection*) when the user clicks the Add Person button.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remember from the detection step previously in this chapter, the output
    of our detection stage will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Mat preprocessedFace`: The preprocessed face (if face and eyes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: were detected).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Rect faceRect`: The detected face region coordinates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Point leftEye`, `rightEye`: The detected left and right eye center coordinates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So we should check if a preprocessed face was returned and draw a rectangle
    and circles around the face and eyes if they were detected as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We will overlay the current preprocessed face at the top-center of the window
  prefs: []
  type: TYPE_NORMAL
- en: 'as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The following screenshot shows the displayed GUI when in the Detection mode.
  prefs: []
  type: TYPE_NORMAL
- en: The preprocessed face is shown at the top-center, and the detected face and
  prefs: []
  type: TYPE_NORMAL
- en: 'eyes are marked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Collection mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We enter the Collection mode when the user clicks on the Add Person button to
    signal that they want to begin collecting faces for a new person. As mentioned
    previously, we have limited the face collection to one face per second and then
    only if it has changed noticeably from the previously collected face. And remember,
    we decided to collect not only the preprocessed face but also the mirror image
    of the preprocessed face.
  prefs: []
  type: TYPE_NORMAL
- en: In the Collection mode, we want to show the most recent face of each known person
    and let the user click on one of those people to add more faces to them or click
    the Add Person button to add a new person to the collection. The user must click
    somewhere in the middle of the window to continue to the next (*Training mode*)
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'So first we need to keep a reference to the latest face that was collected
    for each person. We''ll do this by updating the `m_latestFaces` array of integers,
    which just stores the array index of each person, from the big `preprocessedFaces`
    array (that is, the collection of all faces of all the people). As we also store
    the mirrored face in that array, we want to reference the second last face, not
    the last face. This code should be appended to the code that adds a new face (and
    mirrored face) to the `preprocessedFaces` array as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We just have to remember to always grow or shrink the `m_latestFaces` array
    whenever a new person is added or deleted (for example, due to the user clicking
    on the Add Person button). Now let''s display the most recent face for each of
    the collected people, on the right-hand side of the window (both in the Collection
    mode and Recognition mode later) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We also want to highlight the current person being collected, using a thick
    red border around their face. This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The following partial screenshot shows the typical display when faces for several
    people have been collected. The user can click any of the people at the top-right
    to collect more faces for that person.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7829_07_15.png)'
  prefs: []
  type: TYPE_IMG
- en: Training mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the user finally clicks in the middle of the window, the face-recognition
    algorithm will begin training on all the collected faces. But it is important
    to make sure there have been enough faces or people collected, otherwise the program
    may crash. In general, this just requires making sure there is at least one face
    in the training set (which implies there is at least one person). But the Fisherfaces
    algorithm looks for comparisons between people, so if there are less than two
    people in the training set, it will also crash. So we must check whether the selected
    face-recognition algorithm is Fisherfaces. If it is, then we require at least
    two people with faces, otherwise we require at least one person with a face. If
    there isn't enough data, then the program goes back to the Collection mode so
    the user can add more faces before training.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check if there are at least two people with collected faces, we can make
    sure that when a user clicks on the Add Person button, a new person is only added
    if there isn''t any empty person (that is, a person that was added but does not
    have any collected faces yet). If there are just two people and we are using the
    Fisherfaces algorithm, then we must make sure an `m_latestFaces` reference was
    set for the last person during the Collection mode. `m_latestFaces[i]` is initialized
    to -1 when there still haven''t been any faces added to that person, and then
    it becomes `0` or higher once faces for that person have been added. This is done
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The training may take a fraction of a second or it may take several seconds
    or even minutes, depending on how much data is collected. Once the training of
    collected faces is complete, the face recognition system will automatically enter
    the *Recognition mode*.
  prefs: []
  type: TYPE_NORMAL
- en: Recognition mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Recognition mode, a confidence meter is shown next to the preprocessed
    face, so the user knows how reliable the recognition is. If the confidence level
    is higher than the unknown threshold, it will draw a green rectangle around the
    recognized person to show the result easily. The user can add more faces for further
    training if they click on the Add Person button or one of the existing people,
    which causes the program to return to the Collection mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have obtained the recognized identity and the similarity with the reconstructed
    face as mentioned earlier. To display the confidence meter, we know that the L2
    similarity value is generally between 0 to 0.5 for high confidence and between
    0.5 to 1.0 for low confidence, so we can just subtract it from 1.0 to get the
    confidence level between 0.0 to 1.0\. Then we just draw a filled rectangle using
    the confidence level as the ratio shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: To highlight the recognized person, we draw a green rectangle around their face
  prefs: []
  type: TYPE_NORMAL
- en: 'as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The following partial screenshot shows a typical display when running in Recognition
    mode, showing the confidence meter next to the preprocessed face
  prefs: []
  type: TYPE_NORMAL
- en: at the top-center, and highlighting the recognized person in the top-right corner.
  prefs: []
  type: TYPE_NORMAL
- en: Checking and handling mouse clicks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have all our GUI elements drawn, we just need to process mouse
    events. When we initialized the display window, we told OpenCV that we want a
    mouse event callback to our `onMouse` function. We don''t care about mouse movement,
    only the mouse clicks, so first we skip the mouse events that aren''t for the
    left-mouse-button click as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: As we obtained the drawn rectangle bounds of the buttons when drawing them,
    we just check if the mouse click location is in any of our button regions by calling
    OpenCV's `inside()` function. Now we can check for each button we have created.
  prefs: []
  type: TYPE_NORMAL
- en: When the user clicks on the Add Person button, we just add 1 to the `m_numPersons`
    variable, allocate more space in the `m_latestFaces` variable, select the new
    person for collection, and begin the Collection mode (no matter which mode we
    were previously in).
  prefs: []
  type: TYPE_NORMAL
- en: But there is one complication; to ensure that we have at least one face for
    each
  prefs: []
  type: TYPE_NORMAL
- en: 'person when training, we will only allocate space for a new person if there
    isn''t already a person with zero faces. This will ensure that we can always check
    the value of `m_latestFaces[m_numPersons-1]` to see if a face has been collected
    for every person. This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This method can be used to test for other button clicks, such as toggling the
    debug flag as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'To handle the Delete All button, we need to empty various data structures that
    are local to our main loop (that is, not accessible from the mouse event callback
    function), so we change to the Delete All mode and then we can delete everything
    from inside the main loop. We also must deal with the user clicking the main window
    (that is, not a button). If they clicked on one of the people on the right-hand
    side, then we want to select that person and change to the Collection mode. Or
    if they clicked in the main window while in the Collection mode, then we want
    to change to the Training mode. This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has shown you all the steps required to create a real-time face
    recognition app, with enough preprocessing to allow some differences between the
    training set conditions and the testing set conditions, just using basic algorithms.
    We used face detection to find the location of a face within the camera image,
    followed by several forms of face preprocessing to reduce the effects of different
    lighting conditions, camera and face orientations, and facial expressions. We
    then trained an Eigenfaces or Fisherfaces machine-learning system with the preprocessed
    faces we collected, and finally we performed face recognition to see who the person
    is with face verification providing a confidence metric in case it is an unknown
    person.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than providing a command-line tool that processes image files in an offline
    manner, we combined all the preceding steps into a self-contained real-time GUI
    program to allow immediate use of the face recognition system. You should be able
    to modify the behavior of the system for your own purposes, such as to allow an
    automatic login of your computer, or if you are interested in improving the recognition
    reliability then you can read conference papers about recent advances in face
    recognition to potentially improve each step of the program until it is reliable
    enough for your specific needs. For example, you could improve the face preprocessing
    stages, or use a more advanced machine-learning algorithm, or an even better face
    verification algorithm, based on methods at [h t t p ://w w w . f a c e - r e
    c . o r g /a l g o r i t h m s /](http://www.face-rec.org/algorithms/) and [h
    t t p ://w w w . c v p a p e r s . c o m](http://www.cvpapers.com).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Rapid Object Detection using a Boosted Cascade of Simple Features*, *P. Viola'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and M.J. Jones*, *Proceedings of the IEEE Transactions on CVPR 2001*, *Vol.
    1*,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*pp. 511-518*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*An Extended Set of Haar-like Features for Rapid Object Detection*, *R. Lienhart
    and J. Maydt*, *Proceedings of the IEEE Transactions on ICIP 2002*, *Vol. 1*,
    *pp. 900-903*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Face Description with Local Binary Patterns: Application to Face Recognition*,
    *T. Ahonen, A. Hadid and M. Pietikäinen*, *Proceedings of the IEEE Transactions
    on PAMI 2006*, *Vol. 28*, *Issue 12*, *pp. 2037-2041*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning OpenCV: Computer Vision with the OpenCV Library*, *G. Bradski and
    A. Kaehler*, *pp. 186-190*, *O''Reilly Media*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Eigenfaces for recognition*, *M. Turk and A. Pentland*, *Journal of Cognitive
    Neuroscience 3*, *pp. 71-86*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection*,
    *P.N. Belhumeur, J. Hespanha and D. Kriegman*, *Proceedings of the IEEE Transactions
    on PAMI 1997*, *Vol. 19*, *Issue 7*, *pp. 711-720*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Face Recognition with Local Binary Patterns*, *T. Ahonen, A. Hadid and M.
    Pietikäinen*, *Computer Vision - ECCV 2004*, *pp. 469-48*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
