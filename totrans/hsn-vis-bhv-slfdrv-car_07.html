<html><head></head><body><div><div><h1 id="_idParaDest-115"><a id="_idTextAnchor118"/><em class="italic">Chapter 5</em>: Deep Learning Workflow</h1>
			<p>In this chapter, we will go through the steps that you might perform while training your neural network, and when putting it into production. We will discuss more about the theory behind deep learning, to explain better what we actually did in <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, but we will stay mostly focused on arguments related to self-driving cars. We will also introduce some concepts that will help us to achieve better precision on CIFAR-10, a famous dataset of small images. We are sure that the theory exposed in this chapter, plus the more practical knowledge associated with <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, and <a href="B16322_06_Final_JM_ePUB.xhtml#_idTextAnchor142"><em class="italic">Chapter 6</em></a>,<em class="italic"> Improving Your Neural Network</em>, will give you enough tools to be able to perform tasks that are common in the field of self-driving cars.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Obtaining or creating the dataset</li>
				<li>Training, validation, and test datasets</li>
				<li>Classifiers</li>
				<li>Data augmentation</li>
				<li>Defining the model</li>
				<li>How to tune convolutional layers, <code>MaxPooling</code> layers, and dense layers</li>
				<li>Training and the role of randomness</li>
				<li>Underfitting and overfitting</li>
				<li>Visualization of activations</li>
				<li>Running inference</li>
				<li>Retraining</li>
			</ul>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor119"/>Technical requirements</h1>
			<p>To be able to use the code explained in this chapter, you need to have the following tools and modules installed:</p>
			<ul>
				<li>Python 3.7</li>
				<li>The NumPy module</li>
				<li>The Matplotlib module</li>
				<li>The TensorFlow module</li>
				<li>The Keras module</li>
				<li>The OpenCV-Python module</li>
			</ul>
			<p>The code for the chapter can be found at <a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter5">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter5</a>.</p>
			<p>The Code in Action videos for this chapter can be found here:</p>
			<p><a href="https://bit.ly/3dJrcys">https://bit.ly/3dJrcys</a></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor120"/>Obtaining the dataset</h1>
			<p>Once you have <a id="_idIndexMarker310"/>a task that you want to perform with a neural network, the first step is usually to obtain the dataset, which is the data that you need to feed to the neural network. In the tasks that we perform in this book, the dataset is usually composed of images or videos, but it could be anything, or a mix of images and other data.</p>
			<p>The dataset represents the input that you feed to your neural network, but as you may have noticed, your dataset also contains the desired output, the labels. We will call <code>x</code> the input to the neural network, and <code>y</code> the output. The dataset is composed of the inputs/features (for example, the images in the MNIST dataset), and the output/labels (for example, the number associated with each image).</p>
			<p>We have different dataset types. Let's start with the easiest – the datasets included in Keras – before proceeding to the next ones.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor121"/>Datasets in the Keras module</h2>
			<p>Usually a <a id="_idIndexMarker311"/>dataset is a lot of data. It's normal to train a neural network on tens of thousands of images, but the best neural networks are trained with many millions <a id="_idIndexMarker312"/>of images. So how do we use them?</p>
			<p>The easiest way, which is usually mostly helpful for experiments, is to use the datasets included in Keras, as we did in <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, using <code>load_data()</code>, shown as follows:</p>
			<pre>from keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()</pre>
			<p>Keras provides a variety of datasets:</p>
			<ul>
				<li>MNIST – the classification of digits</li>
				<li>CIFAR10 – the classification of small images</li>
				<li>CIFAR100 – the classification of small images</li>
				<li>IMDB movie review sentiment classification</li>
				<li>Reuters newswire classification</li>
				<li>Fashion MNIST dataset</li>
				<li>Boston Housing prices</li>
			</ul>
			<p>These datasets are useful in learning how to build neural networks in general. In the next section, we will look at some datasets that are more useful for self-driving cars.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor122"/>Existing datasets</h2>
			<p>Luckily, there are <a id="_idIndexMarker313"/>several interesting public datasets available, but you have to always carefully check the license to see what you are allowed to do with it, and eventually get or acquire a more permissive license.</p>
			<p>The following are some datasets related to self-driving cars that you might want to check:</p>
			<ul>
				<li>BDD100K, a large-scale diverse driving video database; refer to <a href="https://bdd-data.berkeley.edu/">https://bdd-data.berkeley.edu/</a>.</li>
				<li>Bosch small traffic lights database; refer to <a href="https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset">https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset</a>.</li>
				<li>CULane, a large-scale dataset for academic research on traffic lane detection; refer to <a href="https://xingangpan.github.io/projects/CULane.html">https://xingangpan.github.io/projects/CULane.html</a>.</li>
				<li>KITTI Vision Benchmark Suite; refer to <a href="http://www.cvlibs.net/datasets/kitti/">http://www.cvlibs.net/datasets/kitti/</a>.</li>
				<li>Mapillary Traffic Sign Dataset; refer to <a href="https://www.mapillary.com/dataset/trafficsign">https://www.mapillary.com/dataset/trafficsign</a>.</li>
			</ul>
			<p>In addition, there are other more generic datasets that you might find interesting, in particular, ImageNet, <a href="http://www.image-net.org/">http://www.image-net.org/</a>, an image dataset organized according to the WordNet hierarchy.</p>
			<p>This dataset contains millions of URLs pointing to images on the internet, and has been very influential in developing neural networks. We will talk more about this later.</p>
			<p>Public datasets are great, but you might consider their content, as it is not uncommon to have some images incorrectly classified. This is not necessarily a big deal for your neural network, but you might still want to get a dataset as good as possible.</p>
			<p>If you cannot find a satisfactory dataset, you can always generate one. Let's see how you can quickly build good datasets for self-driving car tasks.</p>
			<h3>Synthetic datasets</h3>
			<p>When possible, you might <a id="_idIndexMarker314"/>want to generate a dataset from a program <a id="_idIndexMarker315"/>that can create <em class="italic">good enough</em> images. We used this technique in <a href="B16322_03_Final_NM_ePUB.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a>, <em class="italic">Lane Detection</em>, where we detected pedestrians from Carla, and images from the open source video game Speed Dreams, and you could write your own generator using a 3D engine or 3D modeling software.</p>
			<p>These are a relatively easy, quick, and very cheap way to generate massive datasets, and, in fact, are sometimes invaluable, as in many cases you can automatically annotate the images and save a lot of time. However synthetic images tend to be less complex than real images, with the result that your network will probably not perform as well as you think in a real-world scenario. We will use this technique in <a href="B16322_08_Final_NM_ePUB.xhtml#_idTextAnchor182"><em class="italic">Chapter 8</em></a>, <em class="italic">Behavioral Cloning</em>.</p>
			<p>One of <a id="_idIndexMarker316"/>the best simulators available, if not the best, is Carla.</p>
			<p>Carla, the <a id="_idIndexMarker317"/>open source simulator for autonomous driving research, uses <a id="_idIndexMarker318"/>the following websites:</p>
			<ul>
				<li><a href="https://carla.org/">https://carla.org/</a></li>
				<li><a href="https://github.com/carla-simulator/carla">https://github.com/carla-simulator/carla</a></li>
			</ul>
			<p>You can use it to generate the images that you need for your tasks.</p>
			<p>When this is not enough, you have to follow a manual process.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor123"/>Your custom dataset</h2>
			<p>Sometimes, you may have no satisfactory alternative and you need to collect the images by yourself. This <a id="_idIndexMarker319"/>might require the collection of footage and the classification of thousands of images. If the images are extracted from a video, you might be able to just classify a video, and then extract hundreds or thousands of images from it.</p>
			<p>Sometimes this is not the case, and you need to go through tens of thousands of images by yourself. Alternatively, you can use the services of specialized companies to label the images for you. </p>
			<p>Sometimes you might have the images, but the classification might be difficult. Imagine having access to video footage of cars, and then having to annotate the images, adding the boxes where the cars are. If you are lucky, you might get access to a neural network that can <a id="_idIndexMarker320"/>do the job for you. You might still need to manually go through the result, and reclassify some images, but this can still save a lot of work.</p>
			<p>Next, we will learn in depth about these datasets.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor124"/>Understanding the three datasets</h1>
			<p>In reality, you don't need one dataset, but ideally three. These are required for training, validation, and testing. Before defining them, please consider that unfortunately sometimes, there is <a id="_idIndexMarker321"/>some confusion regarding the meaning of validation and test, typically where only two datasets are available, as in this case, validation and test datasets coincide. We did the same in <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, where we used the test dataset as validation.</p>
			<p>Let's now define these three datasets, and then we can explain how ideally we should have tested the MNIST dataset:</p>
			<ul>
				<li><strong class="bold">Training dataset</strong>: This is the <a id="_idIndexMarker322"/>dataset used to train the neural network, and it is typically the biggest of the three datasets.</li>
				<li><strong class="bold">Validation dataset</strong>: This is usually a hold-out part of the training dataset that is not used <a id="_idIndexMarker323"/>for training, but only to evaluate the performance of a model and tune its hyperparameters (for example, the topology of the network or the learning rate of the optimizer).</li>
				<li><strong class="bold">Test dataset</strong>: Ideally this <a id="_idIndexMarker324"/>is a throw-away dataset used to evaluate the performance of the model once all the tuning is complete.</li>
			</ul>
			<p>You cannot use the training dataset to evaluate the performance of a model because the training dataset is used by the optimizer to train the network, so this is the best case scenario. However, we usually don't need the neural network to perform well in the training dataset but in whatever the user will throw at it. So, we need the network to be able to <em class="italic">generalize</em>. Going back to the student metaphor from <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, a high score in the training dataset means that the student learned the book (the training dataset) by heart, but what we want is the student to have understood the content of the book and be able to apply this knowledge to real-world situations (the validation dataset).</p>
			<p>So, if validation represents the real world, why do we need the test dataset? The problem is that while tuning your network, you will make choices that will be biased toward the validation dataset (such as choosing one model instead of another one based on its performance in the validation dataset). As a result, the performance in the validation dataset might still be higher than in the real world.</p>
			<p>The test dataset solves this problem as we only apply it after all the tuning. That also explains why ideally, we want to throw away the test dataset after using it only once. </p>
			<p>This can <a id="_idIndexMarker325"/>be impractical, but not always, as sometimes you can easily generate some samples on demand.</p>
			<p>So, how could we use three datasets in the MNIST task? Maybe you remember from <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, that the MNIST dataset has 60,000 (60 K) samples for training and 10,000 (10 K) for testing.</p>
			<p>Ideally, you could use the following approach:</p>
			<ul>
				<li>The training dataset could use the full 60,000 samples intended for training.</li>
				<li>The validation dataset could use the 10,000 samples intended for testing (as we did).</li>
				<li>The test dataset could be generated on demand, writing digits on the spot.</li>
			</ul>
			<p>After discussing the three datasets, we can now see how to split your full dataset into three parts. While this seems an easy operation, you need to be careful in terms of how you go about it.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor125"/>Splitting the dataset</h2>
			<p>Given your full dataset, you might need to split it into training, validation, and testing parts. As stated <a id="_idIndexMarker326"/>previously, ideally you want the test to be generated on the spot, but if this is not possible, you might choose to use 15-20% of the total dataset for testing.</p>
			<p>Of the remaining dataset, you might use 15-20% as validation.</p>
			<p>If you have many samples, you might use a smaller percentage for validation and testing. If you have a small dataset, after you are satisfied with your model performance (such as if you chose it because it performs well both on the validation and on the test dataset), you might add the test dataset to the training dataset to get more samples. If you do this, there is no point in evaluating the performance in the test dataset, as effectively, it will become part of the training. In this case, you trust the results in the validation dataset.</p>
			<p>But even with the same size, not all splits are created equal. Let's take a practical example. </p>
			<p>You want to detect cats and dogs. You have a dataset of 10,000 pictures. You decide to use 8 K for training, 2 K for validation, and testing is done via the real-time recording of a video of 1 dog and 1 cat that you have at home; every time you test, you make a new video. Looks perfect. What can possibly go wrong?</p>
			<p>First, you need more or less an equal number of cats and dogs in each dataset. If that's not the case, the network will be biased toward one of them. Intuitively, if during training, the network sees that 90% of the images are of dogs, just predicting always a dog will give you 90% accuracy!</p>
			<p>You read that it is a best practice to randomize the order of the samples and you do it. Then you split. Your model performs well in the training, validation, and test datasets. Everything looks good. Then you try with pets of a few friends, and nothing works. What happened?</p>
			<p>One possibility is that your split is not good in terms of measuring generalization. Even if you have 10 K images, they might have been taken from 100 pets (including yours), and every dog and cat is present 100 times, in slightly different positions (for example, from a video). If you shuffle the samples, all the dogs and cats will be present in all the datasets, so validation and testing will be relatively easy, as the network <em class="italic">already knows</em> those pets.</p>
			<p>If, instead, you keep 20 pets for validation, and take care to not include pictures of your pets in the training or validation dataset, then your estimation would be much more realistic, and you <a id="_idIndexMarker327"/>have a chance to build a neural network that is much better at generalizing.</p>
			<p>Now that we have three datasets, it's time to define the task that we need to perform, which typically will be image classification.</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor126"/>Understanding classifiers</h1>
			<p>Deep learning can be used for many different tasks. For what concerns images and CNN, a very common <a id="_idIndexMarker328"/>task is classification. Given an image, the neural network needs to classify it, using one of the labels provided during training. Not surprisingly, a network of this type is called a <em class="italic">classifier</em>.</p>
			<p>To do so, the neural network will have one output for each label (for example, on the 10 digits MNIST dataset, we have 10 labels and so 10 outputs) and only one output should be 1, while all the other outputs should be 0.</p>
			<p>How will a neural network achieve this state? Well, it doesn't. The neural network produces floating point outputs as a result of the internal multiplications and sums, and very seldom you get a similar output. However, we can consider the highest value as the hot one (1), and all the others can be considered cold (0).</p>
			<p>We usually apply a softmax layer at the end of the neural network, which converts the outputs in to probability, meaning that the sum of the output after softmax will be 1.0. This is quite convenient, as we can easily know how confident the neural network is regarding the prediction. Keras offers a method in the model to get the probability, <code>predict()</code>, and one to get the label, <code>predict_classes()</code>. The label can easily be converted to the one-hot encoding format, if you need it, using <code>to_categorical()</code>.</p>
			<p>If you need to go from one-hot encoding to a label, you can use the <code>argmax()</code> NumPy function.</p>
			<p>Now we know what our task is, classifying images, but we need to be sure that our dataset is similar to what our network will need to detect when deployed in production.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor127"/>Creating a real-world dataset</h2>
			<p>When you <a id="_idIndexMarker329"/>collect your dataset, either by using your images or an other suitable dataset, you need to take care that the images reflect <a id="_idIndexMarker330"/>conditions that you might find in real life. For example, you should try to get <em class="italic">problematic images</em>, listed as follows, as you will most likely encounter these problems in production:</p>
			<ul>
				<li>Bad light (over-exposed and under-exposed)</li>
				<li>Strong shadows</li>
				<li>Obstacles obstructing the object</li>
				<li>Object partially out of the picture</li>
				<li>Object rotated</li>
			</ul>
			<p>If you cannot easily obtain these types of images, you can use data augmentation, which is what our next section is about.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor128"/>Data augmentation</h2>
			<p>Data augmentation is <a id="_idIndexMarker331"/>the process of increasing the samples in <a id="_idIndexMarker332"/>your dataset, and deriving new pictures from the one that you already have; for example, reducing the brightness or rotating them.</p>
			<p>Keras includes a convenient way to augment your dataset, <code>ImageDataGenerator()</code>, which randomly applies the specified transformations, but unfortunately is not particularly well documented and it lacks some coherence in terms of the parameters. Therefore, we will now analyze some of the most useful transformations. For clarity, we will build a generator with only one parameter, to see the effect, but you will most likely want to use more than one at the same time, which we will do later. </p>
			<p>The <code>ImageDataGenerator()</code> constructor accepts many parameters, such as the following:</p>
			<ul>
				<li><code>brightness_range</code>: This will change the brightness of the image and it accepts a list of two arguments, the minimum and the maximum brightness, for example, [0.1, 1.5].</li>
				<li><code>rotation_range</code>: This will rotate the image and accept a single parameter that represents the range of rotation in degrees, for example, 60.</li>
				<li><code>width_shift_range</code>: This will shift the image horizontally; it accepts the parameter in different forms. I would recommend using the list of acceptable values, such as [-50, -25, 25, 50].</li>
				<li><code>height_shift_range</code>: This will shift the image vertically; it accepts the parameter in different <a id="_idIndexMarker333"/>forms. I would recommend using the list of acceptable values, such as [-50, -25, 25, 50].</li>
				<li><code>shear_range</code>: This is <a id="_idIndexMarker334"/>the shear intensity, accepting a number in degrees, such as 60.</li>
				<li><code>zoom_range</code>: This zooms in or zooms out of the image and it accepts a list of two arguments, the minimum and the maximum zoom, such as [0.5, 2].</li>
				<li><code>horizontal_flip</code>: This flips the image horizontally, and the parameter is a Boolean.</li>
				<li><code>vertical_flip</code>: This flips the image vertically, and the parameter is a Boolean.</li>
			</ul>
			<p>Of these, the horizontal flip is usually quite effective.</p>
			<p>The following figure shows the result of augmenting the images with brightness, rotation, width shift, and height shift:</p>
			<div><div><img src="img/Figure_5.1_B16322.jpg" alt="Figure 5.1 – ImageDataGenerator() results. From the top: brightness_range=[0.1, 1.5], rotation_range=60, width_shift_range=[-50, -25, 25, 50], and height_shift_range=[-75, -35, 35, 75]" width="401" height="602"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – ImageDataGenerator() results. From the top: brightness_range=[0.1, 1.5], rotation_range=60, width_shift_range=[-50, -25, 25, 50], and height_shift_range=[-75, -35, 35, 75]</p>
			<p>The following <a id="_idIndexMarker335"/>images are generated using shearing, zoom, horizontal <a id="_idIndexMarker336"/>flip, and vertical flip:</p>
			<div><div><img src="img/Figure_5.2_B16322.jpg" alt="Figure 5.2 – ImageDataGenerator() results. From the top: shear_range=60, zoom_range=[0.5, 2], horizontal_flip=True, and vertical_flip=True" width="192" height="289"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – ImageDataGenerator() results. From the top: shear_range=60, zoom_range=[0.5, 2], horizontal_flip=True, and vertical_flip=True</p>
			<p>The effects <a id="_idIndexMarker337"/>are usually combined, as follows:</p>
			<pre>datagen = ImageDataGenerator(brightness_range=[0.1, 1.5], rotation_range=60, width_shift_range=[-50, -25, 25, 50], horizontal_flip=True)</pre>
			<p>And this <a id="_idIndexMarker338"/>is the final result:</p>
			<div><div><img src="img/Figure_5.3_B16322.jpg" alt="Figure 5.3 – ImageDataGenerator() results. Parameters applied: brightness_range=[0.1, 1.5], rotation_range=60, width_shift_range=[-50, -25, 25, 50], and horizontal_flip=True" width="480" height="180"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – ImageDataGenerator() results. Parameters applied: brightness_range=[0.1, 1.5], rotation_range=60, width_shift_range=[-50, -25, 25, 50], and horizontal_flip=True</p>
			<p>Intuitively, the network should become much more tolerant to variations in the image, and it should learn to generalize better.</p>
			<p>Please keep in <a id="_idIndexMarker339"/>mind that the data augmentation of Keras is more like a data substitution, as it replaces the original images, meaning that the original, unchanged images are not sent to the neural network, unless the random combination <a id="_idIndexMarker340"/>is as such that they are presented unchanged.</p>
			<p>The great effect of data augmentation is that the samples will change at every epoch. So, to be clear, data augmentation in Keras does not increase the number of samples per epoch, but the samples will change every epoch, according to the specified transformation. You might want to train more epochs.</p>
			<p>Next, we will see how to build the model.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor129"/>The model</h1>
			<p>Now that <a id="_idIndexMarker341"/>you have a dataset of images and you know what you want to do (for instance, a classification), it's time to build your model!</p>
			<p>We assume that you are working on a <em class="italic">convolutional neural network</em>, so you might even just use convolutional blocks, <em class="italic">MaxPooling</em>, and <em class="italic">dense layers</em>. But how to size them? How many layers should be used?</p>
			<p>Let's do some tests with CIFAR-10, as MINST is too easy, and see what happens. We will not change the other parameters, but just play with these layers a bit.</p>
			<p>We will also train for 5 epochs, so as to speed up training. This is not about getting the best neural network; it is about measuring the impact of some parameters.</p>
			<p>Our starting point is a network with one convolutional layer, one MaxPooling layer, and one dense layer, shown as follows:</p>
			<pre>model = Sequential()
model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu'))
model.add(MaxPooling2D())
model.add(Flatten())
model.add(Dense(units = 256, activation = "relu"))
model.add(Dense(num_classes))
model.add(Activation('softmax'))</pre>
			<p>The following <a id="_idIndexMarker342"/>is a summary of this:</p>
			<pre>_______________________________________________________________
Layer (type)                 Output Shape              Param #   
===============================================================
conv2d_1 (Conv2D)            (None, 30, 30, 8)         224       
_______________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 15, 15, 8)         0         
_______________________________________________________________
flatten_1 (Flatten)          (None, 1800)              0         
_______________________________________________________________
dense_1 (Dense)              (None, 256)               461056    
_______________________________________________________________
dense_2 (Dense)              (None, 10)                2570      
_______________________________________________________________
activation_1 (Activation)    (None, 10)                0         
===============================================================
Total params: 463,850
Trainable params: 463,850
Non-trainable params: 0</pre>
			<p>You can <a id="_idIndexMarker343"/>see that this is such a simple network, and it already has 463 K parameters. The number of layers is misleading. You don't necessarily need many layers to get a slow network.</p>
			<p>This is the performance:</p>
			<pre>Training time: 90.96391367912292
Min Loss: 0.8851623952198029
Min Validation Loss: 1.142119802236557
Max Accuracy: 0.68706
Max Validation Accuracy: 0.6068999767303467</pre>
			<p>Now, the next step is to tune it. So, let's try that.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor130"/>Tuning convolutional layers</h2>
			<p>Let's <a id="_idIndexMarker344"/>use 32 channels in the convolutional layer:</p>
			<pre>Total params: 1,846,922
Training time: 124.37444043159485
Min Loss: 0.6110964662361145
Min Validation Loss: 1.0291267457723619
Max Accuracy: 0.78486
Max Validation Accuracy: 0.6568999886512756</pre>
			<p>Not bad! The <a id="_idIndexMarker345"/>accuracy increased, and despite being 4 times bigger than before, it is less than 50% slower.</p>
			<p>Let's now try to stack 4 layers instead:</p>
			<pre>model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu'))
model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu', padding = "same"))
model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu', padding = "same"))
model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu', padding = "same"))</pre>
			<p>Let's check the size of the network, using <code>model.s<a id="_idTextAnchor131"/>ummary()</code> as usual:</p>
			<pre>Total params: 465,602</pre>
			<p>It's just slightly bigger than the initial model! The reason is that most of the parameters are present due to the dense layer, and stacking convolutional layers of the same size does not change the parameters required by the dense layer. And this is the result:</p>
			<pre>Training time: 117.05060386657715
Min Loss: 0.6014562886440754
Min Validation Loss: 1.0268916247844697
Max Accuracy: 0.7864
Max Validation Accuracy: 0.6520000100135803</pre>
			<p>It is very similar – a bit faster and the accuracy is basically the same. The network can now learn <a id="_idIndexMarker346"/>more complex functions because it has multiple layers. However, it has a much smaller dense layer, so it loses some accuracy because of that.</p>
			<p>Instead <a id="_idIndexMarker347"/>of using the <em class="italic">same</em> padding, let's try to use <code>valid</code>, which will reduce the size of the output of the convolutional layer every time:</p>
			<pre>model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu'))
model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu', padding="valid"))
model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu', padding="valid"))
model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu', padding="valid"))</pre>
			<p>The number of parameters decreased significantly, from 465,602:</p>
			<pre>Total params: 299,714</pre>
			<p>We are now using fewer than 300 K parameters, shown as follows:</p>
			<pre>Training time: 109.74382138252258
Min Loss: 0.8018992121839523
Min Validation Loss: 1.0897881112098693
Max Accuracy: 0.71658
Max Validation Accuracy: 0.6320000290870667</pre>
			<p>Very interestingly, the training accuracy dropped 7%, as the network is too small for this task. However, the validation accuracy only went down 2%.</p>
			<p>Let's now <a id="_idIndexMarker348"/>use the initial model, but with the same <a id="_idIndexMarker349"/>padding, as that would give us a slightly bigger image to work with after the convolution:</p>
			<pre>model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], padding="same", activation='relu'))
Total params: 527,338</pre>
			<p>We now have more parameters, and this is the performance:</p>
			<pre>Training time: 91.4407947063446
Min Loss: 0.7653912879371643
Min Validation Loss: 1.0724352446556091
Max Accuracy: 0.73126
Max Validation Accuracy: 0.6324999928474426</pre>
			<p>Compared to the reference model, both accuracies improved, and the time is almost unchanged, so this was a positive experiment.</p>
			<p>Let's now increase the size of the kernel to 7x7:</p>
			<pre>model.add(Conv2D(8, (7, 7), input_shape=x_train.shape[1:], padding="same", activation='relu'))
Total params: 528,298</pre>
			<p>There is a negligible increase in the number of parameters, as the kernels are now bigger. But how does it perform? Let's check:</p>
			<pre>Training time: 94.85121083259583
Min Loss: 0.7786661441159248
Min Validation Loss: 1.156547416305542
Max Accuracy: 0.72674
Max Validation Accuracy: 0.6090999841690063</pre>
			<p>Not well. It is <a id="_idIndexMarker350"/>slightly slower and slightly less accurate. It is <a id="_idIndexMarker351"/>difficult to know why; maybe it's because the input image is too small.</p>
			<p>We know that it is a typical pattern to add a MaxPooling layer after a convolutional layer, so let's see how we can tune it.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor132"/>Tuning MaxPooling</h2>
			<p>Let's go <a id="_idIndexMarker352"/>back to the previous model and let's just drop <code>MaxPooling</code>:</p>
			<pre>Total params: 1,846,250</pre>
			<p>Removing <code>MaxPooling</code> means that the dense layer is now 4 times as big, since the resolution <a id="_idIndexMarker353"/>of the convolutional layer is no longer reduced:</p>
			<pre>Training time: 121.01851439476013
Min Loss: 0.8000291277170182
Min Validation Loss: 1.2463579467773438
Max Accuracy: 0.71736
Max Validation Accuracy: 0.5710999965667725</pre>
			<p>This does not seem very efficient. Compared to the original network, it is slower, the accuracy improved, but the validation accuracy decreased. Compared to the networks with four convolutional layers, it has the same speed, but a far inferior validation accuracy.</p>
			<p>It seems that <code>MaxPooling</code> improves generalization while reducing computations. Not surprisingly, it is widely used.</p>
			<p>Let's now <a id="_idIndexMarker354"/>increase the number of MaxPooling layers:</p>
			<pre>model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu'))
model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu', padding = "same"))
model.add(MaxPooling2D())
model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu', padding = "same"))
model.add(Conv2D(8, (3, 3), input_shape=x_train.shape[1:], activation='relu', padding = "same"))
model.add(MaxPooling2D())</pre>
			<p>The size <a id="_idIndexMarker355"/>is now much smaller because the second convolutional layers are now one fourth of the size:</p>
			<pre>Total params: 105,154</pre>
			<p>Let's check the performance:</p>
			<pre>Training time: 105.30972981452942
Min Loss: 0.8419396163749695
Min Validation Loss: 0.9395202528476715
Max Accuracy: 0.7032
Max Validation Accuracy: 0.6686999797821045</pre>
			<p>While the training accuracy is not great, the validation accuracy is the best that we achieved, and all <a id="_idIndexMarker356"/>while using only 100 K parameters!</p>
			<p>After tuning <a id="_idIndexMarker357"/>the convolutional part of the network, is time to see how we can tune the part composed by dense layers.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor133"/>Tuning the dense layer</h2>
			<p>Let's go <a id="_idIndexMarker358"/>back to the initial model, and increase the dense layer 4 times, which is to 1,024 neurons:</p>
			<pre>Total params: 1,854,698</pre>
			<p>As expected, the number <a id="_idIndexMarker359"/>of parameters increased almost four fold. But what about performance?</p>
			<pre>Training time: 122.05767631530762
Min Loss: 0.6533840216350555
Min Validation Loss: 1.093649614238739
Max Accuracy: 0.7722
Max Validation Accuracy: 0.630299985408783</pre>
			<p>The training accuracy is not bad, but the validation accuracy is lower compared with the best models.</p>
			<p>Let's try to use three dense layers:</p>
			<pre>model.add(Dense(units = 512, activation = "relu"))
model.add(Dense(units = 256, activation = "relu"))
model.add(Dense(units = 128, activation = "relu"))</pre>
			<p>Now we get the following parameters:</p>
			<pre>Total params: 1,087,850</pre>
			<p>The number of parameters is now lower:</p>
			<pre>Training time: 111.73353481292725
Min Loss: 0.7527586654126645
Min Validation Loss: 1.1094331634044647
Max Accuracy: 0.7332
Max Validation Accuracy: 0.6115000247955322</pre>
			<p>The result <a id="_idIndexMarker360"/>is maybe somehow disappointing. We should probably not count too much on increasing the number of dense layers.</p>
			<p>The next <a id="_idIndexMarker361"/>step now is to train the network. Let's begin.</p>
			<h3>Training the network</h3>
			<p>We are now <a id="_idIndexMarker362"/>ready to discuss the training phase in greater depth, which is where the <em class="italic">magic happens</em>. We will not even attempt to describe <a id="_idIndexMarker363"/>the mathematical concepts behind it. We will just discuss in very generic and simplified terms the algorithm that is used to train neural networks.</p>
			<p>We need some definitions:</p>
			<ul>
				<li><strong class="bold">Loss function</strong> or <strong class="bold">cost function</strong>: A function <a id="_idIndexMarker364"/>that computes <a id="_idIndexMarker365"/>how far the prediction of the neural network is from <a id="_idIndexMarker366"/>the expected label; it could be the <strong class="bold">MSE</strong> (which is the <strong class="bold">mean squared error</strong>) or something more elaborate.</li>
				<li><strong class="bold">Derivative</strong>: The derivative <a id="_idIndexMarker367"/>of a function is a new function that can measure how much a function is changing (and in which direction) in a specific point. For example, if you imagine being in a car, the speed can be your initial function, and its derivative is the acceleration. If the speed is constant, the derivative (for example, the acceleration) is zero; if the speed is increasing, the derivative <a id="_idIndexMarker368"/>will be positive and if the speed is decreasing, the derivative will be negative.</li>
				<li><strong class="bold">Local minimum</strong>: The job <a id="_idIndexMarker369"/>of a neural network is to minimize the loss function. Given the incredible number of parameters, the function of the neural network can be very complex and therefore reaching a global minimum can be impossible, but the network could still reach a good local minimum.</li>
				<li><strong class="bold">Convergence</strong>: If the <a id="_idIndexMarker370"/>network keeps approaching a good local minimum, then it is converging.</li>
			</ul>
			<p>Using these definitions, we will now see how the training actually works.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor134"/>How to train the network</h2>
			<p>The algorithm <a id="_idIndexMarker371"/>is composed of two parts, and, to simplify, let's <a id="_idIndexMarker372"/>say that it is performed for every sample and, of course, the whole thing is repeated for every epoch. So, let's see how it works:</p>
			<ul>
				<li><strong class="bold">Forward pass</strong>: At the end of the day, your neural network is just a function with <a id="_idIndexMarker373"/>many parameters (weights and possibly biases) and many operations that, when provided with an input, can compute some outputs. In the forward pass, we compute the prediction and the loss.</li>
				<li><strong class="bold">Backward pass</strong>: The <a id="_idIndexMarker374"/>optimizer (for example, Adam or stochastic gradient descent) goes <em class="italic">backward</em> (from the last layer to the first layer) updating all the weights (for instance, all the parameters) trying to minimize the loss function; the <em class="italic">learning rate</em> (a number between 0 and 1.0, typically worth 0.01 or less) determines how much the weights will be adjusted.</li>
			</ul>
			<p>A bigger learning rate makes them train faster, but it might skip local minimums, while a smaller learning rate might converge, but taking too much time. Optimizers are actively researched to improve training speed as much as possible, and they are dynamically changing the learning rate to improve speed and precision. </p>
			<p>Adam is an example of an optimizer that can dynamically change the learning rate for each parameter:</p>
			<div><div><img src="img/Figure_5.4_B16322.jpg" alt="Figure 5.4 – Gradient descent looking for a minimum" width="705" height="600"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Gradient descent looking for a minimum</p>
			<p>While writing <a id="_idIndexMarker375"/>an algorithm to train a neural network is <a id="_idIndexMarker376"/>very complex, the concept is, in a way, similar to somebody trying to learn to play pool, while repeating the same complex shoot until it works. You choose the point that you want to hit (label), you make your move (forward pass), you evaluate how far you ended up from the target, and then you retry adjusting the power, direction, and all the other variables at play (weights). We also have a way to do this with random initialization. Let's try this next.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor135"/>Random initialization</h2>
			<p>You might <a id="_idIndexMarker377"/>wonder what the values of the parameters are the first time that you run the neural network. Initializing the weights to zero does not work well, while having small random numbers is quite effective. Keras has a variety of algorithms that you can choose from, and you can also change the standard deviation.</p>
			<p>An interesting <a id="_idIndexMarker378"/>consequence of this is that the neural network starts with a considerable amount of random data, and you might notice that training the same model on the same dataset actually produces different results. Let's try with our previous basic CIFAR-10 CNN.</p>
			<p>The first attempt produces the following results:</p>
			<pre>Min Loss: 0.8791077242803573
Min Validation Loss: 1.1203862301826477
Max Accuracy: 0.69174
Max Validation Accuracy: 0.5996000170707703</pre>
			<p>The second attempt produces the following results:</p>
			<pre>Min Loss: 0.8642362675189972
Min Validation Loss: 1.1310886552810668
Max Accuracy: 0.69624
Max Validation Accuracy: 0.6100000143051147</pre>
			<p>You can try to reduce the randomness using the following code:</p>
			<pre>from numpy.random import seed
seed(1)
import tensorflow as tf
tf.random.set_seed(1)</pre>
			<p>However, if you train on a GPU, there could still be a number of variations. You should take this into consideration while you tune your network model, or you risk disqualifying small improvements due to randomness.</p>
			<p>The next stage is to see what overfitting and underfitting are.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor136"/>Overfitting and underfitting</h2>
			<p>While <a id="_idIndexMarker379"/>training a neural network, you will fight between <strong class="bold">underfitting</strong> and <strong class="bold">overfitting</strong>. Let's <a id="_idIndexMarker380"/>see how:</p>
			<ul>
				<li>Underfitting is <a id="_idIndexMarker381"/>where the model is too simple, and it is not able to learn the dataset properly. You need to add parameters, filters, and neurons to increase the capacity of the model.</li>
				<li>Overfitting is <a id="_idIndexMarker382"/>where your model is big enough to learn the training dataset, but it is not able to generalize (for example, it <em class="italic">memorizes</em> the dataset but it is not good when you provide other data).</li>
			</ul>
			<p>You can also see it from the plot over time of accuracy and loss:</p>
			<div><div><img src="img/Figure_5.5_B16322.jpg" alt="Figure 5.5 – Underfitting: the model is too small (7,590 parameters) and is not learning much" width="633" height="466"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Underfitting: the model is too small (7,590 parameters) and is not learning much</p>
			<p>The <a id="_idIndexMarker383"/>preceding graph shows an extreme underfitting, and the <a id="_idIndexMarker384"/>accuracies stay very low. Now refer to the following graph:</p>
			<div><div><img src="img/Figure_5.6_B16322.jpg" alt="Figure 5.6 – Overfitting: the model is very big (29,766,666 parameters) and is not generalizing well" width="636" height="468"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Overfitting: the model is very big (29,766,666 parameters) and is not generalizing well</p>
			<p><em class="italic">Figure 5.6</em> shows a somewhat extreme example of a neural network in overfitting. You may notice <a id="_idIndexMarker385"/>that while the training loss keeps decreasing with the epochs, the validation loss reaches a minimum at epoch one, and then keeps <a id="_idIndexMarker386"/>increasing. The minimum of the validation loss is where you want to stop your training. In <a href="B16322_06_Final_JM_ePUB.xhtml#_idTextAnchor142"><em class="italic">Chapter 6</em></a>, <em class="italic">Improving Your Neural Network</em>, we will see a technique that allows us to do more or less that—<em class="italic">early stopping</em>.</p>
			<p>While you might hear that overfitting is a big problem, actually it could be a good strategy first trying to get a model that can overfit the training dataset, and then apply techniques that can reduce the overfitting and improve generalization. However, this is good only if you can overfit the training dataset with a very high degree of accuracy.</p>
			<p>We will see very effective ways to reduce overfitting in <a href="B16322_06_Final_JM_ePUB.xhtml#_idTextAnchor142"><em class="italic">Chapter 6</em></a>, <em class="italic">Improving Your Neural Network</em>, but one thing to consider is that a smaller model is less prone to overfit, and it is <a id="_idIndexMarker387"/>usually also faster. So, while you are trying to <a id="_idIndexMarker388"/>overfit the training dataset, try not to use a model that is extremely big.</p>
			<p>In this section, we have seen how to use the graph of the losses to understand where we are in the training of our network. In the next section, we will see how to visualize the activations in order to get an idea of what our network is learning.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor137"/>Visualizing the activations</h1>
			<p>Now we can <a id="_idIndexMarker389"/>train a neural network. Great. But what exactly is the neural network able to see and understand? That's a difficult question to answer, but as convolutions output an image, we could try to show this. Let's now try to show the activation for the first 10 images of the MINST test dataset:</p>
			<ol>
				<li>First, we need to build a model, derived from our previous model, that reads from the input and gets as output the convolutional layer that we want. The name can be taken from the summary. We will visualize the first convolutional layer, <code>conv2d_1</code>:<pre>conv_layer = next(x.output for x in model.layers if     x.output.name.startswith(conv_name))act_model = models.Model(inputs=model.input, outputs=[conv_layer])activations = act_model.predict(x_test[0:num_predictions, :, :, :])</pre></li>
				<li>Now, for each test image, we can take all the activations and chain them together to get an image:<pre>col_act = []
for pred_idx, act in enumerate(activations):
    row_act = []
    for idx in range(act.shape[2]):
        row_act.append(act[:, :, idx])
    col_act.append(cv2.hconcat(row_act))</pre></li>
				<li>And then we can show it:<pre>plt.matshow(cv2.vconcat(col_act), cmap='viridis')plt.show()</pre></li>
			</ol>
			<p>This is <a id="_idIndexMarker390"/>the result for the first convolutional layer, <code>conv2d_1</code>, which has 6 channels, 28x28:</p>
			<div><div><img src="img/Figure_5.7_B16322.jpg" alt="Figure 5.7 – MNIST, activations of the first convolutional layer" width="370" height="573"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – MNIST, activations of the first convolutional layer</p>
			<p>This looks interesting, but trying to understand the activations, and what the channels learned to recognize, always involves some guesswork. The last channel seems to focus on horizontal lines, and the third and fourth channels are not very strong, which might mean that the network is not properly trained or it is already bigger than required. But it looks good.</p>
			<p>Let's now <a id="_idIndexMarker391"/>check the second layer, <code>conv2d_2</code>, which has 16 channels, 10x10:</p>
			<div><div><img src="img/Figure_5.8_B16322.jpg" alt="Figure 5.8 – MNIST, activations of the second convolutional layer" width="651" height="428"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – MNIST, activations of the second convolutional layer</p>
			<p>Now it's more complex – the outputs are much smaller, and we have more channels. It seems that some channels are detecting horizontal lines, and some of them are focusing on diagonal or vertical lines. And what about the first MaxPooling layer, <code>max_pooling2d_1</code>? It's a lower resolution than the original channel, at 10x10, but as it selects the maximum activation, it should be understandable. Refer to the following screenshot:</p>
			<div><div><img src="img/Figure_5.9_B16322.jpg" alt="Figure 5.9 – MNIST, activations of the first MaxPooling layer" width="363" height="568"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – MNIST, activations of the first MaxPooling layer</p>
			<p>Indeed, the <a id="_idIndexMarker392"/>activation looks good. Just for fun, let's check the second MaxPooling layer, <code>max_pooling2d_2</code>, which is only 5x5:</p>
			<div><div><img src="img/Figure_5.10_B16322.jpg" alt="Figure 5.10 – MNIST, activations of the second MaxPooling layer" width="647" height="422"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – MNIST, activations of the second MaxPooling layer</p>
			<p>Now it looks chaotic, but it still looks like some channels are recognizing horizontal lines and some are focusing on vertical ones. Here is when the dense layers will come into play, as they try to make sense of these activations that are difficult to understand but not at all random. </p>
			<p>Visualizing the <a id="_idIndexMarker393"/>activation is useful to get an idea of what the neural network is learning, and how the channels are used, and it is another tool that you can use while training your neural network, above all when you feel that it is not learning well and you are looking for problems.</p>
			<p>Now we will talk about inference, which is actually the whole point of training a neural network.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor138"/>Inference</h1>
			<p>Inference is the <a id="_idIndexMarker394"/>process of giving an input to your network and getting a classification or prediction. When the neural network has been trained and deployed in production, we use it, for example, to classify images or to decide how to drive in a road, and this process is called inference.</p>
			<p>The first step is to load the model:</p>
			<pre>model = load_model(os.path.join(dir_save, model_name))</pre>
			<p>Then you simply call <code>predict()</code>, which is the method for inference in Keras. Let's try with the first test sample of MNIST:</p>
			<pre>x_pred = model.predict(x_test[0:1, :, :, :])print("Expected:", np.argmax(y_test))print("First prediction probabilities:", x_pred)print("First prediction:", np.argmax(x_pred))</pre>
			<p>This is the result for my MNIST network:</p>
			<pre>Expected: 7
First prediction probabilities: [[6.3424804e-14 6.1755254e-06 2.5011676e-08 2.2640785e-07 9.0170204e-08 7.4626680e-11 5.6195684e-13 9.9999273e-01 1.9735349e-09 7.3219508e-07]]
First prediction: 7</pre>
			<p>The result of <code>predict()</code> is an array of probabilities, which is very handy for evaluating the confidence <a id="_idIndexMarker395"/>of the network. In this case, all the numbers are very close to zero, except for the number <code>7</code>, which is therefore the prediction of the network, with a confidence above 99.999%! In real life, unfortunately, you seldom see a network working so well!</p>
			<p>After inference, sometimes you want to periodically retrain on new samples, to improve the network. Let's see what this entails next.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor139"/>Retraining</h1>
			<p>Sometimes, once <a id="_idIndexMarker396"/>you get a neural network that performs well, you job is done. Sometimes, however, you might want to retrain it on new samples, to get better precision (as your dataset is now bigger) or to get fresher results if your training dataset becomes obsolete relatively quickly.</p>
			<p>In some cases, you might even want to retrain continuously, for example, every week, and have the new model automatically deployed in production.</p>
			<p>In this case, it's critical that you have a strong procedure in place to verify the performance of your new model in the validation dataset and, hopefully, in a new, throwaway test dataset. It may also be advisable to keep a backup of all the models and try to find a <a id="_idIndexMarker397"/>way to monitor the performance in production, to quickly identify anomalies. In the case of a self-driving car, I expect a model to undergo rigorous automated and manual testing before being deployed in production, but other industries that don't have safety concerns might be much less strict.</p>
			<p>With this, we conclude our topic on retraining.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor140"/>Summary</h1>
			<p>This has been a dense chapter, but hopefully you got a better overview of what neural networks are and how to train them.</p>
			<p>We talked a lot about the dataset, including how to get correct datasets for training, validation, and testing. We described what a classifier is and we implemented data augmentation. Then we discussed the model and how to tune the convolutional layers, the MaxPooling layers, and the dense layers. We saw how training is done, what backpropagation is, discussed the role of randomness on the initialization of the weights, and we saw graphs of underfitting and overfitting networks. To understand how well our CNN is doing, we went as far as visualizing the activations. Then we discussed inference and retraining.</p>
			<p>This means that you now have sufficient knowledge to choose or create a dataset and train a neural network from scratch, and you will be able to understand if a change in the model or in the dataset improves precision.</p>
			<p>In <a href="B16322_06_Final_JM_ePUB.xhtml#_idTextAnchor142"><em class="italic">Chapter 6</em></a>, <em class="italic">Improving Your Neural Network</em>, we will see how to apply all this knowledge in practice, so as to improve the precision of a neural network significantly.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor141"/>Questions</h1>
			<p>After reading this chapter, you should be able to answer the following questions:</p>
			<ol>
				<li value="1">Can you reuse the test dataset?</li>
				<li>What is data augmentation?</li>
				<li>Does data augmentation in Keras add images to your dataset?</li>
				<li>Which layer tends to have the highest number of parameters?</li>
				<li>Watching the plot of the losses, how can you tell that a network is overfitting?</li>
				<li>Is it always bad to have a network that overfits?</li>
			</ol>
		</div>
	</div>



  </body></html>