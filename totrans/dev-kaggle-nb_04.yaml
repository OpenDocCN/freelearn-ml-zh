- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take a Break and Have a Beer or Coffee in London
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We continue our journey around the world using data by exploring two datasets
    in this chapter with geographically distributed information. The first dataset
    is *Every Pub in England* (see *Reference 1*). This dataset contains the unique
    ID, name, address, postcode, and information regarding the geographical position
    of almost every pub in England. The second dataset is called *Starbucks Locations
    Worldwide* (see *Reference 3*) which contains store number, name, and ownership
    details, as well as street address, city, and geographical information (latitude
    and longitude) for all Starbucks stores in the world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from combining these two datasets, we will also add additional geographical
    support data. We will learn how to work with missing data, how to perform imputation
    if needed, how to visualize geographical data, how to clip and merge polygon data,
    how to generate custom maps, and how to create multiple layers over them. These
    are just a few tricks that we will learn in this chapter, but in a nutshell, the
    following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Detailed data analysis for pubs in England and Starbucks across the world
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combined geospatial analysis of pubs and Starbucks in London
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pretext for this chapter’s exploration of geospatial analysis tools and
    techniques is to analyze how the pubs and Starbucks coffee shops geographically
    interwind, answering such questions as “If somebody had enjoyed a few pints of
    ale in a pub in downtown London and then fancied a coffee, how far would they
    have to go to the nearest Starbucks coffee shop?” Or, to give another example,
    “For the current Starbucks shop, which pubs are closer to this one than to any
    other Starbucks coffee shop?” Of course, these are not the only questions we will
    try to answer, but we wanted to give you a glimpse of what we will achieve by
    the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Pubs in England
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Every Pub in England* dataset (*Reference 1*) contains data about 51,566
    pubs in England, including the pub name, the address, the postal code, the geographical
    position (both by easting and northing and by latitude and longitude), and the
    local authority. I created a notebook, *Every Pub in England – Data Exploration*
    (*Reference 2*) to investigate this data. The code snippets in the current section
    are mainly from this notebook. It might be easier for you to follow the notebook
    in parallel with the explanations in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality check
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the data quality check, we will use the `info()` and `describe()` functions
    to get a first glimpse. These two can be considered the first place to start.
    Then, we can also use our custom data quality statistics functions defined in
    the previous chapter. Because we will keep using them, we will group them in a
    utility script. I call this utility script `data_quality_stats`, and I defined
    in this module the functions `missing_data`, `most_frequent_values`, and `unique_values`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the functions defined in this utility script, we need to first add it
    to the notebook. From the **File** menu, we will select the **Add utility script**
    menu item, and then add the utility script by selecting it in the **Add Data**
    panel on the right-hand side of the editor window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Adding a utility script to the notebook'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will add `import` to one of the first notebook cells:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the results after applying this function to our `pub_df` dataframe.
    *Figure.4.2* shows the missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Missing values'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that there are two missing values for local authorities. Other than
    that, it appears that there are no others. We need to be alert with regard to
    the missing values, as some might be hidden; for instance, a missing value could
    be replaced according to a convention with a specific value (like using “-1” to
    indicate null values for positive numbers or “NA” for categorical cases).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.3* depicts the most frequent values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Most frequent values'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look now at the most frequent values, we can observe that for both **latitude**
    and **longitude**, there are 70 items with the value **\N**. It’s interesting
    that, there are **70** most frequent values for **easting** and **northing**.
    Easting and northing are geographic Cartesian coordinates: easting refers to the
    eastward-measured distance, while northing refers to the northward-measured distance.
    According to the **Universal Transverse Mercator** (**UTM**) coordinate system,
    northing is the distance to the Equator; Easting, in the same coordinate system,
    is the distance to the “false easting,” which is uniquely defined in each UTM
    zone. We can also observe that the most frequently used name for a pub is **The
    Red Lion**, and that there are **8** pubs in **Lancaster University**. As for
    the unique values, we can observe that there are more addresses than postcodes
    and more latitudes and longitudes than postcodes.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.4* depicts the unique values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Unique values'
  prefs: []
  type: TYPE_NORMAL
- en: The number of unique values for **address** is larger than the one for **postcode**
    (more addresses on the same postcode). The total number of different local authorities
    is **376**. Additionally, notice that the number of unique names is smaller than
    the number of unique addresses (presumably, there are several popular pub names).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check a bit more about the two missing local authority values. It is
    odd, since there are only two missing values, which is not expected. We also know
    that we have 70 missing values for both **latitude** and **longitude**, and those
    are marked with **\N**. Look at the rows containing this missing local authority
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated
    with medium confidence](img/B20963_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Rows with local authority information missing'
  prefs: []
  type: TYPE_NORMAL
- en: It appears that the information is missing because when the parser used by pandas
    to read the CSV file encountered the sequence **\”,”**, it was not able to distinguish
    the comma separator (**,**). Therefore, for those two lines, it merged **name**
    with **address** and then shifted left every column by one position, thus corrupting
    every column, from **address** to **local_authority**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two options to address this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: One option is to try and give a list of separators to the parser. In our case,
    it will be a bit tricky, since we have only a comma separator. Also, if we try
    to use a multi-character separator, we will need to switch to a different engine,
    Python, because the default engine does not work with multi-character separators.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second option, and the preferred one, is to write a small piece of code
    to fix the issue in the two rows where we spotted it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the piece of code to fix the issue with the two rows. We use the indexes
    of the two rows (we can see them in *Figure 4.5* – the first column, without a
    name) to identify them and perform the correction only on these rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 4.6*, we can see that the name and address are now split and assigned
    to the correct column, and the rest of the columns were shifted to the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B20963_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Rows with local authority information after correction'
  prefs: []
  type: TYPE_NORMAL
- en: If we check the missing data again, it will appear that no other data is missing.
    We already know that, in fact, there are 70 missing latitudes and longitudes;
    they are just marked with **\N**. If we check separately the latitude or longitude
    columns that have this value and then the rows where both columns have the same
    value, we can conclude that there are only 70 rows in total with this anomaly.
    For the same rows, we see that **northing** and **easting** have unique values,
    and these values are not correct.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, we will not be able to reconstruct the latitude and longitude
    from **easting** and **northing**. When checking the corresponding postcode, address,
    and local authority for these rows, we can see that there are multiple locations,
    in multiple local authority regions. There are 65 different postcodes in these
    70 rows. Since we do have the postcodes, we will be able to use them to reconstruct
    the latitude and longitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this purpose, we will include the **Open Postcode Geo** dataset (see *Reference
    4*) in our analysis. This dataset contains more than 2.5 million rows and many
    other columns, besides the postcode, latitude, and longitude. We read the CSV
    file from the **Open Postcode Geo** dataset, select only four columns (**postcode**,
    **country**, **latitude**, and **longitude**), and filter out any rows with postcodes
    that are not included in the list of postcodes, from the 70 rows we targeted in
    our original dataset with pubs. We set as `None` the values of `longitude` and
    `latitude` for the 70 rows with missing geographical data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We merge the two resulting datasets (the one with pubs and the one with postcodes),
    and we fill in the missing values for **latitude** and **longitude** in the *left*
    columns with the values from the *right* columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’ve replaced all the missing data in the targeted rows with valid latitude
    and longitude values. *Figure 4.7* is a snapshot of what the combined dataset
    looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B20963_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Combined dataset snapshot (every pub in England and Open Postcode)'
  prefs: []
  type: TYPE_NORMAL
- en: Now with imputation done, we can continue with data exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start by exploring the frequency of each pub name and local authority.
    To represent this information, we will reuse the `colormap` and `plot` functions
    developed in the previous chapter. I created a utility script that is imported
    in the same way as the data statistics utility script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After importing, we will extract the county and the city (if the address line
    contains more than two commas) and analyze the word frequency for those. The city
    is extracted with the simple code shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 4.8*, we show the top 10 pubs per local authority:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A bar graph with different colored bars  Description automatically generated](img/B20963_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Pubs per local authority (top 10)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.9* shows the top 10 pubs per county. We extract the county by retrieving
    the last substring after the comma from the address. In some cases, it is not
    a county but a large municipality, like London:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A bar graph with different colored bars  Description automatically generated](img/B20963_04_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Pubs per county (top 10)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.10* shows the distribution of words in pub names and addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Distribution of words in the pub names (left) and addresses (right)'
  prefs: []
  type: TYPE_NORMAL
- en: Because we have the geographical position of pubs, we would like to visualize
    this information. We can represent the positions of the pubs using the `folium`
    Python library and `folium plugin` `MarkerCluster`. Folium (which wraps some of
    the most popular Leaflet external plugins) is an excellent way to display geographically
    distributed information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to show the UK map is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To add markers, we can add the following code (the code to initialize the folium
    map layer is not included):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can also add, besides locations, pop-up information for the `MarkerCluster`,
    as well as custom icons.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.11* shows the folium (leaflet) map for the British Isles based on
    OpenStreetMap, without the pub information layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Map  Description automatically generated](img/B20963_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Map of the British Isles without the pub information layer'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.12* shows a map of the British Isles with the pub information layer
    added, using the MarkerCluster plugin. With MarkerCluster, the markers are replaced
    dynamically, with a widget showing the number of markers in a certain area. When
    zooming in on an area, the MarkerCluster display changes dynamically, showing
    a more detailed view of the markers’ distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Map  Description automatically generated](img/B20963_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Map of the British Isles with the pub information layer added'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.13* shows a zoomed-in version of the previous map. The region that
    we zoom in on is the southern part of the British mainland:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Map  Description automatically generated](img/B20963_04_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Map of the British Isles with the pub information layer added,
    zoomed in on the southern region, including the London area'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.14* zooms in on the London area. The clusters are broken into smaller
    groups, which appear as individual markers as we zoom in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, map  Description automatically generated](img/B20963_04_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14: Zooming in on the London area'
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative way to visualize the pub concentration is by using a heatmap.
    Heatmaps can create a very good intuition of the spatial distribution of data.
    They show distribution density with color shades, as shown in *Figure 4.15*. Heatmaps
    are useful to show the density of data points continuously, and it is also easier
    to evaluate their intensity at different locations using heatmaps. Because heatmaps
    use interpolation techniques to create a smooth transition between data points,
    they can provide a more visually appealing representation of the data distribution.
    You can see two zoom levels with a heatmap view of the pub distribution for all
    of Great Britain (left) and for the southwest tip of the mainland (right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: Maps using folium and Heatmap to show location density distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that there are no pubs from Northern Ireland included. This is because
    the collection of the pub data excluded it as it isn’t part of Great Britain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to represent the spatial distribution of the pub data is using
    the Voronoi polygons (or a Voronoi diagram) associated with the pubs’ positions.
    **Voronoi polygons** represent the dual graph of a **Delaunay tessellation**.
    Let’s explain these two concepts that we just introduced: Voronoi polygons and
    Delaunay tessellation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a distribution of points in a plane, we can use the Delaunay tessellation
    to generate the triangular tessellation for this set of points. This graph is
    a set of triangles whose edges connect all the points, without crossing over.
    If we draw the mediators of the edges in the Delaunay graph, the network generated
    from the intersection of those new lines’ segments forms the Voronoi polygons
    mesh. In *Figure 4.16*, we show a set of points and then the Voronoi diagram associated
    with these points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16: A set of points in a plane and the Voronoi polygons generated
    from this set of points'
  prefs: []
  type: TYPE_NORMAL
- en: This Voronoi polygon graph has an interesting property. Inside a Voronoi polygon,
    all points are closer to the weight center of the polygon (which is one of the
    vertices of the original graph) than to any of the weight centers of any other
    neighboring polygon. Therefore, the Voronoi polygons drawn from our pubs’ geographical
    position will accurately represent the pubs’ concentration and will also show,
    with a good approximation, the area “covered” by a certain pub. We will use the
    Voronoi diagram, formed from the Voronoi polygons, to show the virtual area covered
    by each pub.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we extract the Voronoi polygons using the *Voronoi* function `from`
    `scipy.spatial`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can represent the Voronoi polygons associated with the pubs (from `pub_voronoi`)
    using the `voronoi_plot_2d` function (see *Figure 4.17*). However, the graph has
    a few problems. First, there are many polygons that are very difficult to distinguish.
    Then, the pubs’ locations (with dots in the graph) are not very legible. Another
    issue is that the polygons on the border are not aligned with the territory, creating
    unwanted artifacts that are not informative of the real area “covered” by a certain
    pub inside the Great Britain territory. We will apply a series of transformations
    to eliminate the aforementioned problems with the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code creates an image of Voronoi polygons, as shown in *Figure
    4.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B20963_04_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: 2D plot of Voronoi polygons, extended outside the territory (not
    clipped)'
  prefs: []
  type: TYPE_NORMAL
- en: If we want to represent the geographical area “covered” by each polygon only
    inside the territorial boundaries of Great Britain, we will have to clip the Voronoi
    polygons generated from the pubs’ position with the polygons describing the territory
    boundary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, we have access to Kaggle for shapefile data file formats for various
    countries. For our purpose, we will import the UK ESRI shapefile data from the
    *GADM Data for UK* dataset (see *Reference 5*). This dataset provides incrementally
    detailed shapefile data, ranging from external boundaries (level 0) to country
    level (level 1) and county level (level 2) for the entire territory. Shapefiles
    can be read with several libraries; in this case, I preferred to use the `geopandas`
    library. This library has multiple useful features for our analysis. One of the
    advantages of selecting this library is that, while it adds functionality for
    manipulating and visualizing geospatial data, it keeps the user-friendliness and
    versatility of the `pandas` library. We load the files with incremental resolution
    for the territory information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The data is loaded using the `geopandas` `read_file` function. This returns
    a `GeoDataFrame` object, a special type of DataFrame. It’s an extension of the
    DataFrame objects used with `pandas` and includes geospatial data. If a DataFrame
    typically includes columns of type integer, float, text, and date, a GeoDataFrame
    will also include columns with data specific for spatial analysis, for example,
    polygons associated with the representation of geospatial regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is useful to inspect the geospatial data before using it to clip the Voronoi
    polygons. Let’s visualize the three different resolution data. We can do this
    using the plot function associated with each **GeoDataFrame**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B20963_04_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Shapefile data for the UK for the entire territory, country level,
    and county level (left to right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We already observed that the pubs are only in England, Scotland, and Wales,
    not in Northern Ireland. If we clip the Voronoi polygons for the pubs using the
    UK-level data, we could encounter a situation where Voronoi polygons containing
    pubs from the western coast of England and Wales might spill over into the territory
    of Northern Ireland. This could result in an unwanted artifact. To avoid this,
    we can process the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract from the country-level shapefile only the data for England, Scotland,
    and Wales.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merge the polygon data from the three countries using the `dissolve` method
    from `geopandas`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting content is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: Shapefile data with England, Scotland, and Wales after filtering
    Northern Ireland and using dissolve to merge the polygons'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have the right clipping polygon for the Voronoi polygons from the three
    countries. Before clipping the polygons, we need to extract them from the Voronoi
    object. The following code does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we have everything we need to perform the clipping operation. We
    start by converting the list of Voronoi polygons into a `GeoDataFrame` object,
    similar to the `uk_countries_dissolved` object that we will use to clip them.
    We are clipping the polygons so that, when we represent them, the polygons will
    not extend over the boundary. For the clipping operation to be performed correctly,
    and without errors, we will have to use the same projection as for the clipping
    object. We use the `clip` function from the `geopandas` library. This operation
    is highly time- and CPU-intensive. On Kaggle infrastructure, running the entire
    operation (with the CPU) for the 45,000 polygons in our list takes 35 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code plots the entire collection of clipped polygons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure 4.20*, we can see the resulting plot. There are areas with a larger
    concentration of pubs (smaller polygons) and areas where there is a large distance
    between two pubs (in certain areas of Scotland, for example).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.20: Voronoi polygons from the pub geospatial distribution, clipped
    using the dissolved country-level data from the three countries selected (England,
    Wales, and Scotland)'
  prefs: []
  type: TYPE_NORMAL
- en: Another modality to show the spatial distribution of pubs is to aggregate the
    data at the local authority level and to build Voronoi polygons around the geospatial
    center of the pub distribution for that local authority. Each new Voronoi polygon
    center is the mean latitude/longitude coordinates for each pub in the current
    local authority. The resulting polygon mesh does not reconstruct the spatial distribution
    of the local authorities, but it represents with good accuracy the relative pub
    distribution. The resulting Voronoi polygon set is clipped using the same clipping
    polygons as before. To be more precise, before we used clipping polygons, the
    contours were obtained by dissolving the country-level shapefile data. We can
    use a graded colormap to represent the density of the pubs per area. Let’s see
    the code to create and visualize this mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a dataset that contains, for each local authority, the number
    of pubs and the average latitude and longitude of the pub locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we calculate the Voronoi polygons associated with this distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We clip the resulting polygons with the same polygon used before for clipping
    (a result of selecting England, Wales, and Scotland and dissolving the shapefiles
    into one single shapefile):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code plots the Voronoi polygons from the aggregated pub geospatial
    distribution at a local authority level (the center of a Voronoi polygon is the
    average latitude/longitude for all pubs in the local authority area), clipped
    using the dissolved country-level (three countries are selected: England, Scotland,
    and Wales) data. We use a green color gradient for the pub density per area (see
    *Figure 4.21*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We used Voronoi polygons to visualize the pubs’ geographical distribution. In
    *Figure 4.20*, we show each polygon with a different color. Because the points
    inside a Voronoi polygon are closer to the polygon center than to any other neighboring
    polygon center, each polygon is approximately the area covered by the pub positioned
    in the center of the polygon. In *Figure 4.21*, we use Voronoi polygons to build
    around the geometrical center of the pubs’ distribution inside each local authority.
    We then use a color gradient to represent the relative pub density for each local
    authority. By using these original visualization techniques, we were able to represent
    more intuitively the spatial distribution of the pubs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.21: Voronoi polygons with a color intensity proportional to the pub
    density per local authority area'
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to investigate this data in the upcoming section, when we mix
    data from the pub dataset with the data from the Starbucks dataset. We intend
    to combine the information from the two datasets, using the Voronoi polygon areas
    to evaluate the relative distances between pubs and Starbucks in the London area.
  prefs: []
  type: TYPE_NORMAL
- en: By manipulating Voronoi polygons generated for pubs and Starbucks coffee shops,
    we will analyze the relative spatial distribution of pubs and Starbucks, generating
    maps where we can see, for example, a group of pubs that are closest to a Starbucks.
    The geometric properties of Voronoi polygons will prove to be extremely useful
    to do this.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, let’s proceed and explore the Starbucks dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Starbucks around the world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start the analysis for the *Starbucks Locations Worldwide* dataset with a
    detailed **Exploratory Data Analysis** (**EDA**) in the notebook *Starbucks Location
    Worldwide - Data Exploration*. (see *Reference 6*). You might want to follow the
    notebook in parallel with the text in the current section. The tools used in this
    dataset are imported from the `data_quality_stats` and `plot_style_utils` utility
    scripts. Before starting our analysis, it is important to explain that the dataset
    used for this analysis is from Kaggle and was collected 6 years ago.
  prefs: []
  type: TYPE_NORMAL
- en: Preliminary data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset has 25,600 rows. Some fields have just a few missing values. **Latitude**
    and **Longitude** have 1 value missing each, while there are 2 missing values
    for **Street Address** and 15 missing values for **City**. The fields that have
    the most missing data are **Postcode** (5.9%) and **Phone Number** (26.8%). In
    *Figure 4.22*, we can see a sample of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B20963_04_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: First rows of the Starbucks Locations Worldwide dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the most frequent values report, we can learn a few interesting
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B20963_04_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.23: The most frequent values for the Starbucks Locations Worldwide
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the state with the greatest number of Starbucks coffee shops is
    CA (USA). As per the city, the largest number of shops are in Shanghai. There
    is a unique address with up to 11 shops. Additionally, most of the shops per timezone
    are in the New York timezone .
  prefs: []
  type: TYPE_NORMAL
- en: Univariate and bivariate data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this dataset, I chose a color map blending the colors of Starbucks with
    green and shades of brown, like those of the high-quality roasted coffee they
    offer to their clients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shape  Description automatically generated](img/B20963_04_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.24: Notebook color map, blending Starbucks colors with the shades
    of roasted coffee'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the preceding custom colormap for the univariate analysis graphs.
    In the following figure, we show the distribution of coffee shops by country code.
    Most Starbucks are in the United States, with over 13,000 entries, followed by
    China, Canada and Japan:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.25: Coffee shops by country code. The US has the most, followed by
    China, Canada, and Japan'
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the distribution by state/province in *Figure 4.26*, we can see
    that in first place is California, with more than 25,000\. In second place is
    Texas, with over 1,000 coffee shops, and in third place is England, with fewer
    than 1,000\. Distribution by timezone shows that the most represented is the US
    East Coast timezone (the New York timezone).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.26: Coffee shops by State/Province code. California (CA) has the largest
    number of coffee shops, followed by Texas (TX)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, the majority of the coffee shops are in the New York (US East
    Coast) timezone:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.27: Coffee shops by timezone code, with the majority in the New York
    (US East Coast) timezone'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on, the ownership of Starbucks coffee shops is shown in *Figure 4.28*.
    We can observe that most of the coffee shops are company-owned (12,000), followed
    by licensed (more than 9,000), joint ventures (4,000), and franchises (fewer than
    1,000):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.28: Coffee shop ownership types'
  prefs: []
  type: TYPE_NORMAL
- en: 'It will be interesting to see next how the ownership type varies depending
    on the country. Let’s represent the company ownership by country. The following
    figure shows the number of coffee shops per country for the top 10 countries.
    We use a logarithmic scale, due to data skewness (a measure of the asymmetry of
    the probability distribution). In other words, in a small number of countries,
    there are many coffee shops, while in the rest of the countries, there is a much
    smaller number. The United States has two types of ownership: company-owned and
    licensed. China has mostly joint ventures and company-owned, with a smaller number
    of licensed. In Japan, the majority of shops are joint ventures, with almost an
    equal amount of licensed and company-owned.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a number of countries/regions  Description automatically generated](img/B20963_04_29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.29: Coffee shops per country, grouped by ownership type'
  prefs: []
  type: TYPE_NORMAL
- en: In the following figure, we show the number of coffee shops per city, grouped
    by ownership type. Because the city’s names are written in multiple forms (with
    vernacular characters in lowercase and uppercase), I first unified the notation
    (and aligned all with an English name). The first cities are Shanghai, Seoul,
    and Beijing. Shanghai and Seoul have joint-venture coffee shops, while Beijing
    has only company-owned Starbucks coffee shops.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of different colored bars  Description automatically generated](img/B20963_04_30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.30: Coffee shops per city, grouped by ownership type'
  prefs: []
  type: TYPE_NORMAL
- en: We performed univariate and bivariate analyses on the Starbucks coffee shops
    dataset. Now, we have a good understanding of the feature distribution and interactions.
    Moving on, let’s perform another geospatial analysis, using and extending the
    tools that we already tested with the analysis of the pubs in England.
  prefs: []
  type: TYPE_NORMAL
- en: Geospatial analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by observing the distribution of Starbucks in the world. We use the
    folium library and MarkerCluster to represent on a dynamic map the geospatial
    distribution of coffee shops in the entire world. The code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Folium/leaflet maps are browsable. We can pan, zoom in, and zoom out. In *Figure
    4.31*, we show the entire world’s distribution of coffee shops:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Map  Description automatically generated](img/B20963_04_31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.31: Worldwide Starbucks coffee shop distribution using folium over
    leaflets and MarkerCluster'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4.32*, we show a zoom-in on the continental United States and Canada
    area. Clearly, the East and West coasts dominate Starbucks coffee shop distribution
    in the United States.
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bubble chart  Description automatically generated](img/B20963_04_32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.32: Starbucks coffee shop distribution in the United States'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to represent the spatial distribution of Starbucks coffee shops
    is to use the `geopandas` plot function. First, we will show the number of shops
    per country. For this, we aggregate the coffee shops per country:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To represent the geospatial distribution with `geopandas`, we need to use the
    `ISO3` country codes (country codes with three letters). In the Starbucks distribution
    dataset, we have only `ISO2` (country codes with two letters). We can include
    a dataset that contains the equivalences, or we can import a Python package that
    will do the conversions for us. We will opt for the second solution and `pip install`,
    and then import the `country-conversion` Python package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, using `geopandas`, we load a dataset with polygon shapes for all countries,
    with low resolution. We then merge the two datasets (with shops and polygons):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Before displaying the country polygons, with the fill color adjusted to represent
    proportionally the number of Starbucks coffee shops in the current country, we
    will display a wireframe with all the countries so that we can also see on the
    map the countries without Starbucks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'With `geopandas`, we can apply a logarithmic colormap, which helps in representing
    the total number of Starbucks coffee shops across countries with a skewed distribution
    more effectively. It ensures a well-distributed color scheme, allowing us to differentiate
    between countries with a smaller number of coffee shops and those at the top in
    this regard. We also draw some of the latitude lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This map is informative, but these countries have very different areas, populations,
    and population densities. To understand better the density of Starbucks coffee
    shops, we will also plot the number of shops per million citizens (for each country)
    and the number of shops per 1,000 square kilometers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.33: geopandas map showing the coffee shop density (the log scale)
    at the world level'
  prefs: []
  type: TYPE_NORMAL
- en: For the preceding map, we chose `geopanda`s precisely because it allows us to
    represent regions with color intensity on a logarithmic scale.
  prefs: []
  type: TYPE_NORMAL
- en: In the `world` dataset, we have the population estimate but we do not have the
    country area information. To calculate the Starbucks density per square kilometers,
    we need to also include the area. We can include a new dataset, with the country
    area, or we can use the features of `geopandas` to obtain the area from the polygons.
    With the current Mercator projection adapted to display the map in a way that
    is legible, the area is not calculated correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will copy the `world` dataset so that the transformation does not distort
    the polygons in the Mercator projection. We will then apply the transformation
    on the copy using the `Cylindrical equal-area` projection. This projection preserves
    the areas, and this is what we need for our calculations. After we perform the
    transformation, we concatenate the area to the `world` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s verify that we have calculated the area correctly. We sample a few countries,
    and we verify that the areas match official records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![Graphical user interface, table, calendar  Description automatically generated
    with medium confidence](img/B20963_04_34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.34: Area verification for the United States of America, Romania, and
    the UK'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, for all the countries, the calculated area with the method used
    yielded correct surfaces that match the values in official records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have all we need to prepare and display the maps with Starbucks densities
    per country, relative to area and population. The code for the calculation of
    the Starbucks densities is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, using the following code, we draw the distribution of Starbucks per 1
    million people at the country level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We show the graph drawn with the preceding code in *Figure 4.35*. The countries
    with the highest number of Starbucks per million people are the United States,
    Canada, and the United Emirates, followed by Taiwan, South Korea, the UK, and
    Japan.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing chart  Description automatically generated](img/B20963_04_35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.35: Starbucks per 1 million people – distribution per country'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each country, the number of Starbucks coffee shops per 1,000 square kilometers
    is displayed in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing chart  Description automatically generated](img/B20963_04_36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.36: Starbucks per 1,000 square kilometers – distribution per country'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the highest concentration of coffee shops is in countries like
    South Korea, Taiwan, Japan, and the UK.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do a quick summary to wrap up this section. We analyzed the two datasets
    with pubs in England and Starbucks worldwide to get a good understanding of the
    data distribution in the two datasets. We also introduced several techniques and
    tools for geospatial data manipulation and analysis. We learned how to draw shapefile
    data, how to extract polygons from a shapefile, how to clip polygon sets using
    another set of polygons, and how to generate Voronoi polygons. This was all preparation
    for the main part of the analysis in this chapter, where we will combine the two
    datasets and learn how to generate multi-layer maps, where the information from
    the two datasets is combined creatively. Our goal is twofold: to introduce you
    to more advanced ways to analyze geospatial data, and to use creatively the methods
    introduced to see how we can get insights from both data sources, once combined.'
  prefs: []
  type: TYPE_NORMAL
- en: Pubs and Starbucks in London
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, our analysis was focused on the individual datasets `Every Pub in
    England` and `Starbucks Locations Worldwide`. To support some of the data analysis
    tasks related to these two separate datasets, we have also added two more datasets,
    one with the geographical position of postal codes, replacing the missing latitude
    and longitude data, and one with shapefile data for the UK to clip the Voronoi
    polygons generated from pubs’ positions, aligning them with the land contour of
    the island.
  prefs: []
  type: TYPE_NORMAL
- en: In the current section, we will combine the information from the two main data
    sources analyzed separately and apply methods developed during this preliminary
    analysis, supporting the objective of our study. This will focus on a smaller
    region, where we have both a high density of pubs and a concentration of Starbucks
    coffee shops, in London. We can already hypothesize that the geospatial concentration
    of Starbucks is smaller than the concentration of pubs.
  prefs: []
  type: TYPE_NORMAL
- en: We would like to see where the closest Starbucks is so that we can sober up
    with a coffee after we’ve had a few pints of ale. We already learned that Voronoi
    polygons have an interesting characteristic – any point inside a polygon is closer
    to its center than to any neighboring center. We will represent the pub locations
    in the London area, superposed over the Voronoi polygons generated from the Starbucks
    locations in the same area.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook associated with this section is `Coffee or Beer in London – Your
    Choice!`, (see *Reference 11*). You might find it useful to follow the notebook
    along with the text in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by reading the CSV files from the two datasets, `Every Pub in England`
    and `Starbucks Locations Worldwide`. We also now read the `GBR_adm2.shp` shapefile
    (with Great Britain local authorities’ borders data) from `GDM Data for the UK`
    and the data from `Open Postcode Geo`. In this last file, we just filter four
    columns (postcode, country, latitude, and longitude).
  prefs: []
  type: TYPE_NORMAL
- en: 'From the pub data, we only select the entries that have as a local authority
    one of the 32 London boroughs. We add to this subset the City of London, which
    is not one of the boroughs. The City of London is in the center of London, and
    some of the pubs are located there, which we would like to include. We use the
    same list to filter the data in the shapefile data. To check that we have correctly
    selected all the shapefile data, we display the boroughs (and the City of London)
    polygons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the following figure, observe that the City of London is missing (left).
    We have London in shapefile names, so we will just replace London with the City
    of London in the shapefile data. After the correction (right), we can see that
    by unifying the notation for the City of London, we now have all local authorities
    correctly represented on our map. Now, we have selected all areas that we want
    to include in our analysis of pubs and Starbucks coffee shops in the London area.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.37: London boroughs (left) and the London boroughs and the City of
    London (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also select Starbucks coffee shop data for the same sub-regions. For Starbucks
    data, the selection is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We incorporate country information into the filtering criteria because London
    and the names of various other London boroughs are found across North America,
    where many cities borrow names from Great Britain.
  prefs: []
  type: TYPE_NORMAL
- en: We are aware, based on our previous analysis of the pub data, that some pubs
    have missing latitude and longitude, marked with **\\N**. Carry out the same transformations,
    including merging with the `Open Postcode Geo` data and cleaning, as discussed
    in the previous subsection for these pub rows. This process will involve assigning
    latitude and longitude data based on postcode matching.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, using the following code, we check that the pubs and Starbucks selected
    with the preceding criteria are all within the boundaries (or very close to those)
    of the London boroughs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We observe that there are two Starbucks that are quite remote from London.
    We set an additional condition for the selected Starbucks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the resulting figure, you’ll see the pubs (cross) and Starbucks (points)
    in the London boroughs and the City of London, after filtering items outside these
    local authorities’ zones and correcting the misattributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.38: Pubs (cross) and Starbucks (points) in the London boroughs and
    the City of London after filtering items'
  prefs: []
  type: TYPE_NORMAL
- en: There are still a few points outside the boundaries, but for now, we should
    be fine. These points will be filtered out once we use the local authority polygons
    to clip the Voronoi polygons associated with each pub and coffee shop. We observed
    a strange artifact regarding the alignment of Starbucks. All Starbucks shops seem
    to be aligned horizontally. This is because Starbucks positions are given with
    only two decimals (Starbucks coffee shops are from a global geolocation dataset,
    where the location is given with smaller precision), while the pubs are given
    six decimals. Consequently, the Starbucks shops appear to be aligned. Their positions
    are rounded to two decimals, and due to the close position of the coffee shops,
    they appear to be aligned, especially along the latitude lines.
  prefs: []
  type: TYPE_NORMAL
- en: Geospatial analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s represent the Voronoi polygons for pubs and Starbucks shops in London
    and its boroughs. We start by generating those polygons, using the same code we
    used before for our data analysis on `Every Pub in England`. First, let’s do the
    pubs in the area. The code in the notebook is now more compact, since we are using
    the `geospatial_utils` utility script. The following code generates the object
    with the Voronoi polygons collection and then visualizes the collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: For this, the preceding code uses two functions defined in `geospatial_utils`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first function, `get_voronoi_polygons`, creates a list of Voronoi polygons
    from a list of points, with the *x* and *y* coordinates representing the longitude
    and latitude, respectively. To do this, it uses the Voronoi function in the `scipy.spatial`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The second function, `plot_voronoi_polygons`, plots a `spacy.spatial.Voronoi`
    object, which is a collection of Voronoi polygons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The collection of polygons generated is first extracted as a list of polygons,
    using the `extract_voronoi_polygon_list` function already defined in the previous
    section (and which was just moved to the new utility script). Then, the polygons
    are clipped using the external boundary of London boroughs, obtained by dissolving
    the `borroughs_df` GeoDataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for `clip_polygons` is defined as well in the `geospatial_utils` utility
    script. In the `clip_polygons` function, we use a list of polygons, `poly_clipping`,
    to clip polygons in another list, `poly_list_origin`. We transform the list with
    original polygons, `poly_list_origin`, in a `geopandas` DataFrame. We perform
    the clipping operation using the geopandas `clip` function. The resulting list
    of polygons clipped, `polygons_clipped`, is returned by the `clip_polygons` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the Voronoi polygons from pub locations in London
    (left) and the boroughs’ boundary (right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.39: Pubs’ Voronoi polygons in the London boroughs and the City of
    London (left) and the London boroughs’ boundary (right). We use the boundary polygon
    to clip the Voronoi polygons'
  prefs: []
  type: TYPE_NORMAL
- en: The following figure shows the boroughs’ boundary and the pubs’ position, as
    well as the Voronoi polygons associated with these locations. We can observe that
    the areas with the greatest pub density are in the City of London and its neighboring
    boroughs to the west, except for Tower Hamlets, which has only one pub.
  prefs: []
  type: TYPE_NORMAL
- en: '![Map  Description automatically generated](img/B20963_04_40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.40: Pubs’ Voronoi polygons in the London boroughs and the City of
    London (clipped), showing pubs’ locations and boroughs’ boundaries'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we perform the same operations for the Starbucks coffee shop locations.
    We generate the Voronoi polygons and clip them with the same London borough border
    polygon, obtained by dissolving all the boroughs’ polygons. The following figure
    shows the boroughs’ boundaries and the Starbucks shops’ positions, as well as
    the Voronoi polygons associated with these locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Map  Description automatically generated](img/B20963_04_41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.41: Starbucks’ Voronoi polygons in the London boroughs and the City
    of London (clipped), showing the shops’ locations and boroughs’ boundaries'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for generating the Voronoi polygons object, visualizing it, extracting
    from it the list of polygons, and then clipping it is given here. First, let’s
    see the code to generate the Voronoi polygons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next is the code for extracting the list of polygons from the Voronoi polygons
    object and the code for clipping the polygons, using the borroughs’ boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `within_polygon` function, we can identify the locations that are
    inside a polygon. The function is implemented in the `geospatial_utils` module.
    The function uses the `within` property of the `Point` object from the `shapely.geometry`
    library module. We apply the operation, for a given polygon, to all the points
    created from the longitude/latitude of all items (in our case, the pubs), getting
    the status (`within`, `outside`) of points relative to the reference polygon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code applies the `within_polygon` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In the following figure, the pubs within the selected area (shown in the notebook
    associated with the book by a light brown and dark green fill color) are closer
    to the position of the Starbucks coffee shop centered on the selected area than
    to any other neighboring Starbucks coffee shop. The rest of the pubs are shown
    with a light green color. We can repeat the procedure for all polygons (and also
    for the boroughs’ polygons).
  prefs: []
  type: TYPE_NORMAL
- en: '![Map  Description automatically generated](img/B20963_04_42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.42: Pubs outside and inside a Starbucks Voronoi polygon area'
  prefs: []
  type: TYPE_NORMAL
- en: We can represent the same items, pubs and Starbucks coffee shops, using folium
    maps too. These maps will allow interactions, including zooming in, zooming out,
    and panning. We can add multiple layers over a base map. Let’s start by representing
    the London boroughs as the first layer of the map. On top of that, we will show
    the pubs in the London area. Each pub will have a popup as well, displaying the
    name of the pub and the address. We can select from multiple map tile providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because I prefer to have a clearer background, I opted for two tile sources:
    “Stamen toner” and “CartoDB positron.” For both options, the tiles are either
    black and white or pale colors, so the overlapping layers can be seen more easily.
    The following is the code to show the tiles (with “Stamen toner”) in the London
    area, the contour of London boroughs (first layer of the map), and each pub location
    with `CircleMarker` (the second layer over the map). Each pub will have a popup,
    showing the pub name and address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure displays the map created with the preceding code. On this
    map, we show on superposed layers the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: The London boroughs and City of London map areas, using “Stamen Toner” tiles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The London boroughs and City of London boundaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pubs in the preceding areas, shown with `CircleMarker`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, for each pub, if selected, one popup showing the pub name and address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Map  Description automatically generated](img/B20963_04_43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.43: Leaflet map with the boroughs’ borders and the pub locations in
    the London area'
  prefs: []
  type: TYPE_NORMAL
- en: In the notebook, I show more images with Starbucks Voronoi polygons and locations,
    as well as maps with multiple layers of polygons and markers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful operation that we can perform is to calculate the area of polygons.
    The function to calculate areas for all polygons in a GeoDataFrame is `get_polygons_area`,
    which is also defined in `geospatial_utils`. It applies a transformation to the
    projection, in `cylindrical equal area`, on a copy of the GeoDataFrame. This projection
    will preserve the areas. We then add the `area` column to the original GeoDataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We calculate the area for the boroughs, and then we count the number of pubs
    per borough. Then, we divide the number of pubs/boroughs by the borough area to
    obtain the pub density (in pubs per square kilometer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to represent the density with a continuous color scale, but we
    would like to use colors from our custom colormap. We can create our own continuous
    color map and use as seeds a few of the colors in our color list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'For the pub density graph, we would like to use, with this custom colormap,
    a logarithmic scale. We can achieve this with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the pub numbers in each borough (left) and the pub
    density per borough (right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_04_44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.44: Pub numbers (left) and pub density on a logarithmic scale (right)
    per London borough'
  prefs: []
  type: TYPE_NORMAL
- en: In the notebook associated with this section, *Coffee or Beer in London – Your
    Choice!* (see *Reference 11*), I also show the pub numbers and pub density per
    Starbucks Voronoi polygons area. The various techniques displayed in this section
    have hopefully equipped you with a starting toolset for the analysis and visualization
    of geospatial data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to work with geographical information and maps,
    how to manipulate geometry data (clip and merge polygon data, cluster data to
    generate maps with fewer details, and remove subsets of geospatial data), and
    superpose several layers of data over maps. We also learned how to modify and
    extract information from a shapefile using `geopandas` and custom code, as well
    as creating or calculating geospatial features, like terrain area or geospatial
    object density. Additionally, we extracted reusable functions and grouped them
    in two utility scripts, which is Kaggle terminology for independent Python modules.
    These utility scripts can be imported like any other library and integrated with
    your notebook code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to put to try out some of the tools and techniques
    for geospatial analysis, for a data analytics competition.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every Pub in England, Kaggle Datasets: [https://www.kaggle.com/datasets/rtatman/every-pub-in-england](https://www.kaggle.com/datasets/rtatman/every-pub-in-england)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Every Pub in England – Data Exploration, Kaggle Notebook: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-04/every-pub-in-england-data-exploration.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-04/every-pub-in-england-data-exploration.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Starbucks Locations Worldwide, Kaggle Datasets: [https://www.kaggle.com/datasets/starbucks/store-locations](https://www.kaggle.com/datasets/starbucks/store-locations)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open Postcode Geo, Kaggle Datasets: [https://www.kaggle.com/datasets/danwinchester/open-postcode-geo](https://www.kaggle.com/datasets/danwinchester/open-postcode-geo)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'GADM Data for UK, Kaggle Datasets: [https://www.kaggle.com/datasets/gpreda/gadm-data-for-uk](https://www.kaggle.com/datasets/gpreda/gadm-data-for-uk)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Starbucks Location Worldwide – Data Exploration, Kaggle Notebook: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-04/starbucks-location-worldwide-data-exploration.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-04/starbucks-location-worldwide-data-exploration.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Polygon overlay in Leaflet Map: [https://stackoverflow.com/questions/59303421/polygon-overlay-in-leaflet-map](https://stackoverflow.com/questions/59303421/polygon-overlay-in-leaflet-map)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geopandas area: [https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.area.html](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.area.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scipy Spatial Voronoi – extract Voronoi polygons and represent them: [https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Voronoi.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Voronoi.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Getting polygon areas using GeoPandas: [https://gis.stackexchange.com/questions/218450/getting-polygon-areas-using-geopandas](https://gis.stackexchange.com/questions/218450/getting-polygon-areas-using-geopandas)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Coffee or Beer in London – Your Choice!, Kaggle Notebook: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-04/coffee-or-beer-in-london-your-choice.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-04/coffee-or-beer-in-london-your-choice.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/kaggle](https://packt.link/kaggle)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code9220780366773140.png)'
  prefs: []
  type: TYPE_IMG
