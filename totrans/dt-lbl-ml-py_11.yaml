- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labeling Audio Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will embark on this transformative journey through the realms
    of real-time audio capture, cutting-edge transcription with the Whisper model,
    and audio classification using a **convolutional neural network** (**CNN**), with
    a focus on spectrograms. Additionally, we’ll explore innovative audio augmentation
    techniques. This chapter not only equips you with the tools and techniques essential
    for comprehensive audio data labeling but also unveils the boundless possibilities
    that lie at the intersection of AI and audio processing, redefining the landscape
    of audio data labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to a journey through the intricate world of audio data labeling! In
    this chapter, we embark on an exploration of cutting-edge techniques and technologies
    that empower us to unravel the richness of audio content. Our adventure unfolds
    through a diverse set of topics, each designed to enhance your understanding of
    audio processing and labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Our journey begins with the dynamic realm of real-time audio capture using microphones.
    We delve into the art of voice classification, using the random forest classifier
    to discern and categorize distinct voices in the captured audio.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we introduce the groundbreaking Whisper model, a powerful tool for transcribing
    uploaded audio data. Witness the seamless integration of the Whisper model with
    OpenAI for accurate transcriptions, followed by a meticulous labeling process.
    As we unfold the capabilities of the Whisper model, we draw insightful comparisons
    with other open source models dedicated to audio data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Our journey takes a visual turn as we explore the creation of spectrograms,
    visually capturing the intricate details of sound. The transformative CNNs come
    into play, elevating audio classification through visual representations. Learn
    the art of labeling spectrograms, unraveling a new dimension in audio processing.
  prefs: []
  type: TYPE_NORMAL
- en: Prepare to expand your horizons as we venture into the realm of augmented data
    for audio labeling. Discover the transformative impact of noise augmentation,
    time-stretching, and pitch-shifting on audio data. Uncover the techniques to enhance
    the robustness of your labeled audio datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Our exploration culminates in the innovative domain of Azure Cognitive Services.
    Immerse yourself in the capabilities of Azure to transform speech to text and
    achieve speech translation. Witness the seamless integration of Azure Cognitive
    Services, revolutionizing the landscape of audio processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Capturing real-time voice using a microphone and classifying voices using the
    random forest classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploading audio data and transcribing an audio file using OpenAI’s Whisper model
    and then labeling the transcription.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comparison of the Whisper model with other open source models for audio data
    analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a spectrogram for audio data and then labeling the spectrogram, using
    CNN for audio classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmenting audio data such as noise augmentation, time-stretching, and pitch-shifting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Cognitive Services for speech-to-text and speech translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to install the following Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '**openai-whisper** is the Python library provided by OpenAI, offering access
    to the powerful Whisper **Automatic Speech Recognition** (**ASR**) model. It allows
    you to transcribe audio data with state-of-the-art accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**librosa** is a Python package for music and audio analysis. It provides tools
    for various tasks, such as loading audio files, extracting features, and performing
    transformations, making it a valuable library for audio data processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**pytube** is a lightweight, dependency-free Python library for downloading
    YouTube videos. It simplifies the process of fetching video content from YouTube,
    making it suitable for various applications involving YouTube data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**transformers** is a popular Python library developed by Hugging Face. It
    provides pre-trained models and various utilities for **natural language processing**
    (**NLP**) tasks. This includes transformer-based models such as BERT and GPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**joblib** is a Python library for lightweight pipelining in Python. It is
    particularly useful for parallelizing and caching computations, making it efficient
    for tasks involving parallel processing and job scheduling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Downloading FFmpeg
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**FFmpeg** is a versatile and open source multimedia framework that facilitates
    the handling, conversion, and manipulation of audio and video files ([https://ffmpeg.org/download.html](https://ffmpeg.org/download.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: To download FFmpeg for macOS, select `static FFmpeg binaries for macOS 64-bit`
    from [https://evermeet.cx/ffmpeg/](https://evermeet.cx/ffmpeg/). Download `ffmpeg-6.1.1.7z`
    and extract and copy it to your `<home directory>/<new folder>/bin`. Change `ffmpeg`
    executable file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download FFmpeg for a Windows OS, select Windows builds by BtbN: [https://github.com/BtbN/FFmpeg-Builds/releases](https://github.com/BtbN/FFmpeg-Builds/releases).
    Download `ffmpeg-master-latest-win64-gpl.zip`. Extract and set the `path` environment
    variable of the extracted `ffmpeg` bin folder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter is available at GitHub here: [https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch11](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch11).'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to explore the Whisper model along with other machine learning models
    available in the Azure Machine Learning model catalog, you can create a free Azure
    account at [https://azure.microsoft.com/en-us/free](https://azure.microsoft.com/en-us/free).
    Then, you can try Azure Machine Learning for free at [https://azure.microsoft.com/en-us/products/machine-learning/](https://azure.microsoft.com/en-us/products/machine-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: Real-time voice classification with Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an era marked by the integration of advanced technologies into our daily
    lives, real-time voice classification systems have emerged as pivotal tools across
    various domains. The Python script in this section, showcasing the implementation
    of a real-time voice classification system using the Random Forest classifier
    from scikit-learn, is a testament to the versatility and significance of such
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: The primary objective of this script is to harness the power of machine learning
    to differentiate between positive audio samples, indicative of human speech (voice),
    and negative samples, representing background noise or non-vocal elements. By
    employing the Random Forest classifier, a robust and widely used algorithm from
    the scikit-learn library, the script endeavors to create an efficient model capable
    of accurately classifying real-time audio input.
  prefs: []
  type: TYPE_NORMAL
- en: The real-world applications of this voice classification system are extensive,
    ranging from enhancing user experiences in voice-controlled smart devices to enabling
    automated voice commands in robotics. Industries such as telecommunications, customer
    service, and security can leverage real-time voice classification to enhance communication
    systems, automate processes, and bolster security protocols.
  prefs: []
  type: TYPE_NORMAL
- en: Whether it involves voice-activated virtual assistants, hands-free communication
    in automobiles, or voice-based authentication systems, the ability to classify
    and understand spoken language in real time is pivotal. This script provides a
    foundational understanding of the implementation process, laying the groundwork
    for developers and enthusiasts to integrate similar voice classification mechanisms
    into their projects and contribute to the evolution of voice-centric applications
    in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the Python script that demonstrates a real-time voice classification
    system, using the Random Forest classifier from scikit-learn. The goal is to capture
    audio samples, distinguish between positive samples (voice) and negative samples
    (background noise or non-voice), and train a model for voice classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s us see the Python code that provides a simple framework to build a real-time
    voice classification system, allowing you to collect your own voice samples to
    train and test the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Import the Python libraries**: First, let’s import the requisite libraries
    using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`capture_audio` function uses the `sounddevice` library to record real-time
    audio. The user is prompted to speak, and the function captures audio for a specified
    duration (the default is five seconds):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`collect_training_data` function gathers training data for voice and non-voice
    samples. For positive samples (voice), the user is prompted to speak, and audio
    data is recorded using the `capture_audio` function. For negative samples (background
    noise or non-voice), the user is prompted to create ambient noise without speaking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`X`) and corresponding labels (`y`). The data is shuffled to ensure a balanced
    distribution during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`train_test_split` function from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`VoiceClassifier` class is defined, encapsulating the random forest model.
    An instance of the `VoiceClassifier` is created, and the model is trained using
    the positive and negative training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Make predictions**: The trained model predicts labels for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`accuracy_score` function, comparing predicted labels with actual labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When you run this code, it prompts with the following pop-up window to enter
    and speak.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Prompt to start speaking
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you speak a few sentences that will be recorded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Trained model accuracy
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomForestClassifier`, the model was previously trained to discern between
    positive samples (voice) and negative samples (non-voice or background noise).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary objective of this script is to demonstrate the seamless integration
    of the pre-trained voice classification model into a real-time voice inference
    system. You are prompted to provide audio input by pressing *Enter* and speaking
    for a few seconds, after which the model predicts whether the input contains human
    speech or non-voice elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Inference output
  prefs: []
  type: TYPE_NORMAL
- en: In a similar manner, we can use this model to label a voice as male or female
    to analyze customer calls and understand target customers.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the real-time voice classification inference that holds significant
    relevance in numerous scenarios, including voice-activated applications, security
    systems, and communication devices. By loading a pre-trained model, users can
    experience the instantaneous and accurate classification of voice in real-world
    situations.
  prefs: []
  type: TYPE_NORMAL
- en: Whether applied to enhance accessibility features, automate voice commands,
    or implement voice-based security protocols, this script serves as a practical
    example of deploying machine learning models for voice classification in real-time
    scenarios. As technology continues to advance, the seamless integration of voice
    inference models contributes to the evolution of user-friendly and responsive
    applications across various domains.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how to transcribe audio using the OpenAI Whisper model.
  prefs: []
  type: TYPE_NORMAL
- en: Transcribing audio using the OpenAI Whisper model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to see how to transcribe audio file to text using
    the **OpenAI Whisper** model and then label the audio transcription using the
    OpenAI **large language** **model** (**LLM**).
  prefs: []
  type: TYPE_NORMAL
- en: '**Whisper** is an open source ASR model developed by OpenAI. It is trained
    on nearly 700,000 hours of multilingual speech data and is capable of transcribing
    audio to text in almost 100 different languages. According to OpenAI, Whisper
    “approaches human level robustness and accuracy on English speech recognition.”'
  prefs: []
  type: TYPE_NORMAL
- en: In a recent benchmark study, Whisper was compared to other open source ASR models,
    such as wav2vec 2.0 and Kaldi. The study found that Whisper performed better than
    wav2vec 2.0 in terms of accuracy and speed across five different use cases, including
    conversational AI, phone calls, meetings, videos, and earnings calls.
  prefs: []
  type: TYPE_NORMAL
- en: Whisper is also known for its affordability, accuracy, and features. It is best
    suited for audio-to-text use cases and is not well-suited for text-to-audio or
    speech synthesis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The Whisper model can be imported as a Python library. The other option is to
    use the Whisper model available in the model catalog at **Azure Machine** **Learning
    studio**.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the process of transcribing audio using the OpenAI Whisper ASR using
    the Python library now. It’s crucial to ensure the existence and accessibility
    of the specified audio file for successful transcription. The transcribed text
    is likely stored in `text['text']`, as indicated by the `print` statement.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to install the whisper model, as mentioned in the *technical
    requirements* section. Then, we import the OpenAI Whisper model.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – importing the Whisper model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us import the required Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `whisper` library is imported, which is the library providing access to
    the OpenAI Whisper ASR model. The `pytube` library is imported to download YouTube
    videos.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – loading the base Whisper model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us load the base Whisper model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The Whisper model is loaded using the `whisper.load_model` function with the
    `"base"` argument. This loads the base version of the Whisper ASR model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us download the audio stream from a YouTube video. Even though we are using
    a video file, we are only focusing on the audio of the YouTube video and downloading
    an audio stream from it. Alternatively, you can directly use any audio file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The YouTube video URL is specified. Using the `pytube.YouTube` class, the video
    data is fetched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This code utilizes the `pytube` library to download the audio stream from a
    video hosted on a platform such as YouTube. Let’s examine the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '`audio = data.streams.get_audio_only()`: This line fetches the audio stream
    of the video. It uses the `get_audio_only()` method to obtain a stream containing
    only the audio content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio.download()`: Once the audio stream is obtained, this line downloads
    the audio content. The download is performed in the default format, which is typically
    an MP4 file containing only the audio data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, the code extracts the audio stream from a video and downloads it
    as an MP4 file, preserving only the audio content.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – setting up FFmpeg
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whisper is designed to transcribe audio, but it requires a specific format for
    processing. The format required by Whisper for processing audio is WAV format.
    Whisper is designed to transcribe audio in WAV format, and it may not directly
    support other formats. Therefore, audio data that needs to be processed by Whisper
    should be provided in the WAV format. FFmpeg acts as a bridge by converting various
    audio formats (such as MP3, WAV, or AAC) into a format that Whisper can handle.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the input is in the MP3 format, FFmpeg can convert it to a format
    suitable for Whisper. Whisper typically requires audio data in WAV format, so
    FFmpeg can convert the input MP3 file to WAV during the process. This conversion
    allows the audio data to be in a format compatible with the requirements of the
    Whisper model.
  prefs: []
  type: TYPE_NORMAL
- en: Without this conversion, Whisper wouldn’t be able to process the audio effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scenarios where real-time transcription is needed (such as streaming a **real-time
    messaging protocol** (**RTMP**) feed), FFmpeg helps segment the audio stream.
    It splits the continuous audio into smaller chunks (e.g., 30-second segments)
    that can be processed individually. Each segment is then passed to Whisper for
    transcription:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The code sets the FFmpeg environment variable to the path of the `ffmpeg` executable.
    This is necessary for handling audio and video files.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – transcribing the YouTube audio using the Whisper model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s transcribe the YouTube audio using the Whisper model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – A snippet of the code output
  prefs: []
  type: TYPE_NORMAL
- en: The Whisper model is loaded again to ensure that it uses the base model. The
    transcribe function is called on the model with the filename of the audio file
    as an argument. The resulting transcribed text is printed using `text['text']`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The provided filename in `model.transcribe` is `Mel Spectrograms with Python
    and Librosa Audio Feature Extraction.mp4`. Make sure this file exists and is accessible
    for the code to transcribe successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see another code example on how to transcribe an audio file to text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s perform sentiment analysis to label this text transcribed from a
    customer call.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying a transcription using Hugging Face transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s use Hugging Face transformers to classify the output text from the
    previous customer call audio transcription and perform sentiment analysis to label
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The following code snippet utilizes Hugging Face’s transformers library to perform
    sentiment analysis on a given text. It begins by importing the necessary module,
    and then it loads a pre-trained sentiment analysis pipeline from Hugging Face’s
    transformers. The code defines an example text that expresses dissatisfaction
    with a product not yet received. Subsequently, the sentiment analysis pipeline
    is applied to classify the sentiment of the text, and the result is displayed
    by printing it to the console. The sentiment analysis model outputs a label indicating
    the sentiment, such as positive or negative, along with a confidence score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the Python library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Hands-on – labeling audio data using a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how to train the CNN network on audio data and
    use it to label the audio data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates the process of labeling audio data using a
    CNN. The code outlines how to employ a CNN to label audio data, specifically training
    the model on a dataset of cat and dog audio samples. The goal is to classify new,
    unseen audio data as either a cat or a dog. Let’s take the cat and dog sample
    audio data and train the CNN model. Then, we will send new unseen data to the
    model to predict whether it is a cat or a dog:'
  prefs: []
  type: TYPE_NORMAL
- en: '`load_and_preprocess_data` function. The `load_and_preprocess_data` function
    processes the audio data, converting it into mel spectrograms and resizing them
    for model compatibility.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`train_test_split`, and labels are converted to one-hot encoding.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a neural network model**: A CNN model is created using TensorFlow
    and Keras, comprising convolutional layers, pooling layers, and fully connected
    layers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compile the model**: The model is compiled with an Adam optimizer, categorical
    cross-entropy loss, and accuracy as the evaluation metric.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train the model**: The CNN model is trained on the training data for a specified
    number of epochs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluate the accuracy of the model**: The accuracy of the trained model is
    evaluated on the testing set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Save the trained model**: The trained model is saved for future use.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test the new audio file**: Finally, the saved model is loaded, and a new
    audio file (in this case, a cat meow) is processed and classified, with class
    probabilities and accuracy displayed for each class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In summary, this code provides a comprehensive guide on using a CNN to label
    audio data, from data loading and preprocessing to model training, evaluation,
    and prediction on new audio samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import all the required Python modules as the first step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 1: Load and preprocess data**: Now, let’s load and pre-process the data
    for cats and dogs. The source of this dataset is [https://www.kaggle.com/datasets/mmoreaux/audio-cats-and-dogs](https://www.kaggle.com/datasets/mmoreaux/audio-cats-and-dogs):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This code establishes the folder structure for a dataset containing the `''cat''`
    and `''dog''`, categories, with the data located at the specified directory, `''../cats_dogs/data/''`.
    Next, let’s pre-process the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This code defines a function named `load_and_preprocess_data` that loads and
    preprocesses audio data from a specified directory. It iterates through each class
    of audio, reads `.wav` files, and uses the Librosa library to convert the audio
    data into a mel spectrogram. We learned about the mel spectrogram in [*Chapter
    10*](B18944_10.xhtml#_idTextAnchor221) in the *Visualizing audio data with Matplotlib
    and Librosa – spectrogram* *visualization* section.
  prefs: []
  type: TYPE_NORMAL
- en: The mel spectrogram is then resized to a target shape (128x128) before being
    appended to the data list, along with the corresponding class labels. The function
    returns the preprocessed data and labels as NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: '`load_and_preprocess_data` function to load and preprocess the data. The labels
    are then converted into one-hot encoding using the `to_categorical` function.
    Finally, the data is split into training and testing sets with an 80–20 ratio
    using the `train_test_split` function, ensuring reproducibility with a specified
    random seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Create a neural network model**: This code defines a neural network
    model for audio classification. The model architecture includes convolutional
    layers with max pooling for feature extraction, followed by a flattening layer.
    Subsequently, there is a dense layer with ReLU activation for further feature
    processing. The final output layer utilizes softmax activation to produce class
    probabilities. The model is constructed using the Keras functional API, specifying
    the input and output layers, and is ready to be trained on the provided data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`0.001`, categorical cross-entropy as the loss function (suitable for multi-class
    classification), and accuracy as the evaluation metric. The `model.summary()`
    command provides a concise overview of the model’s architecture, including the
    number of parameters and the structure of each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Model summary
  prefs: []
  type: TYPE_NORMAL
- en: '`X_train` and `y_train`) for `20` epochs, with a batch size of `32`. The validation
    data (`X_test` and `y_test`) is used to evaluate the model’s performance during
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Test the accuracy of the model**: After training completion, it calculates
    the test accuracy of the model on the separate test set and prints the accuracy
    score, providing insights into the model’s effectiveness in classifying audio
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Accuracy of the model
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Save the** **trained model**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8: Test the new audio file**: Let’s classify the new audio file and
    label it using this saved model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This code defines a function, `test_audio`, to preprocess and classify an audio
    file. It loads and preprocesses the audio data from the specified file path using
    Librosa, generating a mel spectrogram. The spectrogram is then resized and reshaped
    to match the input dimensions expected by the model. This function is designed
    to prepare audio data for classification using a neural network model, providing
    a streamlined way to apply the trained model to new audio files for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s make `./cat-meow-14536.mp3`). The `test_audio` function is employed
    to preprocess the audio file and obtain class probabilities and the predicted
    class index. The `model.predict` method generates predictions, and the class probabilities
    are extracted from the result. The predicted class index is determined by identifying
    the class with the highest probability. This process demonstrates how the trained
    model can be utilized to classify new audio data, providing insights into the
    content of the tested audio file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code snippet iterates through all the classes in the model and
    prints the predicted probabilities for each class, based on the audio file classification.
    For each class, it displays the class label and its corresponding probability,
    providing a comprehensive view of the model’s confidence in assigning the audio
    file to each specific category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code calculates and reveals the predicted class and accuracy
    of the audio file classification. It identifies the predicted class using the
    index with the highest probability, retrieves the corresponding class label and
    accuracy from the results, and then prints the predicted class along with its
    associated accuracy. This provides a concise summary of the model’s prediction
    for the given audio file and the confidence level associated with the classification.
    Calculate and display the predicted class and accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Output showing probabilities and accuracy of the model
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to transcribe audio data using machine learning. Now, let’s
    see how to do audio data augmentation and train a model with augmented data. Finally,
    we will see and compare the accuracy with and without augmented data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring audio data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s see how to manipulate audio data by adding noise, using NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise to audio data during training helps the model become more robust
    in real-world scenarios, where there might be background noise or interference.
    By exposing a model to a variety of noisy conditions, it learns to generalize
    better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmenting audio data with noise prevents a model from memorizing specific
    patterns in the training data. This encourages the model to focus on more general
    features, which can lead to better generalization on unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This code defines a function named `add_noise` that adds random noise to an
    input data array. The level of noise is controlled by the `noise_factor` parameter.
    The function generates random noise using NumPy, adds it to the original data,
    and then returns the augmented data. To ensure the data type consistency, the
    augmented data is cast back to the same type as the elements in the original data
    array. This function can be used for data augmentation, a technique commonly employed
    in machine learning to enhance model robustness by introducing variations in the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test this function using sample audio data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Representation of the augmented data
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s retrain our CNN model using data augmentation for the classification
    of dogs and cats sounds that we saw in the *Hands-on – labeling audio data using
    a* *CNN* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we introduced data augmentation by adding random noise `(noise_factor
    * noise)` to the audio data before spectrogram conversion. This helps improve
    the model’s robustness by exposing it to varied instances of the same class during
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Accuracy of the model
  prefs: []
  type: TYPE_NORMAL
- en: By using this noise-augmented audio data, the model accuracy increased from
    0.946 to 0.964\. Depending on the data, we can apply data augmentation and test
    the accuracy to decide whether data augmentation is required.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see three more data augmentation techniques applied to an original audio
    file – time-stretching, pitch-shifting, and dynamic range compression.
  prefs: []
  type: TYPE_NORMAL
- en: The following Python script employs the librosa library for audio processing,
    loading an initial audio file that serves as the baseline for augmentation. Subsequently,
    functions are defined to apply each augmentation technique independently. Time-stretching
    alters the temporal duration of the audio, pitch-shifting modifies the pitch without
    affecting speed, and dynamic range compression adjusts the volume dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: The augmented waveforms are visually presented side by side with the original
    waveform using Matplotlib. This visualization aids in understanding the transformative
    impact of each augmentation technique on the audio data. Through this script,
    you will gain insights into the practical implementation of audio augmentation,
    a valuable practice for creating diverse and robust datasets for machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'As audio data labeling becomes increasingly integral to various applications,
    mastering the art of augmentation ensures the generation of comprehensive datasets,
    thereby enhancing the effectiveness of machine learning models. Whether applied
    to speech recognition, sound classification, or voice-enabled applications, audio
    augmentation is a powerful technique for refining and enriching audio datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – Data augmentation techniques – time stretching, pitch shifting,
    and dynamic range compression
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to another interesting topic in labeling audio data in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Azure Cognitive Services – the speech service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Azure Cognitive Services offers a comprehensive set of speech-related services
    that empower developers to integrate powerful speech capabilities into their applications.
    Some key speech services available in Azure AI include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speech-to-text (speech recognition)**: This converts spoken language into
    written text, enabling applications to transcribe audio content such as voice
    commands, interviews, or conversations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech Translation**: This translates spoken language into another language
    in real time, facilitating multilingual communication. This service is valuable
    for applications requiring language translation for global audiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These Azure Cognitive Services speech capabilities cater to a diverse range
    of applications, from accessibility features and voice-enabled applications to
    multilingual communication and personalized user experiences. Developers can leverage
    these services to enhance the functionality and accessibility of their applications
    through seamless integration of speech-related features.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Azure Speech service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s create a speech service using the Azure portal, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Azure portal at [https://portal.azure.com](https://portal.azure.com),
    search for `speech service`, and then create a new service.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the following screenshot, enter the project details such as your
    resource group speech service name and region details. Then, click on the **Review
    + create** button to create a speech service in an Azure environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Create Speech Services
  prefs: []
  type: TYPE_NORMAL
- en: Now, your Azure speech service is deployed, and you can go to that speech service
    resource by clicking on the **Go to resource** button on the deployment screen.
    Then, on the speech service resource screen, click on **Go to speech studio**.
    In **Speech Studio**, you can see various services for captioning with speech
    to text, post call transcription and analytics, and a live chat avatar, as shown
    in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Speech Studio
  prefs: []
  type: TYPE_NORMAL
- en: Speech to text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s try to use the speech to text service. As shown in the following
    screenshot, you can drag and drop an audio file or upload it, and record audio
    with microphone. You can see the corresponding **Text** or **JSON** tabs on the
    right-side window for the uploaded audio file, as shown in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – Real-time speech to text
  prefs: []
  type: TYPE_NORMAL
- en: Speech translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s see how to translate the speech. On the following screen, we are
    translating from English to French. Let’s choose a spoken language and a target
    language.
  prefs: []
  type: TYPE_NORMAL
- en: Then, speak in English and record the audio with a microphone. The translated
    text in French is shown on the right side of the window, as shown in the following
    screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Translated text test results
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the original text on the **Original text** tab, as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_11_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – Original text in test results
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to transcribe from speech to text and translate from English
    to French using Azure speech services. Aside from this, there are many other Azure
    speech services in Azure Speech Studio that you can apply, based on your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored three key sections that delve into the comprehensive
    process of handling audio data. The journey began with the upload of audio data,
    leveraging the Whisper model for transcription, and subsequently labeling the
    transcriptions using OpenAI. Following this, we ventured into the creation of
    spectrograms and employed CNNs to label these visual representations, unraveling
    the intricate details of sound through advanced neural network architectures.
    The chapter then delved into audio labeling with augmented data, thereby enhancing
    the dataset for improved model training. Finally, we saw the Azure Speech service
    for speech to text and speech translation. This multifaceted approach equips you
    with a holistic understanding of audio data processing, from transcription to
    visual representation analysis and augmented labeling, fostering a comprehensive
    skill set in audio data labeling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we will explore different hands-on tools for
    data labeling.
  prefs: []
  type: TYPE_NORMAL
