- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Labeling Audio Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标注音频数据
- en: In this chapter, we will embark on this transformative journey through the realms
    of real-time audio capture, cutting-edge transcription with the Whisper model,
    and audio classification using a **convolutional neural network** (**CNN**), with
    a focus on spectrograms. Additionally, we’ll explore innovative audio augmentation
    techniques. This chapter not only equips you with the tools and techniques essential
    for comprehensive audio data labeling but also unveils the boundless possibilities
    that lie at the intersection of AI and audio processing, redefining the landscape
    of audio data labeling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将踏上这场变革之旅，穿越实时音频捕获、使用Whisper模型进行尖端转录以及使用**卷积神经网络**（**CNN**）进行音频分类的领域，重点关注频谱图。此外，我们还将探索创新的音频增强技术。本章不仅为你提供了全面音频数据标注所必需的工具和技术，还揭示了AI与音频处理交叉处的无限可能性，重新定义了音频数据标注的格局。
- en: Welcome to a journey through the intricate world of audio data labeling! In
    this chapter, we embark on an exploration of cutting-edge techniques and technologies
    that empower us to unravel the richness of audio content. Our adventure unfolds
    through a diverse set of topics, each designed to enhance your understanding of
    audio processing and labeling.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到音频数据标注错综复杂世界的旅程！在本章中，我们将探索一系列前沿技术和方法，这些技术和方法赋予我们揭示音频内容丰富性的能力。我们的冒险通过一系列多样化的主题展开，每个主题都旨在增强你对音频处理和标注的理解。
- en: Our journey begins with the dynamic realm of real-time audio capture using microphones.
    We delve into the art of voice classification, using the random forest classifier
    to discern and categorize distinct voices in the captured audio.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程始于使用麦克风进行动态实时音频捕获的领域。我们深入探讨声音分类的艺术，使用随机森林分类器来辨别和分类捕获音频中的不同声音。
- en: Then, we introduce the groundbreaking Whisper model, a powerful tool for transcribing
    uploaded audio data. Witness the seamless integration of the Whisper model with
    OpenAI for accurate transcriptions, followed by a meticulous labeling process.
    As we unfold the capabilities of the Whisper model, we draw insightful comparisons
    with other open source models dedicated to audio data analysis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们介绍突破性的Whisper模型，这是一个强大的转录上传音频数据的工具。见证Whisper模型与OpenAI的无缝集成，实现准确的转录，随后进行细致的标注过程。随着我们展开Whisper模型的能力，我们将与其他致力于音频数据分析的开源模型进行深入的比较。
- en: Our journey takes a visual turn as we explore the creation of spectrograms,
    visually capturing the intricate details of sound. The transformative CNNs come
    into play, elevating audio classification through visual representations. Learn
    the art of labeling spectrograms, unraveling a new dimension in audio processing.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程转向视觉领域，探索频谱图的创建，直观地捕捉声音的复杂细节。变革性的CNN发挥作用，通过视觉表示提升音频分类。学习标注频谱图的艺术，揭开音频处理的新维度。
- en: Prepare to expand your horizons as we venture into the realm of augmented data
    for audio labeling. Discover the transformative impact of noise augmentation,
    time-stretching, and pitch-shifting on audio data. Uncover the techniques to enhance
    the robustness of your labeled audio datasets.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 准备拓展你的视野，随着我们进入音频标注增强数据的领域。发现噪声增强、时间拉伸和音高转换对音频数据的影响。揭示增强你标注音频数据集鲁棒性的技术。
- en: Our exploration culminates in the innovative domain of Azure Cognitive Services.
    Immerse yourself in the capabilities of Azure to transform speech to text and
    achieve speech translation. Witness the seamless integration of Azure Cognitive
    Services, revolutionizing the landscape of audio processing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的探索最终聚焦于创新的Azure认知服务领域。沉浸于Azure的能力，将语音转换为文本并实现语音翻译。见证Azure认知服务的无缝集成，彻底改变了音频处理领域的格局。
- en: 'We are going to cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Capturing real-time voice using a microphone and classifying voices using the
    random forest classifier
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用麦克风捕获实时声音并使用随机森林分类器对声音进行分类
- en: Uploading audio data and transcribing an audio file using OpenAI’s Whisper model
    and then labeling the transcription.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上传音频数据，使用OpenAI的Whisper模型转录音频文件，然后标注转录内容。
- en: A comparison of the Whisper model with other open source models for audio data
    analysis
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Whisper模型与其他开源音频数据分析模型进行比较
- en: Creating a spectrogram for audio data and then labeling the spectrogram, using
    CNN for audio classification
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为音频数据创建频谱图，然后使用CNN进行音频分类
- en: Augmenting audio data such as noise augmentation, time-stretching, and pitch-shifting
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强音频数据，如噪声增强、时间拉伸和音高转换
- en: Azure Cognitive Services for speech-to-text and speech translation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure 语音识别和语音翻译认知服务
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We are going to install the following Python libraries.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将安装以下 Python 库。
- en: '**openai-whisper** is the Python library provided by OpenAI, offering access
    to the powerful Whisper **Automatic Speech Recognition** (**ASR**) model. It allows
    you to transcribe audio data with state-of-the-art accuracy:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**openai-whisper** 是 OpenAI 提供的 Python 库，提供了对强大的 Whisper **自动语音识别**（**ASR**）模型的访问。它允许你以最先进的准确性转录音频数据：'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**librosa** is a Python package for music and audio analysis. It provides tools
    for various tasks, such as loading audio files, extracting features, and performing
    transformations, making it a valuable library for audio data processing:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**librosa** 是一个用于音乐和音频分析的 Python 包。它提供了加载音频文件、提取特征和执行转换等任务的工具，使其成为音频数据处理的有价值库：'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**pytube** is a lightweight, dependency-free Python library for downloading
    YouTube videos. It simplifies the process of fetching video content from YouTube,
    making it suitable for various applications involving YouTube data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**pytube** 是一个轻量级、无依赖的 Python 库，用于下载 YouTube 视频。它简化了从 YouTube 获取视频内容的过程，使其适用于涉及
    YouTube 数据的多种应用：'
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**transformers** is a popular Python library developed by Hugging Face. It
    provides pre-trained models and various utilities for **natural language processing**
    (**NLP**) tasks. This includes transformer-based models such as BERT and GPT:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**transformers** 是由 Hugging Face 开发的一个流行的 Python 库。它提供了预训练模型和各种用于 **自然语言处理**（**NLP**）任务的工具。这包括基于转换器的模型，如
    BERT 和 GPT：'
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**joblib** is a Python library for lightweight pipelining in Python. It is
    particularly useful for parallelizing and caching computations, making it efficient
    for tasks involving parallel processing and job scheduling:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**joblib** 是一个用于 Python 的轻量级管道库。它特别适用于并行化和缓存计算，对于涉及并行处理和作业调度的任务来说效率很高：'
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Downloading FFmpeg
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载 FFmpeg
- en: '**FFmpeg** is a versatile and open source multimedia framework that facilitates
    the handling, conversion, and manipulation of audio and video files ([https://ffmpeg.org/download.html](https://ffmpeg.org/download.html)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**FFmpeg** 是一个多才多艺的开源多媒体框架，它促进了音频和视频文件的处理、转换和操作 ([https://ffmpeg.org/download.html](https://ffmpeg.org/download.html))。'
- en: To download FFmpeg for macOS, select `static FFmpeg binaries for macOS 64-bit`
    from [https://evermeet.cx/ffmpeg/](https://evermeet.cx/ffmpeg/). Download `ffmpeg-6.1.1.7z`
    and extract and copy it to your `<home directory>/<new folder>/bin`. Change `ffmpeg`
    executable file.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载适用于 macOS 的 FFmpeg，请从 [https://evermeet.cx/ffmpeg/](https://evermeet.cx/ffmpeg/)
    选择 `static FFmpeg binaries for macOS 64-bit`。下载 `ffmpeg-6.1.1.7z` 并解压，然后将其复制到你的
    `<home directory>/<new folder>/bin`。更改 `ffmpeg` 可执行文件。
- en: 'To download FFmpeg for a Windows OS, select Windows builds by BtbN: [https://github.com/BtbN/FFmpeg-Builds/releases](https://github.com/BtbN/FFmpeg-Builds/releases).
    Download `ffmpeg-master-latest-win64-gpl.zip`. Extract and set the `path` environment
    variable of the extracted `ffmpeg` bin folder.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载适用于 Windows 操作系统的 FFmpeg，请选择 BtbN 的 Windows 构建：[https://github.com/BtbN/FFmpeg-Builds/releases](https://github.com/BtbN/FFmpeg-Builds/releases)。下载
    `ffmpeg-master-latest-win64-gpl.zip`。解压并设置解压的 `ffmpeg` bin 文件夹的 `path` 环境变量。
- en: 'The code for this chapter is available at GitHub here: [https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch11](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch11).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可在 GitHub 上找到：[https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch11](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch11)。
- en: Azure Machine Learning
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure 机器学习
- en: If you want to explore the Whisper model along with other machine learning models
    available in the Azure Machine Learning model catalog, you can create a free Azure
    account at [https://azure.microsoft.com/en-us/free](https://azure.microsoft.com/en-us/free).
    Then, you can try Azure Machine Learning for free at [https://azure.microsoft.com/en-us/products/machine-learning/](https://azure.microsoft.com/en-us/products/machine-learning/).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想探索 Azure 机器学习模型目录中可用的 Whisper 模型以及其他机器学习模型，你可以在 [https://azure.microsoft.com/en-us/free](https://azure.microsoft.com/en-us/free)
    创建一个免费的 Azure 账户。然后，你可以在 [https://azure.microsoft.com/en-us/products/machine-learning/](https://azure.microsoft.com/en-us/products/machine-learning/)
    免费尝试 Azure 机器学习。
- en: Real-time voice classification with Random Forest
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林进行实时语音分类
- en: In an era marked by the integration of advanced technologies into our daily
    lives, real-time voice classification systems have emerged as pivotal tools across
    various domains. The Python script in this section, showcasing the implementation
    of a real-time voice classification system using the Random Forest classifier
    from scikit-learn, is a testament to the versatility and significance of such
    applications.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个将先进技术融入我们日常生活的时代，实时语音分类系统在各种领域已成为关键工具。本节中的 Python 脚本展示了使用 scikit-learn 的随机森林分类器实现实时语音分类系统的过程，证明了此类应用的灵活性和重要性。
- en: The primary objective of this script is to harness the power of machine learning
    to differentiate between positive audio samples, indicative of human speech (voice),
    and negative samples, representing background noise or non-vocal elements. By
    employing the Random Forest classifier, a robust and widely used algorithm from
    the scikit-learn library, the script endeavors to create an efficient model capable
    of accurately classifying real-time audio input.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本的主要目标是利用机器学习的力量来区分正音频样本，这些样本表明人类语音（语音），以及负样本，代表背景噪音或非语音元素。通过使用来自 scikit-learn
    库的随机森林分类器，一个强大且广泛使用的算法，该脚本试图创建一个高效的模型，能够准确地对实时音频输入进行分类。
- en: The real-world applications of this voice classification system are extensive,
    ranging from enhancing user experiences in voice-controlled smart devices to enabling
    automated voice commands in robotics. Industries such as telecommunications, customer
    service, and security can leverage real-time voice classification to enhance communication
    systems, automate processes, and bolster security protocols.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此语音分类系统的实际应用范围广泛，从增强语音控制智能设备的用户体验，到使机器人自动化语音命令。电信、客户服务和安全等行业可以利用实时语音分类来增强通信系统，自动化流程，并加强安全协议。
- en: Whether it involves voice-activated virtual assistants, hands-free communication
    in automobiles, or voice-based authentication systems, the ability to classify
    and understand spoken language in real time is pivotal. This script provides a
    foundational understanding of the implementation process, laying the groundwork
    for developers and enthusiasts to integrate similar voice classification mechanisms
    into their projects and contribute to the evolution of voice-centric applications
    in the real world.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不论是语音激活的虚拟助手、汽车中的免提通信，还是基于语音的认证系统，实时对 spoken language 进行分类和理解的能力至关重要。此脚本提供了对实现过程的基础理解，为开发者和爱好者将类似的语音分类机制集成到他们的项目中，并为现实世界中以语音为中心的应用程序的演变做出贡献奠定了基础。
- en: Let’s see the Python script that demonstrates a real-time voice classification
    system, using the Random Forest classifier from scikit-learn. The goal is to capture
    audio samples, distinguish between positive samples (voice) and negative samples
    (background noise or non-voice), and train a model for voice classification.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个使用 scikit-learn 的随机森林分类器的实时语音分类系统的 Python 脚本。目标是捕获音频样本，区分正样本（语音）和负样本（背景噪音或非语音），并训练一个语音分类模型。
- en: 'Let’s us see the Python code that provides a simple framework to build a real-time
    voice classification system, allowing you to collect your own voice samples to
    train and test the model:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看提供构建实时语音分类系统简单框架的 Python 代码，允许您收集自己的语音样本进行模型训练和测试：
- en: '**Import the Python libraries**: First, let’s import the requisite libraries
    using the following code snippet:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**导入 Python 库**：首先，让我们使用以下代码片段导入所需的库：'
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`capture_audio` function uses the `sounddevice` library to record real-time
    audio. The user is prompted to speak, and the function captures audio for a specified
    duration (the default is five seconds):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`capture_audio` 函数使用 `sounddevice` 库记录实时音频。用户会被提示说话，该函数会捕获指定时长（默认为五秒）的音频：'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`collect_training_data` function gathers training data for voice and non-voice
    samples. For positive samples (voice), the user is prompted to speak, and audio
    data is recorded using the `capture_audio` function. For negative samples (background
    noise or non-voice), the user is prompted to create ambient noise without speaking:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`collect_training_data` 函数收集语音和非语音样本的训练数据。对于正样本（语音），用户会被提示说话，并使用 `capture_audio`
    函数记录音频数据。对于负样本（背景噪音或非语音），用户会被提示在不说话的情况下创建环境噪音：'
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`X`) and corresponding labels (`y`). The data is shuffled to ensure a balanced
    distribution during training:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`X`) 和相应的标签 (`y`)。数据被随机打乱以确保在训练过程中的平衡分布：'
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`train_test_split` function from scikit-learn:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_test_split` 函数来自 scikit-learn：'
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`VoiceClassifier` class is defined, encapsulating the random forest model.
    An instance of the `VoiceClassifier` is created, and the model is trained using
    the positive and negative training data:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了 `VoiceClassifier` 类，封装了随机森林模型。创建了一个 `VoiceClassifier` 实例，并使用正负训练数据对模型进行训练：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Make predictions**: The trained model predicts labels for the test set:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**进行预测**：训练好的模型对测试集进行标签预测：'
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`accuracy_score` function, comparing predicted labels with actual labels:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`accuracy_score` 函数，比较预测标签与实际标签：'
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When you run this code, it prompts with the following pop-up window to enter
    and speak.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此代码时，会弹出一个窗口提示您输入并说话。
- en: '![](img/B18944_11_01.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18944_11_01.jpg)'
- en: Figure 11.1 – Prompt to start speaking
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 开始说话的提示
- en: 'Then, you speak a few sentences that will be recorded:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您说几句话，这些话将被记录：
- en: '![](img/B18944_11_02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18944_11_02.jpg)'
- en: Figure 11.2 – Trained model accuracy
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 训练模型准确率
- en: '`RandomForestClassifier`, the model was previously trained to discern between
    positive samples (voice) and negative samples (non-voice or background noise).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier`，该模型之前被训练以区分正样本（语音）和负样本（非语音或背景噪音）。'
- en: 'The primary objective of this script is to demonstrate the seamless integration
    of the pre-trained voice classification model into a real-time voice inference
    system. You are prompted to provide audio input by pressing *Enter* and speaking
    for a few seconds, after which the model predicts whether the input contains human
    speech or non-voice elements:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本脚本的初级目标是展示预训练的语音分类模型与实时语音推理系统的无缝集成。您将被提示通过按下 *Enter* 键并说几秒钟话来提供音频输入，之后模型将预测输入是否包含人类语音或非语音元素：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/B18944_11_03.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18944_11_03.jpg)'
- en: Figure 11.3 – Inference output
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 推理输出
- en: In a similar manner, we can use this model to label a voice as male or female
    to analyze customer calls and understand target customers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们可以使用此模型来标注语音为男性或女性，以分析客户通话并了解目标客户。
- en: We have seen the real-time voice classification inference that holds significant
    relevance in numerous scenarios, including voice-activated applications, security
    systems, and communication devices. By loading a pre-trained model, users can
    experience the instantaneous and accurate classification of voice in real-world
    situations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了实时语音分类推理在众多场景中的重要性，包括语音激活的应用、安全系统和通信设备。通过加载预训练的模型，用户可以体验在现实世界情况下语音的即时和准确分类。
- en: Whether applied to enhance accessibility features, automate voice commands,
    or implement voice-based security protocols, this script serves as a practical
    example of deploying machine learning models for voice classification in real-time
    scenarios. As technology continues to advance, the seamless integration of voice
    inference models contributes to the evolution of user-friendly and responsive
    applications across various domains.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无论应用于增强无障碍功能、自动化语音命令或实施基于语音的安全协议，此脚本都是部署机器学习模型进行实时语音分类的实际示例。随着技术的不断进步，语音推理模型的无缝集成有助于推动各个领域用户友好和响应式应用的演变。
- en: Now, let’s see how to transcribe audio using the OpenAI Whisper model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用 OpenAI Whisper 模型进行音频转录。
- en: Transcribing audio using the OpenAI Whisper model
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 OpenAI Whisper 模型进行音频转录
- en: In this section, we are going to see how to transcribe audio file to text using
    the **OpenAI Whisper** model and then label the audio transcription using the
    OpenAI **large language** **model** (**LLM**).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何使用 **OpenAI Whisper** 模型将音频文件转录成文本，然后使用 OpenAI **大型语言模型**（**LLM**）对音频转录进行标注。
- en: '**Whisper** is an open source ASR model developed by OpenAI. It is trained
    on nearly 700,000 hours of multilingual speech data and is capable of transcribing
    audio to text in almost 100 different languages. According to OpenAI, Whisper
    “approaches human level robustness and accuracy on English speech recognition.”'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**Whisper** 是由 OpenAI 开发的开源 ASR 模型。它在近 70 万小时的跨语言语音数据上进行了训练，并且能够将音频转录成近 100
    种不同的语言。根据 OpenAI 的说法，Whisper “在英语语音识别方面接近人类水平的鲁棒性和准确性。”'
- en: In a recent benchmark study, Whisper was compared to other open source ASR models,
    such as wav2vec 2.0 and Kaldi. The study found that Whisper performed better than
    wav2vec 2.0 in terms of accuracy and speed across five different use cases, including
    conversational AI, phone calls, meetings, videos, and earnings calls.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的一项基准研究中，Whisper与其他开源ASR模型，如wav2vec 2.0和Kaldi进行了比较。研究发现，在包括对话AI、电话、会议、视频和收益电话在内的五个不同用例中，Whisper在准确性和速度方面均优于wav2vec
    2.0。
- en: Whisper is also known for its affordability, accuracy, and features. It is best
    suited for audio-to-text use cases and is not well-suited for text-to-audio or
    speech synthesis tasks.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper还以其经济性、准确性和功能而闻名。它最适合音频到文本用例，并且不适合文本到音频或语音合成任务。
- en: The Whisper model can be imported as a Python library. The other option is to
    use the Whisper model available in the model catalog at **Azure Machine** **Learning
    studio**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper模型可以作为Python库导入。另一种选择是使用在**Azure Machine Learning studio**模型目录中可用的Whisper模型。
- en: Let’s see the process of transcribing audio using the OpenAI Whisper ASR using
    the Python library now. It’s crucial to ensure the existence and accessibility
    of the specified audio file for successful transcription. The transcribed text
    is likely stored in `text['text']`, as indicated by the `print` statement.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看使用Python库通过OpenAI Whisper ASR进行音频转录的过程。确保指定音频文件的存在和可访问性对于成功转录至关重要。转录的文本可能存储在`text['text']`中，如`print`语句所示。
- en: First, we need to install the whisper model, as mentioned in the *technical
    requirements* section. Then, we import the OpenAI Whisper model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装Whisper模型，如*技术要求*部分所述。然后，我们导入OpenAI Whisper模型。
- en: Step 1 – importing the Whisper model
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步 – 导入Whisper模型
- en: 'Let us import the required Python libraries:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入所需的Python库：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `whisper` library is imported, which is the library providing access to
    the OpenAI Whisper ASR model. The `pytube` library is imported to download YouTube
    videos.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 导入了`whisper`库，这是一个提供访问OpenAI Whisper ASR模型的库。同时导入了`pytube`库以下载YouTube视频。
- en: Step 2 – loading the base Whisper model
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 加载基础Whisper模型
- en: 'Let us load the base Whisper model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载基础Whisper模型：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The Whisper model is loaded using the `whisper.load_model` function with the
    `"base"` argument. This loads the base version of the Whisper ASR model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`whisper.load_model`函数和`"base"`参数加载Whisper模型。这加载了Whisper ASR模型的基础版本。
- en: 'Let us download the audio stream from a YouTube video. Even though we are using
    a video file, we are only focusing on the audio of the YouTube video and downloading
    an audio stream from it. Alternatively, you can directly use any audio file:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从YouTube视频下载音频流。尽管我们使用的是视频文件，但我们只关注YouTube视频的音频，并从中下载音频流。或者，您可以直接使用任何音频文件：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The YouTube video URL is specified. Using the `pytube.YouTube` class, the video
    data is fetched:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 指定YouTube视频URL。使用`pytube.YouTube`类，获取视频数据：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This code utilizes the `pytube` library to download the audio stream from a
    video hosted on a platform such as YouTube. Let’s examine the preceding code snippet:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码利用`pytube`库从托管在YouTube等平台上的视频下载音频流。让我们来检查前面的代码片段：
- en: '`audio = data.streams.get_audio_only()`: This line fetches the audio stream
    of the video. It uses the `get_audio_only()` method to obtain a stream containing
    only the audio content.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio = data.streams.get_audio_only()`: 此行获取视频的音频流。它使用`get_audio_only()`方法获取只包含音频内容的流。'
- en: '`audio.download()`: Once the audio stream is obtained, this line downloads
    the audio content. The download is performed in the default format, which is typically
    an MP4 file containing only the audio data.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio.download()`: 一旦获取到音频流，此行将下载音频内容。下载以默认格式进行，通常是只包含音频数据的MP4文件。'
- en: In summary, the code extracts the audio stream from a video and downloads it
    as an MP4 file, preserving only the audio content.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，代码从视频中提取音频流，并将其下载为MP4文件，仅保留音频内容。
- en: Step 3 – setting up FFmpeg
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 设置FFmpeg
- en: Whisper is designed to transcribe audio, but it requires a specific format for
    processing. The format required by Whisper for processing audio is WAV format.
    Whisper is designed to transcribe audio in WAV format, and it may not directly
    support other formats. Therefore, audio data that needs to be processed by Whisper
    should be provided in the WAV format. FFmpeg acts as a bridge by converting various
    audio formats (such as MP3, WAV, or AAC) into a format that Whisper can handle.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper旨在转录音频，但需要特定的格式进行处理。Whisper处理音频所需的格式是WAV格式。Whisper旨在以WAV格式转录音频，可能不支持其他格式。因此，需要由Whisper处理的音频数据应提供为WAV格式。FFmpeg作为桥梁，将各种音频格式（如MP3、WAV或AAC）转换为Whisper可以处理的格式。
- en: For example, if the input is in the MP3 format, FFmpeg can convert it to a format
    suitable for Whisper. Whisper typically requires audio data in WAV format, so
    FFmpeg can convert the input MP3 file to WAV during the process. This conversion
    allows the audio data to be in a format compatible with the requirements of the
    Whisper model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果输入是MP3格式，FFmpeg可以将其转换为Whisper适用的格式。Whisper通常需要WAV格式的音频数据，因此FFmpeg可以在处理过程中将输入MP3文件转换为WAV格式。这种转换使得音频数据能够与Whisper模型的要求兼容。
- en: Without this conversion, Whisper wouldn’t be able to process the audio effectively.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 没有这种转换，Whisper将无法有效地处理音频。
- en: 'In scenarios where real-time transcription is needed (such as streaming a **real-time
    messaging protocol** (**RTMP**) feed), FFmpeg helps segment the audio stream.
    It splits the continuous audio into smaller chunks (e.g., 30-second segments)
    that can be processed individually. Each segment is then passed to Whisper for
    transcription:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要实时转录的场景中（例如流式传输**实时消息协议**（RTMP）数据），FFmpeg有助于分割音频流。它将连续的音频分割成更小的块（例如，30秒的段），这些块可以单独处理。然后，每个段被传递给Whisper进行转录：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The code sets the FFmpeg environment variable to the path of the `ffmpeg` executable.
    This is necessary for handling audio and video files.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将FFmpeg环境变量设置为`ffmpeg`可执行文件的路径。这对于处理音频和视频文件是必要的。
- en: Step 4 – transcribing the YouTube audio using the Whisper model
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 使用Whisper模型转录YouTube音频
- en: 'Now, let’s transcribe the YouTube audio using the Whisper model:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Whisper模型转录YouTube音频：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here''s the output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '![](img/B18944_11_04.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18944_11_04.jpg)'
- en: Figure 11.4 – A snippet of the code output
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 – 代码输出的片段
- en: The Whisper model is loaded again to ensure that it uses the base model. The
    transcribe function is called on the model with the filename of the audio file
    as an argument. The resulting transcribed text is printed using `text['text']`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 再次加载Whisper模型以确保它使用基础模型。使用音频文件的文件名作为参数调用模型的transcribe函数。使用`text['text']`打印出转录的文本。
- en: Note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The provided filename in `model.transcribe` is `Mel Spectrograms with Python
    and Librosa Audio Feature Extraction.mp4`. Make sure this file exists and is accessible
    for the code to transcribe successfully.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在`model.transcribe`中提供的文件名为`Mel Spectrograms with Python and Librosa Audio Feature
    Extraction.mp4`。请确保此文件存在且可访问，以便代码能够成功转录。
- en: 'Now, let’s see another code example on how to transcribe an audio file to text:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看另一个代码示例，演示如何将音频文件转录成文本：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here is the output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, let’s perform sentiment analysis to label this text transcribed from a
    customer call.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对从客户通话转录的文本执行情感分析以进行标记。
- en: Classifying a transcription using Hugging Face transformers
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Hugging Face transformers对转录进行分类
- en: Now, let’s use Hugging Face transformers to classify the output text from the
    previous customer call audio transcription and perform sentiment analysis to label
    it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Hugging Face transformers对之前客户通话音频转录的输出文本进行分类，并执行情感分析以对其进行标记。
- en: The following code snippet utilizes Hugging Face’s transformers library to perform
    sentiment analysis on a given text. It begins by importing the necessary module,
    and then it loads a pre-trained sentiment analysis pipeline from Hugging Face’s
    transformers. The code defines an example text that expresses dissatisfaction
    with a product not yet received. Subsequently, the sentiment analysis pipeline
    is applied to classify the sentiment of the text, and the result is displayed
    by printing it to the console. The sentiment analysis model outputs a label indicating
    the sentiment, such as positive or negative, along with a confidence score.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段使用Hugging Face的transformers库对给定文本进行情感分析。它首先导入必要的模块，然后从Hugging Face的transformers中加载一个预训练的情感分析管道。代码定义了一个表示对尚未收到的产品不满的示例文本。随后，将情感分析管道应用于对文本进行情感分类，并将结果通过打印到控制台显示。情感分析模型输出一个标签，表示情感，如正面或负面，以及一个置信度分数。
- en: 'Let’s import the Python library:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入Python库：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is the output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Hands-on – labeling audio data using a CNN
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践操作 - 使用CNN标注音频数据
- en: In this section, we will see how to train the CNN network on audio data and
    use it to label the audio data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解如何使用CNN在音频数据上训练网络，并使用它来标注音频数据。
- en: 'The following code demonstrates the process of labeling audio data using a
    CNN. The code outlines how to employ a CNN to label audio data, specifically training
    the model on a dataset of cat and dog audio samples. The goal is to classify new,
    unseen audio data as either a cat or a dog. Let’s take the cat and dog sample
    audio data and train the CNN model. Then, we will send new unseen data to the
    model to predict whether it is a cat or a dog:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了使用CNN标注音频数据的过程。代码概述了如何使用CNN标注音频数据，特别是如何在猫和狗音频样本数据集上训练模型。目标是分类新的、未见过的音频数据为猫或狗。让我们使用猫和狗的样本音频数据来训练CNN模型。然后，我们将新的未见数据发送到模型以预测它是一只猫还是一只狗：
- en: '`load_and_preprocess_data` function. The `load_and_preprocess_data` function
    processes the audio data, converting it into mel spectrograms and resizing them
    for model compatibility.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`load_and_preprocess_data`函数。`load_and_preprocess_data`函数处理音频数据，将其转换为梅尔频谱图，并调整大小以适应模型。'
- en: '`train_test_split`, and labels are converted to one-hot encoding.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train_test_split`，并将标签转换为独热编码。'
- en: '**Create a neural network model**: A CNN model is created using TensorFlow
    and Keras, comprising convolutional layers, pooling layers, and fully connected
    layers.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建神经网络模型**：使用TensorFlow和Keras创建一个CNN模型，包括卷积层、池化层和全连接层。'
- en: '**Compile the model**: The model is compiled with an Adam optimizer, categorical
    cross-entropy loss, and accuracy as the evaluation metric.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编译模型**：模型使用Adam优化器、分类交叉熵损失和准确率作为评估指标进行编译。'
- en: '**Train the model**: The CNN model is trained on the training data for a specified
    number of epochs.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练模型**：CNN模型在指定数量的epoch上对训练数据进行训练。'
- en: '**Evaluate the accuracy of the model**: The accuracy of the trained model is
    evaluated on the testing set.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估模型的准确率**：在测试集上评估训练模型的准确率。'
- en: '**Save the trained model**: The trained model is saved for future use.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**保存训练好的模型**：将训练好的模型保存以供将来使用。'
- en: '**Test the new audio file**: Finally, the saved model is loaded, and a new
    audio file (in this case, a cat meow) is processed and classified, with class
    probabilities and accuracy displayed for each class.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试新的音频文件**：最后，加载保存的模型，并处理一个新音频文件（在这种情况下，是一只猫的叫声），对其进行分类，并显示每个类别的概率和准确率。'
- en: In summary, this code provides a comprehensive guide on using a CNN to label
    audio data, from data loading and preprocessing to model training, evaluation,
    and prediction on new audio samples.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，此代码提供了一个关于如何使用卷积神经网络（CNN）对音频数据进行标注的全面指南，从数据加载和预处理到模型训练、评估以及在新的音频样本上的预测。
- en: 'Let’s import all the required Python modules as the first step:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入所有必需的Python模块：
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Step 1: Load and preprocess data**: Now, let’s load and pre-process the data
    for cats and dogs. The source of this dataset is [https://www.kaggle.com/datasets/mmoreaux/audio-cats-and-dogs](https://www.kaggle.com/datasets/mmoreaux/audio-cats-and-dogs):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：加载和预处理数据**：现在，让我们加载并预处理猫和狗的数据。该数据集的来源是[https://www.kaggle.com/datasets/mmoreaux/audio-cats-and-dogs](https://www.kaggle.com/datasets/mmoreaux/audio-cats-and-dogs)：'
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This code establishes the folder structure for a dataset containing the `''cat''`
    and `''dog''`, categories, with the data located at the specified directory, `''../cats_dogs/data/''`.
    Next, let’s pre-process the data:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码为包含 `'cat'` 和 `'dog'` 类别的数据集建立文件夹结构，数据位于指定的目录 `'../cats_dogs/data/'` 中。接下来，让我们预处理数据：
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This code defines a function named `load_and_preprocess_data` that loads and
    preprocesses audio data from a specified directory. It iterates through each class
    of audio, reads `.wav` files, and uses the Librosa library to convert the audio
    data into a mel spectrogram. We learned about the mel spectrogram in [*Chapter
    10*](B18944_10.xhtml#_idTextAnchor221) in the *Visualizing audio data with Matplotlib
    and Librosa – spectrogram* *visualization* section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个名为 `load_and_preprocess_data` 的函数，用于从指定的目录加载和预处理音频数据。它遍历每个音频类别，读取 `.wav`
    文件，并使用 Librosa 库将音频数据转换为梅尔频谱图。我们在 *第10章* 中学习了梅尔频谱图，在 *使用 Matplotlib 和 Librosa
    可视化音频数据 – 频谱图* 可视化部分。
- en: The mel spectrogram is then resized to a target shape (128x128) before being
    appended to the data list, along with the corresponding class labels. The function
    returns the preprocessed data and labels as NumPy arrays.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将梅尔频谱图调整到目标形状（128x128），然后将其附加到数据列表中，以及相应的类别标签。该函数返回预处理后的数据和标签作为 NumPy 数组。
- en: '`load_and_preprocess_data` function to load and preprocess the data. The labels
    are then converted into one-hot encoding using the `to_categorical` function.
    Finally, the data is split into training and testing sets with an 80–20 ratio
    using the `train_test_split` function, ensuring reproducibility with a specified
    random seed:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_and_preprocess_data` 函数来加载数据并预处理。然后使用 `to_categorical` 函数将标签转换为独热编码。最后，使用
    `train_test_split` 函数将数据按 80-20 的比例分为训练集和测试集，确保使用指定的随机种子具有可重复性：'
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Step 3: Create a neural network model**: This code defines a neural network
    model for audio classification. The model architecture includes convolutional
    layers with max pooling for feature extraction, followed by a flattening layer.
    Subsequently, there is a dense layer with ReLU activation for further feature
    processing. The final output layer utilizes softmax activation to produce class
    probabilities. The model is constructed using the Keras functional API, specifying
    the input and output layers, and is ready to be trained on the provided data:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：创建神经网络模型**：此代码定义了一个用于音频分类的神经网络模型。模型架构包括用于特征提取的卷积层和最大池化层，随后是一个展平层。然后是一个具有
    ReLU 激活的密集层，用于进一步的特征处理。最终输出层使用 softmax 激活来产生类别概率。该模型使用 Keras 功能 API 构建，指定了输入和输出层，并准备好在提供的数据上进行训练：'
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`0.001`, categorical cross-entropy as the loss function (suitable for multi-class
    classification), and accuracy as the evaluation metric. The `model.summary()`
    command provides a concise overview of the model’s architecture, including the
    number of parameters and the structure of each layer:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`0.001`，交叉熵损失函数（适用于多类分类），准确度作为评估指标。`model.summary()` 命令提供了模型架构的简要概述，包括参数数量和每层的结构：'
- en: '[PRE29]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here is the output:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '![](img/B18944_11_05.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18944_11_05.jpg)'
- en: Figure 11.5 – Model summary
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 – 模型摘要
- en: '`X_train` and `y_train`) for `20` epochs, with a batch size of `32`. The validation
    data (`X_test` and `y_test`) is used to evaluate the model’s performance during
    training:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train` 和 `y_train`) 进行 `20` 个周期训练，批大小为 `32`。验证数据 (`X_test` 和 `y_test`) 用于在训练期间评估模型性能：'
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Step 6: Test the accuracy of the model**: After training completion, it calculates
    the test accuracy of the model on the separate test set and prints the accuracy
    score, providing insights into the model’s effectiveness in classifying audio
    data:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：测试模型的准确度**：在训练完成后，它计算模型在单独的测试集上的测试准确度，并打印准确度分数，为模型在分类音频数据方面的有效性提供见解：'
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here is the output:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '![](img/B18944_11_06.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18944_11_06.jpg)'
- en: Figure 11.6 – Accuracy of the model
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – 模型的准确度
- en: '**Step 7: Save the** **trained model**:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：保存** **训练好的模型**：'
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '**Step 8: Test the new audio file**: Let’s classify the new audio file and
    label it using this saved model:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：测试新的音频文件**：让我们使用这个保存的模型对新的音频文件进行分类并标记：'
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This code defines a function, `test_audio`, to preprocess and classify an audio
    file. It loads and preprocesses the audio data from the specified file path using
    Librosa, generating a mel spectrogram. The spectrogram is then resized and reshaped
    to match the input dimensions expected by the model. This function is designed
    to prepare audio data for classification using a neural network model, providing
    a streamlined way to apply the trained model to new audio files for prediction.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码定义了一个函数，`test_audio`，用于预处理和分类音频文件。它使用Librosa从指定的文件路径加载并预处理音频数据，生成梅尔频谱图。然后，频谱图被调整大小和形状以匹配模型期望的输入维度。此函数旨在为使用神经网络模型进行分类准备音频数据，提供了一种将训练好的模型应用于新音频文件进行预测的简化方法。
- en: 'Now, let’s make `./cat-meow-14536.mp3`). The `test_audio` function is employed
    to preprocess the audio file and obtain class probabilities and the predicted
    class index. The `model.predict` method generates predictions, and the class probabilities
    are extracted from the result. The predicted class index is determined by identifying
    the class with the highest probability. This process demonstrates how the trained
    model can be utilized to classify new audio data, providing insights into the
    content of the tested audio file:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过添加噪声来操作`./cat-meow-14536.mp3`)。使用`test_audio`函数预处理音频文件，并获取类别概率和预测的类别索引。`model.predict`方法生成预测，类别概率从结果中提取。预测的类别索引是通过识别概率最高的类别来确定的。这个过程展示了如何利用训练好的模型来分类新的音频数据，为测试音频文件的内容提供了见解：
- en: '[PRE34]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following code snippet iterates through all the classes in the model and
    prints the predicted probabilities for each class, based on the audio file classification.
    For each class, it displays the class label and its corresponding probability,
    providing a comprehensive view of the model’s confidence in assigning the audio
    file to each specific category:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段遍历模型中的所有类别，并根据音频文件分类打印每个类别的预测概率。对于每个类别，它显示类别标签及其对应的概率，提供了模型对将音频文件分配给每个特定类别的置信度的全面视图：
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following code calculates and reveals the predicted class and accuracy
    of the audio file classification. It identifies the predicted class using the
    index with the highest probability, retrieves the corresponding class label and
    accuracy from the results, and then prints the predicted class along with its
    associated accuracy. This provides a concise summary of the model’s prediction
    for the given audio file and the confidence level associated with the classification.
    Calculate and display the predicted class and accuracy:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码计算并显示音频文件分类的预测类别和准确率。它使用具有最高概率的索引识别预测类别，从结果中检索相应的类别标签和准确率，然后打印预测的类别及其相关的准确率。这为给定音频文件和分类的置信水平提供了一个简洁的总结。计算并显示预测类别和准确率：
- en: '[PRE36]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This is the output we get:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的输出：
- en: '![](img/B18944_11_07.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18944_11_07.jpg)'
- en: Figure 11.7 – Output showing probabilities and accuracy of the model
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 – 显示模型概率和准确率的输出
- en: We have seen how to transcribe audio data using machine learning. Now, let’s
    see how to do audio data augmentation and train a model with augmented data. Finally,
    we will see and compare the accuracy with and without augmented data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何使用机器学习转录音频数据。现在，让我们看看如何进行音频数据增强并使用增强数据训练模型。最后，我们将比较带有和不带有增强数据的准确率。
- en: Exploring audio data augmentation
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索音频数据增强
- en: Let’s see how to manipulate audio data by adding noise, using NumPy.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用NumPy添加噪声来操作音频数据。
- en: Adding noise to audio data during training helps the model become more robust
    in real-world scenarios, where there might be background noise or interference.
    By exposing a model to a variety of noisy conditions, it learns to generalize
    better.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中向音频数据添加噪声有助于模型在现实场景中变得更加鲁棒，在这些场景中可能会有背景噪声或干扰。通过让模型接触各种噪声条件，它学会了更好地泛化。
- en: 'Augmenting audio data with noise prevents a model from memorizing specific
    patterns in the training data. This encourages the model to focus on more general
    features, which can lead to better generalization on unseen data:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在音频数据中添加噪声可以防止模型记住训练数据中的特定模式。这促使模型关注更通用的特征，这可能导致在未见数据上的更好泛化：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This code defines a function named `add_noise` that adds random noise to an
    input data array. The level of noise is controlled by the `noise_factor` parameter.
    The function generates random noise using NumPy, adds it to the original data,
    and then returns the augmented data. To ensure the data type consistency, the
    augmented data is cast back to the same type as the elements in the original data
    array. This function can be used for data augmentation, a technique commonly employed
    in machine learning to enhance model robustness by introducing variations in the
    training data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码定义了一个名为`add_noise`的函数，该函数向输入数据数组添加随机噪声。噪声水平由`noise_factor`参数控制。该函数使用NumPy生成随机噪声，将其添加到原始数据中，然后返回增强后的数据。为了确保数据类型的一致性，增强后的数据被转换回与原始数据数组中的元素相同的数据类型。此函数可用于数据增强，这是一种在机器学习中常用的技术，通过在训练数据中引入变化来增强模型的鲁棒性。
- en: 'Let’s test this function using sample audio data, as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用样本音频数据来测试这个函数，如下所示：
- en: '[PRE38]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here is the output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '![](img/B18944_11_08.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18944_11_08.jpg)'
- en: Figure 11.8 – Representation of the augmented data
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8 – 增强数据的表示
- en: 'Now, let’s retrain our CNN model using data augmentation for the classification
    of dogs and cats sounds that we saw in the *Hands-on – labeling audio data using
    a* *CNN* section:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用数据增强重新训练我们的CNN模型，用于分类我们之前在*动手实践 – 使用CNN标记音频数据*部分看到的狗和猫的声音：
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In this code, we introduced data augmentation by adding random noise `(noise_factor
    * noise)` to the audio data before spectrogram conversion. This helps improve
    the model’s robustness by exposing it to varied instances of the same class during
    training:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们在频谱转换之前向音频数据添加随机噪声`(noise_factor * noise)`，这有助于通过在训练过程中暴露模型于同一类别的不同实例来提高模型的鲁棒性：
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Here is the output:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '![](img/B18944_11_09.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18944_11_09.jpg)'
- en: Figure 11.9 – Accuracy of the model
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9 – 模型的准确率
- en: By using this noise-augmented audio data, the model accuracy increased from
    0.946 to 0.964\. Depending on the data, we can apply data augmentation and test
    the accuracy to decide whether data augmentation is required.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这种噪声增强的音频数据，模型准确率从0.946提升到0.964。根据数据的不同，我们可以应用数据增强并测试准确率，以决定是否需要数据增强。
- en: Let’s see three more data augmentation techniques applied to an original audio
    file – time-stretching, pitch-shifting, and dynamic range compression.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看三种应用于原始音频文件的数据增强技术——时间拉伸、音高转换和动态范围压缩。
- en: The following Python script employs the librosa library for audio processing,
    loading an initial audio file that serves as the baseline for augmentation. Subsequently,
    functions are defined to apply each augmentation technique independently. Time-stretching
    alters the temporal duration of the audio, pitch-shifting modifies the pitch without
    affecting speed, and dynamic range compression adjusts the volume dynamics.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python脚本使用librosa库进行音频处理，加载一个作为增强基准的初始音频文件。随后，定义了独立应用每种增强技术的函数。时间拉伸改变音频的时间长度，音高转换修改音高而不影响速度，动态范围压缩调整音量动态。
- en: The augmented waveforms are visually presented side by side with the original
    waveform using Matplotlib. This visualization aids in understanding the transformative
    impact of each augmentation technique on the audio data. Through this script,
    you will gain insights into the practical implementation of audio augmentation,
    a valuable practice for creating diverse and robust datasets for machine learning
    models.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Matplotlib将增强的波形与原始波形并排展示。这种可视化有助于理解每种增强技术对音频数据的影响。通过这个脚本，您将了解音频增强的实际实现，这对于创建多样化和鲁棒的机器学习模型数据集是一项宝贵的实践。
- en: 'As audio data labeling becomes increasingly integral to various applications,
    mastering the art of augmentation ensures the generation of comprehensive datasets,
    thereby enhancing the effectiveness of machine learning models. Whether applied
    to speech recognition, sound classification, or voice-enabled applications, audio
    augmentation is a powerful technique for refining and enriching audio datasets:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 随着音频数据标记在各个应用中变得越来越重要，掌握增强的艺术可以确保生成全面的数据集，从而提高机器学习模型的有效性。无论应用于语音识别、声音分类还是语音启用应用，音频增强都是一种强大的技术，用于精炼和丰富音频数据集：
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here is the output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '![](img/B18944_11_10.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18944_11_10.jpg)'
- en: Figure 11.10 – Data augmentation techniques – time stretching, pitch shifting,
    and dynamic range compression
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – 数据增强技术 – 时间拉伸、音调转换和动态范围压缩
- en: Now, let’s move on to another interesting topic in labeling audio data in the
    next section.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入下一节，探讨音频数据标注的另一个有趣话题。
- en: Introducing Azure Cognitive Services – the speech service
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Azure 认知服务 – 语音服务
- en: 'Azure Cognitive Services offers a comprehensive set of speech-related services
    that empower developers to integrate powerful speech capabilities into their applications.
    Some key speech services available in Azure AI include the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 认知服务提供了一套全面的语音相关服务，使开发者能够将强大的语音功能集成到他们的应用程序中。Azure AI 中可用的关键语音服务包括以下内容：
- en: '**Speech-to-text (speech recognition)**: This converts spoken language into
    written text, enabling applications to transcribe audio content such as voice
    commands, interviews, or conversations.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音到文本（语音识别）**：这项服务将口语语言转换为书面文本，使应用程序能够转录音频内容，如语音命令、访谈或对话。'
- en: '**Speech Translation**: This translates spoken language into another language
    in real time, facilitating multilingual communication. This service is valuable
    for applications requiring language translation for global audiences.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音翻译**：这种服务可以将口语语言实时翻译成另一种语言，促进多语言沟通。对于需要为全球受众进行语言翻译的应用程序，这项服务非常有价值。'
- en: These Azure Cognitive Services speech capabilities cater to a diverse range
    of applications, from accessibility features and voice-enabled applications to
    multilingual communication and personalized user experiences. Developers can leverage
    these services to enhance the functionality and accessibility of their applications
    through seamless integration of speech-related features.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 Azure 认知服务语音能力适用于各种应用，从辅助功能、语音启用应用程序到多语言沟通和个性化用户体验。开发者可以利用这些服务通过无缝集成语音相关功能来增强其应用程序的功能性和可访问性。
- en: Creating an Azure Speech service
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Azure 语音服务
- en: Let’s create a speech service using the Azure portal, as follows.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Azure 门户创建一个语音服务，如下所示。
- en: Go to the Azure portal at [https://portal.azure.com](https://portal.azure.com),
    search for `speech service`, and then create a new service.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 Azure 门户 [https://portal.azure.com](https://portal.azure.com)，搜索 `speech
    service`，然后创建一个新的服务。
- en: As shown in the following screenshot, enter the project details such as your
    resource group speech service name and region details. Then, click on the **Review
    + create** button to create a speech service in an Azure environment.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，输入项目详情，例如您的资源组语音服务名称和区域详情。然后，点击**审查 + 创建**按钮在 Azure 环境中创建语音服务。
- en: '![](img/B18944_11_11.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18944_11_11.jpg)'
- en: Figure 11.11 – Create Speech Services
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11 – 创建语音服务
- en: Now, your Azure speech service is deployed, and you can go to that speech service
    resource by clicking on the **Go to resource** button on the deployment screen.
    Then, on the speech service resource screen, click on **Go to speech studio**.
    In **Speech Studio**, you can see various services for captioning with speech
    to text, post call transcription and analytics, and a live chat avatar, as shown
    in the following screenshot.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您的 Azure 语音服务已部署，您可以通过点击部署屏幕上的**转到资源**按钮访问该语音服务资源。然后，在语音服务资源屏幕上，点击**转到语音工作室**。在**语音工作室**中，您可以看到各种语音到文本字幕、通话后转录和分析以及实时聊天头像等服务，如图下所示。
- en: '![](img/B18944_11_12.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18944_11_12.jpg)'
- en: Figure 11.12 – Speech Studio
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 – 语音工作室
- en: Speech to text
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音转文本
- en: Now, let’s try to use the speech to text service. As shown in the following
    screenshot, you can drag and drop an audio file or upload it, and record audio
    with microphone. You can see the corresponding **Text** or **JSON** tabs on the
    right-side window for the uploaded audio file, as shown in the following screenshot.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用语音转文本服务。如图下所示，您可以将音频文件拖放到窗口中或上传它，并使用麦克风录制音频。您可以在右侧窗口中看到上传音频文件的相应**文本**或**JSON**标签，如图下所示。
- en: '![](img/B18944_11_13.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18944_11_13.jpg)'
- en: Figure 11.13 – Real-time speech to text
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13 – 实时语音转文本
- en: Speech translation
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音翻译
- en: Now, let’s see how to translate the speech. On the following screen, we are
    translating from English to French. Let’s choose a spoken language and a target
    language.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何进行语音翻译。在接下来的屏幕上，我们正在将英语翻译成法语。让我们选择一种口语语言和目标语言。
- en: Then, speak in English and record the audio with a microphone. The translated
    text in French is shown on the right side of the window, as shown in the following
    screenshot.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，用英语说话并使用麦克风录制音频。右侧窗口中显示了翻译成法语的文本，如下面的截图所示。
- en: '![](img/B18944_11_14.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18944_11_14.jpg)'
- en: Figure 11.14 – Translated text test results
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 – 翻译文本测试结果
- en: 'We can also see the original text on the **Original text** tab, as shown in
    the following screenshot:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在“原文”标签页上看到原始文本，如下面的截图所示：
- en: '![](img/B18944_11_15.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18944_11_15.jpg)'
- en: Figure 11.15 – Original text in test results
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15 – 测试结果中的原文
- en: We have seen how to transcribe from speech to text and translate from English
    to French using Azure speech services. Aside from this, there are many other Azure
    speech services in Azure Speech Studio that you can apply, based on your requirements.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何使用Azure语音服务从语音转录到文本，以及从英语翻译到法语。除此之外，在Azure语音工作室中还有许多其他Azure语音服务，您可以根据需求应用。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored three key sections that delve into the comprehensive
    process of handling audio data. The journey began with the upload of audio data,
    leveraging the Whisper model for transcription, and subsequently labeling the
    transcriptions using OpenAI. Following this, we ventured into the creation of
    spectrograms and employed CNNs to label these visual representations, unraveling
    the intricate details of sound through advanced neural network architectures.
    The chapter then delved into audio labeling with augmented data, thereby enhancing
    the dataset for improved model training. Finally, we saw the Azure Speech service
    for speech to text and speech translation. This multifaceted approach equips you
    with a holistic understanding of audio data processing, from transcription to
    visual representation analysis and augmented labeling, fostering a comprehensive
    skill set in audio data labeling techniques.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了处理音频数据的三个关键部分，深入了解了这一综合过程。旅程始于音频数据的上传，利用Whisper模型进行转录，随后使用OpenAI对转录进行标注。在此之后，我们进入了创建频谱图并使用卷积神经网络（CNN）对这些视觉表示进行标注的阶段，通过高级神经网络架构揭示声音的复杂细节。然后，章节深入探讨了使用增强数据的音频标注，从而增强了数据集以改善模型训练。最后，我们看到了Azure语音服务在语音到文本和语音翻译方面的应用。这种多方面的方法使您能够全面了解音频数据处理，从转录到视觉表示分析以及增强标注，培养在音频数据标注技术方面的全面技能集。
- en: In the next and final chapter, we will explore different hands-on tools for
    data labeling.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章和最后一章中，我们将探讨用于数据标注的不同实用工具。
