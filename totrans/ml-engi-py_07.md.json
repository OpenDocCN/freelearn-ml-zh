["```py\npip3 install torch \n```", "```py\n    inputs = torch.tensor(X_train, dtype=torch.float32)\n    labels = torch.tensor(y_train, dtype=torch.long) \n    ```", "```py\n    import torch\n    import torch.nn as nn\n\n    class NeuralNetwork(nn.Module):\n        def __init__(self):\n            super(NeuralNetwork, self).__init__()\n            self.sequential = nn.Sequential(\n                nn.Linear(13, 64),\n                nn.ReLU(),\n                nn.Linear(64, 32),\n                nn.ReLU(),\n                nn.Linear(32, 16),\n                nn.ReLU(),\n                nn.Linear(16, 3)\n            )\n        def forward(self, x):\n            x = self.sequential(x)\n            return x \n    ```", "```py\n    criterion = nn.CrossEntropyLoss() \n    ```", "```py\n    import torch.optim as optim\n    model = NeuralNetwork()\n    optimizer = torch.optim.Adam(\n            model.parameters(),\n            lr=0.001\n    ) \n    ```", "```py\n    for epoch in range(500):\n        running_loss = 0.0\n        optimizer.zero_grad()\n\n        inputs = torch.tensor(X_train, dtype=torch.float32)\n        labels = torch.tensor(y_train, dtype=torch.long)\n\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step() \n    ```", "```py\n    model_path = \"path/to/model/my_model.pt\"\n    torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': loss,\n                }, model_path) \n    ```", "```py\n    model = NeuralNetwork()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    checkpoint = torch.load(model_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss'] \n    ```", "```py\n    with torch.inference_mode(): \n    ```", "```py\n    inputs = torch.tensor(X_test, dtype=torch.float32)\n    labels = torch.tensor(y_test, dtype=torch.long)\n    outputs = net(inputs)\n    _, predicted = torch.max(outputs.data, 1)\n    correct = (predicted == labels).sum().item()\n    total = labels.size(0)\n    accuracy = correct / total\n    print('Accuracy on the test set: %.2f %%' % (100 * accuracy)) \n    ```", "```py\nfrom transformers import pipeline\nsummarizer = pipeline(\"summarization\", model= \"google/pegasus-xsum\") \n```", "```py\ntext = \"Customer: Hi, I am looking for some help regarding my recent purchase of a bouquet of flowers. ChatBot: Sure, how can I help you today? Customer: I purchased a bouquet the other day, but it has not arrived. ChatBot: What is the order ID? Customer: 0123456\\. ChatBot: Please wait while I fetch the details of your order... It doesn't seem like there was an order placed as you described; are you sure of the details you have provided?\" \n```", "```py\nsummary = summarizer(text)\nprint(summary) \n```", "```py\n[{'summary_text': 'This is a live chat conversation between a customer and a ChatBot.'}] \n```", "```py\n    import datasets\n    from datasets import load_dataset\n\n    def fetch_dataset(dataset_name: str=\"amazon_reviews_multi\",\n                      configuration: str=\"en\", split: str=\"train\"\n                      ) -> datasets.arrow_dataset.Dataset:\n        '''\n        Fetch dataset from HuggingFace datasets server.\n        '''\n        dataset = load_dataset(dataset_name, configuration, split=split)\n        return dataset \n    ```", "```py\n    import typing\n    from transformers import AutoTokenizer\n\n    def tokenize_dataset(tokenizer: AutoTokenizer, \n                         dataset: datasets.arrow_dataset.Dataset,\n                         sample=True) -> datasets.arrow_dataset.Dataset:\n        '''\n        Tokenize the HuggingFace dataset object and format for use in\n        later Pytorch logic.\n        '''\n        tokenized_dataset = dataset.map(\n            lambda x: tokenizer(x[\"review_body\"], padding=\"max_length\",\n                                truncation=True),\n            batched=True\n        )\n        # Torch needs the target column to be named \"labels\"\n        tokenized_dataset = tokenized_dataset.rename_column(\"stars\",\n                                                             \"labels\")\n\n        # We can format the dataset for Torch using this method.\n        tokenized_dataset.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"token_type_ids\",\n                                    \"attention_mask\", \"labels\"]\n        )\n        # Let's downsample to speed things up for testing\n        if sample==True:\n            tokenized_dataset_small = tokenized_dataset.\\\n                                      shuffle(seed=42).select(range(10))\n            return tokenized_dataset_small\n        else:\n            return tokenized_dataset \n    ```", "```py\n    from torch.utils.data import DataLoader\n\n    def create_dataloader(\n        tokenized_dataset: datasets.arrow_dataset.Dataset,\n        batch_size: int = 16,\n        shuffle: bool = True\n        ):\n        dataloader = DataLoader(tokenized_dataset,\n                                shuffle=shuffle,\n                                batch_size=batch_size)\n        return dataloader \n    ```", "```py\n    from torch.optim import AdamW\n    from transformers import get_scheduler\n\n    def configure_scheduler_optimizer(\n        model: typing.Any,\n        dataloader: typing.Any,\n        learning_rate: float,\n        num_training_steps: int) -> tuple[typing.Any, typing.Any]:\n        '''\n        Return a learning scheduler for use in training using the AdamW\n        optimizer\n        '''\n        optimizer = AdamW(model.parameters(), lr=learning_rate)\n        lr_scheduler = get_scheduler(\n            name=\"linear\", \n            optimizer=optimizer, \n            num_warmup_steps=0, \n            num_training_steps=num_training_steps\n        )\n        return lr_scheduler, optimizer \n    ```", "```py\n    import torch\n    from tqdm.auto import tqdm\n\n    def transfer_learn(\n        model: typing.Any, \n        dataloader: typing.Any,\n        learning_rate: float = 5e-5,\n        num_epochs: int = 3,\n        progress_bar: bool = True )-> typing.Any:\n        device = torch.device(\"cuda\") if torch.cuda.is_available() else\\\n                 torch.device(\"cpu\")\n        model.to(device)\n\n        num_training_steps = num_epochs * len(dataloader)\n        lr_scheduler, optimizer = configure_scheduler_optimizer(\n            model = model, \n            dataloader = dataloader,\n            learning_rate = learning_rate,\n            num_training_steps = num_training_steps\n        )\n\n        if progress_bar:\n            progress_bar = tqdm(range(num_training_steps))\n        else:\n            pass\n        model.train()\n        for epoch in range(num_epochs):\n            for batch in dataloader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = model(**batch)\n                loss = outputs.loss\n                loss.backward()\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                if progress_bar:\n                    progress_bar.update(1)\n                else:\n                    pass\n        return model \n    ```", "```py\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n    tokenized_dataset = tokenize_dataset(tokenizer=tokenizer,\n                                         dataset=dataset, sample=True)\n    dataloader = create_dataloader(tokenized_dataset=tokenized_dataset)\n    model = AutoModelForSequenceClassification.from_pretrained(\n                \"bert-base-cased\", num_labels=6) # 0-5 stars\n    transfer_learned_model = transfer_learn(\n        model = model,\n        dataloader=dataloader\n    ) \n    ```", "```py\n    import evaluate\n\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else\\\n        torch.device(\"cpu\")\n    metric = evaluate.load(\"accuracy\")\n    model.eval()\n    eval_dataset = fetch_dataset(split=\"test\")\n    tokenized_eval_dataset = tokenize_dataset(\n        tokenizer=tokenizer,dataset=eval_dataset, sample=True)\n    eval_dataloader = create_dataloader(\n        tokenized_dataset=tokenized_eval_dataset)\n    for batch in eval_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        metric.add_batch(predictions=predictions,\n                          references=batch[\"labels\"])\n    metric.compute() \n    ```", "```py\n    {'accuracy': 0.8} \n    ```", "```py\n    from transformers import TrainingArguments \n    training_args = TrainingArguments(output_dir=\"trainer_checkpoints\") \n    ```", "```py\n    import numpy as np\n    import evaluate\n\n    metric = evaluate.load(\"accuracy\")\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions,\n                               references=labels) \n    ```", "```py\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n    ) \n    ```", "```py\n    trainer.train(). \n    ```", "```py\ndef optuna_hp_space(trial):\n    return {\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4,\n                                              log=True)\n    } \n```", "```py\ndef model_init():\n    model = AutoModelForSequenceClassification.from_pretrained(\n                \"bert-base-cased\", num_labels=6)\n    return model \n```", "```py\ntrainer = Trainer(\n    model=None,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    model_init=model_init,\n) \n```", "```py\nbest_run = trainer.hyperparameter_search(\n    n_trials=20, \n    direction=\"maximize\", \n    hp_space=optuna_hp_space\n) \n```", "```py\n    pip install langchain\n    pip install openai \n    ```", "```py\n    import os\n    openai_key = os.getenv('OPENAI_API_KEY') \n    ```", "```py\n    from langchain.chat_models import ChatOpenAI\n    gpt = ChatOpenAI(model_name='''gpt-3.5-turbo''') \n    ```", "```py\n    template = '''Question: {question}\n                  Answer: '''\n    prompt = PromptTemplate(\n      template=template,\n      input_variables=['question']\n    ) \n    ```", "```py\n    # user question\n    question = \"Where does Andrew McMahon, author of 'Machine Learning\n                Engineering with Python', work?\"\n    # create prompt template > LLM chain\n    llm_chain = LLMChain(\n      prompt=prompt,\n      llm=gpt\n    ) \n    ```", "```py\n    print(llm_chain.run(question)) \n    ```", "```py\n    As an AI language model, I do not have access to real-time information. However, Andrew McMahon is a freelance data scientist and software engineer based in Bristol, United Kingdom. \n    ```", "```py\nquestions = [\n  {'question': '''Where does Andrew McMahon, author of 'Machine Learning Engineering with Python', work?'''},\n  {'question': 'What is MLOps?'},\n  {'question': 'What is ML engineering?'},\n  {'question': 'What's your favorite flavor of ice cream?'}\n]\nprint(llm_chain.generate(questions)) \n```", "```py\ngenerations=[[ChatGeneration(text='As an AI modeler and a data scientist, Andrew McMahon works at Cisco Meraki, a subsidiary of networking giant Cisco, in San Francisco Bay Area, USA.', generation_info=None, message=AIMessage(content='As an AI modeler and a data scientist, Andrew McMahon works at Cisco Meraki, a subsidiary of networking giant Cisco, in San Francisco Bay Area, USA.', additional_kwargs={}))], …] \n```"]