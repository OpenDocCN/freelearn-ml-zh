<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer096">
<h1 class="chapter-number" id="_idParaDest-266"><a id="_idTextAnchor269"/>16</h1>
<h1 id="_idParaDest-267"><a id="_idTextAnchor270"/>Photorealism in Computer Vision</h1>
<p>In this chapter, you will learn why we need photorealistic synthetic data in computer vision. Then, you will explore the main approaches to generating photorealistic synthetic data. After that, you will comprehend the main challenges and limitations. Although this chapter focuses on computer vision, the discussion can be generalized to other domains <span class="No-Break">and applications.</span></p>
<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Synthetic data photorealism for <span class="No-Break">computer vision</span></li>
<li><span class="No-Break">Photorealism approaches</span></li>
<li>Photorealism <span class="No-Break">evaluation metrics</span></li>
<li>Challenges and limitations of photorealistic <span class="No-Break">synthetic data</span></li>
</ul>
<h1 id="_idParaDest-268"><a id="_idTextAnchor271"/>Synthetic data photorealism for computer vision</h1>
<p>In this section, you <a id="_idIndexMarker649"/>will learn why <a id="_idIndexMarker650"/>photorealism is essential in computer vision. Photorealism of synthetic data is one of the main factors that mitigates the domain gap between real and synthetic data. Thus, training computer vision models on photorealistic synthetic data improves the performance of these models on real data. For more details, please refer to <em class="italic">Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding</em> (<a href="https://arxiv.org/abs/2011.02523">https://arxiv.org/abs/2011.02523</a>) and <em class="italic">A Review of Synthetic Image Data and Its Use in Computer Vision</em> (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9698631">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9698631</a>). Additionally, synthetic data can be used to evaluate computer vision algorithms. However, evaluating these models on non-photorealistic synthetic data may cause these models to show poor performance not because of the challenging nature of the test scenarios but because of the domain gap itself. Thus, photorealistic synthetic data is essential to effectively train and accurately evaluate <span class="No-Break">ML models.</span></p>
<p>Now, let us discuss the main<a id="_idIndexMarker651"/> benefits of utilizing <a id="_idIndexMarker652"/>photorealistic synthetic data. First, let us delve into <span class="No-Break">feature extraction.</span></p>
<h2 id="_idParaDest-269"><a id="_idTextAnchor272"/>Feature extraction</h2>
<p>Computer vision<a id="_idIndexMarker653"/> algorithms usually rely on automatic feature extraction, which is learned in the training stage. ML models learn how to identify the most reliable and discriminative features and patterns, which subsequent submodules leverage to learn the actual task, such as semantic segmentation, depth estimation, and visual object tracking. Training your computer vision model on non-photorealistic synthetic data that oversimplifies the real world will lead to inappropriate feature extraction. Conversely, photorealistic synthetic data helps the ML model to learn how to extract discriminative features. Thus, the ML model will perform well in the real world. This is because realistic data helps ML models to better understand the relationship between scene elements, how they affect each other, and how they contribute to the task <span class="No-Break">being learned.</span></p>
<h2 id="_idParaDest-270"><a id="_idTextAnchor273"/>Domain gap</h2>
<p>Photorealistic synthetic <a id="_idIndexMarker654"/>data mitigates the domain gap between synthetic and real domains. The main reason is that realistic data partially resamples the real data, which helps computer vision models to be trained on data that is closer to the environment where the model will be deployed in practice. Thus, the model can still generalize well from the synthetic data learned in the training stage. On the other hand, large-scale, diverse, but non-realistic synthetic data may enlarge the domain gap and significantly hinder <span class="No-Break">the performance.</span></p>
<h2 id="_idParaDest-271"><a id="_idTextAnchor274"/>Robustness</h2>
<p>Creating simulators <a id="_idIndexMarker655"/>that simulate realistic lighting, textures, shaders, animations, and camera movements enables researchers to generate large-scale and diverse synthetic training datasets that properly reflect the challenges and varieties in the real world. Thus, computer vision algorithms can be trained on more real scenarios to learn how to adapt to the actual complexities of the real world. This is important to make computer vision algorithms more robust in the real world where collecting real data is extremely expensive or <span class="No-Break">not applicable.</span></p>
<h2 id="_idParaDest-272"><a id="_idTextAnchor275"/>Benchmarking performance</h2>
<p>Synthetic data <a id="_idIndexMarker656"/>provides a more accurate and efficient way to generate the ground truth (refer to <a href="B18494_05.xhtml#_idTextAnchor083"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>). The ground truth is essential for assessing ML models’ performance. Photorealistic synthetic data enables us to ensure that the performance of synthetic data appropriately reflects that expected in the real world. Conversely, non-realistic synthetic data is less suitable for accurate evaluation <span class="No-Break">and benchmarking.</span></p>
<h1 id="_idParaDest-273"><a id="_idTextAnchor276"/>Photorealism approaches</h1>
<p>In this section, you <a id="_idIndexMarker657"/>will learn about and explore the main approaches usually deployed to generate photorealistic synthetic data. We will learn about <span class="No-Break">the following:</span></p>
<ul>
<li>Physically <span class="No-Break">based rendering</span></li>
<li>Neural <span class="No-Break">style transfer</span></li>
</ul>
<h2 id="_idParaDest-274"><a id="_idTextAnchor277"/>Physically Based Rendering (PBR)</h2>
<p>The <strong class="bold">Physically Based Rendering</strong> (<strong class="bold">PBR</strong>) approach is<a id="_idIndexMarker658"/> widely <a id="_idIndexMarker659"/>used in game engines such as <em class="italic">Unity</em> and <em class="italic">Unreal</em> to accurately simulate how materials in the 3D virtual world interact with light. In the real world, this is a complex process, thus it requires a significant understanding of optics and many simplifications to make these processes applicable to game engines. <strong class="bold">Physically based materials</strong> are essential <a id="_idIndexMarker660"/>to this approach. They resemble how similar materials in the real world interact with light. These materials usually have properties and parameters that are calculated based on real measurements from real-world materials. The properties may include absorption, scattering, and refraction coefficients and parameters. It should be noted that the main principle behind <a id="_idIndexMarker661"/>PBR is <strong class="bold">energy conservation</strong>, which means that light energy reflected and scattered by a material should not exceed the total incoming or received light energy by <span class="No-Break">this material.</span></p>
<p>As expected, deploying a photorealistic rendering pipeline will help us to simulate and render more accurate and realistic light behaviors and materials. Thus, we can generate more photorealistic <a id="_idIndexMarker662"/><span class="No-Break">synthetic</span><span class="No-Break"><a id="_idIndexMarker663"/></span><span class="No-Break"> data.</span></p>
<h2 id="_idParaDest-275"><a id="_idTextAnchor278"/>Neural style transfer</h2>
<p><strong class="bold">Neural style transfer</strong> is a well-known<a id="_idIndexMarker664"/> technique<a id="_idIndexMarker665"/> that transfers an artistic style from one image to another while preserving the content of the latter. This method can be applied to synthetic datasets to improve their photorealism and thus mitigate the domain gap between synthetic and real data. For example, the <strong class="bold">Sim2Real</strong>-style <a id="_idIndexMarker666"/>transfer model can be deployed to bridge the gap between synthetic and real data for the task of pose estimation. For more information, please refer to <em class="italic">Sim2Real Instance-Level Style Transfer for 6D Pose Estimation</em> (<a href="https://arxiv.org/abs/2203.02069">https://arxiv.org/abs/2203.02069</a>). Additionally, there are many interesting works that explore how to adapt the <em class="italic">GTA5</em> synthetic dataset (https://www.v7labs.com/open-datasets/gta5), which was generated from the <em class="italic">Grand Theft Auto </em><em class="italic">V</em> video game, to the real <em class="italic">Cityscapes</em> <span class="No-Break">dataset (</span><a href="https://www.cityscapes-dataset.com"><span class="No-Break">https://www.cityscapes-dataset.com</span></a><span class="No-Break">).</span></p>
<h1 id="_idParaDest-276"><a id="_idTextAnchor279"/>Photorealism evaluation metrics</h1>
<p>One of the<a id="_idIndexMarker667"/> main issues within this subject matter is quantitatively assessing the photorealism of the generated synthetic images. In this section, we will explore the main metrics usually used. We will explore <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Structural Similarity Index </strong><span class="No-Break"><strong class="bold">Measure</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SSIM</strong></span><span class="No-Break">)</span></li>
<li><strong class="bold">Learned Perceptual Image Patch </strong><span class="No-Break"><strong class="bold">Similarity</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LPIPS</strong></span><span class="No-Break">)</span></li>
<li><span class="No-Break">Expert evaluation</span></li>
</ul>
<h2 id="_idParaDest-277"><a id="_idTextAnchor280"/>Structural Similarity Index Measure (SSIM)</h2>
<p>SSIM is <a id="_idIndexMarker668"/>one of the<a id="_idIndexMarker669"/> most widely used metrics to measure the structural similarity between two images. It was first introduced in the paper titled <em class="italic">Image quality assessment: from error visibility to structural similarity</em> (<a href="https://ieeexplore.ieee.org/document/1284395">https://ieeexplore.ieee.org/document/1284395</a>). The SSIM metric does not compare individual pixels of the two images. However, it considers a group of pixels assuming that spatially close pixels have inter-dependencies. These dependencies can be linked to the actual structure of objects that were captured and presented by the given images. SSIM specifically focuses on the spatial relationships among pixels, such as edges and textures, to assess how close an image is to a <span class="No-Break">reference one.</span></p>
<p>Recently, it was shown that SSIM may lead to incorrect or unexpected results when utilized to compare images or when included in the training loss of ML models. For more information, please refer to <em class="italic">Understanding </em><span class="No-Break"><em class="italic">SSIM</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/2006.13846.pdf"><span class="No-Break">https://arxiv.org/pdf/2006.13846.pdf</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-278"><a id="_idTextAnchor281"/>Learned Perceptual Image Patch Similarity (LPIPS)</h2>
<p><strong class="bold">LPIPS</strong> measures<a id="_idIndexMarker670"/> the<a id="_idIndexMarker671"/> distance between images in the feature space by leveraging networks trained for computer vision tasks on large-scale datasets, for example, <em class="italic">VGG</em> trained on the <em class="italic">ImageNet</em> dataset. It was found that LPIPS gives more similar results to how humans perceive similarity between images. For more information, please refer to <em class="italic">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</em> (<a href="https://arxiv.org/abs/1801.03924">https://arxiv.org/abs/1801.03924</a>). In this paper, it was found that ML models trained on complex visual tasks learn a rich, general-purpose, and useful visual representation of the world. This knowledge can be leveraged to assess the visual similarity between images in a similar manner to how humans may perceive <span class="No-Break">this similarity.</span></p>
<h2 id="_idParaDest-279"><a id="_idTextAnchor282"/>Expert evaluation</h2>
<p>In certain <a id="_idIndexMarker672"/>applications, we<a id="_idIndexMarker673"/> may need to request a domain expert evaluation of the generated synthetic images. For example, assume your generative model is generating synthetic <a id="_idIndexMarker674"/>images of <strong class="bold">Computerized Tomography</strong> (<strong class="bold">CT</strong>) scans that will be used later to train a cancer prediction ML model. We can still leverage qualitative metrics, such as SSIM to assess structural similarity with a real data counterpart, <strong class="bold">Peak Signal to Noise Ratio</strong> (<strong class="bold">PSNR</strong>) to measure<a id="_idIndexMarker675"/> the quality of the generated images, <strong class="bold">Fréchet Inception Distance</strong> (<strong class="bold">FID</strong>) to<a id="_idIndexMarker676"/> give us an idea about the realism and diversity of the generated samples, and LPIPS to assess the perceptual similarity to real data. However, expert evaluation is still essential for these critical problems. Expert evaluation of the synthetically generated data is essential to verify its validity, quality, and diversity. Most importantly, this<a id="_idIndexMarker677"/> evaluation<a id="_idIndexMarker678"/> is essential to confirm that synthetic data adheres to <span class="No-Break">ethical standards.</span></p>
<p>Next, let us discuss some of the main challenges in generating photorealistic <span class="No-Break">synthetic data.</span></p>
<h1 id="_idParaDest-280"><a id="_idTextAnchor283"/>Challenges and limitations of photorealistic synthetic data</h1>
<p>In this section, you will <a id="_idIndexMarker679"/>explore the main challenges that hinder generating photorealistic synthetic data in practice. We will highlight the <span class="No-Break">following limitations.</span></p>
<h2 id="_idParaDest-281"><a id="_idTextAnchor284"/>Creating hyper-realistic scenes</h2>
<p>The real world is <a id="_idIndexMarker680"/>complex, diverse, and intricate with details. Scene<a id="_idIndexMarker681"/> elements in reality have various shapes, sophisticated dynamics, and highly non-linear interactions. Additionally, our vision and perception of the world are limited and subject to many factors, such as cognitive biases and color perception. Additionally, we may judge photorealism differently based on the context and evaluator. For example, what is more photorealistic, realistic foreground objects and a non-realistic background or the opposite? All these aspects together make generating highly realistic scenes rather hard <span class="No-Break">in practice.</span></p>
<h2 id="_idParaDest-282"><a id="_idTextAnchor285"/>Resources versus photorealism trade-off</h2>
<p>Budget, time, skills, and other factors limit the photorealism of the generated synthetic data. As expected, simulating <a id="_idIndexMarker682"/>realistic worlds populated <a id="_idIndexMarker683"/>with high-poly 3D models and diverse, realistic animations necessitates substantial computational resources. Additionally, employing advanced and complex light- rendering mechanisms, such as ray tracing and PBR, further increases the demand for more processing capabilities and resources. <strong class="bold">Ray tracing</strong> is a<a id="_idIndexMarker684"/> rendering technique that can simulate the realistic behavior of light and its complex interactions with scene elements. Thus, there is always a trade-off observed between resources <span class="No-Break">and photorealism.</span></p>
<p>Therefore, it is very important to identify what you mean by photorealism for your particular problem. Additionally, you need to carefully consider which metrics you will deploy to assess the <a id="_idIndexMarker685"/>quality and photorealism of the generated synthetic data, taking into account the <span class="No-Break">available resources.</span></p>
<h1 id="_idParaDest-283"><a id="_idTextAnchor286"/>Summary</h1>
<p>In this chapter, you learned the main reasons that motivate researchers to strive to achieve high photorealism in generated synthetic data for computer vision problems. You learned about two main approaches usually utilized for that aim. Then, you explored well-known quantitative and qualitative measures deployed to assess the photorealism of the generated synthetic data. Finally, you examined some issues that hinder generating ideal photorealistic synthetic data in practice. In the next and final chapter, we will wrap up and conclude <span class="No-Break">the book.</span></p>
</div>
</div></body></html>