- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Using Google Cloud ML Best Practices
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用谷歌云ML最佳实践
- en: In this chapter, we will discuss the best practices for implementing **Machine
    Learning** (**ML**) in Google Cloud. We will go through an implementation of a
    customer-trained ML model development process in GCP and provide recommendations
    throughout.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在谷歌云中实施**机器学习**（**ML**）的最佳实践。我们将通过在GCP中实施一个客户训练的ML模型开发过程，并提供全程的建议。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: ML environment setup
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML环境设置
- en: ML data storage and processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML数据存储和处理
- en: ML model training
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML模型训练
- en: ML model deployment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML模型部署
- en: ML workflow orchestration
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML工作流程编排
- en: ML model continuous monitoring
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML模型持续监控
- en: This chapter aims to integrate the knowledge we have learned so far in this
    book and apply it to a customer-trained ML project. We will start by setting up
    the ML environment.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在将我们在这本书中学到的知识整合起来，并将其应用于客户训练的ML项目。我们将首先设置ML环境。
- en: ML environment setup
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML环境设置
- en: In [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094), *Developing and Deploying
    ML Models*, in the *Preparing the platform* section, we learned about the ML platform
    in the Cloud. Then, in [*Chapter 7*](B18333_07.xhtml#_idTextAnchor143), *Exploring
    Google Cloud Vertex AI*, we introduced the Vertex AI services. For a customer-trained
    model development platform, we recommend **Vertex AI Workbench user-managed notebooks**.
    Let’s look at the details from the prospects of performance, cost, and security.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B18333_04.xhtml#_idTextAnchor094)“开发和部署ML模型”的*准备平台*部分，我们学习了云中的ML平台。然后，在[*第7章*](B18333_07.xhtml#_idTextAnchor143)“探索谷歌云Vertex
    AI”中，我们介绍了Vertex AI服务。对于客户训练的模型开发平台，我们推荐**Vertex AI Workbench用户管理的笔记本**。让我们从性能、成本和安全的角度来看一下细节。
- en: With Vertex AI Workbench user-managed notebooks, you have the flexibility and
    options to implement **performance excellency**. You can create an instance with
    the existing deep learning VM images that have the latest ML and data science
    libraries preinstalled, along with the latest accelerator drivers. Depending on
    your data, model, and workloads, you can choose the right VM instance type to
    fit your environment and optimize performance, from general-purpose compute (E2,
    N1, N2, and N2D), to memory-optimized (M1 and M2), to compute-optimized (C2),
    and so on. You can also create a notebooks instance based on a custom container
    to tailor your ML environment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Vertex AI Workbench用户管理的笔记本，您有灵活性和选项来实现**性能卓越**。您可以使用预安装了最新ML和数据科学库以及最新加速器驱动程序的现有深度学习虚拟机镜像创建一个实例。根据您的数据、模型和工作负载，您可以选择合适的虚拟机实例类型以适应您的环境并优化性能，从通用计算（E2、N1、N2和N2D），到内存优化（M1和M2），到计算优化（C2），等等。您还可以基于自定义容器创建笔记本实例，以定制您的ML环境。
- en: With Vertex AI Workbench user-managed notebooks, you can go with GCP best practices
    and **reduce costs**. As with any Google Cloud services, treat your notebook’s
    VM instances as flexible and disposable resources; when you train ML models on
    the VM instances, make sure that you store all your data in Cloud Storage or BigQuery,
    instead of storing it in the instances' local storage, such as persistent disks,
    so that you can stop or delete the instances when you are done with the ML experimenting
    or training. During the ML model development process, always monitor the VM instances’
    performances and costs, and scale out/in based on the workloads while utilizing
    Google’s Managed instance groups (MIGs).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Vertex AI Workbench用户管理的笔记本，您可以遵循GCP最佳实践并**降低成本**。与任何谷歌云服务一样，将您的笔记本的虚拟机实例视为灵活且可丢弃的资源；当您在虚拟机实例上训练ML模型时，请确保将所有数据存储在云存储或BigQuery中，而不是存储在实例的本地存储中，例如持久磁盘，这样您就可以在完成ML实验或训练后停止或删除实例。在ML模型开发过程中，始终监控虚拟机实例的性能和成本，并根据工作负载进行扩展/缩减，同时利用谷歌的托管实例组（MIGs）。
- en: '**Security** is always an important area we need to address in the Cloud. Some
    of the best security practices are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**安全**始终是我们需要在云中解决的问题的重要领域。以下是一些最佳安全实践：'
- en: With Vertex AI Workbench user-managed notebooks, we recommend that a user-managed
    notebooks instance is created for each member of the data science team. If a team
    member is involved in multiple projects, we recommend using multiple user-managed
    notebooks instances for the member and treating each instance as a virtual workspace.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Vertex AI工作台用户管理的笔记本，我们建议为数据科学团队的每个成员创建一个用户管理的笔记本实例。如果团队成员参与多个项目，我们建议为该成员使用多个用户管理的笔记本实例，并将每个实例视为一个虚拟工作空间。
- en: Operating Vertex AI requires collaboration among a variety of teams, and it
    is super important to determine which groups or systems will be responsible for
    which functions. From a networking point of view, you should configure user-managed
    notebooks to use shared VPCs if possible, to minimize access to the notebook instances.
    Restrictive firewall rules must also be enabled to limit access to notebook instances
    and other Vertex AI resources.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Vertex AI需要各种团队的协作，并且确定哪些团队或系统将负责哪些功能非常重要。从网络的角度来看，如果可能的话，应配置用户管理的笔记本使用共享VPC，以最小化对笔记本实例的访问。还必须启用限制性防火墙规则，以限制对笔记本实例和其他Vertex
    AI资源的访问。
- en: For data and model storing, we also recommend storing the training data and
    training model in the same project for reproducibility. In a Google organization
    with multiple folders and multiple projects, the best practice is leveraging Google
    IAM roles and groups.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于数据和模型存储，我们还建议将训练数据和训练模型存储在同一项目中，以提高可重复性。在具有多个文件夹和多个项目的Google组织中，最佳实践是利用Google
    IAM角色和组。
- en: For data protection, we recommend using Google Cloud organization policies and
    Data Loss Prevention (DLP) tools to protect Personal Identifiable Information
    (PII) data. Data encryption is also recommended for storing data at rest and in
    transit in the Vertex AI notebooks instances. Vertex AI supports **Customer-Managed
    Encryption Keys** (**CMEK**) across most of its components.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了数据保护，我们建议使用Google Cloud组织策略和数据丢失预防（DLP）工具来保护个人信息（PII）数据。还建议在Vertex AI笔记本实例中存储数据时进行数据加密。Vertex
    AI支持在其大多数组件中使用**客户管理的加密密钥**（**CMEK**）。
- en: Now that we have understood the environment setup, let’s move to data storage
    and processing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了环境设置，让我们转向数据存储和处理。
- en: ML data storage and processing
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习数据存储和处理
- en: As we discussed in [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094), *Developing
    and Deploying ML Models*, storing data involves collecting raw data from various
    data sources and storing it in a centralized repository. On the other hand, data
    processing includes both data engineering and feature engineering. Data engineering
    is the process of converting raw data (the data in its source form) into prepared
    data (the dataset in the form that is ready to be input into ML tasks). Feature
    engineering then tunes the prepared data to create the features expected by the
    ML model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第4章*](B18333_04.xhtml#_idTextAnchor094)“开发和部署机器学习模型”中讨论的，存储数据涉及从各种数据源收集原始数据并将其存储在集中式存储库中。另一方面，数据处理包括数据工程和特征工程。数据工程是将原始数据（源数据形式的数据）转换为准备数据（准备好的数据集，用于输入到机器学习任务中）的过程。特征工程随后调整准备好的数据以创建机器学习模型所期望的特征。
- en: For structured data, we recommend using **Google Cloud BQ** to store and process
    it. For unstructured data, videos, audio, and image data, we recommend using **Google
    Cloud** object storage to store them and **Google Cloud Dataflow** or **Dataproc**
    to process them. As we have discussed, **Dataflow** is a managed service that
    uses the **Apache Beam** programming model to convert unstructured data into binary
    formats and can improve data ingestion performance. Dataproc is a managed **Apache
    Spark** and **Apache Hadoop** service that leverages open source data tools for
    batch processing, querying, streaming, and ML.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于结构化数据，我们建议使用**Google Cloud BQ**来存储和处理。对于非结构化数据，如视频、音频和图像数据，我们建议使用**Google
    Cloud**对象存储来存储它们，并使用**Google Cloud Dataflow**或**Dataproc**来处理它们。正如我们之前讨论的，**Dataflow**是一个托管服务，它使用**Apache
    Beam**编程模型将非结构化数据转换为二进制格式，可以提高数据摄取性能。Dataproc是一个托管**Apache Spark**和**Apache Hadoop**服务，它利用开源数据工具进行批量处理、查询、流处理和机器学习。
- en: For supervised ML, which needs labeled datasets, we recommend using the **Google
    Vertex AI Data Labeling** service, especially for unstructured data. From a security
    point of view, we recommend using **Google Cloud IAM** to manage data access in
    Cloud Storage and BQ, using GCP DLP to manage PII and other sensitive data, and
    using GCP Key Management Services (KMS) for data encryption key management.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要标记数据集的监督式机器学习，我们推荐使用**Google Vertex AI数据标注**服务，特别是对于非结构化数据。从安全角度考虑，我们推荐使用**Google
    Cloud IAM**来管理云存储和BQ中的数据访问，使用GCP DLP来管理PII和其他敏感数据，以及使用GCP密钥管理服务（KMS）进行数据加密密钥管理。
- en: Once the data has been preprocessed, we recommend using a Vertex AI-managed
    dataset to create a link between your data and custom-trained models and provide
    descriptive statistics to split data into training, validation, and testing subsets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据预处理完成，我们建议使用Vertex AI管理的数据集来创建您数据和自定义训练模型之间的链接，并提供描述性统计信息以将数据分割成训练、验证和测试子集。
- en: Depending on the ML model features, many methods can be utilized in feature
    engineering. We recommend using **Vertex AI Feature Store**, which can be used
    to create new features from the data lakes, schedule data processing, and feature
    engineering jobs, ingest them into Vertex Feature Store for online or batch serving,
    and share the common features within the data science team.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 根据机器学习模型的特点，可以在特征工程中利用许多方法。我们推荐使用**Vertex AI特征存储**，它可以用于从数据湖中创建新特征，安排数据处理和特征工程作业，将它们导入Vertex特征存储进行在线或批量服务，并在数据科学团队内部共享常用特征。
- en: ML model training
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型训练
- en: ML model training is a critical phase in ML development, which is why we recommend
    using GCP Vertex AI Training. Instead of manually adjusting hyperparameters with
    numerous training runs for optimal values, we recommend the automated Vertex AI
    training model enhancer to test different hyperparameter configurations, and **Google
    Vertex AI TensorBoard** to track, share, and compare model metrics such as loss
    functions to visualize model graphs. This allows you to compare various experiments
    for parameter tuning and model optimization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型训练是机器学习开发的关键阶段，这就是我们推荐使用GCP Vertex AI训练的原因。我们建议使用自动化的Vertex AI训练模型增强器来测试不同的超参数配置，而不是手动调整超参数进行多次训练以获得最佳值，并使用**Google
    Vertex AI TensorBoard**来跟踪、共享和比较模型指标，如损失函数，以可视化模型图。这允许您比较各种实验以进行参数调整和模型优化。
- en: Using Vertex AI Workbench user-managed notebooks, you can develop your code
    conveniently and interactively, and we recommend operationalizing your code for
    reproducibility and scalability and running your code in either Vertex training
    or **Vertex AI Pipelines**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Vertex AI Workbench用户管理的笔记本，您可以方便且交互式地开发代码，我们建议将您的代码进行操作化以实现可重复性和可扩展性，并在Vertex训练或**Vertex
    AI Pipelines**中运行您的代码。
- en: After model training, it is recommended that you use **Vertex Explainable AI**
    to study and gain insights regarding feature contributions and understand your
    model’s behavior. Vertex Explainable AI helps you understand your model’s outputs
    – it tells you how much each feature in the data contributed to the predicted
    result. Then, you can use this information to see whether your model is behaving
    as expected, to recognize bias (if there is any) in your models, and get some
    ideas to improve your model and your training data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练后，建议您使用**Vertex可解释AI**来研究和了解特征贡献，并理解您模型的行为。Vertex可解释AI帮助您理解模型输出——它告诉您数据中的每个特征对预测结果贡献了多少。然后，您可以使用这些信息来查看您的模型是否按预期运行，识别模型中的偏差（如果有），并获得一些改进模型和训练数据的想法。
- en: ML model deployment
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型部署
- en: ML model deployment refers to putting a model into production. Once an ML model
    has been deployed into production, it can be used to predict new data. We recommend
    using the Vertex AI console or API to deploy a trained ML model. With Vertex AI,
    we can serve the model in production with batch prediction; we recommend specifying
    the appropriate hardware for your model and determining how to pass inputs to
    the model. With Vertex AI, we can also serve the model with online endpoint prediction;
    we recommend using Vertex AI Feature Store’s online serving API and turning on
    automatic scaling with a minimum of two nodes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ML模型部署指的是将模型投入生产。一旦ML模型被部署到生产环境中，它就可以用来预测新的数据。我们建议使用Vertex AI控制台或API来部署训练好的ML模型。使用Vertex
    AI，我们可以通过批量预测在生产中提供服务；我们建议为您的模型指定合适的硬件，并确定如何将输入传递给模型。使用Vertex AI，我们还可以通过在线端点预测来提供服务；我们建议使用Vertex
    AI Feature Store的在线服务API，并开启至少两个节点的自动扩展。
- en: ML workflow orchestration
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML工作流程编排
- en: As we discussed in [*Chapter 7*](B18333_07.xhtml#_idTextAnchor143), *Exploring
    Google Cloud Vertex AI*, Vertex AI Pipelines is a fully managed service that allows
    you to retrain your models as often as necessary so that you can adapt to changes
    and maintain performance over time. We recommend Vertex AI Pipelines for Cloud
    ML workflow orchestration.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第7章*](B18333_07.xhtml#_idTextAnchor143)中讨论的，*探索Google Cloud Vertex AI*，Vertex
    AI管道是一个完全托管的服务，允许您根据需要重新训练模型，以便您能够适应变化并保持性能。我们推荐使用Vertex AI管道进行云ML工作流程编排。
- en: If you’re using the **Google TensorFlow framework**, we recommend using **TensorFlow
    Extended** to define your pipeline and the operations for each step, then executing
    it on Vertex AI’s serverless pipeline system. TensorFlow provides pre-built components
    for common steps in the Vertex AI workflow, such as data ingestion, data validation,
    and training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是**Google TensorFlow框架**，我们建议使用**TensorFlow Extended**来定义您的管道和每个步骤的操作，然后在Vertex
    AI的无服务器管道系统中执行它。TensorFlow为Vertex AI工作流程中的常见步骤提供了预构建的组件，例如数据摄取、数据验证和训练。
- en: If you are using other frameworks, we recommend using **Kubeflow Pipeline**,
    which is very flexible and allows you to use simple code to construct pipelines.
    Kubeflow Pipeline also provides Google Cloud pipeline components such as Vertex
    AI AutoML.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用其他框架，我们建议使用**Kubeflow Pipeline**，它非常灵活，允许您使用简单的代码构建管道。Kubeflow Pipeline还提供了Google
    Cloud管道组件，如Vertex AI AutoML。
- en: ML model continuous monitoring
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML模型持续监控
- en: 'Once you’ve deployed your model into production, you need to monitor model
    performance continuously to ensure that it is performing as expected. We recommend
    using Vertex AI, which provides two ways to monitor your ML models:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您将模型部署到生产环境中，您需要持续监控模型性能，以确保其按预期运行。我们建议使用Vertex AI，它提供了两种监控您的ML模型的方法：
- en: '**Skew detection**, which looks for the degree of distortion between your model
    training and production data.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**倾斜检测**，寻找模型训练和生产数据之间的扭曲程度。'
- en: '**Drift detection**, which looks for drift in your production data. Drift occurs
    when the statistical properties of the inputs and the target change over time
    and cause predictions to become less accurate as time passes.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**漂移检测**，寻找生产数据中的漂移。漂移发生在输入和目标的统计属性随时间变化，导致预测随着时间的推移变得越来越不准确。'
- en: For skew and drift detection, we recommend setting up a model monitoring job
    by providing a pointer to the training data that you used to train your model,
    and then tuning the thresholds that are used for alerting to measure skew or drift
    occurring in your data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于倾斜和漂移检测，我们建议通过提供一个指向您用于训练模型的训练数据的指针来设置模型监控作业，然后调整用于警报的阈值，以测量数据中发生的倾斜或漂移。
- en: You can also use feature attributions in Vertex Explainable AI to detect data
    drift or skew as an early indicator that model performance may be degrading. For
    example, let’s say that your model originally relied on five features to make
    predictions in training and test data, but when going into production, it began
    to rely on entirely different features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在Vertex Explainable AI中使用特征归因来检测数据漂移或倾斜，作为模型性能可能下降的早期指标。例如，假设您的模型最初依赖于五个特征在训练和测试数据中进行预测，但进入生产后，它开始完全依赖于不同的特征。
- en: Summary
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the best practices for implementing ML in Google
    Cloud, with a focus on custom-trained models based on your data and code.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了在Google Cloud中实施ML的最佳实践，重点关注基于您的数据和代码的定制训练模型。
- en: This chapter concludes *Part 3* of this book, in which we have discussed Google
    BQ and BQML for training ML models from structured data, Google ML training frameworks
    such as TensorFlow and Keras, the Google ML training suite Vertex AI, Google Cloud
    ML APIs, and the best ML practices in Google Cloud.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了本书的第三部分*，其中我们讨论了从结构化数据训练ML模型的Google BQ和BQML，Google ML训练框架如TensorFlow和Keras，Google
    ML训练套件Vertex AI，Google Cloud ML API以及Google Cloud中的最佳ML实践。
- en: In the fourth part of this book, we will prepare for the Google Cloud Certified
    Professional ML Engineer certification by understanding the certification’s requirements
    and deep diving into some of the certification’s practice questions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第四部分，我们将通过了解认证的要求并深入研究一些认证的实践问题来为Google Cloud Certified Professional ML
    Engineer认证做准备。
- en: Further reading
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more about what was covered in this chapter, take a look at the following
    resources:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于本章所涵盖的内容，请查看以下资源：
- en: '[https://www.tensorflow.org/tfx](https://www.tensorflow.org/tfx)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/tfx](https://www.tensorflow.org/tfx)'
- en: '[https://www.kubeflow.org/](https://www.kubeflow.org/)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kubeflow.org/](https://www.kubeflow.org/)'
- en: '[https://cloud.google.com/architecture/ml-on-gcp-best-practices](https://cloud.google.com/architecture/ml-on-gcp-best-practices)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://cloud.google.com/architecture/ml-on-gcp-best-practices](https://cloud.google.com/architecture/ml-on-gcp-best-practices)'
- en: '[https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)'
- en: 'Part 4: Accomplishing GCP ML Certification'
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分：完成GCP ML认证
- en: In this part, we focus on the Google Cloud Professional Machine Learning Engineer
    certification. We introduce the GCP ML certification and Google’s official guides.
    We study the certification exam questions by integrating the knowledge and skills
    learned from the book.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分，我们专注于Google Cloud Professional Machine Learning Engineer认证。我们介绍了GCP ML认证和Google的官方指南。我们通过整合从书中学到的知识和技能来研究认证考试问题。
- en: 'This part comprises the following chapter:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 10*](B18333_10.xhtml#_idTextAnchor179), Achieving the GCP ML Certification'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B18333_10.xhtml#_idTextAnchor179)，实现GCP ML认证'
