- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Google Cloud ML Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the best practices for implementing **Machine
    Learning** (**ML**) in Google Cloud. We will go through an implementation of a
    customer-trained ML model development process in GCP and provide recommendations
    throughout.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: ML environment setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML data storage and processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML model deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML workflow orchestration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML model continuous monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter aims to integrate the knowledge we have learned so far in this
    book and apply it to a customer-trained ML project. We will start by setting up
    the ML environment.
  prefs: []
  type: TYPE_NORMAL
- en: ML environment setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094), *Developing and Deploying
    ML Models*, in the *Preparing the platform* section, we learned about the ML platform
    in the Cloud. Then, in [*Chapter 7*](B18333_07.xhtml#_idTextAnchor143), *Exploring
    Google Cloud Vertex AI*, we introduced the Vertex AI services. For a customer-trained
    model development platform, we recommend **Vertex AI Workbench user-managed notebooks**.
    Let’s look at the details from the prospects of performance, cost, and security.
  prefs: []
  type: TYPE_NORMAL
- en: With Vertex AI Workbench user-managed notebooks, you have the flexibility and
    options to implement **performance excellency**. You can create an instance with
    the existing deep learning VM images that have the latest ML and data science
    libraries preinstalled, along with the latest accelerator drivers. Depending on
    your data, model, and workloads, you can choose the right VM instance type to
    fit your environment and optimize performance, from general-purpose compute (E2,
    N1, N2, and N2D), to memory-optimized (M1 and M2), to compute-optimized (C2),
    and so on. You can also create a notebooks instance based on a custom container
    to tailor your ML environment.
  prefs: []
  type: TYPE_NORMAL
- en: With Vertex AI Workbench user-managed notebooks, you can go with GCP best practices
    and **reduce costs**. As with any Google Cloud services, treat your notebook’s
    VM instances as flexible and disposable resources; when you train ML models on
    the VM instances, make sure that you store all your data in Cloud Storage or BigQuery,
    instead of storing it in the instances' local storage, such as persistent disks,
    so that you can stop or delete the instances when you are done with the ML experimenting
    or training. During the ML model development process, always monitor the VM instances’
    performances and costs, and scale out/in based on the workloads while utilizing
    Google’s Managed instance groups (MIGs).
  prefs: []
  type: TYPE_NORMAL
- en: '**Security** is always an important area we need to address in the Cloud. Some
    of the best security practices are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: With Vertex AI Workbench user-managed notebooks, we recommend that a user-managed
    notebooks instance is created for each member of the data science team. If a team
    member is involved in multiple projects, we recommend using multiple user-managed
    notebooks instances for the member and treating each instance as a virtual workspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating Vertex AI requires collaboration among a variety of teams, and it
    is super important to determine which groups or systems will be responsible for
    which functions. From a networking point of view, you should configure user-managed
    notebooks to use shared VPCs if possible, to minimize access to the notebook instances.
    Restrictive firewall rules must also be enabled to limit access to notebook instances
    and other Vertex AI resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For data and model storing, we also recommend storing the training data and
    training model in the same project for reproducibility. In a Google organization
    with multiple folders and multiple projects, the best practice is leveraging Google
    IAM roles and groups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For data protection, we recommend using Google Cloud organization policies and
    Data Loss Prevention (DLP) tools to protect Personal Identifiable Information
    (PII) data. Data encryption is also recommended for storing data at rest and in
    transit in the Vertex AI notebooks instances. Vertex AI supports **Customer-Managed
    Encryption Keys** (**CMEK**) across most of its components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have understood the environment setup, let’s move to data storage
    and processing.
  prefs: []
  type: TYPE_NORMAL
- en: ML data storage and processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094), *Developing
    and Deploying ML Models*, storing data involves collecting raw data from various
    data sources and storing it in a centralized repository. On the other hand, data
    processing includes both data engineering and feature engineering. Data engineering
    is the process of converting raw data (the data in its source form) into prepared
    data (the dataset in the form that is ready to be input into ML tasks). Feature
    engineering then tunes the prepared data to create the features expected by the
    ML model.
  prefs: []
  type: TYPE_NORMAL
- en: For structured data, we recommend using **Google Cloud BQ** to store and process
    it. For unstructured data, videos, audio, and image data, we recommend using **Google
    Cloud** object storage to store them and **Google Cloud Dataflow** or **Dataproc**
    to process them. As we have discussed, **Dataflow** is a managed service that
    uses the **Apache Beam** programming model to convert unstructured data into binary
    formats and can improve data ingestion performance. Dataproc is a managed **Apache
    Spark** and **Apache Hadoop** service that leverages open source data tools for
    batch processing, querying, streaming, and ML.
  prefs: []
  type: TYPE_NORMAL
- en: For supervised ML, which needs labeled datasets, we recommend using the **Google
    Vertex AI Data Labeling** service, especially for unstructured data. From a security
    point of view, we recommend using **Google Cloud IAM** to manage data access in
    Cloud Storage and BQ, using GCP DLP to manage PII and other sensitive data, and
    using GCP Key Management Services (KMS) for data encryption key management.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been preprocessed, we recommend using a Vertex AI-managed
    dataset to create a link between your data and custom-trained models and provide
    descriptive statistics to split data into training, validation, and testing subsets.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the ML model features, many methods can be utilized in feature
    engineering. We recommend using **Vertex AI Feature Store**, which can be used
    to create new features from the data lakes, schedule data processing, and feature
    engineering jobs, ingest them into Vertex Feature Store for online or batch serving,
    and share the common features within the data science team.
  prefs: []
  type: TYPE_NORMAL
- en: ML model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML model training is a critical phase in ML development, which is why we recommend
    using GCP Vertex AI Training. Instead of manually adjusting hyperparameters with
    numerous training runs for optimal values, we recommend the automated Vertex AI
    training model enhancer to test different hyperparameter configurations, and **Google
    Vertex AI TensorBoard** to track, share, and compare model metrics such as loss
    functions to visualize model graphs. This allows you to compare various experiments
    for parameter tuning and model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Using Vertex AI Workbench user-managed notebooks, you can develop your code
    conveniently and interactively, and we recommend operationalizing your code for
    reproducibility and scalability and running your code in either Vertex training
    or **Vertex AI Pipelines**.
  prefs: []
  type: TYPE_NORMAL
- en: After model training, it is recommended that you use **Vertex Explainable AI**
    to study and gain insights regarding feature contributions and understand your
    model’s behavior. Vertex Explainable AI helps you understand your model’s outputs
    – it tells you how much each feature in the data contributed to the predicted
    result. Then, you can use this information to see whether your model is behaving
    as expected, to recognize bias (if there is any) in your models, and get some
    ideas to improve your model and your training data.
  prefs: []
  type: TYPE_NORMAL
- en: ML model deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML model deployment refers to putting a model into production. Once an ML model
    has been deployed into production, it can be used to predict new data. We recommend
    using the Vertex AI console or API to deploy a trained ML model. With Vertex AI,
    we can serve the model in production with batch prediction; we recommend specifying
    the appropriate hardware for your model and determining how to pass inputs to
    the model. With Vertex AI, we can also serve the model with online endpoint prediction;
    we recommend using Vertex AI Feature Store’s online serving API and turning on
    automatic scaling with a minimum of two nodes.
  prefs: []
  type: TYPE_NORMAL
- en: ML workflow orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 7*](B18333_07.xhtml#_idTextAnchor143), *Exploring
    Google Cloud Vertex AI*, Vertex AI Pipelines is a fully managed service that allows
    you to retrain your models as often as necessary so that you can adapt to changes
    and maintain performance over time. We recommend Vertex AI Pipelines for Cloud
    ML workflow orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using the **Google TensorFlow framework**, we recommend using **TensorFlow
    Extended** to define your pipeline and the operations for each step, then executing
    it on Vertex AI’s serverless pipeline system. TensorFlow provides pre-built components
    for common steps in the Vertex AI workflow, such as data ingestion, data validation,
    and training.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using other frameworks, we recommend using **Kubeflow Pipeline**,
    which is very flexible and allows you to use simple code to construct pipelines.
    Kubeflow Pipeline also provides Google Cloud pipeline components such as Vertex
    AI AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: ML model continuous monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you’ve deployed your model into production, you need to monitor model
    performance continuously to ensure that it is performing as expected. We recommend
    using Vertex AI, which provides two ways to monitor your ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Skew detection**, which looks for the degree of distortion between your model
    training and production data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drift detection**, which looks for drift in your production data. Drift occurs
    when the statistical properties of the inputs and the target change over time
    and cause predictions to become less accurate as time passes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For skew and drift detection, we recommend setting up a model monitoring job
    by providing a pointer to the training data that you used to train your model,
    and then tuning the thresholds that are used for alerting to measure skew or drift
    occurring in your data.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use feature attributions in Vertex Explainable AI to detect data
    drift or skew as an early indicator that model performance may be degrading. For
    example, let’s say that your model originally relied on five features to make
    predictions in training and test data, but when going into production, it began
    to rely on entirely different features.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the best practices for implementing ML in Google
    Cloud, with a focus on custom-trained models based on your data and code.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes *Part 3* of this book, in which we have discussed Google
    BQ and BQML for training ML models from structured data, Google ML training frameworks
    such as TensorFlow and Keras, the Google ML training suite Vertex AI, Google Cloud
    ML APIs, and the best ML practices in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In the fourth part of this book, we will prepare for the Google Cloud Certified
    Professional ML Engineer certification by understanding the certification’s requirements
    and deep diving into some of the certification’s practice questions.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about what was covered in this chapter, take a look at the following
    resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/tfx](https://www.tensorflow.org/tfx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.kubeflow.org/](https://www.kubeflow.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/architecture/ml-on-gcp-best-practices](https://cloud.google.com/architecture/ml-on-gcp-best-practices)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 4: Accomplishing GCP ML Certification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we focus on the Google Cloud Professional Machine Learning Engineer
    certification. We introduce the GCP ML certification and Google’s official guides.
    We study the certification exam questions by integrating the knowledge and skills
    learned from the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18333_10.xhtml#_idTextAnchor179), Achieving the GCP ML Certification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
