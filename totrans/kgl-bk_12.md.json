["```py\nimport os\nimport glob\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport cv2\nfrom skimage.io import imshow, imread, imsave\n# imgaug\nimport imageio\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n# Albumentations\nimport albumentations as A\n# Keras\n# from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n# Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport seaborn as sns\nfrom IPython.display import HTML, Image\n# Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\") \n```", "```py\ndef load_image(image_id):\n    file_path = image_id \n    image = imread(Image_Data_Path + file_path)\n    return image \n```", "```py\ndef gallery(array, ncols=3):\n    nindex, height, width, intensity = array.shape\n    nrows = nindex//ncols\n    assert nindex == nrows*ncols\n    result = (array.reshape(nrows, ncols, height, width, intensity)\n              .swapaxes(1,2)\n              .reshape(height*nrows, width*ncols, intensity))\n    return result \n```", "```py\ndata_dir = '../input/cassava-leaf-disease-classification/'\nImage_Data_Path = data_dir + '/train_images/'\ntrain_data = pd.read_csv(data_dir + '/train.csv')\n# We load and store the first 10 images in memory for faster access\ntrain_images = train_data[\"image_id\"][:10].apply(load_image) \n```", "```py\ncurr_img = train_images[7]\nplt.figure(figsize = (15,15))\nplt.imshow(curr_img)\nplt.axis('off') \n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,\narray_to_img, img_to_array, load_img \ndatagen = ImageDataGenerator( \n        rotation_range = 40, \n        shear_range = 0.2, \n        zoom_range = 0.2, \n        horizontal_flip = True, \n        brightness_range = (0.5, 1.5)) \ncurr_img_array = img_to_array(curr_img)\ncurr_img_array = curr_img_array.reshape((1,) + curr_img_array.shape) \n```", "```py\ni = 0\nfor batch in datagen.flow(\n    curr_img_array,\n    batch_size=1,\n    save_to_dir='.',\n    save_prefix='Augmented_image',\n    save_format='jpeg'):\n    i += 1\n    # Hard-coded stop - without it, the generator enters an infinite loop\n    if i > 9: \n        break \n```", "```py\naug_images = []\nfor img_path in glob.glob(\"*.jpeg\"):\n    aug_images.append(mpimg.imread(img_path))\nplt.figure(figsize=(20,20))\nplt.axis('off')\nplt.imshow(gallery(np.array(aug_images[0:9]), ncols = 3))\nplt.title('Augmentation examples') \n```", "```py\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras import layers \n```", "```py\npretrained_base = tf.keras.models.load_model(\n    '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',\n)\npretrained_base.trainable = False \n```", "```py\nmodel = tf.keras.Sequential([\n    # Preprocessing layers\n    preprocessing.RandomFlip('horizontal'), # Flip left-to-right\n    preprocessing.RandomContrast(0.5), # Contrast change by up to 50%\n    # Base model\n    pretrained_base,\n    # model head definition \n    layers.Flatten(),\n    layers.Dense(6, activation='relu'),\n    layers.Dense(1, activation='sigmoid'),\n]) \n```", "```py\nimport albumentations as A\nhorizontal_flip = A.HorizontalFlip(p=1)\nrotate = A.ShiftScaleRotate(p=1)\ngaus_noise = A.GaussNoise() \nbright_contrast = A.RandomBrightnessContrast(p=1) \ngamma = A.RandomGamma(p=1) \nblur = A.Blur() \n```", "```py\nimg_flip = horizontal_flip(image = curr_img)\nimg_gaus = gaus_noise(image = curr_img)\nimg_rotate = rotate(image = curr_img)\nimg_bc = bright_contrast(image = curr_img)\nimg_gamma = gamma(image = curr_img)\nimg_blur = blur(image = curr_img) \n```", "```py\nimg_list = [img_flip['image'],img_gaus['image'], img_rotate['image'],\n            img_bc['image'], img_gamma['image'], img_blur['image']]\nplt.figure(figsize=(20,20))\nplt.axis('off')\nplt.imshow(gallery(np.array(img_list), ncols = 3))\nplt.title('Augmentation examples') \n```", "```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.optimizers import Adam\nimport os, cv2, json\nfrom PIL import Image \n```", "```py\nclass CFG:    \n    # config\n    WORK_DIR = '../input/cassava-leaf-disease-classification'\n    BATCH_SIZE = 8\n    EPOCHS = 5\n    TARGET_SIZE = 512\ndef create_model():\n    conv_base = EfficientNetB0(include_top = False, weights = None,\n                               input_shape = (CFG.TARGET_SIZE,\n                               CFG.TARGET_SIZE, 3))\n    model = conv_base.output\n    model = layers.GlobalAveragePooling2D()(model)\n    model = layers.Dense(5, activation = \"softmax\")(model)\n    model = models.Model(conv_base.input, model)\n    model.compile(optimizer = Adam(lr = 0.001),\n                  loss = \"sparse_categorical_crossentropy\",\n                  metrics = [\"acc\"])\n    return model \n```", "```py\ndef activation_layer_vis(img, activation_layer = 0, layers = 10):\n    layer_outputs = [layer.output for layer in model.layers[:layers]]\n    activation_model = models.Model(inputs = model.input,\n                                    outputs = layer_outputs)\n    activations = activation_model.predict(img)\n\n    rows = int(activations[activation_layer].shape[3] / 3)\n    cols = int(activations[activation_layer].shape[3] / rows)\n    fig, axes = plt.subplots(rows, cols, figsize = (15, 15 * cols))\n    axes = axes.flatten()\n\n    for i, ax in zip(range(activations[activation_layer].shape[3]), axes):\n        ax.matshow(activations[activation_layer][0, :, :, i],\n                   cmap = 'viridis')\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show() \n```", "```py\ntrain_labels = pd.read_csv(os.path.join(CFG.WORK_DIR, \"train.csv\"))\nSTEPS_PER_EPOCH = len(train_labels)*0.8 / CFG.BATCH_SIZE\nVALIDATION_STEPS = len(train_labels)*0.2 / CFG.BATCH_SIZE \n```", "```py\ntrain_labels.label = train_labels.label.astype('str')\ntrain_datagen = ImageDataGenerator(\n    validation_split = 0.2, preprocessing_function = None,\n        rotation_range = 45, zoom_range = 0.2,\n        horizontal_flip = True, vertical_flip = True,\n        fill_mode = 'nearest', shear_range = 0.1,\n        height_shift_range = 0.1, width_shift_range = 0.1)\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_labels, \n    directory = os.path.join(CFG.WORK_DIR, \"train_images\"),\n    subset = \"training\", \n    x_col = \"image_id\",y_col = \"label\", \n    target_size = (CFG.TARGET_SIZE, CFG.TARGET_SIZE),\n    batch_size = CFG.BATCH_SIZE, \n    class_mode = \"sparse\")\nvalidation_datagen = ImageDataGenerator(validation_split = 0.2)\nvalidation_generator = validation_datagen.flow_from_dataframe(\n        train_labels,\n        directory = os.path.join(CFG.WORK_DIR, \"train_images\"),\n        subset = \"validation\", \n        x_col = \"image_id\",y_col = \"label\", \n        target_size = (CFG.TARGET_SIZE, CFG.TARGET_SIZE),\n        batch_size = CFG.BATCH_SIZE, class_mode = \"sparse\") \n```", "```py\nmodel = create_model()\nmodel.summary() \n```", "```py\nModel: \"functional_1\"\n__________________________________________________________________________\nLayer (type)                  Output Shape         Param # Connected to\n==========================================================================\ninput_1 (InputLayer)          [(None, 512, 512, 3) 0\n__________________________________________________________________________\nrescaling (Rescaling)         (None, 512, 512, 3)  0       input_1[0][0]\n__________________________________________________________________________\nnormalization (Normalization) (None, 512, 512, 3)  7       rescaling[0][0]\n___________________________________________________________________________\nstem_conv_pad (ZeroPadding2D) (None, 513, 513, 3)  0       normalization[0][0]\n___________________________________________________________________________\nstem_conv (Conv2D)              (None, 256, 256, 32) 864    stem_conv_pad[0][0]\n___________________________________________________________________________\nstem_bn (BatchNormalization)    (None, 256, 256, 32) 128    stem_conv[0][0]\n___________________________________________________________________________\nstem_activation (Activation)    (None, 256, 256, 32) 0      stem_bn[0][0]\n___________________________________________________________________________\nblock1a_dwconv (DepthwiseConv2D (None, 256, 256, 32) 288    stem_activation[0][0]\n___________________________________________________________________________\nblock1a_bn (BatchNormalization) (None, 256, 256, 32) 128    block1a_dwconv[0][0]\n___________________________________________________________________________ \n```", "```py\nmodel_save = ModelCheckpoint('./EffNetB0_512_8_best_weights.h5', \n                             save_best_only = True, \n                             save_weights_only = True,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1) \n```", "```py\nearly_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001,\n                           patience = 5, mode = 'min',\n                           verbose = 1, restore_best_weights = True) \n```", "```py\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, \n                              patience = 2, min_delta = 0.001, \n                              mode = 'min', verbose = 1) \n```", "```py\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    epochs = CFG.EPOCHS,\n    validation_data = validation_generator,\n    validation_steps = VALIDATION_STEPS,\n    callbacks = [model_save, early_stop, reduce_lr]\n) \n```", "```py\nEpoch 00001: val_loss improved from inf to 0.57514, saving model to ./EffNetB0_512_8_best_weights.h5 \n```", "```py\nactivation_layer_vis(img_tensor, 0) \n```", "```py\nss = pd.read_csv(os.path.join(CFG.WORK_DIR, \"sample_submission.csv\"))\npreds = []\nfor image_id in ss.image_id:\n    image = Image.open(os.path.join(CFG.WORK_DIR,  \"test_images\",\n                                    image_id))\n    image = image.resize((CFG.TARGET_SIZE, CFG.TARGET_SIZE))\n    image = np.expand_dims(image, axis = 0)\n    preds.append(np.argmax(model.predict(image)))\nss['label'] = preds \n```", "```py\ndf = pd.read_csv('../input/global-wheat-detection/train.csv')\ndf.head(3) \n```", "```py\nbboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1],\n                                  sep=',')))\nbboxs \n```", "```py\narray([[834., 222.,  56.,  36.],\n       [226., 548., 130.,  58.],\n       [377., 504.,  74., 160.],\n       ...,\n       [134., 228., 141.,  71.],\n       [430.,  13., 184.,  79.],\n       [875., 740.,  94.,  61.]]) \n```", "```py\nfor i, column in enumerate(['x', 'y', 'w', 'h']):\n    df[column] = bboxs[:,i]\ndf.drop(columns=['bbox'], inplace=True)\ndf['x_center'] = df['x'] + df['w']/2\ndf['y_center'] = df['y'] + df['h']/2\ndf['classes'] = 0\ndf = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]\ndf.head(3) \n```", "```py\n# stratify on source\nsource = 'train'\n# Pick a single fold for demonstration's sake\nfold = 0 \nval_index = set(df[df['fold'] == fold]['image_id'])\n# Loop through the bounding boxes per image\nfor name,mini in tqdm(df.groupby('image_id')):\n    # Where to save the files\n    if name in val_index:\n        path2save = 'valid/'\n    else:\n        path2save = 'train/'   \n    # Storage path for labels\n    if not os.path.exists('convertor/fold{}/labels/'.\n                          format(fold)+path2save):\n        os.makedirs('convertor/fold{}/labels/'.format(fold)+path2save)\n    with open('convertor/fold{}/labels/'.format(fold)+path2save+name+\".\n              txt\", 'w+') as f:\n   # Normalize the coordinates in accordance with the Yolo format requirements\n        row = mini[['classes','x_center','y_center','w','h']].\n        astype(float).values\n        row = row/1024\n        row = row.astype(str)\n        for j in range(len(row)):\n            text = ' '.join(row[j])\n            f.write(text)\n            f.write(\"\\n\")\n    if not os.path.exists('convertor/fold{}/images/{}'.\n                          format(fold,path2save)):\n        os.makedirs('convertor/fold{}/images/{}'.format(fold,path2save))\n    # No preprocessing needed for images => copy them as a batch\n    sh.copy(\"../input/global-wheat-detection/{}/{}.jpg\".\n            format(source,name),\n            'convertor/fold{}/images/{}/{}.jpg'.\n            format(fold,path2save,name)) \n```", "```py\n!git clone https://github.com/ultralytics/yolov5  && cd yolov5 &&\npip install -r requirements.txt \n```", "```py\nyaml_text = \"\"\"train: /kaggle/working/convertor/fold0/images/train/\n            val: /kaggle/working/convertor/fold0/images/valid/\n            nc: 1\n            names: ['wheat']\"\"\"\nwith open(\"wheat.yaml\", 'w') as f:\n    f.write(yaml_text)\n%cat wheat.yaml \n```", "```py\n!python ./yolov5/train.py --img 512 --batch 2 --epochs 3 --workers 2 --data wheat.yaml --cfg \"./yolov5/models/yolov5s.yaml\" --name yolov5x_fold0 --cache \n```", "```py\nDownloading the pretrained weights, setting up Weights&Biases https://wandb.ai/site integration, GitHub sanity check.\nDownloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\nwandb: (1) Create a W&B account\nwandb: (2) Use an existing W&B account\nwandb: (3) Don't visualize my results\nwandb: Enter your choice: (30 second timeout) \nwandb: W&B disabled due to login timeout.\ntrain: weights=yolov5/yolov5s.pt, cfg=./yolov5/models/yolov5s.yaml, data=wheat.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=2, imgsz=512, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=2, project=yolov5/runs/train, name=yolov5x_fold0, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\ngithub: up to date with https://github.com/ultralytics/yolov5 ![](img/B17574_10_002.png)\nYOLOv5 ![](img/B17574_10_003.png) v6.1-76-gc94736a torch 1.9.1 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\nWeights & Biases: run 'pip install wandb' to automatically track and visualize YOLOv5 ![](img/B17574_10_003.png) runs (RECOMMENDED)\nTensorBoard: Start with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5/yolov5s.pt...\n100%|██████████████████████████████████████| 14.1M/14.1M [00:00<00:00, 40.7MB/s] \n```", "```py\nOverriding model.yaml nc=80 with nc=1\n                 from  n    params  module                                  arguments\n  0                -1  1    3520  models.common.Conv                      [3, 32, 6, 2, 2]\n  1                -1  1    18560  models.common.Conv                      [32, 64, 3, 2]\n  2                -1  1    18816  models.common.C3                        [64, 64, 1]\n  3                -1  1    73984  models.common.Conv                      [64, 128, 3, 2]\n  4                -1  2    115712  models.common.C3                        [128, 128, 2]\n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]\n  6                -1  3    625152  models.common.C3                        [256, 256, 3]\n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]\n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]\n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]\n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]\n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']\n 12           [-1, 6]  1         0  models.common.Concat                    [1]\n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]\n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]\n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']\n 16           [-1, 4]  1         0  models.common.Concat                    [1]\n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]\n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]\n 19          [-1, 14]  1         0  models.common.Concat                    [1]\n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]\n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]\n 22          [-1, 10]  1         0  models.common.Concat                    [1]\n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]\n 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\nYOLOv5s summary: 270 layers, 7022326 parameters, 7022326 gradients, 15.8 GFLOPs\nTransferred 342/349 items from yolov5/yolov5s.pt\nScaled weight_decay = 0.0005\noptimizer: SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\nalbumentations: Blur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\ntrain: Scanning '/kaggle/working/convertor/fold0/labels/train' images and labels\ntrain: New cache created: /kaggle/working/convertor/fold0/labels/train.cache\ntrain: Caching images (0.0GB ram): 100%|██████████| 51/51 [00:00<00:00, 76.00it/\nval: Scanning '/kaggle/working/convertor/fold0/labels/valid' images and labels..\nval: New cache created: /kaggle/working/convertor/fold0/labels/valid.cache\nval: Caching images (2.6GB ram): 100%|██████████| 3322/3322 [00:47<00:00, 70.51i\nPlotting labels to yolov5/runs/train/yolov5x_fold0/labels.jpg... \nAutoAnchor: 6.00 anchors/target, 0.997 Best Possible Recall (BPR). Current anchors are a good fit to dataset ![](img/B17574_10_002.png)\nImage sizes 512 train, 512 val\nUsing 2 dataloader workers \n```", "```py\nStarting training for 3 epochs...\n     Epoch   gpu_mem       box       obj       cls    labels  img_size\n       0/2    0.371G    0.1196   0.05478         0        14       512: 100%|███\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@\n                 all       3322     147409    0.00774     0.0523    0.00437   0.000952\n     Epoch   gpu_mem       box       obj       cls    labels  img_size\n       1/2    0.474G    0.1176   0.05625         0         5       512: 100%|███\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@\n                 all       3322     147409    0.00914     0.0618    0.00493    0.00108\n     Epoch   gpu_mem       box       obj       cls    labels  img_size\n       2/2    0.474G    0.1146   0.06308         0        12       512: 100%|███\n               Class     Images     Labels          P          R     mAP@.5 mAP@\n                 all       3322     147409    0.00997     0.0674    0.00558    0.00123\n3 epochs completed in 0.073 hours.\nOptimizer stripped from yolov5/runs/train/yolov5x_fold0/weights/last.pt, 14.4MB\nOptimizer stripped from yolov5/runs/train/yolov5x_fold0/weights/best.pt, 14.4MB\nValidating yolov5/runs/train/yolov5x_fold0/weights/best.pt...\nFusing layers... \nYOLOv5s summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 0.120s exceeded\n               Class     Images     Labels          P          R     mAP@.5 mAP@\n                 all       3322     147409    0.00997     0.0673    0.00556    0.00122\nResults saved to yolov5/runs/train/yolov5x_fold0 \n```", "```py\n!python ./yolov5/detect.py --weights ./yolov5/runs/train/yolov5x_fold0/weights/best.pt --img 512 --conf 0.1 --source /kaggle/input/global-wheat-detection/test --save-txt --save-conf --exist-ok \n```", "```py\n!ls ./yolov5/runs/detect/exp/labels/ \n```", "```py\n2fd875eaa.txt  53f253011.txt  aac893a91.txt  f5a1f0358.txt\n348a992bb.txt  796707dd7.txt  cc3532ff6.txt \n```", "```py\n!cat 2fd875eaa.txt \n```", "```py\n0 0.527832 0.580566 0.202148 0.838867 0.101574\n0 0.894531 0.587891 0.210938 0.316406 0.113519 \n```", "```py\ndef convert(s):\n    x = int(1024 * (s[1] - s[3]/2))\n    y = int(1024 * (s[2] - s[4]/2))\n    w = int(1024 * s[3])\n    h = int(1024 * s[4])\n\n    return(str(s[5]) + ' ' + str(x) + ' ' + str(y) + ' ' + str(w)\n           + ' ' + str(h)) \n```", "```py\nwith open('submission.csv', 'w') as myfile:\n    # Prepare submission\n    wfolder = './yolov5/runs/detect/exp/labels/'\n    for f in os.listdir(wfolder):\n        fname = wfolder + f\n        xdat = pd.read_csv(fname, sep = ' ', header = None)\n        outline = f[:-4] + ' ' + ' '.join(list(xdat.apply(lambda s:\n                                     convert(s), axis = 1)))\n        myfile.write(outline + '\\n')\n\nmyfile.close() \n```", "```py\n!cat submission.csv \n```", "```py\n53f253011 0.100472 61 669 961 57 0.106223 0 125 234 183 0.1082 96 696 928 126 0.108863 515 393 86 161 0.11459 31 0 167 209 0.120246 517 466 89 147\naac893a91 0.108037 376 435 325 188\n796707dd7 0.235373 684 128 234 113\ncc3532ff6 0.100443 406 752 144 108 0.102479 405 87 4 89 0.107173 576 537 138 94 0.113459 256 498 179 211 0.114847 836 618 186 65 0.121121 154 544 248 115 0.125105 40 567 483 199\n2fd875eaa 0.101398 439 163 204 860 0.112546 807 440 216 323\n348a992bb 0.100572 0 10 440 298 0.101236 344 445 401 211\nf5a1f0358 0.102549 398 424 295 96 \n```", "```py\n!pip install pycocotools\n!pip install 'git+https://github.com/facebookresearch/detectron2.git' \n```", "```py\n# from pycocotools.coco import COCO\nimport skimage.io as io\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport json,itertools\nfrom sklearn.model_selection import GroupKFold\n# Config\nclass CFG:\n    data_path = '../input/sartorius-cell-instance-segmentation/'\n    nfolds = 5 \n```", "```py\n# From https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formatted (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int)\n                       for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction \n```", "```py\n# From https://newbedev.com/encode-numpy-array-using-uncompressed-rle-for-\n# coco-dataset\ndef binary_mask_to_rle(binary_mask):\n    rle = {'counts': [], 'size': list(binary_mask.shape)}\n    counts = rle.get('counts')\n    for i, (value, elements) in enumerate(\n            itertools.groupby(binary_mask.ravel(order='F'))):\n        if i == 0 and value == 1:\n            counts.append(0)\n        counts.append(len(list(elements)))\n    return rle \n```", "```py\ndef coco_structure(train_df):\n    cat_ids = {name: id+1 for id, name in enumerate(\n        train_df.cell_type.unique())}\n    cats = [{'name': name, 'id': id} for name, id in cat_ids.items()]\n    images = [{'id': id, 'width': row.width, 'height': row.height,\n               'file_name':f'train/{id}.png'} for id,\n               row in train_df.groupby('id').agg('first').iterrows()]\n    annotations = []\n    for idx, row in tqdm(train_df.iterrows()):\n        mk = rle_decode(row.annotation, (row.height, row.width))\n        ys, xs = np.where(mk)\n        x1, x2 = min(xs), max(xs)\n        y1, y2 = min(ys), max(ys)\n        enc =binary_mask_to_rle(mk)\n        seg = {\n            'segmentation':enc, \n            'bbox': [int(x1), int(y1), int(x2-x1+1), int(y2-y1+1)],\n            'area': int(np.sum(mk)),\n            'image_id':row.id, \n            'category_id':cat_ids[row.cell_type], \n            'iscrowd':0, \n            'id':idx\n        }\n        annotations.append(seg)\n    return {'categories':cats, 'images':images,'annotations':annotations} \n```", "```py\ntrain_df = pd.read_csv(CFG.data_path + 'train.csv')\ngkf = GroupKFold(n_splits = CFG.nfolds)\ntrain_df[\"fold\"] = -1\ny = train_df.width.values\nfor f, (t_, v_) in enumerate(gkf.split(X=train_df, y=y,\n                             groups=train_df.id.values)):\n    train_df.loc[v_, \"fold\"] = f\n\nfold_id = train_df.fold.copy() \n```", "```py\nall_ids = train_df.id.unique()\n# For fold in range(CFG.nfolds):\nfor fold in range(4,5):    \n    train_sample = train_df.loc[fold_id != fold]\n    root = coco_structure(train_sample)\n    with open('annotations_train_f' + str(fold) + \n              '.json', 'w', encoding='utf-8') as f:\n        json.dump(root, f, ensure_ascii=True, indent=4)\n\n    valid_sample = train_df.loc[fold_id == fold]\n    print('fold ' + str(fold) + ': produced')\nfor fold in range(4,5):    \n    train_sample = train_df.loc[fold_id == fold]\n    root = coco_structure(train_sample)\n    with open('annotations_valid_f' + str(fold) + \n              '.json', 'w', encoding='utf-8') as f:\n        json.dump(root, f, ensure_ascii=True, indent=4)\n\n    valid_sample = train_df.loc[fold_id == fold]\n    print('fold ' + str(fold) + ': produced') \n```", "```py\nfrom datetime import datetime\nimport os\nimport pandas as pd\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport detectron2\nfrom pathlib import Path\nimport random, cv2, os\nimport matplotlib.pyplot as plt\n# Import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor, DefaultTrainer\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer, ColorMode\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.utils.logger import setup_logger\nfrom detectron2.evaluation.evaluator import DatasetEvaluator\nfrom detectron2.engine import BestCheckpointer\nfrom detectron2.checkpoint import DetectionCheckpointer\nsetup_logger()\nimport torch \n```", "```py\nclass CFG:\n    wfold = 4\n    data_folder = '../input/sartorius-cell-instance-segmentation/'\n    anno_folder = '../input/sartoriusannotations/'\n    model_arch = 'mask_rcnn_R_50_FPN_3x.yaml'\n    nof_iters = 10000 \n    seed = 45 \n```", "```py\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(CFG.seed) \n```", "```py\n# Taken from https://www.kaggle.com/theoviel/competition-metric-map-iou\ndef precision_at(threshold, iou):\n    matches = iou > threshold\n    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n    return np.sum(true_positives), np.sum(false_positives),\n    np.sum(false_negatives)\ndef score(pred, targ):\n    pred_masks = pred['instances'].pred_masks.cpu().numpy()\n    enc_preds = [mask_util.encode(np.asarray(p, order='F'))\n                 for p in pred_masks]\n    enc_targs = list(map(lambda x:x['segmentation'], targ))\n    ious = mask_util.iou(enc_preds, enc_targs, [0]*len(enc_targs))\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, ious)\n        p = tp / (tp + fp + fn)\n        prec.append(p)\n    return np.mean(prec) \n```", "```py\nclass MAPIOUEvaluator(DatasetEvaluator):\n    def __init__(self, dataset_name):\n        dataset_dicts = DatasetCatalog.get(dataset_name)\n        self.annotations_cache = {item['image_id']:item['annotations']\n                                  for item in dataset_dicts}\n\n    def reset(self):\n        self.scores = []\n    def process(self, inputs, outputs):\n        for inp, out in zip(inputs, outputs):\n            if len(out['instances']) == 0:\n                self.scores.append(0)    \n            else:\n                targ = self.annotations_cache[inp['image_id']]\n                self.scores.append(score(out, targ))\n    def evaluate(self):\n        return {\"MaP IoU\": np.mean(self.scores)} \n```", "```py\nclass Trainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        return MAPIOUEvaluator(dataset_name)\n    def build_hooks(self):\n        # copy of cfg\n        cfg = self.cfg.clone()\n        # build the original model hooks\n        hooks = super().build_hooks()\n        # add the best checkpointer hook\n        hooks.insert(-1, BestCheckpointer(cfg.TEST.EVAL_PERIOD, \n                                         DetectionCheckpointer(self.model,\n                                         cfg.OUTPUT_DIR),\n                                         \"MaP IoU\",\n                                         \"max\",\n                                         ))\n        return hooks \n```", "```py\ndataDir=Path(CFG.data_folder)\nregister_coco_instances('sartorius_train',{}, CFG.anno_folder + \n                        'annotations_train_f' + str(CFG.wfold) + \n                        '.json', dataDir)\nregister_coco_instances('sartorius_val',{}, CFG.anno_folder + \n                        'annotations_valid_f' + str(CFG.wfold) + \n                        '.json', dataDir)\nmetadata = MetadataCatalog.get('sartorius_train')\ntrain_ds = DatasetCatalog.get('sartorius_train') \n```", "```py\ncfg = get_cfg()\ncfg.INPUT.MASK_FORMAT='bitmask'\ncfg.merge_from_file(model_zoo.get_config_file('COCO-InstanceSegmentation/' +\n                    CFG.model_arch))\ncfg.DATASETS.TRAIN = (\"sartorius_train\",)\ncfg.DATASETS.TEST = (\"sartorius_val\",)\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url('COCO-InstanceSegmentation/'\n                    + CFG.model_arch)\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.001\ncfg.SOLVER.MAX_ITER = CFG.nof_iters\ncfg.SOLVER.STEPS = []\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  \ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .4\ncfg.TEST.EVAL_PERIOD = len(DatasetCatalog.get('sartorius_train')) \n                           // cfg.SOLVER.IMS_PER_BATCH \n```", "```py\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = Trainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train() \n```", "```py\nTHRESHOLDS = [.18, .35, .58]\nMIN_PIXELS = [75, 150, 75] \n```", "```py\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formatted\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs) \n```", "```py\ndef get_masks(fn, predictor):\n    im = cv2.imread(str(fn))\n    pred = predictor(im)\n    pred_class = torch.mode(pred['instances'].pred_classes)[0]\n    take = pred['instances'].scores >= THRESHOLDS[pred_class]\n    pred_masks = pred['instances'].pred_masks[take]\n    pred_masks = pred_masks.cpu().numpy()\n    res = []\n    used = np.zeros(im.shape[:2], dtype=int) \n    for mask in pred_masks:\n        mask = mask * (1-used)\n        # Skip predictions with small area\n        if mask.sum() >= MIN_PIXELS[pred_class]:\n            used += mask\n            res.append(rle_encode(mask))\n    return res \n```", "```py\ndataDir=Path(CFG.data_folder)\nids, masks=[],[]\ntest_names = (dataDir/'test').ls() \n```", "```py\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/\"+\n                    CFG.arch+\".yaml\"))\ncfg.INPUT.MASK_FORMAT = 'bitmask'\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 3 \ncfg.MODEL.WEIGHTS = CFG.model_folder + 'model_best_f' + \n                    str(CFG.wfold)+'.pth' \ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\ncfg.TEST.DETECTIONS_PER_IMAGE = 1000\npredictor = DefaultPredictor(cfg) \n```", "```py\nencoded_masks = get_masks(test_names[0], predictor)\n_, axs = plt.subplots(1,2, figsize = (40, 15))\naxs[1].imshow(cv2.imread(str(test_names[0])))\nfor enc in encoded_masks:\n    dec = rle_decode(enc)\naxs[0].imshow(np.ma.masked_where(dec == 0, dec)) \n```", "```py\nfor fn in test_names:\n    encoded_masks = get_masks(fn, predictor)\n    for enc in encoded_masks:\n        ids.append(fn.stem)\n        masks.append(enc)\npd.DataFrame({'id':ids, 'predicted':masks}).to_csv('submission.csv', \n                                                   index=False)\npd.read_csv('submission.csv').head() \n```"]