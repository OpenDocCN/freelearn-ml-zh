["```py\nusing NetworkType = loss_mean_squared<fc<\n    1,\n    htan<fc<\n        8,\n        htan<fc<16,\n                htan<fc<32, input<matrix<double>>>>>>>>>>;\n```", "```py\n    input<matrix<double>\n    ```", "```py\n    fc<32, input<matrix<double>>\n    ```", "```py\n    htan<fc<32, input<matrix<double>>>\n    ```", "```py\n    htan<fc<16, htan<fc<32, input<matrix<double>>>>>>\n    ```", "```py\n    htan<fc<8, htan<fc<16, htan<fc<32,\n        input<matrix<double>>>>>>>>\n    ```", "```py\n    fc<1, htan<fc<8, htan<fc<16, htan<fc<32,\n        input<matrix<double>>>>>>>>>\n    ```", "```py\n    loss_mean_squared<...>\n    ```", "```py\nsize_t n = 10000;\n...\nstd::vector<matrix<double>> x(n);\nstd::vector<float> y(n);\n...\nusing NetworkType = loss_mean_squared<\nfc < 1, htan < fc < 8, htan < fc < 16, htan < fc < 32,\n    input < matrix < double >>>>>>>>>>;\nNetworkType network;\nfloat weight_decay = 0.0001f;\nfloat momentum = 0.5f;\nsgd solver(weight_decay, momentum);\ndnn_trainer<NetworkType> trainer(network, solver);\ntrainer.set_learning_rate(0.01);\ntrainer.set_learning_rate_shrink_factor(1);  // disable learning rate \n                                            //changes\ntrainer.set_mini_batch_size(64);\ntrainer.set_max_num_epochs(500);\ntrainer.be_verbose();\ntrainer.train(x, y);\nnetwork.clean();\nauto predictions = network(new_x);\n```", "```py\nMeanSquaredError loss;\nConstInitialization init(0.);\nFFN<MeanSquaredError, ConstInitialization> model( loss, init);\n```", "```py\nmodel.Add<Linear>(8);\nmodel.Add<ReLU>();\n...\n```", "```py\nsize_t epochs = 100;\nens::MomentumSGD optimizer(\n    /*stepSize=*/0.01,\n    /*batchSize=*/ 64,\n    /*maxIterations=*/ epochs * x.n_cols,\n    /*tolerance=*/1e-10,\n    /*shuffle=*/false);\n```", "```py\noptimizer.UpdatePolicy().Momentum() = 0.5;\n```", "```py\nmodel.Train(x, y, optimizer);\n```", "```py\nmodel.Train(x, y, optimizer, ens::ProgressBar());\n```", "```py\nens::StoreBestCoordinates<arma::mat> best_params;\nmodel.Train(scaled_x,\n            scaled_y,\n            optimizer,\n            ens::ProgressBar(),\n            ens::EarlyStopAtMinLoss(20),\n            best_params);\n```", "```py\nMeanSquaredError loss;\nConstInitialization init(0.);\nFFN<MeanSquaredError, ConstInitialization> model( loss, init);\nmodel.Add<Linear>(8);\nmodel.Add<ReLU>();\nmodel.Add<Linear>(16);\nmodel.Add<ReLU>();\nmodel.Add<Linear>(32);\nmodel.Add<ReLU>();\nmodel.Add<Linear>(1);\n// Define optimizer\nsize_t epochs = 100;\nens::MomentumSGD optimizer(\n    /*stepSize=*/0.01,\n    /*batchSize= */ 64,\n    /*maxIterations= */ epochs * x.n_cols,\n    /*tolerance=*/1e-10,\n    /*shuffle=*/false);\nens::StoreBestCoordinates<arma::mat> best_params;\nmodel.Train(x, y, optimizer, ens::ProgressBar(),\n            ens::EarlyStopAtMinLoss(20), best_params);\n```", "```py\narma::mat predictions;\nmodel.Predict(x, predictions);\n```", "```py\nmodel.Parameters() = best_params.BestCoordinates();\n```", "```py\nfl::Sequential model;\nmodel.add(fl::View({1, 1, 1, -1}));\nmodel.add(fl::Linear(1, 8));\nmodel.add(fl::ReLU());\n```", "```py\nauto loss = fl::MeanSquaredError();\n```", "```py\nfloat learning_rate = 0.01;\nfloat momentum = 0.5;\nauto sgd = fl::SGDOptimizer(model.params(), learning_rate, momentum);\n```", "```py\nconst int epochs = 500;\nfor (int epoch_i = 0; epoch_i < epochs; ++epoch_i) {\n  for (auto& batch : batch_dataset) {\n    // Forward propagation\n    auto predicted = model(fl::input(batch[0]));\n    // Calculate loss\n    auto local_batch_size = batch[0].shape().dim(0);\n    auto target =\n        fl::reshape(batch[1], {1, 1, 1, local_batch_size});\n    auto loss_value =\n        loss(predicted,\n             fl::noGrad(target));  // Backward propagation\n    loss_value.backward();\n    // Optimization - updating parameters\n    sgd.step();\n    // clearing graients\n    sgd.zeroGrad();\n  }\n}\n```", "```py\nauto predicted = model(fl::noGrad(x));\n```", "```py\n#include <torch/torch.h>\n#include <opencv2/opencv.hpp>\n#include <string>\nclass MNISTDataset\n    : public torch::data::Dataset<MNISTDataset> {\n public:\n  MNISTDataset(const std::string& images_file_name,\n               const std::string& labels_file_name);\n  // torch::data::Dataset implementation\n  torch::data::Example<> get(size_t index) override;\n  torch::optional<size_t> size() const override;\n private:\n  void ReadLabels(const std::string& labels_file_name);\n  void ReadImages(const std::string& images_file_name);\n  uint32_t rows_ = 0;\n  uint32_t columns_ = 0;\n  std::vector<unsigned char> labels_;\n  std::vector<cv::Mat> images_;\n}\n```", "```py\nMNISTDataset::MNISTDataset(\n    const std::string& images_file_name,\n    const std::string& labels_file_name) {\n  ReadLabels(labels_file_name);\n  ReadImages(images_file_name);\n}\n```", "```py\ntorch::optional<size_t> MNISTDataset::size() const {\n    return labels_.size();\n}\n```", "```py\ntorch::data::Example<> MNISTDataset::get(size_t index) {\n  return {\n      CvImageToTensor(images_[index]),\n      torch::tensor(static_cast<int64_t>(labels_[index]),\n                    torch::TensorOptions()\n                        .dtype(torch::kLong)\n                        .device(torch::DeviceType::CUDA))};\n}\n```", "```py\ntorch::Tensor CvImageToTensor(const cv::Mat& image) {\n  assert(image.channels() == 1);\n  std::vector<int64_t> dims{\n      static_cast<int64_t>(1),\n      static_cast<int64_t>(image.rows),\n      static_cast<int64_t>(image.cols)};\n  torch::Tensor tensor_image =\n      torch::from_blob(image.data, torch::IntArrayRef(dims),\n                       // clone is required to copy data\n                       // from temporary object\n                       torch::TensorOptions()\n                           .dtype(torch::kFloat)\n                           .requires_grad(false))\n          .clone();\n  return tensor_image.to(torch::DeviceType::CUDA);\n}\n```", "```py\nvoid MNISTDataset::ReadLabels(\n    const std::string& labels_file_name) {\n  std::ifstream labels_file(\n      labels_file_name,\n      std::ios::binary | std::ios::binary);\n  labels_file.exceptions(std::ifstream::failbit |\n                         std::ifstream::badbit);\n  if (labels_file) {\n    uint32_t magic_num = 0;\n    uint32_t num_items = 0;\n    if (read_header(&magic_num, labels_file) &&\n        read_header(&num_items, labels_file)) {\n      labels_.resize(static_cast<size_t>(num_items));\n      labels_file.read(\n          reinterpret_cast<char*>(labels_.data()),\n          num_items);\n    }\n  }\n}\n```", "```py\ntemplate <class T>\nbool read_header(T* out, std::istream& stream) {\n  auto size = static_cast<std::streamsize>(sizeof(T));\n  T value;\n  if (!stream.read(reinterpret_cast<char*>(&value), size)) {\n    return false;\n  } else {\n    // flip endianness\n    *out = (value << 24) | ((value << 8) & 0x00FF0000) |\n           ((value >> 8) & 0X0000FF00) | (value >> 24);\n    return true;\n  }\n}\n```", "```py\nvoid MNISTDataset::ReadImages(\n    const std::string& images_file_name) {\n  std::ifstream images_file(\n      images_file_name,\n      std::ios::binary | std::ios::binary);\n  labels_file.exceptions(std::ifstream::failbit |\n                         std::ifstream::badbit);\n  if (labels_file) {\n    uint32_t magic_num = 0;\n    uint32_t num_items = 0;\n    rows_ = 0;\n    columns_ = 0;\n    if (read_header(&magic_num, labels_file) &&\n        read_header(&num_items, labels_file) &&\n        read_header(&rows_, labels_file) &&\n        read_header(&columns_, labels_file)) {\n      assert(num_items == labels_.size());\n      images_.resize(num_items);\n      cv::Mat img(static_cast<int>(rows_),\n                  static_cast<int>(columns_), CV_8UC1);\n      for (uint32_t i = 0; i < num_items; ++i) {\n        images_file.read(reinterpret_cast<char*>(img.data),\n                         static_cast<std::streamsize>(\n                             img.size().area()));\n        img.convertTo(images_[i], CV_32F);\n        images_[i] /= 255;  // normalize\n        cv::resize(images_[i], images_[i],\n                   cv::Size(32, 32));  // Resize to\n        // 32x32 size\n      }\n    }\n  }\n}\n```", "```py\n#include <torch/torch.h>\nclass LeNet5Impl : public torch::nn::Module {\n    public:\n        LeNet5Impl();\n        torch::Tensor forward(torch::Tensor x);\n    private:\n        torch::nn::Sequential conv_;\n        torch::nn::Sequential full_;\n};\nTORCH_MODULE(LeNet5);\n```", "```py\nclass Name : public torch::nn::ModuleHolder<Impl> {}\n```", "```py\nstatic std::vector<int64_t> k_size = {2, 2};\nstatic std::vector<int64_t> p_size = {0, 0};\nLeNet5Impl::LeNet5Impl() {\n  conv_ = torch::nn::Sequential(\n      torch::nn::Conv2d(torch::nn::Conv2dOptions(1, 6, 5)),\n      torch::nn::Functional(torch::tanh),\n      torch::nn::Functional(\n          torch::avg_pool2d,\n          /*kernel_size*/ \n          /*kernel_size*/ torch::IntArrayRef(k_size),\n          /*stride*/ torch::IntArrayRef(k_size),\n          /*padding*/ torch::IntArrayRef(p_size),\n          /*ceil_mode*/ false,\n          /*count_include_pad*/ false),\n      torch::nn::Conv2d(torch::nn::Conv2dOptions(6, 16, 5)),\n      torch::nn::Functional(torch::tanh),\n      torch::nn::Functional(\n          torch::avg_pool2d,\n          /*kernel_size*/ torch::IntArrayRef(k_size),\n          /*stride*/ torch::IntArrayRef(k_size),\n          /*padding*/ torch::IntArrayRef(p_size),\n          /*ceil_mode*/ false,\n          /*count_include_pad*/ false),\n      torch::nn::Conv2d(\n          torch::nn::Conv2dOptions(16, 120, 5)),\n      torch::nn::Functional(torch::tanh));\n  register_module(\"conv\", conv_);\n  full_ = torch::nn::Sequential(\n      torch::nn::Linear(torch::nn::LinearOptions(120, 84)),\n      torch::nn::Functional(torch::tanh),\n      torch::nn::Linear(torch::nn::LinearOptions(84, 10)));\n  register_module(\"full\", full_);\n}\n```", "```py\ntorch::Tensor LeNet5Impl::forward(at::Tensor x) {\n    auto output = conv_->forward(x);\n    output = output.view({x.size(0), -1});\n    output = full_->forward(output);\n    output = torch::log_softmax(output, -1);\n    return output;\n}\n```", "```py\nauto train_images = root_path / \"train-images-idx3-ubyte\";\nauto train_labels = root_path / \"train-labels-idx1-ubyte\";\nauto test_images = root_path / \"t10k-images-idx3-ubyte\";\nauto test_labels = root_path / \"t10k-labels-idx1-ubyte\";\n// initialize train dataset\n// ----------------------------------------------\nMNISTDataset train_dataset(train_images.native(),\n                           train_labels.native());\nauto train_loader = torch::data::make_data_loader(\n    train_dataset.map(torch::data::transforms::Stack<>()),\n    torch::data::DataLoaderOptions()\n        .batch_size(256)\n        .workers(8));\n// initialize test dataset\n// ----------------------------------------------\nMNISTDataset test_dataset(test_images.native(),\n                          test_labels.native());\nauto test_loader = torch::data::make_data_loader(\n    test_dataset.map(torch::data::transforms::Stack<>()),\n    torch::data::DataLoaderOptions()\n        .batch_size(1024)\n        .workers(8));\n```", "```py\nLeNet5 model;\nmodel->to(torch::DeviceType::CUDA);\n```", "```py\ndouble learning_rate = 0.01;\ndouble weight_decay = 0.0001; // regularization parameter\ntorch::optim::SGD optimizer(\n    model->parameters(),\n    torch::optim::SGDOptions(learning_rate)\n        .weight_decay(weight_decay)\n        .momentum(0.5));\n```", "```py\nint epochs = 100;\nfor (int epoch = 0; epoch < epochs; ++epoch) {\n  model->train();  // switch to the training mode\n  // Iterate the data loader to get batches from the dataset\n  int batch_index = 0;\n  for (auto& batch : (*train_loader)) {\n    // Clear gradients\n    optimizer.zero_grad();\n    // Execute the model on the input data\n    torch::Tensor prediction = model->forward(batch.data);\n    // Compute a loss value to estimate error of our model\n    // target should have size of [batch_size]\n    torch::Tensor loss =\n        torch::nll_loss(prediction, batch.target);\n    // Compute gradients of the loss and parameters of our\n    // model\n    loss.backward();\n    // Update the parameters based on the calculated\n    // gradients.\n    optimizer.step();\n    // Output the loss every 10 batches.\n    if (++batch_index % 10 == 0) {\n      std::cout << \"Epoch: \" << epoch\n                << \" | Batch: \" << batch_index\n                << \" | Loss: \" << loss.item<float>()\n                << std::endl;\n    }\n  }\n```", "```py\nfor (auto& batch : (*train_loader)){\n...\n}\n```", "```py\nmodel->eval(); // switch to the training mode\nunsigned long total_correct = 0;\nfloat avg_loss = 0.0;\nfor (auto& batch : (*test_loader)) {\n  // Execute the model on the input data\n  torch::Tensor prediction = model->forward(batch.data);\n  // Compute a loss value to estimate error of our model\n  torch::Tensor loss =\n      torch::nll_loss(prediction, batch.target);\n  avg_loss += loss.sum().item<float>();\n  auto pred = std::get<1>(prediction.detach_().max(1));\n  total_correct += static_cast<unsigned long>(\n      pred.eq(batch.target.view_as(pred))\n          .sum()\n          .item<long>());\n}\navg_loss /= test_dataset.size().value();\ndouble accuracy = (static_cast<double>(total_correct) /\n                   test_dataset.size().value());\nstd::cout << \"Test Avg. Loss: \" << avg_loss\n          << \" |\n    Accuracy : \" << accuracy << std::endl;\n```", "```py\nauto pred = std::get<1>(prediction.detach_().max(1));\n```", "```py\ntotal_correct += static_cast<unsigned long>(\n    pred.eq(batch.target.view_as(pred)).sum().item<long>());\n```"]