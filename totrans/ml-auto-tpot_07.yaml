- en: '*Chapter 5*: Parallel Training with TPOT and Dask'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*：使用 TPOT 和 Dask 进行并行训练'
- en: In this chapter, you'll dive into a bit of a more advanced topic; that is, automated
    machine learning. You'll learn how to handle machine learning tasks in a parallel
    manner by distributing the work on a Dask cluster. This chapter will be more theoretical
    than the previous two, but you will still learn many useful things.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将深入探讨一个更高级的话题；那就是自动化机器学习。你将学习如何通过在 Dask 集群上分配工作来以并行方式处理机器学习任务。本章将比前两章更具理论性，但你仍然会学到许多有用的东西。
- en: We'll cover essential topics and ideas behind parallelism in Python, and you'll
    learn how to achieve parallelism in a couple of different ways. Then, we'll dive
    deep into the Dask library, explore its basic functionality, and see how you can
    tie it with TPOT.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖 Python 并行背后的基本主题和思想，你将学习以几种不同的方式实现并行。然后，我们将深入探讨 Dask 库，探索其基本功能，并了解如何将其与
    TPOT 结合使用。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to parallelism in Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 并行编程简介
- en: Introduction to the Dask library
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 库简介
- en: Training machine learning models with TPOT and Dask
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TPOT 和 Dask 训练机器学习模型
- en: Let's get started!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: No prior exposure to Dask or even parallel programming is required for you to
    read and understand this chapter. Previous experience is helpful, as fitting this
    big of a concept into a few pages is close to impossible. You should still be
    able to follow and fully understand everything written here as all of the concepts
    will be explained.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读和理解这一章之前，你不需要有任何关于 Dask 或甚至并行编程的经验。虽然将如此大的概念压缩到几页纸上几乎是不可能的，但你应该仍然能够跟上并完全理解这里所写的一切，因为所有概念都将得到解释。
- en: 'You can download the source code and dataset for this chapter here: [https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter05](https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter05).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此处下载本章的源代码和数据集：[https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter05](https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter05)。
- en: Introduction to parallelism in Python
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 并行编程简介
- en: Executing tasks sequentially (where the second one starts after the first one
    finishes) is required in some situations. For example, maybe the input of the
    second function relies on the output of the first one. If that's the case, these
    two functions (processes) can't be executed at the same time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，需要按顺序执行任务（即第二个任务在第一个任务完成后开始）。例如，第二个函数的输入可能依赖于第一个函数的输出。如果是这样，这两个函数（进程）不能同时执行。
- en: But more often than not, that's not the case. Just imagine your program is connecting
    to three different API endpoints before the dashboard is displayed. The first
    API returns the current weather conditions, the second one returns the stock prices,
    and the last one returns today's exchange rates. There's no point in making the
    API calls one after the other. They don't rely on each other, so running them
    sequentially would be a huge waste of time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但大多数情况下并非如此。想象一下，在仪表盘显示之前，你的程序正在连接到三个不同的 API 端点。第一个 API 返回当前的天气状况，第二个返回股票价格，最后一个返回今天的汇率。一个接一个地调用
    API 没有意义。它们之间没有依赖关系，所以按顺序运行它们会浪费大量时间。
- en: Not only that, but it would also be a waste of CPU cores. Most modern PCs have
    at least four CPU cores. If you're running things sequentially, you're only using
    a single core. Why not use all of them if you can?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，这还会浪费 CPU 核心。大多数现代电脑至少有四个 CPU 核心。如果你是按顺序运行任务的，你只在使用一个核心。为什么不能使用所有这些核心呢？
- en: One of the ways to achieve parallelism in Python is with multiprocessing. It
    is a process-based parallelism technique. As you would imagine, Python has a `multiprocessing`
    library built into it, and this section will teach you how to use it. With Python
    3.2 and beyond, this library stopped being the recommended way of implementing
    multiprocessing in your apps. There's a new kid on the block, and its name is
    `concurrent.futures`. It's yet another built-in library you'll learn how to use
    in this section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中实现并行的一种方式是使用多进程。这是一种基于进程的并行技术。正如你所想象的那样，Python 内置了一个 `multiprocessing`
    库，本节将教你如何使用它。从 Python 3.2 及更高版本开始，这个库不再被推荐用于在应用程序中实现多进程。有一个新的库出现了，它的名字叫 `concurrent.futures`。这又是另一个你将在本节中学习如何使用的内置库。
- en: The simplest way to explain and understand multiprocessing is with Python's
    built-in `time` library. You can use it to track time differences and to pause
    program execution intentionally, among other things. This is just what we need
    because we can put in many print statements with some time gaps between them,
    and then see how the program acts when it's run sequentially and how it acts when
    it's run in parallel.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 解释和理解多进程的最简单方式是使用 Python 的内置 `time` 库。你可以用它来跟踪时间差异，以及故意暂停程序执行等。这正是我们所需要的，因为我们可以在它们之间插入一些时间间隔的打印语句，然后观察程序在顺序执行和并行执行时的行为。
- en: You will learn how multiprocessing works in Python through a couple of hands-on
    examples.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你将通过几个动手实例了解 Python 中多进程的工作方式。
- en: 'For starters, please take a look at the following code snippet. In it, the
    `sleep_func()` function has been declared. Its task is to print a message, pause
    the program executing for 1 second, and then to print another message as the function
    completes. We can monitor the time this function takes to run for an arbitrary
    number of times (let''s say five) and then print out the execution time duration.
    The snippet is as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请查看以下代码片段。在这个片段中，已经声明了 `sleep_func()` 函数。它的任务是打印一条消息，暂停程序执行 1 秒，然后在函数完成时打印另一条消息。我们可以监控这个函数运行任意次数（比如说五次）并打印出执行时间。代码片段如下：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The corresponding output is shown here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的输出如下所示：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So, what happened here? Nothing unexpected, to say the least. The `sleep_func()`
    function executed sequentially five times. The execution time is approximately
    5 seconds. You could also simplify the preceding snippet in the following manner:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这里发生了什么？至少说，没有什么意外。`sleep_func()` 函数按顺序执行了五次。执行时间大约为 5 秒。你也可以以下面的方式简化前面的代码片段：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result is identical, as you would expect:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与你预期的一样：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Is there a problem with this approach? Well, yes. We're wasting both time and
    CPU cores. The functions aren't dependent in any way, so why don't we run them
    in parallel? As we mentioned previously, there are two ways of doing this. Let's
    examine the older way first, through the `multiprocessing` library.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有什么问题吗？嗯，是的。我们浪费了时间和 CPU 核心。这些函数在某种程度上并不依赖，那么我们为什么不在并行中运行它们呢？正如我们之前提到的，有两种方法可以做到这一点。让我们首先检查通过
    `multiprocessing` 库的老方法。
- en: It's a bit of a lengthy approach because it requires declaring a process, starting
    it, and joining it. It's not so tedious if you have only a few, but what if there
    are tens of processes in your program? It can become tedious fast.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有点繁琐，因为它需要声明一个进程、启动它并加入它。如果你只有几个进程，这并不那么麻烦，但如果你程序中有成百上千个进程呢？这很快就会变得繁琐。
- en: 'The following code snippet demonstrates how to run the `sleep_func()` function
    three times in parallel:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段演示了如何并行运行 `sleep_func()` 函数三次：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is shown here:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, each of the three processes was launched independently and in
    parallel, so they all managed to finish in a single second.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，三个进程都是独立且并行启动的，因此它们都成功地在同一秒内完成了任务。
- en: Both `Process()` and `start()` are self-explanatory, but what is the `join()`
    function doing? Simply put, it tells Python to wait until the process is complete.
    If you call `join()` on all of the processes, the last two code lines won't execute
    until all of the processes are finished. For fun, try to remove the `join()` calls;
    you'll immediately get the gist.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`Process()` 和 `start()` 都是显而易见的，但 `join()` 函数在做什么呢？简单来说，它告诉 Python 等待进程完成。如果你在所有进程中调用
    `join()`，则最后两行代码不会执行，直到所有进程都完成。为了好玩，尝试移除 `join()` 调用；你会立即明白其含义。'
- en: You now have a basic intuition behind multiprocessing, but the story doesn't
    end here. Python 3.2 introduced a new, improved way of executing tasks in parallel.
    The `concurrent.futures` library is the best one available as of yet, and you'll
    learn how to use it next.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在对多进程有了基本的直觉，但故事还没有结束。Python 3.2 引入了一种新的、改进的并行执行任务的方法。`concurrent.futures`
    库是目前最好的库，你将学习如何使用它。
- en: 'With it, you don''t have to manage processes manually. Every executed function
    will return something, which is `None` in the case of our `sleep_func()` function.
    You can change it by returning the last statement instead of printing it. Furthermore,
    this new approach uses `ProcessPoolExecutor()` to run. You don''t need to know
    anything about it; just remember that it is used to execute multiple processes
    at the same time. Codewise, simply put everything you want to run in parallel
    inside. This approach unlocks two new functions:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用它，你不需要手动管理进程。每个执行过的函数都会返回一些东西，在我们的`sleep_func()`函数中是`None`。你可以通过返回最后一个语句而不是打印它来改变它。此外，这种新的方法使用`ProcessPoolExecutor()`来运行。你不需要了解任何关于它的东西；只需记住，它用于同时执行多个进程。从代码的角度来看，简单地将你想要并行运行的所有内容放在里面。这种方法解锁了两个新的函数：
- en: '`submit()`: Used to run the function in parallel. The returned results will
    be appended to a list so that we can print them (or do anything else) with the
    next function.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`submit()`: 用于并行运行函数。返回的结果将被追加到列表中，这样我们就可以在下一个函数中打印它们（或做任何其他事情）。'
- en: '`result()`: Used to obtain the returned value from the function. We''ll simply
    print the result, but you''re free to do anything else.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`result()`: 用于从函数中获取返回值。我们只需简单地打印结果，但你也可以做任何其他事情。'
- en: 'To recap, we''ll append the results to a list, and then print them out as the
    functions finish executing. The following snippet shows you how to implement multiprocessing
    with the most recent Python approach:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们将结果追加到列表中，然后在函数执行完毕后打印它们。以下代码片段展示了如何使用最新的Python方法实现多进程：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The results are shown here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, the program behaves similarly to what we had previously, with
    a few added benefits – you don't have to manage processes on your own, and the
    syntax is much cleaner.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，程序的行为与我们之前所做的一样，增加了一些好处——你不需要自己管理进程，而且语法更加简洁。
- en: The one issue we have so far is the lack of function parameters. Currently,
    we're just calling a function that doesn't accept any parameters. That won't be
    the case most of the time, so it's important to learn how to handle function parameters
    as early as possible.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前遇到的一个问题是缺少函数参数。目前，我们只是调用一个不接受任何参数的函数。这种情况在大多数时候都不会发生，因此尽早学习如何处理函数参数是很重要的。
- en: We'll introduce a single parameter to our `sleep_func()` function that allows
    us to specify how long the execution will be paused. The print statements inside
    the function are updated accordingly. The sleep times are defined within the `sleep_seconds`
    list, and the value is passed to `append()` at each iteration as a second parameter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向`sleep_func()`函数引入一个参数，允许我们指定执行将被暂停多长时间。函数内的打印语句相应地更新。暂停时间在`sleep_seconds`列表中定义，并在每次迭代中将该值作为第二个参数传递给`append()`。
- en: 'The entire snippet is shown here:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 整个代码片段如下所示：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The results are shown here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That's how you can handle function parameters in parallel processing. Keep in
    mind that the executing time won't be exactly the same on every machine, as the
    runtime duration will depend on your hardware. As a general rule, you should definitely
    see a speed improvement compared to a non-parallelized version of the script.
    You now know the basics of parallel processing. In the next section, you'll learn
    where Python's Dask library comes into the picture, and in the section afterward,
    you'll combine parallel programming, Dask, and TPOT in order to build machine
    learning models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这样你就可以在并行处理中处理函数参数了。请记住，每个机器上的执行时间可能不会完全相同，因为运行时间将取决于你的硬件。一般来说，你应该看到与脚本的非并行版本相比的速度提升。你现在已经了解了并行处理的基础。在下一节中，你将了解Python的Dask库是如何进入画面的，在接下来的章节中，你将结合并行编程、Dask和TPOT来构建机器学习模型。
- en: Introduction to the Dask library
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask库简介
- en: You can think of Dask as one of the most revolutionary Python libraries for
    data processing at scale. If you are a regular pandas and NumPy user, you'll love
    Dask. The library allows you to work with data NumPy and pandas doesn't allow
    because they don't fit into the RAM.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将Dask视为数据规模处理中最革命的Python库之一。如果你是常规的pandas和NumPy用户，你会喜欢Dask。这个库允许你处理NumPy和pandas不允许的数据，因为它们不适合RAM。
- en: Dask supports both NumPy array and pandas DataFrame data structures, so you'll
    quickly get up to speed with it. It can run either on your computer or a cluster,
    making it that much easier to scale. You only need to write the code once and
    then choose the environment that you'll run it in. It's that simple.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 支持 NumPy 数组和 pandas DataFrame 数据结构，所以你将很快熟悉它。它可以在你的电脑或集群上运行，这使得扩展变得更加容易。你只需要编写一次代码，然后选择你将运行它的环境。就这么简单。
- en: One other thing to note is that Dask allows you to run code in parallel with
    minimal changes. As you saw earlier, processing things in parallel means the execution
    time decreases, which is generally the behavior we want. Later, you'll learn how
    parallelism in Dask works with `dask.delayed`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另有一点需要注意，Dask 允许你通过最小的改动并行运行代码。正如你之前看到的，并行处理事物意味着执行时间减少，这是我们通常希望的行为。稍后，你将学习
    Dask 中的并行性是如何通过 `dask.delayed` 实现的。
- en: 'To get started, you''ll have to install the library. Make sure the correct
    environment is activated. Then, execute the following from the Terminal:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，你必须安装这个库。确保正确的环境被激活。然后，从终端执行以下命令：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There are other installation options. For example, you could install only the
    arrays or DataFrames module, but it's a good idea to install everything from the
    start. Don't forget to put quotes around the library name, as not doing so will
    result in an error.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有其他安装选项。例如，你可以只安装数组或 DataFrames 模块，但最好从一开始就安装所有内容。不要忘记在库名称周围加上引号，否则会导致错误。
- en: If you've installed everything, you'll have access to three Dask collections
    – arrays, DataFrames, and bags. All of these can store datasets that are larger
    than your RAM size, and they can all partition data between RAM and a hard drive.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经安装了所有东西，你将能够访问三个 Dask 集合——数组、DataFrames 和 bags。所有这些都可以存储比你的 RAM 大的数据集，并且它们都可以在
    RAM 和硬盘之间分区数据。
- en: "Let's start with Dask arrays and compare them with a NumPy alternative. You\
    \ can create \La NumPy array of ones with 1,000x1,000x1,000 dimensions by executing\
    \ the following code cell in a Notebook environment. The `%%time` magic command\
    \ is used to measure the time needed for the cell to finish with the execution:"
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 Dask 数组开始，并与 NumPy 的替代品进行比较。你可以在 Notebook 环境中执行以下代码单元，创建一个具有 1,000x1,000x1,000
    维度的 NumPy 单位数组。`%%time` 魔法命令用于测量单元格执行完成所需的时间：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Constructing larger arrays than this one results in a memory error on my machine,
    but this will do just fine for the comparisons. The corresponding output is shown
    here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 构建比这个更大的数组会导致我的机器出现内存错误，但这对比较来说已经足够了。相应的输出如下所示：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you can see, it took 4.35 seconds to create this array. Now, let''s do the
    same with Dask:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，创建这个数组花费了 4.35 秒。现在，让我们用 Dask 来做同样的事情：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As you can see, the only change is in the library import name. The executing
    time results will probably come as a surprise if this is your first encounter
    with the Dask library. They are shown here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，唯一的变化在于库导入名称。如果你第一次遇到 Dask 库，执行时间的结果可能会让你感到惊讶。它们在这里展示：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Yes, you are reading this right. Dask took 696 microseconds to create an array
    of identical dimensions, which is 6,250 times faster. Sure, you shouldn't expect
    this drastic reduction in execution time in the real world, but the differences
    should still be quite significant.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你确实在读这段话。Dask 创建了一个具有相同维度的数组，耗时 696 微秒，这比之前快了 6,250 倍。当然，你不应该期望在现实世界中执行时间会有这么大的减少，但差异仍然应该相当显著。
- en: Next, let's take a look at Dask DataFrames. The syntax should, once again, feel
    very similar, so it shouldn't take you much time to learn the library. To fully
    demonstrate Dask's capabilities, we'll create some large datasets that won't be
    able to fit in the memory of a single laptop. To be more precise, we'll create
    10 CSV files that are time series-based, each presenting data for a single year
    aggregated by seconds and measured through five different features. That's a lot,
    and it will definitely take some time to create, but you should end up with 10
    datasets where each is around 1 GB in size. If you have a laptop with 8 GB of
    RAM like me, there's no way you could fit it in memory.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看 Dask DataFrames。语法应该再次感觉非常相似，所以你不需要花太多时间来学习这个库。为了完全展示 Dask 的功能，我们将创建一些大型数据集，这些数据集将无法适应单个笔记本电脑的内存。更准确地说，我们将创建
    10 个基于时间序列的 CSV 文件，每个文件代表一年的数据，按秒聚合并通过五个不同的特征进行测量。这有很多，创建它们肯定需要一些时间，但最终你应该会有 10
    个数据集，每个数据集大约有 1 GB 的大小。如果你像我一样有一个 8 GB RAM 的笔记本电脑，你根本无法将其放入内存中。
- en: 'The following code snippet creates these datasets:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段创建了这些数据集：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Just make sure to have this `/data` folder where your Notebook is and you''ll
    be good to go. Also, make sure you have 10 GB of disk space if you''re following
    along. The last line, `!ls data/`, lists all the files located in the `data` folder.
    Here''s what you should see:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 只需确保你的笔记本位于这个`/data`文件夹中，你就可以顺利开始了。另外，如果你要跟进度，请确保你有10 GB的磁盘空间。最后一行，`!ls data/`，列出了`data`文件夹中所有文件。你应该看到以下内容：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let''s take a look at how much time it takes pandas to read in a single
    CSV file and perform a simple aggregation. To be more precise, the dataset is
    grouped by month and the sum is extracted. The following code snippet demonstrates
    how to do this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看pandas读取单个CSV文件并执行简单聚合操作需要多少时间。更精确地说，数据集按月份分组，并提取总和。以下代码片段演示了如何进行此操作：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The results are shown here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在这里显示：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, it took pandas 42 seconds to perform this computation. Not too
    shabby, but what if you absolutely need to load in all of the datasets and perform
    computations? Let's explore that next.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，pandas执行这个计算需要42秒。这并不算太糟糕，但如果你绝对需要加载所有数据集并执行计算呢？让我们接下来探索这个问题。
- en: 'You can use the `glob` library to get paths to desired files in a specified
    folder. You can then read all of them individually, and use the `concat()` function
    from pandas to stack them together. The aggregation is performed in the same way:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`glob`库来获取指定文件夹中所需文件的路径。然后你可以单独读取它们，并使用pandas的`concat()`函数将它们堆叠在一起。聚合操作以相同的方式进行：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There isn't much to say here – the Notebook simply breaks. Storing 10 GB+ of
    data into RAM isn't feasible for an 8 GB RAM machine. One way you could get around
    this would be to load data in chunks, but that's a headache of its own.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有太多可说的——笔记本只是简单地崩溃了。将10 GB+的数据存储到8 GB RAM的机器中是不切实际的。你可以通过分块加载数据来解决这个问题，但这又是一个头疼的问题。
- en: 'What can Dask do to help? Let''s learn how to load in these CSVs with Dask
    and perform the same aggregation. You can use the following snippet to do so:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Dask能做些什么来帮助呢？让我们学习如何使用Dask加载这些CSV文件并执行相同的聚合操作。你可以使用以下代码片段来完成：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The results will once again surprise you:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将再次让你感到惊讶：
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: That's correct – in less than 4 minutes, Dask managed to read over 10 GB of
    data to an 8 GB RAM machine. That alone should make you reconsider NumPy and pandas,
    especially if you're dealing with large amounts of data or you expect to deal
    with it in the near future.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是正确的——在不到4分钟内，Dask成功地将超过10 GB的数据读取到8 GB RAM的机器上。仅此一点就足以让你重新考虑NumPy和pandas，尤其是如果你正在处理大量数据或者你预计在不久的将来会处理数据。
- en: Finally, there are Dask bags. They are used for storing and processing general
    Python data types that can't fit into memory – for example, log data. We won't
    explore this data structure, but it's nice to know it exists.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有Dask的bags。它们用于存储和处理无法放入内存的通用Python数据类型——例如，日志数据。我们不会探索这个数据结构，但了解它的存在是很好的。
- en: On the other hand, we will explore the concept of parallel processing with Dask.
    You learned in the previous section that there are no valid reasons to process
    data or perform any other operation sequentially, as the input of one doesn't
    rely on the output of another.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们将使用Dask来探索并行处理的概念。在上一节中，你已经了解到没有有效的理由要按顺序处理数据或执行任何其他操作，因为一个操作的输入并不依赖于另一个操作的输出。
- en: '*Dask delayed* allows for parallel execution. Sure, you can still rely only
    on the multiprocessing concepts we learned earlier, but why? It can be a tedious
    approach, and Dask has something better to offer. With Dask, there''s no need
    to change the programming syntax, as was the case with pure Python. You just need
    to annotate a function you want to be parallelized with the `@dask.delayed` decorator
    and you''re good to go!'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dask延迟*允许并行执行。当然，你仍然可以依赖我们之前学到的多进程概念，但为什么还要这样做呢？这可能是一个繁琐的方法，而Dask有更好的解决方案。使用Dask，你不需要改变编程语法，就像纯Python那样。你只需要使用`@dask.delayed`装饰器来注释你想并行化的函数，然后就可以开始了！'
- en: You can parallelize multiple functions and then place them inside a computational
    graph. That's what we'll do next.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将多个函数并行化，然后将它们放入计算图中。这正是我们接下来要做的。
- en: 'The following code snippet declares two functions:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段声明了两个函数：
- en: '`cube()`: Returns a cube of a number'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cube()`: 返回一个数字的立方'
- en: '`multiply()`: Multiplies all numbers in a list and returns the product'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multiply()`: 将列表中的所有数字相乘并返回乘积'
- en: 'Here are the library imports you''ll need:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你需要的库导入：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let''s run the first function on five numbers and call the second function
    on the results to see what happens. Note the call to `time.sleep()` inside the
    `cube()` function. This will make spotting differences between parallelized and
    non-parallelized functions that much easier:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在五个数字上运行第一个函数，并在结果上调用第二个函数，看看会发生什么。注意`cube()`函数内部的`time.sleep()`调用。这将使我们在并行化和非并行化函数之间发现差异变得容易得多：
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This is your regular (sequential) data processing. There''s nothing wrong with
    it, especially when there are so few and simple operations. The corresponding
    output is shown here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您的常规（顺序）数据处理。这没有什么问题，尤其是在操作如此少且简单的情况下。相应的输出如下：
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As expected, the code cell took around 5 seconds to run because of sequential
    execution. Now, let''s see the modifications you have to make to parallelize these
    functions:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，由于顺序执行，代码单元运行了大约5秒钟。现在，让我们看看您需要对这些函数进行哪些修改以实现并行化：
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'So, there''s only the `@delayed` decorator and a call to `compute()` on the
    graph. The results are displayed here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，只需要`@delayed`装饰器和在图上调用`compute()`。结果如下所示：
- en: '[PRE26]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As expected, the whole thing took just over a second to run because of the
    parallel execution. The previously declared computational graph comes with one
    more handy feature – it''s easy to visualize. You''ll need to have *GraphViz*
    installed on your machine and as a Python library. The procedure is different
    for every OS, so we won''t go through it here. A quick Google search will tell
    you how to install it. Once you''re done, you can execute the following line of
    code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，由于并行执行，整个过程仅用了不到一秒钟。之前声明的计算图还有一个方便的特性——它很容易可视化。您需要在您的机器上安装*GraphViz*，并将其作为Python库。对于每个操作系统，安装过程都不同，所以我们在这里不详细说明。快速Google搜索会告诉您如何安装它。一旦完成安装，您可以执行以下代码行：
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The corresponding visualization is shown here:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的可视化显示如下：
- en: '![Figure 5.1 – Visualization of a Dask computational graph'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.1 – Dask计算图的可视化'
- en: '](img/B16954_05_001.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16954_05_001.jpg](img/B16954_05_001.jpg)'
- en: Figure 5.1 – Visualization of a Dask computational graph
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – Dask计算图的可视化
- en: As you can see from the graph, the `cube()` function is called five times in
    parallel, and its results are stored in the buckets above it. Then, the `multiply()`
    function is called with these values and stores the product in the top bucket.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从图中所见，`cube()`函数被并行调用五次，其结果存储在上面的桶中。然后，使用这些值调用`multiply()`函数，并将乘积存储在顶部的桶中。
- en: That's all you need to know about the basics of Dask. You've learned how to
    work with Dask arrays and DataFrames, and also how to use Dask to process operations
    in parallel. Not only that, but you've also learned the crucial role Dask plays
    in modern-day data science and machine learning. Dataset sizes often exceed available
    memory, so modern solutions are required.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Dask的基本知识，您需要了解的就是这些。您已经学会了如何使用Dask数组和数据框，以及如何使用Dask并行处理操作。不仅如此，您还了解了Dask在现代数据科学和机器学习中的关键作用。数据集的大小通常超过可用内存，因此需要现代解决方案。
- en: In the following section, you'll learn how to train TPOT automated machine learning
    models with Dask.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习如何使用Dask训练TPOT自动化机器学习模型。
- en: Training machine learning models with TPOT and Dask
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TPOT和Dask训练机器学习模型
- en: Optimizing machine learning pipelines is, before everything, a time-consuming
    process. We can shorten it potentially significantly by running things in parallel.
    Dask and TPOT work great when combined, and this section will teach you how to
    train TPOT models on a Dask cluster. Don't let the word "cluster" scare you, as
    your laptop or PC will be enough.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 优化机器学习管道是首要的任务，这是一个耗时的过程。我们可以通过并行运行来显著缩短它。当与TPOT结合使用时，Dask和TPOT工作得很好，本节将教会您如何在Dask集群上训练TPOT模型。不要让“集群”这个词吓到您，因为您的笔记本电脑或PC就足够了。
- en: 'You''ll have to install one more library to continue, and it is called `dask-ml`.
    As its name suggests, it''s used to perform machine learning with Dask. Execute
    the following from the Terminal to install it:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装一个额外的库才能继续，它被称为`dask-ml`。正如其名所示，它用于使用Dask进行机器学习。从终端执行以下命令来安装它：
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once that''s done, you can open up Jupyter Lab or your favorite Python code
    editor and start coding. Let''s get started:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，您可以打开Jupyter Lab或您喜欢的Python代码编辑器并开始编码。让我们开始：
- en: Let's start with library imports. We'll also make a dataset decision here. This
    time, we won't spend any time on data cleaning, preparation, or examination. The
    goal is to have a dataset ready as soon as possible. The `load_digits()` function
    from scikit-learn comes in handy because it is designed to fetch many 8x8 pixel
    digit images for classification.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从库导入开始。我们还会在这里做出数据集的决定。这次，我们不会在数据清洗、准备或检查上花费任何时间。目标是尽快准备好数据集。scikit-learn
    中的 `load_digits()` 函数很有用，因为它旨在获取许多 8x8 像素的数字图像以进行分类。
- en: 'As some of the libraries often fill up your screen with unnecessary warnings,
    we''ll use the `warnings` library to ignore them. Refer to the following snippet
    for all the library imports:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于一些库经常用不必要的警告填满你的屏幕，我们将使用 `warnings` 库来忽略它们。有关所有库导入的参考，请参阅以下代码片段：
- en: '[PRE29]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The only new thing here is the `Client` class from `dask.distributed`. It is
    used to establish a connection with the Dask cluster (your computer, in this case).
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里唯一的新事物是来自 `dask.distributed` 的 `Client` 类。它用于与 Dask 集群（在这种情况下是你的计算机）建立连接。
- en: 'You''ll now make an instance of the client. This will immediately start the
    Dask cluster and use all the CPU cores you have available. Here''s the code for
    instance creation and checking where the cluster runs:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在将创建一个客户端实例。这将立即启动 Dask 集群并使用你所有可用的 CPU 核心。以下是创建实例和检查集群运行位置的代码：
- en: '[PRE30]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once executed, you should see the following output:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行后，你应该看到以下输出：
- en: '![Figure 5.2 – Information on the Dask cluster'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 5.2 – Dask 集群信息]'
- en: '](img/B16954_05_002.jpg)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B16954_05_002.jpg]'
- en: Figure 5.2 – Information on the Dask cluster
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 5.2 – Dask 集群信息]'
- en: 'You can click on the dashboard link, and it will take you to http://127.0.0.1:8787/status.
    The following screenshot shows what the dashboard should look like when it''s
    opened for the first time (no tasks running):'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以点击仪表板链接，它将带你到 http://127.0.0.1:8787/status。以下截图显示了仪表板首次打开时的样子（没有运行任务）：
- en: '![Figure 5.3 – Dask cluster dashboard (no running tasks)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 5.3 – Dask 集群仪表板（无运行任务）]'
- en: '](img/B16954_05_003.jpg)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B16954_05_003.jpg]'
- en: Figure 5.3 – Dask cluster dashboard (no running tasks)
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.3 – Dask 集群仪表板（无运行任务）
- en: The dashboard will become much more colorful once you start training the models.
    We'll do the necessary preparation next.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦开始训练模型，仪表板将变得更加多彩。我们将在下一步进行必要的准备。
- en: You can call the `load_digits()` function to get the image data and then use
    the `train_test_split()` function to split the images into subsets for training
    and testing. The train/test ratio is 50:50 for this example, as we don't want
    to spend too much time on the training. The ratio should be higher for the training
    set in almost any scenario, so make sure to remember that.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以调用 `load_digits()` 函数来获取图像数据，然后使用 `train_test_split()` 函数将图像分割成训练和测试子集。在这个例子中，训练/测试比例是
    50:50，因为我们不想在训练上花费太多时间。在几乎任何情况下，训练集的比例都应该更高，所以请确保记住这一点。
- en: 'Once the split is done, you can call `.shape` on the subsets to check their
    dimensionality. Here''s the entire code snippet:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分割完成后，你可以对子集调用 `.shape` 来检查它们的维度。以下是整个代码片段：
- en: '[PRE31]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The corresponding output is shown in the following figure:'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相应的输出在以下图中显示：
- en: '![Figure 5.4 – Dimensionality of training and testing subsets'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 5.4 – 训练和测试子集的维度]'
- en: '](img/B16954_05_004.jpg)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B16954_05_004.jpg]'
- en: Figure 5.4 – Dimensionality of training and testing subsets
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.4 – 训练和测试子集的维度
- en: Next stop – model training.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下一个目标 – 模型训练。
- en: 'You now have everything needed to train models with TPOT and Dask. You can
    do so in a very similar fashion to what you did previously. The key parameter
    here is `use_dask`. You should set it to `True` if you want to use Dask for training.
    The other parameters are well known:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在拥有使用 TPOT 和 Dask 训练模型所需的一切。你可以以与之前非常相似的方式这样做。这里的关键参数是 `use_dask`。如果你想使用 Dask
    进行训练，你应该将其设置为 `True`。其他参数都是众所周知的：
- en: '[PRE32]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, you''re ready to call the `fit()` function and train the model on the
    training subset. Here''s a line of code for doing so:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，你已准备好调用 `fit()` 函数并在训练子集上训练模型。以下是执行此操作的代码行：
- en: '[PRE33]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The appearance of the Dask dashboard will change immediately after you start
    training the model. Here''s what it will look like a couple of minutes into the
    process:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦开始训练模型，Dask 仪表板的界面将立即改变。以下是过程进行几分钟后的样子：
- en: '![Figure 5.5 – Dask dashboard during training'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.5 – 训练期间的 Dask 仪表板]'
- en: '](img/B16954_05_005.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16954_05_005.jpg]'
- en: Figure 5.5 – Dask dashboard during training
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 训练期间的 Dask 仪表板
- en: 'After 10 minutes, TPOT will finish optimizing the pipeline, and you''ll see
    the following output in your Notebook:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 10 分钟后，TPOT 将完成管道优化，你将在你的笔记本中看到以下输出：
- en: '![Figure 5.6 – TPOT optimization outputs'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.6 – TPOT 优化输出'
- en: '](img/B16954_05_006.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16954_05_006.jpg)'
- en: Figure 5.6 – TPOT optimization outputs
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – TPOT 优化输出
- en: And that's all you need to do to combine TPOT and Dask.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 TPOT 和 Dask 结合起来，你需要做的是所有这些。
- en: You now know how to train models on a Dask cluster, which is the recommended
    way of doing things for larger datasets and more challenging problems.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道如何在 Dask 集群上训练模型，这是处理更大数据集和更具挑战性问题的推荐方法。
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter was packed with information not only about TPOT and training models
    in a parallel manner, but also about parallelism in general. You've learned a
    lot – from how to parallelize basic functions that do nothing but sleep for a
    while, to parallelizing function with parameters and Dask fundamentals, to training
    machine learning models with TPOT and Dask on a Dask cluster.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不仅包含了关于 TPOT 和以并行方式训练模型的信息，还包含了关于并行性的通用信息。你已经学到了很多——从如何并行化只做了一段时间休眠的基本函数，到并行化带有参数的函数和
    Dask 基础知识，再到在 Dask 集群上使用 TPOT 和 Dask 训练机器学习模型。
- en: By now, you know how to solve regression and classification tasks in an automated
    manner, and how to parallelize the training process. The following chapter, [*Chapter
    6*](B16954_06_Final_SK_ePub.xhtml#_idTextAnchor073)*, Getting Started with Deep
    Learning – Crash Course in Neural Networks*, will provide you with the required
    knowledge on neural networks. It will form a basis for [*Chapter 7*](B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086)*,
    Neural Network Classifier with TPOT*, where we'll dive deep into training automated
    machine learning models with state-of-the-art neural network algorithms.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你已经知道了如何以自动化的方式解决回归和分类任务，以及如何并行化训练过程。接下来的章节，[*第 6 章*](B16954_06_Final_SK_ePub.xhtml#_idTextAnchor073)*，*深度学习入门
    – 神经网络速成课*，将为你提供关于神经网络所需的知识。这将构成[*第 7 章*](B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086)*，使用
    TPOT 的神经网络分类器*的基础，我们将深入探讨使用最先进的神经网络算法训练自动化机器学习模型。
- en: As always, please feel free to practice solving both regression and classification
    tasks with TPOT, but this time, try to parallelize the process with Dask.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，请随意练习使用 TPOT 解决回归和分类任务，但这次，尝试使用 Dask 并行化这个过程。
- en: Q&A
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问答
- en: Define the term "parallelism."
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义“并行性”这个术语。
- en: Explain which types of tasks can and can't be parallelized.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释哪些类型的任务可以并行化，哪些不可以。
- en: List and explain three options for implementing parallelism in your applications
    (all are listed in this chapter).
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出并解释在应用程序中实现并行化的三种选项（所有这些都在本章中列出）。
- en: What is Dask and what makes it superior to NumPy and pandas for larger datasets?
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dask 是什么？它为什么比 NumPy 和 pandas 在处理更大数据集时更优越？
- en: Name and explain three basic data structures that are implemented in Dask.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 名称并解释在 Dask 中实现的三种基本数据结构。
- en: What is a Dask cluster?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dask 集群是什么？
- en: What do you have to do to tell TPOT it should use Dask for training?
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要做什么来告诉 TPOT 它应该使用 Dask 进行训练？
