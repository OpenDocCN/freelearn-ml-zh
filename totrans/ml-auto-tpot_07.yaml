- en: '*Chapter 5*: Parallel Training with TPOT and Dask'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you'll dive into a bit of a more advanced topic; that is, automated
    machine learning. You'll learn how to handle machine learning tasks in a parallel
    manner by distributing the work on a Dask cluster. This chapter will be more theoretical
    than the previous two, but you will still learn many useful things.
  prefs: []
  type: TYPE_NORMAL
- en: We'll cover essential topics and ideas behind parallelism in Python, and you'll
    learn how to achieve parallelism in a couple of different ways. Then, we'll dive
    deep into the Dask library, explore its basic functionality, and see how you can
    tie it with TPOT.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to parallelism in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the Dask library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training machine learning models with TPOT and Dask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No prior exposure to Dask or even parallel programming is required for you to
    read and understand this chapter. Previous experience is helpful, as fitting this
    big of a concept into a few pages is close to impossible. You should still be
    able to follow and fully understand everything written here as all of the concepts
    will be explained.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the source code and dataset for this chapter here: [https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter05](https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter05).'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to parallelism in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Executing tasks sequentially (where the second one starts after the first one
    finishes) is required in some situations. For example, maybe the input of the
    second function relies on the output of the first one. If that's the case, these
    two functions (processes) can't be executed at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: But more often than not, that's not the case. Just imagine your program is connecting
    to three different API endpoints before the dashboard is displayed. The first
    API returns the current weather conditions, the second one returns the stock prices,
    and the last one returns today's exchange rates. There's no point in making the
    API calls one after the other. They don't rely on each other, so running them
    sequentially would be a huge waste of time.
  prefs: []
  type: TYPE_NORMAL
- en: Not only that, but it would also be a waste of CPU cores. Most modern PCs have
    at least four CPU cores. If you're running things sequentially, you're only using
    a single core. Why not use all of them if you can?
  prefs: []
  type: TYPE_NORMAL
- en: One of the ways to achieve parallelism in Python is with multiprocessing. It
    is a process-based parallelism technique. As you would imagine, Python has a `multiprocessing`
    library built into it, and this section will teach you how to use it. With Python
    3.2 and beyond, this library stopped being the recommended way of implementing
    multiprocessing in your apps. There's a new kid on the block, and its name is
    `concurrent.futures`. It's yet another built-in library you'll learn how to use
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to explain and understand multiprocessing is with Python's
    built-in `time` library. You can use it to track time differences and to pause
    program execution intentionally, among other things. This is just what we need
    because we can put in many print statements with some time gaps between them,
    and then see how the program acts when it's run sequentially and how it acts when
    it's run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn how multiprocessing works in Python through a couple of hands-on
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'For starters, please take a look at the following code snippet. In it, the
    `sleep_func()` function has been declared. Its task is to print a message, pause
    the program executing for 1 second, and then to print another message as the function
    completes. We can monitor the time this function takes to run for an arbitrary
    number of times (let''s say five) and then print out the execution time duration.
    The snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'So, what happened here? Nothing unexpected, to say the least. The `sleep_func()`
    function executed sequentially five times. The execution time is approximately
    5 seconds. You could also simplify the preceding snippet in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is identical, as you would expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Is there a problem with this approach? Well, yes. We're wasting both time and
    CPU cores. The functions aren't dependent in any way, so why don't we run them
    in parallel? As we mentioned previously, there are two ways of doing this. Let's
    examine the older way first, through the `multiprocessing` library.
  prefs: []
  type: TYPE_NORMAL
- en: It's a bit of a lengthy approach because it requires declaring a process, starting
    it, and joining it. It's not so tedious if you have only a few, but what if there
    are tens of processes in your program? It can become tedious fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates how to run the `sleep_func()` function
    three times in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, each of the three processes was launched independently and in
    parallel, so they all managed to finish in a single second.
  prefs: []
  type: TYPE_NORMAL
- en: Both `Process()` and `start()` are self-explanatory, but what is the `join()`
    function doing? Simply put, it tells Python to wait until the process is complete.
    If you call `join()` on all of the processes, the last two code lines won't execute
    until all of the processes are finished. For fun, try to remove the `join()` calls;
    you'll immediately get the gist.
  prefs: []
  type: TYPE_NORMAL
- en: You now have a basic intuition behind multiprocessing, but the story doesn't
    end here. Python 3.2 introduced a new, improved way of executing tasks in parallel.
    The `concurrent.futures` library is the best one available as of yet, and you'll
    learn how to use it next.
  prefs: []
  type: TYPE_NORMAL
- en: 'With it, you don''t have to manage processes manually. Every executed function
    will return something, which is `None` in the case of our `sleep_func()` function.
    You can change it by returning the last statement instead of printing it. Furthermore,
    this new approach uses `ProcessPoolExecutor()` to run. You don''t need to know
    anything about it; just remember that it is used to execute multiple processes
    at the same time. Codewise, simply put everything you want to run in parallel
    inside. This approach unlocks two new functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`submit()`: Used to run the function in parallel. The returned results will
    be appended to a list so that we can print them (or do anything else) with the
    next function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`result()`: Used to obtain the returned value from the function. We''ll simply
    print the result, but you''re free to do anything else.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To recap, we''ll append the results to a list, and then print them out as the
    functions finish executing. The following snippet shows you how to implement multiprocessing
    with the most recent Python approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the program behaves similarly to what we had previously, with
    a few added benefits â€“ you don't have to manage processes on your own, and the
    syntax is much cleaner.
  prefs: []
  type: TYPE_NORMAL
- en: The one issue we have so far is the lack of function parameters. Currently,
    we're just calling a function that doesn't accept any parameters. That won't be
    the case most of the time, so it's important to learn how to handle function parameters
    as early as possible.
  prefs: []
  type: TYPE_NORMAL
- en: We'll introduce a single parameter to our `sleep_func()` function that allows
    us to specify how long the execution will be paused. The print statements inside
    the function are updated accordingly. The sleep times are defined within the `sleep_seconds`
    list, and the value is passed to `append()` at each iteration as a second parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire snippet is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: That's how you can handle function parameters in parallel processing. Keep in
    mind that the executing time won't be exactly the same on every machine, as the
    runtime duration will depend on your hardware. As a general rule, you should definitely
    see a speed improvement compared to a non-parallelized version of the script.
    You now know the basics of parallel processing. In the next section, you'll learn
    where Python's Dask library comes into the picture, and in the section afterward,
    you'll combine parallel programming, Dask, and TPOT in order to build machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the Dask library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can think of Dask as one of the most revolutionary Python libraries for
    data processing at scale. If you are a regular pandas and NumPy user, you'll love
    Dask. The library allows you to work with data NumPy and pandas doesn't allow
    because they don't fit into the RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Dask supports both NumPy array and pandas DataFrame data structures, so you'll
    quickly get up to speed with it. It can run either on your computer or a cluster,
    making it that much easier to scale. You only need to write the code once and
    then choose the environment that you'll run it in. It's that simple.
  prefs: []
  type: TYPE_NORMAL
- en: One other thing to note is that Dask allows you to run code in parallel with
    minimal changes. As you saw earlier, processing things in parallel means the execution
    time decreases, which is generally the behavior we want. Later, you'll learn how
    parallelism in Dask works with `dask.delayed`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, you''ll have to install the library. Make sure the correct
    environment is activated. Then, execute the following from the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: There are other installation options. For example, you could install only the
    arrays or DataFrames module, but it's a good idea to install everything from the
    start. Don't forget to put quotes around the library name, as not doing so will
    result in an error.
  prefs: []
  type: TYPE_NORMAL
- en: If you've installed everything, you'll have access to three Dask collections
    â€“ arrays, DataFrames, and bags. All of these can store datasets that are larger
    than your RAM size, and they can all partition data between RAM and a hard drive.
  prefs: []
  type: TYPE_NORMAL
- en: "Let's start with Dask arrays and compare them with a NumPy alternative. You\
    \ can create \La NumPy array of ones with 1,000x1,000x1,000 dimensions by executing\
    \ the following code cell in a Notebook environment. The `%%time` magic command\
    \ is used to measure the time needed for the cell to finish with the execution:"
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Constructing larger arrays than this one results in a memory error on my machine,
    but this will do just fine for the comparisons. The corresponding output is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, it took 4.35 seconds to create this array. Now, let''s do the
    same with Dask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the only change is in the library import name. The executing
    time results will probably come as a surprise if this is your first encounter
    with the Dask library. They are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Yes, you are reading this right. Dask took 696 microseconds to create an array
    of identical dimensions, which is 6,250 times faster. Sure, you shouldn't expect
    this drastic reduction in execution time in the real world, but the differences
    should still be quite significant.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's take a look at Dask DataFrames. The syntax should, once again, feel
    very similar, so it shouldn't take you much time to learn the library. To fully
    demonstrate Dask's capabilities, we'll create some large datasets that won't be
    able to fit in the memory of a single laptop. To be more precise, we'll create
    10 CSV files that are time series-based, each presenting data for a single year
    aggregated by seconds and measured through five different features. That's a lot,
    and it will definitely take some time to create, but you should end up with 10
    datasets where each is around 1 GB in size. If you have a laptop with 8 GB of
    RAM like me, there's no way you could fit it in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet creates these datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Just make sure to have this `/data` folder where your Notebook is and you''ll
    be good to go. Also, make sure you have 10 GB of disk space if you''re following
    along. The last line, `!ls data/`, lists all the files located in the `data` folder.
    Here''s what you should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s take a look at how much time it takes pandas to read in a single
    CSV file and perform a simple aggregation. To be more precise, the dataset is
    grouped by month and the sum is extracted. The following code snippet demonstrates
    how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it took pandas 42 seconds to perform this computation. Not too
    shabby, but what if you absolutely need to load in all of the datasets and perform
    computations? Let's explore that next.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `glob` library to get paths to desired files in a specified
    folder. You can then read all of them individually, and use the `concat()` function
    from pandas to stack them together. The aggregation is performed in the same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: There isn't much to say here â€“ the Notebook simply breaks. Storing 10 GB+ of
    data into RAM isn't feasible for an 8 GB RAM machine. One way you could get around
    this would be to load data in chunks, but that's a headache of its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'What can Dask do to help? Let''s learn how to load in these CSVs with Dask
    and perform the same aggregation. You can use the following snippet to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will once again surprise you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: That's correct â€“ in less than 4 minutes, Dask managed to read over 10 GB of
    data to an 8 GB RAM machine. That alone should make you reconsider NumPy and pandas,
    especially if you're dealing with large amounts of data or you expect to deal
    with it in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there are Dask bags. They are used for storing and processing general
    Python data types that can't fit into memory â€“ for example, log data. We won't
    explore this data structure, but it's nice to know it exists.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we will explore the concept of parallel processing with Dask.
    You learned in the previous section that there are no valid reasons to process
    data or perform any other operation sequentially, as the input of one doesn't
    rely on the output of another.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dask delayed* allows for parallel execution. Sure, you can still rely only
    on the multiprocessing concepts we learned earlier, but why? It can be a tedious
    approach, and Dask has something better to offer. With Dask, there''s no need
    to change the programming syntax, as was the case with pure Python. You just need
    to annotate a function you want to be parallelized with the `@dask.delayed` decorator
    and you''re good to go!'
  prefs: []
  type: TYPE_NORMAL
- en: You can parallelize multiple functions and then place them inside a computational
    graph. That's what we'll do next.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet declares two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cube()`: Returns a cube of a number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multiply()`: Multiplies all numbers in a list and returns the product'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the library imports you''ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the first function on five numbers and call the second function
    on the results to see what happens. Note the call to `time.sleep()` inside the
    `cube()` function. This will make spotting differences between parallelized and
    non-parallelized functions that much easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This is your regular (sequential) data processing. There''s nothing wrong with
    it, especially when there are so few and simple operations. The corresponding
    output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the code cell took around 5 seconds to run because of sequential
    execution. Now, let''s see the modifications you have to make to parallelize these
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'So, there''s only the `@delayed` decorator and a call to `compute()` on the
    graph. The results are displayed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the whole thing took just over a second to run because of the
    parallel execution. The previously declared computational graph comes with one
    more handy feature â€“ it''s easy to visualize. You''ll need to have *GraphViz*
    installed on your machine and as a Python library. The procedure is different
    for every OS, so we won''t go through it here. A quick Google search will tell
    you how to install it. Once you''re done, you can execute the following line of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding visualization is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 â€“ Visualization of a Dask computational graph'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_05_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 â€“ Visualization of a Dask computational graph
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the graph, the `cube()` function is called five times in
    parallel, and its results are stored in the buckets above it. Then, the `multiply()`
    function is called with these values and stores the product in the top bucket.
  prefs: []
  type: TYPE_NORMAL
- en: That's all you need to know about the basics of Dask. You've learned how to
    work with Dask arrays and DataFrames, and also how to use Dask to process operations
    in parallel. Not only that, but you've also learned the crucial role Dask plays
    in modern-day data science and machine learning. Dataset sizes often exceed available
    memory, so modern solutions are required.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, you'll learn how to train TPOT automated machine learning
    models with Dask.
  prefs: []
  type: TYPE_NORMAL
- en: Training machine learning models with TPOT and Dask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimizing machine learning pipelines is, before everything, a time-consuming
    process. We can shorten it potentially significantly by running things in parallel.
    Dask and TPOT work great when combined, and this section will teach you how to
    train TPOT models on a Dask cluster. Don't let the word "cluster" scare you, as
    your laptop or PC will be enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll have to install one more library to continue, and it is called `dask-ml`.
    As its name suggests, it''s used to perform machine learning with Dask. Execute
    the following from the Terminal to install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Once that''s done, you can open up Jupyter Lab or your favorite Python code
    editor and start coding. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with library imports. We'll also make a dataset decision here. This
    time, we won't spend any time on data cleaning, preparation, or examination. The
    goal is to have a dataset ready as soon as possible. The `load_digits()` function
    from scikit-learn comes in handy because it is designed to fetch many 8x8 pixel
    digit images for classification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As some of the libraries often fill up your screen with unnecessary warnings,
    we''ll use the `warnings` library to ignore them. Refer to the following snippet
    for all the library imports:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The only new thing here is the `Client` class from `dask.distributed`. It is
    used to establish a connection with the Dask cluster (your computer, in this case).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You''ll now make an instance of the client. This will immediately start the
    Dask cluster and use all the CPU cores you have available. Here''s the code for
    instance creation and checking where the cluster runs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once executed, you should see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.2 â€“ Information on the Dask cluster'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_05_002.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.2 â€“ Information on the Dask cluster
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can click on the dashboard link, and it will take you to http://127.0.0.1:8787/status.
    The following screenshot shows what the dashboard should look like when it''s
    opened for the first time (no tasks running):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.3 â€“ Dask cluster dashboard (no running tasks)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_05_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.3 â€“ Dask cluster dashboard (no running tasks)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The dashboard will become much more colorful once you start training the models.
    We'll do the necessary preparation next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can call the `load_digits()` function to get the image data and then use
    the `train_test_split()` function to split the images into subsets for training
    and testing. The train/test ratio is 50:50 for this example, as we don't want
    to spend too much time on the training. The ratio should be higher for the training
    set in almost any scenario, so make sure to remember that.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the split is done, you can call `.shape` on the subsets to check their
    dimensionality. Here''s the entire code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The corresponding output is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.4 â€“ Dimensionality of training and testing subsets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_05_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.4 â€“ Dimensionality of training and testing subsets
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next stop â€“ model training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You now have everything needed to train models with TPOT and Dask. You can
    do so in a very similar fashion to what you did previously. The key parameter
    here is `use_dask`. You should set it to `True` if you want to use Dask for training.
    The other parameters are well known:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you''re ready to call the `fit()` function and train the model on the
    training subset. Here''s a line of code for doing so:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The appearance of the Dask dashboard will change immediately after you start
    training the model. Here''s what it will look like a couple of minutes into the
    process:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5 â€“ Dask dashboard during training'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_05_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 â€“ Dask dashboard during training
  prefs: []
  type: TYPE_NORMAL
- en: 'After 10 minutes, TPOT will finish optimizing the pipeline, and you''ll see
    the following output in your Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 â€“ TPOT optimization outputs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_05_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 â€“ TPOT optimization outputs
  prefs: []
  type: TYPE_NORMAL
- en: And that's all you need to do to combine TPOT and Dask.
  prefs: []
  type: TYPE_NORMAL
- en: You now know how to train models on a Dask cluster, which is the recommended
    way of doing things for larger datasets and more challenging problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was packed with information not only about TPOT and training models
    in a parallel manner, but also about parallelism in general. You've learned a
    lot â€“ from how to parallelize basic functions that do nothing but sleep for a
    while, to parallelizing function with parameters and Dask fundamentals, to training
    machine learning models with TPOT and Dask on a Dask cluster.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you know how to solve regression and classification tasks in an automated
    manner, and how to parallelize the training process. The following chapter, [*Chapter
    6*](B16954_06_Final_SK_ePub.xhtml#_idTextAnchor073)*, Getting Started with Deep
    Learning â€“ Crash Course in Neural Networks*, will provide you with the required
    knowledge on neural networks. It will form a basis for [*Chapter 7*](B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086)*,
    Neural Network Classifier with TPOT*, where we'll dive deep into training automated
    machine learning models with state-of-the-art neural network algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: As always, please feel free to practice solving both regression and classification
    tasks with TPOT, but this time, try to parallelize the process with Dask.
  prefs: []
  type: TYPE_NORMAL
- en: Q&A
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Define the term "parallelism."
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain which types of tasks can and can't be parallelized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List and explain three options for implementing parallelism in your applications
    (all are listed in this chapter).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Dask and what makes it superior to NumPy and pandas for larger datasets?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name and explain three basic data structures that are implemented in Dask.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a Dask cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do you have to do to tell TPOT it should use Dask for training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
