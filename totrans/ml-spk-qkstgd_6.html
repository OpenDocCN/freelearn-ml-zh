<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Natural Language Processing Using Apache Spark</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll study and implement common algorithms that are used in NLP, which can help us develop machines that are capable of automatically analyzing and understanding human text and speech in context. Specifically, we will study and implement the following classes of computer science algorithms related to NLP:</p>
<ul>
<li>Feature transformers, including the following:
<ul>
<li>Tokenization</li>
<li>Stemming</li>
<li>Lemmatization</li>
<li>Normalization</li>
</ul>
</li>
<li>Feature extractors, including the following :
<ul>
<li>Bag of words</li>
<li>Term frequency–inverse document frequency </li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feature transformers</h1>
                </header>
            
            <article>
                
<p>The fundamental concept behind natural language processing is treating human text and speech as data—just like the structured and unstructured numerical and categorical data sources we have encountered in this book thus far—while preserving its <em>context</em>. However, natural language is notoriously difficult to understand, even for humans, let alone machines! Not only does natural language consist of hundreds of different spoken languages, with different writing systems, but it also poses other challenges, such as different tones, inflections, slang, abbreviations, metaphors, and sarcasm. Writing systems and communication platforms in particular provide us with text that may contain spelling mistakes, unconventional grammar, and sentences that are loosely structured.</p>
<p>Our first challenge, therefore, is to convert natural language into data that can be used by a machine while preserving its underlying context. Furthermore, when applied to machine learning, we also need to convert natural language into feature vectors in order to train machine learning models. Well, there are two broad classes of computer science algorithms that help us with these challenges—<strong>feature extractors</strong>, which help us extract relevant features from the natural language data, and <strong>feature transformers</strong>, which help us scale, convert, and/or modify these features in preparation for subsequent modelling. In this subsection, we will discuss feature transformers and how they can help us convert our natural language data into structures that are easier to process. First, let's introduce some common definitions within NLP.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Document</h1>
                </header>
            
            <article>
                
<p>In NLP, a document represents a logical container of text. The container itself can be anything that makes sense within the context of your use case. For example, one document could refer to a single article, record, social media posting, or tweet.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Corpus</h1>
                </header>
            
            <article>
                
<p>Once you have defined what your document represents, a corpus is defined as a logical collection of documents. Using the previous examples, a corpus could represent a collection of articles (for example, a magazine or blog) or a collection of tweets (for example, tweets with a particular hashtag).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preprocessing pipeline</h1>
                </header>
            
            <article>
                
<p>One of the basic tasks involved in NLP is the preprocessing of your documents in an attempt to standardize the text from different sources as much as possible. Not only does preprocessing help us to standardize text, it often reduces the size of the raw text, thereby reducing the computational complexity of subsequent processes and models. The following subsections describe common preprocessing techniques that may constitute a typical ordered preprocessing pipeline.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tokenization</h1>
                </header>
            
            <article>
                
<p>Tokenization refers to the technique of splitting up your text into individual <em>tokens</em> or terms. Formally, a token is defined as a sequence of characters that represents a subset of the original text. Informally, tokens are typically just the different words that make up the original text, and that have been segmented using the whitespace and other punctuation characters. For example, the sentence "Machine Learning with Apache Spark" may result in a collection of tokens persisted in an array or list expressed as <kbd>["Machine", "Learning", "with", "Apache", "Spark"]</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Stop words</h1>
                </header>
            
            <article>
                
<p>Stop words are common words in a given language that are used to structure a sentence grammatically, but that are not necessarily helpful in determining its underlying meaning or sentiment. For example, in the English language, common stop words include <em>and</em>, <em>I</em>, <em>there</em>, <em>this</em>, and <em>with</em>. A common preprocessing technique is to therefore remove these words from the collection of tokens by filtering based on a language-specific lookup of stop words. Using our previous example, our filtered list of tokens would be <kbd>["Machine", "Learning", "Apache", "Spark]</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Stemming</h1>
                </header>
            
            <article>
                
<p>Stemming refers to the technique of reducing words to a common base or <em>stem</em>. For example, the words "connection", "connections", "connective", "connected", and "connecting" can all be reduced to their common stem of "connect". Stemming is not a perfect process, and stemming algorithms are liable to make mistakes. However, for the purposes of reducing the size of a dataset in order to train a machine learning model, it is a valuable technique. Using our previous example, our filtered list of stems would be <kbd>["Machin", "Learn", "Apach", "Spark"]</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Lemmatization</h1>
                </header>
            
            <article>
                
<p>While stemming quickly reduces words to a base form, it does not take into account the context, and can therefore not differentiate between words that have different meanings depending on their position within a sentence or context. Lemmatization does not crudely reduce words purely based on a common stem, but instead aims to remove inflectional endings only in order to return a dictionary form of a word called its <em>lemma</em>. For example, the words <em>am</em>, <em>is</em>, <em>being</em>, and <em>was</em> can be reduced to the lemma <em>be</em>, while a stemmer would not be able to infer this contextual meaning.</p>
<p>While lemmatization can be used to preserve context and meaning to a better extent, it comes at the cost of additional computational complexity and processing time. Using our previous example, our filtered list of lemmas may therefore look like <kbd>["Machine", "Learning", "Apache", "Spark"]</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Normalization</h1>
                </header>
            
            <article>
                
<p>Finally, normalization refers to a wide variety of common techniques that are used to standardize text. Typical normalization techniques include converting all text to lowercase, removing selected characters, punctuation and other sequences of characters (typically using regular expressions), and expanding abbreviations by applying language-specific dictionaries of common abbreviations and slang terms.</p>
<p class="mce-root"><em>Figure 6.1</em> illustrates a typical ordered preprocessing pipeline that is used to standardize raw written text:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-580 image-border" src="Images/2166c5b8-2ae4-494c-aeff-0addd02bdefa.png" style="width:39.42em;height:25.75em;" width="1049" height="688"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6.1: Typical preprocessing pipeline</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feature extractors</h1>
                </header>
            
            <article>
                
<p>We have seen how feature transformers allow us to convert, modify, and standardize our documents using a preprocessing pipeline, resulting in the conversion of raw text into a collection of tokens. <em>Feature extractors</em> take these tokens and generate feature vectors from them that may then be used to train machine learning models. Two common examples of typical feature extractors that are used in NLP are the <strong>bag of words</strong> and <strong>term frequency–inverse document frequency</strong> (<strong>TF–IDF</strong>) algorithms.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Bag of words</h1>
                </header>
            
            <article>
                
<p>The <em>bag of words</em> approach simply counts the number of occurrences of each unique word in the raw or tokenized text. For example, given the text "Machine Learning with Apache Spark, Apache Spark's MLlib and Apache Kafka", the bag of words approach would provide us with the following numerical feature vector:</p>
<table style="border-color: #000000;border-collapse: collapse" border="1">
<tbody>
<tr>
<td style="padding: 5px">
<p>Machine</p>
</td>
<td style="padding: 5px">
<p>Learning</p>
</td>
<td style="padding: 5px">
<p>with</p>
</td>
<td style="padding: 5px">
<p>Apache</p>
</td>
<td style="padding: 5px">
<p>Spark</p>
</td>
<td style="padding: 5px">
<p>MLlib</p>
</td>
<td style="padding: 5px">
<p>Kafka</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p>1</p>
</td>
<td style="padding: 5px">
<p>1</p>
</td>
<td style="padding: 5px">
<p>1</p>
</td>
<td style="padding: 5px">
<p>3</p>
</td>
<td style="padding: 5px">
<p>2</p>
</td>
<td style="padding: 5px">
<p>1</p>
</td>
<td style="padding: 5px">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Note that each unique word is a feature or dimension, and that the bag of words approach is a simple technique that is often employed as a baseline model with which to compare the performance of more advanced feature extractors.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Term frequency–inverse document frequency</h1>
                </header>
            
            <article>
                
<p><strong>TF–IDF</strong> aims to improve upon the bag of words approach by providing an indication of how <em>important</em> each word is, taking into account how often that word appears across the entire corpus.</p>
<p class="mce-root">Let us use <em>TF(t, d)</em> to denote the <strong>term frequency</strong>, which is the number of times that a term, <em>t</em>, appears in a document, <em>d</em>. Let's also use <em>DF(t, D)</em> to <span>denote</span><span> </span><span>the</span> <strong>document frequency</strong><span>, which is the number of documents in our corpus, </span><em>D</em><span>, that contain the term </span><em>t</em><span>. We can then define the</span> <strong>inverse document frequency</strong> <em>IDF(t, D)</em> <span>as follows:</span></p>
<p style="padding-left: 180px"><img src="Images/a66d7f42-032c-4cd8-8362-92c147274cbc.png" style="width:13.83em;height:2.33em;" width="2060" height="350"/></p>
<p>The IDF provides us with a measure of how important a term is, taking into account how often that term appears across the entire corpus, where <em>|D|</em> is the total number of documents in our corpus, <em>D</em>. Terms that are less common across the corpus have a higher IDF metric. Note, however, that because of the use of the logarithm, if a term appears in all documents, its IDF becomes 0—that is, <em>log(1)</em>. IDF, therefore, provides a metric whereby more value is placed on rarer terms that are important in describing documents.</p>
<p>Finally, to calculate the TF–IDF measure, we simply multiply the term frequency by the inverse document frequency as follows:</p>
<p style="padding-left: 150px"><img src="Images/fc877716-e3b7-4aba-a75e-9a744a014d5c.png" style="width:17.25em;height:1.25em;" width="3170" height="220"/></p>
<p>This implies that the TF–IDF measure increases proportionally with the number of times that a word appears in a document, offset by the frequency of the word across the entire corpus. This is important because the term frequency alone may highlight words such as "a", "I", and "the" that appear very often in a given document but that do not help us determine the underlying meaning or sentiment of the text. By employing TF–IDF, we can reduce the impact of these types of words on our analysis.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Case study – sentiment analysis</h1>
                </header>
            
            <article>
                
<p>Let's now apply these feature transformers and feature extractors to a very modern real-world use case—sentiment analysis. In sentiment analysis, the goal is to classify the underlying human sentiment—for example, whether the writer is positive, neutral, or negative towards the subject of a text. To many organizations, sentiment analysis is an important technique that is used to better understand their customers and target markets. For example, sentiment analysis can be used by retailers to gauge the public's reaction to a particular product, or by politicians to assess public mood towards a policy or news item. In our case study, we will examine tweets about airlines in order to predict whether customers are saying positive or negative things about them. Our analysis could then be used by airlines in order to improve their customer service by focusing on those tweets that have been classified as negative in sentiment.</p>
<div class="packt_infobox">The corpus of tweets that we will use for our case study has been downloaded from <strong>Figure Eight</strong>, a company that provides businesses with high-quality training datasets for real-world machine learning. Figure Eight also provides a Data for Everyone platform containing open datasets that are available for download by the public, and which may be found at <a href="https://www.figure-eight.com/data-for-everyone/">https://www.figure-eight.com/data-for-everyone/</a>.</div>
<p>If you open <kbd>twitter-data/airline-tweets-labelled-corpus.csv</kbd> in any text editor from either the GitHub repository accompanying this book or from Figure Eight's Data for Everyone platform, you will find a collection of 14,872 tweets about major airlines that were scraped from Twitter in February 2015. These tweets have also been pre-labelled for us, with a sentiment classification of positive, negative, or neutral. The pertinent columns in this dataset are described in the following table:</p>
<table style="border-color: #000000;border-collapse: collapse" border="1">
<tbody>
<tr>
<td style="padding: 5px">
<p><strong>Column Name</strong></p>
</td>
<td style="padding: 5px">
<p><strong>Data Type</strong></p>
</td>
<td style="padding: 5px">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>unit_id</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Long</kbd></p>
</td>
<td style="padding: 5px">
<p>Unique identifier (primary key)</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>airline_sentiment</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>String</kbd></p>
</td>
<td style="padding: 5px">
<p>Sentiment classification—positive, neutral, or negative</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>airline</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>String</kbd></p>
</td>
<td style="padding: 5px">
<p>Name of the airline</p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>text</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>String</kbd></p>
</td>
<td style="padding: 5px">
<p>Textual content of the tweet</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Our goal will be to use this corpus of tweets in order to train a machine learning model to predict whether future tweets about a given airline are positive or negative in sentiment towards that airline.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">NLP pipeline</h1>
                </header>
            
            <article>
                
<p>Before we look at the Python code for our case study, let's visualize the end-to-end NLP pipeline that we will construct. Our NLP pipeline for this case study is illustrated in <em>Figure 6.2</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/e322457d-6d42-4f82-999a-a2a446d5862e.png" style="width:26.08em;height:16.75em;" width="1168" height="750"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6.2: End-to-end NLP pipeline</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">NLP in Apache Spark</h1>
                </header>
            
            <article>
                
<p>As of Spark 2.3.2, tokenization and stop-word removal feature transformers (among a wide variety of others), and the TF–IDF feature extractor is available natively in <kbd>MLlib</kbd>. Although stemming, lemmatization, and standardization can be achieved indirectly through transformations on Spark dataframes in Spark 2.3.2 (via <strong>user-defined functions</strong> (<strong>UDFs</strong>) and map functions that are applied to RDDs), we will be using a third-party Spark library called <kbd>spark-nlp</kbd> to perform these feature transformations. This third-party library has been designed to extend the features already available in <kbd>MLlib</kbd> by providing an easy-to-use API for distributed NLP annotations on Spark dataframes at scale. To learn more about <kbd>spark-nlp</kbd>, please visit <a href="https://nlp.johnsnowlabs.com/">https://nlp.johnsnowlabs.com/</a>. Finally, we will use the estimators and transformers that are already available natively in <kbd>MLlib</kbd>—as we have seen in previous chapters—to train our final machine learning classification models.</p>
<div class="packt_infobox">Note that by using feature transformers and extractors native to <kbd>MLlib</kbd> followed by feature transformers provided by the third-party <kbd>spark-nlp</kbd> library, before finally applying native <kbd>MLlib</kbd> estimators, we will be required to explicitly define and develop data transformation stages in our pipeline in order to conform to the underlying data structures expected by the two different libraries. While this is not recommended for production-grade pipelines because of its inefficiencies, one of the purposes of this section is to demonstrate how to use both libraries for NLP. Readers will then be in an informed position to choose a suitable library, depending on the requirements of the use case in question.</div>
<p class="mce-root">Depending on your environment setup, there are a few methods that are available that can be used to install <kbd>spark-nlp</kbd>, as described at <a href="https://nlp.johnsnowlabs.com/quickstart.html">https://nlp.johnsnowlabs.com/quickstart.html</a>. However, based on the local development environment that we provisioned in <a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank"/><a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank">Chapter 2</a>,<em> Setting Up a Local Development Environment</em>, we will install <kbd>spark-nlp</kbd> using <kbd>pip</kbd>, which is another commonly used Python package manager that comes bundled with the Anaconda distribution that we have already installed (at the time of writing, <kbd>spark-nlp</kbd> is not available via the <kbd>conda</kbd> repositories, and so we shall use <kbd>pip</kbd> instead). To install <kbd>spark-nlp</kbd> for our Python environment, simply execute the following command, which will install version 1.7.0 of <kbd>spark-nlp</kbd> (which is the latest version as of writing, and which is compatible with Spark 2.x):</p>
<pre><strong>&gt; pip install spark-nlp==1.7.0</strong></pre>
<p class="mce-root"><span>We then need to tell Spark where it can find the <kbd>spark-nlp</kbd> library. We can do this either by defining an additional parameter in</span> <kbd>{SPARK_HOME}/conf/spark-defaults.conf</kbd> <span>or by setting the <kbd>spark.jars</kbd> configuration within our code when instantiating a Spark context, as follows:</span></p>
<pre>conf = SparkConf().set("spark.jars", '/opt/anaconda3/lib/python3.6/sitepackages/sparknlp/lib/sparknlp.jar')<br/>   .setAppName("Natural Language Processing - Sentiment Analysis")<br/>sc = SparkContext(conf=conf)</pre>
<p>Please refer to <a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank"/><a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank"/><a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank">Chapter 2</a>, <em>Setting Up a Local Development Environment</em>, for further details regarding defining the configuration for Apache Spark. Note that in a multinode Spark cluster, all third-party Python packages either need to be installed on all Spark nodes or your Spark application itself needs to be packaged into a self-contained file containing all third-party dependencies. This self-contained file is then distributed to all nodes in the Spark cluster.</p>
<p>We are now ready to develop our NLP pipeline in Apache Spark in order to perform sentiment analysis on our corpus of airline tweets! Let's go through the following steps:</p>
<div class="mce-root packt_infobox">The following subsections describe each of the pertinent cells in the corresponding Jupyter notebook for this use case, called <kbd>chp06-01-natural-language-processing.ipynb</kbd>. It can be found in the GitHub repository accompanying this book.</div>
<ol>
<li>As well as importing the standard PySpark dependencies, we also need to import the relevant <kbd>spark-nlp</kbd> dependencies, including its <kbd>Tokenizer</kbd>, <kbd>Stemmer</kbd>, and <kbd>Normalizer</kbd> classes, as follows:</li>
</ol>
<pre style="padding-left: 60px">import findspark<br/>findspark.init()<br/>from pyspark import SparkContext, SparkConf<br/>from pyspark.sql import SQLContext<br/>from pyspark.sql.functions import *<br/>from pyspark.sql.types import StructType, StructField<br/>from pyspark.sql.types import LongType, DoubleType, IntegerType, StringType, BooleanType<br/>from pyspark.ml.feature import VectorAssembler<br/>from pyspark.ml.feature import StringIndexer<br/>from pyspark.ml.feature import Tokenizer<br/>from pyspark.ml.feature import StopWordsRemover<br/>from pyspark.ml.feature import HashingTF, IDF<br/>from pyspark.ml import Pipeline, PipelineModel<br/>from pyspark.ml.classification import DecisionTreeClassifier<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator<br/>from pyspark.mllib.evaluation import MulticlassMetrics<br/><br/>from sparknlp.base import *<br/>from sparknlp.annotator import Tokenizer as NLPTokenizer<br/>from sparknlp.annotator import Stemmer, Normalizer</pre>
<ol start="2">
<li><span>Next, we instantiate a</span> <kbd>SparkContext</kbd> <span>as usual. Note, however, that in this case, we explicitly tell Spark where to find the <kbd>spark-nlp</kbd> library using the</span> <kbd>spark-jars</kbd><span> configuration parameter. We can then invoke the</span> <kbd>getConf()</kbd> <span>method on our</span> <kbd>SparkContext</kbd> <span>instance to review the current Spark configuration, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">conf = SparkConf().set("spark.jars", '/opt/anaconda3/lib/python3.6/site-packages/sparknlp/lib/sparknlp.jar')<br/>   .setAppName("Natural Language Processing - Sentiment Analysis")<br/>sc = SparkContext(conf=conf)<br/>sqlContext = SQLContext(sc)<br/>sc.getConf().getAll()</pre>
<ol start="3">
<li><span>After loading our corpus of airline tweets from</span> <kbd>twitter-data/airline-tweets-labelled-corpus.csv</kbd> <span>into a Spark dataframe called</span> <kbd>airline_tweets_df</kbd><span>, we generate a new label column. The existing dataset already contains a label column called</span> <kbd>airline_sentiment</kbd><span>, which is either</span> <kbd>"positive"</kbd><span>,</span> <kbd>"neutral"</kbd><span>, or</span> <kbd>"negative"</kbd> <span>based on a manual pre-classification. Although positive messages are naturally always welcome, in reality, the most useful messages are usually the negative ones. By automatically identifying and studying the negative messages, organizations can focus more efficiently on how to improve their products and services based on negative feedback. Therefore, we will create a new label column called</span> <kbd>negative_sentiment_label</kbd><span> that is</span> <kbd>"true"</kbd> <span>if the underlying sentiment has been classified as</span> <kbd>"negative"</kbd><span> and</span> <kbd>"false"</kbd> <span>otherwise, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px">airline_tweets_with_labels_df = airline_tweets_df<br/>   .withColumn("negative_sentiment_label",<br/>      when(col("airline_sentiment") == "negative", lit("true"))<br/>      .otherwise(lit("false")))<br/>   .select("unit_id", "text", "negative_sentiment_label")</pre>
<ol start="4">
<li class="mce-root"><span>We are now ready to build and apply our preprocessing pipeline to our corpus of raw tweets! Here, we demonstrate how to utilize the feature transformers native to Spark's <kbd>MLlib</kbd>, namely its</span> <kbd>Tokenizer</kbd> <span>and</span> <kbd>StopWordsRemover</kbd> <span>transformers. First, we tokenize the raw textual content of each tweet using the</span> <kbd>Tokenizer</kbd> <span>transformer, resulting in a new column containing a list of parsed tokens. We then pass this column containing the tokens to the</span> <kbd>StopWordsRemover</kbd> <span>transformer, which removes English language (default) stop words from this list, resulting in a new column containing the list of filtered tokens. In the next cell, we will demonstrate how to utilize the feature transformers available in the <kbd>spark-nlp</kbd> third-party library. However, <kbd>spark-nlp</kbd> requires a column of a</span> <kbd>string</kbd> <span>type as its initial input, not a</span> list <span>of tokens. Therefore, the final statement concatenates the list of filtered tokens back into a whitespace-delimited <kbd>string</kbd> column, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">filtered_df = airline_tweets_with_labels_df<br/>   .filter("text is not null")<br/>tokenizer = Tokenizer(inputCol="text", outputCol="tokens_1")<br/>tokenized_df = tokenizer.transform(filtered_df)<br/>remover = StopWordsRemover(inputCol="tokens_1", <br/>   outputCol="filtered_tokens")<br/>preprocessed_part_1_df = remover.transform(tokenized_df)<br/>preprocessed_part_1_df = preprocessed_part_1_df<br/>   .withColumn("concatenated_filtered_tokens",<br/>      concat_ws(" ", col("filtered_tokens")))</pre>
<ol start="5">
<li><span>We can now demonstrate how to utilize the feature transformers and annotators available in the <kbd>spark-nlp</kbd> third-party library, namely its</span> <kbd>DocumentAssember</kbd> <span>transformer and</span> <kbd>Tokenizer</kbd>, <kbd>Stemmer</kbd> , <span>and</span> <kbd>Normalizer</kbd> <span>annotators. First, we create annotated</span> documents <span>from our string column that are required as the initial input into the <kbd>spark-nlp</kbd> pipelines. Then, we apply the <kbd>spark-nlp</kbd></span> <kbd>Tokenizer</kbd> <span>and</span> <kbd>Stemmer</kbd> <span>annotators to convert our filtered list of tokens into a list of</span> <em>stems</em><span>. Finally, we apply its</span> <kbd>Normalizer</kbd> <span>annotator, which converts the stems into lowercase by default. All of these stages are defined within a</span> <em>pipeline</em>, <span>which, as we saw in <a href="ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml" target="_blank">Chapter 4</a>, <em>Supervised Learning Using Apache Spark</em>, is an ordered list of machine learning and data transformation steps that is executed on a Spark dataframe.</span></li>
</ol>
<p style="padding-left: 60px">We <span>execute our pipeline on our dataset, resulting in a new dataframe called</span> <kbd>preprocessed_df</kbd><span> from which we keep only the relevant columns that are required for subsequent analysis and modelling, namely</span> <kbd>unit_id</kbd><span> (unique record identifier),</span> <kbd>text</kbd><span> (original raw textual content of the tweet),</span> <kbd>negative_sentiment_label</kbd><span> (our new label), and</span> <kbd>normalised_stems</kbd><span> (a <kbd>spark-nlp</kbd> array of filtered, stemmed, and normalized tokens as a result of our preprocessing pipeline), as shown in the following code:</span></p>
<pre style="padding-left: 60px">document_assembler = DocumentAssembler()<br/>   .setInputCol("concatenated_filtered_tokens")<br/>tokenizer = NLPTokenizer()<br/>   .setInputCols(["document"]).setOutputCol("tokens_2")<br/>stemmer = Stemmer().setInputCols(["tokens_2"])<br/>   .setOutputCol("stems")<br/>normalizer = Normalizer()<br/>   .setInputCols(["stems"]).setOutputCol("normalised_stems")<br/>pipeline = Pipeline(stages=[document_assembler, tokenizer, stemmer,<br/>   normalizer])<br/>pipeline_model = pipeline.fit(preprocessed_part_1_df)<br/>preprocessed_df = pipeline_model.transform(preprocessed_part_1_df)<br/>preprocessed_df.select("unit_id", "text", <br/>   "negative_sentiment_label", "normalised_stems")</pre>
<ol start="6">
<li><span>Before we can create feature vectors from our array of stemmed tokens using <kbd>MLlib</kbd>'s</span> native <span>feature extractors, there is one final preprocessing step. The column containing our stemmed tokens, namely</span> <kbd>normalised_stems</kbd>, <span>persists these tokens in a specialized <kbd>spark-nlp</kbd> array structure. We need to convert this <kbd>spark-nlp</kbd> array back into a standard list of tokens so that we may apply <kbd>MLlib</kbd>'s native TF–IDF algorithms to it. We achieve this by first</span> exploding <span>the <kbd>spark-nlp</kbd> array structure, which has the effect of creating a new dataframe observation for every element in this array. We then group our Spark dataframe by</span> <kbd>unit_id</kbd>, <span>which is the primary ke</span><span>y for each unique tweet, before aggregating the stems using the whitespace delimiter into a new</span> string <span>column called <kbd>tokens</kbd>. Finally, we apply the</span> <kbd>split</kbd> <span>function to this column to convert the aggregated string into a</span> list <span>of strings or tokens, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px">exploded_df = preprocessed_df<br/>   .withColumn("stems", explode("normalised_stems"))<br/>   .withColumn("stems", col("stems").getItem("result"))<br/>   .select("unit_id", "negative_sentiment_label", "text", "stems")<br/><br/>aggregated_df = exploded_df.groupBy("unit_id")<br/>   .agg(concat_ws(" ", collect_list(col("stems"))), <br/>      first("text"), first("negative_sentiment_label"))<br/>   .toDF("unit_id", "tokens", "text", "negative_sentiment_label")<br/>   .withColumn("tokens", split(col("tokens"), " ")<br/>      .cast("array&lt;string&gt;"))</pre>
<ol start="7">
<li><span>We are now ready to generate feature vectors from our list of filtered, stemmed, and normalized tokens! As discussed, we will be using the TF–IDF feature extractor to generate feature vectors rather than the basic bag of words approach. The TF–IDF feature extractor is native to <kbd>MLlib</kbd> and comes in two parts. First, we generate</span> the <strong>term frequency</strong> (<strong>TF</strong>) <span>feature vectors by passing our list of tokens into <kbd>MLlib</kbd>'s</span> <kbd>HashingTF</kbd> <span>transformer. We then</span> <em>fit</em> <span><kbd>MLlib</kbd>'s</span> <strong>inverse document frequency</strong> (<strong>IDF</strong>) <span>estimator to our dataframe containing the term frequency feature vectors, as shown in the following code. The result is a new Spark dataframe with our TF–IDF feature vectors contained in a column called</span> <kbd>features</kbd>:</li>
</ol>
<pre style="padding-left: 60px">hashingTF = HashingTF(inputCol="tokens", outputCol="raw_features",<br/>   numFeatures=280)<br/>features_df = hashingTF.transform(aggregated_df)<br/>idf = IDF(inputCol="raw_features", outputCol="features")<br/>idf_model = idf.fit(features_df)<br/>scaled_features_df = idf_model.transform(features_df)</pre>
<ol start="8">
<li><span>As we saw in <a href="ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml" target="_blank">Chapter 4</a>, <em>Supervised Learning Using Apache Spark</em>, since our label column is categorical in nature, we need to apply <kbd>MLlib</kbd>'s</span> <kbd>StringIndexer</kbd> <span>to it in order to identify and index all possible classifications. The result is a new Spark dataframe with an indexed label column called <kbd>"label"</kbd>, which is 0.0 if</span> <kbd>negative_sentiment_label</kbd><span> is</span> <kbd>true</kbd><span>, and 1.0 if</span> <kbd>negative_sentiment_label</kbd><span> is</span> <kbd>false</kbd>, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">indexer = StringIndexer(inputCol = "negative_sentiment_label", <br/>   outputCol = "label").fit(scaled_features_df)<br/>scaled_features_indexed_label_df = indexer.transform(scaled_features_df)</pre>
<ol start="9">
<li><span>We are now ready to create training and test dataframes in order to train and evaluate subsequent machine learning models. We achieve this as normal, using the</span> <kbd>randomSplit</kbd> <span>method (as shown in the following code), but in this case, 90% of all observations will go into our training dataframe, with the remaining 10% going into our test dataframe:</span></li>
</ol>
<pre style="padding-left: 60px">train_df, test_df = scaled_features_indexed_label_df<br/>   .randomSplit([0.9, 0.1], seed=12345)</pre>
<ol start="10">
<li>In this example, we will be training a supervised decision tree classifier (see <a href="ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml" target="_blank">Chapter 4</a>, <em>Supervised Learning Using Apache Spark</em>) in order to help us classify whether a given tweet is positive or negative in sentiment. As in <a href="ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml" target="_blank">Chapter 4</a>, <em>Supervised Learning Using Apache Spark</em>, we fit <kbd>MLlib</kbd>'s <kbd>DecisionTreeClassifier</kbd> estimator <span>to</span> our training dataframe in order to train our classification tree, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">decision_tree = DecisionTreeClassifier(featuresCol = 'features',<br/>   labelCol = 'label')<br/>decision_tree_model = decision_tree.fit(train_df)</pre>
<ol start="11">
<li><span>Now that we have a trained classification tree, we can apply it to our test dataframe in order to classify test tweets. As we did in <a href="ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml" target="_blank">Chapter 4</a>, <em>Supervised Learning Using Apache Spark</em>, we apply our trained classification tree to the test dataframe using the</span> <kbd>transform()</kbd> <span>method (as shown in the following code), and afterwards study its predicted classifications:</span></li>
</ol>
<pre style="padding-left: 60px">test_decision_tree_predictions_df = decision_tree_model<br/>   .transform(test_df)<br/>print("TEST DATASET PREDICTIONS AGAINST ACTUAL LABEL: ")<br/>test_decision_tree_predictions_df.select("prediction", "label",<br/>   "text").show(10, False)</pre>
<p style="padding-left: 90px"><span>For example, our decision tree classifier has predicted that the following tweets from our test dataframe are negative in sentiment:</span></p>
<ul>
<li style="padding-left: 90px"><span>"I need you...to be a better airline. ^LOL"</span></li>
<li style="padding-left: 90px">"if you can't guarantee parents will sit with their children, don't sell tickets with that promise"</li>
<li style="padding-left: 90px">"resolved and im sick and tired of waiting on you. I want my refund and I'd like to speak to someone about it."</li>
<li style="padding-left: 90px">"I would have loved to respond to your website until I saw the really long form. In business the new seats are bad"</li>
</ul>
<p style="padding-left: 90px">A human would <span>also</span><span> </span><span>probably classify these tweets as negative in sentiment! But more importantly, airlines can use this model and the tweets that it identifies to focus on areas for improvement. Based on this sample of tweets, such areas would include website usability, ticket marketing, and the time taken to process refunds.</span></p>
<ol start="12">
<li>Finally, in order to quantify the accuracy of our trained classification tree, let's compute its confusion matrix on the test data using the following code:</li>
</ol>
<pre style="padding-left: 60px">predictions_and_label = test_decision_tree_predictions_df<br/>   .select("prediction", "label").rdd<br/>metrics = MulticlassMetrics(predictions_and_label)<br/>print("N = %g" % test_decision_tree_predictions_df.count())<br/>print(metrics.confusionMatrix())</pre>
<p style="padding-left: 60px"><span>The resulting confusion matrix looks as follows:</span></p>
<table style="border-color: #000000;border-collapse: collapse;width: 631px;height: 321px" border="1">
<tbody>
<tr>
<td style="padding: 5px;width: 144px" class="CDPAlignRight CDPAlign">
<p style="padding-left: 180px"/>
</td>
<td style="padding: 5px;width: 206px" class="CDPAlignRight CDPAlign">
<p><strong>Predict <em>y </em>= 0 (Negative)</strong></p>
</td>
<td style="padding: 5px;width: 247px" class="CDPAlignRight CDPAlign">
<p><strong>Predict <em>y</em> = 1 (Non-Negative)</strong></p>
</td>
</tr>
<tr>
<td style="padding: 5px;width: 144px" class="CDPAlignRight CDPAlign">
<p><strong>Actual <em>y</em> = 0</strong></p>
<p><strong>(Negative)</strong></p>
</td>
<td style="padding: 5px;width: 206px" class="CDPAlignRight CDPAlign">
<p>725</p>
</td>
<td style="padding: 5px;width: 247px" class="CDPAlignRight CDPAlign">
<p>209</p>
</td>
</tr>
<tr>
<td style="padding: 5px;width: 144px" class="CDPAlignRight CDPAlign">
<p><strong>Actual <em>y</em> = 1</strong></p>
<p><strong>(Non-Negative)</strong></p>
</td>
<td style="padding: 5px;width: 206px" class="CDPAlignRight CDPAlign">
<p>244</p>
</td>
<td style="padding: 5px;width: 247px" class="CDPAlignRight CDPAlign">
<p>325</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p style="padding-left: 60px">We can interpret this confusion matrix as follows—out of a total of 1,503 test tweets, our model exhibits the following properties:</p>
<ul>
<li>Correctly classifies 725 tweets as negative in sentiment that are actually negative</li>
<li>Correctly classifies 325 tweets as non-negative in sentiment that are actually non-negative</li>
<li>Incorrectly classifies 209 tweets as non-negative in sentiment that are actually negative</li>
<li>Incorrectly classifies 244 tweets as negative in sentiment that are actually non-negative</li>
<li>Overall accuracy = 70%</li>
<li>Overall error rate = 30%</li>
<li>Sensitivity = 57%</li>
<li>Specificity = 78%</li>
</ul>
<p style="padding-left: 60px" class="mce-root">So, based on a default threshold value of 0.5 (which in this case study is fine because we have no preference over what type of error is better), our decision tree classifier has an overall accuracy rate of 70%, which is quite good!</p>
<ol start="13">
<li>For the sake of completeness, let's train a decision tree classifier, but using the feature vectors that are derived from the bag of words algorithm. Note that we already computed these feature vectors when we applied the <kbd>HashingTF</kbd> transformer to our preprocessed corpus to calculate the term frequency (TF) feature vectors. Therefore, we can just repeat our machine learning pipeline, but based only on the output of the <kbd>HashingTF</kbd> transformer instead, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Create Training and Test DataFrames based on the Bag of Words Feature Vectors<br/>bow_indexer = StringIndexer(inputCol = "negative_sentiment_label",<br/>   outputCol = "label").fit(features_df)<br/>bow_features_indexed_label_df = bow_indexer.transform(features_df)<br/>   .withColumnRenamed("raw_features", "features")<br/>bow_train_df, bow_test_df = bow_features_indexed_label_df<br/>   .randomSplit([0.9, 0.1], seed=12345)<br/><br/># Train a Decision Tree Classifier using the Bag of Words Feature Vectors<br/>bow_decision_tree = DecisionTreeClassifier(featuresCol = <br/>   'features', labelCol = 'label')<br/>bow_decision_tree_model = bow_decision_tree.fit(bow_train_df)<br/><br/># Apply the Bag of Words Decision Tree Classifier to the Test DataFrame and generate the Confusion Matrix<br/>bow_test_decision_tree_predictions_df = bow_decision_tree_model<br/>   .transform(bow_test_df)<br/>bow_predictions_and_label = bow_test_decision_tree_predictions_df<br/>   .select("prediction", "label").rdd<br/>bow_metrics = MulticlassMetrics(bow_predictions_and_label)<br/>print("N = %g" % bow_test_decision_tree_predictions_df.count())<br/>print(bow_metrics.confusionMatrix())</pre>
<p style="padding-left: 60px"><span>Note that the resulting confusion matrix is exactly the same as when we applied our decision tree classifier that had been trained on the</span> <em>scaled</em> <span>feature vectors using the</span> <kbd>IDF</kbd> <span>estimator (given the same random split seed and size of the training dataframe). This is because of the fact that our corpus of tweets is relatively small at 14,872 documents, and therefore the effect of scaling the</span> term frequency (<kbd>TF</kbd>) <span>feature vectors based on the frequency across the corpus will have a negligible impact on the predictive quality of this specific model.</span></p>
<ol start="14">
<li>A very useful feature provided by <kbd>MLlib</kbd> is the ability to save trained machine learning models to disk for later use. We can take advantage of this feature by saving our trained decision tree classifier to the local disk of our single-node development environment. In multi-node clusters, trained models may also be saved to a distributed file system, such as the Apache Hadoop Distributed File system (see <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>) by simply using the relevant file system prefix (for example <kbd>hdfs://&lt;HDFS NameNode URL&gt;/&lt;HDFS Path&gt;</kbd>), as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">bow_decision_tree_model.save('&lt;Target filesystem path to save MLlib Model&gt;')</pre>
<div class="packt_infobox"><br/>
Our trained decision tree classifier for performing sentiment analysis of airline tweets has also been pushed to the GitHub repository accompanying this book, and may be found in <kbd>chapter06/models/airline-sentiment-analysis-decision-tree-classifier</kbd>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have studied, implemented, and evaluated common algorithms that are used in natural language processing. We have preprocessed a corpus of documents using feature transformers and generated feature vectors from the resulting processed corpus using feature extractors. We have also applied these common NLP algorithms to machine learning. We trained and tested a sentiment analysis model that we used to predict the underlying sentiment of tweets so that organizations may improve their product and service offerings. In <a href="cad17bf3-6d9d-4486-a405-3d5103b072c5.xhtml" target="_blank">Chapter 8</a>, <em>Real-Time Machine Learning Using Apache Spark</em>, we will extend our sentiment analysis model to operate in real time using Spark Streaming and Apache Kafka.</p>
<p>In the next chapter, we will take a hands-on exploration through the exciting and cutting-edge world of deep learning!</p>


            </article>

            
        </section>
    </div>



  </body></html>