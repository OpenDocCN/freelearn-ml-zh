<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Feature Engineering</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">In recent years, engineers and executives have been attempting to implement <strong>machine learning</strong> (<strong>ML</strong>) and <strong>artificial intelligence</strong> (<strong>AI</strong>) to solve problems that, for the most part, have been solved using fairly manual methodologies. A great example would have to be advancements in <strong>natural language processing</strong> (<strong>NLP</strong>) and more specifically in</span> natural language generation and understanding. <span class="s1">Even more specifically, we point to AI systems that are able to read in raw text from a user (perhaps a disgruntled user of the latest smartphone) and can articulately and accurately respond with the prose of a human and the speed of a machine. In this chapter, we will be introducing topics of feature engineering, such as: </span></p>
<ul>
<li>Motivating examples of why feature engineering matters</li>
<li>Basic understanding of machine learning, including performance, evaluation</li>
<li>A detailed list of the chapters included in this book</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivating example – AI-powered communications</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">Meet Arty, our AI chat system that is able to handle and respond to incoming customer support requests, just as any of our human employees would. Arty is endowed with the knowledge of our company and is ready to go at a moment’s notice.</span></p>
<p><span class="s1">Here is how a sample dialogue between a human and an AI customer support system would transpire:<br/></span></p>
<table class="t1">
<tbody>
<tr>
<td class="td1">
<p class="p6"><strong><span class="s2">Human</span></strong></p>
</td>
<td class="td2">
<p class="p6"><strong><span class="s2">AI</span></strong></p>
</td>
</tr>
<tr>
<td class="td3">
<p class="p6"><span class="s2">Hello, my phone is broken.</span></p>
</td>
<td class="td4">
<p class="p6"><span class="s2">Sorry to hear that, how is it broken?</span></p>
</td>
</tr>
<tr>
<td class="td5">
<p class="p6"><span class="s2">It’s frozen and I can’t reset it.</span></p>
</td>
<td class="td6">
<p class="p6"><span class="s2">What kind of phone is it?</span></p>
</td>
</tr>
<tr>
<td class="td7">
<p class="p6"><span class="s2">The new iDroid 28</span></p>
</td>
<td class="td8">
<p class="p6"><span class="s2">Ahh, I see. Hold the power and volume down button for 20 seconds and it should reset.</span></p>
</td>
</tr>
<tr>
<td class="td5">
<p class="p6"><span class="s2">It worked, thanks!</span></p>
</td>
<td class="td6">
<p class="p6"><span class="s2">No problem, have a great day.</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="p4"><span class="s1">The reason that these types of systems are exciting and are disrupting major markets is the simplicity of such a complicated system. Let us break it down. On the surface, you might think, <em>what an easy problem!</em> The person has a simple problem with a simple solution.</span> <span class="s1">A request comes in and a response comes out. <em>Hello, my phone froze, what should I do?</em> Easy, just reset it. And sure, on the surface, that is what is happening here:</span></p>
<pre>from Arty import AI<br/>AI.respond_to("my phone froze, what should I do?")<br/>&gt;&gt; "reset it."</pre>
<p class="p4"><span class="s1">The tough part comes in when you look at it from the AI’s perspective. It hasn’t had the entire human experience that we have had. It hasn’t had the privilege to read <em>The Illiad</em> or even <em>Clifford the Big Red Dog</em> and learn to internalize their messages. The point is, the AI hasn’t had a lot of experience in reading things. This AI has probably been given a few hundred thousand (maybe even millions) of previous chat dialogues of people in the past and was told to</span> figure it out.</p>
<p><span class="s1">The following is a sample of data given to our AI system based on previous chat logs:</span></p>
<table class="t1" style="width: 861px;height: 372px">
<tbody>
<tr>
<td class="td9">
<p class="p6"><strong><span class="s2">Request</span></strong></p>
</td>
<td class="td10">
<p class="p6"><strong><span class="s2">Response</span></strong></p>
</td>
</tr>
<tr>
<td class="td11">
<p class="p6"><span class="s2">Helllo</span></p>
</td>
<td class="td12">
<p class="p6"><span class="s2">Hi, what seems to be the problem?</span></p>
</td>
</tr>
<tr>
<td class="td5">
<p class="p6"><span class="s2">My phone doesn’t work!!!!</span></p>
</td>
<td class="td6">
<p class="p6"><span class="s2">Oh noo!!!! What’s going on with it?</span></p>
</td>
</tr>
<tr>
<td class="td13">
<p class="p6">&gt;Hold <span class="s2">on, I have to go walk my dog. BRB.</span></p>
</td>
<td class="td14">
<p class="p6"><span class="s2">OK. I will wait.</span></p>
</td>
</tr>
<tr>
<td class="td13">
<p class="p6"><span class="s2">Hey.</span></p>
</td>
<td class="td14">
<p class="p6"><span class="s2">Hello. My name is Mark, how can I help?</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="p7"><span class="s1">The data is organized into two columns where the</span> <kbd>Request</kbd> <span class="s1">column represents what the end user types into a chat support dialogue. The next column,</span> <kbd>Response</kbd><span class="s1">, represents the customer support agent’s response to the incoming message.</span></p>
<p class="p4"><span class="s1">While reading over the thousands of typos, angry messages, and disconnected chats, the AI starts to think that it has this customer support thing down. Once this happens, the humans set the AI loose on new chats coming in. The humans, not realizing their mistake, start to notice that the AI hasn’t fully gotten the hang of this yet. The AI can’t seem to recognize even simple messages and keeps returning nonsensical responses. It’s easy to think that the AI just needs more time or more data, but these solutions are just band-aids to the bigger problem, and often do not even solve the issue in the first place.</span></p>
<p class="p4"><span class="s1">The underlying problem is likely that the data given to the AI in the form of raw text wasn’t good enough and the AI wasn’t able to pick up on the nuances of the English language. For example, some of the problems would likely include:</span></p>
<ul class="ol1">
<li class="li4"><span class="s1">Typos artificially expand the AI’s vocabulary without cause. <em>Helllo</em> and <em>hello</em> are two different words that are not related to each other.</span></li>
<li class="li4"><span class="s1">Synonyms mean nothing to the AI. Words such as <em>hello</em> and <em>hey</em> have no similarity and therefore make the problem artificially harder.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why feature engineering matters</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">Data scientists and machine learning engineers frequently gather data in order to solve a problem. Because the problem they are attempting to solve is often highly relevant and exists and occurs naturally in this messy world, the data that is meant to represent the problem can also end up being quite messy and unfiltered, and often incomplete.</span></p>
<p class="p4"><span class="s1">This is why in the past several years, positions with titles such as <em>Data Engineer</em> have been popping up. These engineers have the unique job of engineering pipelines and architectures designed to handle and transform raw data into something usable by the rest of the company, particularly the data scientists and machine learning engineers. This job is not only as important as the machine learning experts’ job of creating machine learning pipelines, it is often overlooked and undervalued.</span></p>
<p class="p4"><span class="s1">A survey conducted by data scientists in the field revealed that over 80% of their time was spent capturing, cleaning, and organizing data. The remaining less than 20% of their time was spent creating these machine learning pipelines that end up dominating the conversation. Moreover, these data scientists are spending most of their time preparing the data; more than 75% of them also reported that preparing data was the least enjoyable part of their process.</span></p>
<p>Here are the findings of the survey mentioned earlier:</p>
<p>Following is the graph of the what Data Scientist spend the most time doing:</p>
<div class="CDPAlignCenter CDPAlign"><img height="251" src="assets/842bd915-a8ce-4600-a5cf-58f6e238686d.png" width="251"/></div>
<p>As seen from the preceding graph, we breakup the Data Scientists's task in the following percentage :</p>
<ul>
<li><strong>Building training sets</strong>: 3%</li>
<li><strong>Cleaning and organizing data</strong>: 60%</li>
<li><strong>Collecting data for sets</strong>: 19%</li>
<li><strong>Mining data for patterns</strong>: 9%</li>
<li><strong>Refining algorithms</strong>: 5%</li>
</ul>
<p class="p4">A similar pie diagram for what is the least enjoyable part of data science:</p>
<div class="CDPAlignCenter CDPAlign"><img height="307" src="assets/3266eab3-da62-484c-975a-8eacc7f272c3.png" width="306"/></div>
<p>From the graph a similar poll for the least enjoyable part of data science revealed: </p>
<ul>
<li><strong>Building training sets</strong>: 10 %</li>
<li><strong>Cleaning and organizing data</strong>: 57%</li>
<li><strong>Collecting data sets</strong>: 21%</li>
<li><strong>Mining for data patterns</strong>: 3%</li>
<li><strong>Refining algorithms</strong>: 4%</li>
<li><strong>Others</strong>: 5%</li>
</ul>
<p class="p8"><span class="s1">The uppermost chart represents the percentage of time that data scientists spend on different parts of the process. Over 80% of a data scientists' time is spent preparing data for further use. The lower chart represents the percentage of those surveyed reporting their least enjoyable part of the process of data science. Over 75% of them report that preparing data is their least enjoyable part.</span></p>
<div class="p10 packt_tip"><span class="s3">Source of the data: <a href="https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/" target="_blank"><span class="s4">https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/</span></a></span>.</div>
<p class="p4"><span class="s1">A stellar data scientist knows that preparing data is not only so important that it takes up most of their time, they also know that it is an arduous process and can be unenjoyable. Far too often, we take for granted clean data given to us by machine learning competitions and academic sources. More than 90% of data, the data that is interesting, and the most useful, exists in this raw format, like in the AI chat system described earlier.</span></p>
<p class="p4"><span class="s1"><strong>Preparing data</strong> can be a vague phrase.</span> Preparing <span class="s1">takes into account capturing data, storing data, cleaning data, and so on. As seen in the charts shown earlier, a smaller, but still majority chunk of a data scientist's time is spent on cleaning and organizing data. It is in this process that our Data Engineers are the most useful to us. Cleaning refers to the process of transforming data into a format that can be easily interpreted by our cloud systems and databases. Organizing generally refers to a more radical transformation. Organizing tends to involve changing the entire format of the dataset into a much neater format, such as transforming raw chat logs into a tabular row/column structure.</span></p>
<p>Here is an illustration of <strong>Cleaning</strong> and <strong>Organizing</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="300" src="assets/7541d1c9-4f5c-41c7-8fe7-8f0528f84ec7.png" width="413"/><span class="s1"><span class="Apple-converted-space"> </span></span></div>
<p class="p7"><span class="s1">The top transformation represents cleaning up a sample of server logs that include both the data and a text explanation of what is occurring on the servers. Notice that while cleaning, the <strong>&amp;amp;</strong> character, which is a Unicode character, was transformed into a more readable ampersand (<strong>&amp;</strong>). The cleaning phase left the document pretty much in the same exact format as before. The bottom organizing transformation was a much more radical one. It turned the raw document into a row/column structure, in which each row represents a single action taken by the server and the columns represent attributes of the server action. In this case, the two attributes are <strong>D</strong></span><strong>ate</strong> and <strong>Text</strong>.</p>
<p class="p4"><span class="s1">Both cleaning and organizing fall under a larger category of data science, which just so happens to be the topic of this book, feature engineering.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is feature engineering?</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">Finally, the title of the book.</span></p>
<p class="p4"><span class="s1">Yes, folks, feature engineering will be the topic of this book. We will be focusing on the process of cleaning and organizing data for the purposes of machine learning pipelines. We will also go beyond these concepts and look at more complex transformations of data in the forms of mathematical formulas and neural understanding, but we are getting ahead of ourselves. Let’s start a high level.</span></p>
<div class="p4 packt_infobox"><span class="s1"><strong>Feature engineering</strong> is the process of transforming data into features that better represent the underlying problem, resulting in improved machine learning performance.</span></div>
<p class="p4"><span>To break this definition down a bit further, let's look at precisely what feature engineering entails:</span></p>
<ul>
<li class="p4"><span class="s1"><span class="s1"><strong>Process of transforming data</strong>: Note that we are not specifying raw data, unfiltered data, and so on. Feature engineering can be applied to data at any stage. Oftentimes, we will be applying feature engineering techniques to data that is already <em>processed</em> in the eyes of the data distributor. It is also important to mention that the data that we will be working with will usually be in a tabular format. The data will be organized into rows (observations) and columns (attributes). There will be times when we will start with data at its most raw form, such as in the examples of the server logs mentioned previously, but for the most part, we will deal with data already somewhat cleaned and organized.</span></span></li>
<li class="p4"><strong><span>F</span>eatures</strong>: The word features will obviously be used a lot in this book. At its most basic level, a feature is an attribute of data that is meaningful to the machine learning process. Many times we will be diagnosing tabular data and identifying which columns are features and which are merely attributes.</li>
<li class="p4"><strong>Better represent the underlying problem</strong>: The data that we will be working with will always serve to represent a specific problem in a specific domain. It is important to ensure that while we are performing these techniques, we do not lose sight of the bigger picture. We want to transform data so that it better represents the bigger problem at hand.</li>
<li class="p4"><strong>Resulting in improved machine learning performance</strong>: Feature engineering exists as a single part of the process of data science. As we saw, it is an important and oftentimes undervalued part. The eventual goal of feature engineering is to obtain data that our learning algorithms will be able to extract patterns from and use in order to obtain better results. We will talk in depth about machine learning metrics and results later on in this book, but for now, know that we perform feature engineering not only to obtain cleaner data, but to eventually use that data in our machine learning pipelines.</li>
</ul>
<p class="p4"><span class="s1">We know what you’re thinking, <em>why should I spend my time reading about a process that people say they do not enjoy doing?</em> We believe that many people do not enjoy the process of feature engineering because they often do not have the benefits of understanding the results of the work that they do.</span></p>
<p class="p4"><span class="s1">Most companies employ both data engineers and machine learning engineers. The data engineers are primarily concerned with the preparation and transformation of the data, while the machine learning engineers usually have a working knowledge of learning algorithms and how to mine patterns from already cleaned data.</span></p>
<p class="p4"><span class="s1">Their jobs are often separate but intertwined and iterative. The data engineers will present a dataset for the machine learning engineers, which they will claim they cannot get good results from, and ask the Data Engineers to try to transform the data further, and so on, and so forth. This process can not only be monotonous and repetitive, it can also hurt the bigger picture.</span></p>
<p class="p4"><span class="s1">Without having knowledge of both feature and machine learning engineering, the entire process might not be as effective as it could be. That’s where this book comes in. We will be talking about feature engineering and how it relates directly to machine learning. It will be a results-driven approach where we will deem techniques as helpful if, and only if, they can lead to a boost in performance. It is worth now diving a bit into the basics of data, the structure of data, and machine learning, to ensure standardization of terminology.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the basics of data and machine learning</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">When we talk about data, we are generally dealing with tabular data, that is, data that is organized into rows and columns. Think of this as being able to be opened in a spreadsheet technology such as Microsoft Excel. Each row of data, otherwise known as an <strong>observation</strong>, represents a single instance/example of a problem. If our data belongs to the domain of day-trading in the stock market, an observation might represent an hour’s worth of changes in the overall market and price.</span></p>
<p class="p4"><span class="s1">For example, when dealing with the domain of network security, an observation could represent a possible attack or a packet of data sent over a wireless system.</span></p>
<p><span class="s1">The following shows sample tabular data in the domain of cyber security and more specifically, network intrusion:</span></p>
<table class="t1">
<tbody>
<tr>
<td class="td15">
<p class="p11"><strong><span class="s2">DateTime</span></strong></p>
</td>
<td class="td15">
<p class="p11"><strong><span class="s2">Protocol</span></strong></p>
</td>
<td class="td16">
<p class="p11"><strong><span class="s2">Urgent</span></strong></p>
</td>
<td class="td15">
<p class="p11"><strong><span class="s2">Malicious</span></strong></p>
</td>
</tr>
<tr>
<td class="td17">
<p class="p6"><span class="s2">June 2nd, 2018</span></p>
</td>
<td class="td17">
<p class="p6"><span class="s2">TCP</span></p>
</td>
<td class="td18">
<p class="p6"><span class="s2">FALSE</span></p>
</td>
<td class="td17">
<p class="p6"><span class="s2">TRUE</span></p>
</td>
</tr>
<tr>
<td class="td19">
<p class="p6"><span class="s2">June 2nd, 2018</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">HTTP</span></p>
</td>
<td class="td20">
<p class="p6"><span class="s2">TRUE</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">TRUE</span></p>
</td>
</tr>
<tr>
<td class="td21">
<p class="p6"><span class="s2">June 2nd, 2018</span></p>
</td>
<td class="td21">
<p class="p6"><span class="s2">HTTP</span></p>
</td>
<td class="td22">
<p class="p6"><span class="s2">TRUE</span></p>
</td>
<td class="td21">
<p class="p6"><span class="s2">FALSE</span></p>
</td>
</tr>
<tr>
<td class="td19">
<p class="p6"><span class="s2">June 3rd, 2018</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">HTTP</span></p>
</td>
<td class="td20">
<p class="p6"><span class="s2">FALSE</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">TRUE</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="p8"><span class="s1">We see that each row or observation consists of a network connection and we have four attributes of the observation:</span> <kbd>DateTime</kbd>, <kbd>Protocol</kbd>, <kbd>Urgent</kbd>, and <kbd>Malicious</kbd><span class="s1">. While we will not dive into these specific attributes, we will simply notice the structure of the data given to us in a tabular format.</span></p>
<p class="p4"><span class="s1">Because we will, for the most part, consider our data to be tabular, we can also look at specific instances where the matrix of data has only one column/attribute. For example, if we are building a piece of software that is able to take in a single image of a room and output whether or not there is a human in that room. The data for the input might be represented as a matrix of a single column where the single column is simply a URL to a photo of a room and nothing else.</span></p>
<p>For example, considering the following table of table that has only a single column titled, <kbd>Photo URL</kbd>. The values of the table are URLs (these are fake and do not lead anywhere and are purely for example) of photos that are relevant to the data scientist:</p>
<table class="t1" style="width: 865px;height: 403px">
<tbody>
<tr>
<td class="td23">
<p class="p12"><span class="s2">Photo URL</span></p>
</td>
</tr>
<tr>
<td class="td24">
<p class="p13"><span class="s5"><a href="http://photo-storage.io/room/1">http://photo-storage.io/room/1</a></span></p>
</td>
</tr>
<tr>
<td class="td25">
<p class="p13"><span class="s5"><a href="http://photo-storage.io/room/2">http://photo-storage.io/room/2</a></span></p>
</td>
</tr>
<tr>
<td class="td26">
<p class="p13"><span class="s5"><a href="http://photo-storage.io/room/3">http://photo-storage.io/room/3</a></span></p>
</td>
</tr>
<tr>
<td class="td25">
<p class="p13"><span class="s5"><a href="http://photo-storage.io/room/4">http://photo-storage.io/room/4</a></span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="p8"><span class="s1">The data that is inputted into the system might only be a single column, such as in this case. In our ability to create a system that can analyze images, the input might simply be a URL to the image in question. It would be up to us as data scientists to engineer features from the URL.</span></p>
<p class="p4"><span class="s1">As data scientists, we must be ready to ingest and handle data that might be large, small, wide, narrow (in terms of attributes), sparse in completion (there might be missing values), and be ready to utilize this data for the purposes of machine learning.</span> <span class="s1">Now’s a good time to talk more about that. Machine learning algorithms belong to a class of algorithms that are defined by their ability to extract and exploit patterns in data to accomplish a task based on historical training data. Vague, right? machine learning can handle many types of tasks, and therefore we will leave the definition of machine learning as is and dive a bit deeper.</span></p>
<p class="p4"><span class="s1">We generally separate machine learning into two main types, supervised and unsupervised learning. Each type of machine learning algorithm can benefit from feature engineering, and therefore it is important that we understand each type.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">Oftentimes, we hear about feature engineering in the specific context of supervised learning, otherwise known as predictive analytics. Supervised learning algorithms specifically deal with the task of predicting a value, usually one of the attributes of the data, using the other attributes of the data. Take, for example, the dataset representing the network intrusion:</span></p>
<table class="t1">
<tbody>
<tr>
<td class="td15">
<p class="p11"><strong><span class="s2">DateTime</span></strong></p>
</td>
<td class="td15">
<p class="p11"><strong><span class="s2">Protocol</span></strong></p>
</td>
<td class="td16">
<p class="p11"><strong><span class="s2">Urgent</span></strong></p>
</td>
<td class="td15">
<p class="p11"><strong><span class="s2">Malicious</span></strong></p>
</td>
</tr>
<tr>
<td class="td17">
<p class="p6"><span class="s2">June 2nd, 2018</span></p>
</td>
<td class="td17">
<p class="p6"><span class="s2">TCP</span></p>
</td>
<td class="td18">
<p class="p6"><span class="s2">FALSE</span></p>
</td>
<td class="td17">
<p class="p6"><span class="s2">TRUE</span></p>
</td>
</tr>
<tr>
<td class="td19">
<p class="p6"><span class="s2">June 2nd, 2018</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">HTTP</span></p>
</td>
<td class="td20">
<p class="p6"><span class="s2">TRUE</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">TRUE</span></p>
</td>
</tr>
<tr>
<td class="td21">
<p class="p6"><span class="s2">June 2nd, 2018</span></p>
</td>
<td class="td21">
<p class="p6"><span class="s2">HTTP</span></p>
</td>
<td class="td22">
<p class="p6"><span class="s2">TRUE</span></p>
</td>
<td class="td21">
<p class="p6"><span class="s2">FALSE</span></p>
</td>
</tr>
<tr>
<td class="td19">
<p class="p6"><span class="s2">June 3rd, 2018</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">HTTP</span></p>
</td>
<td class="td20">
<p class="p6"><span class="s2">FALSE</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">TRUE</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>This is the same dataset as before, but let's dissect it further in the context of predictive analytics.</p>
<p class="p4"><span class="s1">Notice that we have four attributes of this dataset:</span> <kbd>DateTime</kbd>, <kbd>Protocol</kbd>, <kbd>Urgent</kbd>, and <kbd>Malicious</kbd>. Suppose now that the malicious attribute contains values that represent whether or not the observation was a malicious intrusion attempt. So in our ver<span class="s1">y small dataset of four network connections, the first, second, and fourth connection were malicious attempts to intrude a network.</span></p>
<p class="p4"><span class="s1">Suppose further that given this dataset, our task is to be able to take in three of the attributes (<kbd>datetime</kbd>, <kbd>protocol</kbd>, and <kbd>urgent</kbd>) and be able to accurately predict the value of malicious. In laymen’s terms, we want a system that can map the values of <kbd>datetime</kbd>, <kbd>protocol</kbd>, and <kbd>urgent</kbd> to the values in malicious. This is exactly how a supervised learning problem is set up:</span></p>
<pre>Network_features = pd.DataFrame({'datetime': ['6/2/2018', '6/2/2018', '6/2/2018', '6/3/2018'], 'protocol': ['tcp', 'http', 'http', 'http'], 'urgent': [False, True, True, False]})<br/>Network_response = pd.Series([True, True, False, True])<br/>Network_features<br/>&gt;&gt;<br/> datetime protocol  urgent
0  6/2/2018      tcp   False
1  6/2/2018     http    True
2  6/2/2018     http    True
3  6/3/2018     http   False<br/>Network_response<br/>&gt;&gt;<br/> 0     True
1     True
2    False
3     True
dtype: bool</pre>
<p class="p4"><span class="s1">When we are working with supervised learning, we generally call the attribute (usually only one of them, but that is not necessary) of the dataset that we are attempting to predict the response of. The remaining attributes of the dataset are then called the <strong>features</strong>.</span></p>
<p class="p4"><span class="s1">Supervised learning can also be considered the class of algorithms attempting to exploit the structure in data. By this, we mean that the machine learning algorithms try to extract patterns in usually very nice and neat data. As discussed earlier, we should not always expect data to come in tidy; this is where feature engineering comes in.</span></p>
<p class="p4"><span class="s1">But if we are not predicting something, what good is machine learning you may ask? I’m glad you did. Before machine learning can exploit the structure of data, sometimes we have to alter or even create structure. That’s where unsupervised learning becomes a valuable tool.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">Supervised learning is all about making predictions. We utilize features of the data and use them to make informative predictions about the response of the data. If we aren’t making predictions by exploring structure, we are attempting to extract structure from our data. We generally do so by applying mathematical transformations to numerical matrix representations of data or iterative procedures to obtain new sets of features.</span></p>
<p class="p4"><span class="s1">This concept can be a bit more difficult to grasp than supervised learning, and so I will present a motivating example to help elucidate how this all works.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning example – marketing segments</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">Suppose we are given a large (one million rows) dataset where each row/observation is a single person with basic demographic information (age, gender, and so on) as well as the number of items purchased, which represents how many items this person has bought from a particular store:</span></p>
<table class="t1">
<tbody>
<tr>
<td class="td27">
<p class="p11"><strong><span class="s2">Age</span></strong></p>
</td>
<td class="td28">
<p class="p11"><strong><span class="s2">Gender</span></strong></p>
</td>
<td class="td27">
<p class="p11"><strong><span class="s2">Number of items purchased</span></strong></p>
</td>
</tr>
<tr>
<td class="td29">
<p class="p6"><span class="s2">25</span></p>
</td>
<td class="td30">
<p class="p6"><span class="s2">F</span></p>
</td>
<td class="td29">
<p class="p6"><span class="s2">1</span></p>
</td>
</tr>
<tr>
<td class="td31">
<p class="p6"><span class="s2">28</span></p>
</td>
<td class="td32">
<p class="p6"><span class="s2">F</span></p>
</td>
<td class="td31">
<p class="p6"><span class="s2">23</span></p>
</td>
</tr>
<tr>
<td class="td33">
<p class="p6"><span class="s2">61</span></p>
</td>
<td class="td34">
<p class="p6"><span class="s2">F</span></p>
</td>
<td class="td33">
<p class="p6"><span class="s2">3</span></p>
</td>
</tr>
<tr>
<td class="td31">
<p class="p6"><span class="s2">54</span></p>
</td>
<td class="td32">
<p class="p6"><span class="s2">M</span></p>
</td>
<td class="td31">
<p class="p6"><span class="s2">17</span></p>
</td>
</tr>
<tr>
<td class="td33">
<p class="p6"><span class="s2">51</span></p>
</td>
<td class="td34">
<p class="p6"><span class="s2">M</span></p>
</td>
<td class="td33">
<p class="p6"><span class="s2">8</span></p>
</td>
</tr>
<tr>
<td class="td31">
<p class="p6"><span class="s2">47</span></p>
</td>
<td class="td32">
<p class="p6"><span class="s2">F</span></p>
</td>
<td class="td31">
<p class="p6"><span class="s2">3</span></p>
</td>
</tr>
<tr>
<td class="td33">
<p class="p6"><span class="s2">27</span></p>
</td>
<td class="td34">
<p class="p6"><span class="s2">M</span></p>
</td>
<td class="td33">
<p class="p6"><span class="s2">22</span></p>
</td>
</tr>
<tr>
<td class="td31">
<p class="p6"><span class="s2">31</span></p>
</td>
<td class="td32">
<p class="p6"><span class="s2">F</span></p>
</td>
<td class="td31">
<p class="p6"><span class="s2">14</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="p8"><span class="s1">This is a sample of our marketing dataset where each row represents a single customer with three basic attributes about each person. Our goal will be to segment this dataset into types or <strong>clusters</strong> of people so that the company performing the analysis can understand the customer profiles much better.</span></p>
<p class="p4"><span class="s1">Now, of course, We’ve only shown <span>8</span> out of one million rows, which can be daunting. Of course, we can perform basic descriptive statistics on this dataset and get averages, standard deviations, and so on of our numerical columns; however, what if we wished to segment these one million people into different <strong>types</strong> so that the marketing department can have a much better sense of the types of people who shop and create more appropriate advertisements for each segment?</span></p>
<p class="p4"><span class="s1">Each type of customer would exhibit particular qualities that make that segment unique. For example, they may find that 20% of their customers fall into a category they like to call young and wealthy that are generally younger and purchase several items.</span></p>
<p class="p4"><span class="s1">This type of analysis and the creation of these types can fall under a specific type of unsupervised learning called <strong>clustering</strong>. We will discuss this machine learning algorithm in further detail later on in this book, but for now, clustering will create a new feature that separates out the people into distinct types or clusters:</span></p>
<table class="t1">
<tbody>
<tr>
<td class="td35">
<p class="p11"><strong><span class="s2">Age</span></strong></p>
</td>
<td class="td35">
<p class="p11"><strong><span class="s2">Gender</span></strong></p>
</td>
<td class="td36">
<p class="p11"><strong><span class="s2">Number of items purchased</span></strong></p>
</td>
<td class="td35">
<p class="p11"><strong><span class="s2">Cluster</span></strong></p>
</td>
</tr>
<tr>
<td class="td37">
<p class="p6"><span class="s2">25</span></p>
</td>
<td class="td37">
<p class="p6"><span class="s2">F</span></p>
</td>
<td class="td38">
<p class="p6"><span class="s2">1</span></p>
</td>
<td class="td37">
<p class="p6"><span class="s2">6</span></p>
</td>
</tr>
<tr>
<td class="td19">
<p class="p6"><span class="s2">28</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">F</span></p>
</td>
<td class="td20">
<p class="p6"><span class="s2">23</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">1</span></p>
</td>
</tr>
<tr>
<td class="td21">
<p class="p6"><span class="s2">61</span></p>
</td>
<td class="td21">
<p class="p6"><span class="s2">F</span></p>
</td>
<td class="td22">
<p class="p6"><span class="s2">3</span></p>
</td>
<td class="td21">
<p class="p6"><span class="s2">3</span></p>
</td>
</tr>
<tr>
<td class="td19">
<p class="p6"><span class="s2">54</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">M</span></p>
</td>
<td class="td20">
<p class="p6"><span class="s2">17</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">2</span></p>
</td>
</tr>
<tr>
<td class="td21">
<p class="p6"><span class="s2">51</span></p>
</td>
<td class="td21">
<p class="p6"><span class="s2">M</span></p>
</td>
<td class="td22">
<p class="p6"><span class="s2">8</span></p>
</td>
<td class="td21">
<p class="p6"><span class="s2">3</span></p>
</td>
</tr>
<tr>
<td class="td19">
<p class="p6"><span class="s2">47</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">F</span></p>
</td>
<td class="td20">
<p class="p6"><span class="s2">3</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">8</span></p>
</td>
</tr>
<tr>
<td class="td21">
<p class="p6"><span class="s2">27</span></p>
</td>
<td class="td21">
<p class="p6"><span class="s2">M</span></p>
</td>
<td class="td22">
<p class="p6"><span class="s2">22</span></p>
</td>
<td class="td21">
<p class="p6"><span class="s2">5</span></p>
</td>
</tr>
<tr>
<td class="td19">
<p class="p6"><span class="s2">31</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">F</span></p>
</td>
<td class="td20">
<p class="p6"><span class="s2">14</span></p>
</td>
<td class="td19">
<p class="p6"><span class="s2">1</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="p8"><span class="s1">This shows our customer dataset after a clustering algorithm has been applied. Note the new column at the end called</span> <kbd>cluster</kbd> <span class="s1">that represents the types of people that the algorithm has identified. The idea is that the people who belong to <span>similar</span> clusters <em>behave</em> similarly in regards to the data (have similar ages, genders, purchase behaviors). Perhaps cluster six might be renamed as <em>young buyers</em>.</span></p>
<p class="p4"><span class="s1">This example of clustering shows us why sometimes we aren’t concerned with predicting anything, but instead wish to understand our data on a deeper level by adding new and interesting features, or even removing irrelevant features.</span></p>
<div class="p4 packt_infobox"><span class="s1">Note that we are referring to every column as a feature because there is no response in unsupervised learning since there is no prediction occurring.</span></div>
<p class="p4"><span class="s1">It’s all starting to make sense now, isn’t it? These features that we talk about repeatedly are what this book is primarily concerned with. Feature engineering involves the understanding and transforming of features in relation to both unsupervised and supervised learning.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation of machine learning algorithms and feature engineering procedures</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">It is important to note that in literature, oftentimes there is a stark contrast between the terms <em>features</em> and <em>attributes</em>. The term <strong>attribute</strong> is generally given to columns in tabular data, while the term <strong>feature</strong> is generally given only to attributes that contribute to the success of machine learning algorithms. That is to say, some attributes can be unhelpful or even hurtful to our machine learning systems. For example, when predicting how long a used car will last before requiring servicing, the color of the car will probably not very indicative of this value.</span></p>
<p class="p4"><span class="s1">In this book, we will generally refer to all columns as features until they are proven to be unhelpful or hurtful. When this happens, we will usually cast those attributes aside in the code. It is extremely important, then, to consider the basis for this decision. How does one evaluate a machine learning system and then use this evaluation to perform feature engineering?</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of feature engineering procedures – can anyone really predict the weather?</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">Consider a machine learning pipeline that was built to predict the weather. For the sake of simplicity in our introduction chapter, assume that our algorithm takes in atmospheric data directly from sensors and is set up to predict between one of two values, <em>sun</em> or <em>rain</em>. This pipeline is then, clearly, a classification pipeline that can only spit out one of two answers. We will run this algorithm at the beginning of every day. If the algorithm outputs <em>sun</em> and the day is mostly sunny, the algorithm was correct, likewise, if the algorithm predicts <em>rain</em> and the day is mostly rainy, the algorithm was correct. In any other instance, the algorithm would be considered incorrect. If we run the algorithm every day for a month, we would obtain nearly 30 values of the predicted weather and the actual, observed weather. We can calculate an accuracy of the algorithm. Perhaps the algorithm predicted correctly for 20 out of the 30 days, leading us to label the algorithm with a two out of three or about 67% accuracy. Using this standardized value or accuracy, we could tweak our algorithm and see if the accuracy goes up or down.</span></p>
<p class="p4"><span class="s1">Of course, this is an oversimplification, but the idea is that for any machine learning pipeline, it is essentially useless if we cannot evaluate its performance using a set of standard metrics and therefore, feature engineering as applied to the bettering of machine learning, is impossible without said evaluation procedure. Throughout this book, we will revisit this idea of evaluation; however, let’s talk briefly about how, in general, we will approach this idea.</span></p>
<p class="p4"><span class="s1">When presented with a topic in feature engineering, it will usually involve transforming our dataset (as per the definition of feature engineering). In order to definitely say whether or not a particular feature engineering procedure has helped our machine learning algorithm, we will follow the steps detailed in the following section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Steps to evaluate a feature engineering procedure</h1>
                </header>
            
            <article>
                
<p>Here are the steps to evaluate a feature engineering procedure:</p>
<ol>
<li>Obtain a baseline performance of the machine learning model before applying any feature engineering procedures</li>
<li>Apply feature engineering and combinations of feature engineering procedures</li>
<li>For each application of feature engineering, obtain a performance measure and compare it to our baseline performance</li>
<li>If the delta (change in) performance precedes a threshold (usually defined by the human), we deem that procedure helpful and apply it to our machine learning pipeline</li>
<li>This change in performance will usually be measured as a percentage (if the baseline went from 40% accuracy to 76% accuracy, that is a 90% improvement)</li>
</ol>
<p class="p4"><span class="s1">In terms of performance, this idea varies between machine learning algorithms. Most good primers on machine learning will tell you that there are dozens of accepted metrics when practicing data science.</span></p>
<p class="p4"><span class="s1">In our case, because the focus of this book is not necessarily on machine learning and rather on the understanding and transformation of features, we will use baseline machine learning algorithms and associated baseline metrics in order to evaluate the feature engineering procedures.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating supervised learning algorithms</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">When performing predictive modeling, otherwise known as <strong>supervised learning</strong>, performance is directly tied to the model’s ability to exploit structure in the data and use that structure to make appropriate predictions. In general, we can further break down supervised learning into two more specific types, <strong>classification</strong> (predicting qualitative responses) and <strong>regression</strong> (predicting quantitative responses).</span></p>
<p class="p4"><span class="s1">When we are evaluating classification problems, we will directly calculate the accuracy of a logistic regression model using a five-fold cross-validation:</span></p>
<pre class="mce-root"># Example code for evaluating a classification problem<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_val_score<br/>X = some_data_in_tabular_format<br/>y = response_variable<br/>lr = LinearRegression()<br/>scores = cross_val_score(lr, X, y, cv=5, scoring='accuracy')<br/>scores<br/>&gt;&gt; [.765, .67, .8, .62, .99]</pre>
<p class="p4"><span class="s1">Similarly, when evaluating a regression problem, we will use the <strong>mean squared error</strong> (<strong>MSE</strong>) of a linear regression using a five-fold cross-validation:</span></p>
<pre class="mce-root"># Example code for evaluating a regression problem<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.model_selection import cross_val_score<br/>X = some_data_in_tabular_format<br/>y = response_variable<br/>lr = LinearRegression()<br/>scores = cross_val_score(lr, X, y, cv=5, scoring='mean_squared_error')<br/>scores<br/>&gt;&gt; [31.543, 29.5433, 32.543, 32.43, 27.5432]</pre>
<p class="p4"><span class="s1">We will use these two linear models instead of newer, more advanced models for their speed and their low variance. This way, we can be surer that any increase in performance is directly related to the feature engineering procedure and not to the model’s ability to pick up on obscure and hidden patterns.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating unsupervised learning algorithms</h1>
                </header>
            
            <article>
                
<p class="p4"><span class="s1">This is a bit trickier. Because unsupervised learning is not concerned with predictions, we cannot directly evaluate performance based on how well the model can predict a value. That being said, if we are performing a cluster analysis, such as in the previous marketing segmentation example, then we will usually utilize the <strong>silhouette coefficient</strong> (a measure of separation and cohesion of clusters between -1 and 1) and some human-driven analysis to decide if a feature engineering procedure has improved model performance or if we are merely wasting our time.</span></p>
<p>Here is an example of using Python and scikit-learn to import and calculate the silhouette coefficient for some fake data:</p>
<pre><span>attributes = tabular_data<br/>cluster_labels = outputted_labels_from_clustering<br/><br/>from </span>sklearn.metrics import silhouette_score<br/>silhouette_score(attributes, cluster_labels)</pre>
<p>We will spend much more time on unsupervised learning later on in this book as it becomes more relevant. Most of our examples will revolve around predictive analytics/supervised learning.</p>
<div class="p17 packt_tip"><span class="s1">It is important to remember that the reason we are standardizing algorithms and metrics is so that we may showcase the power of feature engineering and so that you may repeat our procedures with success. Practically, it is conceivable that you are optimizing for something other than accuracy (such as a true positive rate, for example) and wish to use decision trees instead of logistic regression. This is not only fine but encouraged. You should always remember though to follow the steps to evaluating a feature engineering procedure and compare baseline and post-engineering performance.</span></div>
<div>
<p>It is possible that you are not reading this book for the purposes of improving machine learning performance. Feature engineering is useful in other domains such as hypothesis testing and general statistics. In a few examples in this book, we will be taking a look at feature engineering and data transformations as applied to a statistical significance of various statistical tests. We will be exploring metrics such as R<sup>2 </sup>and p-values in order to make judgements about how our procedures are helping.</p>
</div>
<p>In general, we will quantify the benefits of feature engineering in the context of three categories:</p>
<ul>
<li><strong>Supervised learning</strong>: Otherwise known as <strong>predictive analytics</strong>
<ul>
<li>Regression analysis—predicting a <em>quantitative</em> variable:
<ul>
<li>Will utilize MSE as our primary metric of measurement</li>
</ul>
</li>
<li>Classification analysis—predicting a <em>qualitative</em> variable
<ul>
<li>Will utilize accuracy as our primary metric of measurement</li>
</ul>
</li>
</ul>
</li>
<li><strong>Unsupervised learning</strong>: Clustering—the assigning of meta-attributes as denoted by the behavior of data:
<ul>
<li>Will utilize the silhouette coefficient as our primary metric of measurement</li>
</ul>
</li>
<li><strong>Statistical testing</strong>: Using correlation coefficients, t-tests, chi-squared tests, and others to evaluate and quantify the usefulness of our raw and transformed data</li>
</ul>
<p><span>In the following few sections, we will look at what will be covered throughout this book.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature understanding – what’s in my dataset?</h1>
                </header>
            
            <article>
                
<p class="p3">In our first subtopic, we will start to build our fundamentals in dealing with data. By understanding the data in front of us, we can start to have a better idea of where to go next. We will begin to explore the different types of data out there as well as how to recognize the type of data inside datasets. We will look at datasets from several domains and identify how they are different from each other and how they are similar to each other. Once we are able to comfortably examine data and identify the characteristics of different attributes, we can start to understand the types of transformations that are allowed and that promise to improve our machine learning algorithms.</p>
<p>Among the different methods of understanding, we will be looking at:</p>
<ul>
<li>Structured versus unstructured data</li>
<li>The four levels of data</li>
<li>Identifying missing data values</li>
<li>Exploratory data analysis</li>
<li>Descriptive statistics</li>
<li>Data visualizations</li>
</ul>
<p>We will begin at a basic level by identifying the structure of, and then the types of data in front of us. Once we are able to understand what the data is, we can start to fix problems with the data. As an example, we must know how much of our data is missing and what to do when we have missing data.</p>
<p>Make no mistake, data visualizations, descriptive statistics, and exploratory data analysis are all a part of feature engineering. We will be exploring each of these procedures from the perspective of the machine learning engineer. Each of these procedures has the ability to enhance our machine learning pipelines and we will test and alter hypotheses about our data using them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature improvement – cleaning datasets</h1>
                </header>
            
            <article>
                
<p>In this topic, we take the results of our understanding of the data and use them in order to clean the dataset. Much of this book will flow in such a way, using results from previous sections to be able to work on current sections. In feature improvement, our understanding will allow us to begin our first manipulations of datasets. We will be using mathematical transformations to enhance the given data, but not remove or insert any new attributes (this is for the next chapters).</p>
<p>We will explore several topics in this section, including:</p>
<ul>
<li>Structuring unstructured data</li>
<li>Data imputing—inserting data where there was not a data before (missing data)</li>
<li>Normalization of data:<br/>
<ul>
<li>Standardization (known as z-score normalization)</li>
<li>Min-max scaling</li>
<li>L1 and L2 normalization (projecting into different spaces, fun stuff)</li>
</ul>
</li>
</ul>
<p>By this point in the book, we will be able to identify whether our data has a <em>structure</em> or not. That is, whether our data is in a nice, tabular format. If it is not, this chapter will give us the tools to transform that data into a more tabular format. This is imperative when attempting to create machine learning pipelines.</p>
<p>Data imputing is a particularly interesting topic. The ability to fill in data where data was missing previously is trickier than it sounds. We will be proposing all kinds of solutions from the very, very easy, merely removing the column altogether, boom no more missing data, to the interestingly complex, using machine learning on the rest of the features to fill in missing spots. Once we have filled in a bulk of our missing data, we can then measure how that affected our machine learning algorithms.</p>
<p>Normalization uses (generally simple) mathematical tools used to change the scaling of our data. Again, this ranges from the easy, turning miles into feet or pounds into kilograms, to the more difficult, such as projecting our data onto the unit sphere (more on that to come).</p>
<p>This chapter and remaining chapters will be much more heavily focused on our quantitative feature engineering procedure evaluation flow. Nearly every single time we look at a new dataset or feature engineering procedure, we will put it to the test. We will be grading the performance of various feature engineering methods on the merits of machine learning performance, speed, and other metrics. This text should only be used as a reference and not as a guide to select with feature engineering the procedures you are allowed to <strong>ignore</strong> based on difficulty and change in performance. Every new data task comes with its own caveats and may require different procedures than the previous data task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature selection – say no to bad attributes</h1>
                </header>
            
            <article>
                
<p>By this chapter, we will have a<span> </span>level<span> </span>of comfort when dealing with new datasets. We will have under our belt the abilities to<span> </span>understand and clean the data<span> </span>in front of us. Once we are able to work with the data given to us, we can start to make big decisions such as,<span> </span><em>at what point is a feature actually an attribute</em>. Recall that by this distinction, feature versus attribute, the question really is, <em>which columns are not helping my ML pipeline and therefore are hurting my pipeline and should be removed?</em> This chapter focuses on techniques used to make the decision of which attributes to get rid of in our dataset. We will explore several statistical and iterative processes that will aid us in this decision.</p>
<p>Among these processes are:</p>
<ul>
<li>Correlation coefficients</li>
<li>Identifying and removing multicollinearity</li>
<li>Chi-squared tests</li>
<li>Anova tests</li>
<li>Interpretation of p-values</li>
<li>Iterative feature selection</li>
<li>Using machine learning to measure entropy and information gain</li>
</ul>
<p>All of these procedures will attempt to suggest the removal of features and will give different reasons for doing so. Ultimately, it will be up to us, the data scientists, to make the final call over which features will be allowed to remain and contribute to our machine learning algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature construction – can we build it?</h1>
                </header>
            
            <article>
                
<p>While in previous chapters we focused heavily on removing features that were not helping us with our machine learning pipelines, this chapter will look at techniques in creating brand new features and placing them correctly within our dataset. These new features will ideally hold new information and generate new patterns that ML pipelines will be able to exploit and use to increase performance.</p>
<p>These created features can come from many places. Oftentimes, we will create new features out of existing features given to us. We can create new features by applying transformations to existing features and placing the resulting vectors alongside their previous counterparts. We will also look at adding new features from separate party systems. As an example, if we are working with data attempting to cluster people based on shopping behaviors, then we might benefit from adding in census data that is separate from the corporation and their purchasing data. However, this will present a few problems:</p>
<ul>
<li>If the census is aware of 1,700 Jon does and the corporation only knows 13, how do we know which of the 1,700 people match up to the 13? This is called <em>entity matching</em></li>
<li>The census data would be quite large and entity matching would take a very long time</li>
</ul>
<p>These problems and more make for a fairly difficult procedure but oftentimes create a very dense and data-rich environment.</p>
<p>In this chapter, we will take some time to talk about the manual creation of features through highly unstructured data. Two big examples are text and images. These pieces of data by themselves are incomprehensible to machine learning and artificial intelligence pipelines, so it is up to us to manually create features that represent the images/pieces of text. As a simple example, imagine <span>that we are making the basics of a self-driving car and to start, we want to make a model that can take in an image of what the car is seeing in front of it and decide whether or not it should stop. The raw image is not good enough because a machine learning algorithm would have no idea what to do with it. We have to manually construct features out of it. Given this raw image, we can split it up in a few ways:</span></p>
<ul>
<li>We could consider the color intensity of each pixel and consider each pixel an attribute:
<ul>
<li>For example, if the camera of the car produces images of 2,048 x 1,536 pixels, we would have 3,145,728 columns</li>
</ul>
</li>
<li>We could consider each row of pixels as an attribute and the average color of each row being the value:
<ul>
<li>In this case, there would only be 1,536 rows</li>
</ul>
</li>
<li>We could project this image into space where features represent objects within the image. This is the hardest of the three and would look something like this:</li>
</ul>
<table>
<tbody>
<tr>
<td>
<p><strong>Stop sign</strong></p>
</td>
<td>
<p><strong>Cat</strong></p>
</td>
<td>
<p><strong>Sky</strong></p>
</td>
<td>
<p><strong>Road</strong></p>
</td>
<td>
<p><strong>Patches of grass</strong></p>
</td>
<td>
<p><strong>Submarine</strong></p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignLeft CDPAlign">1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Where each feature is an object that may or may not be within the image and the value represents the number of times that object appears in the image. If a model were given this information, it would be a fairly good idea to stop!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature transformation – enter math-man</h1>
                </header>
            
            <article>
                
<p>This chapter is where things get mathematical and interesting. We'll have talked about understating features and cleaning them. We'll also have looked at how to remove and add new features. In our feature construction chapter, we had to manually create these new features. We, the human, had to use our brains and come up with those three ways of decomposing that image of a stop sign. Sure, we can create code that makes the features automatically, but we ultimately chose what features we wanted to use. </p>
<p>This chapter will start to look at the automatic creation of these features as it applies to mathematical dimensionality. If we regard our data as vectors in an n-space (n being the number of columns), we will ask ourselves, <em>can we create a new dataset in a k-space (where k &lt; n) that fully or nearly represents the original data, but might give us speed boosts or performance enhancements in machine learning?</em> The goal here is to create a dataset of smaller dimensionality that performs better than our original dataset at a larger dimensionality.</p>
<p>The first question here is, <em>weren't we creating data in smaller dimensionality before when we were feature selecting? If we start with 17 features and remove five, we've reduced the dimensionality to 12, right?</em> Yes, of course! However, we aren't talking simply about removing columns here, we are talking about using complex mathematical transformations (usually taken from our studies in linear algebra) and applying them to our datasets.</p>
<p>One notable example we will spend some time on is called<strong> </strong><strong>Principal Components Analysis</strong> (<strong>PCA</strong>)<em>. </em>It is a transformation that breaks down our data into three different datasets, and we can use these results to create brand new datasets that can outperform our original!</p>
<p>Here is a visual example is taken from a Princeton University research experiment that used PCA to exploit patterns in gene expressions. This is a great application of dimensionality reduction as there are so many genes and combinations of genes, it would take even the most sophisticated algorithms in the world plenty of time to process them:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3172690f-5e88-42ff-af38-dabdf8adfe90.png"/></div>
<p>In the preceding screenshot, <strong>A</strong> represents the original dataset, where <strong>U</strong>, <strong>W</strong>, and <strong>V<sup>T</sup></strong> represent the results of a singular value decomposition. The results are then put together to make a brand new dataset that can replace <strong>A</strong> to a certain extent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature learning – using AI to better our AI</h1>
                </header>
            
            <article>
                
<p>The cherry on top, a cherry powered by the most sophisticated algorithms used today in the automatic construction of features for the betterment of machine learning and AI pipelines.</p>
<p>The previous chapter dealt with automatic feature creation using mathematical formulas, but once again, in the end, it is us, the humans, that choose the formulas and reap the benefits of them. This chapter will outline algorithms that are not in and of themselves a mathematical formula, but an architecture attempting to understand and model data in such a way that it will exploit patterns in data in order to create new data. This may sound vague at the moment, but we hope to get you excited about it!</p>
<p>We will focus mainly on neural algorithms that are specially designed to use a neural network design (nodes and weights). These algorithms will then impose features onto the data in such a way that can sometimes be unintelligible to humans, but extremely useful for machines. Some of the topics we'll look at are:</p>
<ul>
<li>Restricted Boltzmann machines</li>
<li>Word2Vec/GLoVe for word embedding</li>
</ul>
<p>Word2Vec and GLoVe are two ways of adding large dimensionality data to seemingly word tokens in the text. For example, if we look at a visual representation of the results of a Word2Vec algorithm, we might see the following:</p>
<div class="CDPAlignCenter CDPAlign"><img height="234" src="assets/bdac3750-c12c-4cfe-8929-73dc0cb6835a.png" width="227"/></div>
<p>By representing words as vectors in Euclidean space, we can achieve mathematical-esque results. In the previous example, by adding these automatically generated features we can <em>add</em> and <em>subtract</em> words by adding and subtracting their vector representations as given to us by Word2Vec. We can then generate interesting conclusions, such as <strong>king+man-woman=queen</strong>. Cool!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Feature engineering is a massive task to be undertaken by data scientists and machine learning engineers. It is a task that is imperative to having successful and production-ready machine learning pipelines. In the coming seven chapters, we are going to explore six major aspects of feature engineering:</p>
<ul>
<li>Feature understanding: learning how to identify data based on its qualities and quantitative state</li>
<li>Feature improvement: cleaning and imputing missing data values in order to maximize the dataset's value</li>
<li>Feature selection -statistically selecting and subsetting feature sets in order to reduce the noise in our data</li>
<li>Feature construction - building new features with the intention of exploiting feature interactions</li>
<li>Feature transformation - extracting latent (hidden) structure within datasets in order to <span>mathematically transform our datasets into something new (and usually better)</span></li>
<li>Feature learning - harnessing the power of deep learning to view data in a whole new light that will open up new problems to be solved.</li>
</ul>
<p class="p3">In this book, we will be exploring feature engineering as it relates to our machine learning endeavors. By breaking down this large topic into our subtopics and diving deep into each one in separate chapters, we will be able to get a much broader and more useful understanding of how these procedures work and how to apply each one in Python.</p>
<p>In our next chapter, we will dive straight into our first subsection, <em>Feature understanding</em>. We will finally be getting our hands on some real data, so let's begin!</p>


            </article>

            
        </section>
    </body></html>