["```py\nimport math\nimport os, random\nimport numpy as np\nimport pandas as pd\nimport mldatasets\nfrom sklearn import metrics, ensemble, tree, inspection,\\\n                    model_selection\nimport catboost as cb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap\nfrom pdpbox import pdp, info_plots\nfrom PyALE import ale\nfrom lime.lime_tabular import LimeTabularExplainer \nusedcars dataset:\n```", "```py\nusedcars_df = mldatasets.load(\"usedcars\", prepare=True) \n```", "```py\nusedcars_orig_df = usedcars_df.copy()\nusedcars_df = mldatasets.make_dummies_with_limits(\n    usedcars_df,\n    'fuel',\n    min_recs=500,\n    defcatname='other'\n)\nusedcars_df = mldatasets.make_dummies_with_limits(\n    usedcars_df,\n    'make_cat'\n)\nusedcars_df = mldatasets.make_dummies_with_limits(\n    usedcars_df,\n    'model_type',\n    min_recs=500,\n    defcatname='other'\n)\nusedcars_df = mldatasets.make_dummies_with_limits(\n    usedcars_df,\n    'condition',\n    min_recs=200,\n    defcatname='other'\n)\nusedcars_df = mldatasets.make_dummies_with_limits(\n    usedcars_df,\n    'drive'\n)\nusedcars_df = mldatasets.make_dummies_with_limits(\n    usedcars_df,\n    'title_status',\n    min_recs=500,\n    defcatname='other'\n) \nobject columns by turning them into a series of binary columns (of the uint8 type). However, there are still a few object columns left. We can find out which ones like this:\n```", "```py\nusedcars_df.dtypes[lambda x: x == object].index.tolist() \n```", "```py\nusedcars_df = usedcars_df.select_dtypes(\n    include=(int,float,np.uint8)\n) \n```", "```py\nrand = 42\nos.environ['PYTHONHASHSEED']=str(rand)\nnp.random.seed(rand)\nrandom.seed(rand)\ntarget_col = 'price'\nX = usedcars_df.drop([target_col], axis=1)\ny = usedcars_df[target_col]\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X, y, test_size=0.25, random_state=rand\n) \n```", "```py\n two classifiers, CatBoost and Random Forest:\n```", "```py\ncb_mdl = cb.CatBoostRegressor(\n    depth=7, learning_rate=0.2, random_state=rand, verbose=False\n)\ncb_mdl = cb_mdl.fit(X_train, y_train)\nrf_mdl =ensemble.RandomForestRegressor(n_jobs=-1,random_state=rand)\nrf_mdl = rf_mdl.fit(X_train.to_numpy(), y_train.to_numpy()) \n```", "```py\nmdl = cb_mdl\ny_train_pred, y_test_pred = mldatasets.evaluate_reg_mdl(\n    mdl, X_train, X_test, y_train, y_test\n) \n```", "```py\nthresh = 4000\npct_under = np.where(\n    np.abs(y_test - y_test_pred) < thresh, 1, 0\n).sum() / len(y_test)\nprint(f\"Percentage of test samples under ${thresh:,.0f} in absolute\n      error {pct_under:.1%}\") \n```", "```py\ncb_mdl with rf_mdl in the first line.\n```", "```py\ntree.plot_tree(rf_mdl.estimators_[0], filled=True, max_depth=2,\\\n              feature_names=X_train.columns) \n```", "```py\nrf_feat_imp = rf_mdl.feature_importances_\nprint(rf_feat_imp) \n```", "```py\ncb_feat_imp = cb_mdl.feature_importances_\nprint(cb_feat_imp) \nFigure 4.3:\n```", "```py\nfeat_imp_df = pd.DataFrame(\n    {\n        'feature':X_train.columns,\n        'cb_feat_imp':cb_feat_imp,\n        'rf_feat_imp':rf_feat_imp*100\n    }\n)\nfeat_imp_df = feat_imp_df.sort_values('cb_feat_imp', ascending=False)\nfeat_imp_df.style.format(\n    '{:.2f}%', subset=['cb_feat_imp', 'rf_feat_imp'])\n    .bar(subset=['cb_feat_imp', 'rf_feat_imp'], color='#4EF', width=60\n) \n```", "```py\ncb_perm_imp = inspection.permutation_importance(\n    cb_mdl, X_test, y_test, n_repeats=10, random_state=rand,\\\n    scoring='neg_mean_absolute_error'\n)\nrf_perm_imp = inspection.permutation_importance(\n    rf_mdl, X_test.to_numpy(), y_test.to_numpy(), n_repeats=10,\\\n    random_state=rand, scoring='neg_mean_absolute_error'\n) \n```", "```py\nperm_imp_df = pd.DataFrame(\n    {\n        'feature':X_train.columns,\n        'cb_perm_mean':cb_perm_imp.importances_mean,\n        'cb_perm_std':cb_perm_imp.importances_std,\n        'rf_perm_mean':rf_perm_imp.importances_mean,\n        'rf_perm_std':rf_perm_imp.importances_std\n    }\n)\nperm_imp_df = perm_imp_df.sort_values(\n    'cb_perm_mean', ascending=False\n)\nperm_imp_df.style.format(\n    '{:.4f}', subset=['cb_perm_mean', 'cb_perm_std', 'rf_perm_mean',\n    'rf_perm_std']).bar(subset=['cb_perm_mean', 'rf_perm_mean'],\\\n    color='#4EF', width=60\n) \n```", "```py\nrf_fn = lambda x: rf_mdl.predict(x)\nX_train_summary = shap.kmeans(X_train.to_numpy(), 50)\nX_test_sample = X_test.sample(frac=0.02)\nrf_kernel_explainer = shap.KernelExplainer(rf_fn, X_train_summary)\nrf_shap_values = rf_kernel_explainer.shap_values(\n    X_test_sample.to_numpy()\n) \n```", "```py\ncb_tree_explainer = shap.TreeExplainer(cb_mdl)\ncb_shap_values = cb_tree_explainer.shap_values(X_test) \n```", "```py\ncb_shap_imp = np.mean(np.abs(cb_shap_values),0) \n```", "```py\nshap_imp_df = pd.DataFrame(\n    {\n        'feature':X_train.columns,\n        'cb_shap_imp':cb_shap_imp,\n        'rf_shap_imp':rf_shap_imp\n    }\n)\nshap_imp_df = shap_imp_df.sort_values('cb_shap_imp', ascending=False)\nshap_imp_df.style.format(\n    '{:.4f}', subset=['cb_shap_imp', 'rf_shap_imp']).bar(\n    subset=['cb_shap_imp', 'rf_shap_imp'], color='#4EF', width=60\n) \n```", "```py\ncb_explainer = shap.Explainer(cb_mdl)\ncb_shap = cb_explainer(X_test) \n```", "```py\nprint(\"Values dimensions: %s\" % (cb_shap.values.shape,))\nprint(\"Data dimensions: %s\" % (cb_shap.data.shape,)) \n```", "```py\nshap.plots.bar(cb_shap, max_display=15) \nFigure 4.6:\n```", "```py\nyr_cohort = np.where(\n    X_test.year > 2014, \"Newer Car\", \"Older Car\"\n)\nshap.plots.bar(cb_shap.cohorts(yr_cohort).abs.mean(0)) \nFigure 4.7:\n```", "```py\nshap.plots.beeswarm(cb_shap, max_display=15) \n```", "```py\nshap.plots.partial_dependence(\n    \"year\", cb_mdl.predict, X_test, ice=False, model_expected_value=True,\\\n    feature_expected_value=True\n) \n```", "```py\npdp_single_feature = pdp.PDPIsolate(\n    model=cb_mdl, df=X_test, model_features=X_test.columns,\\\n    feature='year', feature_name='year', n_classes=0,\\\n    n_jobs=-1)\nfig, axes = pdp_single_feature.plot(plot_pts_dist=True)\nfig.show()\n    fig, axes = pdp_single_feature.plot(\n    plot_pts_dist=True, plot_lines=True, frac_to_plot=0.01\n)\nfig.show() \n    fig, axes = pdp_single_feature.plot(\n    to_bins=True, plot_lines=True, frac_to_plot=0.01\n)\nfig.show() \nFigure 4.10:\n```", "```py\ncont_feature_l = [\n    'year', 'odometer', 'cylinders', 'epa_displ', 'make_pop',\\\n    'make_yr_pop', 'model_yr_pop'\n]\nmake_cat_feature_l = [\n    'make_cat_obsolete', 'make_cat_regular', 'make_cat_premium',\\\n    'make_cat_luxury', 'make_cat_luxury_sports'\n]\nbin_feature_l = ['model_premier', 'auto_trans']\ncat_feature_l = make_cat_feature_l + bin_feature_l \n```", "```py\nfor feature in cont_feature_l:\n    pdp_single_feature = pdp.PDPIsolate(\n        model=cb_mdl, df=X_test, model_features=X_test.columns,\\\n        feature=feature, feature_name=feature, n_classes=0,\\\n        n_jobs=-1\n    )\n    fig, axes = pdp_single_feature.plot(\n        to_bins=True, plot_lines=True, frac_to_plot=0.01,\\\n        show_percentile=True, engine='matplotlib'\n    ) \n```", "```py\nmake_cat is a categorical feature that was one-hot encoded, so itâ€™s not natural to create PDP plots as if each category were an independent binary feature.\n```", "```py\npdp_multi_feature = pdp.PDPIsolate(\n    model=cb_mdl, df=X_test, model_features=X_test.columns,\\\n    feature=make_cat_feature_l, feature_name=\"make_cat\", n_classes=0, n_jobs=-1\n)\nfig, axes = pdp_multi_feature.plot(\n    plot_lines=True, frac_to_plot=0.01, show_percentile=True\n)\nfig.show()\npredict_plot = info_plots.PredictPlot(\n    model=cb_mdl, df=X_test, feature_name=\"make_cat\", n_classes=0,\n    model_features=X_test.columns, feature=make_cat_feature_l\n)\nfig, _, _ = predict_plot.plot()\nfig.show() \nFigure 4.12:\n```", "```py\nshap.plots.scatter(\n    cb_shap[:,\"odometer\"], color=cb_shap[:,\"year\"], xmin=\"percentile(1)\",\\\n    xmax=\"percentile(99)\", alpha=0.2\n)\nshap.plots.scatter(\n    cb_shap[:,\"long\"], color=cb_shap[:,\"epa_displ\"],\\\n    xmin=\"percentile(0.5)\", xmax=\"percentile(99.5)\", alpha=0.2, x_jitter=0.5\n) \nFigure 4.13:\n```", "```py\nshap.plots.scatter(\n    cb_shap[:,\"make_cat_luxury\"], color=cb_shap[:,\"year\"], x_jitter=0.4,\\\n    alpha=0.2, hist=False\n) \nFigure 4.14:\n```", "```py\nX_test_no_outliers = X_test[\n    (X_test.year.quantile(.01) <= X_test.year) &\\\n    (X_test.year <= X_test.year.quantile(.99)) &\\\n    (X_test.odometer.quantile(.01) <= X_test.odometer) &\\\n    (X_test.odometer <= X_test.odometer.quantile(.99))\n]\nale_effect = ale(\n    X=X_test_no_outliers, model=cb_mdl, feature=['odometer'],\\\n    feature_type='continuous', grid_size=80\n)\nplt.show()\nale_effect = ale(\n    X=X_test_no_outliers, model=cb_mdl,\\\n    feature=['make_cat_luxury'],feature_type='discrete'\n)\nplt.show() \nFigure 4.15:\n```", "```py\nX_samp = X.sample(frac=0.1)\ny_samp = y.loc[X_samp.index]\nclustering = shap.utils.hclust(X_samp, y_samp) \n```", "```py\nshap.plots.bar(cb_shap, clustering=clustering, clustering_cutoff=0.7) \nFigure 4.16:\n```", "```py\nfeatures_l = [\"year\", \"odometer\"]\nale_effect = ale(\n    X=X_test_no_outliers, model=cb_mdl, feature=features_l,\\\n    feature_type='continuous', grid_size=50, include_CI=False\n)\nplt.show() \n```", "```py\nnp.set_printoptions(suppress=True)\nprint(clustering) \n```", "```py\n[[  2\\.     21\\.      0\\.      2\\.   ]\n [ 22\\.     52\\.      0\\.      3\\.   ]\n [ 23\\.     53\\.      0\\.      4\\.   ]\n [ 24\\.     54\\.      0\\.      5\\.   ]\n [ 20\\.     55\\.      0\\.      6\\.   ]\n [ 12\\.     14\\.      0.235   2\\.   ]\n [  0\\.     57\\.      0.253   3\\.   ]\n [  1\\.     58\\.      0.262   4\\.   ]\n [ 13\\.     59\\.      0.289   5\\.   ]\n [  4\\.      7\\.      0.383   2\\.   ]\n [  9\\.     11\\.      0.394   2\\.   ]\n [ 43\\.     44\\.      0.429   2\\.   ]\n [ 15\\.     17\\.      0.462   2\\.   ]\n [  5\\.     56\\.      0.477   7\\.   ]\n [ 10\\.     61\\.      0.605   3\\.   ]\n [ 46\\.     49\\.      0.647   2\\.   ]\n [ 62\\.     66\\.      0.654   5\\.   ]\n [ 51\\.     67\\.      0.665   3\\.   ]\n [  6\\.     68\\.      0.67    6\\.   ]\n [ 31\\.     70\\.      0.673   7\\.   ]\n [ 65\\.     71\\.      0.694  14\\.   ]\n [ 25\\.     32\\.      0.717   2\\.   ]\n[ 41\\.    101\\.      0.997  52\\.   ]] \n```", "```py\nfeatures_l = [\"cylinders\", \"epa_displ\"]\nale_effect = ale(X=X_test, model=cb_mdl, feature=features_l,\\\n    feature_type='discrete', grid_size=50\n) \nFigure 4.18:\n```", "```py\nfeatures_l = [\"long\", \"lat\"]\npdp_interaction_feature = pdp.PDPInteract(\n    model=cb_mdl, df=X_test, model_features=X_test.columns,\\\n    features=features_l, feature_names=features_l, n_classes=0,\\\n    num_grid_points=15, n_jobs=-1\n)\nfig, _ = pdp_interaction_feature.plot(\n    plot_type='contour', plot_pdp=False\n)\nfig.show() \nFigure 4.19:\n```", "```py\nfig, axes, summary_df = info_plots.InteractPredictPlot(\n    model=cb_mdl, X=X_test, features=features_l,\\\n    feature_names=features_l, num_grid_points=(15,12)\n)\nfig, axes, summary_df = info_plots.InteractTargetPlot(\n    df=X_train.join(y_train), target='price', features=features_l,\\\n    feature_names=features_l, num_grid_points=(15,12)\n) \nFigure 4.20:\n```"]