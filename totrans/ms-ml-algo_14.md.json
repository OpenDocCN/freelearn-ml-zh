["```py\nimport numpy as np\n\nwidth = 15\nheight = 5\n\ny_final = width - 1\nx_final = height - 1\n\ny_wells = [0, 1, 3, 5, 5, 7, 9, 11, 12, 14]\nx_wells = [3, 1, 2, 0, 4, 1, 3, 2, 4, 1]\n\nstandard_reward = -0.1\ntunnel_rewards = np.ones(shape=(height, width)) * standard_reward\n\nfor x_well, y_well in zip(x_wells, y_wells):\n    tunnel_rewards[x_well, y_well] = -5.0\n\ntunnel_rewards[x_final, y_final] = 5.0\n```", "```py\nimport numpy as np\n\nnb_actions = 4\n\npolicy = np.random.randint(0, nb_actions, size=(height, width)).astype(np.uint8)\ntunnel_values = np.zeros(shape=(height, width))\n```", "```py\nimport numpy as np\n\ngamma = 0.9\n\ndef policy_evaluation():\n    old_tunnel_values = tunnel_values.copy() \n\n    for i in range(height):\n        for j in range(width): \n            action = policy[i, j]\n\n            if action == 0:\n                if i == 0:\n                    x = 0\n                else:\n                    x = i - 1\n                y = j\n\n            elif action == 1:\n                if j == width - 1:\n                    y = width - 1\n                else:\n                    y = j + 1\n                x = i\n\n            elif action == 2:\n                if i == height - 1:\n                    x = height - 1\n                else:\n                    x = i + 1\n                y = j\n\n            else:\n                if j == 0:\n                    y = 0\n                else:\n                    y = j - 1\n                x = i\n\n            reward = tunnel_rewards[x, y]\n            tunnel_values[i, j] = reward + (gamma * old_tunnel_values[x, y])\n\ndef is_final(x, y):\n    if (x, y) in zip(x_wells, y_wells) or (x, y) == (x_final, y_final):\n        return True\n    return False\n\ndef policy_improvement():\n    for i in range(height):\n        for j in range(width):\n            if is_final(i, j):\n                continue\n\n            values = np.zeros(shape=(nb_actions, ))\n\n            values[0] = (tunnel_rewards[i - 1, j] + (gamma * tunnel_values[i - 1, j])) if i > 0 else -np.inf\n            values[1] = (tunnel_rewards[i, j + 1] + (gamma * tunnel_values[i, j + 1])) if j < width - 1 else -np.inf\n            values[2] = (tunnel_rewards[i + 1, j] + (gamma * tunnel_values[i + 1, j])) if i < height - 1 else -np.inf\n            values[3] = (tunnel_rewards[i, j - 1] + (gamma * tunnel_values[i, j - 1])) if j > 0 else -np.inf\n\n            policy[i, j] = np.argmax(values).astype(np.uint8)\n```", "```py\nimport numpy as np\n\nnb_max_epochs = 100000\ntolerance = 1e-5\n\ne = 0\n\nwhile e < nb_max_epochs:\n    e += 1\n    old_tunnel_values = tunnel_values.copy()\n    policy_evaluation()\n\n    if np.mean(np.abs(tunnel_values - old_tunnel_values)) < tolerance:\n        old_policy = policy.copy()\n        policy_improvement()\n\n        if np.sum(policy - old_policy) == 0:\n            break\n```", "```py\nimport numpy as np\n\ntunnel_values = np.zeros(shape=(height, width))\n```", "```py\nimport numpy as np\n\ndef value_evaluation():\n    old_tunnel_values = tunnel_values.copy() \n\n    for i in range(height):\n        for j in range(width): \n            rewards = np.zeros(shape=(nb_actions, ))\n            old_values = np.zeros(shape=(nb_actions, ))\n\n            for k in range(nb_actions):\n                if k == 0:\n                    if i == 0:\n                        x = 0\n                    else:\n                        x = i - 1\n                    y = j\n\n                elif k == 1:\n                    if j == width - 1:\n                        y = width - 1\n                    else:\n                        y = j + 1\n                    x = i\n\n                elif k == 2:\n                    if i == height - 1:\n                        x = height - 1\n                    else:\n                        x = i + 1\n                    y = j\n\n                else:\n                    if j == 0:\n                        y = 0\n                    else:\n                        y = j - 1\n                    x = i\n\n                rewards[k] = tunnel_rewards[x, y]\n                old_values[k] = old_tunnel_values[x, y]\n\n            new_values = np.zeros(shape=(nb_actions, ))\n\n            for k in range(nb_actions):\n                new_values[k] = rewards[k] + (gamma * old_values[k])\n\n            tunnel_values[i, j] = np.max(new_values)\n\ndef policy_selection():\n    policy = np.zeros(shape=(height, width)).astype(np.uint8)\n\n    for i in range(height):\n        for j in range(width):\n            if is_final(i, j):\n                continue\n\n            values = np.zeros(shape=(nb_actions, ))\n\n            values[0] = (tunnel_rewards[i - 1, j] + (gamma * tunnel_values[i - 1, j])) if i > 0 else -np.inf\n            values[1] = (tunnel_rewards[i, j + 1] + (gamma * tunnel_values[i, j + 1])) if j < width - 1 else -np.inf\n            values[2] = (tunnel_rewards[i + 1, j] + (gamma * tunnel_values[i + 1, j])) if i < height - 1 else -np.inf\n            values[3] = (tunnel_rewards[i, j - 1] + (gamma * tunnel_values[i, j - 1])) if j > 0 else -np.inf\n\n            policy[i, j] = np.argmax(values).astype(np.uint8)\n\n    return policy\n```", "```py\nimport numpy as np\n\ne = 0\n\npolicy = None\n\nwhile e < nb_max_epochs:\n    e += 1\n    old_tunnel_values = tunnel_values.copy()\n    value_evaluation()\n\n    if np.mean(np.abs(tunnel_values - old_tunnel_values)) < tolerance:\n        policy = policy_selection()\n        break\n```", "```py\nimport numpy as np\n\npolicy = np.random.randint(0, nb_actions, size=(height, width)).astype(np.uint8)\ntunnel_values = np.zeros(shape=(height, width))\n```", "```py\nimport numpy as np\n\nxy_grid = np.meshgrid(np.arange(0, height), np.arange(0, width), sparse=False)\nxy_grid = np.array(xy_grid).T.reshape(-1, 2)\n\nxy_final = list(zip(x_wells, y_wells))\nxy_final.append([x_final, y_final])\n\nxy_start = []\n\nfor x, y in xy_grid:\n    if (x, y) not in xy_final:\n        xy_start.append([x, y])\n\nxy_start = np.array(xy_start)\n\ndef starting_point():\n    xy = np.squeeze(xy_start[np.random.randint(0, xy_start.shape[0], size=1)])\n    return xy[0], xy[1]\n```", "```py\nmax_steps = 1000\nalpha = 0.25\n\ndef episode():\n    (i, j) = starting_point()\n    x = y = 0\n\n    e = 0\n\n    while e < max_steps:\n        e += 1\n\n        action = policy[i, j]\n\n        if action == 0:\n            if i == 0:\n                x = 0\n            else:\n                x = i - 1\n            y = j\n\n        elif action == 1:\n            if j == width - 1:\n                y = width - 1\n            else:\n                y = j + 1\n            x = i\n\n        elif action == 2:\n            if i == height - 1:\n                x = height - 1\n            else:\n                x = i + 1\n            y = j\n\n        else:\n            if j == 0:\n                y = 0\n            else:\n                y = j - 1\n            x = i\n\n        reward = tunnel_rewards[x, y]\n        tunnel_values[i, j] += alpha * (reward + (gamma * tunnel_values[x, y]) - tunnel_values[i, j])\n\n        if is_final(x, y):\n            break\n        else:\n            i = x\n            j = y\n```", "```py\ndef policy_selection():\n    for i in range(height):\n        for j in range(width):\n            if is_final(i, j):\n                continue\n\n            values = np.zeros(shape=(nb_actions, ))\n\n            values[0] = (tunnel_rewards[i - 1, j] + (gamma * tunnel_values[i - 1, j])) if i > 0 else -np.inf\n            values[1] = (tunnel_rewards[i, j + 1] + (gamma * tunnel_values[i, j + 1])) if j < width - 1 else -np.inf\n            values[2] = (tunnel_rewards[i + 1, j] + (gamma * tunnel_values[i + 1, j])) if i < height - 1 else -np.inf\n            values[3] = (tunnel_rewards[i, j - 1] + (gamma * tunnel_values[i, j - 1])) if j > 0 else -np.inf\n\n            policy[i, j] = np.argmax(values).astype(np.uint8) \n```", "```py\nn_episodes = 5000\n\nfor _ in range(n_episodes):\n    episode()\n    policy_selection()\n```"]