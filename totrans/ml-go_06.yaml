- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, a set of data can be organized into a set of clusters. For example, you
    may be able to organize data into clusters that correspond to certain underlying
    properties (such as demographic properties including age, sex, geography, employment
    status, and so on) or certain underlying processes (such as browsing, shopping,
    bot interactions, and other such behaviors on a website). The machine learning
    techniques to detect and label these clusters are referred to as **clustering**
    techniques, naturally.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, the machine learning algorithms that we have explored have
    been **supervised**. That is, we have a set of features or attributes paired with
    a corresponding label or number that we are trying to predict. We use this labeled
    data to fit our model to the behavior that we already knew about prior to training
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Most clustering techniques are **unsupervised**. As opposed to supervised techniques
    for regression and classification, we often do not know about the clusters in
    our dataset prior to finding them with a clustering model. Thus, we go into a
    clustering problem with an unlabeled dataset and an algorithm, and we generate
    the cluster labels for our data using the clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Further, clustering techniques are distinguished from other machine learning
    techniques in that it is rather difficult to say what the *correct* or *accurate*
    clusters for the given dataset are. Depending on how many clusters you are looking
    for and the measures that you are using for similarity between data points, you
    might end up with a variety of sets of clusters, each having some underlying meaning.
    This does not mean that clustering techniques cannot be evaluated or validated,
    but it does mean that we need to understand our limitations and be careful when
    quantifying our results.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding clustering model jargon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Clustering is quite unique and comes with it''s own set of terms, which are
    shown below. Keep in mind that the following list is only a partial list as there
    are many different types of clustering with corresponding jargon:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clusters** or **groups**: Each of these clusters or groups is a collection
    of data points into which our clustering technique organizes our data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intra****-group** or **intra-cluster**: Clusters resulting from clustering
    can be evaluated using a measure of similarity between data points and other data
    points in the same resulting cluster. This is called intra-group or intra-cluster
    evaluation and similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inter****-group**or **inter-cluster**: Clusters resulting from clustering
    can be evaluated using a measure of dissimilarity between data points and other
    data points in other resulting clusters. This is called inter-group or inter-cluster
    evaluation and dissimilarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internal criteria**: Often, we do not have a gold standard set of cluster
    labels that we can use to evaluate our resulting clusters. In these cases, we
    utilize inter and intra cluster similarities to measure the performance of our
    clustering technique.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External criteria**: In other cases, we might have a gold standard for cluster
    labels or grouping, such as a standard generated by human judges. These scenarios
    allow us to evaluate our clustering techniques using the standard, or external,
    criteria.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distance** or **similarity**: This is a measure of how close two data points
    are. This could be a Euclidean distance in the space of your features or some
    other measure of closeness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring Distance or Similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to cluster data points together, we need to define and utilize some
    distance or similarity that quantitatively defines the closeness between data
    points. Choosing this measure is an essential part of every clustering project
    because it directly influences how the clusters are generated. Clusters resulting
    from the use of one similarity measure might be very different from those resulting
    from the use of another similarity measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common and simple of these distance measures is the **Euclidean distance**
    or the **squared Euclidean distance**. This is simply the straight line distance
    between two data points in your space of features (you might remember this distance
    as it was also used in our kNN example in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml),
    *Classification*) or quantity squared. However, there are a whole host of other,
    sometimes more complicated, distance metrics. A few of these are shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1ed3dd1-5ce1-4287-b5a4-64303d9af3cc.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, the **Manhattan** distance is the absolute *x* distance plus *y*
    distance between the points, and the **Minkowski** distance generalizes between
    the Euclidean distance and the Manhattan distance. These distance metrics will
    be more robust against unusual values (or outliers) in your data, as compared
    to the Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: Other distance metrics, such as the **Hamming** distance, are applicable to
    certain kinds of data, such as strings. In the example shown, the Hamming distance
    between **Golang** and **Gopher** is four because there are four positions in
    the strings in which the strings are different. Thus, the Hamming distance might
    be a good choice of distance metric if you are working with text data, such as
    news articles or tweets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes here, we will mostly stick to the Euclidean distance. This
    distance is implemented in `gonum.org/v1/gonum/floats` via the `Distance()` function.
    By way of illustration, let''s say that we want to calculate the distance between
    a point at `(1, 2)` and a point at `(3, 4)`. We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating clustering techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we are not trying to predict a number or category, our previously discussed
    evaluation metrics for continuous and discrete variables do not really apply to
    clustering techniques. That does not mean that we will just avoid measuring the
    performance of clustering algorithms. We need to know how well our clustering
    is performing. We just need to introduce a few clustering-specific evaluation
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Internal clustering evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we do not have a gold standard set of labels for our clusters for comparison,
    we are stuck with evaluating how well our clustering technique performs using
    internal criteria. In other words, we can still evaluate our clustering by making
    similarity and dissimilarity measurements within the clusters themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first of these internal metrics that we will present here is called the
    **silhouette coefficient**. The silhouette coefficient can be calculated for each
    clustered data point as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f60ed8e-a9ef-4476-b62d-85d6e4c4bd18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *a* is the mean distance between a data point and all other points in
    the same cluster (the Euclidean distance, for example), and *b* is the mean distance
    between a data point and all other points in the cluster nearest to the data point's
    cluster. The average of this silhouette coefficient for all data points represents
    how tightly packed the points are in each cluster. This average could be taken
    per cluster or for data points in all clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try calculating this for the iris dataset, which can be seen as a set
    of three clusters corresponding to each of the three iris species. First, in order
    to calculate the silhouette coefficient, we need to know the **centroids** of
    the three clusters. These centroids are simply the central points of the three
    clusters (in our four-dimensional feature space), and they will allow us to determine
    which cluster is the nearest to a certain data point's cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we need to parse our iris data set file (originally introduced
    in [Chapter 1](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml), *Gathering and Organizing
    Data*), separate our records by cluster label, average the features in each cluster,
    and then calculate the corresponding centroids. First, we will define a `type`
    for our centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can create a map that contains the centroid for each of our iris flowers
    species using `github.com/kniren/gota/dataframe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this gives us our centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to actually calculate the silhouette coefficients for each data
    point. To do this, let''s modify the preceding code such that we have access to
    each filtered set of data points outside of the `for loop`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also create a convenience function to retrieve floats values from a
    row in a `dataframe.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now loop over our records calculating the *a* and *b* that we need for
    the silhouette coefficients. We will also average the silhouette coefficients
    to get an overall evaluation metric for our clusters, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this example evaluation yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How do we know if `0.51` is a good or bad average silhouette coefficient? Well,
    remember that the silhouette coefficient is proportional to the difference between
    mean intra-cluster distances and mean inter-cluster distances, and it is always
    going to be somewhere between *0.0* and *1.0*. Thus, higher values (those closer
    to *1.0*) imply closer packed clusters and are more distinct from other clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Often, we might want to adjust our number of clusters and/or clustering technique
    to optimize the silhouette score (make it larger, that is). Here, we are working
    with data that has actually been hand-labeled, so `0.51` has to be a good score
    for this dataset. For other datasets, it may be higher or lower depending on the
    existence of clusters in the data and your choice of similarity metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'The silhouette score is by no means the only way to evaluate our clusters internally.
    We could actually just use the quantities *a* or *b* from the silhouette score
    to evaluate the homogeneity of our clusters, or each clusters, dissimilarity with
    other clusters, respectively. Also, we could use the mean distance between points
    in a cluster and the centroid of the cluster to measure tightly packed clusters.
    Still further, we could use a variety of other evaluation metrics that will not
    be covered here in detail, such as the **Calinski-Harabaz index** (which is discussed
    further here: [http://datamining.rutgers.edu/publication/internalmeasures.pdf](http://datamining.rutgers.edu/publication/internalmeasures.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: External clustering evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we have a ground truth or gold standard for our clusters, then we can utilize
    a variety of external clustering evaluation techniques. This ground truth or gold
    standard means that we have access to, or can get (via manual human annotation),
    a set of data points where the true or desired cluster labels have been annotated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, we do not have access to this sort of clustering gold standard, and,
    thus, we will not cover these sorts of evaluation techniques in detail here. However,
    if interested or relevant, you can look into the **Adjusted Rand index**, **Mutual
    Information**, **Fowlkes-Mallows scores**, **completeness**, and **V-measures**,
    which are all relevant external clustering evaluation metrics (read more details
    about these here: [https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first clustering technique that we will cover here, and probably the most
    well-known clustering technique, is called **k-means** c**lustering**, or just
    **k-means**. k-means is an iterative method in which data points are clustered
    around cluster centroids that are adjusted during each iteration. The technique
    is relatively easy to grasp, but there are some related subtleties that are easy
    to miss. We will make sure to highlight these as we explore the technique.
  prefs: []
  type: TYPE_NORMAL
- en: As k-means clustering is so easy to implement, there are many proof-of-concept
    implementations of the algorithm in Go. You can find these by searching for k-means
    on this link ([https://golanglibs.com/top?q=kmeans](https://golanglibs.com/top?q=kmeans)).
    However, we will utilize a implementation that is recent and fairly straightforward
    to use, `github.com/mash/gokmeans`.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say that we have a bunch of data points defined by two variables, *x[1]*
    and *x[2]*. These data points naturally exhibit some grouping into clusters, as
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb2a3812-3684-4fe1-95d0-7b5cd79dab6b.png)'
  prefs: []
  type: TYPE_IMG
- en: To automatically cluster these points using k-means, we would first need to
    choose how many clusters will result from the clustering. This is the parameter
    *k*, which gives k-means its name. In this case, let's use *k = 3*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We would then randomly choose the *x[1]* and *x[2]* locations of *k* centroids.
    These random centroids will serve as our starting point for the algorithm. Such
    random centroids are shown in the following figure via Xs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97050bef-b68e-4829-8bd6-dc77a83420c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To optimize these centroids and cluster our points, we then iteratively perform
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign each data point to a cluster corresponding to the nearest centroid (as
    measured by our choice distance metric, such as Euclidean distance).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the mean *x[1]* and *x[2]* locations within each cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update each centroid's location to the calculated *x[1]* and *x[2]* locations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat steps one to three until the assignment in step one no longer changes.
    This process is illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16178c0b-00bb-4f1e-86b0-39c78e5f880b.png)'
  prefs: []
  type: TYPE_IMG
- en: There is only so much that you can illustrate with a static figure, but hopefully
    this is helpful. If you would like to visually step through the k-means process
    to better understand the updates, you should check out [http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html](http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html),
    which includes an interactive animation of the k-means clustering process.
  prefs: []
  type: TYPE_NORMAL
- en: k-means assumptions and pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'k-means may seem like a very simple algorithm, which it is. However, it does
    make some underlying assumptions about your data, which are easy to overlook:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spherical** or **spatially grouped clusters**: k-means basically draws spherical
    or spatially close areas in our feature space to find clusters. This means that
    for non-spherical clusters (essentially, clusters that do not look like grouped
    blobs in our features space), k-means is likely to fail. To make this idea more
    concrete, non-spherical clusters, for which k-means will likely behave poorly,
    might look like the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1c96eb59-50de-4777-8844-4a552126a4cc.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Similar size**: k-means also assumes that your clusters are all of a similar
    size. Small outlying clusters can lead the simple k-means algorithm going off
    course to produce strange groupings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moreover, there are a couple of pitfalls that we can fall into when using k-means
    to cluster our data:'
  prefs: []
  type: TYPE_NORMAL
- en: The choice of *k* is up to us. This means that we could choose an illogical
    *k*, but it also means that we could just continue increasing *k* until we have
    a cluster for each of our points (which would be pretty good clustering because
    each point is exactly the same as itself). To help guide your choice of *k*, you
    should utilize an **elbow graph** approach. In this approach, you increase *k*
    while calculating your evaluation metric. As you increase *k*, your evaluation
    metric should keep getting better, but eventually there will be an inflection
    point that indicates diminishing returns. The ideal *k* is at this elbow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not guaranteed that k-means will always converge to the same clusters.
    As you are starting from random centroids, your k-means algorithm could converge
    to different local minimums on different runs. You should be aware of this and
    run your k-means algorithm from a variety of initializations to ensure stability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means clustering example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset that we will be using to illustrate clustering techniques is about
    delivery drivers. The dataset looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first column, `Driver_ID`, includes various anonymous identifications of
    particular drivers. The second and third columns are attributes that we will utilize
    in our clusters. The `Distance_Feature` column is a mean distance driven per data,
    and `Speeding_Feature` is a mean percentage of time during which the driver is
    driving 5+ miles per hour faster than the speed limit.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the clustering will be to cluster the delivery drivers into groups
    based on `Distance_Feature` and `Speeding_Feature`. Remember, this is an unsupervised
    learning technique, and thus, we do not really know what clusters should or could
    be formed in the data. The hope is that we will learn something about the drivers
    that we did not know at the start of the exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Yes, you guessed it! We have a new dataset and we need to profile this dataset
    to learn a little more about it. Let''s first calculate summary statistics with
    `github.com/kniren/dataframe` and create histograms of each feature using `gonum.org/v1/plot`.
    We have already done this multiple times in [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml),
    *Regression* and [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml), *Classification*,
    so we will not rehash the code here. Let''s just look at the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Wow! Looks like most drivers speed about 10% of the time, which is kind of scary.
    One driver even appears to speed 100% of the time. I hope I'm not on his route.
  prefs: []
  type: TYPE_NORMAL
- en: 'The histograms features are shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c6d6099-bc44-4b9b-9d77-4b0e0d8c9466.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It looks like there is an interesting structure in the `Distance_Feature` data.
    This will actually factor into our clustering soon, but we can get another view
    of this structure by creating a scatter plot of our feature space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this creates the following scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d94a843-e1da-4bc9-975c-89ec82726b6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see a little bit more of the structure that we saw in the histograms.
    There appears to be at least two clear clusters of data here. This intuition about
    our data can serve as a mental check during the formal application of our clustering
    techniques, and it can give us a starting point to experiment with values of *k.*
  prefs: []
  type: TYPE_NORMAL
- en: Generating clusters with k-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s get our hands dirty by actually applying k-means clustering to the
    delivery driver data. To utilize `github.com/mash/gokmeans`, we first need to
    create a slice of `gokmeans.Node` values, which will be input into the clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, generating our clusters is as easy as calling the `gomeans.Train(...)`
    function. Specifically, we will call this function with *k = 2* and a maximum
    of `50` iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Running all of this together gives the following centroids for the generated
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Nice! We have generated our first clusters. Now, we need to move on to evaluating
    the legitimacy of these clusters.
  prefs: []
  type: TYPE_NORMAL
- en: I have just output the centroids of the clusters here, because that is really
    all we need to know the group points. If we want to know if a data point is in
    the first or second cluster, we just need to calculate the distance to those centroids.
    The closer of the centroids corresponds to the group containing the data point.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the generated clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first way that we can evaluate the clusters that we just generated is visually.
    Let''s create another scatter plot. However, this time let''s use different shapes
    for each of the groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the following scatterplot, which clearly shows our successful
    clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7416b8f-e070-48c4-ba9c-31af7ec9893c.png)'
  prefs: []
  type: TYPE_IMG
- en: Qualitatively, we can see that there is one cluster of driver that drive short
    distances primarily, and one cluster of drivers that drive long distances primarily.
    These actually correspond to rural and city delivery drivers respectively (or
    short haul and long haul drivers).
  prefs: []
  type: TYPE_NORMAL
- en: 'To more quantitatively evaluate our clusters, we can calculate the within-cluster
    mean distance between points in a cluster and the cluster centroid. To help us
    in this endeavor, let''s create a function that will make things a little easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to evaluate our clusters, we simply need to call this function for each
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this gives us the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, cluster one (the pink cluster in the scatter plot) is about twice
    as compact (tightly packed, that is) as cluster two. This is consistent without
    the plot and gives us a little more quantitative information about the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Note, that here it was pretty clear that we were looking for two clusters. However,
    in other cases, the number of clusters may not be clear upfront, especially in
    cases where you have more features than you can visualize. In these scenarios,
    it is important that you utilize a method, such as the `elbow` method, to determine
    a proper *k*. More information about this method can be found at [https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/](https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/).
  prefs: []
  type: TYPE_NORMAL
- en: Other clustering techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a host of other clustering techniques that are not discussed here.
    These include DBSCAN and Hierarchical clustering. Unfortunately, the current implementations
    in Go are limited for these other clustering options. DBSCAN is implemented in
    `https://github.com/sjwhitworth/golearn`, but, to my knowledge, there are no current
    implementations of other clustering techniques.
  prefs: []
  type: TYPE_NORMAL
- en: This creates a great opportunity for contributions to the community! Clustering
    techniques are often not complicated and creating an implementation of another
    clustering technique might be a great way to give back to the Go data science
    community. Feel free to reach out to the author in Gophers Slack (`@dwhitena`)
    or other data science gophers in `#data-science` on Gophers Slack if you want
    to discuss an implementation, ask questions, or get help!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Distance metrics and evaluating clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation of clustering overview: [https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A comparison of various distance/similarity metrics: [http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visualizing k-means clustering: [https://www.naftaliharris.com/blog/visualizing-k-means-clustering/](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/mash/gokmeans` docs: [https://godoc.org/github.com/mash/gokmeans](https://godoc.org/github.com/mash/gokmeans)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have introduced the general principles of clustering, learned
    how to evaluate generated clusters, and learned how to use a Go implementation
    of k-means clustering. You should now be in good shape to detect grouping structure
    in your datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the modeling of time series data, such as stock prices,
    sensor data, and so on.
  prefs: []
  type: TYPE_NORMAL
