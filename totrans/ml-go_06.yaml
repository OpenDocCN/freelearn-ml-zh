- en: Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Often, a set of data can be organized into a set of clusters. For example, you
    may be able to organize data into clusters that correspond to certain underlying
    properties (such as demographic properties including age, sex, geography, employment
    status, and so on) or certain underlying processes (such as browsing, shopping,
    bot interactions, and other such behaviors on a website). The machine learning
    techniques to detect and label these clusters are referred to as **clustering**
    techniques, naturally.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一组数据可以被组织成一组聚类。例如，你可能能够将数据组织成与某些潜在属性（如包括年龄、性别、地理、就业状态等人口统计属性）或某些潜在过程（如浏览、购物、机器人交互以及网站上的其他此类行为）相对应的聚类。用于检测和标记这些聚类的机器学习技术被称为**聚类**技术，这是很自然的。
- en: Up to this point, the machine learning algorithms that we have explored have
    been **supervised**. That is, we have a set of features or attributes paired with
    a corresponding label or number that we are trying to predict. We use this labeled
    data to fit our model to the behavior that we already knew about prior to training
    the model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所探讨的机器学习算法都是**监督式**的。也就是说，我们有一组特征或属性与相应的标签或数字配对，这是我们试图预测的。我们使用这些带标签的数据来调整我们的模型以适应我们在训练模型之前已经了解的行为。
- en: Most clustering techniques are **unsupervised**. As opposed to supervised techniques
    for regression and classification, we often do not know about the clusters in
    our dataset prior to finding them with a clustering model. Thus, we go into a
    clustering problem with an unlabeled dataset and an algorithm, and we generate
    the cluster labels for our data using the clustering algorithm.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数聚类技术都是**无监督式**的。与回归和分类的监督式技术相反，我们在使用聚类模型找到聚类之前，通常不知道数据集中的聚类。因此，我们带着未标记的数据集和算法进入聚类问题，并使用聚类算法为我们生成数据集的聚类标签。
- en: Further, clustering techniques are distinguished from other machine learning
    techniques in that it is rather difficult to say what the *correct* or *accurate*
    clusters for the given dataset are. Depending on how many clusters you are looking
    for and the measures that you are using for similarity between data points, you
    might end up with a variety of sets of clusters, each having some underlying meaning.
    This does not mean that clustering techniques cannot be evaluated or validated,
    but it does mean that we need to understand our limitations and be careful when
    quantifying our results.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，聚类技术与其他机器学习技术区分开来，因为很难说给定数据集的**正确**或**准确**聚类是什么。根据你寻找的聚类数量以及你用于数据点之间相似度的度量，你可能会得到一系列不同的聚类集合，每个集合都有一些潜在的意义。这并不意味着聚类技术不能被评估或验证，但这确实意味着我们需要了解我们的局限性，并在量化我们的结果时要小心。
- en: Understanding clustering model jargon
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类模型术语
- en: 'Clustering is quite unique and comes with it''s own set of terms, which are
    shown below. Keep in mind that the following list is only a partial list as there
    are many different types of clustering with corresponding jargon:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类非常独特，并带有它自己的一套术语，如下所示。请记住，以下列表只是一个部分列表，因为有许多不同类型的聚类及其术语：
- en: '**Clusters** or **groups**: Each of these clusters or groups is a collection
    of data points into which our clustering technique organizes our data points.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**或**组**：这些聚类或组中的每一个都是我们的聚类技术组织数据点的数据点集合。'
- en: '**Intra****-group** or **intra-cluster**: Clusters resulting from clustering
    can be evaluated using a measure of similarity between data points and other data
    points in the same resulting cluster. This is called intra-group or intra-cluster
    evaluation and similarity.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组内**或**簇内**：通过聚类产生的聚类可以使用数据点与其他相同结果簇中的数据点之间的相似度来评估。这被称为组内或簇内评估和相似度。'
- en: '**Inter****-group**or **inter-cluster**: Clusters resulting from clustering
    can be evaluated using a measure of dissimilarity between data points and other
    data points in other resulting clusters. This is called inter-group or inter-cluster
    evaluation and dissimilarity.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组间**或**簇间**：通过聚类产生的聚类可以使用数据点与其他结果簇中的数据点之间的差异度来评估。这被称为组间或簇间评估和差异度。'
- en: '**Internal criteria**: Often, we do not have a gold standard set of cluster
    labels that we can use to evaluate our resulting clusters. In these cases, we
    utilize inter and intra cluster similarities to measure the performance of our
    clustering technique.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部标准**：通常，我们并没有一个可以用来评估我们结果聚类的金标准聚类标签集。在这些情况下，我们利用聚类内和聚类间的相似性来衡量我们的聚类技术的性能。'
- en: '**External criteria**: In other cases, we might have a gold standard for cluster
    labels or grouping, such as a standard generated by human judges. These scenarios
    allow us to evaluate our clustering techniques using the standard, or external,
    criteria.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部标准**：在其他情况下，我们可能有一个金标准的聚类标签或分组标准，例如由人类评委生成的一个标准。这些场景允许我们使用标准或外部标准来评估我们的聚类技术。'
- en: '**Distance** or **similarity**: This is a measure of how close two data points
    are. This could be a Euclidean distance in the space of your features or some
    other measure of closeness.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**距离**或**相似度**：这是衡量两个数据点之间接近程度的一个度量。这可能是特征空间中的欧几里得距离或某种其他接近程度的度量。'
- en: Measuring Distance or Similarity
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量距离或相似度
- en: In order to cluster data points together, we need to define and utilize some
    distance or similarity that quantitatively defines the closeness between data
    points. Choosing this measure is an essential part of every clustering project
    because it directly influences how the clusters are generated. Clusters resulting
    from the use of one similarity measure might be very different from those resulting
    from the use of another similarity measure.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据点聚在一起，我们需要定义并利用一些距离或相似度，这些距离或相似度可以定量地定义数据点之间的接近程度。选择这个度量是每个聚类项目的一个基本部分，因为它直接影响到聚类的生成方式。使用一个相似度度量生成的聚类可能与使用另一个相似度度量生成的聚类非常不同。
- en: 'The most common and simple of these distance measures is the **Euclidean distance**
    or the **squared Euclidean distance**. This is simply the straight line distance
    between two data points in your space of features (you might remember this distance
    as it was also used in our kNN example in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml),
    *Classification*) or quantity squared. However, there are a whole host of other,
    sometimes more complicated, distance metrics. A few of these are shown in the
    following diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些距离度量中最常见且简单的是**欧几里得距离**或**平方欧几里得距离**。这仅仅是两个数据点在特征空间中的直线距离（你可能记得这个距离，因为它也用于我们第5章中的kNN示例[Chapter
    5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml)，*分类*)或数量平方。然而，还有许多其他，有时更复杂的距离度量。以下图表中展示了其中的一些：
- en: '![](img/a1ed3dd1-5ce1-4287-b5a4-64303d9af3cc.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a1ed3dd1-5ce1-4287-b5a4-64303d9af3cc.png)'
- en: For example, the **Manhattan** distance is the absolute *x* distance plus *y*
    distance between the points, and the **Minkowski** distance generalizes between
    the Euclidean distance and the Manhattan distance. These distance metrics will
    be more robust against unusual values (or outliers) in your data, as compared
    to the Euclidean distance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，**曼哈顿距离**是两点之间的绝对 *x* 距离加上 *y* 距离，而**闵可夫斯基距离**则是对欧几里得距离和曼哈顿距离的推广。与欧几里得距离相比，这些距离度量在面对数据中的异常值（或离群值）时将更加稳健。
- en: Other distance metrics, such as the **Hamming** distance, are applicable to
    certain kinds of data, such as strings. In the example shown, the Hamming distance
    between **Golang** and **Gopher** is four because there are four positions in
    the strings in which the strings are different. Thus, the Hamming distance might
    be a good choice of distance metric if you are working with text data, such as
    news articles or tweets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其他距离度量，如**汉明距离**，适用于某些类型的数据，例如字符串。在示例中，**Golang**和**Gopher**之间的汉明距离是四，因为在字符串中有四个位置它们是不同的。因此，如果你在处理文本数据，如新闻文章或推文，汉明距离可能是一个好的距离度量选择。
- en: 'For our purposes here, we will mostly stick to the Euclidean distance. This
    distance is implemented in `gonum.org/v1/gonum/floats` via the `Distance()` function.
    By way of illustration, let''s say that we want to calculate the distance between
    a point at `(1, 2)` and a point at `(3, 4)`. We can do this as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的目的中，我们将主要坚持使用欧几里得距离。这个距离在`gonum.org/v1/gonum/floats`中通过`Distance()`函数实现。为了说明，假设我们想要计算点`(1,
    2)`和点`(3, 4)`之间的距离。我们可以这样做：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Evaluating clustering techniques
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估聚类技术
- en: As we are not trying to predict a number or category, our previously discussed
    evaluation metrics for continuous and discrete variables do not really apply to
    clustering techniques. That does not mean that we will just avoid measuring the
    performance of clustering algorithms. We need to know how well our clustering
    is performing. We just need to introduce a few clustering-specific evaluation
    metrics.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不是试图预测一个数字或类别，我们之前讨论的连续和离散变量的评估指标并不适用于聚类技术。这并不意味着我们将避免测量聚类算法的性能。我们需要知道我们的聚类表现如何。我们只需要引入一些特定的聚类评估指标。
- en: Internal clustering evaluation
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内部聚类评估
- en: If we do not have a gold standard set of labels for our clusters for comparison,
    we are stuck with evaluating how well our clustering technique performs using
    internal criteria. In other words, we can still evaluate our clustering by making
    similarity and dissimilarity measurements within the clusters themselves.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有为我们的簇设置一组金标准标签进行比较，我们就只能使用内部标准来评估我们的聚类技术表现。换句话说，我们仍然可以通过在簇内部进行相似性和差异性测量来评估我们的聚类。
- en: 'The first of these internal metrics that we will present here is called the
    **silhouette coefficient**. The silhouette coefficient can be calculated for each
    clustered data point as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的第一种内部指标称为**轮廓系数**。轮廓系数可以按以下方式计算每个聚类数据点：
- en: '![](img/4f60ed8e-a9ef-4476-b62d-85d6e4c4bd18.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f60ed8e-a9ef-4476-b62d-85d6e4c4bd18.jpg)'
- en: Here, *a* is the mean distance between a data point and all other points in
    the same cluster (the Euclidean distance, for example), and *b* is the mean distance
    between a data point and all other points in the cluster nearest to the data point's
    cluster. The average of this silhouette coefficient for all data points represents
    how tightly packed the points are in each cluster. This average could be taken
    per cluster or for data points in all clusters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*a*是数据点到同一簇中所有其他点的平均距离（例如欧几里得距离），而*b*是数据点到其簇最近簇中所有其他点的平均距离。所有数据点的这个轮廓系数的平均值表示每个簇中点的紧密程度。这个平均值可以按簇或所有簇中的数据点来计算。
- en: Let's try calculating this for the iris dataset, which can be seen as a set
    of three clusters corresponding to each of the three iris species. First, in order
    to calculate the silhouette coefficient, we need to know the **centroids** of
    the three clusters. These centroids are simply the central points of the three
    clusters (in our four-dimensional feature space), and they will allow us to determine
    which cluster is the nearest to a certain data point's cluster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试计算鸢尾花数据集的轮廓系数，这可以看作是三个簇的集合，对应于三种鸢尾花物种。首先，为了计算轮廓系数，我们需要知道三个簇的**质心**。这些质心仅仅是三个簇的中心点（在我们的四维特征空间中），这将允许我们确定哪个簇离某个数据点的簇最近。
- en: 'To this end, we need to parse our iris data set file (originally introduced
    in [Chapter 1](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml), *Gathering and Organizing
    Data*), separate our records by cluster label, average the features in each cluster,
    and then calculate the corresponding centroids. First, we will define a `type`
    for our centroids:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，我们需要解析我们的鸢尾花数据集文件（最初在[第1章](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml)，*收集和组织数据*）中引入的），根据簇标签分离我们的记录，计算每个簇中的特征平均值，然后计算相应的质心。首先，我们将为我们的质心定义一个`type`：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we can create a map that contains the centroid for each of our iris flowers
    species using `github.com/kniren/gota/dataframe`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建一个包含我们每种鸢尾花物种质心的映射，使用`github.com/kniren/gota/dataframe`：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Compiling and running this gives us our centroids:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行此代码将给我们提供我们的质心：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we need to actually calculate the silhouette coefficients for each data
    point. To do this, let''s modify the preceding code such that we have access to
    each filtered set of data points outside of the `for loop`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要实际计算每个数据点的轮廓系数。为此，让我们修改前面的代码，以便我们可以在`for loop`外部访问每个过滤后的数据点集：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s also create a convenience function to retrieve floats values from a
    row in a `dataframe.DataFrame`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再创建一个方便的函数来从`dataframe.DataFrame`的行中检索浮点值：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now loop over our records calculating the *a* and *b* that we need for
    the silhouette coefficients. We will also average the silhouette coefficients
    to get an overall evaluation metric for our clusters, as shown in the following
    code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以遍历我们的记录，计算用于轮廓系数的`a`和`b`。我们还将计算轮廓系数的平均值，以获得我们簇的整体评估指标，如下面的代码所示：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Compiling and running this example evaluation yields the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行此示例评估会产生以下结果：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How do we know if `0.51` is a good or bad average silhouette coefficient? Well,
    remember that the silhouette coefficient is proportional to the difference between
    mean intra-cluster distances and mean inter-cluster distances, and it is always
    going to be somewhere between *0.0* and *1.0*. Thus, higher values (those closer
    to *1.0*) imply closer packed clusters and are more distinct from other clusters.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何知道`0.51`是否是一个好或坏的轮廓系数平均值？嗯，记住轮廓系数与平均簇内距离和平均簇间距离之间的差异成正比，它总是在`0.0`和`1.0`之间。因此，更高的值（那些接近`1.0`的值）意味着簇更紧密地堆积，并且与其他簇更明显。
- en: Often, we might want to adjust our number of clusters and/or clustering technique
    to optimize the silhouette score (make it larger, that is). Here, we are working
    with data that has actually been hand-labeled, so `0.51` has to be a good score
    for this dataset. For other datasets, it may be higher or lower depending on the
    existence of clusters in the data and your choice of similarity metric.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可能想要调整我们的簇数量和/或聚类技术以优化轮廓分数（使其更大）。在这里，我们正在处理实际手工标记的数据，所以`0.51`对于这个数据集来说必须是一个好分数。对于其他数据集，它可能更高或更低，这取决于数据中簇的存在以及你选择的相似性度量。
- en: 'The silhouette score is by no means the only way to evaluate our clusters internally.
    We could actually just use the quantities *a* or *b* from the silhouette score
    to evaluate the homogeneity of our clusters, or each clusters, dissimilarity with
    other clusters, respectively. Also, we could use the mean distance between points
    in a cluster and the centroid of the cluster to measure tightly packed clusters.
    Still further, we could use a variety of other evaluation metrics that will not
    be covered here in detail, such as the **Calinski-Harabaz index** (which is discussed
    further here: [http://datamining.rutgers.edu/publication/internalmeasures.pdf](http://datamining.rutgers.edu/publication/internalmeasures.pdf)).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓分数绝对不是评估我们簇内部结构的唯一方法。我们实际上可以使用轮廓分数中的`a`或`b`量来评估我们簇的同质性，或者每个簇与其他簇的不相似性。此外，我们可以使用簇中点与簇质心的平均距离来衡量紧密堆积的簇。更进一步，我们可以使用各种其他评估指标，这些指标在此不详细讨论，例如**Calinski-Harabaz指数**（进一步讨论见：[http://datamining.rutgers.edu/publication/internalmeasures.pdf](http://datamining.rutgers.edu/publication/internalmeasures.pdf))。
- en: External clustering evaluation
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部聚类评估
- en: If we have a ground truth or gold standard for our clusters, then we can utilize
    a variety of external clustering evaluation techniques. This ground truth or gold
    standard means that we have access to, or can get (via manual human annotation),
    a set of data points where the true or desired cluster labels have been annotated.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有我们簇的地面真相或黄金标准，那么我们可以利用各种外部聚类评估技术。这个地面真相或黄金标准意味着我们可以访问，或者可以通过人工标注获得，一组数据点，其中已标注了真实的或期望的簇标签。
- en: 'Often, we do not have access to this sort of clustering gold standard, and,
    thus, we will not cover these sorts of evaluation techniques in detail here. However,
    if interested or relevant, you can look into the **Adjusted Rand index**, **Mutual
    Information**, **Fowlkes-Mallows scores**, **completeness**, and **V-measures**,
    which are all relevant external clustering evaluation metrics (read more details
    about these here: [https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们无法访问这种聚类黄金标准，因此我们不会在这里详细讨论这些评估技术。然而，如果你感兴趣或相关，你可以查看**调整后的兰德指数**、**互信息**、**Fowlkes-Mallows分数**、**完整性**和**V度量**，这些都是相关的外部聚类评估指标（更多详细信息请参阅：[https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html))。
- en: k-means clustering
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means聚类
- en: The first clustering technique that we will cover here, and probably the most
    well-known clustering technique, is called **k-means** c**lustering**, or just
    **k-means**. k-means is an iterative method in which data points are clustered
    around cluster centroids that are adjusted during each iteration. The technique
    is relatively easy to grasp, but there are some related subtleties that are easy
    to miss. We will make sure to highlight these as we explore the technique.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将要介绍的第一种聚类技术，可能是最著名的聚类技术，被称为 **k-means** 聚类，或简称 **k-means**。k-means 是一种迭代方法，其中数据点围绕在每次迭代中调整的簇质心周围聚类。这个技术相对容易理解，但有一些相关的细微差别很容易忽略。我们将确保在探讨这个技术时突出这些内容。
- en: As k-means clustering is so easy to implement, there are many proof-of-concept
    implementations of the algorithm in Go. You can find these by searching for k-means
    on this link ([https://golanglibs.com/top?q=kmeans](https://golanglibs.com/top?q=kmeans)).
    However, we will utilize a implementation that is recent and fairly straightforward
    to use, `github.com/mash/gokmeans`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 k-means 聚类很容易实现，因此有大量的算法实现示例在 Go 中。您可以通过在此链接上搜索 k-means 来找到这些示例（[https://golanglibs.com/top?q=kmeans](https://golanglibs.com/top?q=kmeans)）。然而，我们将使用一个最近且相对简单易用的实现，`github.com/mash/gokmeans`。
- en: Overview of k-means clustering
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means 聚类概述
- en: 'Let''s say that we have a bunch of data points defined by two variables, *x[1]*
    and *x[2]*. These data points naturally exhibit some grouping into clusters, as
    shown in the following figure:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组由两个变量 *x[1]* 和 *x[2]* 定义的数据点。这些数据点自然地表现出一些聚类，如下面的图所示：
- en: '![](img/eb2a3812-3684-4fe1-95d0-7b5cd79dab6b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb2a3812-3684-4fe1-95d0-7b5cd79dab6b.png)'
- en: To automatically cluster these points using k-means, we would first need to
    choose how many clusters will result from the clustering. This is the parameter
    *k*, which gives k-means its name. In this case, let's use *k = 3*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 k-means 算法自动聚类这些点，我们首先需要选择聚类将产生多少个簇。这是参数 *k*，它赋予了 k-means 算法其名称。在这种情况下，让我们使用
    *k = 3*。
- en: 'We would then randomly choose the *x[1]* and *x[2]* locations of *k* centroids.
    These random centroids will serve as our starting point for the algorithm. Such
    random centroids are shown in the following figure via Xs:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将随机选择 *k* 个质心的 *x[1]* 和 *x[2]* 位置。这些随机质心将作为算法的起点。以下图中的 Xs 显示了这些随机质心：
- en: '![](img/97050bef-b68e-4829-8bd6-dc77a83420c2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97050bef-b68e-4829-8bd6-dc77a83420c2.png)'
- en: 'To optimize these centroids and cluster our points, we then iteratively perform
    the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化这些质心并聚类我们的点，我们接下来迭代执行以下操作：
- en: Assign each data point to a cluster corresponding to the nearest centroid (as
    measured by our choice distance metric, such as Euclidean distance).
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个数据点分配到与最近质心相对应的簇（根据我们选择的距离度量，如欧几里得距离）。
- en: Calculate the mean *x[1]* and *x[2]* locations within each cluster.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个簇内 *x[1]* 和 *x[2]* 位置的均值。
- en: Update each centroid's location to the calculated *x[1]* and *x[2]* locations.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个质心的位置更新为计算出的 *x[1]* 和 *x[2]* 位置。
- en: 'Repeat steps one to three until the assignment in step one no longer changes.
    This process is illustrated in the following figure:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 重复步骤一至三，直到步骤一中的分配不再改变。这个过程在下面的图中得到了说明：
- en: '![](img/16178c0b-00bb-4f1e-86b0-39c78e5f880b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16178c0b-00bb-4f1e-86b0-39c78e5f880b.png)'
- en: There is only so much that you can illustrate with a static figure, but hopefully
    this is helpful. If you would like to visually step through the k-means process
    to better understand the updates, you should check out [http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html](http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html),
    which includes an interactive animation of the k-means clustering process.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用静态图说明的只有这么多，但希望这有所帮助。如果您想通过可视化 k-means 过程来更好地理解更新，您应该查看 [http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html](http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html)，其中包含
    k-means 聚类过程的交互式动画。
- en: k-means assumptions and pitfalls
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means 假设和陷阱
- en: 'k-means may seem like a very simple algorithm, which it is. However, it does
    make some underlying assumptions about your data, which are easy to overlook:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法可能看起来非常简单，它确实是。然而，它确实对您的数据做出了一些潜在的假设，这些假设很容易被忽视：
- en: '**Spherical** or **spatially grouped clusters**: k-means basically draws spherical
    or spatially close areas in our feature space to find clusters. This means that
    for non-spherical clusters (essentially, clusters that do not look like grouped
    blobs in our features space), k-means is likely to fail. To make this idea more
    concrete, non-spherical clusters, for which k-means will likely behave poorly,
    might look like the following:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**球形** 或 **空间分组聚类**：k-means 基本上在我们的特征空间中绘制球形或空间接近的区域以找到聚类。这意味着对于非球形聚类（本质上，在我们特征空间中看起来不像分组团块的聚类），k-means
    很可能失败。为了使这个想法更具体，k-means 很可能表现不佳的非球形聚类可能看起来如下：'
- en: '![](img/1c96eb59-50de-4777-8844-4a552126a4cc.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1c96eb59-50de-4777-8844-4a552126a4cc.png)'
- en: '**Similar size**: k-means also assumes that your clusters are all of a similar
    size. Small outlying clusters can lead the simple k-means algorithm going off
    course to produce strange groupings.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似大小**：k-means 还假设您的聚类大小相似。小的异常聚类可能导致简单的 k-means 算法偏离轨道，产生奇怪的分组。'
- en: 'Moreover, there are a couple of pitfalls that we can fall into when using k-means
    to cluster our data:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在使用 k-means 对数据进行聚类时，我们可能会陷入几个陷阱：
- en: The choice of *k* is up to us. This means that we could choose an illogical
    *k*, but it also means that we could just continue increasing *k* until we have
    a cluster for each of our points (which would be pretty good clustering because
    each point is exactly the same as itself). To help guide your choice of *k*, you
    should utilize an **elbow graph** approach. In this approach, you increase *k*
    while calculating your evaluation metric. As you increase *k*, your evaluation
    metric should keep getting better, but eventually there will be an inflection
    point that indicates diminishing returns. The ideal *k* is at this elbow.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k* 的选择取决于我们。这意味着我们可能选择一个不合理的 *k*，但也意味着我们可以继续增加 *k*，直到每个点都有一个聚类（这将是一个非常不错的聚类，因为每个点都与其自身完全相同）。为了帮助您选择
    *k*，您应该利用 **肘图** 方法。在这种方法中，您在计算评估指标的同时增加 *k*。随着 *k* 的增加，您的评估指标应该持续改善，但最终会出现一个拐点，表明收益递减。理想的
    *k* 就在这个拐点。'
- en: It is not guaranteed that k-means will always converge to the same clusters.
    As you are starting from random centroids, your k-means algorithm could converge
    to different local minimums on different runs. You should be aware of this and
    run your k-means algorithm from a variety of initializations to ensure stability.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并不能保证 k-means 总是收敛到相同的聚类。由于您是从随机质心开始的，您的 k-means 算法可能在不同的运行中收敛到不同的局部最小值。您应该意识到这一点，并从不同的初始化中运行
    k-means 算法以确保稳定性。
- en: k-means clustering example
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means 聚类示例
- en: 'The dataset that we will be using to illustrate clustering techniques is about
    delivery drivers. The dataset looks like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集来展示聚类技术是关于快递司机的。数据集看起来是这样的：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first column, `Driver_ID`, includes various anonymous identifications of
    particular drivers. The second and third columns are attributes that we will utilize
    in our clusters. The `Distance_Feature` column is a mean distance driven per data,
    and `Speeding_Feature` is a mean percentage of time during which the driver is
    driving 5+ miles per hour faster than the speed limit.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列，`Driver_ID`，包括各种特定的司机的匿名标识。第二列和第三列是我们将在聚类中使用的属性。`Distance_Feature` 列是每次数据的平均行驶距离，而
    `Speeding_Feature` 是司机在速度限制以上行驶 5+ 英里每小时的时间百分比的平均百分比。
- en: The goal of the clustering will be to cluster the delivery drivers into groups
    based on `Distance_Feature` and `Speeding_Feature`. Remember, this is an unsupervised
    learning technique, and thus, we do not really know what clusters should or could
    be formed in the data. The hope is that we will learn something about the drivers
    that we did not know at the start of the exercise.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的目标将是根据 `Distance_Feature` 和 `Speeding_Feature` 将快递司机聚类成组。记住，这是一个无监督学习技术，因此我们实际上并不知道数据中应该或可能形成哪些聚类。希望我们能从练习开始时不知道的司机那里学到一些东西。
- en: Profiling the data
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析
- en: 'Yes, you guessed it! We have a new dataset and we need to profile this dataset
    to learn a little more about it. Let''s first calculate summary statistics with
    `github.com/kniren/dataframe` and create histograms of each feature using `gonum.org/v1/plot`.
    We have already done this multiple times in [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml),
    *Regression* and [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml), *Classification*,
    so we will not rehash the code here. Let''s just look at the results:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你猜对了！我们有一个新的数据集，我们需要对这个数据集进行配置，以便更多地了解它。让我们首先使用 `github.com/kniren/dataframe`
    来计算摘要统计信息，并使用 `gonum.org/v1/plot` 创建每个特征的直方图。我们已经在[第4章](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml)，*回归*和[第5章](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml)，*分类*中多次这样做，所以这里我们不会重复代码。让我们看看结果：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Wow! Looks like most drivers speed about 10% of the time, which is kind of scary.
    One driver even appears to speed 100% of the time. I hope I'm not on his route.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！看起来大多数司机大约10%的时间会超速，这有点可怕。有一位司机似乎100%的时间都在超速。我希望我不要在他的路线上。
- en: 'The histograms features are shown in the following graph:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图特征在以下图中显示：
- en: '![](img/2c6d6099-bc44-4b9b-9d77-4b0e0d8c9466.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![散点图](img/2c6d6099-bc44-4b9b-9d77-4b0e0d8c9466.png)'
- en: 'It looks like there is an interesting structure in the `Distance_Feature` data.
    This will actually factor into our clustering soon, but we can get another view
    of this structure by creating a scatter plot of our feature space:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来在 `Distance_Feature` 数据中有一个有趣的结构。这实际上很快就会在我们的聚类中起作用，但我们可以通过创建特征空间的散点图来获取这个结构的另一个视角：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Compiling and running this creates the following scatter plot:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行它创建以下散点图：
- en: '![](img/9d94a843-e1da-4bc9-975c-89ec82726b6e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![散点图](img/9d94a843-e1da-4bc9-975c-89ec82726b6e.png)'
- en: Here, we can see a little bit more of the structure that we saw in the histograms.
    There appears to be at least two clear clusters of data here. This intuition about
    our data can serve as a mental check during the formal application of our clustering
    techniques, and it can give us a starting point to experiment with values of *k.*
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到比直方图中更多的一些结构。这里似乎至少有两个清晰的数据簇。关于我们数据的这种直觉可以在我们正式应用聚类技术时作为一个心理检查，并且它可以给我们提供一个实验
    *k* 值的起点。
- en: Generating clusters with k-means
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k-means生成聚类
- en: 'Now let''s get our hands dirty by actually applying k-means clustering to the
    delivery driver data. To utilize `github.com/mash/gokmeans`, we first need to
    create a slice of `gokmeans.Node` values, which will be input into the clustering:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过实际应用k-means聚类到配送司机数据上来动手实践。为了利用 `github.com/mash/gokmeans`，我们首先需要创建一个
    `gokmeans.Node` 值的切片，这将作为聚类的输入：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, generating our clusters is as easy as calling the `gomeans.Train(...)`
    function. Specifically, we will call this function with *k = 2* and a maximum
    of `50` iterations:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，生成我们的聚类就像调用 `gomeans.Train(...)` 函数一样简单。具体来说，我们将使用 *k = 2* 和最大 `50` 次迭代来调用此函数：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Running all of this together gives the following centroids for the generated
    clusters:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 运行所有这些，得到以下生成的聚类的中心点：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Nice! We have generated our first clusters. Now, we need to move on to evaluating
    the legitimacy of these clusters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们已经生成了我们的第一个聚类。现在，我们需要继续评估这些聚类的合法性。
- en: I have just output the centroids of the clusters here, because that is really
    all we need to know the group points. If we want to know if a data point is in
    the first or second cluster, we just need to calculate the distance to those centroids.
    The closer of the centroids corresponds to the group containing the data point.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里只输出了聚类的中心点，因为那是我们真正需要知道组点的。如果我们想知道一个数据点是在第一个还是第二个簇中，我们只需要计算到那些中心点的距离。中心点越接近，对应的就是包含数据点的组。
- en: Evaluating the generated clusters
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估生成的聚类
- en: 'The first way that we can evaluate the clusters that we just generated is visually.
    Let''s create another scatter plot. However, this time let''s use different shapes
    for each of the groups:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以评估我们刚刚生成的聚类的第一种方式是视觉上的。让我们创建另一个散点图。然而，这次让我们为每个组使用不同的形状：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This code generates the following scatterplot, which clearly shows our successful
    clustering:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码生成了以下散点图，清楚地显示了我们的成功聚类：
- en: '![](img/e7416b8f-e070-48c4-ba9c-31af7ec9893c.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![散点图](img/e7416b8f-e070-48c4-ba9c-31af7ec9893c.png)'
- en: Qualitatively, we can see that there is one cluster of driver that drive short
    distances primarily, and one cluster of drivers that drive long distances primarily.
    These actually correspond to rural and city delivery drivers respectively (or
    short haul and long haul drivers).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 定性来看，我们可以看到有一个主要驾驶短距离的司机聚类，还有一个主要驾驶长距离的司机聚类。这实际上分别对应于农村和城市配送司机（或短途和长途司机）。
- en: 'To more quantitatively evaluate our clusters, we can calculate the within-cluster
    mean distance between points in a cluster and the cluster centroid. To help us
    in this endeavor, let''s create a function that will make things a little easier:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更定量地评估我们的聚类，我们可以计算聚类内点与聚类质心的平均距离。为了帮助我们完成这项任务，让我们创建一个将使事情变得更容易的函数：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, to evaluate our clusters, we simply need to call this function for each
    cluster:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了评估我们的聚类，我们只需为每个聚类调用此函数：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Running this gives us the following metrics:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此操作会给我们以下指标：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As we can see, cluster one (the pink cluster in the scatter plot) is about twice
    as compact (tightly packed, that is) as cluster two. This is consistent without
    the plot and gives us a little more quantitative information about the clusters.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，第一个聚类（散点图中的粉色聚类）比第二个聚类紧凑约两倍（即紧密堆积）。这一点在没有图表的情况下也是一致的，并为我们提供了关于聚类的更多定量信息。
- en: Note, that here it was pretty clear that we were looking for two clusters. However,
    in other cases, the number of clusters may not be clear upfront, especially in
    cases where you have more features than you can visualize. In these scenarios,
    it is important that you utilize a method, such as the `elbow` method, to determine
    a proper *k*. More information about this method can be found at [https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/](https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里很明显我们正在寻找两个聚类。然而，在其他情况下，聚类的数量可能一开始并不明确，尤其是在你拥有的特征多于你能可视化的情况下。在这些场景中，利用一种方法，如`elbow`方法，来确定合适的*k*是很重要的。关于此方法的更多信息可以在[https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/](https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/)找到。
- en: Other clustering techniques
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他聚类技术
- en: There are a host of other clustering techniques that are not discussed here.
    These include DBSCAN and Hierarchical clustering. Unfortunately, the current implementations
    in Go are limited for these other clustering options. DBSCAN is implemented in
    `https://github.com/sjwhitworth/golearn`, but, to my knowledge, there are no current
    implementations of other clustering techniques.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里没有讨论的其他聚类技术有很多。这些包括DBSCAN和层次聚类。不幸的是，Go语言中当前对这些其他聚类选项的实现有限。DBSCAN在`https://github.com/sjwhitworth/golearn`中实现，但据我所知，目前还没有其他聚类技术的实现。
- en: This creates a great opportunity for contributions to the community! Clustering
    techniques are often not complicated and creating an implementation of another
    clustering technique might be a great way to give back to the Go data science
    community. Feel free to reach out to the author in Gophers Slack (`@dwhitena`)
    or other data science gophers in `#data-science` on Gophers Slack if you want
    to discuss an implementation, ask questions, or get help!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这为社区贡献创造了极好的机会！聚类技术通常并不复杂，实现另一种聚类技术的实现可能是回馈Go数据科学社区的一种很好的方式。如果您想讨论实现、提问或寻求帮助，请随时在Gophers
    Slack（`@dwhitena`）或Gophers Slack的`#data-science`频道联系作者或其他数据科学爱好者！
- en: References
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Distance metrics and evaluating clusters:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 距离度量与聚类评估：
- en: 'Evaluation of clustering overview: [https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类评估概述：[https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
- en: 'A comparison of various distance/similarity metrics: [http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种距离/相似度度量的比较：[http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059)
- en: 'Visualizing k-means clustering: [https://www.naftaliharris.com/blog/visualizing-k-means-clustering/](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化k-means聚类：[https://www.naftaliharris.com/blog/visualizing-k-means-clustering/](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)
- en: '`github.com/mash/gokmeans` docs: [https://godoc.org/github.com/mash/gokmeans](https://godoc.org/github.com/mash/gokmeans)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/mash/gokmeans` 文档：[https://godoc.org/github.com/mash/gokmeans](https://godoc.org/github.com/mash/gokmeans)'
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have introduced the general principles of clustering, learned
    how to evaluate generated clusters, and learned how to use a Go implementation
    of k-means clustering. You should now be in good shape to detect grouping structure
    in your datasets.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了聚类的通用原则，学习了如何评估生成的聚类，以及如何使用Go语言实现的k-means聚类。现在，你应该能够很好地检测数据集中的分组结构。
- en: Next, we will discuss the modeling of time series data, such as stock prices,
    sensor data, and so on.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论时间序列数据的建模，例如股票价格、传感器数据等。
