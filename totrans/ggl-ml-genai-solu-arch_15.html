<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer178" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-266"><a id="_idTextAnchor328" class="calibre6 pcalibre pcalibre1"/>13</h1>
<h1 id="_idParaDest-267" class="calibre5"><a id="_idTextAnchor329" class="calibre6 pcalibre pcalibre1"/>ML Governance and the Google Cloud Architecture Framework</h1>
<p class="calibre3">As technologists, we often, of course, find the technological aspects of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) to be the most fun and exciting parts, while legal and regulatory concepts don’t always inspire us quite as much. However, these concepts are required to build robust solutions at scale in production. They are what help us make the transition from hobbyist activities to designing reliable systems that can be vital to a company’s success or even affect millions of <span>people’s lives.</span></p>
<p class="calibre3">With that in mind, this chapter will cover the <span>following subjects:</span></p>
<ul class="calibre16">
<li class="calibre8"><span>ML governance</span></li>
<li class="calibre8">An overview of the Google Cloud <span>Architecture Framework</span></li>
<li class="calibre8">Architecture Framework concepts about AI/ML workloads on <span>Google Cloud</span></li>
</ul>
<p class="calibre3">Let’s go ahead and dive right into our <span>first topic.</span></p>
<h1 id="_idParaDest-268" class="calibre5"><a id="_idTextAnchor330" class="calibre6 pcalibre pcalibre1"/>ML governance</h1>
<p class="calibre3">ML governance<a id="_idIndexMarker1402" class="calibre6 pcalibre pcalibre1"/> refers to everything that’s required to manage ML models within an organization, throughout the entire model development life cycle. As models can play a significant role in critical decision-making processes, it’s important to ensure that they are transparent, reliable, fair, and secure, and we need to implement structured frameworks to achieve these goals. These frameworks include policies and best practices that ensure responsible and ethical use of data and <span>ML technologies.</span></p>
<p class="calibre3">When discussing ML governance in this chapter, I will also include data governance in the scope of the discussion, because the use of data is so inherent in the ML life cycle. Let’s <span>start there.</span></p>
<h2 id="_idParaDest-269" class="calibre9"><a id="_idTextAnchor331" class="calibre6 pcalibre pcalibre1"/>Data governance</h2>
<p class="calibre3">When it comes<a id="_idIndexMarker1403" class="calibre6 pcalibre pcalibre1"/> to managing data in the ML life cycle, there are a number of aspects that we need to consider, such as data quality, lineage, privacy, security, and retention. Let’s take a look at each of these in <span>more detail.</span></p>
<h3 class="calibre11">Data security, privacy, and access control</h3>
<p class="calibre3">Of all the aspects <a id="_idIndexMarker1404" class="calibre6 pcalibre pcalibre1"/>of a data governance strategy, data security is <a id="_idIndexMarker1405" class="calibre6 pcalibre pcalibre1"/>arguably<a id="_idIndexMarker1406" class="calibre6 pcalibre pcalibre1"/> the most important. Data security incidents tend to make headline news, and those are not the kinds of news headlines you want to be <span>responsible for!</span></p>
<p class="calibre3">Fundamental to data security is the concept of data access control, which, as the name suggests, focuses on who can access the data and how they can access the data. The worst-case scenario is that somebody from outside the company gains access to sensitive data and leaks it publicly or uses it for nefarious purposes such as ransom <span>or sabotage.</span></p>
<p class="calibre3">When it comes to data security, my favorite<a id="_idIndexMarker1407" class="calibre6 pcalibre pcalibre1"/> term is <strong class="bold">defense-in-depth</strong> (<strong class="bold">DiD</strong>), which alludes to the fact that a thorough data security strategy consists of using many different tools to protect the data and other resources. In the following sub-sections, I will outline steps we can take to secure <span>our data.</span></p>
<h4 class="calibre20">Data categorization</h4>
<p class="calibre3">Before defining<a id="_idIndexMarker1408" class="calibre6 pcalibre pcalibre1"/> security policies for your data, it’s important to establish a categorization system in order to understand which elements of data need more focus in terms of protection. For instance, data that is published openly on your website, such as product descriptions and prices, is generally not considered to be top secret, whereas your customers’ credit card details are highly sensitive. You can categorize your data in terms of tiers, such as <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Tier 0</strong>: Highly-sensitive, such as customer passwords and credit <span>card details</span></li>
<li class="calibre8"><strong class="bold">Tier 1</strong>: Sensitive, such as customer purchase or <span>transaction history</span></li>
<li class="calibre8"><strong class="bold">Tier 2</strong>: Somewhat sensitive, such as customer addresses and <span>phone numbers</span></li>
<li class="calibre8"><strong class="bold">Tier 3</strong>: Non-sensitive, such as publicly <span>viewable information</span></li>
</ul>
<p class="calibre3">These are just examples; you would need to work with your organization’s information security experts<a id="_idIndexMarker1409" class="calibre6 pcalibre pcalibre1"/> to determine what categories would make the most sense for <span>your organization.</span></p>
<p class="calibre3">After categorizing our data, let’s discuss how we can <span>secure it.</span></p>
<h4 class="calibre20">Network security</h4>
<p class="calibre3">We can look <a id="_idIndexMarker1410" class="calibre6 pcalibre pcalibre1"/>at the DiD approach as layers in an onion. At the outermost layer, we begin with network security. If unintended users of the data do not have a network path to the data, then that’s a really solid foundation in your security strategy. Network security practices include setting up devices such as firewalls to control what kinds of traffic are allowed to enter a protected network. Google Cloud provides such devices, as well as other network security constructs, such as <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>), which allows you to set up your own private networks and<a id="_idIndexMarker1411" class="calibre6 pcalibre pcalibre1"/> control how to access them, and <strong class="bold">VPC Service Controls</strong> (<strong class="bold">VPC-SC</strong>), which <a id="_idIndexMarker1412" class="calibre6 pcalibre pcalibre1"/>enables you to create a security perimeter around your resources to prevent <span>data exfiltration.</span></p>
<h4 class="calibre20">Authentication and authorization</h4>
<p class="calibre3">The next layer<a id="_idIndexMarker1413" class="calibre6 pcalibre pcalibre1"/> in the onion is authentication and authorization to grant or deny permissions to access resources. Even if somebody gets access to a protected network, the next step is to determine which resources they are allowed to access and what actions they are allowed to perform on those resources. You need to ensure that the data can be accessed only by people and systems that are authorized to do so, and authorization should be based on business criticality. In other words, a person or system should only be able to access a piece of data if they need such access to perform a required business function. At all times, you should be able to easily determine who (or what) has access to which data, <span>and why.</span></p>
<p class="calibre3">Google<a id="_idIndexMarker1414" class="calibre6 pcalibre pcalibre1"/> Cloud <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) can be used to configure and enforce such permissions, or, for software components, additional mechanisms such as <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>) authentication<a id="_idIndexMarker1415" class="calibre6 pcalibre pcalibre1"/> can also be used. A little later in this section, we will also cover data cataloging with Google Cloud Dataplex. Dataplex and Google Cloud BigLake can be used to make it easier for companies to manage and enforce permissions for accessing their data resources. Google Cloud BigQuery offers additional data security mechanisms such as row-level and column-level access control, meaning that not only can you grant or restrict access to tables within BigQuery, but you can more granularly grant or restrict access to specific rows and/or columns within those tables. This provides additional flexibility to protect resources from unintended access. For example, with column-level security, you could configure that only people in the finance department can view columns that contain customers’ credit card details, while other employees and systems cannot. With row-level security, you could<a id="_idIndexMarker1416" class="calibre6 pcalibre pcalibre1"/> configure that sales representatives can only view details for customers in their region and not in <span>other regions.</span></p>
<h4 class="calibre20">Data encryption</h4>
<p class="calibre3">At the innermost<a id="_idIndexMarker1417" class="calibre6 pcalibre pcalibre1"/> layer are the data resources themselves. Best practices suggest that data should be encrypted as a further security measure. In that case, even if a malicious or unintended user gets access to the data, they would need the encryption keys to unencrypt the data. It goes without saying that encryption keys should be stored in a highly secure manner in a separate system, with all of the layers of security implemented to protect them. Again, Google Cloud provides tools to implement all of those layers of security mechanisms, including encryption and key management using Google Cloud <span>Key Management.</span></p>
<h4 class="calibre20">Logging and auditing</h4>
<p class="calibre3">In addition to all <a id="_idIndexMarker1418" class="calibre6 pcalibre pcalibre1"/>of those mechanisms, a strong data security strategy should incorporate auditing and logging implementations to monitor data access and modifications and support audits and forensic investigations to detect or investigate policy violations and data breaches. Google Cloud Logging and Audit Logs can be used for those purposes. <span><em class="italic">Figure 13</em></span><em class="italic">.1</em> shows what kinds of logs can be tracked by Google Cloud <span>Audit Logs:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer173">
<img alt="Figure 13.1: Audit log types in Google Cloud Audit Logs" src="image/B18143_13_1.jpg" class="calibre168"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 13.1: Audit log types in Google Cloud Audit Logs</p>
<p class="calibre3">As we can<a id="_idIndexMarker1419" class="calibre6 pcalibre pcalibre1"/> see in <span><em class="italic">Figure 13</em></span><em class="italic">.1</em>, there are three types of logs that we can capture using Google Cloud <span>Audit Logs.</span></p>
<h4 class="calibre20">Data privacy</h4>
<p class="calibre3">While intrinsically<a id="_idIndexMarker1420" class="calibre6 pcalibre pcalibre1"/> related to data security, data privacy more specifically focuses on the lawful, ethical, and safe handling of <strong class="bold">personal information</strong> (<strong class="bold">PI</strong>). It is<a id="_idIndexMarker1421" class="calibre6 pcalibre pcalibre1"/> one of the most important aspects of data security because privacy violations can severely damage a company’s reputation and <span>customer trust.</span></p>
<p class="calibre3">Not only that, but they can incur serious legal ramifications. There are numerous international standards and regulations that govern data privacy and protection, such as the <strong class="bold">General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>) in the <strong class="bold">European Union</strong> (<strong class="bold">EU</strong>), the <strong class="bold">California Consumer Privacy Act</strong> (<strong class="bold">CCPA</strong>) in the US, and many others globally. These<a id="_idIndexMarker1422" class="calibre6 pcalibre pcalibre1"/> regulations outline rules about data handling, such<a id="_idIndexMarker1423" class="calibre6 pcalibre pcalibre1"/> as collection, storage, processing, and sharing. Navigating these regulations and ensuring compliance in how your systems handle data can be <span>quite challenging.</span></p>
<p class="calibre3">It can also be a challenge to keep track of sensitive data that may be dispersed throughout your datasets. Imagine a company that has petabytes of data that comes from many different sources, such as credit card readers in stores or customer-facing online forms. It’s easy to identify that data being transmitted from a credit card machine needs to be protected, but this may not be as obvious for other data interfaces. For example, perhaps customers are accidentally entering their credit card details into online form fields that are not intended for that purpose (for example, if a customer accidentally pastes their credit card details into a field that was not intended for entering credit card details). Such fields may not be considered sensitive and therefore do not get any special attention from a security perspective. The Google Cloud Sensitive Data Protection service, which now incorporates the Google Cloud <strong class="bold">Data Loss Prevention</strong> (<strong class="bold">DLP</strong>) tool, can <a id="_idIndexMarker1424" class="calibre6 pcalibre pcalibre1"/>help to identify and protect sensitive information in your datasets. If sensitive data is found, you can use the Sensitive Data Protection service to implement protection mechanisms such as de-identification, masking, tokenization, <span>and redaction.</span></p>
<p class="calibre3">In the context of data security, privacy, and access control, it’s important to highlight the concept of shared responsibilities and shared fate on Google Cloud. Rather than risking misstating legal terminology here, I instead recommend reading Google Cloud’s official policy on this<a id="_idIndexMarker1425" class="calibre6 pcalibre pcalibre1"/> topic, which can be found at the <span>following URL:</span></p>
<p class="calibre3"><a href="https://cloud.google.com/architecture/framework/security/shared-responsibility-shared-fate" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/architecture/framework/security/shared-responsibility-shared-fate</span></a></p>
<p class="calibre3">Before we move on to discuss ML model governance, we will round out this section with a discussion of data quality, cataloging, <span>and lineage.</span></p>
<h3 class="calibre11">Data quality, cataloging, and lineage</h3>
<p class="calibre3">Considering that <a id="_idIndexMarker1426" class="calibre6 pcalibre pcalibre1"/>data <a id="_idIndexMarker1427" class="calibre6 pcalibre pcalibre1"/>can be such <a id="_idIndexMarker1428" class="calibre6 pcalibre pcalibre1"/>a valuable resource for a company, it’s important to establish practices that ensure the accuracy, consistency, discoverability, and (depending on the use case) timeliness of data. For example, considering that data is often used to make important business decisions, inaccurate or out-of-date data could have negative impacts on a company’s business. This also applies to business decisions that are automated by ML models. If we feed inaccurate data into an ML model, we will get <span>inaccurate predictions.</span></p>
<p class="calibre3">An effective data governance strategy starts with clear policies, responsibilities, and standards for data quality. We then need to establish frameworks to measure and monitor data quality factors. Best practices in this regard include regular data cleaning processes for correcting data errors, de-duplication, and filling in missing values, as well as automation mechanisms such as regular, automated data <span>quality checks.</span></p>
<p class="calibre3">Data discoverability is also an important factor. After all, even if you have created pristinely curated datasets, they are not very useful if nobody knows they exist. When companies have well-established data management practices, it’s easier for data producers and consumers to know exactly where their data is and the current status of that data, at any given time. In such companies, a robust data catalog forms the heart of the company’s data infrastructure. I’ve worked with many clients over the years in various consulting roles, and you’d be surprised how many companies operate without well-defined data management strategies. Given that data can be the life-blood of an organization, it’s surprising to learn that many companies are not fully aware of exactly what data they own. Various systems throughout the company are producing and gathering data all day every day, but if that data is not being cataloged in some way, it may simply sit in silos in remote, disparate parts of the company, unavailable and unknown to most of the rest of the organization. Bear in mind that data can be used for all kinds of interesting business use cases. If you don’t know what data you have access to, you may be missing out on significant <span>business opportunities.</span></p>
<p class="calibre3">It’s also important to implement data lineage tracking to understand where data comes from and how it gets transformed as it moves through various processing systems within a company. If I find a piece of data somewhere in my company, I want to know how it got there and every step it took along the way. Which systems did it pass through? What did those systems do to the data? Are there intermediate datasets that were created by those other systems in various parts of the company? This is not only important from a business operations perspective but can be required for compliance purposes. For example, if you need to comply with data sovereignty regulations, you better know where your data is at all times. If a customer decides to exercise their right to be forgotten in markets that are subject to GDPR – or other relevant regulations – you will have a really hard time complying if you do not have a good handle on your data. Similarly, if a data breach occurs, data lineage can help identify what data was compromised and understand the potential impacts and recovery <span>steps needed.</span></p>
<p class="calibre3">Fortunately, Google Cloud provides numerous tools to help with each of the aforementioned activities, such as Dataproc and Dataflow for cleaning and processing data, and Dataplex<a id="_idIndexMarker1429" class="calibre6 pcalibre pcalibre1"/> for<a id="_idIndexMarker1430" class="calibre6 pcalibre pcalibre1"/> cataloging, data<a id="_idIndexMarker1431" class="calibre6 pcalibre pcalibre1"/> quality, and data <span>lineage tracking.</span></p>
<p class="calibre3">Next, we will discuss ML <span>model governance.</span></p>
<h2 id="_idParaDest-270" class="calibre9"><a id="_idTextAnchor332" class="calibre6 pcalibre pcalibre1"/>ML model governance</h2>
<p class="calibre3">In this section, we <a id="_idIndexMarker1432" class="calibre6 pcalibre pcalibre1"/>discuss the aspects required to ensure that the models we build and deploy are reliable, scalable, and secure and that they continue to meet those requirements on an ongoing basis. There are a number of factors that we need to incorporate in order to achieve this goal, which we discuss in the <span>following sub-sections.</span></p>
<h3 class="calibre11">Model documentation</h3>
<p class="calibre3">Starting with every <a id="_idIndexMarker1433" class="calibre6 pcalibre pcalibre1"/>developer’s favorite topic: documentation! While documentation is not always the most fun part of a developer’s job, it is essential to building and maintaining production-grade systems. I’ve worked with clients and teams in various companies that have not always done a great job of developing accurate, high-quality documentation, and one thing that you can almost guarantee in the lack of such documentation is that it will make your job a lot more difficult when you need to maintain and improve your systems over time. Imagine that you join a new team and you are tasked with improving the performance of a particular application that uses ML to perform medical diagnoses, and you find that the original application and underlying model were developed years ago by people who have left the company and did not document how they implemented the system. This is not a good place to be in, and you would be surprised how commonly these kinds of scenarios exist in the industry. Perhaps most importantly in the context of this chapter, model documentation can be essential—and sometimes even legally required—for <span>compliance purposes.</span></p>
<p class="calibre3">So, what does high-quality model documentation look like? Generally, our documentation should keep detailed records of factors such as model design, data inputs, transformations, algorithms, and hyperparameters. Let’s take a look at each of these in <span>more detail.</span></p>
<h4 class="calibre20">Model design</h4>
<p class="calibre3">Documentation <a id="_idIndexMarker1434" class="calibre6 pcalibre pcalibre1"/>regarding model design should clearly define the purpose of the model, such as the objectives the model intends to achieve and the context within which it needs to operate. This includes potential use cases and intended users or systems that will interact with the model. We also need to provide a detailed description of the model’s architecture, such as its layers, structures, and interdependencies among different components of the model. Additionally, we should include details regarding the model’s design rationale, such as the reasoning behind choosing this particular model architecture or design, including comparisons <a id="_idIndexMarker1435" class="calibre6 pcalibre pcalibre1"/>with other potential designs that were considered and an explanation of why they were <span>not chosen.</span></p>
<h4 class="calibre20">Data inputs</h4>
<p class="calibre3">Our model <a id="_idIndexMarker1436" class="calibre6 pcalibre pcalibre1"/>documentation should describe the data collection process we used, including sources, methods of collection, and the timeframe during which data was collected. We should list all the features used by the model, including their definitions, types (for example, categorical, and numerical), any assumptions made about the data, and explanations as to why each feature is relevant to the model’s predictions. Additionally, we need to document any known issues in terms of data quality, including missing values, outliers, or inconsistencies, and how these issues were handled <span>or mitigated.</span></p>
<h4 class="calibre20">Transformations</h4>
<p class="calibre3">Another best <a id="_idIndexMarker1437" class="calibre6 pcalibre pcalibre1"/>practice is to detail the steps taken to clean and preprocess the data, such as handling missing data, normalization, encoding techniques, and feature engineering, including explanations of any methods used for feature selection or reduction (such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>), as <a id="_idIndexMarker1438" class="calibre6 pcalibre pcalibre1"/>depicted in <span><em class="italic">Figure 13</em></span><em class="italic">.2</em>), and the rationale for <span>their use:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer174">
<img alt="Figure 13.2: PCA" src="image/B18143_13_2.jpg" class="calibre123"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 13.2: PCA</p>
<h4 class="calibre20">Algorithms</h4>
<p class="calibre3">In our documentation, we<a id="_idIndexMarker1439" class="calibre6 pcalibre pcalibre1"/> should discuss why a particular ML algorithm was chosen, including a comparison with other algorithms that were considered and a rationale explaining the algorithm’s suitability for the problem at hand, citing relevant literature or empirical evidence, as appropriate. It’s also important to detail the configuration of the algorithm, including any customizations specific to <span>the project.</span></p>
<h4 class="calibre20">Hyperparameters</h4>
<p class="calibre3">We should include <a id="_idIndexMarker1440" class="calibre6 pcalibre pcalibre1"/>details on all hyperparameters that the model uses, providing a definition and range for each, as well as the process used for hyperparameter tuning, such as grid search, random search, or Bayesian optimization. Additionally, we should include the final values chosen for each hyperparameter and provide a rationale for why these particular values were chosen, supported by the <a id="_idIndexMarker1441" class="calibre6 pcalibre pcalibre1"/>tuning <span>process’s outcomes.</span></p>
<h4 class="calibre20">Additional factors</h4>
<p class="calibre3">Our documentation<a id="_idIndexMarker1442" class="calibre6 pcalibre pcalibre1"/> should also explain how the model’s performance was evaluated, including the metrics used (<strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>), <strong class="bold">area under the ROC curve</strong> (<strong class="bold">ROC-AUC</strong>)) and the results of these<a id="_idIndexMarker1443" class="calibre6 pcalibre pcalibre1"/> evaluations. We should document any known limitations of the <a id="_idIndexMarker1444" class="calibre6 pcalibre pcalibre1"/>model, document potential biases in the model’s predictions, and describe how these biases could impact different demographic groups or individuals. We also need to detail any regulatory standards or ethical guidelines relevant to the model and discuss how compliance has been ensured. Finally, we need to outline the plan for ongoing monitoring of the model’s performance and behavior in a production environment, including strategies for handling model drift, anomalies, or <span>performance degradation.</span></p>
<h3 class="calibre11">Model versioning</h3>
<p class="calibre3">Just as with code<a id="_idIndexMarker1445" class="calibre6 pcalibre pcalibre1"/> versioning in traditional software development, model versioning in ML projects is essential for ensuring that teams can trace back through the evolution of models, replicate results, roll back to previous versions when necessary, and maintain a record of all changes made throughout a model’s life cycle. This becomes especially important in large or collaborative environments where multiple iterations of models may be developed over time by many different people and teams. It’s also important for debugging, continuous improvement, and audit and compliance purposes. <span><em class="italic">Figure 13</em></span><em class="italic">.3</em> shows an example of model version metadata in the Vertex AI <span>Model Registry:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer175">
<img alt="Figure 13.3: Model version metadata in Vertex AI Model Registry" src="image/B18143_13_3.jpg" class="calibre169"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 13.3: Model version metadata in Vertex AI Model Registry</p>
<p class="calibre3">When I speak of model versioning, I’m referring to more than just the model artifacts, such as the actual model files, weights, and architecture. We should implement version tracking for every relevant item that is used in the model development process. This includes any code associated with the model, be it for preprocessing, training, evaluation, or deployment, as well as datasets, whether they consist of raw data, preprocessed data, or feature-engineered data. Even hyperparameter values and performance evaluation metrics should be versioned so that we can easily understand which versions of these elements pertain to which versions of <span>our models.</span></p>
<p class="calibre3">A well-implemented<a id="_idIndexMarker1446" class="calibre6 pcalibre pcalibre1"/> model versioning tool, such as Vertex AI Model Registry, will also enable us to add custom metadata to more comprehensively track our various <span>model versions.</span></p>
<h3 class="calibre11">Model monitoring</h3>
<p class="calibre3">As we discussed in<a id="_idIndexMarker1447" class="calibre6 pcalibre pcalibre1"/> previous chapters, we need to continuously monitor models after we have deployed them to production. The intention is to identify any drift, anomalies, or degradation in model performance, including important markers such as fairness and explainability metrics. Again, this is not only important<a id="_idIndexMarker1448" class="calibre6 pcalibre pcalibre1"/> for <strong class="bold">business continuity</strong> (<strong class="bold">BC</strong>) purposes but also for compliance reasons. We may have certified that our model is compliant with specific regulations before and during deployment, but it could drift and lapse out of compliance if not regularly monitored on an ongoing basis. If we have set up MLOps pipelines to automate the process of continuously improving our models over time, then this monitoring should extend to all aspects of our pipeline, such as ensuring that we perform data quality monitoring during the data preprocessing steps, in addition to new model validation and other important steps in <span>the process.</span></p>
<p class="calibre3">As with everything else regarding our model development and management implementations, we want our monitoring processes to be automated as much as possible. We should set thresholds beyond which some kind of corrective action is either automatically initiated or a human is notified if there is a problem. For example, if we see that performance or fairness metric values have changed by more than a specified amount, then corrective action is initiated, either by automatically training and evaluating a new model version on updated data or by paging on-call support engineers <span>to intervene.</span></p>
<p class="calibre3">As we also discussed in previous chapters, Vertex AI provides built-in tools for monitoring model <a id="_idIndexMarker1449" class="calibre6 pcalibre pcalibre1"/>performance both after deployment and throughout each relevant step in the model <span>development process.</span></p>
<h3 class="calibre11">Auditing and compliance</h3>
<p class="calibre3">Many industries have<a id="_idIndexMarker1450" class="calibre6 pcalibre pcalibre1"/> strict regulations (for example, GDPR, HIPAA, and various financial regulations) that require certain standards for factors such as data privacy, bias, transparency, and many more. Non-compliance with these regulations can lead to legal penalties and loss of customer trust. If we have workloads that are subject to regulatory standards, then we will need to establish auditing processes to help ensure our workloads remain compliant on an <span>ongoing basis.</span></p>
<p class="calibre3">How we implement such processes will mainly depend on the types of regulations with which we need to comply. Such processes could consist of a regular human-review process, or, as always, it would be best if we could automate auditing processes as much as possible and notify a human only when an issue that appears not to be automatically resolvable occurs. In the case of human-review processes, this is where documentation is inherently important because good-quality documentation can greatly simplify the review process and can make it easier to determine corrective actions when issues are identified. Ideally, we would want to identify potential risks in model performance, security, or reliability before they escalate into larger issues, and establishing regular review processes can help to ensure <span>this happens.</span></p>
<p class="calibre3">For some types of regulations, well-established audit checklists and <strong class="bold">standard operating procedures</strong> (<strong class="bold">SOPs</strong>) can be used, making the auditing task a bit easier. However, bear <a id="_idIndexMarker1451" class="calibre6 pcalibre pcalibre1"/>in mind that the regulatory landscape for ML is still evolving, and organizations must stay abreast of changes to <span>remain compliant.</span></p>
<p class="calibre3">In the previous chapter, we discussed the concept of explainability. Explainability is particularly important in the context of regulatory compliance. If you can’t easily or adequately explain how a given model or system works, then you will have a difficult time ensuring <span>regulatory compliance.</span></p>
<p class="calibre3">Now that we’ve covered many of the important factors of ML governance in quite a bit of detail, let’s zoom back out and focus on the bigger picture again. In the coming sections, we will<a id="_idIndexMarker1452" class="calibre6 pcalibre pcalibre1"/> discuss how to operationalize ML governance, what ML governance looks like in different industries, and how to stay abreast of the evolving ML <span>governance landscape.</span></p>
<h2 id="_idParaDest-271" class="calibre9"><a id="_idTextAnchor333" class="calibre6 pcalibre pcalibre1"/>Operationalization of ML governance</h2>
<p class="calibre3">As I’ve alluded to in each <a id="_idIndexMarker1453" class="calibre6 pcalibre pcalibre1"/>of the previous sections, we usually would want to automate as much of our ML governance practices and processes as possible, and there are tools and platforms that can assist in achieving this goal, such as data catalogs, model management tools, and auditing tools, which I describe in the <span>following sub-sections.</span></p>
<h3 class="calibre11">Data catalogs</h3>
<p class="calibre3">We briefly talked <a id="_idIndexMarker1454" class="calibre6 pcalibre pcalibre1"/>about data cataloging earlier in this chapter. Data catalogs are a kind of metadata management tool that helps companies find and manage large amounts of data spread across their organization, whether on-premises or in the cloud. You can think of a data catalog as a massive inventory of a company’s data assets, designed to let users discover, organize, and understand their data sources. We’ve already introduced Google Cloud Dataplex, which Google describes as an “<em class="italic">intelligent data fabric that enables organizations to centrally discover, manage, monitor, and govern their data across data lakes, data warehouses, and data marts, with consistent controls</em>.” <span><em class="italic">Figure 13</em></span><em class="italic">.4</em> shows an example of a catalog created by Google <span>Cloud Dataplex:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer176">
<img alt="Figure 13.4: Dataplex catalog" src="image/B18143_13_4.jpg" class="calibre170"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 13.4: Dataplex catalog</p>
<p class="calibre3">This concept of consistent controls is particularly relevant in the context of governance. With Dataplex, you can use metadata to describe all of your company’s data assets, and you can manage permissions in a uniform way across different Google Cloud data storage and processing tools. This is, of course, important from a governance perspective, and<a id="_idIndexMarker1455" class="calibre6 pcalibre pcalibre1"/> Dataplex also provides data quality and data lineage functionality, which are also important in the context <span>of governance.</span></p>
<h3 class="calibre11">Model management platforms</h3>
<p class="calibre3">These platforms <a id="_idIndexMarker1456" class="calibre6 pcalibre pcalibre1"/>assist in the model’s entire life cycle, including development, deployment, monitoring, and maintenance. They are essential for activities such as model versioning, experiment tracking, and model performance monitoring. By providing a structured environment for managing ML models, these platforms help ensure that models are reliable and reproducible and that they meet performance expectations. They also facilitate compliance by providing detailed model development and deployment process records. Of course, Vertex AI is Google Cloud’s native model management ecosystem, providing all of the <span>aforementioned features.</span></p>
<p class="calibre3">So far, we’ve focused on aspects of ML governance that are common to many industries. In the<a id="_idIndexMarker1457" class="calibre6 pcalibre pcalibre1"/> next section, let’s take a look at how ML governance applies to <span>specific industries.</span></p>
<h2 id="_idParaDest-272" class="calibre9"><a id="_idTextAnchor334" class="calibre6 pcalibre pcalibre1"/>ML governance in different industries and locations</h2>
<p class="calibre3">What can make <a id="_idIndexMarker1458" class="calibre6 pcalibre pcalibre1"/>regulatory compliance even more complex is that different countries, states, and industries can all have varying regulations. ML governance therefore varies significantly across different industries and geographic locations. In this section, we’ll discuss governance factors for specific regions and sectors, such as healthcare and finance, as well as <span>regional considerations.</span></p>
<h3 class="calibre11">Healthcare</h3>
<p class="calibre3">I currently live in<a id="_idIndexMarker1459" class="calibre6 pcalibre pcalibre1"/> the US, and when I hear the terms “regulatory compliance” and “healthcare” in the same sentence, the first thing that pops into my mind is the <strong class="bold">Health Insurance Portability and Accountability Act</strong> (<strong class="bold">HIPAA</strong>), which grants patients certain rights over their <a id="_idIndexMarker1460" class="calibre6 pcalibre pcalibre1"/>health information, including secure handling and confidentiality, and outlines significant penalties for breaches and non-compliance. If you work in healthcare in the US, you will almost certainly need to be aware of, and work within, the requirements of HIPAA. When designing and implementing ML systems in this industry, you must ensure that the data handling practices throughout the entire model development life cycle comply with those requirements. Other countries have their own regulatory requirements that you must learn and understand if you operate in <span>those areas.</span></p>
<h3 class="calibre11">Finance</h3>
<p class="calibre3">In the finance<a id="_idIndexMarker1461" class="calibre6 pcalibre pcalibre1"/> industry, fraud is perhaps one of the biggest concerns or threats, and the finance industry is heavily regulated to avoid the potential of fraud occurring. If your company operates in the US, for example, then the financial operations of your company will need to abide by regulations such as<a id="_idIndexMarker1462" class="calibre6 pcalibre pcalibre1"/> the <strong class="bold">Sarbanes-Oxley Act</strong> (<strong class="bold">SOX</strong>), which is mainly intended to prevent corporate fraud and improve the reliability and accuracy of corporate disclosures to protect investors. If your systems handle credit card data in any way, then you will likely need to comply with <strong class="bold">Payment Card Industry Data Security Standard</strong> (<strong class="bold">PCI DSS</strong>) regulations, which are a set of <a id="_idIndexMarker1463" class="calibre6 pcalibre pcalibre1"/>security standards relating to the<a id="_idIndexMarker1464" class="calibre6 pcalibre pcalibre1"/> secure handling of credit <span>card information.</span></p>
<h3 class="calibre11">Region-specific governance considerations</h3>
<p class="calibre3">Different regions<a id="_idIndexMarker1465" class="calibre6 pcalibre pcalibre1"/> have different regulatory requirements. For example, if you operate in the EU<a id="_idIndexMarker1466" class="calibre6 pcalibre pcalibre1"/> and the <strong class="bold">European Economic Area</strong> (<strong class="bold">EEA</strong>), then you will be subject to GDPR requirements, which protect the PI of individuals in <span>those regions.</span></p>
<p class="calibre3">The state of California has the CCPA, which regulates how businesses worldwide are allowed to handle the PI of <span>California residents.</span></p>
<p class="calibre3">There are also regulations that govern how data related to children must be handled, such as <a id="_idIndexMarker1467" class="calibre6 pcalibre pcalibre1"/>the <strong class="bold">Children’s Online Privacy Protection Act</strong> (<strong class="bold">COPPA</strong>), which is intended to protect the privacy of children under 13 years <span>of age.</span></p>
<p class="calibre3">In addition to region-specific and industry-specific regulations, we must also ensure that we remain compliant with obligations regarding fairness, transparency, explainability, accountability, and ethics. And, not only do we need to manage the complexity of regulations in different regions and industries, but that complexity is further extended by the fact that regulations may change over time. Let’s discuss this topic in more <span>detail next.</span></p>
<h2 id="_idParaDest-273" class="calibre9"><a id="_idTextAnchor335" class="calibre6 pcalibre pcalibre1"/>Keeping up with the evolving landscape of ML governance</h2>
<p class="calibre3">I can confidently say<a id="_idIndexMarker1468" class="calibre6 pcalibre pcalibre1"/> that the ML industry is one of the most quickly evolving industries at the moment. Companies, governments, and research institutions all over the world continue to invest heavily in this industry. As a result, compliance regulations related to this industry continue to evolve rapidly. In order to consistently remain successful and, quite frankly, ensure that you don’t get into trouble, your company needs to stay current in this ever-evolving landscape. In this section, I outline important concepts and best practices in <span>this space.</span></p>
<h3 class="calibre11">Ongoing education and training</h3>
<p class="calibre3">Your company’s<a id="_idIndexMarker1469" class="calibre6 pcalibre pcalibre1"/> employees need ongoing education to stay current with advances in regulatory requirements. In addition to technical capabilities, your employees must understand ethical considerations and risk management strategies related to ML deployment. Well-informed and well-trained individuals are less likely to make costly errors such as violating data privacy standards. Also, since this is not a static field, new types of bias and ethical dilemmas emerge almost daily. I highly recommend implementing regular training sessions, workshops, and educational resources for employees to learn about the latest trends, tools, ethical considerations, and best practices regarding <span>ML governance.</span></p>
<h3 class="calibre11">Regularly updating governance policies and practices</h3>
<p class="calibre3">Along with the<a id="_idIndexMarker1470" class="calibre6 pcalibre pcalibre1"/> technologies and ethical considerations, the legal landscape for data protection and privacy also continually evolves. Organizations must regularly update their governance policies to comply with new and changing laws and standards (GDPR, CCPA, or industry-specific regulations), and as new technologies and methodologies develop, governance policies must adapt to accommodate and manage them appropriately. What worked for a simple linear regression model might not be sufficient for a <a id="_idIndexMarker1471" class="calibre6 pcalibre pcalibre1"/>complex <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) system. Unfortunately, the threat landscape also continues to develop as new technologies emerge, and what was secure yesterday might not be quite as secure tomorrow. As a result, we need to regularly review and update security policies and practices to protect sensitive data and ML systems against new vulnerabilities and <span>attack strategies.</span></p>
<p class="calibre3">Additionally, in consideration of <a id="_idIndexMarker1472" class="calibre6 pcalibre pcalibre1"/>Google’s <strong class="bold">Site Reliability Engineering</strong> (<strong class="bold">SRE</strong>) practices, while we should do everything we can to avoid a negative event occurring, if such an event does occur, we need to learn from that scenario. As such, we should conduct thorough post-mortems on any issues and use these insights to improve policies and prevent <span>future occurrences.</span></p>
<p class="calibre3">Another set of concepts that are quite closely linked to SRE are encapsulated in the Google Cloud Architecture Framework, which I will discuss in the <span>next section.</span></p>
<h1 id="_idParaDest-274" class="calibre5"><a id="_idTextAnchor336" class="calibre6 pcalibre pcalibre1"/>An overview of the Google Cloud Architecture Framework</h1>
<p class="calibre3">In Google’s<a id="_idIndexMarker1473" class="calibre6 pcalibre pcalibre1"/> own words, “<em class="italic">the Google Cloud Architecture Framework provides recommendations and describes best practices to help architects, developers, administrators, and other cloud practitioners design and operate a cloud topology that’s secure, efficient, resilient, high-performing, and cost-effective</em>.” In this section, we discuss some of the key concepts from the framework and how they can be applied to AI/ML workloads on Google Cloud, especially within the context of <span>ML governance.</span></p>
<p class="calibre3">For reference, the framework documentation can be found at the <span>following URL:</span></p>
<p class="calibre3"><a href="https://cloud.google.com/architecture/framework" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/architecture/framework</span></a></p>
<p class="calibre3">Let’s begin with an overview of the fundamental concepts of the framework, which are referred to <a id="_idIndexMarker1474" class="calibre6 pcalibre pcalibre1"/>as <strong class="bold">pillars</strong>. One way to think of it is that, by ensuring all of these pillars are implemented, we can build a solid and enduring <span>structure (system).</span></p>
<p class="calibre3">The categories of the framework are <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8"><span>System design</span></li>
<li class="calibre8"><span>Operational excellence</span></li>
<li class="calibre8">Security, privacy, <span>and compliance</span></li>
<li class="calibre8"><span>Reliability</span></li>
<li class="calibre8"><span>Cost optimization</span></li>
<li class="calibre8"><span>Performance optimization</span></li>
</ul>
<p class="calibre3">In the following sub-sections, I’ll describe what each category represents, starting with <span>system design.</span></p>
<h2 id="_idParaDest-275" class="calibre9"><a id="_idTextAnchor337" class="calibre6 pcalibre pcalibre1"/>Pillar 1 – System design</h2>
<p class="calibre3">Interestingly, the <em class="italic">System design</em> category <a id="_idIndexMarker1475" class="calibre6 pcalibre pcalibre1"/>in the <a id="_idIndexMarker1476" class="calibre6 pcalibre pcalibre1"/>Google Cloud Architecture Framework is more akin to the foundation of the overall framework than a pillar, because a well-designed system appropriately incorporates all of the pillars. The <em class="italic">System design</em> category<a id="_idIndexMarker1477" class="calibre6 pcalibre pcalibre1"/> encapsulates four <span>core principles:</span></p>
<ul class="calibre16">
<li class="calibre8"><span>Document everything</span></li>
<li class="calibre8">Simplify your design and use fully <span>managed services</span></li>
<li class="calibre8">Decouple <span>your architecture</span></li>
<li class="calibre8">Use a <span>stateless architecture</span></li>
</ul>
<p class="calibre3">Let’s take a <a id="_idIndexMarker1478" class="calibre6 pcalibre pcalibre1"/>look at each of these in <span>more detail.</span></p>
<h3 class="calibre11">Principle 1 – Document everything</h3>
<p class="calibre3">We touched on this <a id="_idIndexMarker1479" class="calibre6 pcalibre pcalibre1"/>principle quite a bit in this chapter’s <em class="italic">ML model governance</em> section. In a broader system design context, we’re not just referring to documentation related to our model, but rather related to every aspect of how our systems are implemented, including aspects such as architecture diagrams and maintenance runbooks. The question I always ask in this context is: if a new member joined our team and were required to quickly learn everything they need to know to improve and maintain a system we’ve built, what are all of the details they would need to review? If any of those details are not adequately documented, then that’s a gap that we need to address by developing the required documentation. This also helps other teams that we need to collaborate with or that need to interact with our system in some way, and it makes everybody’s job easier if regulatory compliance officers need to audit <span>our systems.</span></p>
<h3 class="calibre11">Principle 2 – Simplify your design and use fully managed services</h3>
<p class="calibre3">Ever since I heard of the concept of Occam’s razor, I have been a big fan of it. There are a number of different ways in which it can be summarized, but a fairly common one is: “If there are two possible explanations for a particular phenomenon, use the simpler one.”This can be extended to say: don’t make things more complicated than needed. The opposite of this is the concept of a Rube Goldberg machine, which applies an extremely complex set of mechanisms to achieve a simple goal. While Rube Goldberg machines can be fun to watch, they are generally quite impractical, and they are not what you want to implement in the design of your large-scale, low-latency, highly sensitive <span>production system.</span></p>
<p class="calibre3">Keeping your system design as simple as possible has many benefits, such as making it easier to troubleshoot, maintain, and secure your systems. Highly complex systems with many different components are difficult to troubleshoot when something goes wrong. Similarly, in terms of security, highly complex systems often have a larger <strong class="bold">attack surface</strong>. We <a id="_idIndexMarker1480" class="calibre6 pcalibre pcalibre1"/>will cover that concept in more detail later in <span>this chapter.</span></p>
<p class="calibre3">Another way in which we can make our jobs easier is by offloading responsibilities to a <strong class="bold">cloud service provider</strong> (<strong class="bold">CSP</strong>). One<a id="_idIndexMarker1481" class="calibre6 pcalibre pcalibre1"/> of the primary benefits of cloud computing is that CSPs offer platforms and systems that have been designed to address common needs in the industry. When a company runs all of its workloads in its own data centers, it either needs to build solutions and platforms completely by itself or install and manage software created by other companies. Both of those options incur a lot of staff overhead and require specific training <span>to manage.</span></p>
<p class="calibre3">Let’s take the example of building and maintaining a platform to enable data scientists to develop and deploy ML models. In previous chapters, we outlined the various steps that are required in the model development life cycle. If we are running all of our workloads “on-premises” (that is, not in the cloud), then we need to either design, build, and maintain a <a id="_idIndexMarker1482" class="calibre6 pcalibre pcalibre1"/>platform that supports all of those steps or, as many companies do, we could try to hack something together, using a hodge-podge of random third-party software solutions that all require specific training and don’t necessarily work very well together. In the cloud, however, we can simply use the platform provided by the cloud provider and let them do all of the hard work for us so that our teams can focus on their core competencies and primary objectives rather than building and <span>maintaining infrastructure.</span></p>
<p class="calibre3">Similar concepts exist across other types of workloads. For example, on-premises, we may build and maintain our own Kubernetes environments for our company’s containerized workloads. We would spend a lot of time maintaining those systems, but in the cloud, we could use managed services such as <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>) or <a id="_idIndexMarker1483" class="calibre6 pcalibre pcalibre1"/>Cloud Run. In this context, we talk about services that offload more of the infrastructure management tasks to the cloud provider as “going further up the stack.” In the case of GKE and Cloud Run, Cloud Run would be seen as “further up the stack” because it provides more of a fully managed experience than the basic form of GKE, although the more recent launch of GKE Autopilot also provides a very hands-off approach, in which more of the platform management tasks are implemented by <span>Google Cloud.</span></p>
<h3 class="calibre11">Principle 3 – Decouple your architecture</h3>
<p class="calibre3">This ties in with <em class="italic">principle 2</em> to an extent. It refers to breaking your overall system design into smaller components. A classic example of this is to break a monolithic application into microservices. The reason is that each microservice is easier to manage than a very large and complex monolithic system architecture. Smaller components can be developed and scaled independently, which can improve the speed of development of new features in <span>your systems.</span></p>
<h3 class="calibre11">Principle 4 – Use a stateless architecture</h3>
<p class="calibre3">In a stateless architecture, each transaction is independent. When processing a request, the server does not remember any prior requests or transactions, and the client must send any necessary data for a transaction in each request. In a stateful architecture, on the other hand, the server maintains the state of the client’s session. The context of previous transactions<a id="_idIndexMarker1484" class="calibre6 pcalibre pcalibre1"/> is remembered, and future transactions can be affected by what happened in <span>the past.</span></p>
<p class="calibre3">I would add the words “whenever possible” to the title of this principle because sometimes your system will need to maintain state, but what you would want to do, as much as possible, is minimize the amount of state that needs to be maintained and offload the state management from your application to a separate mechanism such as a <span>caching layer.</span></p>
<p class="calibre3">Stateless architectures are generally easier to scale because they don’t require maintaining client states, allowing requests to be processed by any available server. Stateful architectures require more complex infrastructure to ensure that the client interacts with the same <a id="_idIndexMarker1485" class="calibre6 pcalibre pcalibre1"/>server or that the state is shared, which can be challenging in large-scale environments. Stateful systems also use more resources to manage and store session data, which can further affect scalability and <span>system complexity.</span></p>
<h2 id="_idParaDest-276" class="calibre9"><a id="_idTextAnchor338" class="calibre6 pcalibre pcalibre1"/>Pillar 2 – Operational excellence</h2>
<p class="calibre3">The <em class="italic">Operational excellence</em> pillar is<a id="_idIndexMarker1486" class="calibre6 pcalibre pcalibre1"/> concerned <a id="_idIndexMarker1487" class="calibre6 pcalibre pcalibre1"/>with efficiently running, managing, and monitoring systems on Google Cloud. It includes concepts such as automation, observability, <span>and scalability.</span></p>
<p class="calibre3">This pillar talks about automating system deployments using <strong class="bold">continuous integration and continuous deployment</strong> (<strong class="bold">CI/CD</strong>), and<a id="_idIndexMarker1488" class="calibre6 pcalibre pcalibre1"/> managing your infrastructure using <strong class="bold">infrastructure as code</strong> (<strong class="bold">IaC</strong>). This is <a id="_idIndexMarker1489" class="calibre6 pcalibre pcalibre1"/>a very important concept because it provides all of the benefits of traditional software development, such as version tracking and incremental updates. If you manage your infrastructure updates using version tracking mechanisms, then you can maintain strong records for auditing purposes, and if issues are introduced by any updates, then you can more easily roll back to a previous version that was known to work well. This is often referred to as <strong class="bold">GitOps</strong>, and<a id="_idIndexMarker1490" class="calibre6 pcalibre pcalibre1"/> the opposite of this is referred<a id="_idIndexMarker1491" class="calibre6 pcalibre pcalibre1"/> to as <strong class="bold">ClickOps</strong>. In the case of ClickOps, infrastructure updates are made by people clicking around in a UI. If you have hundreds of people in your technology organization, and every day they are all making updates to your infrastructure by clicking around in a UI, then it can become difficult to coordinate and track these updates over time. Terraform is a popular tool for implementing IaC on <span>Google Cloud.</span></p>
<p class="calibre3">The <em class="italic">Operational excellence</em> pillar <a id="_idIndexMarker1492" class="calibre6 pcalibre pcalibre1"/>also outlines best practices for incorporating testing throughout the software delivery life cycle. This includes unit tests, integration tests, system tests, and other types of tests such as performance and security testing. Rather than testing everything at the end, we should aim to include each type of test as relevant throughout each step in the development life cycle. For example, unit tests could be automated as part of our <span>build process.</span></p>
<p class="calibre3">When deploying<a id="_idIndexMarker1493" class="calibre6 pcalibre pcalibre1"/> software, the <em class="italic">Operational excellence</em> pillar recommends using approaches such as immutable infrastructure updates via blue/green deployments and A/B or canary tests. Google Cloud provides CI/CD tooling that can be used to implement these strategies. The recommendation is to use small but frequent updates to your systems, which are easier to manage and roll back than large, <span>infrequent changes.</span></p>
<p class="calibre3">In terms of observability, this pillar provides recommendations on effectively setting up monitoring, alerting, and logging, including common metrics to keep an eye on, and defining thresholds beyond which some kind of alert or corrective action should be invoked. It also talks about the importance of setting up audit trails to keep track of changes to your systems. For cases in which something does go wrong, it provides guidelines on establishing support and escalation procedures, as well as review processes such as post-mortem assessments to learn from <span>any failures.</span></p>
<p class="calibre3">Of course, it’s also important to ensure that your infrastructure is adequately scaled to handle your expected traffic volumes and that you implement plans to proactively scale accordingly for known <span>peak events.</span></p>
<p class="calibre3">Finally, this pillar covers the importance of automating as many of your system management tasks as possible to minimize how much you need to rely on potentially error-prone manual<a id="_idIndexMarker1494" class="calibre6 pcalibre pcalibre1"/> processes<a id="_idIndexMarker1495" class="calibre6 pcalibre pcalibre1"/> to keep your systems <span>running effectively.</span></p>
<h2 id="_idParaDest-277" class="calibre9"><a id="_idTextAnchor339" class="calibre6 pcalibre pcalibre1"/>Pillar 3 – Security, privacy, and compliance</h2>
<p class="calibre3">This is perhaps the most <a id="_idIndexMarker1496" class="calibre6 pcalibre pcalibre1"/>relevant of the pillars in the context of ML governance, and we’ve already touched on these topics earlier in this chapter, but here, we will take a look at how these concepts are more formally structured within the Google Cloud <span>Architecture Framework.</span></p>
<p class="calibre3">In addition to the concept of DiD that we discussed earlier, Google recommends strategies to<a id="_idIndexMarker1497" class="calibre6 pcalibre pcalibre1"/> implement <strong class="bold">security by default</strong>. This consists of best practices for ensuring that security is built in as the default configuration in your system architecture, including concepts such as the <strong class="bold">principle of least privilege</strong> (<strong class="bold">PoLP</strong>), in which <a id="_idIndexMarker1498" class="calibre6 pcalibre pcalibre1"/>users are given the minimum permissions required to perform their job functions, and nothing more. It also refers to locking down access at the network level. For example, if you know that, in normal operating circumstances, your system should only ever be accessed from one or two other systems, then you could set up network rules that block access from any sources other than those specific systems. Google Cloud also provides an offering called <strong class="bold">Confidential Computing</strong> for <a id="_idIndexMarker1499" class="calibre6 pcalibre pcalibre1"/>processing <span>sensitive data.</span></p>
<p class="calibre3">I want to take this opportunity to highlight that the pillars of the Google Cloud Architecture Framework are often interrelated. For example, we talked about the concept of GitOps in the context of the <em class="italic">Operational excellence</em> pillar. This concept of using IaC to manage how you deploy to your systems is a highly recommended way to establish security-by-default practices. For example, you can create Terraform modules that undergo stringent security assessment processes for setting up your infrastructure in a way that aligns with your corporate security policies and industry-wide best practices. Once those “secure-by-default” modules have been approved, anybody in your company could safely use them to set up the required infrastructure securely. This makes it much easier for your employees to abide by your security policies in terms of provisioning infrastructure. To make it easy for you to provision infrastructure resources that align with security best practices, Google Cloud provides the <strong class="bold">security foundations blueprint</strong>, which<a id="_idIndexMarker1500" class="calibre6 pcalibre pcalibre1"/> you can reference at the <span>following URL:</span></p>
<p class="calibre3"><a href="https://cloud.google.com/architecture/security-foundations" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/architecture/security-foundations</span></a></p>
<p class="calibre3">As an association between this pillar and the pillar of <em class="italic">Operational excellence</em>, the CI/CD pipelines that are used to deploy your resources should have security mechanisms built in. For example, you could use automation to check for security vulnerabilities when artifacts are created. Google Cloud also provides a mechanism called Binary Authorization to validate the contents of Docker containers that are built and deployed by your CI/CD pipelines to ensure that those images contain exactly what you expect them to contain and nothing more. It can also validate that a specific build system or pipeline created a specific container image. If a security check highlights any potential problems at any point in your CI/CD pipeline, the pipeline can automatically be halted to ensure that potential security threats are not introduced into your deployments. Similarly, you can use Google Cloud’s Artifact Analysis feature to scan automatically for potential vulnerabilities in containers stored in Artifact Registry and Container Registry. Even after deployment, you can continuously scan your web applications by using Google Cloud’s Web Security Scanner to identify vulnerabilities in applications deployed to Compute Engine, App Engine, <span>and GKE.</span></p>
<p class="calibre3">This pillar also provides recommendations on proactively identifying and cataloging risks to your company and how to mitigate common risks. Google Cloud has also recently launched the Risk Protection Program, which includes tools such as Risk Manager, to help you <span>manage risks.</span></p>
<p class="calibre3">Of course, <strong class="bold">IAM</strong> is an important component of this pillar. Google<a id="_idIndexMarker1501" class="calibre6 pcalibre pcalibre1"/> Cloud provides many tools that help manage this aspect, such as IAM and Cloud Audit Logs, which we discussed are essential for access management <span>and auditing.</span></p>
<p class="calibre3">This pillar also calls out the importance of using cloud asset management tools such as Cloud Asset Inventory to track all of your company’s technology assets and monitor for deviations from your <span>compliance policies.</span></p>
<p class="calibre3">In addition to all of the topics we’ve covered in this section, the security pillar also covers topics such as network security, data security, privacy, and regulatory compliance, which <a id="_idIndexMarker1502" class="calibre6 pcalibre pcalibre1"/>we covered earlier in this chapter. It also provides details on how to use Google Cloud Assured Workloads to help you meet your compliance obligations, how to monitor for compliance, and how to address data sovereignty, data residency, software sovereignty, and <span>operational sovereignty.</span></p>
<h2 id="_idParaDest-278" class="calibre9"><a id="_idTextAnchor340" class="calibre6 pcalibre pcalibre1"/>Pillar 4 – Reliability</h2>
<p class="calibre3">The <em class="italic">Reliability</em> pillar <a id="_idIndexMarker1503" class="calibre6 pcalibre pcalibre1"/>focuses on <a id="_idIndexMarker1504" class="calibre6 pcalibre pcalibre1"/>concepts <a id="_idIndexMarker1505" class="calibre6 pcalibre pcalibre1"/>such as <strong class="bold">high availability</strong> (<strong class="bold">HA</strong>), scalability, automated change management, and <strong class="bold">disaster recovery</strong> (<strong class="bold">DR</strong>). It<a id="_idIndexMarker1506" class="calibre6 pcalibre pcalibre1"/> covers topics that some of you may know from Google’s SRE<a id="_idIndexMarker1507" class="calibre6 pcalibre pcalibre1"/> practices, such as defining <strong class="bold">service-level indicators</strong> (<strong class="bold">SLIs</strong>), <strong class="bold">service-level objectives</strong> (<strong class="bold">SLOs</strong>), <strong class="bold">service-level agreements</strong> (<strong class="bold">SLAs</strong>), and<a id="_idIndexMarker1508" class="calibre6 pcalibre pcalibre1"/> error <a id="_idIndexMarker1509" class="calibre6 pcalibre pcalibre1"/>budgets. As was the case with the <em class="italic">Operational excellence</em> pillar, the <em class="italic">Reliability</em> pillar includes observability as a major component. It also reiterates some other concepts from the <em class="italic">Operational excellence</em> pillar, such as automating deployments and incremental updates using CI/CD pipelines, and the importance of setting up appropriate observability and alerting mechanisms, <strong class="bold">incident management</strong> (<strong class="bold">IM</strong>), and <a id="_idIndexMarker1510" class="calibre6 pcalibre pcalibre1"/><span>post-mortem practices.</span></p>
<p class="calibre3">This pillar talks about ways of creating redundancy for higher availability in your system architectures, including using multiple Google Cloud zones and regions to mitigate any potential issues that may occur in a particular location. In addition to these kinds of proactive mitigation techniques, it also outlines practices for establishing DR strategies, such as synchronizing data to other regions and establishing playbooks for failing over to those regions <span>if needed.</span></p>
<p class="calibre3">Finally, it goes into much detail regarding best practices for specific Google Cloud products. This pillar contains a wealth of knowledge and much more detail on many specific Google Cloud products than would be appropriate to <span>include here.</span></p>
<h2 id="_idParaDest-279" class="calibre9"><a id="_idTextAnchor341" class="calibre6 pcalibre pcalibre1"/>Pillar 5 – Cost optimization</h2>
<p class="calibre3">You can be guaranteed<a id="_idIndexMarker1511" class="calibre6 pcalibre pcalibre1"/> this is <a id="_idIndexMarker1512" class="calibre6 pcalibre pcalibre1"/>very important to almost every customer. In fact, cost optimization is often one of the main factors that entice companies to move to the cloud in the first place. When companies run their workloads in their own data centers, they often have to purchase and install enough infrastructure (and more) to cater for their highest peak events that may only happen once or twice per year. For the rest of the year, that infrastructure is highly under-utilized, which amounts to a lot of wasted money. In the cloud, however, companies can scale their infrastructure up and down based on what they actually need and therefore do not need to waste money on over-provisioned infrastructure. Also, as discussed earlier in this chapter, offloading infrastructure management to a cloud provider enables companies to invest their time in innovation and developing features that support their <span>core business.</span></p>
<p class="calibre3">The first major focus of this pillar is on the concept of financial <a id="_idIndexMarker1513" class="calibre6 pcalibre pcalibre1"/>operations or <strong class="bold">FinOps</strong>,  which is a cultural paradigm that includes a set of technical processes and business best practices to help organizations optimize and manage their cloud investments more effectively. In this context, it’s important to provide each technology team in the organization with visibility into their cloud spend and for each team to take accountability for that spend. To learn more about FinOps, I recommend reading the Google Cloud FinOps whitepaper, which can be found at the <span>following URL:</span></p>
<p class="calibre3"><a href="https://cloud.google.com/resources/cloud-finops-whitepaper" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/resources/cloud-finops-whitepaper</span></a></p>
<p class="calibre3">Remember that we generally cannot optimize or improve something without monitoring it. As such, the <em class="italic">Cost optimization</em> pillar provides recommendations regarding monitoring costs, analyzing trends, and forecasting future costs. If you forecast that you will spend a certain amount of money in the next year or the next 3 years, you can<a id="_idIndexMarker1514" class="calibre6 pcalibre pcalibre1"/> purchase <strong class="bold">committed use discounts</strong> (<strong class="bold">CUDs</strong>) to save money on those workloads. You can also use labels to categorize your expenses in billing reports, such as attributing resource expenses to specific workloads <span>and environments.</span></p>
<p class="calibre3">The <em class="italic">Cost optimization</em> pillar also provides best practices on <strong class="bold">optimizing</strong> resource usage to reduce costs, such as ensuring that you provision your infrastructure based on your current and projected needs (including some buffer where appropriate), and do not over-provision. This is referred to as <strong class="bold">right-sizing</strong>, and Google Cloud even provides a right-sizing recommender<a id="_idIndexMarker1515" class="calibre6 pcalibre pcalibre1"/> that can highlight opportunities for improving your sizing by identifying resources that appear to be under-utilized (and therefore over-provisioned). You should also use auto-scaling, which, in addition to ensuring that you have enough resources to serve your required traffic volumes, can scale resources down when they’re not needed, thus <span>saving money.</span></p>
<p class="calibre3">When implementing cost optimization mechanisms, it’s important to set up budgets, alerts, and quotas to control your spending. For example, you can specify a certain spending budget and get alerted when you are close to reaching that budget. You can also use quotas to set hard limits on resource usage and can set API caps to limit API usage after a certain threshold <span>is reached.</span></p>
<p class="calibre3">As with the <em class="italic">Reliability</em> pillar, the <em class="italic">Cost optimization</em> pillar provides detailed best practices for many specific Google Cloud<a id="_idIndexMarker1516" class="calibre6 pcalibre pcalibre1"/> products, such as optimizing storage tiers in <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>) or<a id="_idIndexMarker1517" class="calibre6 pcalibre pcalibre1"/> optimizing <a id="_idIndexMarker1518" class="calibre6 pcalibre pcalibre1"/>partitions <span>in BigQuery.</span></p>
<h2 id="_idParaDest-280" class="calibre9"><a id="_idTextAnchor342" class="calibre6 pcalibre pcalibre1"/>Pillar 6 – Performance optimization</h2>
<p class="calibre3">Performance<a id="_idIndexMarker1519" class="calibre6 pcalibre pcalibre1"/> optimization <a id="_idIndexMarker1520" class="calibre6 pcalibre pcalibre1"/>can be linked to cost optimization, so there is some overlap in terms of these concepts. For example, if your systems are performing optimally, then they may be less costly to run. A well-implemented auto-scaling strategy is a prime example of this. The <em class="italic">Performance optimization</em> pillar provides recommendations on how to define performance requirements, how to monitor and analyze performance, and, of course, how to <span>optimize performance.</span></p>
<p class="calibre3">In terms of monitoring and analyzing performance, this refers back to the concept of observability, in which we need to implement and monitor performance metrics such as latency and <span>resource utilization.</span></p>
<p class="calibre3">As with the <em class="italic">Reliability</em> and <em class="italic">Cost optimization</em> pillars, the <em class="italic">Performance optimization</em> pillar also provides many in-depth recommendations for specific Google Cloud products, which is a level of detail beyond what would be appropriate to <span>include here.</span></p>
<p class="calibre3">Now that you understand what the Google Cloud Architecture Framework is and what it consists of, let’s look at how we can apply its concepts in the context of AI/ML on <span>Google Cloud.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Across all of the categories in the Google Cloud Architecture Framework, the theme that is reiterated the most is automation. The idea is to automate everything as much as possible. Vetted, repeatable processes that can run automatically tend to make our jobs easier across <span>all pillars.</span></p>
<h1 id="_idParaDest-281" class="calibre5"><a id="_idTextAnchor343" class="calibre6 pcalibre pcalibre1"/>Architecture Framework concepts about AI/ML workloads on Google Cloud</h1>
<p class="calibre3">In this section, we will assess how the Google Cloud Architecture Framework can be used with regard to AI/ML workloads on Google Cloud. We will use the steps in the model development life cycle to frame our discussion. As a reminder, the steps in the model development life cycle are summarized at a high level in <span><em class="italic">Figure 13</em></span><span><em class="italic">.5</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer177">
<img alt="Figure 13.5: The ML model development life cycle" src="image/B18143_13_5.jpg" class="calibre171"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 13.5: The ML model development life cycle</p>
<p class="calibre3">Let’s begin with the data collection and preparation activities in the model development life cycle, which include gathering, ingesting, storing, and <span>processing data.</span></p>
<p class="callout-heading">Spoiler alert!</p>
<p class="callout">You will notice that we have already been using many of these practices throughout this book. Here, we are calling them out explicitly so that you can understand how they apply to workloads <span>in general.</span></p>
<h2 id="_idParaDest-282" class="calibre9"><a id="_idTextAnchor344" class="calibre6 pcalibre pcalibre1"/>Data collection and preparation</h2>
<p class="calibre3">Data management<a id="_idIndexMarker1521" class="calibre6 pcalibre pcalibre1"/> is perhaps<a id="_idIndexMarker1522" class="calibre6 pcalibre pcalibre1"/> the most important of all the topics related to ML governance. As we know by now, the quality of your data directly impacts the quality of your models. Also, in terms of security, when malicious actors try to access your systems, they are usually after your data, because data is such a valuable resource, and data breaches can have catastrophic effects for your company. Let’s look at how we can apply recommendations in the Google Cloud Architecture Framework regarding data handling. In this case, we will not discuss system design as a separate pillar, because an effective system design encapsulates all of the <span>other pillars.</span></p>
<h3 class="calibre11">Operational excellence in data collection and preparation</h3>
<p class="calibre3">Remember that <a id="_idIndexMarker1523" class="calibre6 pcalibre pcalibre1"/>operational excellence focuses on concepts such as automation, observability, and availability. The following sub-sections explore these concepts in the context of data collection <span>and preparation.</span></p>
<h4 class="calibre20">Process automation and integration</h4>
<p class="calibre3">In earlier chapters of this book, I talked about the importance of building data pipelines to automate our data processing steps. Essentially, we want to establish repeatable processes and then put in place mechanisms to run those processes automatically, either based on a schedule (such as daily, weekly, or monthly) or in reaction to some event, such as new data becoming available. Hence, the concept of implementing data processing pipelines is an application of the automation recommendations outlined in the <em class="italic">Operational excellence</em> pillar of the Google Cloud Architecture Framework. Google Cloud provides many services that we can integrate together to set up data processing pipelines, such as Dataproc, Dataflow, GCS, Pub/Sub, <span>and BigQuery.</span></p>
<h4 class="calibre20">Consistency and standardization</h4>
<p class="calibre3">In the MLOps chapter, I shared experiences I’ve had with various organizations at different levels of maturity in terms of how they implemented their ML workload operations. When companies do not have well-established processes in place, their teams tend to use lots of random different tools that each have their own learning curves, and they maintain their artifacts in silos. These practices are not conducive to company-wide collaboration, and they hinder the scalability and efficiency of a company’s ML operations. I then talked about the importance of standardizing tools and processes throughout the company in order to overcome those limitations. This all relates to the operational excellence pillar of the Google Cloud Architecture Framework. Consistency in tools, libraries, and processes across teams and projects reduces complexity and learning curves, making it easier to manage and scale operations. Perhaps the most relevant example of this, in relation to ML operations, is to use Vertex AI for all of our model development and deployment needs, since it provides a standard set of tools for every step in the model development and management <span>life cycle.</span></p>
<h4 class="calibre20">Observability</h4>
<p class="calibre3">All of the regular system monitoring and logging requirements also apply to AI/ML workloads, and AI/ML workloads also have additional requirements regarding monitoring the quality of model prediction outputs on an ongoing basis to ensure that they do not drift over time. In earlier chapters, we discussed ML-specific metrics to monitor, such as MSE for linear regression use cases or AUC-ROC scores for classification use cases, as well as fairness metrics. Google Cloud Logging and Google Cloud Monitoring can be used for observability purposes at all points in the ML model development life<a id="_idIndexMarker1524" class="calibre6 pcalibre pcalibre1"/> cycle on Google Cloud, from evaluating training validation metrics to tracking the latency of responses from a model deployed on a Vertex AI prediction endpoint, and Vertex AI Model Monitoring can be used to watch <span>for drift.</span></p>
<h3 class="calibre11">Security, privacy, and compliance in data collection and preparation</h3>
<p class="calibre3">By now, we should<a id="_idIndexMarker1525" class="calibre6 pcalibre pcalibre1"/> thoroughly understand that data security and privacy are paramount in almost every company. One sure way to quickly lose customers and damage your company’s reputation is to become a victim of a data breach. My opinion is that managing sensitive data securely is the most important thing you can possibly do; it takes priority over all other considerations in this chapter, this book, and your <span>company’s business.</span></p>
<p class="calibre3">In our overview of this pillar earlier in this chapter, we talked about how data and systems should be protected through multiple layers of defenses (DiD), incorporating factors such as access management, encryption, and network security. The following sub-sections explore these concepts in the context of data collection <span>and preparation.</span></p>
<h4 class="calibre20">Data access control</h4>
<p class="calibre3">When collecting and preparing data in Google Cloud, we can use IAM to ensure that only authorized individuals and services can access the data, and we can control who can view, modify, or interact with the data based on roles and responsibilities. For example, a data scientist might have permission to read and analyze data but not delete it, or finance data may only be accessible to the finance department. This is made easier if we use Google Cloud Dataplex to build a data catalog because Dataplex allows us to centrally manage permissions for our data assets across multiple different GCS and <span>processing services.</span></p>
<h4 class="calibre20">Data protection and encryption</h4>
<p class="calibre3">Sensitive data should always be encrypted, both when it’s stored (at rest) and when it’s being transferred between services or locations (in transit). Google Cloud provides automatic encryption for data stored in its services, and we can use TLS to protect data in transit. For highly sensitive data, we can even encrypt it during processing by using Google Cloud<a id="_idIndexMarker1526" class="calibre6 pcalibre pcalibre1"/> Confidential Computing. Also, during the data preparation phase, sensitive data elements can be masked or tokenized to hide their actual values, therefore enhancing privacy while still allowing the data to be used <span>for analysis.</span></p>
<h4 class="calibre20">Data classification and discovery</h4>
<p class="calibre3">We can use the Google Cloud Sensitive Data Protection service to discover, classify, and mask sensitive elements in datasets. When collecting data, this service can help automatically identify information<a id="_idIndexMarker1527" class="calibre6 pcalibre pcalibre1"/> such as <strong class="bold">personally identifiable information</strong> (<strong class="bold">PII</strong>) or financial data so that we can treat it with higher levels of protection. This is another area in which Google Cloud Dataplex can help because tracking all of our data assets in a data catalog makes classification and <span>discovery easy.</span></p>
<h4 class="calibre20">Auditing and monitoring</h4>
<p class="calibre3">We introduced Cloud Audit Logs earlier. We can use Cloud Audit Logs to keep a detailed log of who accesses the data and what operations they perform, which is important for accountability and traceability. This is especially relevant in ML workloads, where understanding who introduced what data and when can be required for explainability and troubleshooting. And, guess what?! Google Cloud Dataplex integrates with Cloud Audit Logs to generate audit logs for actions that are performed in the <span>data catalog.</span></p>
<h4 class="calibre20">Data retention and deletion</h4>
<p class="calibre3">When using Google Cloud, we can establish policies about how long data should be retained based on its nature and relevance. When using GCS, for example, a retention policy can be specified to prevent an object from being deleted within the timeframe specified by the retention policy. This can be important for regulatory purposes or to comply with legal holds. Conversely, Object Lifecycle Management can be used to automatically delete data after a certain period (as long as it does not conflict with a data retention policy). For sensitive data that you want to delete permanently, Google Cloud provides mechanisms to ensure that the deletion is performed securely and <span>is irrecoverable.</span></p>
<h4 class="calibre20">Compliance frameworks and certifications</h4>
<p class="calibre3">Google Cloud provides tools and documentation to help businesses comply with standards such as GDPR and HIPAA (as well as many more), and it undergoes independent third-party audits to ensure its services comply with common <span>regulatory standards.</span></p>
<h4 class="calibre20">Resilience against threats</h4>
<p class="calibre3">Services such as Cloud Security Command Center and Event Threat Detection allow for continuous data and environment monitoring for potential threats, offering insights and actionable recommendations. Regularly scanning and assessing the systems involved in data collection <a id="_idIndexMarker1528" class="calibre6 pcalibre pcalibre1"/>and preparation for vulnerabilities can help ensure that data isn’t exposed to potential breaches. You can also use VPC network security and VPC-SC to control access to your data storage and processing systems and to prevent <span>data exfiltration.</span></p>
<p class="calibre3">All of the items we discussed in this section are important to ensure that data is handled securely and that user privacy is protected. Ethical considerations also come into play to ensure that data is collected and used in ways that are fair, transparent, and don’t propagate <a id="_idIndexMarker1529" class="calibre6 pcalibre pcalibre1"/>biases, especially when it’ll be used to train ML models that might impact <span>individuals’ lives.</span></p>
<h3 class="calibre11">Reliability in data collection and preparation</h3>
<p class="calibre3">Remember that <a id="_idIndexMarker1530" class="calibre6 pcalibre pcalibre1"/>the <em class="italic">Reliability</em> pillar in the Google Cloud Architecture Framework focuses on ensuring that services and applications perform consistently and meet the expected SLOs, even in the case of unexpected disturbances or increased demands. The following sub-sections discuss how we can apply concepts from the <em class="italic">Reliability</em> pillar in the data collection and preparation phases of the ML model development <span>life cycle.</span></p>
<h4 class="calibre20">Automated data ingestion and processing</h4>
<p class="calibre3">Relying on manual processes for data collection can be prone to errors, while automated data ingestion helps to ensure that data can be collected consistently. We can also automate data validation steps to ensure that incoming data adheres to the expected formats and value ranges, which can prevent corrupted or malformed data from propagating through our data processing and ML pipelines. For data transformation scripts and configurations, we should use version control to ensure that if changes introduce errors, we can easily revert to a previous, <span>stable version.</span></p>
<h4 class="calibre20">Infrastructure resilience</h4>
<p class="calibre3">Many of Google Cloud’s data storage and processing services are either designed for HA by default or provide mechanisms to help you build resilience into your architecture, such as by using multiple machines across multiple zones <span>and regions.</span></p>
<p class="calibre3">If we are designing systems ourselves, we should ensure that data storage and processing infrastructure have redundant components. In the event of a failure, backup systems can take over, ensuring uninterrupted data collection and preparation. We should also implement backup and restore mechanisms to regularly back up raw and processed data. We could store data across multiple zones or regions to safeguard against potential issues in any particular location. This not only protects against data loss but also allows for restoring to a previous state if data becomes corrupted or if there’s a need to revisit earlier data versions. We could also implement load balancing for data ingestion services for high-velocity data streams to help ensure an even distribution of data loads and prevent system overloads. We should also design our infrastructure to scale (up or out) based on demand to ensure reliable performance under varying loads, and we could implement queueing mechanisms to manage <span>data spikes.</span></p>
<h4 class="calibre20">Continuous monitoring and alerts</h4>
<p class="calibre3">As is the case with the <em class="italic">Operational excellence</em> pillar, observability is a key component of this pillar. We should regularly check the health of systems involved in data collection and preparation <a id="_idIndexMarker1531" class="calibre6 pcalibre pcalibre1"/>and implement alerting mechanisms that notify relevant teams when anomalies or failures <span>are detected.</span></p>
<h3 class="calibre11">Cost optimization in data collection and preparation</h3>
<p class="calibre3">Managing costs is important<a id="_idIndexMarker1532" class="calibre6 pcalibre pcalibre1"/> during data collection and preparation due to potentially vast volumes of data, complex preprocessing tasks, and varying infrastructure needs. The following sub-sections discuss how we can apply concepts from the <em class="italic">Cost optimization</em> pillar in the data collection and preparation phases of the ML model development <span>life cycle.</span></p>
<h4 class="calibre20">Efficient data storage</h4>
<p class="calibre3">Storage systems such as GCS and BigQuery provide various classes of storage that are priced differently. From a cost optimization perspective, it’s important that we use the appropriate storage class for our data. For example, frequently accessed data can be stored in Standard storage, while infrequently accessed data could be moved to Nearline or Coldline storage. To make this easier for us to manage, we can implement policies to automatically transition data to cheaper storage classes or delete it once it’s no longer needed. We could also reduce the amount of data stored (and, therefore, reduce our costs) by removing duplicates and compressing our data. In terms of feature storage, we should evaluate the necessity of every feature during the data preparation stage. Removing redundant or low-importance features can significantly reduce storage and <span>computation costs.</span></p>
<h4 class="calibre20">Optimized data processing</h4>
<p class="calibre3">I’m a huge fan of using serverless solutions wherever possible. Not only do we offload the headaches of managing infrastructure to the cloud provider, but with serverless solutions such as BigQuery and Dataflow, we generally only pay for what we use, and we don’t have to worry about over-provisioning (and therefore overpaying for) infrastructure. We can also opt for scalable services such as GKE or Cloud Dataflow that can handle spikes in data processing loads but scale down in low-demand periods, and for non-critical, fault-tolerant data processing tasks, we can use preemptible <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>), which<a id="_idIndexMarker1533" class="calibre6 pcalibre pcalibre1"/> are generally cheaper than <span>regular instances.</span></p>
<p class="calibre3">It’s also important to consider the location of our data, and we should generally aim to process data as near as possible to where it resides for a number of reasons, including cost and latency. For example, storing our data in the <strong class="source-inline">us-central1</strong> region while our processing infrastructure is located in the <strong class="source-inline">us-east4</strong> region would be sub-optimal from a latency perspective and would incur additional network egress costs as the data is transmitted across regions. This also applies in the cases of hybrid cloud infrastructures, in which some of your resources are in the cloud while others are located on your own premises. In such cases, consider the location at which your on-premises resources are connected to the cloud, as well as the data storage location and data processing location. We discussed the various methods of connecting your on-premises resources to Google<a id="_idIndexMarker1534" class="calibre6 pcalibre pcalibre1"/> Cloud (such as VPNs and “interconnects”) in <a href="B18143_03.xhtml#_idTextAnchor059" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 3</em></span></a>, and you can further bolster the security of such hybrid configurations by using VPC-SC to establish a trusted perimeter within which your data is transmitted <span>and processed.</span></p>
<p class="calibre3">Also, if we are using VMs and those VMs need to work together to process our data, we can use <strong class="bold">Google Compute Engine</strong> (<strong class="bold">GCE</strong>) placement<a id="_idIndexMarker1535" class="calibre6 pcalibre pcalibre1"/> policies (specifically, the “compact placement policy”) to specify that our VMs should be located close to each other, which can be particularly <a id="_idIndexMarker1536" class="calibre6 pcalibre pcalibre1"/>important for <strong class="bold">high-performance computing</strong> (<span><strong class="bold">HPC</strong></span><span>) workloads.</span></p>
<p class="calibre3">Finally, where real-time processing isn’t necessary, we can accumulate data and process it in batches, which is often more cost-effective <span>than streaming.</span></p>
<h4 class="calibre20">Cost monitoring and analytics</h4>
<p class="calibre3">We can use tools such as Cost Explorer or custom dashboards in Cloud Monitoring to get insights into our spending patterns and set up billing alerts to notify us of unexpected spikes in costs so that we can intervene accordingly in a timely manner. Additionally, we should regularly analyze our billing reports to identify areas where costs can be trimmed, such as by looking for under-utilized resources <span>or services.</span></p>
<h4 class="calibre20">Cost governance</h4>
<p class="calibre3">A good practice is to set budgets for projects or departments and implement quotas for specific services to prevent unintentional overspending. It’s also important to establish a resource organization and cost attribution strategy. We can use Google Cloud projects, folders, and labels to organize and attribute costs, which makes it easier to track and optimize expenses for specific tasks or teams, and we should promote a culture in which teams are aware of the costs associated with their data handling and processing activities, and encourage <span>cost-saving practices.</span></p>
<h4 class="calibre20">Regular reviews</h4>
<p class="calibre3">We should regularly review the architecture of our data collection and preparation systems because newer and more cost-effective solutions might emerge over time. Similarly, we should periodically evaluate the relevance of the data we’re collecting because some<a id="_idIndexMarker1537" class="calibre6 pcalibre pcalibre1"/> data might become irrelevant over time, and the costs associated with its collection, storage, and processing could <span>be eliminated.</span></p>
<h3 class="calibre11">Performance optimization in data collection and preparation</h3>
<p class="calibre3">As we discussed in<a id="_idIndexMarker1538" class="calibre6 pcalibre pcalibre1"/> the overview sections earlier in this chapter, there are some links between performance optimization and cost optimization because a system that performs optimally will often use resources more efficiently. The following sub-sections discuss how we can apply concepts from the <em class="italic">Performance optimization</em> pillar in the data collection and preparation phases of the ML model development <span>life cycle.</span></p>
<h4 class="calibre20">High-performance data collection</h4>
<p class="calibre3">To optimize our data ingestion processes for real-time data ingestion, we can use services such as Cloud Pub/Sub or Cloud Dataflow, which can help to achieve minimal latency and efficient data streaming. We can also use parallel processing in our data collection strategies by using distributed systems to fetch data from multiple sources concurrently, thus making our data collection <span>more efficient.</span></p>
<h4 class="calibre20">Efficient data storage</h4>
<p class="calibre3">It’s important to use appropriate data structures such as columnar formats (for example, Parquet) for analytics workloads, which can lead to faster querying. In the case of high-performance storage use cases, we can use storage solutions such as Cloud Bigtable for low-latency, high-throughput workloads, which can help to ensure quick data access during the preparation phase. How we index our datasets can also improve the speed of retrieval and querying, which is especially important for large datasets during the data <span>exploration phase.</span></p>
<h4 class="calibre20">Accelerated data processing</h4>
<p class="calibre3">We can use platforms such as Cloud Dataflow and Cloud Dataproc, which provide managed Beam, Spark, and Hadoop clusters, to distribute data processing tasks across multiple nodes. For workloads such as feature engineering or data augmentation tasks in ML, using hardware accelerators such as GPU/TPU acceleration can drastically improve performance. Also, in platforms such as BigQuery, we can write optimized SQL queries to minimize computational overhead and improve <span>processing speed.</span></p>
<h4 class="calibre20">Network optimization</h4>
<p class="calibre3">If we’re transferring large amounts of data from on-premises systems to Google Cloud, dedicated interconnects provide a high-speed, low-latency connection. For collecting data from global<a id="_idIndexMarker1539" class="calibre6 pcalibre pcalibre1"/> sources, <strong class="bold">content delivery networks</strong> (<strong class="bold">CDNs</strong>) ensure optimal data transfer<a id="_idIndexMarker1540" class="calibre6 pcalibre pcalibre1"/> speeds, and we can also use tools such as Traffic Director to manage and optimize network traffic, ensuring efficient data flow <span>between services.</span></p>
<h4 class="calibre20">Resource allocation and auto-scaling</h4>
<p class="calibre3">As we discussed earlier, it’s important to ensure that services automatically scale resources based on demand. For example, Cloud Dataflow can auto-scale worker instances based on the data processing load. We should also tailor VM types and configurations (in terms of memory and CPU resources) to the specific needs of the data collection and <span>preparation tasks.</span></p>
<p class="calibre3">Next, let’s discuss how the Google Cloud Architecture Framework applies to the model building and<a id="_idIndexMarker1541" class="calibre6 pcalibre pcalibre1"/> training steps in our model development <span>life cycle.</span></p>
<h2 id="_idParaDest-283" class="calibre9"><a id="_idTextAnchor345" class="calibre6 pcalibre pcalibre1"/>Model building and training</h2>
<p class="calibre3">As we did in<a id="_idIndexMarker1542" class="calibre6 pcalibre pcalibre1"/> the previous section<a id="_idIndexMarker1543" class="calibre6 pcalibre pcalibre1"/> regarding data collection and preparation, we will discuss the concepts of each pillar in the context of this phase in the model development <span>life cycle.</span></p>
<h3 class="calibre11">Operational excellence in model building and training</h3>
<p class="calibre3">Let’s begin with <a id="_idIndexMarker1544" class="calibre6 pcalibre pcalibre1"/>operational excellence, and how it applies to model building <span>and training.</span></p>
<h4 class="calibre20">Standardized and automated workflows</h4>
<p class="calibre3">The key components here are MLOps pipelines, version control, and CI/CD tooling. We can use Vertex AI Pipelines to create standardized, end-to-end ML pipelines that automate our model training and evaluation steps, including hyperparameter optimization. We can use Google Cloud’s source control tooling to manage our pipeline definition code, and Google Cloud’s CI/CD tooling, such as Cloud Build and Cloud Deploy, to build and deploy our pipeline definitions to Vertex <span>AI Pipelines.</span></p>
<h4 class="calibre20">Observability</h4>
<p class="calibre3">There are two main types of monitoring and logging that we need to implement during the model training and building phase. First, we need to track model performance metrics during model training, such as loss, accuracy, and validation scores. We can do this by using Vertex AI and tools such as TensorBoard. The second is related to system resource monitoring, for which we can use Google Cloud Monitoring to keep an eye on the resource consumption of VMs, TPUs, or GPUs during model training, which can help to achieve optimal resource utilization and timely detection of any potential bottlenecks that <span>might occur.</span></p>
<h4 class="calibre20">Managed infrastructure</h4>
<p class="calibre3">We should use managed infrastructure for our model training and building steps. By using managed infrastructure such as Vertex AI, we automatically use the recommendations<a id="_idIndexMarker1545" class="calibre6 pcalibre pcalibre1"/> outlined by the <em class="italic">Operational excellence</em> pillar in the Google Cloud <span>Architecture Framework.</span></p>
<h3 class="calibre11">Security, privacy, and compliance in model building and training</h3>
<p class="calibre3">Data security will still <a id="_idIndexMarker1546" class="calibre6 pcalibre pcalibre1"/>be a major focus in this section, considering that the training process involves data handling. The following sub-sections discuss how we can apply concepts from the <em class="italic">Security, privacy, and compliance</em> pillar in the model building and training phases of the ML model development <span>life cycle.</span></p>
<h4 class="calibre20">Data security</h4>
<p class="calibre3">The mechanisms here are the same as we discussed in the <em class="italic">Data collection and preparation</em> section. We should ensure that data used for model building and training is encrypted both in transit and at rest. When using sensitive data for training, we can use data masking and tokenization to mask or tokenize specific fields to prevent exposure of PII or other sensitive data points. Additionally, we can use services such as VPC-SC to restrict the services and resources that can access our data, thus creating a secure perimeter around the data used <span>for training.</span></p>
<h4 class="calibre20">Environment security</h4>
<p class="calibre3">We can set up secure training environments to ensure that VMs and containers are securely configured, patched, and hardened, or use managed environments such as Vertex AI, which take care of a lot of these activities for us, and we can use VPC and firewall rules to secure the network traffic related to <span>model training.</span></p>
<h4 class="calibre20">Compliance monitoring</h4>
<p class="calibre3">We can use tools such as Cloud Security Command Center to continuously monitor and ensure that the training environment adheres to compliance standards, and we should also regularly audit data sources for training to ensure compliance with data usage policies, especially if sourcing data from <span>third parties.</span></p>
<h4 class="calibre20">Privacy</h4>
<p class="calibre3">If working with sensitive datasets, we can use techniques such as differential privacy to introduce noise into the data, ensuring individual data points are not identifiable. We can also use data de-identification to remove PI so that it cannot be associated with <span>specific individuals.</span></p>
<p class="calibre3">In addition to all <a id="_idIndexMarker1547" class="calibre6 pcalibre pcalibre1"/>of the aforementioned, we can use IAM to control access to the training environment <span>and artifacts.</span></p>
<h3 class="calibre11">Reliability in model building and training</h3>
<p class="calibre3">The following sub-sections <a id="_idIndexMarker1548" class="calibre6 pcalibre pcalibre1"/>discuss how we can apply concepts from the <em class="italic">Reliability</em> pillar in the model-building and training phases of the ML model development <span>life cycle.</span></p>
<h4 class="calibre20">Data reliability</h4>
<p class="calibre3">As we’ve done in earlier chapters of this book, we can implement validation checks for incoming data to ensure consistency, quality, and completeness. We should also regularly back up training data to prevent data loss and use data versioning <span>for reproducibility.</span></p>
<h4 class="calibre20">Training infrastructure reliability</h4>
<p class="calibre3">We can provision redundant resources in regional or multi-regional deployments to ensure training can continue even if one data center faces issues. In terms of infrastructure scalability, Vertex AI can automatically scale resources based on the training workload. Of course, it’s important to use monitoring tools to keep an eye on resource utilization <span>and health.</span></p>
<h4 class="calibre20">Model training resilience</h4>
<p class="calibre3">We can use checkpointing to save model states at regular intervals during training. In case of interruptions, training could resume from the latest checkpoint rather than starting from scratch. For transient failures in any stage of the model building process, we should implement retry policies to automatically attempt the task again before raising <span>an error.</span></p>
<h4 class="calibre20">Dependency management</h4>
<p class="calibre3">Vertex AI lets us use containerization to ensure consistent software and library versions across training runs, preventing “it works on my machine” issues. This also brings with it all of the other established benefits of containerization, such as standardization and scalability. Think about how we used containers during the practical exercises in our MLOps chapter. We packaged our custom data processing and training code into containers, and then we could use them seamlessly in later stages of the model development process, simply by pointing to the container locations during various steps being implemented on systems such as Vertex AI and Dataproc. This kind of packaging, which facilitates repeatable execution results, is essential for automating steps in an MLOps pipeline, as well as automatically scaling our training and inference workloads based on their varying resource requirements. Such automation is a core benefit of MLOps practices. Furthermore, by using discrete packages of code for each step in the MLOps life cycle in this way, we can scale each step independently, providing flexibility in accordance with the best practices outlined in the <em class="italic">Operational excellence</em> and <em class="italic">Reliability</em> pillars of the Google Cloud <span>Architecture Framework.</span></p>
<p class="calibre3">To further mitigate potential dependency issues, if we have dependencies on external systems such as <a id="_idIndexMarker1549" class="calibre6 pcalibre pcalibre1"/>data providers, we should ensure they have uptime guarantees and <span>fallback mechanisms.</span></p>
<h4 class="calibre20">DR</h4>
<p class="calibre3">It’s important to regularly back up model architectures, configurations, trained weights, and other essential components to allow for quick recovery in case of data corruption or loss. We should establish clear protocols for restoring from backups, ensuring minimal downtime and a quick return to operational status in case of disruptions. The following point cannot be emphasized enough: we must periodically test our recovery procedures. Companies often focus only on backup mechanisms and not on testing the recovery processes. We want to ensure that our recovery processes are effective (that is, they actually work) and<a id="_idIndexMarker1550" class="calibre6 pcalibre pcalibre1"/> efficient (that is, they work as quickly <span>as possible).</span></p>
<h3 class="calibre11">Cost optimization in model building and training</h3>
<p class="calibre3">The following sub-sections<a id="_idIndexMarker1551" class="calibre6 pcalibre pcalibre1"/> discuss how we can apply concepts from the <em class="italic">Cost optimization</em> pillar in the model building and training phases of the ML model development <span>life cycle.</span></p>
<h4 class="calibre20">Resource efficiency</h4>
<p class="calibre3">To optimize costs during model building and training, we should ensure that VMs, GPUs, or TPUs used for training are appropriately sized for the workload. Initially, this may take some experimentation to find the best configuration of resources, but by the time we have standardized our training process into an MLOps pipeline, we should have a good idea of the required resources. Using Vertex AI and serverless services can help us optimize costs because those services can scale our resources based on demand. We can also utilize CUDs to save on <span>computing costs.</span></p>
<p class="calibre3">For training jobs that can handle interruptions, we can use preemptible VMs, which can offer <span>substantial savings.</span></p>
<p class="calibre3">It should also be noted that simpler model architectures can be easier, quicker, and cheaper to train than complex model architectures. It’s also important to shut down all resources when not in use so that we don’t pay when they <span>are idle.</span></p>
<h4 class="calibre20">Review and optimize regularly</h4>
<p class="calibre3">We can use tools such as Google Cloud Cost Management to regularly review and analyze infrastructure costs, and identify opportunities for optimization. As always, we can use budgets, quotas, and billing alerts to help keep our costs under control, and we should periodically<a id="_idIndexMarker1552" class="calibre6 pcalibre pcalibre1"/> review our ML infrastructure, data storage, and associated processes to identify and <span>eliminate inefficiencies.</span></p>
<h3 class="calibre11">Performance optimization in model building and training</h3>
<p class="calibre3">The following sub-sections <a id="_idIndexMarker1553" class="calibre6 pcalibre pcalibre1"/>discuss how we can apply concepts from the <em class="italic">Performance optimization</em> pillar in the model building and training phases of the ML model development <span>life cycle.</span></p>
<h4 class="calibre20">Compute optimization</h4>
<p class="calibre3">To optimize performance, we can use hardware acceleration and specialized hardware such as GPUs and TPUs, which can significantly accelerate the <span>training process.</span></p>
<h4 class="calibre20">Distributed training</h4>
<p class="calibre3">We can distribute the training process across multiple nodes to run in parallel and reduce training time. Also, for hyperparameter tuning, we can use services such as Vertex AI Vizier to perform concurrent trials, significantly reducing the time required to find optimal <span>model parameters.</span></p>
<h4 class="calibre20">Data I/O optimization</h4>
<p class="calibre3">We should use high-throughput data sources and systems for performant workloads so that the data coming into the training process isn’t <span>a bottleneck.</span></p>
<p class="calibre3">As mentioned in other sections of this chapter, it’s important to continuously track performance metrics such as processing speed, memory usage, and I/O throughput using tools such as Google Cloud Monitoring, and then adjusting resources or configurations as needed. We can also use profiling to analyze ML training code, identify performance bottlenecks, and then optimize the most <span>time-consuming segments.</span></p>
<p class="calibre3">Next, let’s discuss how the Google Cloud Architecture Framework applies the model evaluation and<a id="_idIndexMarker1554" class="calibre6 pcalibre pcalibre1"/> deployment steps in our model development <span>life cycle.</span></p>
<h2 id="_idParaDest-284" class="calibre9"><a id="_idTextAnchor346" class="calibre6 pcalibre pcalibre1"/>Model evaluation and deployment</h2>
<p class="calibre3">In this section, we<a id="_idIndexMarker1555" class="calibre6 pcalibre pcalibre1"/> will discuss the concepts of each pillar in the context of model evaluation and deployment in the model development life cycle. Note that, in some phases, the same concepts that we’ve already discussed in previous phases of the model development life cycle still apply. In the remaining sections of this chapter, I will briefly call out when the same concepts <span>apply again.</span></p>
<h3 class="calibre11">Operational excellence in model evaluation and deployment</h3>
<p class="calibre3">Let’s begin with <a id="_idIndexMarker1556" class="calibre6 pcalibre pcalibre1"/>operational excellence, and how it applies to model evaluation <span>and deployment.</span></p>
<h4 class="calibre20">Automation, observability, and scalability</h4>
<p class="calibre3">In this phase of the ML model development life cycle, the same concepts from the <em class="italic">Operational excellence</em> pillar, such as automated workflows, observability, and scalability, which we’ve already discussed in the context of the model building and training phase, apply again here. Basically, we can set up MLOps pipelines that automate our model evaluation and deployment steps using Vertex AI Pipelines, and we can use Google Cloud Monitoring and logging tools to track metrics related to our model evaluations and deployed model performance. We can also use load balancers and Vertex AI auto-scaling infrastructure to ensure that our models can handle varying levels <span>of demand.</span></p>
<h4 class="calibre20">A/B testing and canary deployments</h4>
<p class="calibre3">When deploying new model versions, we can use A/B testing to gradually shift traffic and compare performance against previous versions. Of course, we want to ensure that the newer versions being deployed perform better than the previous versions and that they don’t negatively impact user experience. Using canary deployments, we can deploy new model versions to a small subset of users first, closely monitor performance, and then gradually <a id="_idIndexMarker1557" class="calibre6 pcalibre pcalibre1"/>expand to a broader user base. We should also use model versioning to allow for quick rollbacks if newer versions result in unexpected behaviors <span>or errors.</span></p>
<h3 class="calibre11">Security, privacy, and compliance in model evaluation and deployment</h3>
<p class="calibre3">Again, the same <a id="_idIndexMarker1558" class="calibre6 pcalibre pcalibre1"/>concepts regarding data security and privacy apply here also, as well as access control, compliance regulations, and auditing. In addition to all of that, we can use network security controls and VPC-SC to protect the endpoints on which our models <span>are hosted.</span></p>
<h3 class="calibre11">Reliability in model evaluation and deployment</h3>
<p class="calibre3">In this case, the <a id="_idIndexMarker1559" class="calibre6 pcalibre pcalibre1"/>same concepts of infrastructure resilience, such as deploying resources in multiple zones or regions, also apply here, as well as health checks, load balancing, auto-scaling, DR, monitoring, alerting, and <span>dependency management.</span></p>
<h3 class="calibre11">Cost optimization in model evaluation and deployment</h3>
<p class="calibre3">When discussing<a id="_idIndexMarker1560" class="calibre6 pcalibre pcalibre1"/> cost optimization in the context of model evaluation and deployment, some concepts from our previous phases apply again, such as right-sizing resources, shutting down idle resources, using CUDs, and setting budgets and alerts. It’s also important to note that smaller, simpler models require fewer resources and are therefore cheaper to run than larger, more <span>complex models.</span></p>
<h3 class="calibre11">Performance optimization in model evaluation and deployment</h3>
<p class="calibre3">You won’t be<a id="_idIndexMarker1561" class="calibre6 pcalibre pcalibre1"/> surprised to hear the terms <em class="italic">auto-scaling</em> and <em class="italic">load balancing</em> being used in the context of performance optimization for model evaluation and deployment, as well as optimizing compute and storage resources, and <span>hardware acceleration.</span></p>
<p class="calibre3">We can also use caching mechanisms to improve response times. For example, we can cache frequent prediction results so that repeated requests can be served without invoking the model again, and we can store frequently accessed data or intermediate model evaluation results in memory for <span>quicker access.</span></p>
<p class="calibre3">By now, you have become an expert in the Google Cloud Architecture Framework and how it specifically<a id="_idIndexMarker1562" class="calibre6 pcalibre pcalibre1"/> applies to the ML model development life cycle. Let’s take a moment to summarize everything we covered in <span>this chapter.</span></p>
<h1 id="_idParaDest-285" class="calibre5"><a id="_idTextAnchor347" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">This chapter discussed various aspects of ML model governance, including documentation, versioning, monitoring, auditing, compliance, operationalization, and continuous improvement. We then explored some industry-specific and region-specific regulations, such as HIPAA for healthcare, SOX for finance, GDPR (EU), and <span>CCPA (California).</span></p>
<p class="calibre3">Next, we focused on the Google Cloud Architecture Framework and how to apply its pillars—<em class="italic">Operational excellence</em>, <em class="italic">Security, privacy and compliance</em>, <em class="italic">Reliability</em>, <em class="italic">Cost optimization</em>, and <em class="italic">Performance efficiency</em>—to the various stages of the ML life cycle. We dived deep into each pillar, detailing its relevance across different phases, from data collection and preparation to model evaluation and deployment. This included important concepts, such as cost-efficient model deployment, enhancing security throughout the model life cycle, and maintaining high reliability and performance standards. Overall, this chapter covered many factors related to deploying and managing ML workloads on Google Cloud, with best practices and optimizations <span>in mind.</span></p>
<p class="calibre3">In the next chapter, we’ll take a look at using some other popular tools and frameworks in the industry—such as Spark MLlib and PyTorch—on <span>Google Cloud.</span></p>
</div>
</div></body></html>