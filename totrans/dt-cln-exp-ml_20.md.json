["```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.pipeline import make_pipeline\n    from sklearn.impute import SimpleImputer\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.decomposition import PCA\n    from sklearn.model_selection import RandomizedSearchCV\n    from scipy.stats import uniform\n    from scipy.stats import randint\n    import os\n    import sys\n    sys.path.append(os.getcwd() + “/helperfunctions”)\n    from preprocfunc import OutlierTrans\n    ```", "```py\n    nbagames = pd.read_csv(“data/nbagames2017plus.csv”, parse_dates=[‘GAME_DATE’])\n    nbagames = nbagames.loc[nbagames.WL_HOME.isin([‘W’,’L’])]\n    nbagames.shape\n    (4568, 149)\n    nbagames[‘WL_HOME’] = \\\n      np.where(nbagames.WL_HOME==’L’,0,1).astype(‘int’)\n    nbagames.WL_HOME.value_counts(dropna=False)\n    1    2586\n    0    1982\n    Name: WL_HOME, dtype: int64\n    ```", "```py\n    num_cols = [‘FG_PCT_HOME’,’FTA_HOME’,’FG3_PCT_HOME’,\n      ‘FTM_HOME’,’FT_PCT_HOME’,’OREB_HOME’,’DREB_HOME’,\n      ‘REB_HOME’,’AST_HOME’,’STL_HOME’,’BLK_HOME’,\n      ‘TOV_HOME’, ‘FG_PCT_AWAY’,’FTA_AWAY’,’FG3_PCT_AWAY’,\n      ‘FT_PCT_AWAY’,’OREB_AWAY’,’DREB_AWAY’,’REB_AWAY’,\n      ‘AST_AWAY’,’STL_AWAY’,’BLK_AWAY’,’TOV_AWAY’]\n    nbagames[[‘WL_HOME’] + num_cols].agg([‘count’,’min’,’median’,’max’]).T\n    ```", "```py\n                count       min     median    max\nWL_HOME         4,568.00    0.00    1.00      1.00\nFG_PCT_HOME     4,568.00    0.27    0.47      0.65\nFTA_HOME        4,568.00    1.00    22.00     64.00\nFG3_PCT_HOME    4,568.00    0.06    0.36      0.84\nFTM_HOME        4,568.00    1.00    17.00     44.00\nFT_PCT_HOME     4,568.00    0.14    0.78      1.00\nOREB_HOME       4,568.00    1.00    10.00     25.00\nDREB_HOME       4,568.00    18.00   35.00     55.00\nREB_HOME        4,568.00    22.00   45.00     70.00\nAST_HOME        4,568.00    10.00   24.00     50.00\nSTL_HOME        4,568.00    0.00    7.00      22.00\nBLK_HOME        4,568.00    0.00    5.00      20.00\nTOV_HOME        4,568.00    1.00    14.00     29.00\nFG_PCT_AWAY     4,568.00    0.28    0.46      0.67\nFTA_AWAY        4,568.00    3.00    22.00     54.00\nFG3_PCT_AWAY    4,568.00    0.08    0.36      0.78\nFT_PCT_AWAY     4,568.00    0.26    0.78      1.00\nOREB_AWAY       4,568.00    0.00    10.00     26.00\nDREB_AWAY       4,568.00    18.00   34.00     56.00\nREB_AWAY        4,568.00    22.00   44.00     71.00\nAST_AWAY        4,568.00    9.00    24.00     46.00\nSTL_AWAY        4,568.00    0.00    8.00      19.00\nBLK_AWAY        4,568.00    0.00    5.00      15.00\nTOV_AWAY        4,568.00    3.00    14.00     30.00\n```", "```py\n    corrmatrix = nbagames[[‘WL_HOME’] + num_cols].\\\n      corr(method=”pearson”)\n    sns.heatmap(corrmatrix, xticklabels=corrmatrix.columns,\n      yticklabels=corrmatrix.columns, cmap=”coolwarm”)\n    plt.title(‘Heat Map of Correlation Matrix’)\n    plt.tight_layout()\n    plt.show()\n    ```", "```py\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nbagames[num_cols],\\\n      nbagames[[‘WL_HOME’]],test_size=0.2, random_state=0)\n    ```", "```py\n    pca = PCA(n_components=7)\n    pipe1 = make_pipeline(OutlierTrans(2),\n          SimpleImputer(strategy=”median”),\n          StandardScaler(), pca)\n    pipe1.fit(X_train)\n    ```", "```py\n    components = pd.DataFrame(pipe1[‘pca’].components_,\n      columns=num_cols)\n    components.T.to_excel(‘views/components.xlsx’)\n    ```", "```py\n    components.pc1.abs().nlargest(5)\n    FG_PCT_HOME    0.38\n    REB_AWAY       0.37\n    DREB_AWAY      0.34\n    REB_HOME       0.33\n    FG_PCT_AWAY    0.30\n    Name: pc1, dtype: float64\n    components.pc2.abs().nlargest(5)\n    DREB_HOME      0.38\n    FG_PCT_AWAY    0.37\n    DREB_AWAY      0.32\n    REB_HOME       0.32\n    FG_PCT_HOME    0.29\n    Name: pc2, dtype: float64\n    components.pc3.abs().nlargest(5)\n    FTM_HOME    0.55\n    FTA_HOME    0.53\n    TOV_HOME    0.30\n    STL_AWAY    0.27\n    TOV_AWAY    0.26\n    Name: pc3, dtype: float64\n    ```", "```py\nnp.set_printoptions(precision=3)\npipe1[‘pca’].explained_variance_ratio_\narray([0.145, 0.134, 0.095, 0.086, 0.079, 0.059, 0.054])\nnp.cumsum(pipe1[‘pca’].explained_variance_ratio_)\narray([0.145, 0.279, 0.374, 0.46 , 0.539, 0.598, 0.652])\n```", "```py\nX_train_pca = pd.DataFrame(pipe1.transform(X_train),\n  columns=components.columns, index=X_train.index).join(y_train)\nsns.scatterplot(x=X_train_pca.pc1, y=X_train_pca.pc2, hue=X_train_pca.WL_HOME)\nplt.title(“Scatterplot of First and Second Components”)\nplt.xlabel(“Principal Component 1”)\nplt.ylabel(“Principal Component 2”)\nplt.show()\n```", "```py\n    lr = LogisticRegression()\n    pipe2 = make_pipeline(OutlierTrans(2),\n      SimpleImputer(strategy=”median”), StandardScaler(),\n      pca, lr)\n    lr_params = {\n      “pca__n_components”: randint(3, 20),\n      “logisticregression__C”: uniform(loc=0, scale=10)\n    }\n    rs = RandomizedSearchCV(pipe2, lr_params, cv=4, \n      n_iter=40, scoring=’accuracy’, random_state=1)\n    rs.fit(X_train, y_train.values.ravel())\n    ```", "```py\nrs.best_params_\n{‘logisticregression__C’: 6.865009276815837, ‘pca__n_components’: 19}\nrs.best_score_\n0.9258345296842831\n```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.pipeline import make_pipeline\n    from sklearn.impute import SimpleImputer\n    from sklearn.decomposition import KernelPCA\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.model_selection import RandomizedSearchCV\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import os\n    import sys\n    sys.path.append(os.getcwd() + “/helperfunctions”)\n    from preprocfunc import OutlierTrans\n    ```", "```py\n    un_income_gap = pd.read_csv(“data/un_income_gap.csv”)\n    un_income_gap.set_index(‘country’, inplace=True)\n    un_income_gap[‘incomeratio’] = \\\n      un_income_gap.femaleincomepercapita / \\\n        un_income_gap.maleincomepercapita\n    un_income_gap[‘educratio’] = \\\n      un_income_gap.femaleyearseducation / \\\n         un_income_gap.maleyearseducation\n    un_income_gap[‘laborforcepartratio’] = \\\n      un_income_gap.femalelaborforceparticipation / \\\n         un_income_gap.malelaborforceparticipation\n    un_income_gap[‘humandevratio’] = \\\n      un_income_gap.femalehumandevelopment / \\\n         un_income_gap.malehumandevelopment\n    un_income_gap.dropna(subset=[‘incomeratio’], inplace=True)\n    ```", "```py\n    num_cols = [‘educratio’,’laborforcepartratio’,\n      ‘humandevratio’,’genderinequality’,\n      ‘maternalmortality’,’adolescentbirthrate’,\n      ‘femaleperparliament’,’incomepercapita’]\n    gap_sub = un_income_gap[[‘incomeratio’] + num_cols]\n    gap_sub.\\\n      agg([‘count’,’min’,’median’,’max’]).T\n                           count    min    median  max\n    incomeratio            177.00   0.16   0.60    0.93\n    educratio              169.00   0.24   0.93    1.35\n    laborforcepartratio    177.00   0.19   0.75    1.04\n    humandevratio          161.00   0.60   0.95    1.03\n    genderinequality       155.00   0.02   0.39    0.74\n    maternalmortality     174.00   1.00   60.00  1,100.00\n    adolescentbirthrate    177.00   0.60   40.90  204.80\n    femaleperparliament    174.00   0.00   19.35    57.50\n    incomepercapita        177.00   581.00 10,512.00  123,124.00\n    ```", "```py\n    corrmatrix = gap_sub.corr(method=”pearson”)\n    sns.heatmap(corrmatrix, \n      xticklabels=corrmatrix.columns,\n      yticklabels=corrmatrix.columns, cmap=”coolwarm”)\n    plt.title(‘Heat Map of Correlation Matrix’)\n    plt.tight_layout()\n    plt.show()\n    ```", "```py\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(gap_sub[num_cols],\\\n      gap_sub[[‘incomeratio’]], test_size=0.2,\n      random_state=0)\n    ```", "```py\nrfreg = RandomForestRegressor()\nkpca = KernelPCA()\npipe1 = make_pipeline(OutlierTrans(2),\n  SimpleImputer(strategy=”median”), MinMaxScaler(),\n  kpca, rfreg)\nrfreg_params = {\n ‘kernelpca__n_components’:\n    randint(2, 9),\n ‘kernelpca__gamma’:\n     np.linspace(0.03, 0.3, 10),\n ‘kernelpca__kernel’:\n     [‘linear’, ‘poly’, ‘rbf’, \n      ‘sigmoid’, ‘cosine’],\n ‘randomforestregressor__max_depth’:\n     randint(2, 20),\n ‘randomforestregressor__min_samples_leaf’:\n     randint(5, 11)\n}\n```", "```py\n    rs = RandomizedSearchCV(pipe1, rfreg_params,\n      cv=4, n_iter=40,\n      scoring=’neg_mean_absolute_error’,\n      random_state=1)\n    rs.fit(X_train, y_train.values.ravel())\n    rs.best_params_\n    {‘kernelpca__gamma’: 0.12000000000000001,\n     ‘kernelpca__kernel’: ‘poly’,\n     ‘kernelpca__n_components’: 4,\n     ‘randomforestregressor__max_depth’: 18,\n     ‘randomforestregressor__min_samples_leaf’: 5}\n    rs.best_score_\n    -0.06630618838886537\n    ```", "```py\n    results = \\\n      pd.DataFrame(rs.cv_results_[‘mean_test_score’], \\\n        columns=[‘meanscore’]).\\\n      join(pd.DataFrame(rs.cv_results_[‘params’])).\\\n      sort_values([‘meanscore’], ascending=False)\n    results.iloc[1:3].T\n    39     0 \n    meanscore                              -0.067 -0.070\n    kernelpca__gamma                        0.240  0.180\n    kernelpca__kernel                       rbf    sigmoid\n    kernelpca__n_components                 6      6\n    randomforestregressor__max_depth        12     10\n    randomforestregressor__min_samples_leaf 5      6\n    ```"]