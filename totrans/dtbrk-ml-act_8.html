<html><head></head><body>
		<div id="_idContainer309">
			<h1 class="chapter-number" id="_idParaDest-156"><a id="_idTextAnchor384"/>8</h1>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor385"/>Monitoring, Evaluating, and More</h1>
			<p class="author-quote">“Focus on how the end-user customers perceive the impact of your innovation – rather than on how you, the innovators, perceive it.” — Thomas A. Edison</p>
			<p>Congratulations, you’ve made it to the final chapter! We’ve come a long way, yet there is still more to explore in Databricks. As we wrap up, we will take another look at Lakehouse Monitoring. We’ll focus on monitoring model inference data. After all the work you’ve put in to build a robust model and push it into production, it’s essential to share the learnings, predictions, and other outcomes with a broad audience. Sharing results with dashboards is very common. We will cover how to create visualizations for dashboards in both the new Lakeview dashboards and the standard Databricks SQL dashboards. Deployed models can be shared via a web application. Therefore, we will not only introduce Hugging Face Spaces but also deploy the RAG chatbot using the Gradio app in <em class="italic">Applying our learning</em>. Lastly, we’ll demonstrate how analysts can invoke LLMs via SQL AI Functions! By the end of this chapter, you will be ready to monitor inference data, create visualizations, deploy an ML web app, and use the groundbreaking DBRX open source LLM <span class="No-Break">with SQL.</span></p>
			<p>Here is the roadmap for <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Monitoring <span class="No-Break">your models</span></li>
				<li>Building gold <span class="No-Break">layer visualizations</span></li>
				<li>Connecting <span class="No-Break">your applications</span></li>
				<li>Incorporating LLMs <span class="No-Break">for analysts</span></li>
				<li>Applying <span class="No-Break">our learning</span></li>
			</ul>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor386"/>Monitoring your models</h1>
			<p>The ML lifecycle does<a id="_idIndexMarker528"/> not end at deployment. Once a model is in production, we want to monitor the input data and output results of the model. In <a href="B16865_04.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, we explored two key features of Databricks Lakehouse Monitoring integrated with Unity Catalog: Snapshot and TimeSeries profiles. Snapshot profiles are designed to provide an overview of a dataset at a specific point in time, capturing its current state. This is particularly useful for identifying immediate data quality issues or changes. On the other hand, TimeSeries profiles focus on how data evolves over time, making them ideal for tracking trends, patterns, and gradual changes in <span class="No-Break">data distributions.</span></p>
			<p>Expanding on these capabilities, Databricks also provides an Inference profile, tailored for monitoring machine learning models in production. This advanced profile builds upon the concept of TimeSeries profiles, adding critical functionalities for comprehensive model performance evaluation. It includes model quality metrics, essential for tracking the accuracy and reliability of predictions over time. It also records predictions and, optionally, ground truth labels, directly comparing expected and actual outcomes. This functionality is key for identifying model drift, where shifts in input data or the relationship between inputs and <span class="No-Break">outputs occur.</span></p>
			<p>Inference Tables in Databricks further bolster this monitoring capability. They contain essential elements such as model predictions, input features, timestamps, and potentially ground truth labels. Building a monitor on top of InferenceTables with the corresponding <strong class="source-inline">InferenceLog</strong> allows us to monitor model performance and data <span class="No-Break">drift continuously.</span></p>
			<p>In the event of drift<a id="_idIndexMarker529"/> detection, immediate actions should be taken – data pipeline verification or model retraining and evaluation are recommended. These steps ensure the model adapts to new data patterns, maintaining accuracy and effectiveness. Continuous monitoring against your baseline and cross-model versions is a strategy to adopt when trying to ensure a stable process across various <span class="No-Break">deployed solutions.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> is a code sample for creating an Inference monitor with model quality metrics using the <strong class="source-inline">InferenceLog</strong> profile type. This illustrates a practical application of this monitoring setup. We specify the <strong class="source-inline">schedule</strong> argument to make sure that this monitor is <span class="No-Break">refreshed hourly.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer270">
					<img alt="Figure 8.1 – Creating an Inference profile monitor that refreshes hourly&#13;&#10;" src="image/B16865_08_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Creating an Inference profile monitor that refreshes hourly</p>
			<p>Model monitoring is an effective way to ensure your models are working for you as expected. We hope <a id="_idIndexMarker530"/>this gets you thinking about how you use monitoring in your <span class="No-Break">MLOPs process.</span></p>
			<p>Next, we’ll learn about ways to <span class="No-Break">create dashboards.</span><a id="_idTextAnchor387"/></p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor388"/>Building gold layer visualizations</h1>
			<p>The gold layer in <a id="_idIndexMarker531"/>your lakehouse is the consumption-ready layer. In this layer, final transformations and aggregations crystallize the insights within your data so it is ready for reporting and dashboarding. Being able to share your data with an audience is critical, and there are several options for doing so in the DI Platform. In fact, both Lakeview and Databricks SQL dashboards allow you to transform and aggregate your data within visualizations. Let’s walk through how to <span class="No-Break">do tha<a id="_idTextAnchor389"/>t.</span></p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor390"/>Leveraging Lakeview dashboards</h2>
			<p>Lakeview dashboards<a id="_idIndexMarker532"/> in Databricks are a powerful tool for creating data visualizations and sharing insights hidden in data. Visualizations can be made in the English language, making dashboard creation available to more users. To create a Lakeview dashboard, first click <strong class="bold">Dashboards</strong> in the left-hand navigation bar, and then open the <strong class="bold">Lakeview Dashboards</strong> tab. You will see a <strong class="bold">Create Lakeview dashboard</strong> button at the top right of the screen. Clicking this will open a canvas to create your dashboard. The canvas is where the visualization magic happens. It includes an enhanced visualization library to bring your data to life and makes it easy to add widgets for interactive filtering. The <strong class="bold">Data</strong> tab, shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.2</em>, is where you can define your dataset with SQL or simply by selecting a table. For example, you could select the <strong class="source-inline">ml_in_action.favorita_forecasting.train_set</strong> table. This creates a dataset by selecting all records from the provided table. Notice how we do not <em class="italic">have to </em>write any SQL<a id="_idIndexMarker533"/> or create aggregates in order to visualize <span class="No-Break">data aggregations.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer271">
					<img alt="Figure 8.2 – The Data tab for adding data to your Lakeview dashboard&#13;&#10;" src="image/B16865_08_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – The Data tab for adding data to your Lakeview dashboard</p>
			<p>Once you have a dataset, return to the <strong class="bold">Canvas</strong> tab. Select the <strong class="bold">Add a visualization</strong> button found on the blue bar toward the bottom of your browser window. This gives you a widget to place on your dashboard. Once placed, your widget will look similar to <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer272">
					<img alt="Figure 8.3 – A new Lakeview widget&#13;&#10;" src="image/B16865_08_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – A new Lakeview widget</p>
			<p>In the new widget, you can manually create a visualization using the options on the right-side menu. Alternatively, Databricks Assistant can help you rapidly build a chart using just English. You can write your own question, or explore the suggested queries. We selected the <a id="_idIndexMarker534"/>suggested question <em class="italic">What is the trend of onpromotion over date?</em> to automatically generate a chart, and <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.4</em> is <span class="No-Break">the result.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer273">
					<img alt="Figure 8.4 – English text generated Lakeview widget&#13;&#10;" src="image/B16865_08_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – English text generated Lakeview widget</p>
			<p>When you are ready to share your dashboard, you can publish it! The engine powering Lakeview dashboards is optimized for performance, driving faster interactive charts for your data. It’s also powerful enough to handle streaming data. Furthermore, Lakeview dashboards are unified with the DI Platform through Unity Catalog, providing data lineage. They are<a id="_idIndexMarker535"/> designed for easy sharing across workspaces, meaning users in other workspaces can access your <span class="No-Break">curated dashboa<a id="_idTextAnchor391"/>rd.</span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor392"/>Visualizing big data with Databricks SQL dashboards</h2>
			<p>Lakeview <a id="_idIndexMarker536"/>dashboards are the future of Databricks. However, you can also build dashboards<a id="_idIndexMarker537"/> with <strong class="bold">Databricks SQL</strong> (<strong class="bold">DBSQL</strong>). The SQL editor built into the DI Platform is another way to quickly create visualizations. With a single <strong class="source-inline">select</strong> statement, you can produce the data and use it for <span class="No-Break">multiple visualizations.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">To recreate <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.5</em> in your own workspace, you will want to uncheck the <strong class="bold">LIMIT 1000</strong> box. There is still a limit for the visualizations of 64,000 rows. The best way to get around this is by filtering <span class="No-Break">or aggregating.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.5</em> is an example visualization we created from a simple SQL query against the <em class="italic">Favorita Store </em><span class="No-Break"><em class="italic">Sales</em></span><span class="No-Break"> data.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer274">
					<img alt="Figure 8.5 – After executing the select statement in the DBSQL editor, we create the visualizations without writing any code&#13;&#10;" src="image/B16865_08_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – After executing the select statement in the DBSQL editor, we create the visualizations without writing any code</p>
			<p>Suppose your <a id="_idIndexMarker538"/>dataset has categorical variables that you want to use to filter and compare features, as with the <em class="italic">Favorita </em>sales data. You can add filters within the DBSQL editor without revising the query. To add a filter, click the <strong class="bold">+</strong> and choose either <strong class="bold">filter</strong> or <strong class="bold">parameter</strong>. Both options provide widgets for filtering, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6</em>. You can use the widgets with any visualizations or dashboards associated with <span class="No-Break">the query.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer275">
					<img alt="Figure 8.6 – (L) The configuration for the state and family filter; (R) the result of adding two filters to the Favorita sales query&#13;&#10;" src="image/B16865_08_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – (L) The configuration for the state and family filter; (R) the result of adding two filters to the Favorita sales query</p>
			<p>The dashboard<a id="_idIndexMarker539"/> functionality shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.7</em> is built into Databricks SQL as a way to present the charts and other visualizations created from one or <span class="No-Break">many queries.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer276">
					<img alt="Figure 8.7 – A dashboard with charts created from the Favorita sales data query&#13;&#10;" src="image/B16865_08_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – A dashboard with charts created from the Favorita sales data query</p>
			<p>The built-in visualization capabilities of DBSQL are a quick way to explore data without connecting to an <a id="_idIndexMarker540"/>external dashboarding or data <span class="No-Break">visualization tool.</span></p>
			<p>Next, we’ll look at an example of <a id="_idIndexMarker541"/>using Python <strong class="bold">User-Defined Functions</strong> (<strong class="bold">UDFs</strong>) for reusable Python code <span class="No-Break">within <a id="_idTextAnchor393"/>DBSQL.</span></p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor394"/>Python UDFs</h2>
			<p>Python UDFs <a id="_idIndexMarker542"/>are a way to create reusable snippets of code in Python, and these can be used in DBSQL. In this example, we’ll create a UDF for sales analysts to redact information in a customer record. Line five indicates the language syntax for the function is Python between <strong class="source-inline">$$</strong> <span class="No-Break">signs:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer277">
					<img alt="Figure 8.8 – Creating a Python UDF in DBSQL&#13;&#10;" src="image/B16865_08_8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Creating a Python UDF in DBSQL</p>
			<p>UDFs are defined and managed as part of Unity Catalog. Once a UDF is defined, you can give teams the ability to execute the UDF using <span class="No-Break"><strong class="source-inline">GRANT EXECUTE</strong></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer278">
					<img alt="Figure 8.9 – Granting permissions for the sales-analysts groups to execute a UDF&#13;&#10;" src="image/B16865_08_8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Granting permissions for the sales-analysts groups to execute a UDF</p>
			<p>In this SQL query, we are applying the <strong class="source-inline">redact</strong> UDF to the <span class="No-Break"><strong class="source-inline">contact_info</strong></span><span class="No-Break"> field.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer279">
					<img alt="Figure 8.10 – Using the Python UDF in a SQL query&#13;&#10;" src="image/B16865_08_8_(b).jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Using the Python UDF in a SQL query</p>
			<p>Now that we <a id="_idIndexMarker543"/>have the basics for visualizing data and applying Python UDFs in SQL, let’s cover a couple of tips <span class="No-Break">and tr<a id="_idTextAnchor395"/>icks.</span></p>
			<h3>Tips and tricks</h3>
			<p>This section covers our<a id="_idIndexMarker544"/> tips and tricks related to DBSQL. Some tips apply to DBSQL and Lakeview, but <span class="No-Break">not all:</span></p>
			<ul>
				<li><strong class="bold">Use managed compute (a.k.a. serverless compute) when possible</strong>: Query performance using Databricks’ SQL warehouses is record-setting, as mentioned in <a href="B16865_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. The new managed compute for DBSQL puts the new first query performance at roughly 10 seconds. This means that idle time has drastically been reduced, which translates into <span class="No-Break">cost savings.</span></li>
				<li><strong class="bold">Use a subquery as a parameter filter</strong>: In your query visualizations and dashboards, you can prepopulate drop-down filter boxes. You can do this by creating and saving a query in the SQL editor. For example, you could create a query that returns a distinct list of customer names. In <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.11</em>, we select a query called <strong class="bold">Customer Name Lookup Qry</strong> as a subquery to filter the query visualization <a id="_idIndexMarker545"/>by customer name. Therefore, we can filter on <strong class="bold">Customer</strong> using a <span class="No-Break">drop-down list.</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer280">
					<img alt="Figure 8.11 – Using a subquery as a parameter for a query&#13;&#10;" src="image/B16865_08_9.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Using a subquery as a parameter for a query</p>
			<ul>
				<li><strong class="bold">Schedule report delivery</strong>: If you have users who want to receive an up-to-date dashboard regularly, you can schedule the refresh and have it sent to subscribers. For DBSQL dashboards, remember to turn off <strong class="bold">Enabled</strong> when you are developing so that users don’t get too <span class="No-Break">many updates.</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer281">
					<img alt="Figure 8.12 – Scheduling a dashboard report with subscribers (T) DBSQL (B) Lakeview&#13;&#10;" src="image/B16865_08_10.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer282">
					<img alt="Figure 8.12 – Scheduling a dashboard report with subscribers (T) DBSQL (B) Lakeview&#13;&#10;" src="image/B16865_08_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Scheduling a dashboard report with subscribers (T) DBSQL (B) Lakeview</p>
			<ul>
				<li><strong class="bold">Speed up development with Databricks Assistant</strong>: As we covered in <a href="B16865_04.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, Databricks<a id="_idIndexMarker546"/> Assistant is an AI-based interface that can help generate, transform, fix, and explain code. The Assistant is context-aware, meaning it uses Unity Catalog to look at the metadata of your tables and columns, personalized in your environment. In <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.13</em>, we ask the Assistant to help write a query with syntax for grouping. It sees the metadata of the <strong class="bold">Favorita</strong><em class="italic"> </em><strong class="bold">Stores</strong> table and provides code specific to that table and<a id="_idIndexMarker547"/> the column <span class="No-Break">of interest.</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer283">
					<img alt="Figure 8.13 – Using Databricks Assistant for help writing a query&#13;&#10;" src="image/B16865_08_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Using Databricks Assistant for help writing a query</p>
			<ul>
				<li><strong class="bold">Be informed</strong>: Keep an eye out for important data changes with alerts. Use SQL to calibrate the alert and schedule the condition evaluation at specific intervals through the UI shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.14</em>. You can use HTML to create a formatted <span class="No-Break">alert email.</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer284">
					<img alt="Figure 8.14 – Scheduling alerts to trigger when certain conditions are met&#13;&#10;" src="image/B16865_08_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Scheduling alerts to trigger when certain conditions are met</p>
			<ul>
				<li><strong class="bold">Use tags to track usage</strong>: When creating a new SQL warehouse, use tags to code your<a id="_idIndexMarker548"/> warehouse endpoint with the correct project. Tagging is a great way to understand usage by project or team. System tables contain the information for <span class="No-Break">tracking usag<a id="_idTextAnchor396"/>e.</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer285">
					<img alt="Figure 8.15 – Using tags to connect an endpoint to a project&#13;&#10;" src="image/B16865_08_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – Using tags to connect an endpoint to a pro<a id="_idTextAnchor397"/>ject</p>
			<p>Next, you’ll learn <a id="_idIndexMarker549"/>how to connect your models <span class="No-Break">to applicati<a id="_idTextAnchor398"/>ons.</span></p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor399"/>Connecting your applications</h1>
			<p>You can deploy<a id="_idIndexMarker550"/> your model anywhere using Databricks Model<a id="_idIndexMarker551"/> Serving, which is how you deployed your RAG chatbot model in <a href="B16865_07.xhtml#_idTextAnchor325"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. In this section, we will introduce how to host ML demo <a id="_idIndexMarker552"/>apps in <strong class="bold">Hugging Face</strong> (<strong class="bold">HF</strong>). Having an easy way to host ML apps allows you to build your ML portfolio, showcase your projects at conferences or with stakeholders, and work collaboratively with others in the ML ecosystem. With HF Spaces, you have multiple options for which Python library you use to create a web app. Two common ones are Streamlit <span class="No-Break">and Gradio.</span></p>
			<p>We prefer Gradio. It is an open source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then share a link to your demo or web application in just a few seconds using Gradio’s built-in sharing features. No JavaScript, CSS, or web hosting experience is needed – we <span class="No-Break">love it!</span></p>
			<p>We will walk you through deploying a chatbot to an HF Space in the <em class="italic">Applying our learning</em> section’s<a id="_idIndexMarker553"/> RAG <a id="_idIndexMarker554"/><span class="No-Break">proj<a id="_idTextAnchor400"/>ect work.</span></p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor401"/>Incorporating LLMs for analysts with SQL AI Functions</h1>
			<p>There are many<a id="_idIndexMarker555"/> use <a id="_idIndexMarker556"/>cases where you can integrate an LLM, such as DBRX or OpenAI, for insights. With the Databricks Data Intelligence Platform, it’s also possible for analysts who are most comfortable in SQL to take advantage of advances in machine learning and <span class="No-Break">artificial intelligence.</span></p>
			<p>Within Databricks, you can<a id="_idIndexMarker557"/> use <strong class="bold">AI Functions</strong>, which are built-in SQL functions to access LLMs directly. AI Functions are available for use in the DBSQL interface, SQL warehouse JDBC connection, or via the Spark SQL API. In <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.16</em>, we are leveraging the Databricks <span class="No-Break">SQL editor.</span></p>
			<p class="callout-heading">Foundational Models API</p>
			<p class="callout">The storage and <a id="_idIndexMarker558"/>processing of data for Databricks-hosted foundation models occur entirely within the Databricks Platform. Importantly, this data is not shared with any third-party model providers. This is not necessarily true when using the External Models API, which connects you to services such as OpenAI that have their own data privacy policies. Keep this in mind when you are concerned about data privacy. You may be able to pay for a tier of service that restricts the use of <span class="No-Break">your data.</span></p>
			<p>Let’s do some simple emotion classification. Since the three datasets we’ve been working with don’t include any natural language, we’ll create a small dataset ourselves first. You can also download a dataset (such as the Emotions dataset from Kaggle) or use any other natural language source available <span class="No-Break">to you:</span></p>
			<ol>
				<li>First, let’s explore the built-in <strong class="source-inline">AI_QUERY</strong> DBSQL function. This command will send our prompt to the remote model configured and retrieve the result. We’re using Databricks’ DBRX model, but you can use a variety of other open source and proprietary models as well. Open up the Databricks SQL editor and type in the code as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.16</em>. Let’s write a query to give us a sample sentence that we <span class="No-Break">can classify.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer286">
					<img alt="Figure 8.16 – Crafting a prompt using the AI_QUERY function&#13;&#10;" src="image/B16865_08_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Crafting a prompt using the AI_QUERY function</p>
			<ol>
				<li value="2">If you don’t <a id="_idIndexMarker559"/>have<a id="_idIndexMarker560"/> a dataset ready and don’t want to download one, you can build a function to generate a dataset for you, as shown here. We’re expanding on the prompt from <em class="italic">Step 1</em> to get several sentences back in <span class="No-Break">JSON format.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer287">
					<img alt="Figure 8.17 – Creating a function to generate fake data&#13;&#10;" src="image/B16865_08_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.17 – Creating a function to generate fake data</p>
			<ol>
				<li value="3">Now use the <strong class="source-inline">GENERATE_EMOTIONS_DATA</strong> function to build a small dataset. After a quick review <a id="_idIndexMarker561"/>of <a id="_idIndexMarker562"/>the data, it looks like we have a good sample <span class="No-Break">of emotions.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer288">
					<img alt="Figure 8.18 – Generating fake emotion data&#13;&#10;" src="image/B16865_08_17.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – Generating fake emotion data</p>
			<ol>
				<li value="4">Now, we’re<a id="_idIndexMarker563"/> going to <a id="_idIndexMarker564"/>write a function called <strong class="source-inline">CLASSIFY_EMOTION</strong>. We’re using the AI Function <strong class="source-inline">AI_QUERY</strong> again, but this function will use a new prompt asking the model to classify a given sentence as one of <span class="No-Break">six emotions.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer289">
					<img alt="Figure 8.19 – Creating a function to classify sentences by emotion&#13;&#10;" src="image/B16865_08_18.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.19 – Creating a function to classify sentences by emotion</p>
			<ol>
				<li value="5">Let’s call our<a id="_idIndexMarker565"/> function to <a id="_idIndexMarker566"/>evaluate an example sentence and take a look at <span class="No-Break">the results.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer290">
					<img alt="Figure 8.20 – Calling the CLASSIFY_EMOTION function&#13;&#10;" src="image/B16865_08_19.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.20 – Calling the CLASSIFY_EMOTION function</p>
			<ol>
				<li value="6">Finally, to classify all records in a table, we call the <strong class="source-inline">CLASSIFY_EMOTION</strong> function on the records in our table and view <span class="No-Break">the results.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer291">
					<img alt="Figure 8.21 – Calling the CLASSIFY_EMOTION function on a table&#13;&#10;" src="image/B16865_08_20.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.21 – Calling the CLASSIFY_EMOTION function on a table</p>
			<p>SQL AI Functions are a great way to put the power of LLMs in the hands of SQL users. Solutions like SQL AI Functions still require some technical knowledge. Databricks is researching ways to allow business users direct access to data, with less upfront development<a id="_idIndexMarker567"/> required to<a id="_idIndexMarker568"/> get your team moving even faster. Keep an eye out for exciting new product features that remove the programming experience barrier to unlock the va<a id="_idTextAnchor402"/>lue of <span class="No-Break">your data!</span></p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor403"/>Applying our learning</h1>
			<p>Let’s use what we<a id="_idIndexMarker569"/> have learned to build a SQL chatbot using the Favorita project’s table metadata, monitor the streaming transaction project’s model, and deploy the chatbot that we have assem<a id="_idTextAnchor404"/>bled <span class="No-Break">and evaluated.</span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor405"/>Technical requirements</h2>
			<p>The technical requirements <a id="_idIndexMarker570"/>needed to complete the hands-on examples in this chapter are <span class="No-Break">as follows:</span></p>
			<ul>
				<li> The SQLbot will require <span class="No-Break">OpenAI credentials.</span></li>
				<li>We will use the Databricks Secrets API to store our <span class="No-Break">OpenAI credentials.</span></li>
				<li>You will <a id="_idIndexMarker571"/>need a <strong class="bold">personal access token</strong> (<strong class="bold">PAT</strong>) to deploy your web app to HF. See <em class="italic">Further reading</em> for <span class="No-Break">de<a id="_idTextAnchor406"/>tailed</span><span class="No-Break"><a id="_idIndexMarker572"/></span><span class="No-Break"> instructions.</span></li>
			</ul>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor407"/>Project: Favorita store sales</h2>
			<p>Let’s build a<a id="_idIndexMarker573"/> simple <strong class="bold">SQLbot</strong> using OpenAI’s GPT to ask questions about our Favorita Sales tables. Please note that while this section continues to use the <em class="italic">Favorita Store Sales</em> data, it is not a continuation of the earlier project work. In this example, you’ll create instructions on how the bot can ask for a list of tables, get information from those tables, and sample data from the tables. The SQLbot will be able to build a SQL query and then interpret the results. To run the notebooks in this example, you will need an account with OpenAI on the OpenAI developer site and request a key for the <span class="No-Break">OpenAI API.</span></p>
			<p>To follow along in your own workspace, please open the <span class="No-Break">following notebook:</span></p>
			<p><span class="No-Break"><strong class="source-inline">CH8-01-SQL Chatbot</strong></span></p>
			<p>Keeping secret API keys in your Databricks notebook is far from best practice. You can lock down your notebook access and add a configuration notebook to your <strong class="source-inline">.gitignore</strong> file. However, your ability to remove people’s access may not be in your control, depending on your role. Generally, the permissions of admins include the ability to see all code. The OpenAI API key ties back to your account and your credit card. Note that running the notebook once cost <span class="No-Break">us $0.08.</span></p>
			<p>We added our API key to Databricks secrets. The Secrets API requires the Databricks CLI. We set up our CLI through Homebrew. If you haven’t already, we suggest getting Secrets set up for your workspace. This may require admin assistance. Start by installing or updating the Databricks CLI. You know the CLI is installed correctly when you get version v0.2 or higher. We are working with <strong class="source-inline">Databricks </strong><span class="No-Break"><strong class="source-inline">CLI v0.208.0</strong></span><span class="No-Break">.</span></p>
			<p>We followed these steps to set up our API key as <span class="No-Break">a secret:</span></p>
			<ol>
				<li>Create <span class="No-Break">a scope:</span><pre class="source-code">
databricks secrets create-scope dlia</pre></li>				<li>Create a secret within <span class="No-Break">the scope:</span><pre class="source-code">
databricks secrets put-secret dlia OPENAI_API_KEY</pre></li>				<li>Paste your API key into <span class="No-Break">the prompt.</span></li>
			</ol>
			<p>Once your secret is successfully saved, we can access it via <strong class="source-inline">dbutils.secrets</strong> in <span class="No-Break">our notebooks.</span></p>
			<p>We are all set up to use OpenAI via the API now. We do not have to worry about accidentally committing our API or a coworker running the code, not knowing it costs <span class="No-Break">you money.</span></p>
			<p>Next, let’s focus on creating our SQLbot notebook, step by step, beginning with <span class="No-Break">the setup:</span></p>
			<ol>
				<li>First, we install three libraries: <strong class="source-inline">openai</strong>, <strong class="source-inline">langchain_experimental</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">sqlalchemy-databricks</strong></span><span class="No-Break">.</span></li>
				<li>To create a connection to OpenAI, pass the secret we set up previously and open a <span class="No-Break"><strong class="source-inline">ChatOpenAI</strong></span><span class="No-Break"> connection.</span></li>
				<li>In <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.22</em>, we <a id="_idIndexMarker574"/>create two different models. The first is the default model and the second uses GPT <span class="No-Break">3.5 Turbo.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer292">
					<img alt="Figure 8.22 – OpenAI API connection&#13;&#10;" src="image/B16865_08_21.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.22 – OpenAI API connection</p>
			<ol>
				<li value="4">The setup file does not set your schema variable. Define your schema; we chose <strong class="source-inline">favorita_forecasting</strong>. We have been using <strong class="source-inline">database_name</strong> rather than a schema. However, we specify the database we want to ask SQL questions against, which <span class="No-Break">is different.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer293">
					<img alt="Figure 8.23 – (L) Collecting the table schema and system information schema; (R) dropping unnecessary and repetitive columns&#13;&#10;" src="image/B16865_08_22.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer294">
					<img alt="Figure 8.23 – (L) Collecting the table schema and system information schema; (R) dropping unnecessary and repetitive columns&#13;&#10;" src="image/B16865_08_23.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.23 – (L) Collecting the table schema and system information schema; (R) dropping unnecessary and repetitive columns</p>
			<ol>
				<li value="5">Next, we <a id="_idIndexMarker575"/>create two helper functions. The first function organizes the schema information provided, <strong class="source-inline">table_schemas</strong>, creating a table definition. The second collects two rows of data <span class="No-Break">as examples.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer295">
					<img alt="Figure 8.24 – Helper functions for organizing table information&#13;&#10;" src="image/B16865_08_24.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.24 – Helper functions for organizing table information</p>
			<ol>
				<li value="6">Iterate through the table and column data, leveraging our helper functions to format the SQL <span class="No-Break">database input.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer296">
					<img alt="Figure 8.25 – Iterating through the tables and leveraging the helper functions&#13;&#10;" src="image/B16865_08_25.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.25 – Iterating through the tables and leveraging the helper functions</p>
			<ol>
				<li value="7">We now have all of our data ready to be able to create a SQL database for OpenAI to talk to. You will need to edit <strong class="source-inline">endpoint_http_path</strong> to match the path of an <a id="_idIndexMarker576"/>active SQL warehouse in your workspace. The database is passed to both the default OpenAI model and the GPT <span class="No-Break">3.5 model.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer297">
					<img alt="Figure 8.26 – Create a database for OpenAI to query containing only the information we provided&#13;&#10;" src="image/B16865_08_26.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.26 – Create a database for OpenAI to query containing only the information we provided</p>
			<p>With the setup complete, we can now interact with our SQL chatbot models! Let’s start with a basic question: <em class="italic">Which store sold </em><span class="No-Break"><em class="italic">the most?</em></span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.27</em>, we run both models on the question and get back two <span class="No-Break">different answers.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer298">
					<img alt="Figure 8.27 – The SQL chatbot model’s responses to our question “Which store sold the most?”. (T) db_chain.run(question) (B) chat_chain.run(question)&#13;&#10;" src="image/B16865_08_27.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.27 – The SQL chatbot model’s responses to our question “Which store sold the most?”. (T) db_chain.run(question) (B) chat_chain.run(question)</p>
			<p>As new versions of OpenAI’s GPT model are released, the results and behavior of your SQLbot may change. As new models and approaches become available, it is good practice to test them and see how the changes impact your work and the results of your chatbot. Leveraging <a id="_idIndexMarker577"/>MLflow with your SQLbot experiment will help you track and compare the different features and configurations throughout your <span class="No-Break">production process.</span></p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor408"/>Project -streaming transactions</h2>
			<p>You are <a id="_idIndexMarker578"/>ready to wrap up this project. The production workflow notebooks are the <strong class="source-inline">CH7-08-Production Generating Records</strong>, <strong class="source-inline">CH7-09-Production Auto Loader</strong>, and <strong class="source-inline">CH7-10-Production Feature Engineering</strong> components in the workflow job created in <a href="B16865_07.xhtml#_idTextAnchor325"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. You will run that same job once the new workflow is in place. To follow along in your own workspace, please open the following notebook:<strong class="source-inline"> </strong><span class="No-Break"><strong class="source-inline">CH8-05-Production Monitoring</strong></span></p>
			<p>In the <strong class="source-inline">CH8-05-Production Monitoring</strong> notebook, you create two monitors – one for the <strong class="source-inline">prod_transactions</strong> table and one for the <strong class="source-inline">packaged_transaction_model_predictions</strong> table. See <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.28</em> for <span class="No-Break">the latter.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer299">
					<img alt="Figure 8.28 – The inference table monitor&#13;&#10;" src="image/B16865_08_28.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.28 – The inference table monitor</p>
			<p>Congratulations! The streaming project is complete. We encourage you to add improvements and commit them back to the repository. Here are a few possible examples: add more<a id="_idIndexMarker579"/> validation metrics to the validation notebook, incorporate the inference performance results into the decision to retrain, and make adjustments to the configuration of data generati<a id="_idTextAnchor409"/>on to <span class="No-Break">simulate drift.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor410"/>Project: retrieval-augmented generation chatbot</h2>
			<p>To follow along<a id="_idIndexMarker580"/> in your own workspace, please open the following notebooks <span class="No-Break">and resources:</span></p>
			<ul>
				<li><span class="No-Break">CH8-</span><span class="No-Break"><strong class="source-inline">app.py</strong></span></li>
				<li><strong class="source-inline">CH8-01-Deploy Your Endpoint </strong><span class="No-Break"><strong class="source-inline">with SDK</strong></span></li>
				<li>The <strong class="bold">Hugging Face </strong><span class="No-Break"><strong class="bold">Spaces</strong></span><span class="No-Break"> page</span></li>
			</ul>
			<p>First, we need to ensure our chatbot is deployed using Model Serving, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.30</em>. Here, we are using the fastest way by going through the UI of the Model Serving page. To follow along and serve, we are selecting the registered model that we registered in <a href="B16865_07.xhtml#_idTextAnchor325"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. Select the latest version – in our case, it’s version 4. For this demo project, we expect minimal concurrency so the endpoint will have only four possible concurrent runs and will scale to 0 when no traffic is available. We are enabling inference tables under the same catalog to track and potentially further monitor our payload. We are<a id="_idIndexMarker581"/> not going to demonstrate in this chapter how to set a monitor or data quality pipeline for the RAG project, as it was demonstrated for the streaming project. We encourage you to apply it on <span class="No-Break">your own!</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer300">
					<img alt="Figure 8.29 – Example of the Model Serving deployment via UI&#13;&#10;" src="image/B16865_08_29.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.29 – Example of the Model Serving deployment via UI</p>
			<p>In order for <a id="_idIndexMarker582"/>your application to be able to connect to the resources attached to it, such as Vector Search, the endpoint requires you to provide additional configurations to the endpoint such as your PAT and host under <span class="No-Break"><strong class="bold">Advanced configuration</strong></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer301">
					<img alt="Figure 8.30 – Advanced configuration requirements&#13;&#10;" src="image/B16865_08_30.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.30 – Advanced configuration requirements</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You could also use the Databricks SDK service to deploy your endpoints. If you are interested in seeing how to use SDK deployment, please use the notebook attached under <strong class="source-inline">CH8 - 01 -Deploy Your Endpoint </strong><span class="No-Break"><strong class="source-inline">with SDK</strong></span><span class="No-Break">.</span></p>
			<p>Jump over to the Hugging Face Spaces website. Instructions on how to deploy your first HF Space are explained very well on the main page of HF Spaces, so we will not duplicate them<a id="_idIndexMarker583"/> here. We would like to highlight that we are using a free deployment option of Spaces with 2 CPUs and 16 GB <span class="No-Break">of memory.</span></p>
			<p>When you deploy your Space, it will look <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer302">
					<img alt="Figure 8.31 – Empty Hugging Face Space&#13;&#10;" src="image/B16865_08_31.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.31 – Empty Hugging Face Space</p>
			<p>We would like to highlight a few things that are important in order to connect to your chatbot, which is served in real time with Databricks Model Serving. To connect the chatbot to your HF Space, you must set <strong class="source-inline">API_TOKEN</strong> and an <strong class="source-inline">API_ENDPOINT</strong>. Here’s how to set <span class="No-Break">these values:</span></p>
			<ol>
				<li>Go to <strong class="bold">Settings</strong> in the HF Space <span class="No-Break">you created.</span></li>
				<li>Scroll down to <strong class="bold">Variables </strong><span class="No-Break"><strong class="bold">and secrets</strong></span><span class="No-Break">.</span></li>
				<li>Set your API_ENDPOINT as the URL from the REST API provided on the Databricks Model <span class="No-Break">Serving page.</span></li>
				<li>Set your API_TOKEN using a personal access token generated by Databricks. This is required <a id="_idIndexMarker584"/>to connect to <span class="No-Break">the endpoint.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer303">
					<img alt="Figure 8.32 – Example of Variables and secrets on HF Spaces&#13;&#10;" src="image/B16865_08_32.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.32 – Example of Variables and secrets on HF Spaces</p>
			<ol>
				<li value="5">Once this is set, you are ready to bring your Gradio web app script into your <span class="No-Break">HF Space.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer304">
					<img alt="Figure 8.33 – Example of Variables and secrets on HF Spaces&#13;&#10;" src="image/B16865_08_33.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.33 – Example of Variables and secrets on HF Spaces</p>
			<ol>
				<li value="6">When your endpoint is ready, jump back to your <span class="No-Break">HF Space.</span></li>
				<li>Go to the <strong class="bold">Files</strong> tab under the pre-created Space and click <strong class="bold">+</strong><span class="No-Break"><strong class="bold">Add File</strong></span><span class="No-Break">.</span></li>
				<li>Now add <strong class="source-inline">CH8-app.py</strong> that was given to you for <a href="B16865_08.xhtml#_idTextAnchor384"><span class="No-Break"><em class="italic">Chapter 8</em></span></a> – you can create your own web application. Feel free to experiment with the design according to your <span class="No-Break">business needs.</span></li>
			</ol>
			<p>Let’s talk a bit about the <strong class="source-inline">respond</strong> function in the <strong class="source-inline">CH8-app.py</strong> file – see <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.34</em>, which is passed to our app’s UI chatbot. The <strong class="source-inline">respond</strong> function, in this case, is the caller of your <a id="_idIndexMarker585"/>deployed endpoints, where we do not just send and receive responses but also can shape the format of the input or output. In our case, the endpoint expects to receive a request in the format of a JSON with field inputs with the questions within a list, while the output is a JSON with <span class="No-Break">field predictions.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer305">
					<img alt="Figure 8.34 – The respond function written in the Gradio app&#13;&#10;" src="image/B16865_08_34.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.34 – The respond function written in the Gradio app</p>
			<p>To create a <a id="_idIndexMarker586"/>chatbot, as was mentioned in the introduction section, we are using a simple example from Gradio where we add options such as the title of our application, a description, and example questions. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.35</em> shows the <span class="No-Break">full code.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer306">
					<img alt="Figure 8.35 – The Gadio app.py interface for your LLM&#13;&#10;" src="image/B16865_08_35.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.35 – The Gadio app.py interface for your LLM</p>
			<p>The chatbot is<a id="_idIndexMarker587"/> now available in a more user-friendly interface, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.36</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer307">
					<img alt="Figure 8.36 – The interface for your chatbot application&#13;&#10;" src="image/B16865_08_36.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.36 – The interface for your chatbot application</p>
			<p>Let’s ask a few questions to make sure our RAG chatbot is providing <span class="No-Break">correct results.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer308">
					<img alt="Figure 8.37 – Examples of chatbot answers from our RAG application&#13;&#10;" src="image/B16865_08_37.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.37 – Examples of chatbot answers from our RAG application</p>
			<p>If the responses<a id="_idIndexMarker588"/> look good, your application<a id="_idTextAnchor411"/> is ready to <span class="No-Break">be used!</span></p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor412"/>Summary</h1>
			<p>Sharing insights from your models is an important way to get value from your machine learning practice. Using dashboards as a medium for sharing your information is an accessible way to communicate insights to business users and teams outside of the data science team. In this chapter, we discussed how to build the gold layer, tips on presenting your data using DBSQL dashboards, taking advantage of innovations such as OpenAI models in DBSQL, and how you can share data and AI artifacts through Databricks Marketplace to get the most value from your <span class="No-Break">enterprise data.</span></p>
			<p>We hope you have had a chance to get hands-on with building your lakehouse. From exploration, cleaning, building pipelines, and building models to finding insights hidden in your data, all the way to sharing insights – it’s all doable on the Databricks Platform. We encourage you to take the notebooks and experiment! The authors would love to hear feedback, and whether this has been helpful on your journey with the<a id="_idTextAnchor413"/> <span class="No-Break">Databrick<a id="_idTextAnchor414"/>s Platform.</span></p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor415"/>Questions</h1>
			<p>Let’s test ourselves on what we’ve learned by going through the <span class="No-Break">following questions:</span></p>
			<ol>
				<li>What are some differences between the gold layer and the <span class="No-Break">silver layer?</span></li>
				<li>What is one way you could set an alert to identify that a table has an <span class="No-Break">invalid value?</span></li>
				<li>Why would you choose to use an external <span class="No-Break">dashboarding tool?</span></li>
				<li>If you use a language model through an API such as OpenAI, what are some considerations about the data you send via <span class="No-Break">the API?</span></li>
				<li>What are some reasons a company would share data in <span class="No-Break">Databricks<a id="_idTextAnchor416"/> Marketplace?</span></li>
			</ol>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor417"/>Answers</h1>
			<p>After putting thought into the questions, compare your answers <span class="No-Break">to ours:</span></p>
			<ol>
				<li>The gold layer is more refined and aggregated than the silver layer. The silver layer powers data science and machine learning, and the gold layer powers analytics <span class="No-Break">and dashboarding.</span></li>
				<li>You can monitor the values of a field and send an email alert when the value <span class="No-Break">is invalid.</span></li>
				<li>Sometimes companies use multiple dashboarding tools. You might need to provide data in a dashboard a team is accustomed <span class="No-Break">to using.</span></li>
				<li>If I were a language model through an API, I would be cautious about sending sensitive data, including PII, customer information, or <span class="No-Break">proprietary information.</span></li>
				<li>A company might share data in Databricks Marketplace in order to monetize the data or make it available externally for people to use<a id="_idTextAnchor418"/> easily <span class="No-Break">and secu<a id="_idTextAnchor419"/>rely.</span></li>
			</ol>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor420"/>Further reading</h1>
			<p>In this chapter, we pointed out specific technologies, technical features, and options. Please look at these resources to get deeper into the areas that interest <span class="No-Break">you most:</span></p>
			<ul>
				<li><em class="italic">Databricks SQL Statement Execution </em><span class="No-Break"><em class="italic">API</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/2023/03/07/databricks-sql-statement-execution-api-announcing-public-preview.html"><span class="No-Break">https://www.databricks.com/blog/2023/03/07/databricks-sql-statement-execution-api-announcing-public-preview.html</span></a></li>
				<li><em class="italic">Power to the SQL People: Introducing Python UDFs in Databricks </em><span class="No-Break"><em class="italic">SQL</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html"><span class="No-Break">https://www.databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html</span></a></li>
				<li><em class="italic">Actioning Customer Reviews at Scale with Databricks SQL AI </em><span class="No-Break"><em class="italic">Functions</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/actioning-customer-reviews-scale-databricks-sql-ai-functions"><span class="No-Break">https://www.databricks.com/blog/actioning-customer-reviews-scale-databricks-sql-ai-functions</span></a></li>
				<li><em class="italic">Databricks sets the official data warehousing performance </em><span class="No-Break"><em class="italic">record</em></span><span class="No-Break">: https://dbricks.co/benchmark</span></li>
				<li><em class="italic">Databricks Lakehouse and Data </em><span class="No-Break"><em class="italic">Mesh</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/databricks-lakehouse-and-data-mesh-part-1"><span class="No-Break">https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html</span></a></li>
				<li><em class="italic">Hugging </em><span class="No-Break"><em class="italic">Face</em></span><span class="No-Break">: </span><a href="https://huggingface.co/spaces"><span class="No-Break">https://huggingface.co/spaces</span></a></li>
				<li><span class="No-Break"><em class="italic">Gradio</em></span><span class="No-Break">: </span><a href="https://www.gradio.app/"><span class="No-Break">https://www.gradio.app/</span></a></li>
				<li><em class="italic">Hugging Face </em><span class="No-Break"><em class="italic">Spaces</em></span><span class="No-Break">: </span><a href="https://huggingface.co/docs/hub/en/spaces-overview"><span class="No-Break">https://huggingface.co/docs/hub/en/spaces-overview</span></a></li>
				<li><em class="italic">Databricks Lakehouse monitoring </em><span class="No-Break"><em class="italic">documentation</em></span><span class="No-Break">: </span><a href="https://api-docs.databricks.com/python/lakehouse-monitoring/latest/databricks.lakehouse_monitoring.html#module-databricks.lakehouse_monitoring "><span class="No-Break">https://api-docs.databricks.com/python/lakehouse-monitoring/latest/databricks.lakehouse_monitoring.html#module-databricks.lakehouse_monitoring</span></a></li>
				<li>Databricks personal access token <span class="No-Break">authentication </span><a href="https://docs.databricks.com/en/dev-tools/auth/pat.html"><span class="No-Break">https://docs.databricks.com/en/dev-tools/auth/pat.html</span></a></li>
			</ul>
		</div>
	</body></html>