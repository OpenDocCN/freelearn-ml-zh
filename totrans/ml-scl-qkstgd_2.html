<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Scala for Regression Analysis</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn regression analysis in detail. We will start learning from the regression analysis workflow followed by the <strong>linear regression</strong> (<strong>LR</strong>) and <strong>generalized linear regression</strong> (<strong>GLR</strong>) algorithms. Then we will develop a regression model for predicting slowness in traffic using LR and GLR algorithms and their Spark ML-based implementation in Scala. Finally, we will learn the hyperparameter tuning with cross-validation and the grid searching techniques. Concisely, we will learn the following topics throughout this end-to-end project:</p>
<ul>
<li>An overview of regression analysis</li>
<li>Regression analysis algorithms</li>
<li>Learning regression analysis through examples</li>
<li>Linear regression</li>
<li>Generalized linear regression</li>
<li>Hyperparameter tuning and cross-validation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.</p>
<p>The code files of this chapters can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter02" target="_blank">https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter02</a></p>
<p>Check out the following video to see the Code in Action:</p>
<p><a href="http://bit.ly/2GLlQTl" target="_blank">http://bit.ly/2GLlQTl</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of regression analysis</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we already gained some basic understanding of the <strong>machine learning</strong> (<strong>ML</strong>) process, as we have seen the basic distinction between regression and classification. <span>Regression analysis is a set of statistical processes for estimating the relationships between a set of variables called a dependent variable and one or multiple independent variables. The values of dependent variables depend on the values of independent variables.</span></p>
<p><span>A regression analysis technique helps us to understand this dependency, that is, how the value of the dependent variable changes when any one of the independent variables is changed, while the other independent variables are held fixed.</span> <span>For example, let's assume that there will be more savings in someone's bank when they grow older. Here, the amount of <strong>Savings</strong> (say in million $) depends on age (that is, <strong>Age</strong> in years, for example):<br/></span></p>
<table style="border-collapse: collapse;border-color: #000000" border="1">
<tbody>
<tr>
<td>
<p><strong>Age (years)</strong></p>
</td>
<td>
<p><strong>Savings (million $)</strong></p>
</td>
</tr>
<tr>
<td>
<p>40</p>
</td>
<td>
<p>1.5</p>
</td>
</tr>
<tr>
<td>
<p>50</p>
</td>
<td>
<p>5.5</p>
</td>
</tr>
<tr>
<td>
<p>60</p>
</td>
<td>
<p>10.8</p>
</td>
</tr>
<tr>
<td>
<p>70</p>
</td>
<td>
<p>6.7</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>So, we can plot these two values in a 2D plot, where the dependent variable (<strong>Savings</strong>) is plotted on the <em>y</em>-axis and the independent variable (<strong>Age</strong>) should be plotted on the <em>x</em>-axis. Once these data points are plotted, we can see correlations. If the theoretical chart indeed represents the impact of getting older on savings, then we'll be able to say that the older someone gets, the more savings there will be in their bank account. </p>
<p>Now the question is how can we tell the degree to which age helps someone to get more money in their bank account? To answer this question, one can draw a line through the middle of all of the data points on the chart. This line is called the regression line, which can be calculated precisely using a regression analysis algorithm. A regression analysis algorithm takes either discrete or continuous (or both) input features and produces continuous values.</p>
<div class="packt_tip">A classification task is used for predicting the label of the class attribute, while a regression task is used for making a numeric prediction of the class attribute.</div>
<p>Making a prediction using such a regression model on unseen and new observations is like creating a data pipeline with multiple components working together, where we observe an algorithm's performance in two stages: learning and inference. In the whole process and for making the predictive model a successful one, data acts as the first-class citizen in all ML tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning</h1>
                </header>
            
            <article>
                
<p>One of the important task at the learning stage is to prepare and convert the data into feature vectors (vectors of numbers out of each feature). Training data in feature vector format can be fed into the learning algorithms to train the model, which can be used for inferencing. Typically, and of course based on data size, running an algorithm may take hours (or even days) so that the features converge into a useful model as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-532 image-border" src="assets/4e9cb443-9b8c-406f-b6ad-bd6718fbba96.png" style="width:58.75em;height:11.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Learning and training a predictive model—it shows how to generate the feature vectors from the training data to train the learning algorithm that produces a predictive model</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inferencing</h1>
                </header>
            
            <article>
                
<p>In the inference stage, the trained model is used for making intelligent use of the model, such as predicting from never-before-seen data, making recommendations, and deducing future rules. Typically, it takes less time compared to the learning stage and sometimes even in real time. Thus, inferencing is all about testing the model against new (that is, unobserved) data and evaluating the performance of the model itself, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-533 image-border" src="assets/4c8a4f15-9b49-4993-8a42-50525807292c.png" style="width:55.92em;height:12.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Inferencing from an existing model towards predictive analytics (feature vectors are generated from unknown data for making predictions)</div>
<p>In summary, when using regression analysis the goal is to predict a continuous target variable. Now that we know how to construct a basic workflow for a supervised learning task, knowing a little about available regression algorithms will provide a bit more concrete information on how to apply these regression algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression analysis algorithms</h1>
                </header>
            
            <article>
                
<p>There are numerous algorithms proposed and available, which can be used for the regression analysis. For example, LR tries to find relationships and dependencies between variables. It models the relationship between a continuous dependent variable <em>y</em> (that is, a label or target) and one or more independent variables, <em>x</em>, using a linear function. Examples of regression algorithms include the following:</p>
<ul>
<li><strong>Linear regression</strong> (<strong>LR</strong>)</li>
<li><strong>Generalized linear regression</strong> (<strong>GLR</strong>)</li>
<li><strong>Survival regression</strong> (<strong>SR</strong>)</li>
<li><strong>Isotonic regression</strong> (<strong>IR</strong>)</li>
<li><strong>Decision tree regressor</strong> (<strong>DTR</strong>)</li>
<li><strong>Random forest regression</strong> (<strong>RFR</strong>)</li>
<li><strong>Gradient boosted trees regression</strong> (<strong>GBTR</strong>)</li>
</ul>
<p>We start by explaining regression with the simplest LR algorithm, which models the relationship between a dependent variable, <em>y</em>, which involves a linear combination of interdependent variables, <em>x</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-534 image-border" src="assets/11963a25-c48f-40da-a3e7-5f14b4f4ff65.png" style="width:25.67em;height:12.92em;"/></p>
<p>In the preceding equation letters, <em>β<sub>0</sub></em> and <em>β<sub>1</sub></em> are two constants for <em>y</em>-axis intercept and the slope of the line, respectively. LR is about learning a model, which is a linear combination of features of the input example (data points). </p>
<p>Take a look at the following graph and imagine that the red line is not there. We have a few dotted blue points (data points). Can we reasonably develop a machine learning (regression) model to separate most of them? Now, if we draw a straight line between two classes of data, those get almost separated, don't they? Such a line (red in our case) is called the decision boundary, which is also called the regression line in the case of regression analysis (see the following example for more):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-536 image-border" src="assets/ed09f3bd-c70c-461d-96d9-ceab36e4465d.png" style="width:38.67em;height:22.08em;"/></p>
<p>If we are given a collection of labeled examples, say <img class="fm-editor-equation" src="assets/4bbeb78b-a1d8-46cf-a394-d26d3489e864.png" style="width:6.08em;height:1.58em;"/>, where <em>N</em> is the number of samples in the dataset, <em>x<sub>i</sub></em> is the <em>D</em>-dimensional feature vector of the samples <em>i = 1, 2… N</em>, and <em>y<sub>i</sub></em> is a real-valued <em>y ∈ R</em>, where <em>R</em> denotes the set of all real numbers called the target variable and every feature <em>x<sub>i </sub></em>is a real number. Then combining these, the next step is to build the following mathematical model, <em>f</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/000a67b5-7349-4298-8ca3-3c8e63c0d435.png" style="width:10.00em;height:1.67em;"/></p>
<p>Here, <em>w</em> is a <em>D</em>-dimensional parameterized vector and <em>b</em> is a real number. The notation <em>f<sub>w,b</sub></em> signifies that the model <em>f</em> is parameterized by values <em>w</em> and <em>b</em>. Once we have a well-defined model, it can now be used for making a prediction of unknown <em>y</em> for a given <em>x,</em> that is, <em>y ← f<sub>w,b </sub>(x)</em>. However, there is an issue, as since the model is parametrized with two different values (<em>w</em>, <em>b</em>), this will mean the model tends to produce two different predictions when applied to the same sample, even when coming from the same distribution.</p>
<p class="CDPAlignLeft CDPAlign">Literally, it can be referred as an optimization problem—where the objective is to find the optimal (that is, minimum, for example) values <img class="fm-editor-equation" src="assets/ee4deb14-39bd-475d-b9d7-77be4da025a6.png" style="width:3.42em;height:1.83em;"/> such that the optimal values of parameters will mean the model tends to make more accurate predictions. In short, in the LR model, we intend to find the optimal values for <img class="fm-editor-equation" src="assets/d17f18d1-2969-40c3-bc5c-9d45e3f90ca0.png" style="width:1.00em;height:1.17em;"/> and <img class="fm-editor-equation" src="assets/14c7a856-3f2d-47a4-9510-8404fb177d51.png" style="width:0.67em;height:1.50em;"/> to minimize the following objective function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a0097bfe-fc96-41dc-8b43-7da46a2d8f17.png" style="width:18.75em;height:4.00em;"/></p>
<p>In the preceding equation, the expression <em>(f <sub>w,b</sub> (X<sub>i</sub>) - y<sub>i</sub>)<sup>2</sup></em> is called the <strong>loss function</strong>, which is a measure of penalty (that is, error or loss) for giving the wrong prediction for sample <em>i</em>. This loss function is in the form of squared error loss. However, other loss functions can be used too, as outlined in the following equations:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/11416c4c-02dc-44cb-8170-db2194fe7101.png" style="width:18.58em;height:4.33em;"/></div>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c9bbc527-8893-436e-9f81-be0e0fc582b7.png" style="width:18.67em;height:4.33em;"/></div>
<div>
<p>The <strong>squared error</strong> (<strong>SE</strong>) in equation 1 is called <em>L<sub>2</sub></em> loss, which is the default loss function for the regression analysis task. On the other hand, the <strong>absolute error</strong> (<strong>AE</strong>) in equation (<em>2)</em> is called <em>L<sub>1</sub></em> loss.</p>
</div>
<div class="packt_tip">
<p>In cases where the dataset has many outliers, using <em>L<sub>1</sub></em> loss is recommend more than <em>L<sub>2</sub></em>, because <em>L<sub>1</sub> </em>is more robust against outliers. </p>
</div>
<div>
<p>All model-based learning algorithms have a loss function associated with them. Then we try to find the best model by minimizing the cost function. In our LR case, the cost function is defined by the average loss (also called empirical risk), which can be formulated as the average of all penalties obtained by fitting the model to the training data, which may contain many samples.</p>
</div>
<p><em>Figure 4</em> shows an example of simple linear regression. Let's say the idea is to predict the amount of <strong>Savings</strong> versus <strong>Age</strong>. So, in this case, we have one independent variable <em>x</em> (that is, a set of 1D data points and, in our case, the <strong>Age</strong>) and one dependent variable, <em>y</em> (amount of <strong>Savings (in millions $)</strong>). Once we have a trained regression model, we can use this line to predict the value of the target <em>y<sub>l</sub></em> for a new unlabeled input example, <em>x<sub>l</sub>.</em> However, in the case of <em>D</em> -dimensional feature vectors (for example, <em>2D</em> or <em>3D</em>), it would be a plane (for <em>2D</em>) or a hyperplane (for <em>&gt;=3D</em>):</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-691 image-border" src="assets/113ca892-0d37-4fd7-8dc6-0e149613c560.png" style="width:160.58em;height:65.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4: A regression line separates data points to solve Age versus the amount of Savings: i) the left model separates data points based on training data: ii) the right model predicts for an unknown observation</div>
<p>Now you see why it is important to have the requirement that the regression hyperplane lies as close to the training examples as possible: if the blue line in <em>Figure 4</em> (the model on the right) is far away from the blue dots, the prediction <em>y<sub>l</sub></em> is less likely to be correct. The best fit line, which is expected to pass through most of the data points, is the result of the regression analysis. However, in practice it does not pass through all of the data points because of the existence of regression errors.</p>
<div class="packt_infobox">Regression error is the distance between any data points (actual) and the line (predicted).</div>
<p>Since solving a regression problem is itself an optimization problem, we expect a smaller margin for errors as possible because smaller errors contribute towards higher predictive accuracy, while predicting unseen observations. Although an LR algorithm is not so efficient in many cases, the nicest thing is that an LR model usually does not overfit, which is unlikely for a more complex model.</p>
<p>In the previous chapter, we discussed overfitting (a phenomenon whereby a model that shows a model predicts very well during the training but makes more errors when applied to test set) and underfitting (<span><span class="ember-view">if your training error is low and your validation error is high, then your model is most likely overfitting your training data</span></span>). Often these two phenomena occur due to bias and variance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance metrics</h1>
                </header>
            
            <article>
                
<p>To measure the predictive performance of a regression model, several metrics are proposed and in use in terms of regression errors, which can be outlined as follows:</p>
<ul>
<li><strong>Mean squared error (MSE)</strong>: It is the measure of the <span class="st">difference between the predicted and estimated values, that is,</span> how close a fitted line is to data points. The smaller the MSE, the closer the fit is to the data.</li>
<li><strong>Root mean squared error (RMSE)</strong>: It is the square root of the MSE but has the same units as the quantity plotted on the vertical axis.</li>
<li><strong>R-squared</strong>: It is the coefficient of determination for assessing how close the data is to the fitted regression line ranges between 0 and 1. The higher the R-squared, the better the model fits your data.</li>
<li><strong>Mean absolute error (MAE)</strong>: It is a measure of <em>accuracy</em> for continuous variables without considering their direction. The smaller the MAE, the better the model fits your data.</li>
</ul>
<p>Now that we know how a regression algorithm works and how to evaluate the performance using several metrics, the next important task is to apply this knowledge to solve a real-life problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning regression analysis through examples</h1>
                </header>
            
            <article>
                
<p>In the previous section, we discussed a simple real-life problem (that is, <strong>Age</strong> versus <strong>Savings</strong>). However, in practice, there are several real-life problems where more factors and parameters (that is, data properties) are involved, where regression can be applied too. Let's first introduce a real-life problem. Imagine that you live in Sao Paulo, a city in Brazil, where every day several hours of your valuable time are wasted because of unavoidable reasons such as an immobilized bus, broken truck, vehicle excess, accident victim, overtaking, fire vehicles, incident involving dangerous freight, lack of electricity, fire, and flooding.</p>
<p>Now, to measure how many man hours get wasted, we can we develop an automated technique, which will predict the slowness of traffic such that you can avoid certain routes or at least get some rough estimation of how long it'll take you to reach some point in the city. A predictive analytics application using machine learning is probably one of the preferred solutions for predicting such slowness. Yes, for that we'll use the behavior of the urban traffic of the city of Sao Paulo in Brazil dataset in the next section. <strong><br/></strong></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Description of the dataset</h1>
                </header>
            
            <article>
                
<p>The dataset is downloaded from <a href="https://archive.ics.uci.edu/ml/datasets/Behavior+of+the+urban+traffic+of+the+city+of+Sao+Paulo+in+Brazil">https://archive.ics.uci.edu/ml/datasets/Behavior+of+the+urban+traffic+of+the+city+of+Sao+Paulo+in+Brazil</a>. It contains the records of behavior of the urban traffic of the city of Sao Paulo in Brazil between December 14, 2009 and December 18, 2009. The dataset has the following features:</p>
<ul>
<li><strong>Hour</strong>: Total hours spent on the road</li>
<li><strong>Immobilized bus</strong>: Number of immobilized buses</li>
<li><strong>Broken truck</strong>: Number of broken trucks</li>
<li><strong>Vehicle excess</strong>: Number of redundant vehicles</li>
<li><strong>Accident victim</strong>: Number of accident victims on the road or road side</li>
<li><strong>Running over</strong>: Number of running over or taking over cases </li>
<li><strong>Fire vehicles</strong>: Number of fire trucks and vehicles</li>
</ul>
<ul>
<li><strong>Occurrence involving freight</strong>: Number of <span><span>goods transported in bulk by trucks</span></span></li>
<li><strong>Incident involving dangerous freight</strong>: Number of <span>transporter bulk trucks involved in accident</span></li>
<li><strong>Lack of electricity</strong>: Number of hours without electricity in the affected areas</li>
<li><strong>Fire</strong>: Number of fire incidents</li>
<li><strong>Point of flooding</strong>: Number of points of flooding areas</li>
<li><strong>Manifestations</strong>: Number of places showing construction work ongoing or dangerous signs</li>
<li><strong>Defect in the network of trolleybuses</strong>: Number of defects in the network of trolley buses</li>
<li><strong>Tree on the road</strong>: Number of trees on the road or road side that create obstacles</li>
<li><strong>Semaphore off</strong>: Number of <span class="ILfuVd">mechanical gadgets with arms, lights, or flags that are used as a signal</span></li>
<li><strong>Intermittent semaphore</strong>: Number of <span class="ILfuVd">mechanical gadgets with arms, lights, or flags that are used as a signal</span> for a specific period of time</li>
<li><strong>Slowness in traffic</strong>: Number of average hours people got stuck in traffic because of the preceding reasons</li>
</ul>
<p>The last feature is the target column, which we want to predict. Since I used this dataset, I would like to acknowledge the following publication:</p>
<p>Ferreira, R. P., Affonso, C., &amp; Sassi, R. J. (2011, November). Combination of Artificial Intelligence Techniques for Prediction the Behavior of Urban Vehicular Traffic in the City of Sao Paulo. In 10th Brazilian Congress on Computational Intelligence (CBIC) - Fortaleza, Brazil. (pp.1-7), 2011.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploratory analysis of the dataset</h1>
                </header>
            
            <article>
                
<p>First, we read the training set for the <strong>exploratory data analysis</strong> (<strong>EDA</strong>). Readers can refer to the <kbd>EDA.scala</kbd> file for this. Once extracted, there will be a CSV file named <kbd>Behavior of the urban traffic of the city of Sao Paulo in Brazil.csv</kbd>. Let's rename the file as <kbd>UrbanTraffic.csv</kbd>. Also, <kbd>Slowness in traffic (%)</kbd>, which is the <span>last column,</span> represents the percentage of slowness in an unusual format: it represents the real number with a <span>comma (</span><kbd>,</kbd><span>)</span>, for example, <kbd>4,1</kbd> instead of <kbd>4.1</kbd>. So I replaced all instances of a comma (<kbd>,</kbd>) in that column with a period (<kbd>.</kbd>). Otherwise, the Spark CSV reader will treat the column as a <span><kbd>String</kbd> </span>type:</p>
<pre><strong>val</strong> filePath= "data/UrbanTraffic.csv"</pre>
<p>First, let's load, parse, and create a DataFrame using the <kbd>read.csv()</kbd> method but with the Databricks CSV format (also known as <kbd>com.databricks.spark.csv</kbd>) by setting it to read the header of the CSV file, which is directly applied to the columns' names of the DataFrame created; and the <kbd>inferSchema</kbd> property is set to <kbd>true</kbd>, because if you don't specify the <kbd>inferSchema</kbd> configuration explicitly, the float values would be treated as strings<em>.</em> This might cause <kbd>VectorAssembler</kbd> to raise an exception such as <kbd>java.lang.IllegalArgumentException: Data type StringType is not supported</kbd>:</p>
<pre class="mce-root"><strong>val </strong>rawTrafficDF = spark.read<br/>      .option("header", "true")<br/>      .option("inferSchema", "true")<br/>      .option("delimiter", ";")<br/>      .format("com.databricks.spark.csv")<br/>      .load("data/UrbanTraffic.csv")<br/>      .cache</pre>
<p>Now let's print the schema of the DataFrame we just created to check to make sure the structure is preserved:</p>
<pre class="mce-root">rawTrafficDF.printSchema()</pre>
<p class="mce-root">As seen from the following screenshot, the schema of the Spark DataFrame has been correctly identified. Also, as expected, <span>all the features of </span>my ML algorithms are numeric (in other words, in integer or double format):</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-538 image-border" src="assets/4d2ffd9c-b6b3-41e5-a06e-ef7868fed189.png" style="width:34.33em;height:21.17em;"/></p>
<p>You can see that none of the columns are categorical features. So, we don't need any numeric transformation. Now let's see how many rows there are in the dataset using the <kbd>count()</kbd> method:</p>
<pre>println(rawTrafficDF.count())</pre>
<p class="mce-root">This gives a 135 sample count<strong>.</strong> Now let's see a snapshot of the dataset using the <kbd>show()</kbd> method, but with only some selected columns so that it can make more sense rather than showing all of them. But feel free to use <kbd>rawTrafficDF.show()</kbd> to see all columns:</p>
<pre>rawTrafficDF.select("Hour (Coded)", "Immobilized bus", "Broken Truck", <br/>                    "Vehicle excess", "Fire", "Slowness in traffic (%)").show(5)</pre>
<p class="mce-root">As the <kbd>Slowness in traffic (%)</kbd> column contains continuous values, we have to deal with a regression task. Now that we have seen a snapshot of the dataset, it would be worth seeing some other statistics such as average claim or loss, minimum, and maximum loss of Spark SQL using the <kbd>sql()</kbd> interface:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-539 image-border" src="assets/a076e247-3e92-4e9f-9f63-9848530ada55.png" style="width:118.58em;height:31.17em;"/></div>
<p>However, before that, let's rename the last column from <kbd>Slowness in traffic (%)</kbd> to <kbd>label</kbd>, since the ML model will complain about it. Even after using <kbd>setLabelCol</kbd> on the regression model, it still looks for a column called <kbd>label</kbd>. This introduces a disgusting error saying <kbd>org.apache.spark.sql.AnalysisException: cannot resolve 'label' given input columns</kbd>:</p>
<pre><strong>var </strong>newTrafficDF = rawTrafficDF.withColumnRenamed("Slowness in traffic (%)", "label")</pre>
<p>Since we want to execute some SQL query, we need to create a temporary view so that the operation can be performed in-memory:</p>
<pre>newTrafficDF.createOrReplaceTempView("slDF")</pre>
<p>Now let's average the slowness in the form of a percentage (the deviation with standard hours):</p>
<pre>spark.sql("SELECT avg(label) as avgSlowness FROM slDF").show()</pre>
<p>The preceding line of code should show a 10% delay on <span>average </span>every day across routes and based on other factors: </p>
<pre><br/> <strong>+------------------+</strong><br/><strong> | avgSlowness      |</strong><br/><strong> +------------------+</strong><br/><strong> |10.051851851851858|</strong><br/><strong> +------------------+</strong></pre>
<p>Also, we can see the number of flood points in the city. However, for that we might need some extra effort by changing the column name into a single string since it's a multi-string containing spaces, so SQL won't be able to resolve it:</p>
<pre>newTrafficDF = newTrafficDF.withColumnRenamed("Point of flooding", "NoOfFloodPoint")<br/>spark.sql("SELECT max(NoOfFloodPoint) FROM slDF").show()</pre>
<p>This should show as many as seven flood points that could be very dangerous:</p>
<pre><strong>+-------------------+</strong><br/><strong>|max(NoOfFloodPoint)|</strong><br/><strong>+-------------------+</strong><br/><strong>|                  7|</strong><br/><strong>+-------------------+</strong></pre>
<p>However, the <kbd>describe()</kbd> method will give these types of statistics more flexibly. Let's do it for the selected columns:</p>
<pre>rawTrafficDF.select("Hour (Coded)", "Immobilized bus", "Broken Truck", <br/>                    "Point of flooding", "Fire", "Slowness in traffic (%)")<br/>                    .describe().show()</pre>
<p class="mce-root">So, we can see that the slowness varies between <kbd>3.4</kbd> and <kbd>23.4</kbd>, which is quite high. This is why we need efficient data processing steps so that such a relation can be preserved. Now let's focus on the data preprocessing instead:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-540 image-border" src="assets/1f6af703-9e8a-4f0c-a96b-09601ef91887.png" style="width:164.17em;height:25.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature engineering and data preparation</h1>
                </header>
            
            <article>
                
<p>Now that we have seen some properties of the dataset and since there're no null values or categorical features, we don't need any other preprocessing or intermediate transformations. We just need to do some feature engineering before we can have our training and test sets.</p>
<p>The first step before getting these sets is to prepare training data that is consumable by the Spark regression model. For this, Spark classification and regression algorithms expect two components called <kbd>features</kbd> and <kbd>label</kbd>. Fortunately, we already have the <kbd>label</kbd> column. Next, the <kbd>features</kbd> column has to contain the data from all the columns except the <kbd>label</kbd> column, which can be achieved using the <kbd>VectorAssembler()</kbd> transformer.</p>
<p>Since all the columns are numeric, we can use <kbd>VectorAssembler()</kbd> directly from the Spark ML library to transform a given list of columns into a single vector column. So, let's collect the list of desirable columns. As you may have guessed, we'll have to exclude the <kbd>label</kbd> column, which can be done using the <kbd>dropRight()</kbd> method of standard Scala:</p>
<pre class="mce-root"><strong>val </strong>colNames = newTrafficDF.columns.dropRight(1) <strong>   <br/><br/>val</strong> assembler = new VectorAssembler()<br/>    .setInputCols(colNames)<br/>    .setOutputCol("features")</pre>
<p>Now that we have the <kbd>VectorAssembler()</kbd> estimator, we now call the <kbd>transform()</kbd> method, which will embed selected columns into a single vector column:</p>
<pre>val assembleDF = assembler.transform(newTrafficDF).select("features", "label")  <br/>assembleDF.show()</pre>
<p>As expected, the last line of the preceding code segment shows the assembled DataFrame having <kbd>label</kbd> and <kbd>features</kbd>, which are needed to train an ML algorithm:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-541 image-border" src="assets/a0b68aa4-2797-4b5e-9a8f-644d1d4edf84.png" style="width:14.25em;height:19.17em;"/></p>
<p>We can now proceed to generate separate training and test sets. Additionally, we can cache both the sets for faster in-memory access. We use 60% of the data to train the model and the other 40% will be used to evaluate the model:</p>
<pre class="mce-root"><strong>val</strong> seed = 12345L<br/><strong>val</strong> splits = data.randomSplit(Array(0.60, 0.40), seed)<br/><strong>val</strong> (trainingData, validationData) = (splits(0), splits(1))<br/><br/>trainingData.cache // cache in memory for quicker access<br/>validationData.cache // cache in memory for quicker access</pre>
<div>
<p class="mce-root">That is all we need before we start training the regression models. At first, we start training the LR model and evaluate the performance.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we will develop a predictive analytics model for predicting slowness in traffic for each row of the data using an LR algorithm. First, we create an LR estimator as follows:</p>
<pre class="mce-root"><strong>val</strong> lr = new LinearRegression()<br/>     .setFeaturesCol("features")<br/>     .setLabelCol("label")</pre>
<p>Then we invoke the <kbd>fit()</kbd> method to perform the training on the training set as follows:</p>
<pre>println("Building ML regression model")<br/>val lrModel = lr.fit(trainingData)</pre>
<p>Now we have the fitted model, which means it is now capable of making predictions. So, let's start evaluating the model on the training and validation sets and calculating the RMSE, MSE, MAE, R squared, and so on:</p>
<pre class="mce-root">println("Evaluating the model on the test set and calculating the regression metrics")<br/>// **********************************************************************<br/>val trainPredictionsAndLabels = lrModel.transform(testData).select("label", "prediction")<br/>                                            .map {case Row(label: Double, prediction: Double) <br/>                                            =&gt; (label, prediction)}.rdd<br/><br/>val testRegressionMetrics = new RegressionMetrics(trainPredictionsAndLabels)    </pre>
<p>Great! We have managed to compute the raw prediction on the training and the test sets. Now that we have both the performance metrics on both training and validation sets, let's observe the results of the training and the validation sets:</p>
<pre class="mce-root">val results = "\n=====================================================================\n" +<br/>      s"TrainingData count: ${trainingData.count}\n" +<br/>      s"TestData count: ${testData.count}\n" +<br/>      "=====================================================================\n" +<br/>      s"TestData MSE = ${testRegressionMetrics.meanSquaredError}\n" +<br/>      s"TestData RMSE = ${testRegressionMetrics.rootMeanSquaredError}\n" +<br/>      s"TestData R-squared = ${testRegressionMetrics.r2}\n" +<br/>      s"TestData MAE = ${testRegressionMetrics.meanAbsoluteError}\n" +<br/>      s"TestData explained variance = ${testRegressionMetrics.explainedVariance}\n" +<br/>      "=====================================================================\n"<br/>println(results)</pre>
<p>The preceding code segment should show something similar. Although, because of the randomness, you might experience slightly different output:</p>
<pre><strong>=====================================================================</strong><br/><strong> TrainingData count: 80</strong><br/><strong> TestData count: 55</strong><br/><strong> =====================================================================</strong><br/><strong> TestData MSE = 7.904822843038552</strong><br/><strong> TestData RMSE = 2.8115516788845536</strong><br/><strong> TestData R-squared = 0.3699441827613118</strong><br/><strong> TestData MAE = 2.2173672546414536</strong><br/><strong> TestData explained variance = 20.293395978801147</strong><br/><strong> =====================================================================</strong></pre>
<p>Now that we have the prediction on the test set as well, however, we can't directly say if it's a good or optimal regression model. To improve the result further with lower MAE, Spark also provides the generalized version of linear regression implementation called GLR.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generalized linear regression (GLR)</h1>
                </header>
            
            <article>
                
<p>In an LR, the output is assumed to follow a Gaussian distribution. In contrast, in <strong>generalized linear models</strong> (<strong>GLMs</strong>), the response variable <em>Y<sub>i</sub></em> follows some random distribution from a parametric set of probability distributions of a certain form. As we have seen in the previous example, following and creating a GLR estimator will not be difficult:</p>
<pre class="mce-root"><strong>val</strong> glr = new GeneralizedLinearRegression()<br/>      .setFamily("gaussian")//continuous value prediction (or gamma)<br/>      .setLink("identity")//continuous value prediction (or inverse)<br/>      .setFeaturesCol("features")<br/>      .setLabelCol("label")</pre>
<p>For the GLR-based prediction, the following response and identity link functions are supported based on data types (source: <a href="https://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression">https://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression</a>):</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-542 image-border" src="assets/fbc3b29b-50ad-4c25-9c92-8a25f3fdd716.png" style="width:33.33em;height:17.83em;"/></div>
<p class="mce-root">Then we invoke the <kbd>fit()</kbd> method to perform the training on the training set as follows:</p>
<pre>println("Building ML regression model")<br/><strong>val</strong> glrModel = glr.fit(trainingData)</pre>
<div>
<p>The current implementation through the <kbd>GeneralizedLinearRegression</kbd> interface in Spark supports up to 4,096 features only. Now that we have the fitted model (which means it is now capable of making predictions), let's start evaluating the model on training and validation sets and calculating the RMSE, MSE, MAE, R squared, and so on:</p>
</div>
<pre class="mce-root">// **********************************************************************<br/>println("Evaluating the model on the test set and calculating the regression metrics")<br/>// **********************************************************************<br/>val trainPredictionsAndLabels = glrModel.transform(testData).select("label", "prediction")<br/>                                            .map { case Row(label: Double, prediction: Double) <br/>                                            =&gt; (label, prediction) }.rdd<br/><br/>val testRegressionMetrics = new RegressionMetrics(trainPredictionsAndLabels)</pre>
<p>Great! We have managed to compute the raw prediction on the training and the test sets. Now that we have both the performance metrics on both training and test sets, let's observe the result on the train and the validation sets:</p>
<pre class="mce-root"><strong>val </strong>results = "\n=====================================================================\n" +<br/>      s"TrainingData count: ${trainingData.count}\n" +<br/>      s"TestData count: ${testData.count}\n" +<br/>      "=====================================================================\n" +<br/>      s"TestData MSE = ${testRegressionMetrics.meanSquaredError}\n" +<br/>      s"TestData RMSE = ${testRegressionMetrics.rootMeanSquaredError}\n" +<br/>      s"TestData R-squared = ${testRegressionMetrics.r2}\n" +<br/>      s"TestData MAE = ${testRegressionMetrics.meanAbsoluteError}\n" +<br/>      s"TestData explained variance = ${testRegressionMetrics.explainedVariance}\n" +<br/>      "=====================================================================\n"<br/>println(results)</pre>
<p>The preceding code segment should show similar results. Although, because of the randomness, you might experience slightly different output:</p>
<pre><br/> <strong>=====================================================================</strong><br/><strong> TrainingData count: 63</strong><br/><strong> TestData count: 72</strong><br/><strong> =====================================================================</strong><br/><strong> TestData MSE = 9.799660597570348</strong><br/><strong> TestData RMSE = 3.130440958965741</strong><br/><strong> TestData R-squared = -0.1504361865072692</strong><br/><strong> TestData MAE = 2.5046175463628546</strong><br/><strong> TestData explained variance = 19.241059408685135</strong><br/><strong> =====================================================================</strong></pre>
<p>Using GLR, we can see a slightly worse MAE value and also the RMSE is higher. If you see these two examples, we have not got to tune the hyperparameters but simply let the models train and evaluate a single value of each parameter. We could even use a regularization parameter for reducing overfitting. However, the performance of an ML pipeline often improves with the hyperparameter tuning, which is usually done with grid search and cross-validation. In the next section, we will discuss how we can get even better performance with the cross-validated models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter tuning and cross-validation</h1>
                </header>
            
            <article>
                
<p>In machine learning, the term hyperparameter refers to those parameters that cannot be learned from the regular training process directly. These are the various knobs that you can tweak on your machine learning algorithms. Hyperparameters are usually decided by training the model with different combinations of the parameters and deciding which ones work best by testing them. Ultimately, the combination that provides the best model would be our final hyperparameters. Setting hyperparameters can have a significant influence on the performance of the trained models.</p>
<p>On the other hand, cross-validation is often used in conjunction with hyperparameter tuning. Cross-validation (also know as<strong> </strong>rotation estimation) is a model validation technique for assessing the quality of the statistical analysis and results. Cross-validation helps to describe a dataset to test the model in the training phase using the validation set. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter tuning</h1>
                </header>
            
            <article>
                
<p>Unfortunately, there is no shortcut or straightforward way of choosing the right combination of hyperparameters based on a clear recipe—of course, experience helps. For example, while training a random forest, Matrix factorization, k-means, or a logistic/LR algorithm might be appropriate. Here are some typical examples of such hyperparameters:</p>
<ul>
<li>Number of leaves, bins, or depth of a tree in tree-based algorithms</li>
<li>Number of iterations</li>
<li>Regularization values</li>
<li>Number of latent factors in a matrix factorization</li>
<li>Number of clusters in a k-means clustering and so on</li>
</ul>
<p class="mce-root">Technically, hyperparameters form an <em>n</em>-dimensional space called a param-grid, where <em>n</em> is the number of hyperparameters. Every point in this space is one particular hyperparameter configuration, which is a hyperparameter vector.</p>
<div class="packt_tip"><span><span class="ember-view"><span>As discussed in <a href="33fe7442-ce44-4a18-bac6-0e08e9b1ae1e.xhtml">Chapter 1</a>, <em>Introduction to Machine Learning with Scala</em>, overfitting and underfitting are two problematic phenomena in machine learning. Therefore, sometimes full convergence to a best model parameter set is often not necessary and can be even preferred, because an almost-best-fitting model tends to perform better on new data or settings</span></span></span>. In other words, if you care for a best fitting model, you really don't need the best parameter set.</div>
<p class="mce-root">In practice, we cannot explore every point in this space, so the grid search over a subset in that space is commonly used. The following diagram shows some high-level idea:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-692 image-border" src="assets/01205c83-200d-47ce-aa8a-d7c03f2909ea.png" style="width:163.58em;height:70.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5: Hyperparameter tuning of ML models</div>
<p>Although there are several approaches for such a scheme, random search or grid search are probably the most well-known techniques used:<em><br/></em></p>
<ul>
<li><strong>Grid search</strong>: Using this approach, different hyperparameters are defined in a dictionary that you want to test. Then a param-grid is constructed before feeding them into the ML model such that the training can be performed with the different combinations. Finally, the algorithm tells you for which combination of the hyperparameters you have the highest accuracy.</li>
<li class="graf graf--p graf-after--p"><strong>Random search</strong>: As you can understand, training an ML model with all possible combinations of hyperparameters is a very expensive and time consuming operation. However, often we don't have that much flexibility but still we want to tune those parameters. In such a situation, random search could be a workaround. Random search is performed through evaluating <em>n</em> uniformly random points in the hyperparameter space, and selecting the right combination for which the model gives the best performance.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cross-validation</h1>
                </header>
            
            <article>
                
<p>There are two types of cross-validation, called exhaustive cross-validation, which includes leave-p-out cross-validation and leave-one-out cross-validation, and non-exhaustive cross-validation, which is based on K-fold cross-validation and repeated random sub-sampling cross-validation, for example, 5-fold or 10-fold cross-validation, is very common.</p>
<p>In most of the cases, 10-fold cross-validation is used instead of testing on a validation set. Also, the training set should be as large as possible (because more data with quality features are good to train the model) not only to train the model but because about 5 to 10% of the training set can be used for the cross-validation.</p>
<p>Using the K-fold cross-validation technique, the complete training data is split into K subsets. The model is trained on K-1 subsets; hold the last one for the validation. This process is repeated K times so that each time, one of the K subsets is used as the validation set and the other K-1 subsets are used to form the training set. This way, each of the subsets (fold) is used at least once for both training and validation.</p>
<p>Finally, different machine learning models that have been obtained are joined by a bagging (or boosting) scheme for classifiers or by averaging (that is, regression). The following diagram explains the 10-fold cross-validation technique:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-693 image-border" src="assets/460bc239-46e0-4317-8bfa-48b326d6ae45.png" style="width:51.58em;height:36.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 6:</span><span> 10-fold cross-validation technique</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning and cross-validation in Spark ML</h1>
                </header>
            
            <article>
                
<p>In Spark ML, before performing the cross-validation, we need to have a <kbd>paramGrid</kbd> (that is a grid of parameters). The <kbd>ParamGridBuilder</kbd> interface is used in order to define the hyperparameter space where <kbd>CrossValidator</kbd> has to search and finally, <kbd>CrossValidator()</kbd> takes our pipeline, the hyperparameter space of our LR regressor, and the number of folds for the cross-validation as parameters.</p>
<p>So, let's start creating <kbd>paramGrid</kbd> by specifying the number of maximum iterations, the value of regularization parameter, the value of tolerance, and the elastic network parameters, as follows for the LR model (since we observed lower MAE for this):</p>
<pre class="mce-root">// ***********************************************************<br/>println("Preparing K-fold Cross Validation and Grid Search")<br/>// ***********************************************************<br/>val paramGrid = new ParamGridBuilder()<br/>      .addGrid(lr.maxIter, Array(10, 20, 30, 50, 100, 500, 1000))<br/>      .addGrid(lr.regParam, Array(0.001, 0.01, 0.1))<br/>      .addGrid(lr.tol, Array(0.01, 0.1))<br/>      .build()</pre>
<div>
<p>The regularization parameter reduces overfitting by reducing the variance of your estimated regression parameters. Now, for a better and more stable performance, we can perform 10-fold cross-validation. Since our task is predicting continuous values, we need to define <kbd>RegressionEvaluator</kbd>, that is, the evaluator for regression, which expects two input columns—<kbd>prediction</kbd> and <kbd>label</kbd>—and evaluates the training based on MSE, RMSE, R-squared, and MAE:</p>
</div>
<pre class="mce-root">println("Preparing 10-fold Cross Validation")<br/><strong>val</strong> numFolds = 10 //10-fold cross-validation<br/><strong>val</strong> cv = new CrossValidator()<br/>      .setEstimator(lr)<br/>      .setEvaluator(new RegressionEvaluator())<br/>      .setEstimatorParamMaps(paramGrid)<br/>      .setNumFolds(numFolds)</pre>
<p>Fantastic, we have created the cross-validation estimator. Now it's time to train the LR model:</p>
<pre>println("Training model with the Linear Regression algorithm")<br/><strong>val</strong> cvModel = cv.fit(trainingData)</pre>
<p>By the way, Spark provides a way to save a trained ML model using the <kbd>save()</kbd> method:</p>
<pre>// Save the workflow<br/>cvModel.write.overwrite().save("model/LR_model")  </pre>
<p>Then the same model can be restored from the disk using the <kbd>load()</kbd> method:</p>
<pre><strong>val</strong> sameCVModel = LinearRegressionModel.load("model/LR_model")</pre>
<p class="mce-root">Then we compute the model's metrics on the test set similar to the LR and GLR models:</p>
<pre class="mce-root">println("Evaluating the cross validated model on the test set and calculating the regression metrics")<br/>val trainPredictionsAndLabelsCV = cvModel.transform(testData).select("label", "prediction")<br/>                                      .map { case Row(label: Double, prediction: Double)<br/>                                      =&gt; (label, prediction) }.rdd<br/><br/>val testRegressionMetricsCV = new RegressionMetrics(trainPredictionsAndLabelsCV)</pre>
<p class="mce-root">Finally, we gather the metrics and print to get some insights:</p>
<pre class="mce-root"><strong>val</strong> cvResults = "\n=====================================================================\n" +<br/>      s"TrainingData count: ${trainingData.count}\n" +<br/>      s"TestData count: ${testData.count}\n" +<br/>      "=====================================================================\n" +<br/>      s"TestData MSE = ${testRegressionMetricsCV.meanSquaredError}\n" +<br/>      s"TestData RMSE = ${testRegressionMetricsCV.rootMeanSquaredError}\n" +<br/>      s"TestData R-squared = ${testRegressionMetricsCV.r2}\n" +<br/>      s"TestData MAE = ${testRegressionMetricsCV.meanAbsoluteError}\n" +<br/>      s"TestData explained variance = ${testRegressionMetricsCV.explainedVariance}\n" +<br/>      "=====================================================================\n"<br/>println(cvResults)</pre>
<p>The preceding code segment should show something similar. Although, because of the randomness, you might experience slightly different output:</p>
<pre><strong> =====================================================================</strong><br/><strong> TrainingData count: 80</strong><br/><strong> TestData count: 55</strong><br/><strong> =====================================================================</strong><br/><strong> TestData MSE = 7.889401628365509</strong><br/><strong> TestData RMSE = 2.8088078660466453</strong><br/><strong> TestData R-squared = 0.3510269588724132</strong><br/><strong> TestData MAE = 2.2158433237623667</strong><br/><strong> TestData explained variance = 20.299135214455085</strong><br/><strong> =====================================================================</strong></pre>
<p>As we can see, both the RMSE and MAE are slightly lower than the non-cross validated LR model. Ideally, we should have experienced even lower values for these metrics. However, due to the small size of the training as well as test sets, probably both the LR and GLR models overfitted. Still, we will try to use robust regression analysis algorithms in <a href="6730e23e-eabb-4628-934a-7ac609049563.xhtml" target="_blank">Chapter 4</a>, <em>Scala for Tree-Based Ensemble Techniques</em>. More specifically, we will try to solve the same problem with decision trees, random forest, and GBTRs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how to develop a regression model for analyzing insurance severity claims using LR and GLR algorithms. We have also seen how to boost the performance of the GLR model using cross-validation and grid search techniques, which give the best combination of hyperparameters. Finally, we have seen some frequently asked questions so that the similar regression techniques can be applied for solving other real-life problems.</p>
<p class="mce-root">In the next chapter, we will see another supervised learning technique called classification through a real-life problem called analyzing outgoing customers through churn prediction. Several classification algorithms will be used for making the prediction in Scala. Churn prediction is essential for businesses as it helps you detect customers who are likely to cancel a subscription, product, or service, which also minimizes customer defection by predicting which customers are likely to cancel a subscription to a service.</p>


            </article>

            
        </section>
    </body></html>