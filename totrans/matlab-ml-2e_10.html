<html><head></head><body>
<div id="_idContainer101">
<h1 class="chapter-number" id="_idParaDest-198"><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.1.1">10</span></h1>
<h1 id="_idParaDest-199"><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.2.1">MATLAB Tools for Recommender Systems</span></h1>
<p><span class="koboSpan" id="kobo.3.1">A recommender system is a model that’s designed to anticipate the preferences of a specific user. </span><span class="koboSpan" id="kobo.3.2">When applied to the domain of movies, it transforms into a movie recommendation engine. </span><span class="koboSpan" id="kobo.3.3">The process involves filtering items in a database by predicting the user’s potential ratings and facilitating the connection of users with the most suitable content in the dataset. </span><span class="koboSpan" id="kobo.3.4">This holds significance because, in extensive catalogs, users might not discover all pertinent content. </span><span class="koboSpan" id="kobo.3.5">Effective recommendations enhance content consumption and major platforms such as Netflix heavily depend on them to maintain user engagement. </span><span class="koboSpan" id="kobo.3.6">In this chapter, we will learn the basic concepts of recommender systems and how to build a </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">network intrusion detection system</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">NIDS</span></strong><span class="koboSpan" id="kobo.7.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">using </span></span><span class="No-Break"><a id="_idIndexMarker1092"/></span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">MATLAB.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.12.1">Introducing the basic concepts of </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">recommender systems</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">Finding similar users </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">in data</span></span></li>
<li><span class="koboSpan" id="kobo.16.1">Creating recommender systems for network intrusion detection </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">using MATLAB</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Deploying machine </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">learning models</span></span></li>
</ul>
<h1 id="_idParaDest-200"><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.20.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.21.1">In this chapter, we will introduce basic machine learning concepts. </span><span class="koboSpan" id="kobo.21.2">To understand these topics, a basic knowledge of algebra and mathematical modeling is needed. </span><span class="koboSpan" id="kobo.21.3">You will also required working knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">of MATLAB.</span></span></p>
<p><span class="koboSpan" id="kobo.23.1">To work with the MATLAB code in this chapter, you’ll need the following files (available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">at </span></span><a href="https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition"><span class="No-Break"><span class="koboSpan" id="kobo.25.1">https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.26.1">):</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">CreditCardData.xlsx</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">CreditCardFraudDet.m</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">NDISdata.csv</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.30.1">NDISEnsemble.m</span></strong></span></li>
</ul>
<h1 id="_idParaDest-201"><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.31.1">Introducing the basic concepts of recommender systems</span></h1>
<p><span class="koboSpan" id="kobo.32.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.33.1">recommender system</span></strong><span class="koboSpan" id="kobo.34.1"> is a type</span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.35.1"> of information filtering system that’s designed to suggest items or content to users based on their preferences, historical behavior, or other relevant factors. </span><span class="koboSpan" id="kobo.35.2">These systems are widely used in various online platforms to help users discover products, services, content, and more. </span><span class="koboSpan" id="kobo.35.3">Recommender systems involve two primary entities: </span><strong class="bold"><span class="koboSpan" id="kobo.36.1">users</span></strong><span class="koboSpan" id="kobo.37.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.38.1">items</span></strong><span class="koboSpan" id="kobo.39.1">. </span><span class="koboSpan" id="kobo.39.2">Users are individuals for whom recommendations are generated, and items are the products, content, or services to be recommended. </span><span class="koboSpan" id="kobo.39.3">These items can include movies, books, products, news articles, </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">and more.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">Recommender systems rely on data that captures the interaction between users and items. </span><span class="koboSpan" id="kobo.41.2">This interaction data can include user ratings, purchase history, clicks, views, likes, and any other form of user engagement </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">with items.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">There are different types of </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">recommender systems:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.45.1">Collaborative filtering</span></strong><span class="koboSpan" id="kobo.46.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.47.1">CF</span></strong><span class="koboSpan" id="kobo.48.1">): CF methods</span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.49.1"> make recommendations</span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.50.1"> based on the preferences and behavior of </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">other users.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.52.1">Content-based filtering</span></strong><span class="koboSpan" id="kobo.53.1">: This </span><a id="_idIndexMarker1096"/><span class="koboSpan" id="kobo.54.1">approach recommends items to users based on the attributes of the items and the user’s historical preferences. </span><span class="koboSpan" id="kobo.54.2">It focuses on the content and descriptions </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">of items.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.56.1">Hybrid recommender systems</span></strong><span class="koboSpan" id="kobo.57.1">: These systems integrate various recommendation</span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.58.1"> techniques to furnish recommendations that are both more accurate </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">and diverse.</span></span></li>
</ul>
<h2 id="_idParaDest-202"><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.60.1">Understanding CF</span></h2>
<p><span class="koboSpan" id="kobo.61.1">CF is a popular </span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.62.1">technique that’s used in recommender systems to</span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.63.1"> make personalized recommendations to users based on their interactions and behaviors, as well as the behaviors of similar users. </span><span class="koboSpan" id="kobo.63.2">CF assumes that users who have interacted with items in a similar way in the past will have similar preferences in </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">the future.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">There are also two main types </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">of CF:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.67.1">User-based</span></strong><span class="koboSpan" id="kobo.68.1">: This type</span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.69.1"> of CF recommends items to a user based on the preferences and behaviors of users who are like them. </span><span class="koboSpan" id="kobo.69.2">Compute a similarity score between the target user and all other users in the system. </span><span class="koboSpan" id="kobo.69.3">Common similarity metrics include cosine similarity or Pearson correlation. </span><span class="koboSpan" id="kobo.69.4">Identify a set of neighbor users who are most like the target user. </span><span class="koboSpan" id="kobo.69.5">Recommend items that the target user’s neighbors have interacted with, but the target user </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">has not.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.71.1">Item-based</span></strong><span class="koboSpan" id="kobo.72.1">: This type of CF recommends items to a user based on the similarity of items they have interacted with in the past. </span><span class="koboSpan" id="kobo.72.2">It calculates the similarity between all pairs of items in the system based on user interactions. </span><span class="koboSpan" id="kobo.72.3">Common similarity metrics include cosine similarity and the Jaccard index. </span><span class="koboSpan" id="kobo.72.4">For a target user, it can identify the items they have interacted with. </span><span class="koboSpan" id="kobo.72.5">It can recommend items that are like the items the user has </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">interacted with.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.74.1">CF matrices are often sparse because users interact with only a small fraction of available items. </span><span class="koboSpan" id="kobo.74.2">Techniques such as matrix factorization can help address this issue by finding latent factors in the data. </span><span class="koboSpan" id="kobo.74.3">As the number of users and items grows, the computation of user-user or item-item similarity becomes more computationally expensive. </span><span class="koboSpan" id="kobo.74.4">Optimizations are required for scalability. </span><span class="koboSpan" id="kobo.74.5">CF can struggle to make recommendations for new users or items with little to no interaction history. </span><span class="koboSpan" id="kobo.74.6">Techniques such as content-based recommendations or hybrid models are often used to address </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">this problem.</span></span></p>
<p><span class="koboSpan" id="kobo.76.1">CF often relies on user behavior data, which raises privacy concerns. </span><span class="koboSpan" id="kobo.76.2">Privacy-preserving methods such as differential privacy may be employed to protect user information. </span><span class="koboSpan" id="kobo.76.3">Various </span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.77.1">metrics, including </span><strong class="bold"><span class="koboSpan" id="kobo.78.1">mean absolute error</span></strong><span class="koboSpan" id="kobo.79.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.80.1">MAE</span></strong><span class="koboSpan" id="kobo.81.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.82.1">root mean squared error</span></strong><span class="koboSpan" id="kobo.83.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.84.1">RMSE</span></strong><span class="koboSpan" id="kobo.85.1">), and ranking-based metrics such as precision and </span><a id="_idIndexMarker1102"/><span class="koboSpan" id="kobo.86.1">recall, are used to evaluate the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">CF algorithms.</span></span></p>
<p><span class="koboSpan" id="kobo.88.1">CF has been widely used in recommendation systems in various domains, including e-commerce, movie and music recommendations, social networks, and more. </span><span class="koboSpan" id="kobo.88.2">While it is effective at capturing user preferences, it does have limitations, such as the cold start problem and the need for a sufficient volume of user interactions. </span><span class="koboSpan" id="kobo.88.3">Hybrid approaches that combine CF with</span><a id="_idIndexMarker1103"/><span class="koboSpan" id="kobo.89.1"> other recommendation techniques can overcome some of </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">these limitations.</span></span></p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.91.1">Content-based filtering explained</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.92.1">Content-based filtering</span></strong><span class="koboSpan" id="kobo.93.1"> is a recommendation</span><a id="_idIndexMarker1104"/><span class="koboSpan" id="kobo.94.1"> technique that’s </span><a id="_idIndexMarker1105"/><span class="koboSpan" id="kobo.95.1">used in recommender systems to provide personalized recommendations to users based on the characteristics and features of the items and the user’s preferences. </span><span class="koboSpan" id="kobo.95.2">Unlike CF, which relies on user-item interactions, content-based filtering focuses on the content of items and attempts to match them with </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">user profiles.</span></span></p>
<p><span class="koboSpan" id="kobo.97.1">Each item in the system is described by a set of features or attributes. </span><span class="koboSpan" id="kobo.97.2">These features can vary widely based on the domain but may include things such as genres, keywords, actors, directors (for movies), authors (for books), and more. </span><span class="koboSpan" id="kobo.97.3">For text-based content, natural language processing techniques may be used to extract keywords or topics. </span><span class="koboSpan" id="kobo.97.4">The system maintains a user profile or preference vector for each user, which reflects their preferences for different features or attributes. </span><span class="koboSpan" id="kobo.97.5">This user profile is built based on the items the user has interacted with or </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">explicitly rated.</span></span></p>
<p><span class="koboSpan" id="kobo.99.1">To generate recommendations for a user, the system calculates a similarity score between the user’s profile and the features of items. </span><span class="koboSpan" id="kobo.99.2">The similarity score is often computed using techniques such as cosine similarity, </span><strong class="bold"><span class="koboSpan" id="kobo.100.1">Term Frequency-Inverse Document Frequency</span></strong><span class="koboSpan" id="kobo.101.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.102.1">TF-IDF</span></strong><span class="koboSpan" id="kobo.103.1">), or other distance metrics. </span><span class="koboSpan" id="kobo.103.2">Items with the highest similarity scores</span><a id="_idIndexMarker1106"/><span class="koboSpan" id="kobo.104.1"> are recommended to the user. </span><span class="koboSpan" id="kobo.104.2">These are the items that are most in line with the user’s historical preferences </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">and interests.</span></span></p>
<p><span class="koboSpan" id="kobo.106.1">Content-based filtering is effective at providing personalized recommendations because it considers the individual user’s preferences based on item features. </span><span class="koboSpan" id="kobo.106.2">It doesn’t rely on user-to-user or item-to-item comparisons, making it suitable for new or less active users who might not have much interaction history. </span><span class="koboSpan" id="kobo.106.3">The quality of recommendations heavily depends on the accuracy and richness of item descriptions and features. </span><span class="koboSpan" id="kobo.106.4">Note that ensuring high-quality metadata </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">is crucial.</span></span></p>
<p><span class="koboSpan" id="kobo.108.1">While content-based filtering is good at recommending items such as those a user has interacted with before, it may not introduce as much serendipity or novelty in recommendations compared to CF. </span><span class="koboSpan" id="kobo.108.2">It can also help address the cold start problem for new items if there is sufficient information available about their features. </span><span class="koboSpan" id="kobo.108.3">Many recommender systems combine content-based filtering with CF or other recommendation techniques to improve recommendation quality and address the limitations of </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">each approach.</span></span></p>
<p><span class="koboSpan" id="kobo.110.1">Content-based filtering is commonly used in various domains such as news recommendation, music</span><a id="_idIndexMarker1107"/><span class="koboSpan" id="kobo.111.1"> recommendation, and</span><a id="_idIndexMarker1108"/><span class="koboSpan" id="kobo.112.1"> e-commerce for suggesting products. </span><span class="koboSpan" id="kobo.112.2">When implemented correctly, it can offer valuable recommendations tailored to individual user preferences </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">and needs.</span></span></p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.114.1">Hybrid recommender systems</span></h2>
<p><span class="koboSpan" id="kobo.115.1">Hybrid recommender</span><a id="_idIndexMarker1109"/><span class="koboSpan" id="kobo.116.1"> systems are recommendation</span><a id="_idIndexMarker1110"/><span class="koboSpan" id="kobo.117.1"> systems that combine multiple recommendation techniques to provide more accurate and diverse recommendations. </span><span class="koboSpan" id="kobo.117.2">These systems aim to leverage the strengths of different recommendation approaches, such as CF, content-based filtering, and more, while mitigating their weaknesses. </span><span class="koboSpan" id="kobo.117.3">Hybrid recommender systems are commonly used in various applications to improve recommendation quality and address the limitations of </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">individual methods.</span></span></p>
<p><span class="koboSpan" id="kobo.119.1">In weighted hybrid systems, different recommendation techniques are assigned weights that determine their influence on the final recommendations. </span><span class="koboSpan" id="kobo.119.2">For example, you might assign a higher weight to CF if historical user interactions are more critical, and a lower weight to content-based filtering for item features. </span><span class="koboSpan" id="kobo.119.3">Recommendations are generated by combining the scores from each technique, weighted by </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">their importance.</span></span></p>
<p><span class="koboSpan" id="kobo.121.1">In switching hybrid systems, the recommendation approach is dynamically chosen based on certain conditions or user characteristics. </span><span class="koboSpan" id="kobo.121.2">For instance, if a user has a substantial interaction history, CF might be used, but if they are a new user with minimal history, content-based filtering might be employed. </span><span class="koboSpan" id="kobo.121.3">In this approach, the features or scores generated by different recommendation techniques are combined to create a unified feature vector for items or users. </span><span class="koboSpan" id="kobo.121.4">This combined feature vector is then used to generate recommendations using any single </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">recommendation method.</span></span></p>
<p><span class="koboSpan" id="kobo.123.1">Cascade hybrids use the output of one recommendation technique as input to another. </span><span class="koboSpan" id="kobo.123.2">For example, content-based filtering may be used to generate an initial set of recommendations. </span><span class="koboSpan" id="kobo.123.3">These recommendations can be further refined using CF to </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">improve accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.125.1">In meta-level hybrid systems, different recommendation methods are applied independently, and their outputs are combined using a meta-learner or meta-classifier. </span><span class="koboSpan" id="kobo.125.2">The meta-learner takes the outputs of individual recommendation methods as inputs and provides the final recommendations. </span><span class="koboSpan" id="kobo.125.3">Machine learning algorithms such as decision trees, neural networks, or ensemble methods such as stacking can be used </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">as meta-learners.</span></span></p>
<p><span class="koboSpan" id="kobo.127.1">In the realm of data analysis and user profiling, the process of finding similar users holds immense significance. </span><span class="koboSpan" id="kobo.127.2">Let’s</span><a id="_idIndexMarker1111"/><span class="koboSpan" id="kobo.128.1"> delve into the techniques and </span><a id="_idIndexMarker1112"/><span class="koboSpan" id="kobo.129.1">methodologies that are employed for identifying and categorizing users with similar patterns and behavio</span><a id="_idTextAnchor209"/><span class="koboSpan" id="kobo.130.1">rs within </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">a dataset.</span></span></p>
<h1 id="_idParaDest-205"><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.132.1">Finding similar users in data</span></h1>
<p><span class="koboSpan" id="kobo.133.1">Fraud has consistently </span><a id="_idIndexMarker1113"/><span class="koboSpan" id="kobo.134.1">been a pervasive issue in various forms, but the emergence of new technological tools, such</span><a id="_idIndexMarker1114"/><span class="koboSpan" id="kobo.135.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.136.1">virtual intelligence</span></strong><span class="koboSpan" id="kobo.137.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.138.1">VI</span></strong><span class="koboSpan" id="kobo.139.1">), has expanded the avenues for fraudulent activities. </span><span class="koboSpan" id="kobo.139.2">In today’s world, the use of credit and debit cards has become the standard for making purchases, and as a result, fraud associated with these payment methods is on the rise. </span><span class="koboSpan" id="kobo.139.3">The repercussions of such fraud extend beyond impacting just merchants and banks, who are often left shouldering the </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">financial burden.</span></span></p>
<p><span class="koboSpan" id="kobo.141.1">When a customer falls victim to fraud, they may find themselves burdened with higher interest rates imposed by the bank as they could be categorized as a higher risk profile. </span><span class="koboSpan" id="kobo.141.2">Additionally, fraud incidents can tarnish a merchant’s reputation and image. </span><span class="koboSpan" id="kobo.141.3">If a customer experiences fraud during a transaction, it can erode their trust in the seller, potentially driving them to seek alternatives from competitors for </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">future purchases.</span></span></p>
<p><span class="koboSpan" id="kobo.143.1">Given a set of credit card transaction data, fraud recognition is the process of identifying whether a new transaction belongs to the class of fraudulent or legitimate transactions. </span><span class="koboSpan" id="kobo.143.2">Such a system should not only detect fraudulent transactions but should do so in a </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">cost-effective manner.</span></span></p>
<p><span class="koboSpan" id="kobo.145.1">Examining a transaction with the goal of classification can be carried out through distinct methods, and two specific approaches </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">are suggested:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.147.1">User-level analysis</span></span></li>
<li><span class="koboSpan" id="kobo.148.1">Single </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">transaction analysis</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.150.1">These two approaches represent unique strategies, each having its own set of advantages and limitations. </span><span class="koboSpan" id="kobo.150.2">Each approach entails certain assumptions, varying in their degree of validity. </span><span class="koboSpan" id="kobo.150.3">Regardless, both approaches can yield valuable outcomes. </span><span class="koboSpan" id="kobo.150.4">There are situations where the choice between these methods is determined by the nature of the available data as it may not permit an </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">alternative approach.</span></span></p>
<p><span class="koboSpan" id="kobo.152.1">Analyzing transactions at the individual transaction level refers to an approach where the classification of a transaction is determined by its relationship with all the transactions within the dataset. </span><span class="koboSpan" id="kobo.152.2">Various machine learning algorithms can be employed for this purpose. </span><span class="koboSpan" id="kobo.152.3">For instance, consider </span><a id="_idIndexMarker1115"/><span class="koboSpan" id="kobo.153.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.154.1">k-nearest neighbors</span></strong><span class="koboSpan" id="kobo.155.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.156.1">k-NN</span></strong><span class="koboSpan" id="kobo.157.1">) algorithm, which classifies a transaction based on its similarity to other transactions in the dataset. </span><span class="koboSpan" id="kobo.157.2">If a transaction closely resembles known fraudulent transactions, there’s a likelihood it will be categorized as fraudulent, and conversely, it would be deemed a legitimate transaction. </span><span class="koboSpan" id="kobo.157.3">This mechanism facilitates the identification of data patterns by leveraging similarities among objects of the </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">same class.</span></span></p>
<p><span class="koboSpan" id="kobo.159.1">In a nearest neighbors </span><a id="_idIndexMarker1116"/><span class="koboSpan" id="kobo.160.1">algorithm, a record is categorized by examining all the data in the training set and assigning it the same class as the nearest element. </span><span class="koboSpan" id="kobo.160.2">The underlying assumption is that in a multidimensional space, if two records are “close,” they likely belong to the same class. </span><span class="koboSpan" id="kobo.160.3">To gauge this proximity, it’s important to employ a distance metric. </span><span class="koboSpan" id="kobo.160.4">One example is the Euclidean distance or, more broadly, the Minkowski metric, as we introduced in </span><a href="B21156_04.xhtml#_idTextAnchor084"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.161.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.162.1">, </span><em class="italic"><span class="koboSpan" id="kobo.163.1">Clustering Analysis and </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.164.1">Dimensionality Reduction</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.166.1">The nearest neighbor rule results in a space division</span><a id="_idIndexMarker1117"/><span class="koboSpan" id="kobo.167.1"> known as </span><strong class="bold"><span class="koboSpan" id="kobo.168.1">Voronoi tessellation</span></strong><span class="koboSpan" id="kobo.169.1">. </span><span class="koboSpan" id="kobo.169.2">Each element in the training set delineates a region in which patterns will be classified into the same class. </span><span class="koboSpan" id="kobo.169.3">To enhance the robustness of this mechanism, one approach is to classify a record by considering the k closest records. </span><span class="koboSpan" id="kobo.169.4">The record can then be assigned to the class that has the largest representation among the selected examples. </span><span class="koboSpan" id="kobo.169.5">To reduce sensitivity to the choice of k, each record can contribute to the classification based on a weighted scheme determined by its distance from the element to be classified. </span><span class="koboSpan" id="kobo.169.6">For this process to work effectively, the attributes must possess consistent value scales, which typically necessitates prior normalization during the data preprocessing phase. </span><span class="koboSpan" id="kobo.169.7">Let’s learn how to build a credit card fraud </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">detection system:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.171.1">As usual, we will start by importing the data into the MATLAB workspace. </span><span class="koboSpan" id="kobo.171.2">For </span><a id="_idTextAnchor211"/><span class="koboSpan" id="kobo.172.1">this, we will utilize the credit card fraud detection dataset, which comprises anonymized credit card transactions classified as fraudulent </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">or legitimate:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.174.1">
CreditCardData = readtable('CreditCardData.xlsx');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.175.1">The dataset exclusively consists of numerical input variables, derived from a PCA transformation. </span><span class="koboSpan" id="kobo.175.2">Among the features, </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">V1</span></strong><span class="koboSpan" id="kobo.177.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">V28</span></strong><span class="koboSpan" id="kobo.179.1"> represent the principal components obtained through PCA. </span><span class="koboSpan" id="kobo.179.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.180.1">Class</span></strong><span class="koboSpan" id="kobo.181.1"> feature serves as the response variable and holds a value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.182.1">1</span></strong><span class="koboSpan" id="kobo.183.1"> in cases of fraud and </span><strong class="source-inline"><span class="koboSpan" id="kobo.184.1">0</span></strong><span class="koboSpan" id="kobo.185.1"> otherwise. </span><span class="koboSpan" id="kobo.185.2">To understand how the data is </span><a id="_idIndexMarker1118"/><span class="koboSpan" id="kobo.186.1">distributed, we can count the occurrences in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">class</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.188.1"> column:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.189.1">NumOcc = groupcounts(CreditCardData.Class)
NumOcc =
        3992
         492</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.190.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.191.1">groupcounts()</span></strong><span class="koboSpan" id="kobo.192.1"> function provides the distinct combinations of grouping variables for the table or timetable denoted as </span><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">T</span></strong><span class="koboSpan" id="kobo.194.1">. </span><span class="koboSpan" id="kobo.194.2">It also offers the count of members within each group and the corresponding data percentage, ranging from </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">0</span></strong><span class="koboSpan" id="kobo.196.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">100</span></strong><span class="koboSpan" id="kobo.198.1">. </span><span class="koboSpan" id="kobo.198.2">Groups are established based on rows in the variables contained within </span><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">groupvars</span></strong><span class="koboSpan" id="kobo.200.1"> that share identical unique value combinations. </span><span class="koboSpan" id="kobo.200.2">Each entry in the result table corresponds to an </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">individual group.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.202.1">The dataset is unbalanced on one of the binary classes, we should take this into account when evaluating the results. </span><span class="koboSpan" id="kobo.202.2">Classifying imbalanced data is a common challenge in machine learning when one class significantly outnumbers the other(s). </span><span class="koboSpan" id="kobo.202.3">The primary class (majority class) often dominates, making it challenging for the model to correctly predict the </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">minority class.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.204.1">To check how the data is distributed, we can draw a boxplot of </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">the features:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.206.1">boxchart(CreditCardData{:,1:28})
xlabel('features')
ylabel('values')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.207.1">The following graph will be output (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.208.1">Figure 10</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.209.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">):</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer099">
<span class="koboSpan" id="kobo.211.1"><img alt="Figure 10.1 – Boxplot of the features" src="image/B21156_10_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.212.1">Figure 10.1 – Boxplot of the features</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.213.1">From the analysis of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.214.1">Figure 10</span></em></span><em class="italic"><span class="koboSpan" id="kobo.215.1">.1</span></em><span class="koboSpan" id="kobo.216.1">, we can see that the variables are skewed. </span><span class="koboSpan" id="kobo.216.2">We can also see that </span><a id="_idIndexMarker1119"/><span class="koboSpan" id="kobo.217.1">some of them present potential outliers. </span><span class="koboSpan" id="kobo.217.2">It is recommended to conduct data scaling. </span><span class="koboSpan" id="kobo.217.3">Keep in mind that it’s a best practice to standardize or normalize the data before training a machine learning model. </span><span class="koboSpan" id="kobo.217.4">Through scaling, the data’s units are standardized, making it simpler to compare data from various sources </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">or locations.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.219.1">Data scaling, also known as data normalization or standardization, is a preprocessing technique in machine learning and data analysis that involves transforming the data into a standardized range or distribution. </span><span class="koboSpan" id="kobo.219.2">The primary purpose of data scaling is to make different features or variables in your dataset comparable and to help machine learning algorithms </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">perform better.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.221.1">The choice between scaling and standardization depends on the specific requirements of your dataset and the machine learning algorithm you plan to use. </span><span class="koboSpan" id="kobo.221.2">In general, it is advisable to apply data scaling to your features to prevent certain variables from dominating the learning process, especially in algorithms that are sensitive to feature scales, such as kNN or support </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">vector machines.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.223.1">Data scaling helps in improving the convergence of optimization algorithms, makes features more interpretable, and can also enhance the performance of some machine learning models. </span><span class="koboSpan" id="kobo.223.2">However, it’s important to note that not all algorithms require data scaling, and there are cases where the natural scale of the data is meaningful and should not be altered. </span><span class="koboSpan" id="kobo.223.3">The decision to scale the data should be made with careful consideration of your specific problem and the characteristics of </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">your dataset.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.225.1">We will perform a standardization with a range of </span><strong class="source-inline"><span class="koboSpan" id="kobo.226.1">-1</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.227.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">1</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.230.1">
DataScaled = rescale(CreditCardData{:,1:28},-1,1);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.231.1">Standardization </span><a id="_idIndexMarker1120"/><span class="koboSpan" id="kobo.232.1">with a range of </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">-1</span></strong><span class="koboSpan" id="kobo.234.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">1</span></strong><span class="koboSpan" id="kobo.236.1">, also known as min-max scaling with a specific range, is a method of data scaling that transforms your data to fit within the range of </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">-1</span></strong><span class="koboSpan" id="kobo.238.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.239.1">1</span></strong><span class="koboSpan" id="kobo.240.1">. </span><span class="koboSpan" id="kobo.240.2">This approach is useful when you want to normalize your data while maintaining the possibility of </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">negative values.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.242.1">To check how the data is distributed after we performed data scaling, we can draw a boxplot of </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">the features:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.244.1">boxchart(DataScaled)
xlabel('features')
ylabel('values')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.245.1">The following graph is shown (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.246.1">Figure 10</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.247.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">):</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer100">
<span class="koboSpan" id="kobo.249.1"><img alt="Figure 10.2 – Boxplot of the scaled data" src="image/B21156_10_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.250.1">Figure 10.2 – Boxplot of the scaled data</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.251.1">Now, it is clear that the data has been scaled so that we have the same </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">existence intervals.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.253.1">Before training the model based on machine learning, it is necessary to carry out data splitting. </span><span class="koboSpan" id="kobo.253.2">Data splitting refers to the process of dividing a dataset into separate subsets for training, testing, and validation purposes. </span><span class="koboSpan" id="kobo.253.3">We will perform train-test splitting, which involves dividing the dataset into two parts, typically with a 70-30 or 80-20 split. </span><span class="koboSpan" id="kobo.253.4">The larger portion is used for training the model, while the smaller</span><a id="_idIndexMarker1121"/><span class="koboSpan" id="kobo.254.1"> portion is used for testing </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">its performance:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.256.1">
n = length(CreditCardData.Class);
SplitData = cvpartition(n,'Holdout',0.3);
TrainIndex = training(SplitData);
TrainData = DataScaled(TrainIndex,:);
TestIndex = test(SplitData);
TestData = DataScaled(TestIndex,:);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.257.1">We initiated the process by obtaining the number of observations within our dataset using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">length()</span></strong><span class="koboSpan" id="kobo.259.1"> function. </span><span class="koboSpan" id="kobo.259.2">This function returns the size of the largest dimension in the array, </span><strong class="source-inline"><span class="koboSpan" id="kobo.260.1">X</span></strong><span class="koboSpan" id="kobo.261.1">. </span><span class="koboSpan" id="kobo.261.2">In the context of vectors, this size corresponds to the total number of elements. </span><span class="koboSpan" id="kobo.261.3">Subsequently, we employed the </span><strong class="source-inline"><span class="koboSpan" id="kobo.262.1">cvpartition()</span></strong><span class="koboSpan" id="kobo.263.1"> function to create a random partition for the dataset. </span><span class="koboSpan" id="kobo.263.2">This partition serves as the foundation for constructing essential training and test subsets, which are instrumental in evaluating a statistical model. </span><span class="koboSpan" id="kobo.263.3">To extract the training data index and the test data index from the original dataset, we utilized the </span><strong class="source-inline"><span class="koboSpan" id="kobo.264.1">training</span></strong><span class="koboSpan" id="kobo.265.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.266.1">test</span></strong><span class="koboSpan" id="kobo.267.1"> object functions. </span><span class="koboSpan" id="kobo.267.2">These indices were then applied to extract the corresponding </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">data subsets.</span></span></p></li> <li><span class="koboSpan" id="kobo.269.1">k-NN is a versatile algorithm that’s used not only for classification and regression tasks but also in recommender systems. </span><span class="koboSpan" id="kobo.269.2">It can be adapted for building CF-based</span><a id="_idIndexMarker1122"/><span class="koboSpan" id="kobo.270.1"> recommender systems. </span><span class="koboSpan" id="kobo.270.2">We will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">fitcknn()</span></strong><span class="koboSpan" id="kobo.272.1"> function, </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.274.1">
WeightedKNNModel = fitcknn(...
</span><span class="koboSpan" id="kobo.274.2">    TrainData(:,1:28), ...
</span><span class="koboSpan" id="kobo.274.3">    TrainData(:,29), ...
</span><span class="koboSpan" id="kobo.274.4">    'Distance', 'cosine', ...
</span><span class="koboSpan" id="kobo.274.5">    'Exponent', [], ...
</span><span class="koboSpan" id="kobo.274.6">    'NumNeighbors', 10, ...
</span><span class="koboSpan" id="kobo.274.7">    'DistanceWeight', 'SquaredInverse');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.275.1">The following arguments </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">were passed:</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.277.1">Sample </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.278.1">data</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">TrainData(:,1:28)</span></strong></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.281.1">Response </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.282.1">variable</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.284.1">TrainData(:,29)</span></strong></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.285.1">Distance metric</span></strong><span class="koboSpan" id="kobo.286.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">cosine</span></strong><span class="koboSpan" id="kobo.288.1"> (the expression represents the complement of the cosine of the angle included between observations, treating them </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">as vectors):</span></span><ul><li><span class="koboSpan" id="kobo.290.1">Minkowski </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">distance exponent</span></span></li><li><span class="koboSpan" id="kobo.292.1">Number of nearest neighbors to </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">find: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.294.1">10</span></strong></span></li><li><span class="koboSpan" id="kobo.295.1">Distance weighting function: </span><strong class="source-inline"><span class="koboSpan" id="kobo.296.1">SquaredInverse</span></strong><span class="koboSpan" id="kobo.297.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.298.1">Weight</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.299.1">is 1/distance)</span></span></li></ul></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.300.1">We applied weighted k-NN, a variation of the k-NN algorithm that assigns different weights to the neighbors when making predictions or classifications. </span><span class="koboSpan" id="kobo.300.2">In traditional k-NN, each neighbor has an equal influence on the final decision, but in weighted k-NN, the influence of each neighbor is adjusted based on certain factors, typically the proximity or similarity between the neighbors and the query point. </span><span class="koboSpan" id="kobo.300.3">This allows for more accurate and </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">context-aware predictions.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.302.1">We start by choosing a distance metric to measure the similarity or dissimilarity between data points. </span><span class="koboSpan" id="kobo.302.2">We used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">cosine</span></strong><span class="koboSpan" id="kobo.304.1"> metric, also known as cosine similarity, which is a similarity measure that’s used to determine the similarity between two non-zero vectors in a high-dimensional space. </span><span class="koboSpan" id="kobo.304.2">It is widely used in various fields, including information retrieval, natural language processing, and recommendation </span><a id="_idIndexMarker1123"/><span class="koboSpan" id="kobo.305.1">systems. </span><span class="koboSpan" id="kobo.305.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">cosine</span></strong><span class="koboSpan" id="kobo.307.1"> metric measures the cosine of the angle between two vectors, and it provides a value between -1 and 1, where 1 indicates that the vectors are identical, 0 means that they are orthogonal (completely dissimilar), and -1 implies they </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">are opposed.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.309.1">The algorithm then determines the value of </span><em class="italic"><span class="koboSpan" id="kobo.310.1">k</span></em><span class="koboSpan" id="kobo.311.1">, which represents the number of nearest neighbors to consider when making a prediction. </span><span class="koboSpan" id="kobo.311.2">The choice of k depends on your dataset and the specific problem you’re trying to solve. </span><span class="koboSpan" id="kobo.311.3">You must compute the distances between the query point (the point you want to classify or predict) and all other data points in your dataset, then calculate weights for each neighbor based on their proximity or similarity to the query point. </span><span class="koboSpan" id="kobo.311.4">Common methods for assigning weights include inverse distance, where closer neighbors have higher weights, or similarity-based weighting, where more similar neighbors are given higher weights. </span><span class="koboSpan" id="kobo.311.5">You can also choose the k neighbors with the highest weights. </span><span class="koboSpan" id="kobo.311.6">These neighbors will have a more significant impact on the final prediction. </span><span class="koboSpan" id="kobo.311.7">For classification tasks, assign class labels to the query point based on the majority class among the selected neighbors, with the weights influencing the </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">voting process.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.313.1">Here are the advantages of using </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">weighted k-NN:</span></span></p><ul><li><span class="koboSpan" id="kobo.315.1">Weighted k-NN can provide more accurate predictions because it considers the influence of each neighbor on the </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">final decision</span></span></li><li><span class="koboSpan" id="kobo.317.1">It allows for better handling of imbalanced datasets, where some neighbors may be more informative </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">than others</span></span></li><li><span class="koboSpan" id="kobo.319.1">Weighted k-NN is particularly useful when the neighbors are not </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">equally informative</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.321.1">Weighted k-NN is a flexible and widely used machine learning technique that can be applied to a variety of problems, including classification, regression, and recommendation systems. </span><span class="koboSpan" id="kobo.321.2">It allows you to adapt the algorithm to the specific characteristics of your data and the problem you are trying </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">to solve.</span></span></p></li> <li><span class="koboSpan" id="kobo.323.1">After training the model, we must evaluate its performance. </span><span class="koboSpan" id="kobo.323.2">We will start by using the trained model to predict </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">data labels:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.325.1">
PredData = predict(WeightedKNNModel,TestData(:,1:28));</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.326.1">Here, we </span><a id="_idIndexMarker1124"/><span class="koboSpan" id="kobo.327.1">employed the </span><strong class="source-inline"><span class="koboSpan" id="kobo.328.1">predict()</span></strong><span class="koboSpan" id="kobo.329.1"> function, which furnishes the anticipated response values produced by the generalized linear </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">regression model.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.331.1">Now, we can assess the model’s accuracy, which gauges the extent to which a predictive model’s forecasts match the real or observed values. </span><span class="koboSpan" id="kobo.331.2">It serves as an indicator of the model’s performance in accurately </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">predicting outcomes:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.333.1">accuracy = sum(PredData == TestData(:,29)) / length(TestData(:,29));
fprintf('Accuracy: %.2f%%\n', accuracy * 100);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.334.1">The following result </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">is printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.336.1">Accuracy: 97.03%</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.337.1">The result is excellent, confirming that the choice of the algorithm and training parameters </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">were correct.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.339.1">In the ever-evolving landscape of cybersecurity, the development of effective tools for network intrusion detection is paramount. </span><span class="koboSpan" id="kobo.339.2">The next section will explore how to utilize MATLAB to create </span><a id="_idIndexMarker1125"/><span class="koboSpan" id="kobo.340.1">advanced recommender systems tailored for enhancing network intrusion </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">detection capabilities.</span></span></p>
<h1 id="_idParaDest-206"><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.342.1">Creating recommender systems for network intrusion detection using MATLAB</span></h1>
<p><span class="koboSpan" id="kobo.343.1">A NIDS serves</span><a id="_idIndexMarker1126"/><span class="koboSpan" id="kobo.344.1"> as a</span><a id="_idIndexMarker1127"/><span class="koboSpan" id="kobo.345.1"> security</span><a id="_idIndexMarker1128"/><span class="koboSpan" id="kobo.346.1"> mechanism that’s employed to identify and prevent unauthorized access, malicious activities, and potential threats within a computer network. </span><span class="koboSpan" id="kobo.346.2">It involves monitoring network traffic and analyzing it to identify any suspicious or abnormal behaviors. </span><span class="koboSpan" id="kobo.346.3">The main objective of network intrusion detection is to protect the network from various types of attacks, such as </span><strong class="bold"><span class="koboSpan" id="kobo.347.1">denial-of-service</span></strong><span class="koboSpan" id="kobo.348.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.349.1">DoS</span></strong><span class="koboSpan" id="kobo.350.1">) attacks, malware</span><a id="_idIndexMarker1129"/><span class="koboSpan" id="kobo.351.1"> infections, data leakage, unauthorized access, and other </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">cyber threats.</span></span></p>
<p><span class="koboSpan" id="kobo.353.1">There are two primary methods of network </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">intrusion detection:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.355.1">Signature-based detection</span></strong><span class="koboSpan" id="kobo.356.1">: This </span><a id="_idIndexMarker1130"/><span class="koboSpan" id="kobo.357.1">method involves comparing network traffic patterns with a database of known signatures or patterns of known attacks. </span><span class="koboSpan" id="kobo.357.2">If a match is found, an alert is generated to notify the </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">network administrator.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.359.1">Anomaly-based detection</span></strong><span class="koboSpan" id="kobo.360.1">: This method focuses on identifying abnormal or suspicious network behavior that deviates from the normal patterns. </span><span class="koboSpan" id="kobo.360.2">It uses machine learning algorithms to analyze network traffic and detect any anomalies that could potentially indicate an ongoing or </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">imminent attack.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.362.1">A NIDS can be implemented at different levels within a network architecture, such as at the perimeter, on individual hosts, or within specific network segments. </span><span class="koboSpan" id="kobo.362.2">They collect and analyze network packets, logs, and other network data to identify and alert the system administrators about </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">potential intrusions.</span></span></p>
<p><span class="koboSpan" id="kobo.364.1">The following are some common techniques that are used in network </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">intrusion detection:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.366.1">Packet analysis</span></strong><span class="koboSpan" id="kobo.367.1">: For examining</span><a id="_idIndexMarker1131"/><span class="koboSpan" id="kobo.368.1"> individual network packets to identify specific attack signatures </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">or anomalies</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.370.1">Protocol analysis</span></strong><span class="koboSpan" id="kobo.371.1">: For analyzing network protocols to detect any abnormal or </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">unauthorized activities</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.373.1">Traffic analysis</span></strong><span class="koboSpan" id="kobo.374.1">: For monitoring network traffic patterns to identify any sudden spikes or </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">unusual patterns</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.376.1">Behavior analysis</span></strong><span class="koboSpan" id="kobo.377.1">: For analyzing user or host behavior to detect any unusual or </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">malicious activities</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.379.1">NIDS play a crucial role in safeguarding computer networks against unauthorized access and potential threats. </span><span class="koboSpan" id="kobo.379.2">They </span><a id="_idIndexMarker1132"/><span class="koboSpan" id="kobo.380.1">help </span><a id="_idIndexMarker1133"/><span class="koboSpan" id="kobo.381.1">identify and </span><a id="_idIndexMarker1134"/><span class="koboSpan" id="kobo.382.1">respond to security incidents promptly, minimizing any potential damage or </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">data breaches.</span></span></p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.384.1">Recommender system for NIDS</span></h2>
<p><span class="koboSpan" id="kobo.385.1">In this example, we</span><a id="_idIndexMarker1135"/><span class="koboSpan" id="kobo.386.1"> will adopt a new approach to identifying and </span><a id="_idIndexMarker1136"/><span class="koboSpan" id="kobo.387.1">preventing network attacks: network intrusion detection using a recommender system. </span><span class="koboSpan" id="kobo.387.2">Traditional NIDS rely on fixed rules and signatures to detect known attack patterns. </span><span class="koboSpan" id="kobo.387.3">However, attackers are continually evolving their techniques, making it challenging for these systems to keep up. </span><span class="koboSpan" id="kobo.387.4">By incorporating a recommender system into the network intrusion detection process, it becomes possible to leverage machine learning and data mining techniques to enhance detection capabilities. </span><span class="koboSpan" id="kobo.387.5">The primary objective is to utilize historical network traffic data to build a model that can predict whether a particular network event is normal </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">or malicious.</span></span></p>
<p><span class="koboSpan" id="kobo.389.1">Here is an overview of how network intrusion detection using a recommender </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">system works:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.391.1">Data collection</span></strong><span class="koboSpan" id="kobo.392.1">: Network traffic data is collected and stored for analysis. </span><span class="koboSpan" id="kobo.392.2">This data consists of various network parameters, such as IP addresses, port numbers, protocols, packet sizes, </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">and more.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.394.1">Feature extraction</span></strong><span class="koboSpan" id="kobo.395.1">: Relevant features are extracted from the collected data. </span><span class="koboSpan" id="kobo.395.2">These features can include traffic volume, connection duration, packet headers, and other characteristics that provide insights into </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">network behavior.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.397.1">Data preprocessing</span></strong><span class="koboSpan" id="kobo.398.1">: The gathered data undergoes preprocessing to eliminate noise, address missing values, and normalize the features. </span><span class="koboSpan" id="kobo.398.2">This step ensures that the data is in a suitable format </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">for analysis.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.400.1">Training the recommender system</span></strong><span class="koboSpan" id="kobo.401.1">: The preprocessed data is used to train a recommender system algorithm, such as CF, matrix factorization, or association rule mining. </span><span class="koboSpan" id="kobo.401.2">This algorithm learns the patterns and relationships within </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">the data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.403.1">Building a recommendation model</span></strong><span class="koboSpan" id="kobo.404.1">: The trained algorithm generates a recommendation model based on the network traffic data. </span><span class="koboSpan" id="kobo.404.2">This model can identify the normal network behavior and detect any deviations </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">from it.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.406.1">Real-time monitoring</span></strong><span class="koboSpan" id="kobo.407.1">: The recommendation model is then used to monitor incoming network traffic in real time. </span><span class="koboSpan" id="kobo.407.2">It analyzes the network events and predicts whether they are </span><a id="_idIndexMarker1137"/><span class="koboSpan" id="kobo.408.1">normal </span><a id="_idIndexMarker1138"/><span class="koboSpan" id="kobo.409.1">or </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">potentially malicious.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.411.1">Alert generation</span></strong><span class="koboSpan" id="kobo.412.1">: When the recommendation model detects a potentially malicious network event, it triggers an alert to notify network administrators. </span><span class="koboSpan" id="kobo.412.2">The alert can include information about the detected attack type and </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">recommended countermeasures.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.414.1">Continuous learning</span></strong><span class="koboSpan" id="kobo.415.1">: The recommendation model can continuously update itself over time by incorporating new data and adjusting its detection capabilities. </span><span class="koboSpan" id="kobo.415.2">This ensures that the system remains effective against </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">emerging threats.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.417.1">Overall, network intrusion detection using a recommender system offers a more dynamic and adaptive approach to identifying network attacks. </span><span class="koboSpan" id="kobo.417.2">It leverages machine learning techniques to learn from historical data and make intelligent predictions about the network’s security status. </span><span class="koboSpan" id="kobo.417.3">This</span><a id="_idIndexMarker1139"/><span class="koboSpan" id="kobo.418.1"> can enhance the accuracy and efficiency </span><a id="_idIndexMarker1140"/><span class="koboSpan" id="kobo.419.1">of network security operations by reducing false positives and detecting emerging </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">attack patterns.</span></span></p>
<h2 id="_idParaDest-208"><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.421.1">NIDS using a recommender system in MATLAB</span></h2>
<p><span class="koboSpan" id="kobo.422.1">In this practical </span><a id="_idIndexMarker1141"/><span class="koboSpan" id="kobo.423.1">example, we will build</span><a id="_idIndexMarker1142"/><span class="koboSpan" id="kobo.424.1"> a NIDS-adopting </span><a id="_idIndexMarker1143"/><span class="koboSpan" id="kobo.425.1">recommender system using </span><strong class="bold"><span class="koboSpan" id="kobo.426.1">ensemble methods</span></strong><span class="koboSpan" id="kobo.427.1">. </span><span class="koboSpan" id="kobo.427.2">Ensemble methods are techniques that combine multiple individual models to form a more powerful and accurate predictor. </span><span class="koboSpan" id="kobo.427.3">These individual models, also known as base models or weak models, can be of any type, such as decision trees, support vector machines, or neural networks. </span><span class="koboSpan" id="kobo.427.4">By combining the predictions of these base models, ensemble methods aim to improve the overall performance and generalization ability of the model. </span><span class="koboSpan" id="kobo.427.5">There are several popular ensemble methods, including </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.429.1">Bagging</span></strong><span class="koboSpan" id="kobo.430.1">: Bagging, short </span><a id="_idIndexMarker1144"/><span class="koboSpan" id="kobo.431.1">for </span><strong class="bold"><span class="koboSpan" id="kobo.432.1">bootstrap aggregating</span></strong><span class="koboSpan" id="kobo.433.1">, involves training multiple base models on different subsets of the training data, with replacement. </span><span class="koboSpan" id="kobo.433.2">The final prediction is made by averaging or voting the predictions of the </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">individual models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.435.1">Boosting</span></strong><span class="koboSpan" id="kobo.436.1">: Boosting algorithms train base models sequentially, with each subsequent model focusing more on the samples that were previously misclassified. </span><span class="koboSpan" id="kobo.436.2">The predictions of multiple models are combined using weighted voting or averaging to make the </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">final prediction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.438.1">Random forest</span></strong><span class="koboSpan" id="kobo.439.1">: Random forest is an ensemble method that amalgamates bagging and decision trees. </span><span class="koboSpan" id="kobo.439.2">Numerous decision trees, each trained on a random subset of the features, are integrated through majority voting to formulate the </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">final prediction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.441.1">Adaptive boosting</span></strong><span class="koboSpan" id="kobo.442.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.443.1">AdaBoost</span></strong><span class="koboSpan" id="kobo.444.1">): AdaBoost is</span><a id="_idIndexMarker1145"/><span class="koboSpan" id="kobo.445.1"> a boosting algorithm that assigns weights to the training samples based on their classification errors. </span><span class="koboSpan" id="kobo.445.2">Weak models are trained iteratively, with each subsequent model focusing more on the misclassified samples. </span><span class="koboSpan" id="kobo.445.3">The final prediction is made by combining the predictions of multiple models using </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">weighted voting.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.447.1">Gradient boosting</span></strong><span class="koboSpan" id="kobo.448.1">: Gradient boosting is another boosting algorithm that sequentially trains weak models, with each subsequent model minimizing the loss function using gradient descent. </span><span class="koboSpan" id="kobo.448.2">The predictions of multiple models are combined using weighted averaging to make the </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">final prediction.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.450.1">Ensemble methods have proven to be effective in improving the performance and robustness of predictive models in various domains, including classification, regression, and anomaly detection. </span><span class="koboSpan" id="kobo.450.2">They are widely used in machine learning and data </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">mining applications.</span></span></p>
<p><span class="koboSpan" id="kobo.452.1">As usual, we start by importing the dataset into the MATLAB environment. </span><span class="koboSpan" id="kobo.452.2">We will use the Network Intrusion Detection dataset, which is available in the Kaggle dataset repository (</span><a href="https://www.kaggle.com/datasets/sampadab17/network-intrusion-detection"><span class="koboSpan" id="kobo.453.1">https://www.kaggle.com/datasets/sampadab17/network-intrusion-detection</span></a><span class="koboSpan" id="kobo.454.1">). </span><span class="koboSpan" id="kobo.454.2">The dataset contains a diverse range of simulated intrusions in a military network setting. </span><span class="koboSpan" id="kobo.454.3">The objective was to create a realistic environment resembling a US Air Force LAN, which was then exposed to multiple attacks. </span><span class="koboSpan" id="kobo.454.4">Each connection in this environment represents a sequence of TCP packets, with data flowing between a source IP address and a target</span><a id="_idIndexMarker1146"/><span class="koboSpan" id="kobo.455.1"> IP </span><a id="_idIndexMarker1147"/><span class="koboSpan" id="kobo.456.1">address under a predefined protocol. </span><span class="koboSpan" id="kobo.456.2">Every connection is classified as either normal or as an attack, with a specific attack type assigned to it. </span><span class="koboSpan" id="kobo.456.3">A connection record contains approximately 100 bytes of data. </span><span class="koboSpan" id="kobo.456.4">For each TCP/IP connection, a total of 41 features, both quantitative and qualitative, are collected from both normal and attack data. </span><span class="koboSpan" id="kobo.456.5">These features include three qualitative features and 38 quantitative features. </span><span class="koboSpan" id="kobo.456.6">The class variable in the dataset has two categories – normal </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">and anomalous:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.458.1">We start by importing the dataset into the </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">MATLAB environment:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.460.1">
NIDSTrainData = readtable('NDISdata.csv');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.461.1">We can extract some information about the features using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.462.1">summary()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.463.1"> function:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.464.1">summary(NIDSData)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.465.1">This function prints a summary of the table, displaying the properties description of the variables and some statistics such as min, median, and max for </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">numeric features.</span></span></p></li> <li><span class="koboSpan" id="kobo.467.1">As we did in the </span><em class="italic"><span class="koboSpan" id="kobo.468.1">Finding similar users in data</span></em><span class="koboSpan" id="kobo.469.1"> section, we have to split the data into two subsets: train and test. </span><span class="koboSpan" id="kobo.469.2">We will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.470.1">cvpartition()</span></strong><span class="koboSpan" id="kobo.471.1"> function for this, </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.473.1">
n = length(NIDSData.class);
SplitData = cvpartition(n,'Holdout',0.3);
TrainIndex = training(SplitData);
TrainData = NIDSData(TrainIndex,:);
TestIndex = test(SplitData);
TestData = NIDSData(TestIndex,:);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.474.1">We began by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.475.1">length()</span></strong><span class="koboSpan" id="kobo.476.1"> function to determine the number of observations in our dataset. </span><span class="koboSpan" id="kobo.476.2">This function gives us the size of the largest dimension in </span><a id="_idIndexMarker1148"/><span class="koboSpan" id="kobo.477.1">the </span><a id="_idIndexMarker1149"/><span class="koboSpan" id="kobo.478.1">array, </span><strong class="source-inline"><span class="koboSpan" id="kobo.479.1">X</span></strong><span class="koboSpan" id="kobo.480.1">, which in the case of vectors represents the total number of elements. </span><span class="koboSpan" id="kobo.480.2">Next, we utilized the </span><strong class="source-inline"><span class="koboSpan" id="kobo.481.1">cvpartition()</span></strong><span class="koboSpan" id="kobo.482.1"> function to randomly split the dataset into training and test subsets. </span><span class="koboSpan" id="kobo.482.2">This partition forms the basis for evaluating a statistical model. </span><span class="koboSpan" id="kobo.482.3">We extracted the indices for the training and test data using the training and test object functions and then used these indices to obtain the respective </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">data subsets.</span></span></p></li> <li><span class="koboSpan" id="kobo.484.1">Now, it’s time to train the algorithm for NDIS. </span><span class="koboSpan" id="kobo.484.2">To do this, we will use an algorithm based on </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">ensemble methods:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.486.1">
predictors = TrainData(:,1:41);
response = TrainData.class;</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.487.1">We started by setting the data used for the training and divided the subset into predictors and response variables. </span><span class="koboSpan" id="kobo.487.2">These terms are used in statistical analysis to describe the relationship between independent variables (predictors) and dependent variables (response). </span><span class="koboSpan" id="kobo.487.3">Predictors are variables that are used to predict, explain, or account for the variation in the response variable. </span><span class="koboSpan" id="kobo.487.4">They can be continuous or categorical and may have various levels or values. </span><span class="koboSpan" id="kobo.487.5">The response variable, on the other hand, is the outcome or variable that is being predicted or explained by the predictors. </span><span class="koboSpan" id="kobo.487.6">It is also known as the dependent variable. </span><span class="koboSpan" id="kobo.487.7">In our case, it is the label of the network intrusion detection classification (</span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">anomaly, normal).</span></span></p></li> <li><span class="koboSpan" id="kobo.489.1">Now, we can train </span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">the model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.491.1">
NDISEnsModel = fitcensemble(...
</span><span class="koboSpan" id="kobo.491.2">    predictors, ...
</span><span class="koboSpan" id="kobo.491.3">    response, ...
</span><span class="koboSpan" id="kobo.491.4">    'Method', 'AdaBoostM1');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.492.1">As a classification method, we used </span><strong class="source-inline"><span class="koboSpan" id="kobo.493.1">AdaBoostM1</span></strong><span class="koboSpan" id="kobo.494.1">. </span><strong class="source-inline"><span class="koboSpan" id="kobo.495.1">AdaBoostM1</span></strong><span class="koboSpan" id="kobo.496.1">, also known as adaptive boosting, is a machine learning algorithm that can be used for both classification </span><a id="_idIndexMarker1150"/><span class="koboSpan" id="kobo.497.1">and</span><a id="_idIndexMarker1151"/><span class="koboSpan" id="kobo.498.1"> regression problems. </span><span class="koboSpan" id="kobo.498.2">It is particularly effective in handling complex datasets and improving weak learning algorithms. </span><span class="koboSpan" id="kobo.498.3">The fundamental concept behind </span><strong class="source-inline"><span class="koboSpan" id="kobo.499.1">AdaBoostM1</span></strong><span class="koboSpan" id="kobo.500.1"> is to amalgamate multiple weak classifiers to form a robust classifier. </span><span class="koboSpan" id="kobo.500.2">A weak classifier is a basic model that performs marginally better than random guessing. </span><strong class="source-inline"><span class="koboSpan" id="kobo.501.1">AdaBoostM1</span></strong><span class="koboSpan" id="kobo.502.1"> accomplishes this by iteratively training weak classifiers on varied weighted versions of the dataset. </span><span class="koboSpan" id="kobo.502.2">The main idea behind </span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">AdaBoostM1</span></strong><span class="koboSpan" id="kobo.504.1"> is to combine multiple weak classifiers to create a strong classifier. </span><span class="koboSpan" id="kobo.504.2">A weak classifier is a simple model that performs slightly better than random guessing. </span><strong class="source-inline"><span class="koboSpan" id="kobo.505.1">AdaBoostM1</span></strong><span class="koboSpan" id="kobo.506.1"> achieves this by iteratively training weak classifiers on different weighted versions of </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">the dataset.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.508.1">In each iteration, the algorithm assigns increased weights to instances misclassified in the previous iteration. </span><span class="koboSpan" id="kobo.508.2">This compels the weak classifiers to prioritize the most challenging instances, enhancing their focus on difficult cases. </span><span class="koboSpan" id="kobo.508.3">These weighted weak classifiers are then combined using a weighted majority voting scheme to make final predictions. </span><span class="koboSpan" id="kobo.508.4">In classification tasks, the final prediction is obtained by assigning a class label based on the weighted majority vote of the weak classifiers. </span><span class="koboSpan" id="kobo.508.5">In regression tasks, the final prediction is obtained by averaging the weighted predictions of the </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">weak classifiers.</span></span></p><p class="list-inset"><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">AdaBoostM1</span></strong><span class="koboSpan" id="kobo.511.1"> has several advantages. </span><span class="koboSpan" id="kobo.511.2">It is robust to overfitting and can handle datasets with many features. </span><span class="koboSpan" id="kobo.511.3">It can also handle imbalanced datasets by adjusting the weights of the instances. </span><span class="koboSpan" id="kobo.511.4">Additionally, </span><strong class="source-inline"><span class="koboSpan" id="kobo.512.1">AdaBoostM1</span></strong><span class="koboSpan" id="kobo.513.1"> can be easily parallelized, making it suitable for distributed computing environments. </span><span class="koboSpan" id="kobo.513.2">However, </span><strong class="source-inline"><span class="koboSpan" id="kobo.514.1">AdaBoostM1</span></strong><span class="koboSpan" id="kobo.515.1"> may be sensitive to noisy data and outliers, as it assigns higher weights to misclassified instances. </span><span class="koboSpan" id="kobo.515.2">It also requires careful selection of weak classifiers, as too complex or too weak classifiers may not yield good results. </span><span class="koboSpan" id="kobo.515.3">Overall, </span><strong class="source-inline"><span class="koboSpan" id="kobo.516.1">AdaBoostM1</span></strong><span class="koboSpan" id="kobo.517.1"> is a powerful algorithm that has been widely used in </span><a id="_idIndexMarker1152"/><span class="koboSpan" id="kobo.518.1">many </span><a id="_idIndexMarker1153"/><span class="koboSpan" id="kobo.519.1">applications, including face detection, object recognition, and </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">financial forecasting.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.521.1">Let’s take a look at the model that </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">was trained:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.523.1">NDISEnsModel
NDISEnsModel =
  ClassificationEnsemble
           PredictorNames: {1×41 cell}
             ResponseName: 'Y'
    CategoricalPredictors: [2 3 4]
               ClassNames: {'anomaly'  'normal'}
           ScoreTransform: 'none'
          NumObservations: 17635
               NumTrained: 100
                   Method: 'AdaBoostM1'
             LearnerNames: {'Tree'}
     ReasonForTermination: 'Terminated normally after completing the requested number of training cycles.'
</span><span class="koboSpan" id="kobo.523.2">                  FitInfo: [100×1 double]
       FitInfoDescription: {2×1 cell}</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.524.1">Some information about the model is shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">preceding code.</span></span></p></li> <li><span class="koboSpan" id="kobo.526.1">After training the algorithm, it is time to evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">its performance:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.528.1">
PredData = predict(NDISEnsModel,TestData(:,1:41));</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.529.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.530.1">predict()</span></strong><span class="koboSpan" id="kobo.531.1"> function was utilized to obtain the expected response values generated by the generalized linear regression model. </span><span class="koboSpan" id="kobo.531.2">By evaluating the accuracy of the model, we can measure how well the predictive model’s predictions align</span><a id="_idIndexMarker1154"/><span class="koboSpan" id="kobo.532.1"> with</span><a id="_idIndexMarker1155"/><span class="koboSpan" id="kobo.533.1"> the actual observed values. </span><span class="koboSpan" id="kobo.533.2">This assessment serves as a performance indicator in accurately </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">forecasting outcomes:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.535.1">accuracy = sum(strcmpi(PredData,TestData{:,42})) / height(TestData(:,42));
fprintf('Accuracy: %.2f%%\n', accuracy * 100);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.536.1">To compare predicted data with the actual data, we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.537.1">strcmpi()</span></strong><span class="koboSpan" id="kobo.538.1"> function, which compares two strings, disregarding any discrepancies in letter case. </span><span class="koboSpan" id="kobo.538.2">It returns </span><em class="italic"><span class="koboSpan" id="kobo.539.1">1</span></em><span class="koboSpan" id="kobo.540.1"> (true) if the two are the same, and </span><em class="italic"><span class="koboSpan" id="kobo.541.1">0</span></em><span class="koboSpan" id="kobo.542.1"> (false) otherwise. </span><span class="koboSpan" id="kobo.542.2">To determine if the texts are identical, both their size and content are considered, except for case sensitivity. </span><span class="koboSpan" id="kobo.542.3">The output variable is of the logical type. </span><span class="koboSpan" id="kobo.542.4">The input arguments can comprise various combinations of string arrays, character vectors, and cell arrays of </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">character vectors.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.544.1">The following accuracy was obtained after running the </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">preceding code:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.546.1">Accuracy: 99.79%</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.547.1">These are amazing results that demonstrate that ensemble methods are very effective in classifying </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">network intrusion.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.549.1">Embarking on the final frontier of the data science journey, the deployment of machine learning models marks the critical phase where theoretical prowess transforms into real-world impact. </span><span class="koboSpan" id="kobo.549.2">The</span><a id="_idIndexMarker1156"/><span class="koboSpan" id="kobo.550.1"> next </span><a id="_idIndexMarker1157"/><span class="koboSpan" id="kobo.551.1">section delves into the intricacies and best practices of deploying machine learning models, ensuring their seamless integration into </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">operational environments.</span></span></p>
<h1 id="_idParaDest-209"><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.553.1">Deploying machine learning models</span></h1>
<p><span class="koboSpan" id="kobo.554.1">Deploying machine</span><a id="_idIndexMarker1158"/><span class="koboSpan" id="kobo.555.1"> learning models refers to the process of making a trained model available for making predictions on new, unseen data. </span><span class="koboSpan" id="kobo.555.2">It involves taking the trained model and integrating it into a production environment where it can receive input data, perform predictions, and return the results. </span><span class="koboSpan" id="kobo.555.3">The trained model needs to be organized and packaged into a format suitable for deployment. </span><span class="koboSpan" id="kobo.555.4">This may involve exporting the model into a file format that can be easily loaded and used by other systems. </span><span class="koboSpan" id="kobo.555.5">An </span><strong class="bold"><span class="koboSpan" id="kobo.556.1">application programming interface</span></strong><span class="koboSpan" id="kobo.557.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.558.1">API</span></strong><span class="koboSpan" id="kobo.559.1">) is typically</span><a id="_idIndexMarker1159"/><span class="koboSpan" id="kobo.560.1"> created to expose the machine learning model’s functionality. </span><span class="koboSpan" id="kobo.560.2">The API acts as the interface that other systems or applications can use to send data and receive predictions from </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.562.1">If the model is expected to handle many concurrent requests, the deployment environment may need to be scaled to accommodate the increased load. </span><span class="koboSpan" id="kobo.562.2">This may involve setting up clusters of servers or using cloud-based infrastructure. </span><span class="koboSpan" id="kobo.562.3">Once the model has been deployed, it is important to monitor its performance and behavior to ensure it continues to provide accurate predictions. </span><span class="koboSpan" id="kobo.562.4">Monitoring can involve tracking metrics such as response time, throughput, or error rates to identify any issues or performance degradation. </span><span class="koboSpan" id="kobo.562.5">Machine learning models often need updates or retraining as new data becomes available. </span><span class="koboSpan" id="kobo.562.6">Therefore, it is important to have processes in place for continuous integration and delivery to easily deploy new versions of the model and ensure it stays up </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">to date.</span></span></p>
<h2 id="_idParaDest-210"><a id="_idTextAnchor216"/><span class="koboSpan" id="kobo.564.1">Understanding model compression</span></h2>
<p><span class="koboSpan" id="kobo.565.1">Model compression </span><a id="_idIndexMarker1160"/><span class="koboSpan" id="kobo.566.1">refers to the process of diminishing </span><a id="_idIndexMarker1161"/><span class="koboSpan" id="kobo.567.1">the size of a machine learning model without significantly compromising its performance. </span><span class="koboSpan" id="kobo.567.2">The need for model compression arises in scenarios where the size or computational requirements of a model become critical, such as when deploying models on resource-constrained devices such as smartphones or </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">edge devices.</span></span></p>
<p><span class="koboSpan" id="kobo.569.1">There are several techniques for </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">model compression:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.571.1">Pruning</span></strong><span class="koboSpan" id="kobo.572.1">: Pruning involves</span><a id="_idIndexMarker1162"/><span class="koboSpan" id="kobo.573.1"> removing unnecessary connections or parameters from the model. </span><span class="koboSpan" id="kobo.573.2">Typically, connections with small weights are pruned based on a certain threshold, resulting in a sparser model with </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">fewer parameters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.575.1">Quantization</span></strong><span class="koboSpan" id="kobo.576.1">: Quantization is the process of reducing the precision of the weights and activations in a model. </span><span class="koboSpan" id="kobo.576.2">For example, instead of using 32-bit floating-point values, weights can be represented using 8-bit integers. </span><span class="koboSpan" id="kobo.576.3">Thi</span><a id="_idTextAnchor217"/><span class="koboSpan" id="kobo.577.1">s reduces memory requirements and improves </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">computational efficiency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.579.1">Knowledge distillation</span></strong><span class="koboSpan" id="kobo.580.1">: Knowledge distillation involves training a smaller, “student” model to mimic the predictions of a larger, “teacher” model. </span><span class="koboSpan" id="kobo.580.2">The teacher model’s knowledge is transferred to the student model, allowing for a compact representation of the original model while </span><span class="No-Break"><span class="koboSpan" id="kobo.581.1">maintaining performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.582.1">Low-rank approximation</span></strong><span class="koboSpan" id="kobo.583.1">: Low-rank approximation techniques aim to approximate the weights of a model using low-rank matrices or tensors. </span><span class="koboSpan" id="kobo.583.2">This reduces the number of parameters required to represent the model, leading to a smaller memory footprint and </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">faster computations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.585.1">Compact architectures</span></strong><span class="koboSpan" id="kobo.586.1">: This involves designing or using compact architectures, such as MobileNet or SqueezeNet, that are specifically built to be lightweight </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">and efficient.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.588.1">These compression techniques enable models to be deployed on devices with limited resources or used in scenarios with strict latency or memory constraints. </span><span class="koboSpan" id="kobo.588.2">However, it is important to consider trade-offs as model compression may lead to slight performance degradation in terms of accuracy or </span><span class="No-Break"><span class="koboSpan" id="kobo.589.1">inference speed.</span></span></p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor218"/><span class="koboSpan" id="kobo.590.1">Discovering model pruning techniques</span></h2>
<p><span class="koboSpan" id="kobo.591.1">Model pruning is a</span><a id="_idIndexMarker1163"/><span class="koboSpan" id="kobo.592.1"> machine</span><a id="_idIndexMarker1164"/><span class="koboSpan" id="kobo.593.1"> learning technique that aims to decrease the size and complexity of a trained model by eliminating unnecessary or redundant parameters, connections, or nodes. </span><span class="koboSpan" id="kobo.593.2">The objective of model pruning is to enhance the efficiency and computational performance of the model without significantly </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">compromising accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.595.1">There are several methods of model pruning, including </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.597.1">Weight pruning</span></strong><span class="koboSpan" id="kobo.598.1">: In weight </span><a id="_idIndexMarker1165"/><span class="koboSpan" id="kobo.599.1">pruning, individual weights in the model are set to zero or removed entirely based on their magnitude. </span><span class="koboSpan" id="kobo.599.2">This reduces the number of parameters and can result in a </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">sparser model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.601.1">Neuron pruning</span></strong><span class="koboSpan" id="kobo.602.1">: Neuron pruning entails the removal of entire neurons from the model, guided by their contribution to the overall performance. </span><span class="koboSpan" id="kobo.602.2">Neurons with low activation or minimal impact on the output </span><span class="No-Break"><span class="koboSpan" id="kobo.603.1">are pruned.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.604.1">Filter pruning</span></strong><span class="koboSpan" id="kobo.605.1">: This technique is commonly used in </span><strong class="bold"><span class="koboSpan" id="kobo.606.1">convolutional neural networks</span></strong><span class="koboSpan" id="kobo.607.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.608.1">CNNs</span></strong><span class="koboSpan" id="kobo.609.1">). </span><span class="koboSpan" id="kobo.609.2">Filters</span><a id="_idIndexMarker1166"/><span class="koboSpan" id="kobo.610.1"> are groups of feature detectors that are applied across an image during the convolution operation. </span><span class="koboSpan" id="kobo.610.2">Filter pruning involves removing unnecessary filters that do not contribute significantly to the </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">model’s accuracy.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.612.1">Structured pruning</span></strong><span class="koboSpan" id="kobo.613.1">: Structured pruning involves removing entire layers or blocks of the model, rather than individual weights or neurons. </span><span class="koboSpan" id="kobo.613.2">This method often results in more efficient implementations since removing entire layers reduces both computational complexity and </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">memory requirements.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.615.1">Model pruning can be performed during training or as a post-training optimization step. </span><span class="koboSpan" id="kobo.615.2">It is often combined with other techniques such as regularization methods or quantization to further improve the efficiency of the pruned model. </span><span class="koboSpan" id="kobo.615.3">Pruning can provide significant benefits in terms of model size, speed, and memory usage, making it a useful technique for deploying models on resource-constrained devices or in </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">real-time applications.</span></span></p>
<p><span class="koboSpan" id="kobo.617.1">In MATLAB, you can perform model pruning using various techniques and tools such as Neural Network Toolbox. </span><span class="koboSpan" id="kobo.617.2">The following steps outline a general approach to model pruning </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">in MATLAB:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.619.1">Load or create your initial neural network model using Neural Network Toolbox </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">in MATLAB.</span></span></li>
<li><span class="koboSpan" id="kobo.621.1">Train the neural network model to achieve a reasonably good performance on a given task </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">or dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.623.1">Use the pruning algorithm or technique of your choice to identify and remove unnecessary weights, connections, or nodes from the trained model. </span><span class="koboSpan" id="kobo.623.2">Some commonly used pruning techniques include magnitude-based pruning, sensitivity-based pruning, and </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">weight-decay pruning.</span></span></li>
<li><span class="koboSpan" id="kobo.625.1">Evaluate the pruned model’s performance by testing it on a validation or test dataset. </span><span class="koboSpan" id="kobo.625.2">Make sure that the pruning process does not significantly degrade the </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">model’s performance.</span></span></li>
<li><span class="koboSpan" id="kobo.627.1">Fine-tune or retrain the pruned model, if necessary, to recover any performance degradation due to pruning. </span><span class="koboSpan" id="kobo.627.2">Adjust the learning rate or other training parameters to optimize the pruned </span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">model’s performance.</span></span></li>
<li><span class="koboSpan" id="kobo.629.1">Repeat </span><em class="italic"><span class="koboSpan" id="kobo.630.1">s</span></em><em class="italic"><span class="koboSpan" id="kobo.631.1">teps 3</span></em><span class="koboSpan" id="kobo.632.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.633.1">5</span></em><span class="koboSpan" id="kobo.634.1"> until the desired level of model compression or performance trade-off </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">is achieved.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.636.1">Remember that</span><a id="_idIndexMarker1167"/><span class="koboSpan" id="kobo.637.1"> model pruning is a dynamic and iterative process, and it requires careful consideration of the trade-off between model size, performance, and </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">computational requirements.</span></span></p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor219"/><span class="koboSpan" id="kobo.639.1">Introducing quantization for efficient inference on edge devices</span></h2>
<p><span class="koboSpan" id="kobo.640.1">Quantization is</span><a id="_idIndexMarker1168"/><span class="koboSpan" id="kobo.641.1"> a technique</span><a id="_idIndexMarker1169"/><span class="koboSpan" id="kobo.642.1"> that’s used in model compression to reduce the memory footprint and computational requirements of deep neural networks. </span><span class="koboSpan" id="kobo.642.2">It involves converting the weights and activations of a network from floating-point representation into lower precision representation, such as 8-bit or </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">even lower.</span></span></p>
<p><span class="koboSpan" id="kobo.644.1">Quantization reduces the precision of the model, which can lead to a loss in model accuracy. </span><span class="koboSpan" id="kobo.644.2">However, it has been observed that many neural networks are robust to quantization and can still achieve similar performance with reduced precision representation. </span><span class="koboSpan" id="kobo.644.3">This is especially true for deep neural networks with large numbers </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">of parameters.</span></span></p>
<p><span class="koboSpan" id="kobo.646.1">There are different approaches to quantization in </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">model compression:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.648.1">Weight-only quantization</span></strong><span class="koboSpan" id="kobo.649.1">: In this approach, only the weights of the neural network are</span><a id="_idIndexMarker1170"/><span class="koboSpan" id="kobo.650.1"> quantized, while the activations remain in the original precision. </span><span class="koboSpan" id="kobo.650.2">This reduces the memory requirements significantly as weights typically consume the majority of the memory in a </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">neural network.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.652.1">Full quantization</span></strong><span class="koboSpan" id="kobo.653.1">: In this approach, both the weights and activations are quantized to lower precision. </span><span class="koboSpan" id="kobo.653.2">This provides further reduction in memory requirements and computational complexity but can result in a larger loss in model accuracy compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">weight-only quantization.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.655.1">Dynamic quantization</span></strong><span class="koboSpan" id="kobo.656.1">: Dynamic quantization techniques adaptively adjust the precision of weights and activations during inference based on the input data. </span><span class="koboSpan" id="kobo.656.2">This allows for more flexibility in the precision used, leading to potential accuracy improvements compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">static quantization.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.658.1">Quantization-aware training</span></strong><span class="koboSpan" id="kobo.659.1">: Instead of quantizing the model after training, quantization-aware training incorporates the quantization process during the training phase itself. </span><span class="koboSpan" id="kobo.659.2">This ensures that the model learns to be robust to quantization and results in better accuracy </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">when quantized.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.661.1">Quantization is a widely used technique in model compression and is particularly useful when deploying</span><a id="_idIndexMarker1171"/><span class="koboSpan" id="kobo.662.1"> deep neural </span><a id="_idIndexMarker1172"/><span class="koboSpan" id="kobo.663.1">networks on resource-constrained devices such as edge devices or mobile phones. </span><span class="koboSpan" id="kobo.663.2">It allows for efficient deployment of models with reduced memory requirements and improved computational efficiency, without sacrificing significant </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">model accuracy.</span></span></p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor220"/><span class="koboSpan" id="kobo.665.1">Getting started with knowledge distillation</span></h2>
<p><span class="koboSpan" id="kobo.666.1">Knowledge</span><a id="_idIndexMarker1173"/><span class="koboSpan" id="kobo.667.1"> distillation is a technique that’s</span><a id="_idIndexMarker1174"/><span class="koboSpan" id="kobo.668.1"> used in model compression that refers to the process of reducing the size and complexity of a machine learning model without significant loss in performance. </span><span class="koboSpan" id="kobo.668.2">In knowledge distillation, a large “teacher” model is trained on a dataset and used as a reference to train a smaller “student” model. </span><span class="koboSpan" id="kobo.668.3">The goal is to transfer the knowledge, or the learned representations, from the teacher model to the </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">student model.</span></span></p>
<p><span class="koboSpan" id="kobo.670.1">Knowledge transfer is achieved by training the student model to mimic the outputs of the teacher model. </span><span class="koboSpan" id="kobo.670.2">Typically, this involves using the teacher model’s soft targets or logits instead of the hard labels during training. </span><span class="koboSpan" id="kobo.670.3">Soft targets refer to the probabilities assigned to each class by the teacher model, which can provide more nuanced information compared to the one-hot encoded </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">hard labels.</span></span></p>
<p><span class="koboSpan" id="kobo.672.1">During the training process, the student model tries to minimize the difference between its predictions and the soft targets provided by the teacher model. </span><span class="koboSpan" id="kobo.672.2">This enables the student model to learn from the richer information provided by the teacher, improving its understanding of the data and increasing its performance. </span><span class="koboSpan" id="kobo.672.3">Knowledge distillation can lead to significant model compression as the smaller student model can capture the knowledge of the larger teacher model, often with fewer parameters. </span><span class="koboSpan" id="kobo.672.4">Additionally, the student model can be more efficient in terms of inference time and resource usage, making it suitable for deployment on </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">resource-constrained platforms.</span></span></p>
<p><span class="koboSpan" id="kobo.674.1">In summary, knowledge distillation is a technique that’s used in model compression to transfer knowledge</span><a id="_idIndexMarker1175"/><span class="koboSpan" id="kobo.675.1"> from a larger teacher model to a smaller</span><a id="_idIndexMarker1176"/><span class="koboSpan" id="kobo.676.1"> student model, thereby compressing the model size while </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">maintaining performance.</span></span></p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor221"/><span class="koboSpan" id="kobo.678.1">Learning low-rank approximation</span></h2>
<p><span class="koboSpan" id="kobo.679.1">Low-rank </span><a id="_idIndexMarker1177"/><span class="koboSpan" id="kobo.680.1">approximation is a technique that’s </span><a id="_idIndexMarker1178"/><span class="koboSpan" id="kobo.681.1">used in model compression to reduce the size of a given model. </span><span class="koboSpan" id="kobo.681.2">It involves approximating a high-dimensional weight matrix or tensor by a lower-rank approximation, which significantly reduces the number of parameters needed to represent the model. </span><span class="koboSpan" id="kobo.681.3">In low-rank approximation, a factorization of the weight matrix is performed to decompose it into two or more smaller matrices or tensors. </span><span class="koboSpan" id="kobo.681.4">The rank of the approximation determines the number of smaller matrices or tensors used. </span><span class="koboSpan" id="kobo.681.5">By choosing a lower rank, the resulting approximation will have fewer parameters, making it more compact </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">and efficient.</span></span></p>
<p><span class="koboSpan" id="kobo.683.1">Low-rank approximation can be applied to various types of models, including neural networks, deep learning models, and other machine learning algorithms. </span><span class="koboSpan" id="kobo.683.2">It is especially useful for reducing the computational and memory requirements of large models. </span><span class="koboSpan" id="kobo.683.3">This enables their deployment on resource-constrained devices, such as mobile phones or embedded systems. </span><span class="koboSpan" id="kobo.683.4">One common approach for low-rank approximation is </span><strong class="bold"><span class="koboSpan" id="kobo.684.1">singular value decomposition</span></strong><span class="koboSpan" id="kobo.685.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.686.1">SVD</span></strong><span class="koboSpan" id="kobo.687.1">), which decomposes a matrix into three matrices representing the</span><a id="_idIndexMarker1179"/><span class="koboSpan" id="kobo.688.1"> left singular vectors, singular values, and right singular vectors. </span><span class="koboSpan" id="kobo.688.2">Selecting a subset of the singular values and their corresponding singular vectors allows for the creation of a </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">low-rank approximation.</span></span></p>
<p><span class="koboSpan" id="kobo.690.1">Other techniques for low-rank approximation include Tucker decomposition, which decomposes a tensor into smaller tensors, and tensor-train decomposition, which represents a tensor as a series of matrix products. </span><span class="koboSpan" id="kobo.690.2">These techniques can be applied to higher-order tensors typically found in deep </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.692.1">Overall, low-rank approximation is a powerful technique for model compression, enabling the reduction </span><a id="_idIndexMarker1180"/><span class="koboSpan" id="kobo.693.1">of model size without </span><a id="_idIndexMarker1181"/><span class="koboSpan" id="kobo.694.1">sacrificing too much performance. </span><span class="koboSpan" id="kobo.694.2">It allows for the efficient deployment of models on resource-constrained devices and faster </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">inference times.</span></span></p>
<h1 id="_idParaDest-215"><a id="_idTextAnchor222"/><span class="koboSpan" id="kobo.696.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.697.1">In this chapter, we learned the basic concepts of recommender systems, starting with the definition of these systems and then understanding how the problem is approached. </span><span class="koboSpan" id="kobo.697.2">We analyzed the different types of recommender systems: CF, content-based filtering, and hybrid </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">recommender systems.</span></span></p>
<p><span class="koboSpan" id="kobo.699.1">Next, we saw how to use similarities in the data to identify possible fraudulent uses of credit cards. </span><span class="koboSpan" id="kobo.699.2">To do this, we trained a model based on the nearest neighbor algorithm but using a modified version of the traditional k-NN algorithm, where neighbors are given varying weights during the prediction or </span><span class="No-Break"><span class="koboSpan" id="kobo.700.1">classification process.</span></span></p>
<p><span class="koboSpan" id="kobo.701.1">Then, we saw how to implement a NIDS based on ensemble methods in MATLAB. </span><span class="koboSpan" id="kobo.701.2">Specifically, we adopted an AdaBoost algorithm to identify intrusions in a </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">LAN network.</span></span></p>
<p><span class="koboSpan" id="kobo.703.1">Finally, we introduced the techniques of deploying machine learning models regarding model compression. </span><span class="koboSpan" id="kobo.703.2">We analyzed the most popular model compression techniques, including pruning, quantization, knowledge distillation, and </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">low-rank approximation.</span></span></p>
<p><span class="koboSpan" id="kobo.705.1">In the next chapter, we will learn the basic concepts of anomaly detection and fault diagnosis systems. </span><span class="koboSpan" id="kobo.705.2">We will understand how to identify anomaly functioning using deep learning, as well as how to implement a fault diagnosis system in MATLAB. </span><span class="koboSpan" id="kobo.705.3">Finally, we will discover dropout, L1 and L2 regularization, and </span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">early stopping.</span></span></p>
</div>
</body></html>