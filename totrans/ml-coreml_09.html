<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Object Segmentation Using CNNs</h1>
                </header>
            
            <article>
                
<p class="mce-root">Throughout the chapters in this book, we have seen various machine learning models, each progressively increasing their perceptual abilities. By this, I mean that we were first introduced to a model capable of classifying a single object present in an image. Then came a model that was able to classify <span>not only </span>multiple objects but also their corresponding bounding boxes. In this chapter, we continue this progression by introducing semantic segmentation, in other words, being able to assign each pixel to a specific class, as shown in the following figure: </p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><img src="assets/bd0261d3-7c65-446d-8af5-1071f134fef6.png"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Source: http://cocodataset.org/#explore</div>
<p class="mce-root">This allows for a greater understanding of the scene and, therefore, opportunities for more intelligible interfaces and services. But this is not the main focus of this chapter. In this chapter, we will use semantic segmentation to create an image effects application as a way to demonstrate imperfect predictions. We'll be using this to motivate a discussion on one of the most important aspects of designing and building machine learning (or artificial intelligence) interfaces—dealing with probabilistic, or imperfect, outcomes from models.</p>
<p class="mce-root">By the end of this chapter, you will have:</p>
<ul>
<li class="mce-root">An understanding semantic of segmentation</li>
<li class="mce-root">Built an intuitive understanding of how it is achieved (learned) </li>
<li class="mce-root">Learned how it can be applied in a novel way for real life applications by building an action shot photo effects application </li>
<li class="mce-root">Gained appreciation and awareness for dealing with probability outcomes from machine learning models</li>
</ul>
<p>Let's begin by better understanding what semantic segmentation is and get an intuitive understanding of how it is achieved.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying pixels </h1>
                </header>
            
            <article>
                
<p>As we have already discussed, the desired output of a model performing semantic segmentation is an image with each of its pixels assigned a label of its most likely class (or even a specific instance of a class). Throughout this book, we have also seen that layers of a deep neural network learn features that are activated when a corresponding input that satisfies the particular feature is detected. We can visualize these activations using a technique called <strong>class activation maps</strong> (<strong>CAMs</strong>). The output produces a heatmap of class activations over the input image; the heatmap consists of a matrix of scores associated with a specific class, essentially giving us a spatial map of how intensely the input region activates a specified class. The following figure shows an output of a CAM visualization for the class cat. Here, you can see that the heatmap portrays what the model considers important features (and therefore regions) for this class:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/6d4c72c3-ac9e-430e-af95-f5ff4c284d3b.png"/></div>
<div class="packt_infobox">The preceding figure was produced using the implementation described in the paper <em>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</em> by R. Selvaraju. The approach is to take the output feature map of a convolutional layer and weigh every channel in that feature map by the gradient of the class. For more details of how it works, please refer to the original paper: <a href="https://arxiv.org/abs/1610.02391">https://arxiv.org/abs/1610.02391</a>.</div>
<p>Early attempts of semantic segmentation were made using slightly adapted classification models such as VGG and Alexnet, but they only produced <span>coarse approximations</span>. This can be seen in the preceding figure and is largely due to the network using repetitive pooling layers, which results in loss of spatial information.</p>
<p>U-Net is one architecture that addresses this; it consists of an <strong>encoder</strong> and <strong>decoder</strong>, with the addition of <strong>shortcuts</strong> between the two to preserve spatial information. Released in 2015 by <em>O. Ronneberger</em>, <em>P. Fischer</em>, and <em>T. Brox</em> for biomedical image segmentation, it has since become one of the go-to architectures for segmentation due to its effectiveness (it can be trained on a small dataset) and performance. The following figure shows the modified U-Net we will be using in this chapter:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/dc50598c-dd4e-480f-be82-5bff251864ed.png" style="width:51.67em;height:26.17em;"/></div>
<div class="packt_infobox">U-Net is one of many architectures for semantic segmentation. Sasank Chilamkurthy's post <em>A 2017 Guide to Semantic Segmentation with Deep Learning</em> provides a great overview and comparison of the most popular architectures, available at <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review">http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review</a>. For further details on U-Net, please refer to the original paper mentioned earlier. It is available at <a href="https://arxiv.org/pdf/1505.04597.pdf">https://arxiv.org/pdf/1505.04597.pdf</a>.</div>
<p>On the left in the preceding figure, we have the full network used in this chapter's project, and on the right we have an extract of blocks used in the encoder and decoder parts of the network. As a reminder, the focus of this book is on applying machine learning rather than the details of the models themselves. So for this reason, we won't be delving into the details, but there are a few interesting and useful things worth pointing out.</p>
<p>The first is the general structure of the network; it consists of an encoder and decoder.<strong> </strong>The encoder's role is to capture context. The decoder's task is to use this context and features from the corresponding shortcuts to project its understanding onto pixel space, to get a dense and precise classification. It's a common practice to bootstrap the encoder using an architecture and weights from a trained classification model, such as VGG16. This not only speeds up training but also is likely to increase performance as it brings with it a depth (pun intended) of understanding of images it has been trained on, which is typically from a larger dataset.</p>
<p>Another point <span>worth </span><span>highlighting </span>is those shortcuts between the encoder and decoder. As mentioned previously, they are used to preserve spatial information outputted from convolutional layers from each encoding block before being lost when its downsampled using max pooling. This information is used to assist the model in precise localization. </p>
<div class="packt_infobox">It's the first time in this book that we have seen an upsampling layer. As the name implies, it's a technique that upsamples your image (or feature maps) to a higher resolution. One of the easiest ways is to use the same techniques we use with image upsampling, that is, rescaling the input to a desired size and calculating the values at each point using an interpolation method, such as bilinear interpolation. </div>
<p><span>Lastly, I wanted to bring to your attention the input and outputs of the model. The model is expecting a</span> 448 x 448 color image <span>as its input and outputs a</span> 448 x 448 x 1 <span>(single channel) matrix. If you inspect the architecture, you will notice that the last layer is a</span> sigmoid activation,<span> where a sigmoid function is typically used for binary classification, which is precisely what we are doing here. Typically, you would perform multi-class classification for semantic segmentation tasks, in which case you would replace the sigmoid activation with a</span> softmax <span>activation. An example commonly used when introducing semantic segmentation is scene understanding for self-driving cars. The following is an example of a labeled scene from Cambridge University's Motion-based Segmentation and Recognition Dataset where each color represents a different class:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/269955f2-4095-4b7a-8b25-b96fe313555a.png" style="width:41.67em;height:31.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Source: http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/</div>
<p><span>But in this example, a binary classifier is</span> sufficient,<span> which will become apparent as we go into the details of the project. However, I wanted to highlight it here as the</span> architecture<span> will scale to multi-class classification by simply swapping the last layer with a softmax activation and changing the loss function. </span></p>
<p><span>You have thus seen the architecture we will be using in this chapter. Let's now look at how we will use it and the data used to train the model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data to drive the desired effect – action shots</h1>
                </header>
            
            <article>
                
<p>Now would be a good time to introduce the photo effect we want to create in this chapter. The effect, as I know it, is called an <strong>action shot.</strong> It's essentially a still photograph that shows someone (or something) in motion,<span><span> probably</span></span> best illustrated with an image - like the one shown here: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span><img src="assets/3cc3c762-cb35-4a7c-8882-06f407243dee.png"/> </span></div>
<div class="packt_infobox">As previously mentioned, the model we used in this chapter performs binary (or single-class) classification. This simplification, using a binary classifier instead of a multi-class classifier, has been driven by the intended use that is just segmenting people from the background. Similar to any software project, you should strive for simplicity where you can.</div>
<p>To extract people, we need a model to learn how to recognize people and their associated pixels. For this, we need a dataset consisting of images of people and corresponding images with those pixels of the persons labeled—and lots of them. Unlike <span>datasets for </span>classification, datasets for object segmentation are not so common nor as vast. This is understandable given the additional effort that would be required to label such a dataset. Some common datasets for object segmentation, and ones that are considered for this chapter, include:</p>
<ul>
<li><strong>PASCAL VOC</strong>: A dataset with 9,993 labeled images across 20 classes. You can find the dataset at <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html</a>.</li>
<li><strong><span>Labeled Faces in the Wild (</span>LFW) from University of Massachusetts Amherst</strong>: A dataset comprising 2,927 faces. Each has the hair, skin, and background labeled (three classes). <span>You can find the dataset at</span> <a href="http://vis-www.cs.umass.edu/lfw/part_labels/">http://vis-www.cs.umass.edu/lfw/part_labels/</a>.</li>
<li><strong>Common Objects in Context (COCO) dataset</strong>: A popular dataset for all things related to <span>computer vision</span>, including segmentation. Its segmented datasets comprise approximately 200,000 labeled images across 80 classes. It's the dataset that was used and which we will be briefly exploring in this section. You can find the dataset at <a href="http://cocodataset.org/#home">http://cocodataset.org/#home</a>. </li>
<li>Not considered for this project but good to be aware of is the <strong>Cambridge-driving Labeled Video Database</strong> (<strong>CamVid</strong>) from Cambridge University. As is clear from the name, the dataset is made up of frames from a video feed from a car camera—ideal for anyone interested in training their own self-driving car. <span>You can find the dataset at </span><a href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/">http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/</a>.</li>
</ul>
<div class="packt_infobox">Listing the datasets here is possibly superfluous, but semantic segmentation is such an exciting opportunity with huge potential that I hope listing these here will encourage you to explore and experiment with new applications of it.</div>
<p>Luckily for us, COCO's 13+ GB dataset contains many labeled images of people and a convenient API to make finding relevant images easy. For this chapter, COCO's API was used to find all images including people. Then, these were filtered further, only keeping those that contained either one or two people and whose area covered between 20% and 70% of the image, discarding those images where the person was too small or too large. For each of these images, the contours of each of the persons were fetched and then used to create a binary mask, which then became our labels for our training. The following figure illustrates this process for a single image:</p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><img src="assets/130f0640-fac9-4810-8b88-a875cd0b11a4.png"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Source: The COCO dataset (http://cocodataset.org)</div>
<p>After training on 8,689 images over 40 epochs, an <strong>Intersection over Union</strong> (<strong>IoU</strong>) coefficient <span>(also known as the </span><span><strong>dice coefficient</strong>) </span>of <span>0.8192 </span>was achieved on the validation data (approximately 300).</p>
<div class="packt_infobox">Hopefully, IoU sounds familiar as it was what we used back in <a href="6365f272-41e9-4511-a564-dc0f8db5d3ca.xhtml" target="_blank">Chapter 5</a>, <em>Locating Objects in the World</em>. As a reminder, IoU is an evaluation metric used to measure how well two bounding boxes overlap each other. A perfect overlap, where both bounding boxes overlap each other perfectly, would return 1.0 (which is why the loss is negated for training).</div>
<p>In the following image, we get to see what this looks like, starting with random examples from the validation set. Then, some are manually searched for, like the ones that portray actions:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/d0b5ea40-c7b0-473c-a4ac-2ee4b6e66bbf.png" style="width:38.83em;height:41.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Source: The COCO dataset (http://cocodataset.org)</div>
<p>And here are some examples of action images where the model was able to sufficiently segment the person from the image:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/3eea43d5-3b5b-4f33-b880-7de6224dc15f.png"/></div>
<p>Finally, here are some, out of many, examples of action images where the model was less successful:</p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><img src="assets/991af62f-8fb1-48af-99b2-ccb419db803b.png"/></div>
<p>We have covered the model and training data and examined the outputs of the model. It's now time to turn our attention to the application in this chapter, which we will begin working on in the next section. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the photo effects application</h1>
                </header>
            
            <article>
                
<p>In this section, we will be looking <span>briefly </span>at the application and highlighting some of the interesting pieces of the code, omitting most of it as it has already been discussed in previous chapters. As mentioned in the introduction, this example is to provide a case study for a later section, where we will discuss some broad strategies to use when building intelligent interfaces and services. </p>
<p class="mce-root">If you haven't already, pull down the latest code from the accompanying repository at <a href="https://github.com/packtpublishing/machine-learning-with-core-ml">https://github.com/packtpublishing/machine-learning-with-core-ml</a>. Once downloaded, navigate to the <kbd>Chapter9/Start/</kbd> directory and open the project <kbd>ActionShot.xcodeproj</kbd>.</p>
<p>As mentioned in the previous section, the example for this chapter is an photo effects application. In it, the user is able to take an <em>action shot</em>, have the application extract each person from the frames, and compose them onto the final frame, as illustrated in the following figure:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c6ce6ad3-e903-40ec-b1db-8b989b4294f9.png" style="width:36.83em;height:32.17em;"/></div>
<p>The application consists of two view controllers; one is responsible for capturing the frames and the other for presenting the composite image. The workhorse for the processing, once again, has been delegated to the <kbd>ImageProcessor</kbd> class and it is the perspective from which we will be reviewing this project. </p>
<p><kbd>ImageProcessor</kbd> acts as both the sink and processor; by sink I refer to it being the class that is passed captured frames from the camera, using the <kbd>CameraViewController</kbd>, and holding them in memory for processing. Let's see what the code for this looks like; select <kbd>ImageProcessor.swift</kbd> from the left panel to bring the source code into focus. Let's see what exists; initially paying particular attention to the properties and methods responsible for handling received frames and then move on to their processing.</p>
<p>At the top of the file, you will notice that a protocol has been declared, which is implemented by the <kbd>EffectViewController</kbd>; it is used to broadcast the progress of the tasks:</p>
<pre>protocol ImageProcessorDelegate : class{<br/><br/>    func onImageProcessorFinishedProcessingFrame(<br/>        status:Int, processedFrames:Int, framesRemaining:Int)<br/><br/>    func onImageProcessorFinishedComposition(<br/>        status:Int, image:CIImage?)<br/>}</pre>
<p>The first callback, <kbd>onImageProcessorFinishedProcessingFrame</kbd>, is used to notify the delegate of frame-by-frame processing progress while the other, <kbd>onImageProcessorFinishedComposition</kbd>, is used to notify the delegate once the final image has be created. These discrete callbacks are intentionally split as the processing has been broken down into segmentation and composition. Segmentation is responsible for segmenting each of the frames using our model, and composition is responsible for generating the final image using the processed (segmented) frames. This structure is also mimicked in the layout of the class, with the class broken down into four parts and the flow we will follow in this section.</p>
<p>The first part declares all the variables. The second implements the properties and methods responsible for retrieving the frames while they're being captured. The third contains all the methods for processing the frames, whereby the delegate is notified using the <kbd>onImageProcessorFinishedProcessingFrame</kbd> callback. The final part, and the one we will focus on the most, contains the methods responsible for generating the final image, that is, it composites the frames. Let's peek at the first part to get a sense of what variables are available, which are shown in the following code snippet: </p>
<pre>class ImageProcessor{<br/>    <br/>    weak var delegate : ImageProcessorDelegate?<br/>    <br/>    lazy var model : VNCoreMLModel = {<br/>        do{<br/>            let model = try VNCoreMLModel(<br/>                for: small_unet().model<br/>            )<br/>            return model<br/>        } catch{<br/>            fatalError("Failed to create VNCoreMLModel")<br/>        }<br/>    }()    <br/>    <br/>    var minMaskArea:CGFloat = 0.005<br/>    var targetSize = CGSize(width: 448, height: 448)<br/>    let lock = NSLock()<br/>    var frames = [CIImage]()<br/>    var processedImages = [CIImage]()<br/>    var processedMasks = [CIImage]()<br/>    private var _processingImage = false<br/>    <br/>    init(){<br/>        <br/>    }<br/>}</pre>
<p>Nothing extraordinary. We first declare a property that wraps our model in an instance of <kbd>VNCoreMLModel</kbd> so that we can take advantage of the Vision framework's preprocessing functionality. We then declare a series of variables to deal with storing the frames and handling the processing; we make use of an <kbd>NSLock</kbd> instance to avoid different threads reading stale property values.</p>
<p>The following code snippet, and part of the <kbd>ImageProcessor</kbd> class, includes variables and methods for handling retrieving and releasing the captured frames:</p>
<pre>extension ImageProcessor{<br/>    <br/>    var isProcessingImage : Bool{<br/>        get{<br/>            self.lock.lock()<br/>            defer {<br/>                self.lock.unlock()<br/>            }<br/>            return _processingImage<br/>        }<br/>        set(value){<br/>            self.lock.lock()<br/>            _processingImage = value<br/>            self.lock.unlock()<br/>        }<br/>    }<br/><br/>    var isFrameAvailable : Bool{<br/>        get{<br/>            self.lock.lock()<br/>            let frameAvailable =<br/>                    self.frames.count &gt; 0<br/>            self.lock.unlock()<br/>            return frameAvailable<br/>        }<br/>    }<br/>    <br/>    public func addFrame(frame:CIImage){<br/>        self.lock.lock()<br/>        self.frames.append(frame)<br/>        self.lock.unlock()<br/>    }<br/>    <br/>    public func getNextFrame() -&gt; CIImage?{<br/>        self.lock.lock()<br/>        let frame = self.frames.removeFirst()<br/>        self.lock.unlock()<br/>        return frame<br/>    }<br/>    <br/>    public func reset(){<br/>        self.lock.lock()<br/>        self.frames.removeAll()<br/>        self.processedImages.removeAll()<br/>        self.processedMasks.removeAll()<br/>        self.lock.unlock()<br/>    }<br/>}</pre>
<p>Although fairly verbose, it should all be self-explanatory; probably the only method worth outlying is the method <kbd>addFrame</kbd>, which is called each time a frame is captured by the camera. To give some bearing of how everything is tied together, the following diagram illustrates the general flow whilst capturing frames:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/a160b4da-708f-4a9c-a037-8009ce7a17ac.png" style="width:47.08em;height:21.58em;"/></div>
<p>The details of the flow are covered in the following points: </p>
<ol>
<li>Although capturing of the frames is persistent throughout the lifetime of the <kbd>CameraViewController</kbd>, they are only passed to the <kbd>ImageProcessor</kbd> once flagged once the user taps (and holds) their finger on the action button</li>
<li>During this time, each frame that is captured (at the throttled rate—currently 10 frames per second) is passed to the <kbd>CameraViewController</kbd></li>
<li>This subsequently passes it to the <kbd>ImageProcessor</kbd> using the <kbd>addFrame</kbd> method shown earlier</li>
<li>The capturing stops when the user lifts their finger from the action button and, once finished, the <kbd>EffectsViewController</kbd> is instantiated and presented, along with passing it a reference to the <kbd>ImageProcessor</kbd> with the reference to the captured frames</li>
</ol>
<p>The next part of the <kbd>ImageProcessor</kbd> class is responsible for processing each of these images; this is initiated using the <kbd>processFrames</kbd> method, which is called by the <kbd>EffectsViewController</kbd> once it has loaded. This part has a lot more code, but most of it should be familiar to you as it's the boilerplate code we've used in many of the projects during the course of this book. Let's start by inspecting the <kbd>processFrames</kbd> method, as shown in the following snippet:</p>
<div class="packt_infobox">All of the remaining code is assumed to be inside the <kbd>ImageProcessor</kbd> class for the rest of this chapter unless stated otherwise; that is, the class and class extension declaration will be omitted to make the code easier to read.</div>
<pre>public func processFrames(){<br/>    if !self.isProcessingImage{<br/>        DispatchQueue.global(qos: .background).async {<br/>            self.processesingNextFrame()<br/>        }<br/>    }<br/>}</pre>
<p>This method simply dispatches the method call <kbd>processingNextFrame</kbd> to the background thread. This is mandatory when performing inference with Core ML and also a good practice when performing compute-intensive tasks to avoid locking up the user interface. Let's continue the trail by inspecting the <kbd>processingNextFrame</kbd> method along with the method responsible for returning an instance of a <kbd>VNCoreMLRequest</kbd>, which is shown in the following code snippet:</p>
<pre>func getRequest() -&gt; VNCoreMLRequest{<br/>    let request = VNCoreMLRequest(<br/>        model: self.model,<br/>        completionHandler: { [weak self] request, error in<br/>            <strong>self?.processRequest(for: request, error: error)</strong><br/>    })<br/>    request.imageCropAndScaleOption = .centerCrop<br/>    return request<br/>}<br/><br/>func processesingNextFrame(){<br/>    self.isProcessingImage = true<br/>    <br/>    guard let nextFrame = self.getNextFrame() else{<br/>        self.isProcessingImage = false<br/>        return<br/>    }<br/>    <br/>    var ox : CGFloat = 0<br/>    var oy : CGFloat = 0<br/>    let frameSize = min(nextFrame.extent.width, nextFrame.extent.height)<br/>    if nextFrame.extent.width &gt; nextFrame.extent.height{<br/>        ox = (nextFrame.extent.width - nextFrame.extent.height)/2<br/>    } else if nextFrame.extent.width &lt; nextFrame.extent.height{<br/>        oy = (nextFrame.extent.height - nextFrame.extent.width)/2<br/>    }<br/>    guard let frame = nextFrame<br/>        .crop(rect: CGRect(x: ox,<br/>                           y: oy,<br/>                           width: frameSize,<br/>                           height: frameSize))?<br/>        .resize(size: targetSize) else{<br/>            self.isProcessingImage = false<br/>            return<br/>    }<br/>    <br/>    self.processedImages.append(frame)<br/>    let handler = VNImageRequestHandler(ciImage: frame)<br/>    <br/>    do {<br/>        try handler.perform([self.getRequest()])<br/>    } catch {<br/>        print("Failed to perform classification.\n\(error.localizedDescription)")<br/>        self.isProcessingImage = false<br/>        return<br/>    }<br/>}</pre>
<p>We start off by setting the property <kbd>isProcessingImage</kbd> to <kbd>true</kbd> and checking that we have a frame to process, otherwise exiting early from the method. </p>
<p>The following might seem a little counter-intuitive (because it is); we have seen from previous chapters that <kbd>VNCoreMLRequest</kbd> handles the preprocessing task of resizing the cropping of our images. So, why are we doing it manually here? The reason has more to do with keeping the code simpler and meeting publishing deadlines. In this example, the final image is composited using the resized frames to avoid scaling and offsetting the output from the model, which I'll leave as an exercise for you. So here, we are performing that operation and persisting the result in the array <kbd>processedImages</kbd> to be used in the final stage. Finally, we execute the request, passing in the image, which calls our method <kbd>processRequest</kbd> once finished, passing in the results from the model. </p>
<p>Continuing on our trail, we will now inspect the <kbd>processRequest</kbd> method; as this method is quite long, we will break it down into chunks, working top to bottom:</p>
<pre>func processRequest(for request:VNRequest, error: Error?){<br/>    self.lock.lock()<br/>    let framesReaminingCount = self.frames.count<br/>    let processedFramesCount = self.processedImages.count<br/>    self.lock.unlock()<br/>    <br/><strong>    ...</strong><br/>}</pre>
<p>We start off by getting the latest counts, which will be broadcast to the delegate when this method finishes or fails. Talking of which, the following block verifies that a result was returned of type <kbd>[VNPixelBufferObservation]</kbd>, otherwise notifying the delegate and returning, as shown in the following snippet:</p>
<pre> func processRequest(for request:VNRequest, error: Error?){<br/><strong>    ...</strong> <br/><br/>    guard let results = request.results,<br/>        let pixelBufferObservations = results as? [VNPixelBufferObservation],<br/>        pixelBufferObservations.count &gt; 0 else {<br/>            print("ImageProcessor", #function, "ERROR:",<br/>                  String(describing: error?.localizedDescription))<br/>            <br/>            self.isProcessingImage = false<br/>            <br/>            DispatchQueue.main.async {<br/>                self.delegate?.onImageProcessorFinishedProcessingFrame(<br/>                    status: -1,<br/>                    processedFrames: processedFramesCount,<br/>                    framesRemaining: framesReaminingCount)<br/>            }<br/>            return<br/>    }<br/><br/><strong>    ...</strong><br/>}</pre>
<p>With reference to our result (<kbd>CVBufferPixel</kbd>), our next task is to create an instance of <kbd>CIImage</kbd>, passing in the buffer and requesting the color space to be grayscale to ensure that a single channel image is created. Then, we will be adding it to our <kbd>processedMasks</kbd> array, shown in the following snippet: </p>
<pre>func processRequest(for request:VNRequest, error: Error?){<br/><strong>    ...</strong><br/>    <br/><strong>    let options = [</strong><br/><strong>        kCIImageColorSpace:CGColorSpaceCreateDeviceGray()</strong><br/><strong>        ] as [String:Any]</strong><br/>    <br/>    let ciImage = CIImage(<br/>        cvPixelBuffer: pixelBufferObservations[0].pixelBuffer,<br/>        options: options)<br/>    <br/>    self.processedMasks.append(ciImage)<br/>    <br/><strong>    ...</strong><br/>}</pre>
<p>Only two more things left to do! We notify the delegate that we have finished a frame and proceed to process the next frame, if available:</p>
<pre>func processRequest(for request:VNRequest, error: Error?){<br/><strong>    ...</strong> <br/>    <br/>    DispatchQueue.main.async {<br/>        self.delegate?.onImageProcessorFinishedProcessingFrame(<br/>            status: 1,<br/>            processedFrames: processedFramesCount,<br/>            framesRemaining: framesReaminingCount)<br/>    }<br/>    <br/>    if self.isFrameAvailable{<br/>        self.processesingNextFrame()<br/>    } else{<br/>        self.isProcessingImage = false<br/>    }<br/>}</pre>
<p>This concludes the third part of our <kbd>ImageProcessor</kbd>; at this point, we have two arrays containing the resized captured frames and the segmented images from our model. Before moving on to the final part of this class, let's get a bird's-eye view of what we just did, illustrated in this flow diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/e5a57ffb-0718-4d56-837f-d809613819f8.png" style="width:45.50em;height:31.08em;"/></div>
<p><span>The details of the flow are shown in the following points: </span></p>
<ol>
<li>As mentioned in the preceding diagram, processing is initiated once the <kbd>EffectsViewController</kbd> is loaded, which kicks off the background thread to process each of the captured frames</li>
<li>Each frame is first resized and cropped to match the output of the model</li>
<li>Then, it is added to the <kbd>processedFrames</kbd> array and passed to our model for interference (segmentation)</li>
<li>Once the model returns with the result, we instantiate a single color instance of <kbd>CIImage</kbd></li>
<li>This instance is stored in the array <kbd>processedMasks</kbd> and the delegate is notified of the progress</li>
</ol>
<p>What happens when all frames have been processed? This is what we plan on answering in the next part, where we will discuss the details of how to create the effect. To start with, let's discuss how the process is initiated. </p>
<p><span>Once the delegate (<kbd>EffectsViewController</kbd>) receives a callback, using <kbd>onImageProcessorFinishedProcessingFrame</kbd>, where all of the frames have been processed, it calls the <kbd>compositeFrames</kbd> method from the ImageProcessor to start the process of creating the effect. Let's review this and the existing code within this part of the <kbd>ImageProcessor</kbd> class:</span></p>
<pre>func compositeFrames(){<br/>    <br/>    <strong>var selectedIndicies = self.getIndiciesOfBestFrames()</strong><br/>    if selectedIndicies.count == 0{<br/>        DispatchQueue.main.async {<br/>            self.delegate?.onImageProcessorFinishedComposition(<br/>                status: -1,<br/>                image: self.processedImages.last!)<br/>        }<br/>        return<br/>    }<br/>    <br/>    <strong>var finalImage = self.processedImages[selectedIndicies.last!]<br/></strong>    <strong>selectedIndicies.removeLast()<br/></strong><br/>    <strong>// TODO Composite final image using segments from intermediate frames</strong><br/>    DispatchQueue.main.async {<br/>        self.delegate?.onImageProcessorFinishedComposition(<br/>            status: 1,<br/>            image: finalImage)<br/>    }<br/>}<br/><br/><strong>func getIndiciesOfBestFrames() -&gt; [Int]{</strong><br/><strong>    // TODO; find best frames for the sequence i.e. avoid excessive overlapping</strong><br/><strong>    return (0..&lt;self.processedMasks.count).map({ (i) -&gt; Int in</strong><br/><strong>        return i</strong><br/><strong>    })</strong><br/><strong>}</strong><br/><br/><strong>func getDominantDirection() -&gt; CGPoint{</strong><br/><strong>    var dir = CGPoint(x: 0, y: 0)</strong><br/><strong>    // TODO detected dominate direction</strong><br/><strong>    return dir</strong><br/><strong>}</strong></pre>
<p>I have bolded the important/interesting parts, essentially the parts we will be implementing, but before writing any more code, let's review what we currently have (in terms of processed images) and an approach to creating our effect.</p>
<p>At this stage, we have an array of <kbd>processedFrames</kbd> that contains the resized and cropped versions of the captured images, and we have another array, <kbd>processedMasks</kbd>, containing the single-channel images from our segmentation model. Examples of these are shown in the following figure: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/cb47ab2e-1b0b-46c4-a923-a9aa455d214e.png"/></div>
<p>If we were to composite each of the frames as they are, we would end up with a lot of unwanted artifacts and excessive overlapping. One approach could be to adjust the frames that have been processed (and possibly captured), that is, skip every <em>n</em> frames to spread out the frames. The problem with this approach is that it assumes all subjects will be moving at the same speed; to account for this, you would need to expose this tuning to the user for manual adjustment (which is an reasonable approach). The approach we will take here will be to extract the bounding box for each of the frames, and using the displacement and relative overlap of these to determine when to insert a frame and when to skip a frame.</p>
<p>To calculate the bounding box, we simply scan each line from each of the edges of the image, that is, from <strong>top to bottom</strong>, to determine the top of the object. Then, we do it <strong>bottom to top</strong> to determine the bottom of the <span>object.</span> Similarly, we do it on the horizontal axis, illustrated in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/97e10b70-a9aa-4fe0-9eec-e4d1fa82e31f.png"/></div>
<p>Even with bounding boxes, we still need to determine how far the object should move before inserting a frame. To determine this, we first determine the dominant direction, which is calculated by finding the direction between the first and last frames of the segmented object. This is then used to determine what axis to compare displacement on; that is, if the dominant direction is in the horizontal axis (as shown in the preceding figure), then we measure the displacement across the <em>x </em>axis, ignoring the <em>y </em>axis. We then simply measure the distance between the frames against some predetermined threshold to decide whether to composite the frame or ignore it. This is illustrated in the following figure:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c90dd24b-e51f-4311-b6ad-b04afca724b6.png"/></div>
<p>Let's see what this looks like in code, starting from determining the dominant direction. Add the following code to the <kbd>getD<span>ominant</span>Direction</kbd> method:</p>
<pre>var dir = CGPoint(x: 0, y: 0)<br/><br/>var startCenter : CGPoint?<br/>var endCenter : CGPoint?<br/><br/><strong>// Find startCenter</strong><br/>for i in 0..&lt;self.processedMasks.count{<br/>    let mask = self.processedMasks[i]<br/>    <br/>    guard let maskBB = mask.getContentBoundingBox(),<br/>    (maskBB.width * maskBB.height) &gt;=<br/>        (mask.extent.width * mask.extent.height) * self.minMaskArea<br/>    else {<br/>        continue<br/>    }<br/>    <br/>    startCenter = maskBB.center<br/>    break<br/>}<br/><br/><strong>// Find endCenter</strong><br/>for i in (0..&lt;self.processedMasks.count).reversed(){<br/>    let mask = self.processedMasks[i]<br/>    <br/>    guard let maskBB = mask.getContentBoundingBox(),<br/>    (maskBB.width * maskBB.height) &gt;=<br/>        (mask.extent.width * mask.extent.height) * self.minMaskArea<br/>    else {<br/>        continue<br/>    }<br/>    <br/>    endCenter = maskBB.center<br/>    break<br/>}<br/><br/>if let startCenter = startCenter, let endCenter = endCenter, startCenter != endCenter{<br/>    dir = (startCenter - endCenter).normalised<br/>}<br/><br/>return dir</pre>
<p>As described earlier, we first find the bounding boxes of the start and end of our sequence of frames, and use their centers to calculate the dominate direction. </p>
<div class="packt_infobox">The implementation of the <kbd>CIImage</kbd> method <kbd>getContentBoundingBox</kbd> is omitted here, but it can be found in the accompanying the source code within the <kbd>CIImage+Extension.swift</kbd> file.</div>
<p>Armed with the dominant direction, we can now proceed with determining what frames to include and what frames to ignore. We will implement this in the method <kbd>getIndiciesOfBestFrames</kbd> of the <kbd>ImageProcessor</kbd> class, which iterates over all frames, measuring the overlap and ignoring those that don't meet a specific threshold. The method returns an array of indices that satisfy this threshold to be composited onto the final image. Add the following code to the <kbd>getIndiciesOfBestFrames</kbd> method:</p>
<pre>var selectedIndicies = [Int]()<br/>var previousBoundingBox : CGRect?<br/>let dir = self.getDominateDirection()<br/><br/>for i in (0..&lt;self.processedMasks.count).reversed(){<br/>    let mask = self.processedMasks[i]<br/>   guard let maskBB = mask.getContentBoundingBox(),<br/>        maskBB.width &lt; mask.extent.width * 0.7,<br/>        maskBB.height &lt; mask.extent.height * 0.7 else {<br/>        continue<br/>    }<br/>    <br/>    if previousBoundingBox == nil{<br/>        previousBoundingBox = maskBB<br/>        selectedIndicies.append(i)<br/>    } else{<br/>        let distance = abs(dir.x) &gt;= abs(dir.y)<br/>            ? abs(previousBoundingBox!.center.x - maskBB.center.x)<br/>            : abs(previousBoundingBox!.center.y - maskBB.center.y)<br/>        let bounds = abs(dir.x) &gt;= abs(dir.y)<br/>            ? (previousBoundingBox!.width + maskBB.width) / 4.0<br/>            : (previousBoundingBox!.height + maskBB.height) / 4.0<br/>        <br/>        if distance &gt; bounds * 0.5{<br/>            previousBoundingBox = maskBB<br/>            selectedIndicies.append(i)<br/>        }<br/>    }<br/>    <br/>}<br/><br/>return selectedIndicies.reversed()</pre>
<p>We begin by getting the dominant direction, as discussed earlier, and then proceed to iterate through our sequence of frames in reverse order (reverse as it is assumed that the user's hero<em> </em>shot is the last frame). With each frame, we obtain the bounding box, and if it's the first frame to be checked, we assign it to the variable <kbd>previousBoundingBox</kbd>. This will be used to compare subsequent bounding boxes (and updated to the latest included frame). If <kbd>previousBoundingBox</kbd> is not null, then we calculate the displacement between the two based on the dominant direction, as shown in the following snippet: </p>
<pre>let distance = abs(dir.x) &gt;= abs(dir.y)<br/>    ? abs(previousBoundingBox!.center.x - maskBB.center.x)<br/>    : abs(previousBoundingBox!.center.y - maskBB.center.y) </pre>
<p>We then calculate the minimum length needed to separate the two objects, which is calculated by the combined size of the relative axis divided by 2. This gives us a distance of half of the combined frame, as shown in the following snippet: </p>
<pre>let bounds = abs(dir.x) &gt;= abs(dir.y)<br/>    ? (previousBoundingBox!.width + maskBB.width) / 2.0<br/>    : (previousBoundingBox!.height + maskBB.height) / 2.0</pre>
<p>We then compare the distance with the bounds along with a threshold and proceed to add the frame to the current index if the distance satisfies this threshold:</p>
<pre>if distance &gt; bounds * 0.15{<br/>    previousBoundingBox = maskBB<br/>    selectedIndicies.append(i)<br/>}</pre>
<p>Returning to the <kbd>compositeFrames</kbd> method, we are now ready to composite the selected frames. To achieve this, we will leverage <kbd>CoreImages</kbd> filters; but before doing so, let's quickly review what it is exactly that we want to achieve.</p>
<p>For each selected (processed) image and mask pair, we want to clip out the image and overlay it onto the final image. To improve the effect, we will apply a progressively increasing alpha so that frames closer to the final frame will have an opacity closer to 1.0 while the frames further away will be progressively transparent; this will give us a faded trailing effect. This process is summarized in the following figure:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/9a61021e-de1e-4cd3-8f29-eb8ab88899df.png" style="width:43.08em;height:25.08em;"/></div>
<p>Let's turn this into code by first implementing the filter. As shown earlier, we will be passing the kernel the output image, the overlay and its corresponding mask, and an alpha. Near the top of the <kbd>ImageProcessor</kbd> class, add the following code:</p>
<pre>lazy var compositeKernel : CIColorKernel? = {<br/>    let kernelString = """<br/>        kernel vec4 compositeFilter(<br/>            __sample image,<br/>            __sample overlay,<br/>            __sample overlay_mask,<br/>            float alpha){<br/>            float overlayStrength = 0.0;<br/><br/>            if(overlay_mask.r &gt; 0.0){<br/>                overlayStrength = 1.0;<br/>            }<br/><br/>            overlayStrength *= alpha;<br/>            <br/>            return vec4(image.rgb * (1.0-overlayStrength), 1.0)<br/>                + vec4(overlay.rgb * (overlayStrength), 1.0);<br/>        }<br/>    """<br/>    return CIColorKernel(source:kernelString)<br/>}()</pre>
<p>Previously, we have implemented the <kbd>CIColorKernel</kbd>, which is responsible for compositing all of our frames onto the final image as discussed. We start by testing the mask's value, and if it is 1.0, we assign the strength 1.0 (meaning we want to replace the color at that location of the final image with that of the overlay). Otherwise, we assign 0, ignoring it. Then, we multiply the strength with the blend argument passed to our kernel. Finally, we calculate and return the final color with the statement <kbd>vec4(image.rgb * (1.0-overlayStrength), 1.0) + vec4(overlay.rgb * (overlayStrength), 1.0)</kbd>. With our filter now implemented, let's return the <kbd>compositeFrames</kbd> method and put it to use. Within <kbd>compositeFrames</kbd>, replace the comment <kbd>// TODO Composite final image using segments from intermediate frames</kbd> with the following code:</p>
<pre>let alphaStep : CGFloat = 1.0 / CGFloat(selectedIndicies.count)<br/><br/>for i in selectedIndicies{<br/>    let image = self.processedImages[i]<br/>    let mask = self.processedMasks[i]<br/>    <br/>    let extent = image.extent<br/>    let alpha = CGFloat(i + 1) * alphaStep<br/>    let arguments = [finalImage, image, mask, min(alpha, 1.0)] as [Any]<br/>    if let compositeFrame = self.compositeKernel?.apply(extent: extent, arguments: arguments){<br/>        finalImage = compositeFrame<br/>    }<br/>}</pre>
<p>Most of this should be self-explanatory; we start by calculating an alpha stride that will be used to progressively increase opacity as we get closer to the final frame. We then iterate through all the selected frames, applying the filter we just implemented in the preceding snippet, compositing our final image.</p>
<p>With that done, we have now finished this method and the coding for this chapter. Well done! It's time to test it out; build and run the project to see your hard work in action. The following is a result from a weekend park visit:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/a4caad02-de67-4097-bdf1-90c31c307d45.png" style="width:25.42em;height:49.17em;"/></div>
<p>Before wrapping up this chapter, let's briefly discuss some strategies when working with machine learning models. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with probabilistic results</h1>
                </header>
            
            <article>
                
<p>As alluded to at the beginning of this chapter and seen firsthand in the previous section, working with machine learning models requires a set of new techniques and strategies to deal with uncertainty. The approach <span>taken </span>will be domain-specific, but there are some broad <span>strategies that are worth keeping in mind, and that's what we will cover in this section in the context of the example project of this chapter. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving the model</h1>
                </header>
            
            <article>
                
<p>The first is improving the model. Of course, there may be limitations depending on the source of the model and dataset, but it's important to be able to understand ways in which the model can be improved as its output directly correlates to the quality of the user experience.</p>
<p>In the context of this project, we can augment the model using an existing pre-trained image classifier as the encoder, as mentioned earlier. This not only fast-tracks training, providing more opportunities to iterate, but also is likely to improve performance by having the model transfer existing knowledge from a more comprehensive dataset. </p>
<p>Another is tuning the dataset that the model was trained on. A simple, and relevant, example of how the model can be improved can be seen by any image in which the user is holding an object (which has been labeled). An example of this can be seen in the following figure, in which the guitar is cropped from the person:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/f869a3f5-a10a-40ed-bc75-1b1eeeb40f81.png" style="width:41.67em;height:19.75em;"/> </div>
<p>How you deal with this is dependent on the desired characteristics of your model. In the context of the application presented in this chapter, it would make sense to either perform multi-class classification, including objects normally held by people, or including them in the mask. </p>
<p>Another common technique is <strong>data augmentation</strong>. This is where you artificially adjust the image (input) to increase variance in your dataset, or even adjust it to make it more aligned with the data for your particular use case. Some example augmentations include blurring (useful when dealing with fast moving objects), rotation, adding random noise, color adjustment - essentially any image manipulation effect that introduces nuances that you are likely to get in the real world.</p>
<p>Of course, there are many more techniques and tools to improve the model and data; here, our intention is just to highlight the main areas rather than delve into the details. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Designing in constraints </h1>
                </header>
            
            <article>
                
<p>This is somewhat unavoidable and how we design intelligent interfaces is still in its infancy. That is, how much transparency do you expose to the user? And how do you effectively assist them in building a useful mental model of your system without being distracting or losing the convenience of using the model in the first place? But here, I am simply referring to designing constraints within the experience to increase the chances of the model's success. A great, although slightly unrelated, example of this is household robots and the dishwasher. Despite its non-robotic characteristics, the faithful dishwasher can be considered a first generation robot for household tasks, like Rosie from Jetsons. Unlike Rosie, however, we had not been able to get the dishwasher to adapt to our environment. So, we adapted the environment for the dishwasher, that is, we encapsulated it in a box environment rather than using the existing kitchen sink we're accustomed to.</p>
<p>One simple approach is making the user aware of how to achieve the best results; in this example, it could be as simple as asking them to use a wall as their background. These hints can be delivered before use or delivered when there are signs of poor performance (or both). One approach for automatically detecting poor performance would be to measure the bounding box and its center of mass, comparing it with the expected center of mass, as illustrated in the following figure:</p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><img src="assets/0710dcbb-d0fd-4cb3-8899-36bd5b0f93df.png"/></div>
<p>Which brings us nicely to the next strategy: embedding heuristics. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Embedding heuristics</h1>
                </header>
            
            <article>
                
<p>Heuristics is essentially codifying rules you have in your head to solve a particular task, typically implemented using a function that returns a score. This is used to rank a set of alternatives. In the previous section, <em>Designing in constraints</em>, we saw how we could use the center of mass and the bounding box to determine how well distributed the pixels are for a segmented image. This in turn could be used to rank each frame, by favoring those with a center of mass near the center of the bounding box. We also implemented a type of heuristic in the application when determining which frames to keep and which ones to ignore by measuring the overlap. </p>
<p>Heuristics can be a powerful ally, but be careful to ensure that the heuristics you derive can generalize well to your problem, just as you would expect a good model to. Also, be mindful of the additional computational cost incurred from using them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Post-processing and ensemble techniques</h1>
                </header>
            
            <article>
                
<p>Techniques from image processing, computer vision, and computer graphics can be borrowed to improve the quality of the output as well as detection. As an example, a typical image processing task is performing the morphology operations of opening and closing. This combination is commonly used to remove noise and fill in small holes in a binary image. Another useful post-processing task we could borrow from computer vision is watersheds, a segmentation technique that treats the image as a topographical map, where the intensity of change defines the ridges and the boundary of the fill (or segmentation).</p>
<p>Another tool to use for post-processing is another model. You're familiar with YOLO for object detection. We can apply it to obtain its predicted boundaries of the object, which we can then use to refine our segmentation. Another model, and one being adopted for this task, is <strong>conditional random fields</strong> (<strong>CRF</strong>), which is capable of smoothing out the edges of our mask. </p>
<p>There are a vast number of techniques available from the fields of image processing, computer vision, and computer graphics, and I strongly encourage you to explore each area to build up your tool set.</p>
<div class="packt_infobox">If you are new to computer vision, then I recommend the books <em>Computer Vision and Image Processing</em> by T. Morris and <em>Algorithms for Image Processing and Computer Vision</em> by J. Parker for a pragmatic introduction to the field.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Human assistance</h1>
                </header>
            
            <article>
                
<p>Sometimes it's unavoidable or even desirable to include the human in tuning the output from the model. In these instances, the model is used to assist the user rather than completely automating the task. A few approaches that could be employed for the project in this chapter include the following: </p>
<ul>
<li>Provide an intermediate step where the user can tidy up the masks. By this, I mean allowing the user to erase parts of the mask that have been incorrectly classified or are unwanted by the user.</li>
<li>Present the user with a series of frames and have them select the frames for composition.</li>
<li>Present the user with variations of the final composited image and have them select the one with the most appeal.</li>
</ul>
<p>Another related concept is introducing human-in-the-loop machine learning. This has a human intervening when the model is not confident in its prediction, and it passes the responsibility over to the user for classification and/or correction. The amendments from the user are then stored and used for training the model to improve performance. In this example, we could let the user (or crowd-source this task) segment the image and use this data when re-training the model. Eventually, given sufficient data, the model will improve its performance relevant to the context it is being used in.</p>
<p>I hope this section highlighted the importance of handling uncertainty when working with machine models and provided enough of a springboard so that you can approach designing intelligent applications from the perspectives outlined here. Let's now conclude this chapter by reviewing what we have covered.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced the concept of semantic segmentation, an approach that gives our applications increased perceptual understanding of our photos and videos. It works by training a model to assign each pixel to a specific class. One popular architecture for this is U-Net, which achieves high-precision localization by preserving spatial information, by bridging the convolutional layers. We then reviewed the data used for training along with some example outputs of the model, including examples that highlight the limitations of the model.</p>
<p>We then saw how this model could be used by creating an image effects application, where the segmented images were used to clip people from a series of frames and composite them together to create an action shot. But this is just one example of how semantic segmentation can be applied; it's frequently used in domains such as robotics, security surveillance, and quality assurance in factories, to name a few. How else it can be applied is up to you.</p>
<p>In the final section, we spent some time discussing strategies when dealing with models, specifically, their probabilistic (or level of uncertainty) outputs, to improve user experience. </p>
<p>This is the last of our examples of applying machine learning. In the next, and last, chapter, we'll shift gears and provide a primer into building your own models with the help of Create ML. Let's get started.</p>


            </article>

            
        </section>
    </body></html>