<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;Regression based learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Regression based learning</h1></div></div></div><p>Regression analysis allows us to mathematically model the relationship between two variables using simple algebra. In this chapter, we will focus on covering another supervised learning technique: regression analysis or regression-based learning. In the previous chapter, we covered the basics of statistics that will be of use in this chapter. We will start with understanding how multiple variables can influence the outcome, and how statistical adjustment techniques can be used to arbitrate this influence, understand correlation and regression analysis using real world examples, and take a deep dive into concepts such as confounding and effect modification.</p><p>You will learn the basic and advanced concepts of this technique and get hands-on implementation guidance in simple, multiple linear regression, polynomial regression and logistic regression using Apache Mahout, R, Julia, Apache Spark, and Python.</p><p>At the end of this chapter, readers will have understood the uses and limitations of regression models, learned how to fit linear and logistic regression models to data, statistically inferencing the results and finally, assessing and diagnosing the performance of the models.</p><p>The following diagram depicts different learning models covered in this book, and the techniques highlighted in orange will be dealt in detail in this chapter:</p><div class="mediaobject"><img src="graphics/B03980_10_01.jpg" alt="Regression based learning"/></div><p>The topics listed here are covered in depth in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introduction to correlation and regression analysis; revision of additional statistical concepts such as covariance and correlation coefficients. We will cover the properties of expectation, variance, and covariance in the context of regression models and ANOVA model and diagnostics</li><li class="listitem" style="list-style-type: disc">You will learn simple, linear and multiple linear regressions: linear relationships, linear models, basic assumptions (normality, homoscedasticity, linearity, and independence), and least squares estimation. Overall, you will learn model diagnostics and selection.</li><li class="listitem" style="list-style-type: disc">You will be presented with an overview of generalized linear models (GLMs) and a listing of the regression algorithms under GLM. Also, the phenomena of confounding and effect modification will be presented, and hence realization and adjustments for the same.</li><li class="listitem" style="list-style-type: disc">An introduction to logistic regression, understanding odds and risk ratios, model building logistic regression models, and assessing the same will be covered.</li><li class="listitem" style="list-style-type: disc">Sample implementation using Apache Mahout, R, Apache Spark, Julia, and Python (scikit-learn) libraries and modules will also be covered.</li></ul></div><div class="section" title="Regression analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec48"/>Regression analysis</h1></div></div></div><p>Under supervised learning <a id="id1029" class="indexterm"/>techniques, the learning models that are categorized under statistical methods are instance-based learning methods, Bayesian learning methods, and regression analysis. In this chapter, we will focus on regression analysis and other related regression models. Regression analysis is known to be one of the most important statistical techniques. As mentioned, it is a statistical methodology that is used to measure the relationship and check the validity and strength of the relationship between two or more variables.</p><p>Traditionally, researchers, analysts, and traders have been using regression analysis to build trading strategies to understand the risk contained in a portfolio. Regression methods are used to address both classification and prediction problems.</p><p>We have <a id="id1030" class="indexterm"/>covered some key statistical concepts in the earlier chapter; in this chapter, we will cover some more concepts that are quite relevant in the context of regression analysis. To name a few concepts, we have the measurement of variability, linearity, covariance, coefficients, standard errors, and more.</p><div class="mediaobject"><img src="graphics/B03980_10_02.jpg" alt="Regression analysis"/></div><div class="section" title="Revisiting statistics"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec113"/>Revisiting statistics</h2></div></div></div><p>In the earlier chapter, where we learned the<a id="id1031" class="indexterm"/> Bayesian learning methods, we covered some core statistical measures such as mean, median, mode, and standard deviation. Let's now extend this to some more measures such as variance, covariance, correlation, and the first and second moments of the distribution of a random variable.</p><div class="mediaobject"><img src="graphics/B03980_10_03.jpg" alt="Revisiting statistics"/></div><p>
<span class="strong"><strong>Variance</strong></span> <a id="id1032" class="indexterm"/>is the square <a id="id1033" class="indexterm"/>of standard deviation. If you recollect what standard deviation is, it is an average measure of how much each measurement in the sample deviates from the mean. It is also called the standard deviation of the mean. We can theoretically compute standard deviations for mode and median.</p><p>
<span class="strong"><strong>The range</strong></span> is <a id="id1034" class="indexterm"/>defined as a span of values over which the dataset is spread. Range usually is represented as minimum and maximum values.</p><p>Quartiles, <a id="id1035" class="indexterm"/>deciles, and <a id="id1036" class="indexterm"/>percentiles<a id="id1037" class="indexterm"/> subdivide a distribution of measurements that are similar to the median. The median is known to divide the distribution into half while quartile, decile, and percentile divide the distribution into 1/25, 1/10 and 1/100 respectively.</p><p>
<span class="strong"><strong>First quartile</strong></span> (designated Q1) OR lower quartile is the 25th percentile.</p><p>
<span class="strong"><strong>Third quartile</strong></span> (designated Q3) OR upper quartile is the 75th percentile.</p><p>interquartile range = third quartile – first quartile</p><p>
<span class="strong"><strong>Symmetric</strong></span> and <span class="strong"><strong>skewed data</strong></span>: Median, mean and <a id="id1038" class="indexterm"/>mode for symmetric, positively, and negatively skewed data is represented here:</p><div class="mediaobject"><img src="graphics/B03980_10_04.jpg" alt="Revisiting statistics"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>Symmetric distribution <a id="id1039" class="indexterm"/>has equal mean and median values. For a positively skewed distribution, the median is greater than the mean and for a negatively skewed distribution, the mean value is greater than the median's value.</p></div></div><p>The outlier is an <a id="id1040" class="indexterm"/>observation that is separated from the main cluster of the data. It can impact measures such as mean in a very significant way. Let's take an example to understand this better. We want to understand the average wealth of a group of five people. Say, the individual assets are valued at USD 1M, USD 1.2M, USD 0.9M, USD 1.1M, and USD 12M.</p><p>1+1.2+0.9+1.1+12=16.2</p><p>16.2/5=3.24</p><p>The last observation had an unrealistic impact on the measurement. Let's now see how the median is impacted. Let's sort the assets in ascending order: 0.9M, 1.0M, 1.1M, 1.2M, and 12M. The median is 1.1M. There are two important concepts we must understand. Outliers influence mean more significantly than the median.</p><p>So, you should check the data carefully before choosing the correct statistical quantity.</p><p>Mean represents the average value of the variable and median represents the value of the average variable.</p><p>
<span class="strong"><strong>Covariance</strong></span>
<a id="id1041" class="indexterm"/> is when there are two or more variables of interest (such as stocks of companies, physical properties of materials, etc.); it becomes important to understand whether there is any relation between them. Precisely, what we want to understand is if one of them is varying how does the other variable vary.</p><p>In statistics, two terms explain this behavior:</p><p>The first one is known as covariance. For a data set comprising <span class="emphasis"><em>n</em></span> points of two variables <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span>, the following equation depicts the computation of covariance:</p><div class="mediaobject"><img src="graphics/B03980_10_05.jpg" alt="Revisiting statistics"/></div><p>However, covariance can be a very large number. It is best to express it as a normalized number between -1 and 1 to understand the relation between the quantities. This is achieved by normalizing covariance with standard deviations of both the variables (<span class="emphasis"><em>sx</em></span> and <span class="emphasis"><em>sy</em></span>).</p><p>This is called <span class="strong"><strong>correlation coefficient</strong></span>
<a id="id1042" class="indexterm"/> between <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span>.</p><div class="mediaobject"><img src="graphics/B03980_10_06.jpg" alt="Revisiting statistics"/></div><p>
<span class="strong"><strong>Correlation</strong></span>
<a id="id1043" class="indexterm"/> measures<a id="id1044" class="indexterm"/> the strength of linear dependence between <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> and lies between -1 and 1. The following graph gives you a visual understanding of how the correlation impacts the linear dependence:</p><div class="mediaobject"><img src="graphics/B03980_10_07.jpg" alt="Revisiting statistics"/></div><p>Before we get into the specifics of various regression models, let's first look at the steps for implementing a regression model and analyzing the results.</p><p>The mean and variances are the <span class="emphasis"><em>first and second moments</em></span> of the probability distribution functions of random variables. They are computed as follows:</p><div class="mediaobject"><img src="graphics/B03980_10_08.jpg" alt="Revisiting statistics"/></div><p>After computing the probability distribution for a given random variable, we will compute the mean variance through simple integration.</p><p>Let's compute all these measures using a real-world example.</p><p>Following is the data<a id="id1045" class="indexterm"/> of the stock prices of three companies (company A, company B, and company C) during a period of 14 days. First, compute the returns using the next formula:</p><p>Returns = (current day's price-yesterday's price)/yesterday's price</p><p>From this return, compute mean, median, and pairwise correlation. Do not use the built-in libraries. Use the base formulae even if you use Excel.</p><div class="mediaobject"><img src="graphics/B03980_10_09.jpg" alt="Revisiting statistics"/></div><p>First, let's compute the returns using the formula given previously.</p><div class="mediaobject"><img src="graphics/B03980_10_10.jpg" alt="Revisiting statistics"/></div><p>If we had to compute <a id="id1046" class="indexterm"/>the mean, the values would be as follows:</p><div class="mediaobject"><img src="graphics/B03980_10_11.jpg" alt="Revisiting statistics"/></div><p>To find the median, we <a id="id1047" class="indexterm"/>will first sort the return values in ascending order and then mark the mid value.</p><div class="mediaobject"><img src="graphics/B03980_10_12.jpg" alt="Revisiting statistics"/></div><p>Finally, let's compute the covariance and then correlations using the formulae given in the previous covariance section.</p><div class="mediaobject"><img src="graphics/B03980_10_13.jpg" alt="Revisiting statistics"/></div><div class="section" title="Properties of expectation, variance, and covariance"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec102"/>Properties of expectation, variance, and covariance</h3></div></div></div><p>Let's combine the <a id="id1048" class="indexterm"/>understanding of the previous and current chapters and conclude them.</p><p>This distribution of a variable is the probability of taking a particular value. The expectation is the population mean (which is the probability of the weighted average).</p><p>We can define a variance and standard deviation of the mean.</p><p>Finally, if we are looking at two different variables, we can define covariance and correlations. Now, let's understand how the expectations and variance of two groups can be computed. This becomes particularly useful in the next sections where we will analyze two variables together for linear regression is given here:</p><p>
<span class="emphasis"><em>E(x+y) = E(x) + E(y)</em></span>
</p><p>
<span class="emphasis"><em>E(x+a) = E(x) + E(a) = a + E(x)</em></span>
</p><p>
<span class="emphasis"><em>E(kx) = kE(x)</em></span>
</p><p>Here is a very interesting rule:</p><div class="mediaobject"><img src="graphics/B03980_10_49.jpg" alt="Properties of expectation, variance, and covariance"/></div><p>Essentially, this <a id="id1049" class="indexterm"/>rule says that if we have a portfolio of properties in a given fraction, then the total expectation is the weighted sum of the individual expectations. This is a crucial concept in the portfolio of analytics. If there is a portfolio of 30% company A, 50%, company B, and 20% company C stocks, the expected return of our portfolio is:</p><p>E (Portfolio) = 0.3 E(Company A) + 0.5 E(Company A) + 0.2 E (Company A)</p><div class="section" title="Properties of variance"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl4sec32"/>Properties of variance</h4></div></div></div><p>Given <span class="emphasis"><em>X</em></span>, a random <a id="id1050" class="indexterm"/>variable:</p><p>
<span class="emphasis"><em>Var(X+Y) = Var(X)+Var(Y)+2Cov(X,Y)</em></span>
</p><p>
<span class="emphasis"><em>V(x+a) = V(x)</em></span> (the variance does not change when a constant is added)</p><p>
<span class="emphasis"><em>V(ax) = a2 V(x)</em></span>
</p><p>Let's prove this as it is not obvious:</p><p>Say, <span class="emphasis"><em>Y= aX</em></span>
</p><p>
<span class="emphasis"><em>E(Y) = an E(X)</em></span> (from the previous set of relations)</p><p>
<span class="emphasis"><em>Y-E(Y) = a(X-E(X))</em></span>
</p><p>Squaring both sides and taking expectations:</p><p>
<span class="emphasis"><em>E(Y-E(Y))<sup>2</sup> = a<sup>2</sup> E(X-E(x))<sup>2</sup></em></span>
</p><p>However, the left-hand side is the variance of <span class="emphasis"><em>Y,</em></span> and the right-hand side is the variance of <span class="emphasis"><em>X</em></span>:</p><p>
<span class="emphasis"><em>Var (Y) = a<sup>2</sup>Var(X)</em></span>
</p><p>Another couple of interesting properties of variance can be derived from the above. It follows directly that</p><p>
<span class="emphasis"><em>Var (-y) = Var (y)</em></span>
</p><p>Let's now look at the variance of the portfolio:</p><div class="mediaobject"><img src="graphics/B03980_10_50.jpg" alt="Properties of variance"/></div><p>So, if you have a portfolio of three stocks, the variance of your portfolio (or the standard deviation that is its square root) varies as shown previously. The standard deviation is often called the risk of the portfolio. Ideally, it needs to be as low as possible. From the previous formula, this can be done in two ways:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">By selecting<a id="id1051" class="indexterm"/> the elements whose variance is very low</li><li class="listitem">By selecting the elements whose covariance is very negative</li></ol></div><p>This is a crucial approach to a successful investment.</p></div><div class="section" title="Properties of covariance"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl4sec33"/>Properties of covariance</h4></div></div></div><p>Following are the <a id="id1052" class="indexterm"/>properties of covariance:</p><p>
<span class="emphasis"><em>cov(X, Y) = E[XY] − E[X]E[Y]</em></span>
</p><p>
<span class="emphasis"><em>cov(x, a) = 0</em></span>
</p><p>
<span class="emphasis"><em>cov(x, x) = var(x)</em></span>
</p><p>
<span class="emphasis"><em>cov(y, x) = cov(x, y)</em></span>
</p><p>
<span class="emphasis"><em>cov(ax, by) =abcov(x, y)</em></span>
</p><p>
<span class="emphasis"><em>cov(X+a, Y+b) = cov (X, Y)</em></span>
</p><p>
<span class="emphasis"><em>cov(aX+bY, cW+dV) = accov(X,W) + adcov(X,V) + bccov(Y,W) + bdcov(Y,V)</em></span>
</p><p>
<span class="emphasis"><em>Cor(X,Y) = E[XY]/σXσY</em></span>
</p><p>Let's now see this using a real-world example.</p></div><div class="section" title="Example"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl4sec34"/>Example</h4></div></div></div><p>Two of your best <a id="id1053" class="indexterm"/>friends, Ana and Daniel, are planning to invest in stock markets. As you are the most experienced investor in your friends circle, they approached you for advice. You know Daniel can handle a 10% risk whereas Ana wants the least possible risk. You obviously want to maximize the returns for both. They both want to invest in three items: gold bonds, a top IT company, and a top bank.</p><div class="mediaobject"><img src="graphics/B03980_10_14.jpg" alt="Example"/><div class="caption"><p>SD—Standard Deviation</p></div></div><p>Correlations can be computed as follows:</p><div class="mediaobject"><img src="graphics/B03980_10_15.jpg" alt="Example"/></div><p>Now, let's derive the advice systematically.</p><p>Let's first create a list <a id="id1054" class="indexterm"/>of all the possible weights (assuming you need to compute up to a single decimal point) for three assets. There can be approximately 66 values possible. This means that our friends must pick from one of these choices to invest. Now, calculate the returns for each possible portfolio (a unique combination of weights) using the following formula (again use any language you like):</p><p>
<span class="emphasis"><em>Return from portfolio = W</em></span><sub>g</sub><span class="emphasis"><em> X R</em></span><sub>g</sub><span class="emphasis"><em> + W</em></span><sub>i</sub><span class="emphasis"><em> X R</em></span><sub>i</sub><span class="emphasis"><em> + w</em></span><sub>b</sub><span class="emphasis"><em> X R</em></span><sub>b</sub><span class="emphasis"><em>W</em></span><sub>g</sub></p><p>
<span class="emphasis"><em>W</em></span><sub>i</sub><span class="emphasis"><em>, W</em></span><sub>b</sub><span class="emphasis"><em> = weights</em></span> and</p><p>
<span class="emphasis"><em>R</em></span><sub>i</sub><span class="emphasis"><em>, R</em></span><sub>g</sub><span class="emphasis"><em>, R</em></span><sub>b</sub><span class="emphasis"><em> = returns</em></span>
</p><p>This is because the expectation of a portfolio is the summation of the expectations of individual portfolio multiplied by individual weights.</p><p>The values for the first five portfolios are:</p><div class="mediaobject"><img src="graphics/B03980_10_16.jpg" alt="Example"/></div><p>Compute all the other values.</p><p>Calculate the risk of each portfolio using the following formula:</p><p>Return = Sqrt ((wg*sdg)<sup>2</sup> + (Wi*sdi)<sup>2</sup> + (Wb*sdb)2 + (2*Wg*sdg*Wi*sdi*rgi)+ (2*Wi*sdi*Wb*sdb*rib) + (2*Wb*sdb*wg*sdg*rbg))</p><p>
<span class="emphasis"><em>sdg, sdb, sdi = Risks and rij = correlations of i and j</em></span>
</p><p>This is exactly the same formula for the variance of a portfolio as given in one of the previous sections.</p><div class="mediaobject"><img src="graphics/B03980_10_17.jpg" alt="Example"/></div><p>Now lets compute all the other values.</p><p>Now, all that is needed is to recommend the balanced portfolios for both Ana and Daniel as their risk appetites are known to you. As Ana prefers zero risk, we will pick the point that corresponds to 17.2% returns and 0.87 risks. You can look up in the table and confirm that this is<a id="id1055" class="indexterm"/> obtained with the portfolio of 0.7, 0.2, and 0.1 (Gold, IT, and Bank). As Daniel can take 10% risk, we will see the portfolio that corresponds to 10% risk, which has the highest return.</p><p>Again, this can be read as 0.2, 0.7, and 0.1.</p></div></div><div class="section" title="ANOVA and F Statistics"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec103"/>ANOVA and F Statistics</h3></div></div></div><p>In cases like bivariate and multivariate <a id="id1056" class="indexterm"/>distributions, a good quantity to understand is the way the variance is distributed within the populations or groups and between the populations or groups. This is the process of grouping data into multiple subsets. As you can clearly see, in such situations it really helps to know how variance is distributed among them. Such an analysis is called the <a id="id1057" class="indexterm"/>
<span class="strong"><strong>ANOVA</strong></span> (<span class="strong"><strong>Analysis of Variance</strong></span>). The calculations involved are fairly straightforward.</p><p>Let's take three samples that have their own mean and distribution as depicted here:</p><div class="mediaobject"><img src="graphics/B03980_10_18.jpg" alt="ANOVA and F Statistics"/></div><p>And in terms of an example, see the following:</p><p>
<span class="emphasis"><em>Sample 1= {3, 2, 1}</em></span>
</p><p>
<span class="emphasis"><em>Sample 2= {5, 3, 4}</em></span>
</p><p>
<span class="emphasis"><em>Sample 3= {5, 6, 7}</em></span>
</p><p>
<span class="emphasis"><em>Mean for Sample 1 = 2</em></span>
</p><p>
<span class="emphasis"><em>Mean for Sample 2 = 4</em></span>
</p><p>
<span class="emphasis"><em>Mean for Sample 3 = 6</em></span>
</p><p>
<span class="emphasis"><em>Overall grand mean = (3+2+1+5+3+4+5+6+7) / 9 = 4</em></span>
</p><p>The grand mean (which will be the population mean if the groups cover the entire population) is equal to the mean of means.</p><p>Is it possible <a id="id1058" class="indexterm"/>that the three means come from the same population? If one <a id="id1059" class="indexterm"/>mean value is very different or far from the others, would that mean they are not from the same population? Or are they equally far apart?</p><p>All the previous samples are about relative distance measures from the grand mean.</p><div class="mediaobject"><img src="graphics/B03980_10_19.jpg" alt="ANOVA and F Statistics"/></div><p>Let's now compute the sum of the squares of the entire sample set:</p><p>
<span class="emphasis"><em>(3 − 4)<sup>2</sup> + (2 − 4)<sup>2</sup> + … = 30</em></span>
</p><p>We could have calculated the variance by dividing the previously mentioned quantity with the degrees of freedom <span class="emphasis"><em>(n*m-1)</em></span>:</p><p>
<span class="emphasis"><em>n</em></span>—number of elements in each sample</p><p>
<span class="emphasis"><em>m</em></span>—number of samples</p><p>The property that we<a id="id1060" class="indexterm"/> are trying to establish does not change. Hence, let's just stick with <a id="id1061" class="indexterm"/>the sum of squares instead of the variance. Now, let's compute two quantities: the sum of squares of the group and between the groups.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The sum of squares of the group</strong></span>: Let's take the first group (3, 2, and 1) where the mean is 2. The variation (we are not calling it variance. But, it is definitely a measure of variance) within the group is equal to <span class="emphasis"><em>(3-2)2+…=2</em></span>. Similarly, variation within group 2 and group 3 are equal to <span class="emphasis"><em>2</em></span> and <span class="emphasis"><em>2</em></span>. So, the total variation contributed within the groups is 6. The total number of the degrees of freedom within each group is <span class="emphasis"><em>n-1</em></span>. The total degrees of freedom is <span class="emphasis"><em>(n-1)*m</em></span>. This is 6 in this case.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The sum of squares between the groups</strong></span>: This is measured as the distances between the mean of the group, and the grand mean, which is multiplied by the number of elements in the group mean of group 1 is 2, and grand mean is 4. So, the variation of this group from the grand mean is <span class="emphasis"><em>(2-4)2 * 3 = 12</em></span>. The variation for the second group is 0 and for the third is 12. So, the variation between the groups is 24. The degree of freedom, in this case, is <span class="emphasis"><em>m-1 = 2</em></span>.</li></ul></div><p>So, let's document this:</p><div class="mediaobject"><img src="graphics/B03980_10_20.jpg" alt="ANOVA and F Statistics"/></div><p>So, we see that of the total variation of 30, 6 is contributed by variation within and 24 is contributed by variation between the groups. So most likely, it makes sense to group them separately. Now, let's do some kind of inferential statistics here. Let's assume that the previous values are the ranks obtained by three coaching centers. We want to know whether putting people in a coaching center actually has an impact on their final rank.</p><p>Let's start with a hypothetical argument.</p><p>
<span class="strong"><strong>Null Hypothesis</strong></span> is that <a id="id1062" class="indexterm"/>coaching centers do not have an impact on the rank. Alternative coaching centers do have an impact on the rank.</p><div class="mediaobject"><img src="graphics/B03980_10_21.jpg" alt="ANOVA and F Statistics"/></div><p>If we observe, this measure is not about the values being equal, but it would be a check if the samples come from the same larger population. This measure is called the variability among or between the sample means.</p><p>So in short, ANOVA is a variability ratio represented as follows:</p><p>ANOVA = Variance Between / Variance Within = Distance between the overall mean / internal spread</p><p>Total Variance = Variance Between + Variance Within</p><p>This process of separating total variance into two components is called partitioning:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If the variance between the means is &gt; variance within the means, it will mean that the variability ratio is &gt; 1. Hence, we can conclude that the samples do not belong to the same population.</li><li class="listitem" style="list-style-type: disc">If the variance between the means and within the means is similar, then the ratio almost becomes 1, and this would indicate an overlap.</li><li class="listitem" style="list-style-type: disc">If the variance between the means &lt; the variance with the means, it will mean that the samples are close to the overall mean or the distributions <span class="emphasis"><em>melt</em></span> together.</li></ul></div><p>So, as we can see while dealing with multiple variables that there can be many factors that influence the outcome. Each of these variables will need to be assessed for the independent effect on the relationship between variables. In the next section, two concepts, <span class="strong"><strong>confounding</strong></span>
<a id="id1063" class="indexterm"/> and <a id="id1064" class="indexterm"/>
<span class="strong"><strong>effect modification</strong></span>, will explain the different types of influence factors on the outcome.</p></div></div><div class="section" title="Confounding"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec114"/>Confounding</h2></div></div></div><p>We will start understanding what confounding<a id="id1065" class="indexterm"/> is using an example. Let's assume we are doing a study where we want to determine if the risk of developing heart disease has anything to do with smoking. When a study was done on sample data that had a mix of smokers and non-smokers and those who were detected to have a heart disease over a period of time, a measure of association such as a <span class="emphasis"><em>risk ratio</em></span> was done, and it was found to be 2.0. This can be interpreted as the risk of a smoker developing a heart disease being twice as much as that of a non-smoker. Now, when we look closely at the data, let's assume that we find the age distribution among the smokers and non-smokers is not the same, and it turns out that the age of smokers in the sample is much higher than the age of non-smokers. If we had to correlate this piece of information, is the outcome of developing heart disease to do with the old age, smoking or both?</p><p>An ideal way of measuring the quantitative effect of smoking on developing heart disease is to take a sample of people, observe them smoke over a period of time, collect the data on heart disease development, use the same set of people, and go back in time to run the same assessment when they are not smoking. This would help measure the counterfactual outcomes. The same group of people represents both smokers and non-smokers. Since this is not a possibility, we need to assume there is <span class="emphasis"><em>exchangeability</em></span>. Non-smokers describe smokers if they ever smoke and vice-versa. This, in other words, means the two groups are comparable in all respects/aspects. In the cases where the data samples are not comparable, the condition is termed as confounding, and the property that is responsible for it (in this case, age) is called the <a id="id1066" class="indexterm"/>
<span class="strong"><strong>confounder</strong></span>. If we have to explain this with an example, the fact that all non-smokers are younger, the non-smokers will under-estimate the outcome of older smokers had they not smoked.</p><p>This condition can be represented as shown here:</p><div class="mediaobject"><img src="graphics/B03980_10_22.jpg" alt="Confounding"/></div><p>What we observe is that<a id="id1067" class="indexterm"/> there is a backdoor pathway (through the age property). Confounding can thus be defined in a much simpler term, that is, <span class="emphasis"><em>the existence of a backdoor pathway</em></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>Confounding is a situation in which the effect or association between exposure and outcome is distorted by the presence of another variable.</p></div></div></div><div class="section" title="Effect modification"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec115"/>Effect modification</h2></div></div></div><p>Effect modification <a id="id1068" class="indexterm"/>is the condition when exposures have different values for different groups. This can be observed when the measures of association estimation, like odds ratio, rate ratio, and risk ratio values, are very close to a weighted average of group-specific estimates from the association.</p><p>The effect modifier is the variable that differentially (this can mean positively or negatively) modifies the observed effect on the outcome.</p><p>Let's look at an example. Breast cancer can occur both in men and women; the ratio occurs in both men and women, but the rate at which it occurs in women is 800 times more than men, and the gender factor is a differentiating one for obvious reasons.</p><p>If the effect modifier is not properly identified, this could result in an incorrect crude estimate, and this results in missing the opportunity to understand the relationship between the risk factor and the outcome.</p><p>The following steps need to be followed to study the effect modification for analyzing the data:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Gather information on potential effect modifiers.</li><li class="listitem">Study the effect of the effect modifier, measure the difference, and hold on from matching the values.</li><li class="listitem">Stratify the data by potential effect modifiers and calculate estimates of the effect of the risk on the outcome. Determine if effect modification is present. If so, the estimates can be presented/used.</li></ol></div><p>To review, confounders<a id="id1069" class="indexterm"/> mask a true effect and effect modifiers mean that there is a different effect for different groups.</p></div></div></div>
<div class="section" title="Regression methods"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec49"/>Regression methods</h1></div></div></div><p>As we learned, regression <a id="id1070" class="indexterm"/>allows us to model the relationship between two or more variables, especially when a continuous dependent variable is predicted, based on several independent variables. The independent variables used in regression can be either continuous or dichotomous. In cases where the dependent variable is dichotomous, logistic regression is applied. In cases where the split between the two levels of dependent variables is equal, then both linear and logistic regression would fetch the same results.</p><div class="mediaobject"><img src="graphics/B03980_10_23.jpg" alt="Regression methods"/><div class="caption"><p>Assumptions of regression (most apply to linear regression model family)</p></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Sample cases size</strong></span>: In order to apply regression models, the cases-to-<span class="strong"><strong>Independent Variables</strong></span> (<span class="strong"><strong>IVs</strong></span>)<a id="id1071" class="indexterm"/> ratio should ideally be <a id="id1072" class="indexterm"/>20:1 (for every IV in the model, there need to be 20 cases), the least being 5:1(5 cases for every IV in the model).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data accuracy</strong></span>: Regression <a id="id1073" class="indexterm"/>assumes the basic <a id="id1074" class="indexterm"/>validity of data, and it is expected to run basic data validations before running regression methods. For example, if a variable can have values between 1-5, any value not in the range will need to be corrected.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Outliers</strong></span>: As we learned, outliers<a id="id1075" class="indexterm"/> are those data points that usually have extreme values and don't naturally appear to be a part of the population. Regression assumes that the outlier values are handled.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Missing data</strong></span>: It is important<a id="id1076" class="indexterm"/> to look for missing data and address the same. If a specific variable has many missing values, it might be good to eliminate the variable unless there are too many variables with many missing values. Once the regression process is run, the variable that has no values can be a candidate for exclusion. And to avoid the risk of losing data through elimination, missing value techniques will need to be applied</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Normal distribution</strong></span>: It is necessary <a id="id1077" class="indexterm"/>for the data to be checked to ensure that your data is normally distributed. Plotting data on a histogram is a way to check if the data is normally distributed. The following histogram is an example of normal distribution:<div class="mediaobject"><img src="graphics/B03980_10_24.jpg" alt="Regression methods"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Linear behavior</strong></span>: Linear behavior<a id="id1078" class="indexterm"/> is, in<a id="id1079" class="indexterm"/> simple terms, seeing a straight line relationship between the dependent and independent variables. Any non-linear relationship between the IV and DV is ignored. A bivariate scatterplot is used to test for linearity.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Homoscedasticity</strong></span>: Homoscedasticity<a id="id1080" class="indexterm"/> refers to the constant changes to<a id="id1081" class="indexterm"/> an independent variable for a change in the dependent variable. The following scatter plot is an example of data being homoscedastic, and we can see the concentration of plottings in the center:<div class="mediaobject"><img src="graphics/B03980_10_25.jpg" alt="Regression methods"/></div></li></ul></div><p>Similar to the assumption of linearity, violation of the assumption of homoscedasticity does not invalidate regression but weakens it.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Multicollinearity and singularity</strong></span>: Multicollinearity<a id="id1082" class="indexterm"/> is a case where <a id="id1083" class="indexterm"/>independent variables are highly correlated. In the case of singularity, the independent variables are perfectly correlated and, usually, one IV is a combination of one or more other IVs. Both multicollinearity and singularity can be easily identified using the correlation between IVs.</li></ul></div><p>From the following section onwards, we will cover each of the regression methods in depth as listed in the concept map here:</p><div class="mediaobject"><img src="graphics/B03980_10_26.jpg" alt="Regression methods"/></div><div class="section" title="Simple regression or simple linear regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec116"/>Simple regression or simple linear regression</h2></div></div></div><p>In this case, we will be <a id="id1084" class="indexterm"/>working with just two variables; one dependent variable and<a id="id1085" class="indexterm"/> another independent variable. Simple linear <a id="id1086" class="indexterm"/>regression is all about comparing <a id="id1087" class="indexterm"/>two models; one where there is no independent variable and the best fit line is formed using the dependent variable, and the other that uses the best-fit regression line. Now let's look at an example to understand the best fit line and regression line definitions.</p><p>We will start with a real-world example. Let's assume there is a real-estate dealer and for every real-estate transaction that he does, he gets a commission. Very obviously, the commission amount dependents on the value of the transaction; the higher the value of the transaction, the higher the commission. So in this case, the commission becomes a dependent variable, and the transaction amount becomes an independent variable. In order to predict what could possibly be the next commission amount, let's consider the sample data of the last six transactions as follows:</p><div class="mediaobject"><img src="graphics/B03980_10_27.jpg" alt="Simple regression or simple linear regression"/></div><p>Let's assume that we <a id="id1088" class="indexterm"/>do not have data for the overall transaction amount. If we were to predict the next commission given in<a id="id1089" class="indexterm"/> the previous data, we start by plotting <a id="id1090" class="indexterm"/>it on a graph as shown here:</p><div class="mediaobject"><img src="graphics/B03980_10_28.jpg" alt="Simple regression or simple linear regression"/></div><p>One of the options we have to identify the next commission amount that is given in the data is to compute the mean, which is the best prediction for the sample.</p><div class="mediaobject"><img src="graphics/B03980_10_29.jpg" alt="Simple regression or simple linear regression"/></div><p>Let's plot this point<a id="id1091" class="indexterm"/> on the graph and this would become the <span class="emphasis"><em>best</em></span> fit. Plotting the<a id="id1092" class="indexterm"/> mean <a id="id1093" class="indexterm"/>value <a id="id1094" class="indexterm"/>on the previous graph:</p><div class="mediaobject"><img src="graphics/B03980_10_30.jpg" alt="Simple regression or simple linear regression"/></div><p>Computing the distance for each point from the mean gives the values that are shown in the next graph. This distance measure is called error or residual. The sum of the error for all the points is always found to be zero, and this is the measure of the goodness of the fit.</p><div class="mediaobject"><img src="graphics/B03980_10_31.jpg" alt="Simple regression or simple linear regression"/></div><p>Plotting the<a id="id1095" class="indexterm"/> distance<a id="id1096" class="indexterm"/> on<a id="id1097" class="indexterm"/> the<a id="id1098" class="indexterm"/> graph.</p><div class="mediaobject"><img src="graphics/B03980_10_32.jpg" alt="Simple regression or simple linear regression"/></div><p>We have learned in our earlier chapters about the <span class="strong"><strong>SSE</strong></span> (<span class="strong"><strong>Sum Squared Error</strong></span>)<a id="id1099" class="indexterm"/> value. The error is squared because it makes the value positive and also emphasizes larger deviations. The following table shows the SSE values computed for the sample data:</p><div class="mediaobject"><img src="graphics/B03980_10_33.jpg" alt="Simple regression or simple linear regression"/></div><p>The overall goal <a id="id1100" class="indexterm"/>of a simple linear regression is to build a model that minimizes<a id="id1101" class="indexterm"/> SSE to a maximum extent. Until now, we have <a id="id1102" class="indexterm"/>seen the best fit using a single variable, which<a id="id1103" class="indexterm"/> is the dependent variable. Now, let's assume we get the data for another independent variable in our example. This, in fact, gets us a new regression line that is different from the best fit line that we arrived at previously. It is expected that the new independent variable should significantly reduce the SSE value. In other words, this new regression line should be a better fit for the given data.</p><p>If there is no difference in the earlier best-fit line and the regression line, this would mean that the identified independent variable has no influence on the outcome. Overall, simple linear regression is designed to find the best fitting line using the data that would have the least amount of SSE value.</p><p>Let's now add the independent variable data into our analysis—the real-estate transaction value, as shown in the table here:</p><div class="mediaobject"><img src="graphics/B03980_10_34.jpg" alt="Simple regression or simple linear regression"/></div><p>We will plot a <a id="id1104" class="indexterm"/>scatter plot <a id="id1105" class="indexterm"/>between the dependent and the independent variable.</p><div class="mediaobject"><img src="graphics/B03980_10_35.jpg" alt="Simple regression or simple linear regression"/></div><p>There could be <a id="id1106" class="indexterm"/>multiple lines/equations possible in this context as shown<a id="id1107" class="indexterm"/> in the next graph. In case the data seems to be falling in line, we can proceed. If the data points are scattered all over the place, this is an indication that there is no linearity in data, and we could choose to stop deriving the regression line. We could choose to compute the correlation coefficient here as follows:</p><p>r = 0.866</p><p>This indicates that the relationship between the two variables is strong, and we can proceed to build the regression model.</p><div class="mediaobject"><img src="graphics/B03980_10_36.jpg" alt="Simple regression or simple linear regression"/></div><p>Let's now compute<a id="id1108" class="indexterm"/> the mean <a id="id1109" class="indexterm"/>for the<a id="id1110" class="indexterm"/> <span class="emphasis"><em>x</em></span> <a id="id1111" class="indexterm"/>and <span class="emphasis"><em>y</em></span>-axis; here are the values:</p><div class="mediaobject"><img src="graphics/B03980_10_37.jpg" alt="Simple regression or simple linear regression"/></div><p>These mean values are to then be plotted as a centroid onto the scattered plot, as shown here:</p><div class="mediaobject"><img src="graphics/B03980_10_38.jpg" alt="Simple regression or simple linear regression"/></div><p>The best-fit<a id="id1112" class="indexterm"/> regression line has to go through the centroid that comprises <a id="id1113" class="indexterm"/>the mean of the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> variables. The calculations<a id="id1114" class="indexterm"/> are<a id="id1115" class="indexterm"/> as follows:</p><div class="mediaobject"><img src="graphics/B03980_10_51.jpg" alt="Simple regression or simple linear regression"/></div><div class="mediaobject"><img src="graphics/B03980_10_52.jpg" alt="Simple regression or simple linear regression"/></div><div class="mediaobject"><img src="graphics/B03980_10_53.jpg" alt="Simple regression or simple linear regression"/></div><div class="mediaobject"><img src="graphics/B03980_10_39.jpg" alt="Simple regression or simple linear regression"/></div><p>The final regression line equation looks like this:</p><div class="mediaobject"><img src="graphics/B03980_10_40.jpg" alt="Simple regression or simple linear regression"/></div><p>Plotting the <a id="id1116" class="indexterm"/>previous <a id="id1117" class="indexterm"/>equation<a id="id1118" class="indexterm"/> on<a id="id1119" class="indexterm"/> the scatter plot looks like this:</p><div class="mediaobject"><img src="graphics/B03980_10_41.jpg" alt="Simple regression or simple linear regression"/></div></div><div class="section" title="Multiple regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec117"/>Multiple regression</h2></div></div></div><p>Multiple regression<a id="id1120" class="indexterm"/> is an extension of simple linear regression with one important<a id="id1121" class="indexterm"/> difference, that there can be two or more independent variables used for predicting or explaining the variance in one dependent variable. Adding more independent variables does not necessarily make the regression better. There could potentially be two problems that could arise, one of which is over-fitting. We have covered this in the earlier chapters. Too many independent variables can add to the variance but in reality, they add nothing to the model thus causing over-fitting. Also, adding more independent variables adds more relationships. It is not only that the independent variables are potentially related to the dependent variables, but also there could be a dependency between the independent variables themselves. This condition is called multicollinearity. The ideal expectation is that the independent variables are correlated with the dependent variables, but not with each other.</p><p>As a result of over-fitting and multicollinearity issues, there is a need for preparatory work before a multiple regression analysis work is to be started. The preparatory work can include computing correlations, mapping scatter plots, and running simple linear regression among others.</p><p>Let's say, we have one dependent variable and four independent variables, and there is a multicollinearity risk. This means there are four relationships between the four independent variables and one dependent variable, and among the independent variables, there could be six more. So, there are 10 relationships to consider as shown here. DV stands for dependent variable and IV stands for independent variable.</p><div class="mediaobject"><img src="graphics/B03980_10_42.jpg" alt="Multiple regression"/></div><p>Some independent <a id="id1122" class="indexterm"/>variables are better than others for predicting the dependent variable, and some might not contribute anything to the prediction. There is a need to decide which one of the dependent variables to consider.</p><p>In multiple regression, each <a id="id1123" class="indexterm"/>coefficient is interpreted as the estimated change in <span class="emphasis"><em>y</em></span> corresponding to a one-unit change in the variable, while the rest of the variables are assumed constant.</p><p>The following are the multiple regression equations.</p><div class="mediaobject"><img src="graphics/B03980_10_43.jpg" alt="Multiple regression"/></div><p>Let's say we want to fit an independent variable as a function of a lot of variables (<span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span>, and <span class="emphasis"><em>x</em></span><sup>2</sup>). We can follow a simple procedure to get the coefficients of all the variables. This is applicable for linear, quadratic, and cubic functions.</p><p>The following is the<a id="id1124" class="indexterm"/> step-by-step process:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Order all the points of each variable in a separate column.</li><li class="listitem">Combine all the columns of the independent variables to be represented as a matrix.</li><li class="listitem">Add a column to the 1's at the beginning of the matrix.</li><li class="listitem">Name this matrix as <span class="emphasis"><em>X</em></span> Matrix.</li><li class="listitem">Make a separate column matrix of all independent variables and call it <span class="emphasis"><em>Y</em></span> Matrix.</li><li class="listitem">Compute the coefficients using the formula here (this is the least square regression):<p>
<span class="emphasis"><em>B = (X</em></span><sup>T</sup><span class="emphasis"><em>X)</em></span><sup>-1</sup><span class="emphasis"><em>X</em></span><sup>T</sup><span class="emphasis"><em>Y</em></span>
</p></li></ol></div><p>This is a matrix <a id="id1125" class="indexterm"/>operation, and the resulting vector is the coefficient.</p><p>In multiple regression, a lot of preparatory work needs to be done before running the regression model. It is necessary to step back and perform some analysis on the variables in consideration. Some basic scatter plots can be plotted to check for any correlations and to analyze the relationships between the dependent variables. Techniques like scatter plots, correlation analysis, and individual or group regressions can be used. In case there are any qualitative or categorical variables, we will need to use dummy variables to build the regression model.</p></div><div class="section" title="Polynomial (non-linear) regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec118"/>Polynomial (non-linear) regression</h2></div></div></div><p>While the linear regression <a id="id1126" class="indexterm"/>model <span class="emphasis"><em>y = Xβ + ε</em></span> is a general model that will<a id="id1127" class="indexterm"/> fit any linear relationship in the unknown parameter <span class="emphasis"><em>β</em></span>, polynomial models are applicable in cases where the analyst knows that curvilinear effects are present in the true response function. Polynomial models are also used as approximating functions to the unknown and possibly very complex nonlinear relationship. The polynomial model is the Taylor series expansion of the unknown function.</p><p>If the two variables are linearly related, the scatterplot looks like the following:</p><div class="mediaobject"><img src="graphics/B03980_10_44.jpg" alt="Polynomial (non-linear) regression"/></div><p>From the previous <a id="id1128" class="indexterm"/>bivariate scatterplot, it is clear that there is a linear<a id="id1129" class="indexterm"/> relationship between friends and happiness. The graph says <span class="emphasis"><em>more friends, more happiness</em></span>. What if we talk about a curvilinear relationship between the variables, the number of friends and happiness? This means as the number of friends grows, the happiness grows but only to a certain point. The following graph shows this behavior in data:</p><div class="mediaobject"><img src="graphics/B03980_10_45.jpg" alt="Polynomial (non-linear) regression"/></div><p>If the data is not<a id="id1130" class="indexterm"/> linear, then the process is to make it linear by <a id="id1131" class="indexterm"/>transforming IVs or the DV so that there is a linear relationship between them. This transformation will not always work as there might be a genuine non-linearity in data and behavior. In this case, we will need to include the square of the independent variables in the regression. This is also known as a polynomial/quadratic regression. The <span class="strong"><strong>least squares</strong></span> method<a id="id1132" class="indexterm"/> is used to fit a polynomial regression model as it minimizes the variance in the estimation of the coefficients.</p></div><div class="section" title="Generalized Linear Models (GLM)"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec119"/>Generalized Linear Models (GLM)</h2></div></div></div><p>Let's look at the reasons why a <a id="id1133" class="indexterm"/>linear regression model <a id="id1134" class="indexterm"/>does not work.</p><p>Simple linear regression is a quantitative variable predicting another, multiple regression. It is an extended simple linear regression, but with more independent variables and finally, a nonlinear or polynomial regression is the case where there are two quantitative variables, but the data is curvilinear.</p><p>Now, running a typical linear regression, in the same way, has some problems. Binary data does not have a normal distribution. This is where the need for other regression models comes in. Secondly, the predicted values of the dependent variable can be beyond 0 and 1, which is against the concept of probability. Finally, probabilities are often non-linear and can take majorly low or high values at the extremes.</p><p>GLM is a <a id="id1135" class="indexterm"/>generalization of linear regression that<a id="id1136" class="indexterm"/> supports cases where the independent variables can have distribution error models other than normal distribution. GLM generalizes linear regression as it allows the linear model to be related to the independent variable through a link function, and it also allows the degree of the variance of each measure is a function of its predicted value.</p><p>In short, GLM generalizes linear, logistic, and Poisson regression models.</p></div><div class="section" title="Logistic regression (logit link)"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec120"/>Logistic regression (logit link)</h2></div></div></div><p>Logistic regression<a id="id1137" class="indexterm"/> is an extension of linear regression where the <a id="id1138" class="indexterm"/>dependent variable is a categorical variable that is responsible for the classification of the observations.</p><p>For example, if <span class="emphasis"><em>Y</em></span> denotes whether a particular customer is likely to purchase a product (1) or unlikely to purchase (0), we have a categorical variable with two categories or classes (0 and 1). Logistic regression can solve a classification problem where the class is unknown. This is done using the predictor values classifying a new observation, where the class is unknown, into one of the classes, based on the variable.</p><p>The examples are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Classifying customers as returning (1) or non-returning (0)</li><li class="listitem" style="list-style-type: disc">Predicting if a loan would be approved or rejected, given the credit score</li></ul></div><p>One of the important uses can be to find similarities between predictor values.</p><p>Before we start taking a deep dive into logistic regression, let's revisit the concept of probability and the odds that were covered in the earlier chapter.</p><p>Probability = outcomes of interest / all possible outcomes.</p><p>For example, when a fair coin is tossed, <span class="emphasis"><em>P(heads) = ½ = 0.5</em></span>. When a dice is rolled, <span class="emphasis"><em>P(1 or 2) = 2/6 = 1/3 = 0.33</em></span>. In a deck of cards, <span class="emphasis"><em>P(diamond card) = 13/52 = ¼ = 0.25</em></span>.</p><p>Odds = P(something happening)/P(something not happening) = p/1-p</p><p>For example, when a coin is tossed, <span class="emphasis"><em>odds(heads = 0.5/0.5= 1)</em></span>. When a dice is rolled, <span class="emphasis"><em>odds(1 or 2) = 0.333/0.666 = ½ = 0.5</em></span>. In a deck of cards, <span class="emphasis"><em>odds(diamond card) = 0.25/0.75 = 1/3 = 0.333</em></span>.</p><p>The odds ratio is the ratio of two odds.</p><p>For example, when a coin is tossed, in case of a fair flip:</p><p>
<span class="emphasis"><em>P(heads) = ½= 0.5</em></span> and <span class="emphasis"><em>odds(heads) = 0.5/0.5 = 1 = 1:1</em></span>
</p><p>In case of a loaded coin flip:</p><p>
<span class="emphasis"><em>P(heads) = 0.7 and odds(heads) = 0.7/0.3 = 2.333</em></span>
</p><p>
<span class="emphasis"><em>Odds ratio= odds1/odds0 = 2.333/1 = 2.333</em></span>
</p><p>This means the odds of getting a heads when a loaded coin is flipped is 2.333 times greater than a fair coin.</p><p>Overall, logistic <a id="id1139" class="indexterm"/>regression seeks to:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Model</strong></span> the<a id="id1140" class="indexterm"/> probability of the event occurring depending on the values of the independent variables, which can be categorical or numerical</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Estimate</strong></span> the probability of an event occurring versus not occurring</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Predict</strong></span> the effect of a set of variables on a binary response variable</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Classify</strong></span> the observations to belong to a particular category based on the probability estimation</li></ul></div><div class="section" title="Odds ratio in logistic regression"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec104"/>Odds ratio in logistic regression</h3></div></div></div><p>The odds ratio for a<a id="id1141" class="indexterm"/> variable in logistic regression denotes how the odds<a id="id1142" class="indexterm"/> for one variable changes with the increase of a unit in that variable, keeping the rest of the variables constant.</p><p>Let's take an example to understand this—whether the body weight is dependent on sleep apnea or not. Let's assume that the body weight variable has an odds ratio of 1.07. This means one pound increased in weight could potentially increase the odds of having slept apnea by 1.07 times. This might not be significant. In the case of a 10-pound increase in weight, the odds increase to 1.98, which doubles the odds of the person having slept apnea. It is important that we separate the probability and the odds measures. For example, though the increase in weight by 20 pounds increases the odds of the person having slept by 4 times, the probability that the person's weight has increased by 20 pounds could potentially be very low.</p><p>In logistic regression, there are two important steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Finding the probability of belonging to a particular class. So, if <span class="emphasis"><em>Y = 0</em></span> or <span class="emphasis"><em>1</em></span>, the probability of belonging to <span class="emphasis"><em>class 1</em></span> is <span class="emphasis"><em>P(Y=1)</em></span>.</li><li class="listitem">We will need to use the cut-off values of the probabilities to ensure that each case gets into one of the classes. In case of binary cut-off, a <span class="emphasis"><em>P(Y=1) &gt; 0.5</em></span> will be categorized as <span class="emphasis"><em>1</em></span> and <span class="emphasis"><em>P(Y=0) &lt; 0.5</em></span> will be categorized as <span class="emphasis"><em>0</em></span>.</li></ol></div><div class="section" title="Model"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl4sec35"/>Model</h4></div></div></div><p>
<span class="emphasis"><em>y</em></span><sub>i</sub> is normally <a id="id1143" class="indexterm"/>distributed and takes the value of either 0 or 1 for <span class="emphasis"><em>i = 0,1,…,n</em></span>.</p><p>
<span class="emphasis"><em>y</em></span><sub>i</sub> is equal to {0, 1} where <span class="emphasis"><em>P(y</em></span><sub>i</sub><span class="emphasis"><em> = 1) = p</em></span> and <span class="emphasis"><em>P(y</em></span><sub>i</sub><span class="emphasis"><em> = 0) = 1-p</em></span>
</p><p>
<span class="emphasis"><em>Y = a + bx for P(y</em></span><sub>i</sub><span class="emphasis"><em>= 1)</em></span>
</p><p>
<span class="emphasis"><em>p</em></span><sub>i</sub><span class="emphasis"><em> = a + bx</em></span><sub>i</sub></p><p>Note that <span class="emphasis"><em>p</em></span><sub>i</sub> will not take values between (0, 1). This is fixed by using a non-linear function of predictors such as:</p><div class="mediaobject"><img src="graphics/B03980_10_46.jpg" alt="Model"/></div><p>Clearly, this takes a value between 0 and 1 as <span class="emphasis"><em>x</em></span> varies from -∞ to ∞. From this, <span class="emphasis"><em>a+bx</em></span><sub>i</sub> can be obtained as follows:</p><div class="mediaobject"><img src="graphics/B03980_10_47.jpg" alt="Model"/></div><p>The following curve shows how the function varies:</p><div class="mediaobject"><img src="graphics/B03980_10_48.jpg" alt="Model"/></div></div></div></div><div class="section" title="Poisson regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec121"/>Poisson regression</h2></div></div></div><p>Poisson regression, in the<a id="id1144" class="indexterm"/> context <a id="id1145" class="indexterm"/>of GLM, is a count of data with the independent variable having Poisson distribution and the link function applied is a logarithm of the response that can be modeled using a linear combination of unknown parameters.</p></div></div>
<div class="section" title="Implementing linear and logistic regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec50"/>Implementing linear and logistic regression</h1></div></div></div><p>Refer to the source code <a id="id1146" class="indexterm"/>provided for this chapter for implementing <a id="id1147" class="indexterm"/>linear regression. (source code path <code class="literal">.../chapter10/...</code> under each of the folders for the technology)</p><div class="section" title="Using Mahout"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec122"/>Using Mahout</h2></div></div></div><p>Refer<a id="id1148" class="indexterm"/> to<a id="id1149" class="indexterm"/> the<a id="id1150" class="indexterm"/> folder <code class="literal">.../mahout/chapter10/linearregressionexample/</code>.</p><p>Refer to the <a id="id1151" class="indexterm"/>folder <code class="literal">.../mahout/chapter10/logisticregressionexample/</code>.</p></div><div class="section" title="Using R"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec123"/>Using R</h2></div></div></div><p>Refer<a id="id1152" class="indexterm"/> to <a id="id1153" class="indexterm"/>the<a id="id1154" class="indexterm"/> folder <code class="literal">.../r/chapter10/linearregressionexample/</code>.</p><p>Refer to<a id="id1155" class="indexterm"/> the folder <code class="literal">.../r/chapter10/logisticregressionexample/</code>.</p></div><div class="section" title="Using Spark"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec124"/>Using Spark</h2></div></div></div><p>Refer<a id="id1156" class="indexterm"/> to <a id="id1157" class="indexterm"/>the <a id="id1158" class="indexterm"/>folder <code class="literal">.../spark/chapter10/linearregressionexample/</code>.</p><p>Refer to the<a id="id1159" class="indexterm"/> folder <code class="literal">.../spark/chapter10/logisticregressionexample/</code>.</p></div><div class="section" title="Using scikit-learn"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec125"/>Using scikit-learn</h2></div></div></div><p>Refer <a id="id1160" class="indexterm"/>to the <a id="id1161" class="indexterm"/>folder <code class="literal">.../python-scikit-learn/chapter10/linearregressionexample/</code>
</p><p>Refer <a id="id1162" class="indexterm"/>to<a id="id1163" class="indexterm"/> the folder <code class="literal">.../python-scikit-learn/chapter10/logisticregressionexample/</code>
</p></div><div class="section" title="Using Julia"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec126"/>Using Julia</h2></div></div></div><p>Refer <a id="id1164" class="indexterm"/>to <a id="id1165" class="indexterm"/>the<a id="id1166" class="indexterm"/> folder <code class="literal">.../julia/chapter10/linearregressionexample/</code>.</p><p>Refer to the <a id="id1167" class="indexterm"/>folder <code class="literal">.../julia/chapter10/logisticregressionexample/</code>.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec51"/>Summary</h1></div></div></div><p>In this chapter, you learned regression analysis-based machine learning and, in particular, how to implement linear and logistic regression models using Mahout, R, Python, Julia, and Spark. Additionally, we covered other related concepts of statistics such as variance, covariance, and ANOVA among others. We covered regression models in depth with examples to understand how to apply them to real-world problems. In the next chapter, we will cover deep learning methods.</p></div></body></html>