<html><head></head><body>
		<div id="_idContainer059">
			<h1 id="_idParaDest-69"><a id="_idTextAnchor074"/>Chapter 4: Machine Learning Pipelines</h1>
			<p><a id="_idTextAnchor075"/>In this chapter, we will explore and implement <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) pipelines by going through hands-on examples using the MLOps approach. We will learn more by solving the business problem that we've been working on in <a href="B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, <em class="italic">Code Meets Data</em>. This theoretical and practical approach to learning will ensure that you will have comprehensive knowledge of architecting and implementing ML pipelines for your problems or your company's problems. A ML pipeline has modular scripts or code that perform all the traditional steps in ML, such as data preprocessing, feature engineering, and feature scaling before training or retraining any model. <a id="_idTextAnchor076"/><a id="_idTextAnchor077"/></p>
			<p>We begin this chapter by ingesting the preprocessed data we worked on in the last chapter by performing feature engineering and scaling it to get it in shape for the ML training. We will discover the principles of ML pipelines and implement them on the business problem. Going ahead, we'll look into ML model training, hyperparameter tuning, and the testing of the trained models. Finally, we'll learn about packaging the models and their needed artifacts. We'll register the models for further evaluation and will deploy the ML models. We are going to cover the following main topics in this chapter:</p>
			<ul>
				<li><a id="_idTextAnchor078"/><a id="_idTextAnchor079"/>Going through the basics of ML pipelines</li>
				<li>Data ingestion and feature engineering </li>
				<li>ML training and hyperparameter optimization</li>
				<li>Model testing and defining metrics </li>
				<li>Model packaging</li>
				<li>Registering models and production artifacts</li>
			</ul>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor080"/>Going through the basics of ML pipelines</h1>
			<p>Before we jump into the <a id="_idIndexMarker233"/>implementation of the ML pipeline, let's get the basics right. We will reflect on ML pipelines and set up the needed resources for ML pipeline implementation and then we will get started with data ingestion. Let's demystify ML pipelines by reflecting on the ML pipeline we discussed in <em class="italic">Figure 14</em> of <a href="B16572_01_Final_JM_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Fundamentals of MLOps Workflow</em>. </p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B16572_04_001.jpg" alt=" Figure 4.1 – Machine learning pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 4.1 – Machine learning pipeline</p>
			<p>As shown in <em class="italic">Figure 4.1</em>, a comprehensive ML pipeline consists of the following steps:</p>
			<ol>
				<li>Data ingestion </li>
				<li>Model training </li>
				<li>Model testing </li>
				<li>Model packaging </li>
				<li>Model registering</li>
			</ol>
			<p>We will implement all these steps of the pipeline using the Azure ML service (cloud-based) and MLflow (open source) simultaneously for the sake of a diverse perspective. Azure ML and MLflow are a power couple for MLOps: they exhibit the features shown in <em class="italic">Table 4.1</em>. They are also unique in their capabilities, as we can see from the following table. </p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/01.jpg" alt="Table 4.2 – MLflow versus Azure ML service&#13;&#10;"/>
				</div>
			</div>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/02.jpg" alt="Table 4.2 – MLflow versus Azure ML service&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 4.2 – MLflow versus Azure ML service</p>
			<p>To implement <a id="_idIndexMarker234"/>the ML pipeline, we need a storage resource for our dataset and a computational resource for our ML models. As discussed before in <a href="B16572_02_Final_JM_ePub.xhtml#_idTextAnchor028"><em class="italic">Chapter 2</em></a>, <em class="italic">Characterizing Your Machine Learning Problem</em>, we will perform the computation required to implement the ML pipeline and the business problem, as shown in <em class="italic">Figure 4.2</em>. </p>
			<p>We process the data on our local computer or PC to get started and preprocess the data for our ML training. For <a id="_idIndexMarker235"/>ML training and pipeline implementation, we use compute resources provisioned on the cloud (Microsoft Azure). Even though ML training for the pipeline can be done on your local computer, we will use compute resources on the cloud to learn how to provision and use the needed compute resources for the ML pipeline.</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B16572_04_002.jpg" alt=" Figure 4.3 – Computation location for data and ML tasks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 4.3 – Computation location for data and ML tasks</p>
			<p>Without further ado, let's <a id="_idIndexMarker236"/>configure the needed <a id="_idIndexMarker237"/>compute resources for the ML pipeline using the following steps:</p>
			<ol>
				<li value="1">Go to your ML workspace.<div id="_idContainer052" class="IMG---Figure"><img src="image/B16572_04_003.jpg" alt="Figure 4.4 – Azure Machine Learning workspace&#13;&#10;"/></div><p class="figure-caption">Figure 4.4 – Azure Machine Learning workspace</p></li>
				<li>Go to the <strong class="bold">Compute</strong> option and click the <strong class="bold">Create</strong> button to explore compute options available on the cloud. </li>
				<li>Select the <a id="_idIndexMarker238"/>suitable compute option for the ML model training to be optimal and efficient. <p>Select a suitable <a id="_idIndexMarker239"/>compute option based on your training needs and cost limitations and give it a name. For example, in <em class="italic">Figure 4.4</em>, a compute or virtual machine is selected for the experiment <strong class="bold">Standard_D1_v2</strong>: it is a CPU with 1 Core, 3.5 GB of RAM, and 50 GB of disk space. To select the suggested machine configuration or size, you must check <strong class="bold">select from all options</strong> in the <strong class="bold">Virtual machine size</strong> section. After selecting the desired virtual machine configuration or size, click the <strong class="bold">Next</strong> button to proceed and you will see the screen shown in <em class="italic">Figure 4.4</em>. </p><div id="_idContainer053" class="IMG---Figure"><img src="image/B16572_04_004.jpg" alt="Figure 4.5 – Create a compute resource in an Azure workspace&#13;&#10;"/></div><p class="figure-caption">Figure 4.5 – Create a compute resource in an Azure workspace</p><p>Select a <a id="_idIndexMarker240"/>compute name that is unique within an Azure region (it is recommended to use something like <strong class="source-inline">unique_code-ml-compute1</strong>). The selected compute option in <em class="italic">Figure 4.4</em> is one of the cheapest <a id="_idIndexMarker241"/>compute options and this is sufficient for implementing the ML pipeline for the business problem. For faster implementation and training ML models, it is recommended to use the <strong class="source-inline">STANDARD_DS11_V2</strong> (2 cores, 14 GB RAM) virtual machine size. With this option, training a model will take around 12 minutes.   </p></li>
				<li>Provision the compute resource created previously. After naming and creating the needed compute resource, your compute resource is provisioned, ready, and running for ML training on the cloud, as shown in <em class="italic">Figure 4.5</em>. </li>
			</ol>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B16572_04_005.jpg" alt="Figure 4.6 – Provisioned compute in an AzureML workspace&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Provisioned compute in an AzureML workspace</p>
			<p>After it is <a id="_idIndexMarker242"/>provisioned, select the <strong class="bold">JupyterLab </strong>option. JupyterLab is an open source web-based user interface. It comes with features such as text editor, code editor, terminal, and <a id="_idIndexMarker243"/>custom components integrated in an extensible manner. We will use this as a programming interface connected to the provisioned compute to train the ML models.</p>
			<p>Now we'll begin with the hands-on implementation of the ML pipeline. Follow these steps to <a id="_idIndexMarker244"/>implement the ML pipeline: </p>
			<ol>
				<li value="1">To start the implementation, clone the repository you have imported into the Azure DevOps project. To clone the repository, click on the <strong class="bold">Clone</strong> button in the upper-right corner from the <strong class="bold">Repos</strong> menu and then click on the <strong class="bold">Generate Git Credentials</strong> button. A hash password will be created.<div id="_idContainer055" class="IMG---Figure"><img src="image/B16572_04_006.jpg" alt="Figure 4.7 – Cloning an Azure DevOps Git repository (Generate Git Credentials)&#13;&#10;"/></div><p class="figure-caption">Figure 4.7 – Cloning an Azure DevOps Git repository (Generate Git Credentials)</p></li>
				<li>Copy the HTTPS link from the <strong class="bold">Command Line</strong> section to get the Azure DevOps repository link, like this:<p class="source-code"><strong class="bold">https://xxxxxxxxx@dev.azure.com/xxxxx/Learn_MLOps/_git/Learn_MLOps</strong></p></li>
				<li>Copy the <a id="_idIndexMarker245"/>password generated from <em class="italic">step 1</em> and add it to the link from <em class="italic">step 2</em> by adding the password just after the first username separated by <strong class="source-inline">:</strong> before the <strong class="source-inline">@ </strong>character. Then it is possible to use the following <strong class="source-inline">git clone</strong> command without getting permission errors:<p class="source-code"><strong class="bold">git clone https://user:password_hash@dev.azure.com/user/repo_created</strong></p></li>
				<li>Once you are running JupyterLab, we will access the terminal to clone the repository to the azure compute. To access the terminal, you must select the <strong class="bold">Terminal</strong> option from the <strong class="bold">Launcher</strong>  tab. Another way to access the terminal directly is by using the Terminal link from the Application URI column in the list of compute instances in the Azure ML workspace. Go to the <strong class="bold">Terminal</strong> option of JupyterLab and implement the following (as shown in <em class="italic">Figure 4.7</em>):<p class="source-code">git clone https://xxxxxxxxx@dev.azure.com/xxxxx/Learn_MLOps/_git/Learn_MLOps</p><p>Here is the output:</p><div id="_idContainer056" class="IMG---Figure"><img src="image/B16572_04_007.jpg" alt="Figure 4.8 – Clone the Azure DevOps Git repository on Azure compute&#13;&#10;"/></div><p class="figure-caption">Figure 4.8 – Clone the Azure DevOps Git repository on Azure compute</p></li>
				<li>Go to the <strong class="source-inline">04_MLpipelines</strong> folder and follow <a id="_idIndexMarker246"/>the implementation steps on <strong class="source-inline">ML-pipeline.ipynb</strong> from the cloned repository. All of the following steps are implemented in <strong class="source-inline">ML-pipeline.ipynb</strong>. It is recommended to follow the file instructions to have a better understanding of the implementation and execute the code yourself in a new file as per your setup. <p>So far, we have provisioned the compute resource and cloned the GitHub repository in the compute. </p></li>
				<li>Next, we start implementing the <strong class="source-inline">ML-pipeline.ipynb</strong> file by importing the needed libraries, such as <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">azureml</strong>, <strong class="source-inline">pickle</strong>, <strong class="source-inline">mlflow</strong>, and others, as shown in the following code block:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import warnings</p><p class="source-code">from math import sqrt</p><p class="source-code">warnings.filterwarnings('ignore')</p><p class="source-code">from azureml.core.run import Run</p><p class="source-code">from azureml.core.experiment import Experiment</p><p class="source-code">from azureml.core.workspace import Workspace</p><p class="source-code">from azureml.core.model import Model</p><p class="source-code">from azureml.core.authentication import ServicePrincipalAuthentication</p><p class="source-code">from azureml.train.automl import AutoMLConfig</p><p class="source-code">import pickle</p><p class="source-code">from matplotlib import pyplot as plt</p><p class="source-code">from matplotlib.pyplot import figure</p><p class="source-code">import mlflow</p></li>
				<li>Next, we use setup MLflow (for tracking experiments). Use the <strong class="source-inline">get_mlflow_tracking_url()</strong>  function to get a tracking ID for where MLflow <a id="_idIndexMarker247"/>experiments and artifacts should be logged (in this case, we get the tracking ID for the provisioned training compute). Then use the <strong class="source-inline">set_tracking_uri()</strong>  function to connect to a tracking URI (the uniform resource identifier of a specific resource) for the provisioned training compute. The tracking URI can be either for a remote server, a database connection string, or a local path to log data in a local directory. In our case, we point the tracking URI to the local path by default (on the provisioned training compute): <p class="source-code">uri = workspace.<strong class="bold">get_mlflow_tracking_uri</strong>( )</p><p class="source-code">mlflow.<strong class="bold">set_tracking_uri</strong>(uri)</p><p>The URI defaults to the <strong class="source-inline">mlruns</strong> folder where MLflow artifacts and logs will be saved for experiments. </p></li>
			</ol>
			<p>By setting the <a id="_idIndexMarker248"/>tracking URI for your MLflow experiments, you have set the location for MLflow to save its artifacts and logs in the <strong class="source-inline">mlruns</strong> folder (on your provisioned compute). After executing these commands, check for the current path. You will find the <strong class="source-inline">mlruns</strong> folder.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor081"/>Data ingestion and feature engineering </h1>
			<p>Data is essential to train ML models; without data, there is no ML. Data ingestion is a trigger step for the <a id="_idIndexMarker249"/>ML pipeline. It deals with the volume, velocity, veracity, and variety of data by extracting data from various data sources and ingesting the needed data for model training. </p>
			<p>The ML pipeline is <a id="_idIndexMarker250"/>initiated by ingesting the right data for training the ML models. We will start by accessing the <a id="_idIndexMarker251"/>preprocessed data we registered in the previous chapter. Follow these steps to access and import the preprocessed data and get it ready for ML training: </p>
			<ol>
				<li value="1">Using the <strong class="source-inline">Workspace()</strong> function from the Azure ML SDK, access the data from the datastore in the ML workspace as follows: <p class="source-code">from azureml.core import Workspace, Dataset</p><p class="source-code">subscription_id = 'xxxxxx-xxxxxx-xxxxxxx-xxxxxxx'</p><p class="source-code">resource_group = 'Learn_MLOps'</p><p class="source-code">workspace_name = 'MLOps_WS'</p><p class="source-code">workspace = <strong class="bold">Workspace</strong>(subscription_id, resource_group, workspace_name)</p><p class="callout-heading">Note</p><p class="callout">Insert your own credentials, such as <strong class="source-inline">subscription_id</strong>, <strong class="source-inline">resource_group</strong>, and <strong class="source-inline">workspace_name</strong> and initiate a workspace object using these credentials. </p><p>When these instructions are successfully executed in the JupyterLab, you can run the remaining blocks of code in the next cells.</p></li>
				<li>Import the <a id="_idIndexMarker252"/>preprocessed dataset that was prepared in the previous chapter. The preprocessed dataset is imported using the <strong class="source-inline">.get_by_name()</strong> function from the <strong class="source-inline">Dataset</strong> function from the Azureml SDK and the function is used to retrieve the needed dataset:<p class="source-code"># Importing pre-processed dataset</p><p class="source-code">dataset = Dataset.<strong class="bold">get_by_name</strong> (workspace, name='processed_weather_data_portofTurku')</p><p class="source-code">print(dataset.name, dataset.version)</p></li>
				<li>Upon <a id="_idIndexMarker253"/>successfully retrieving or mounting the dataset, you can confirm by printing <strong class="source-inline">dataset.name</strong> and <strong class="source-inline">dataset.version</strong>, which should print <strong class="source-inline">processed_weather_data_portofTurku 1</strong> or as per the name you have given the <a id="_idIndexMarker254"/>dataset previously.</li>
				<li>After retrieving the preprocessed data, it is vital to split it into training and validation sets in order to train the ML model and test or evaluate it in the training phase and later stages. Hence, we split it into the training and validation sets, by splitting it in the 80% (training set) and 20% (test set) split-ratio as follows:<p class="source-code">df_training = df.iloc[:77160]</p><p class="source-code">df_test = df.drop(df_training.index)</p><p class="source-code">df_training.to_csv('Data/training_data.csv',index=False)</p><p class="source-code">df_test.to_csv('Data/test_data.csv',index=False)</p></li>
				<li>After successfully <a id="_idIndexMarker255"/>splitting the data, these two <a id="_idIndexMarker256"/>datasets are stored and registered to the datastore (connected to the Azure ML workspace) as follows: <p class="source-code">datastore = workspace.get_default_datastore()</p><p class="source-code">datastore.upload(src_dir='Data', target_path='data')</p><p class="source-code">training_dataset = /</p><p class="source-code">Dataset.Tabular.from_delimited_files(datastore.path('data/training_data.csv'))</p><p class="source-code">validation_dataset = /</p><p class="source-code">Dataset.Tabular.from_delimited_files(datastore.path('data/validation_data.csv'))</p><p class="source-code">training_ds = training_dataset.register(workspace=workspace, name='training_dataset',</p><p class="source-code">description='Dataset to use for ML training')</p><p class="source-code">test_ds = validation_dataset.register(workspace=workspace,</p><p class="source-code">                                 name='test_dataset',</p><p class="source-code">description='Dataset for validation ML models')</p></li>
			</ol>
			<p>By using the <strong class="source-inline">register()</strong> function, we <a id="_idIndexMarker257"/>are able to register the training and test datasets, which can be imported later from the datastore. </p>
			<p>Next, we will import the training data and ingest it into the ML pipeline and use the test dataset later to test the model's performance on unseen data in production or for model analysis. </p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor082"/>Data ingestion (training dataset)</h2>
			<p>To ingest training data into the ML pipeline, we start by importing it using the<strong class="source-inline"> get_by_name()</strong> function and converting it to a pandas dataframe using the <strong class="source-inline">to_pandas_dataframe()</strong> function:</p>
			<p class="source-code">dataset = Dataset.get_by_name (workspace, name='training_dataset')</p>
			<p class="source-code">print(dataset.name, dataset.version)</p>
			<p class="source-code">df = dataset.to_pandas_dataframe ( )</p>
			<p>The training dataset is <a id="_idIndexMarker258"/>now retrieved and will be used to further train the ML models. The goal is to train classification models to <a id="_idIndexMarker259"/>predict whether it will rain or not. Hence, select the <strong class="source-inline">Temperature</strong>, <strong class="source-inline">Humidity</strong>, <strong class="source-inline">Wind_speed</strong>, <strong class="source-inline">Wind_bearing</strong>, <strong class="source-inline">Visibility</strong>, <strong class="source-inline">Pressure</strong>, and <strong class="source-inline">Current_weather_conditions</strong> features to train the binary classification models to predict weather conditions in the future (4 hours ahead). </p>
			<p>Follow these steps to select features and scale them: </p>
			<ol>
				<li value="1">Before training the ML models, selecting the right features and scaling the data is vital. Therefore, we select features as follows. The values in the variable <strong class="source-inline">X </strong>represent the independent variables and the variable <strong class="source-inline">Y</strong> is the dependent variable (forecasted weather):  <p class="source-code">X = df[['Temperature_C', 'Humidity', 'Wind_speed_kmph', 'Wind_bearing_degrees', 'Visibility_km', 'Pressure_millibars', 'Current_weather_condition']].values</p><p class="source-code">y = df['Future_weather_condition'].values</p></li>
				<li>Split the training data into the training and testing sets (for training validation after training) using the <strong class="source-inline">train_test_split()</strong> function from <strong class="source-inline">sklearn</strong>. Fixing the random seed (<strong class="source-inline">random_state</strong>) is needed to reproduce a training session by keeping the samples from the previous experiment with the same configuration. Hence, we will use <strong class="source-inline">random_state=1</strong>:<p class="source-code"># Splitting the Training dataset into Train and Test set for ML training</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">X_train, X_val,  y_train, y_val = <strong class="bold">train_test_split</strong>(X, y, test_size=0.2, random_state=1)</p><p>With an 80% (training data) and 20% (test data) split, the training and test datasets are now <a id="_idIndexMarker260"/>ready for feature scaling and ML model training. </p></li>
				<li>For the ML model training to be <a id="_idIndexMarker261"/>optimal and efficient, the data needs to be on the same scale. Therefore, we scale the data using <strong class="source-inline">StandardScalar()</strong> from <strong class="source-inline">sklearn</strong> to calibrate all the numeric values in the data on the same scale:<p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">sc = StandardScaler()</p><p class="source-code">X_train = sc.fit_transform(X_train)</p><p class="source-code">X_val = sc.transform(X_val)</p><p>With this step, the numeric values of the training data are scaled using <strong class="source-inline">StandardScalar</strong> and all the values are transformed in the range of <strong class="source-inline">-1</strong> to <strong class="source-inline">1</strong>, based on <strong class="source-inline">X_train values</strong>. Now we are ready to train ML models (the fun part)! </p></li>
			</ol>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor083"/>Machine learning training and hyperparameter optimization</h1>
			<p>We are all set to do the fun part, training ML <a id="_idIndexMarker262"/>models! This step enables model training; it has modular scripts or code that perform all the traditional steps in ML training, such as fitting and transforming data to train the model and hyperparameter tuning to converge the best model. The <a id="_idIndexMarker263"/>output of this step is a trained ML model.</p>
			<p>To solve the business problem, we will train two well-known models using the <strong class="bold">Support Vector Machine</strong> classifier and the <strong class="bold">Random Forest</strong> classifier. These are chosen based on their popularity and consistency of results; you are free to choose <a id="_idIndexMarker264"/>models of your choice – there are no limitations in this step. First, we will train the Support Vector Machine classifier and then the Random Forest classifier.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor084"/>Support Vector Machine</h2>
			<p><strong class="bold">Support Vector Machine (SVM)</strong> is a popular <a id="_idIndexMarker265"/>supervised learning algorithm (used for classification and regression). The data points are classified using hyperplanes in an N-dimensional space. It is known for producing significant accuracy with less computation power. It is recommended to know SVM, in theory, to better understand the model training in practice. To learn more about <a id="_idIndexMarker266"/>SVM, head here: <a href="https://www.kdnuggets.com/2017/02/yhat-support-vector-machine.html">https://www.kdnuggets.com/2017/02/yhat-support-vector-machine.html</a>.</p>
			<p>Let's get started with <a id="_idIndexMarker267"/>training the SVM classifier: </p>
			<ol>
				<li value="1">We begin by initiating the training or experiment using the <strong class="source-inline">Experiment()</strong> function from the Azure SDK. The purpose of this function is to start a training run or experiment in order to monitor and log the model training performance in the Azure ML workspace: <p class="source-code">myexperiment = Experiment(workspace, "support-vector-machine")</p></li>
				<li>Similarly, the MLflow experiment is also initiated to observe a different perspective:  <p class="source-code">mlflow.set_experiment("mlflow-support-vector-machine")</p><p>Now we have initiated an experiment in both the Azure ML workspace and MLflow. The following training step will be monitored and logged. </p></li>
				<li>Next, we do hyperparameter tuning to find the best parameters to converge the best model. This can be done manually, but more efficient and automatic solutions such as Grid Search or Random Search exist. For training, the SVM classifier uses Grid Search as follows. We proceed by using the <strong class="source-inline">SVC()</strong> and <strong class="source-inline">Grid SearchCV()</strong> functions from <strong class="source-inline">sklearn</strong> and logging the run on Azure ML and MLflow:<p class="source-code">from sklearn.svm import SVC</p><p class="source-code">from sklearn import svm </p><p class="source-code">from sklearn.model_selection import GridSearchCV</p><p class="source-code">parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}</p><p class="source-code">svc = svm.<strong class="bold">SVC</strong>( )</p><p class="source-code"># initialize a run in Azureml and mlflow experiments</p><p class="source-code">run = myexperiment.start_logging()</p><p class="source-code">mlflow.start_run()</p><p class="source-code">run.log("dataset name", dataset.name)</p><p class="source-code">run.log("dataset Version", dataset.version)</p><p class="source-code">svc_grid = <strong class="bold">GridSearchCV</strong>(svc, parameters)</p><p class="source-code">svc_grid.fit(X_train, y_train)</p><p>The goal of this run or <a id="_idIndexMarker268"/>experiment is to train the best SVM model with the best parameters. Grid Search is used to test the different parameter combinations and optimize the convergence of the algorithm to the best performance. Grid Search takes some time to execute (around 15 minutes on the <strong class="source-inline">STANDARD_DS11_V2</strong> (2 cores, 14 GB RAM) compute machine). The result or the output of the Grid Search suggests the best performing parameters to be <strong class="source-inline">C=1</strong> and the kernel as <strong class="source-inline">rbf</strong>. Using <strong class="source-inline">run.log()</strong>, we have logged the dataset used to train the model (the training set) and keep track of the experiment. This data is logged to the Azure ML workspace and the MLflow experiments. </p></li>
				<li>Finally, using the best parameters, a <a id="_idIndexMarker269"/>new model is trained using <strong class="source-inline">C=1</strong> and <strong class="source-inline">kernel='rbf '</strong> as follows:  <p class="source-code">svc = SVC(C=svc_grid.get_params(deep=True)['estimator__C'], kernel=svc_grid.get_params(deep=True)['estimator__kernel'])</p><p class="source-code">svc.fit(X_train, y_train)</p><p class="source-code"># Logging training parameters to AzureML and MLFlow experiments</p><p class="source-code">run.log("C", svc_grid.get_params(deep=True)['estimator__C'])</p><p class="source-code">run.log("Kernel", svc_grid.get_params(deep=True)['estimator__kernel'])</p><p class="source-code">After training the SVC classifier, the following output is shown:</p><p class="source-code">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</p><p class="source-code">  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',</p><p class="source-code">  kernel='rbf', max_iter=-1, probability=False, random_state=None,</p><p class="source-code">  shrinking=True, tol=0.001, verbose=False)</p><p>With this, we have trained the SVM model! We will now train the Random Forest classifier model. </p></li>
			</ol>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor085"/>Random Forest classifier</h2>
			<p>Random Forest is <a id="_idIndexMarker270"/>another popular supervised learning model (used for classification and regression). Random Forest is an ensemble learning method that operates with a multitude of decision trees. Before performing the model training, it is recommended to know the theoretical working of the Random Forest model. To know more about the <a id="_idIndexMarker271"/>Random Forest model, visit <a href="https://www.kdnuggets.com/2020/01/random-forest-powerful-ensemble-learning-algorithm.html">https://www.kdnuggets.com/2020/01/random-forest-powerful-ensemble-learning-algorithm.html</a>.</p>
			<ol>
				<li value="1">To start <a id="_idIndexMarker272"/>training the Random Forest classifier, initialize the experiment in the Azure ML workspace and the MLflow experiment as follows:<p class="source-code">myexperiment = Experiment(workspace, "support-vector-machine")</p><p class="source-code">mlflow.set_experiment("mlflow-support-vector-machine")</p></li>
				<li>After the experiment is successfully initiated, training can be initiated by importing the <strong class="source-inline">RandomForestClassifier()</strong> function from <strong class="source-inline">sklearn.ensemble</strong> and calling the function with the needed parameters, shown as follows. These parameters are randomly chosen (no <strong class="source-inline">Grid Search</strong> is done). <strong class="source-inline">Grid Search</strong> or <strong class="source-inline">RandomizedSearch</strong> can be used to determine the best parameters and optimize the algorithm:<p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">rf = <strong class="bold">RandomForestClassifier</strong> (max_depth=10, random_state=0, n_estimators=100)</p></li>
				<li>The model training is done using the <strong class="source-inline">fit(X_train, y_train)</strong> function by passing the training data to it. The training dataset and parameters are logged to Azure ML and MLflow experiments as follows:<p class="source-code"># initialize runs in Azureml and mlflow</p><p class="source-code">run = myexperiment.start_logging()</p><p class="source-code">mlflow.start_run()</p><p class="source-code"># Log dataset used </p><p class="source-code">run.log("dataset name", dataset.name)</p><p class="source-code">run.log("dataset Version", dataset.version)</p><p class="source-code">rf.fit(X_train, y_train)</p><p class="source-code"># Logging training parameters to AzureML and MLFlow experiments</p><p class="source-code">run.log("max_depth", 10)</p><p class="source-code">run.log("random_state", 0)</p><p class="source-code">run.log("n_estimators", 100)</p></li>
				<li>After training, the <a id="_idIndexMarker273"/>output is shown as follows: <p class="source-code">RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',</p><p class="source-code">max_depth=10, max_features='auto', max_leaf_nodes=None,</p><p class="source-code">min_impurity_decrease=0.0, min_impurity_split=None,</p><p class="source-code">min_samples_leaf=1, min_samples_split=2,</p><p class="source-code">min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,</p><p class="source-code">oob_score=False, random_state=0, verbose=0, warm_start=False)</p><p>This is the expected result when finishing training the Random Forest model. With this, you have successfully finished training the Random Forest model and, in total, two ML models: the SVM classifier and the Random Forest classifier. </p><p>After training, it is vital to test the performance of the model in terms of accuracy and other metrics to <a id="_idIndexMarker274"/>know whether the model is fit enough for the production or testing environment. </p></li>
			</ol>
			<p>Next, we will test the performance of the trained models on the test data that we split before training the models. </p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor086"/>Model testing and defining metrics</h1>
			<p>In this step, we <a id="_idIndexMarker275"/>evaluate the trained model performance on a <a id="_idIndexMarker276"/>separate set of data points, named test data (which was split and versioned earlier, in the data ingestion step). The inference of the trained model is evaluated according to the selected metrics as per the use case. The output of this step is a <a id="_idIndexMarker277"/>report on the trained model performance. </p>
			<p>To gain a comprehensive analysis of the model performance, we will measure the accuracy,  precision, recall, and f-score. This is what they mean in practice in the context of the business problem: </p>
			<ul>
				<li><strong class="bold">Accuracy</strong>: Number of correct predictions by the total number of predictions of data test samples.</li>
				<li><strong class="bold">Precision</strong>: Precision measures the proportion of positives that were correctly predicted as positive. <em class="italic">Precision = True Positives / (True Positives + False Positives)</em></li>
				<li><strong class="bold">Recall</strong>: Recall measures the proportion of actual positives that were identified correctly. <em class="italic">Recall = True Positives / (True Positives + False Negatives)</em></li>
				<li><strong class="bold">F-score</strong>: Both precision and recall are taken into account in the calculation of the f-score. It is the harmonic mean (average) of precision and recall. <em class="italic">F1 Score = 2*(Recall * Precision) / (Recall + Precision)</em>.</li>
			</ul>
			<p>We will measure these <a id="_idIndexMarker278"/>metrics for the trained model on the <a id="_idIndexMarker279"/>validation <a id="_idIndexMarker280"/>dataset. Let's see the results for the SVM classifier and the Random Forest classifier. </p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor087"/>Testing the SVM classifier</h2>
			<p>Using <strong class="source-inline">sklearn.metrics</strong>, we calculate the <strong class="source-inline">accuracy</strong>, <strong class="source-inline">f1_score</strong>, <strong class="source-inline">precision</strong>, and <strong class="source-inline">recall</strong> for the <a id="_idIndexMarker281"/>model performance on test data samples and log them to the Azure ML workspace and MLflow experiments using the <strong class="source-inline">run.log()</strong> function as follows.</p>
			<p>From <strong class="source-inline">sklearn.metrics</strong>, import <strong class="source-inline">accuracy_score</strong>, <strong class="source-inline">f1_score</strong>, <strong class="source-inline">precision_score</strong>, and <strong class="source-inline">recall_score</strong>:</p>
			<p class="source-code">predicted_svc = svc.predict(X_test)</p>
			<p class="source-code">acc = accuracy_score(y_test, predicted_svc)</p>
			<p class="source-code">fscore = f1_score(y_test, predicted_svc, average="macro")</p>
			<p class="source-code">precision = precision_score(y_test, predicted_svc, average="macro")</p>
			<p class="source-code">recall = recall_score(y_test, predicted_svc, average="macro")</p>
			<p class="source-code">run.log("Test_accuracy", acc)</p>
			<p class="source-code">run.log("Precision", precision)</p>
			<p class="source-code">run.log("Recall", recall)</p>
			<p class="source-code">run.log("F-Score", fscore)</p>
			<p class="source-code">run.log("Git-sha", sha)</p>
			<p>The results of the test data metrics are logged in the Azure ML workspace as per the experiment. You can read these logs later after registering the model (we will register the model in <em class="italic">Registering models and production artifacts</em>).</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor088"/>Testing the Random Forest classifier</h2>
			<p>Similar to what we did for the <a id="_idIndexMarker282"/>SVM classifier model, using <strong class="source-inline">sklearn.metrics</strong> we calculate the <strong class="source-inline">accuracy</strong>, <strong class="source-inline">f1_score</strong>, <strong class="source-inline">precision</strong>, and <strong class="source-inline">recall</strong>: </p>
			<p class="source-code">acc = accuracy_score(y_test, predicted_rf)</p>
			<p class="source-code">fscore = f1_score(y_test, predicted_rf, average="macro")</p>
			<p class="source-code">precision = precision_score(y_test, predicted_rf, average="macro")</p>
			<p class="source-code">recall = recall_score(y_test, predicted_rf, average="macro")</p>
			<p class="source-code">run.log("Test_accuracy", acc)</p>
			<p class="source-code">run.log("Precision", precision)</p>
			<p class="source-code">run.log("Recall", recall)</p>
			<p class="source-code">run.log("F-Score", fscore)</p>
			<p class="source-code">run.log("Git-sha", sha)</p>
			<p>The output of the model performance metrics on test data samples are logged to the Azure ML workspace and MLflow experiments using the <strong class="source-inline">run.log()</strong> function. </p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor089"/>Model packaging </h1>
			<p>After the trained model has <a id="_idIndexMarker283"/>been tested in the previous step, the <a id="_idIndexMarker284"/>model can be serialized into a file to be exported to the test or the production environment. Serialized files come with compatibility challenges, such as model interoperability, if not done right. Model interoperability is a challenge, especially when models are trained using different frameworks. For example, if model 1 is trained using <strong class="source-inline">sklearn</strong> and model 2 is trained using TensorFlow, then model 1 cannot be imported or exported using TensorFlow for further model fine-tuning or model inference. </p>
			<p>To avoid this problem, ONNX offers an open standard for model interoperability. ONNX stands for Open Neural Network Exchange. It provides a serialization standard for importing and exporting models. We will use the ONNX format to serialize the models to avoid compatibility and interoperability issues.</p>
			<p>Using ONNX, the trained model is <a id="_idIndexMarker285"/>serialized using the <strong class="source-inline">skl2onnx</strong> library. The <a id="_idIndexMarker286"/>model is serialized as the file <strong class="source-inline">svc.onnx</strong> for further exporting and importing of the model into test and production environments: </p>
			<p class="source-code"># Convert into SVC model into ONNX format file</p>
			<p class="source-code">from skl2onnx import convert_sklearn</p>
			<p class="source-code">from skl2onnx.common.data_types import FloatTensorType</p>
			<p class="source-code">initial_type = [('float_input', FloatTensorType([None, 6]))]</p>
			<p class="source-code">onx = convert_sklearn(svc, initial_types=initial_type)</p>
			<p class="source-code">with open("outputs/svc.onnx", "wb") as f:</p>
			<p class="source-code">    f.write(onx.SerializeToString())</p>
			<p>The output of this code is a serialized <strong class="source-inline">svc.onnx</strong> file. Similarly, using ONNX, we will convert the Random Forest model into a serialized file named <strong class="source-inline">rf.onnx</strong> for further exporting and importing of the model into test and production environments:</p>
			<p class="source-code"># Convert into RF model into ONNX format file</p>
			<p class="source-code">from skl2onnx import convert_sklearn</p>
			<p class="source-code">from skl2onnx.common.data_types import FloatTensorType</p>
			<p class="source-code">initial_type = [('float_input', FloatTensorType([None, 6]))]</p>
			<p class="source-code">onx = convert_sklearn(rf, initial_types=initial_type)</p>
			<p class="source-code">with open("outputs/rf.onnx", "wb") as f:</p>
			<p class="source-code">    f.write(onx.SerializeToString())</p>
			<p>The output of this code is a serialized <strong class="source-inline">rf.onnx</strong> file. Next, we will register these serialized models to the model registry.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor090"/>Registering models and production artifacts </h1>
			<p>In this step, the model that has been serialized or containerized in the previous step is registered and stored in the model registry. A registered model is compiled as a logical container for one or more files that function as a model. For instance, a model made up of multiple files can be registered as a single model in the model registry. By downloading the registered model, all the files can be received. The registered model can be deployed and used for inference on demand.</p>
			<p>Let's register our serialized models in the previous section by using the <strong class="source-inline">model .register()</strong> function from the Azure ML SDK. By using this function, the serialized ONNX file is registered to the workspace for further use and deploying to the test and production environment. Let's <a id="_idIndexMarker287"/>register the serialized SVM classifier model (<strong class="source-inline">svc.onnx</strong>):</p>
			<p class="source-code"># Register Model on AzureML WS</p>
			<p class="source-code">model = Model.register (model_path = './outputs/svc.onnx', # this points to a local file </p>
			<p class="source-code">                       model_name = "support-vector-classifier", </p>
			<p class="source-code">                       tags = {'dataset': dataset.name, 'version': dataset.version, 'hyparameter-C': '1', 'testdata-accuracy': '0.9519'}, </p>
			<p class="source-code">                       model_framework='pandas==0.23.4',</p>
			<p class="source-code">                       description = "Support vector classifier to predict weather at port of Turku",</p>
			<p class="source-code">                       workspace = workspace)</p>
			<p class="source-code">print('Name:', model.name)</p>
			<p class="source-code">print('Version:', model.version)</p>
			<p>The model is <a id="_idIndexMarker288"/>registered by naming and tagging the model as per the need. We can confirm the successful registering of the model by checking the registered model name and version. The output will reflect the model name you used when registering (for example, <strong class="source-inline">support-vector-classifier</strong>) and will show the model version as <strong class="source-inline">1</strong>. Likewise, let's <a id="_idIndexMarker289"/>register the serialized Random Forest classifier model (<strong class="source-inline">rf.onnx</strong>):</p>
			<p class="source-code"># Register Model on AzureML WS</p>
			<p class="source-code">model = Model.register (model_path = './outputs/rf.onnx', # this points to a local file </p>
			<p class="source-code">                       model_name = "random-forest-classifier", </p>
			<p class="source-code">                       tags = {'dataset': dataset.name, 'version': dataset.version, 'hyparameter-C': '1', 'testdata-accuracy': '0.9548'}, </p>
			<p class="source-code">                       model_framework='pandas==0.23.4',</p>
			<p class="source-code">                       description = "Random forest classifier to predict weather at port of Turku",</p>
			<p class="source-code">                       workspace = workspace)</p>
			<p class="source-code">print('Name:', model.name)</p>
			<p class="source-code">print('Version:', model.version)</p>
			<p>After successful registering of the model, the output of the <strong class="source-inline">print</strong> function will reflect the model name you used while registering (<strong class="source-inline">random-forest-classifier</strong>) and will show the model version as <strong class="source-inline">1</strong>. Lastly, we will register production artifacts for inference. Now you can see both models in the <strong class="bold">Models</strong> section of the Azure ML workspace as shown in <em class="italic">Figure 4.8</em>:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B16572_04_008.jpg" alt="Figure 4.9 – Registered SVM model (with test metrics)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – Registered SVM model (with test metrics)</p>
			<p>This way, you can <a id="_idIndexMarker290"/>visualize and analyze your training and testing logs for each model trained in the Azure ML workspace. It offers a bird's-eye view of training and testing the model while enabling traceability for registered models.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor091"/>Registering production artifacts</h2>
			<p>For model inference in real time, a scalar is <a id="_idIndexMarker291"/>needed in order to scale the incoming data on the scale at which the data was scaled for ML training. We will use the same scaler function used for <strong class="source-inline">scaling X_train</strong> using <strong class="source-inline">sc.fit_transform(X_train)</strong> and serialize this variable into a <strong class="source-inline">pickle</strong> file. Lastly, we register this <strong class="source-inline">pickle</strong> file to the workspace for further retrieval and usage as needed (especially for model inference in the test and production environment). Using <strong class="source-inline">pickle</strong>, write the scaler variable <strong class="source-inline">sc</strong> into a <strong class="source-inline">pickle</strong> file using the <strong class="source-inline">pickle.dump()</strong> function as follows.</p>
			<p>Import <strong class="source-inline">pickle</strong> with <strong class="source-inline">open('./outputs/scaler.pkl', 'wb') as scaler_pkl</strong>:</p>
			<p class="source-code">    pickle.dump(sc, scaler_pkl)</p>
			<p>The output of the code will save a serialized <strong class="source-inline">pickle</strong> file for the scaler with the filename <strong class="source-inline">scaler.pkl</strong>. Next, we will <a id="_idIndexMarker292"/>register this file to the model registry to later download and deploy together with our models for inference. The scaler is registered using the <strong class="source-inline">model .register()</strong> function as follows:</p>
			<p class="source-code"># Register Model on AzureML WS</p>
			<p class="source-code">scaler = Model.register(model_path = './outputs/scaler.pkl', # this points to a local file </p>
			<p class="source-code">                       model_name = "scaler", # this is the name the model is registered as</p>
			<p class="source-code">                       tags = {'dataset': dataset.name, 'version': dataset.version}, </p>
			<p class="source-code">                       model_framework='pandas==0.23.4',</p>
			<p class="source-code">                       description = "Scaler used for scaling incoming inference data",</p>
			<p class="source-code">                       workspace = workspace)</p>
			<p class="source-code">print('Name:', scaler.name)</p>
			<p class="source-code">print('Version:', scaler.version)</p>
			<p>Upon saving and registering the scaler object, a registered object can be found on the Azure ML workspace. Likewise, registered models can be tracked, as shown in <em class="italic">Figure 4.8</em>:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B16572_04_009.jpg" alt="Figure 4.10 – Registered models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Registered models</p>
			<p>Congratulations! Both the <a id="_idIndexMarker293"/>SVM classifier and Random Forest classifier, along with the serialized scaler, are registered in the model registry. These models can be downloaded and deployed later. This brings us to the successful implementation of the ML pipeline!</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor092"/>Summary</h1>
			<p>In this chapter, we went through the theory of ML pipelines and practiced them by building ML pipelines for a business problem. We set up tools, resources, and the development environment for training these ML models. We started with the data ingestion step, followed by the model training step, testing step, and packaging step, and finally, we completed the registering step. Congrats! So far, you have implemented a critical building block of the MLOps workflow. </p>
			<p>In the next chapter, we will look into evaluating and packaging production models.</p>
		</div>
	</body></html>