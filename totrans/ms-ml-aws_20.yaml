- en: Tuning Clusters for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many data scientists and machine learning practitioners face the problem of
    scale when attempting to run ML data pipelines over big data. In this chapter,
    we will focus primarily on **Elastic MapReduce** (**EMR**), which is a very powerful
    tool for running very large machine learning jobs. There are many ways to configure
    EMR and not every setup works for every scenario. In this chapter, we will outline
    the main configurations of EMR and how each configuration works for different
    objectives. Additionally, we will present AWS Glue as a tool to catalog the results
    of our big data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the EMR architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning EMR for different applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing data pipelines with Glue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the EMR architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting User
    Behavior with Tree-Based Methods*, we introduced EMR, which is an AWS service
    that allows us to run and scale Apache Spark, Hadoop, HBase, Presto, Hive, and
    other big data frameworks. These big data frameworks typically require a cluster
    of machines running specific software that are correctly configured so that the
    machines are able to communicate with each other. Let's look at the most commonly
    used products within EMR.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many applications, such as Spark and HBase, require Hadoop. The basic installation
    of Hadoop comes with two main services:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hadoop Distributed Filesystem** (**HDFS**): This is a service that allows
    us to store large amounts of data (for example, files that cannot be stored on
    a single machine) across many servers. A NameNode server is responsible for indexing
    which blocks of which file are stored in which server. The blocks of each file
    are replicated across the cluster so that if a machine goes down, we don''t lose
    any information. DataNode servers are responsible for keeping and serving the
    data on each machine. Many other EMR services, such as Apache HBase, Presto, and
    Apache Spark, are able to use HDFS to read and write data. HDFS works well when
    you are using long-running clusters. For clusters that are launched just for the
    purpose of a single job (such as a training job), you should consider having the
    data storage in S3 instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MapReduce**: This framework was the basis for big data crunching for many
    years. By allowing users to specify two functions (a `map` function and a `reduce`
    function), many big data workloads were made possible. The map function is responsible
    for taking chunks of data and transforming them in a one-to-one fashion (for example,
    take the price of every transaction). The reduce function takes the output of
    the map function and aggregates it in some way (such as finding the average transaction
    price per region). MapReduce was designed so that the processing was done on the
    same machines that we store the HDFS file blocks on, to avoid sending large amounts
    of data over the network. This data locality principle proved to be very effective
    for running big data jobs on commodity hardware and with limited network speeds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'EMR allows you to create clusters with three types of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Master node**: This is unique in a cluster and is typically responsible for
    orchestrating work throughout other nodes in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Core nodes**: These kinds of nodes will host HDFS blocks and run a DataNode
    server, hence job tasks running on these nodes may take advantage of data locality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task nodes**: These nodes do not host HDFS blocks but can run arbitrary job
    tasks. Tasks running on these nodes will need to read data from filesystems hosted
    on other machines (for example, core nodes or S3 servers).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is one of the most popular big data frameworks. It extends the
    idea of MapReduce by allowing the user to specify additional high-level functions
    on top of the data. It can perform map and reduce functions but also supports
    filter, group, join, window functions, and many other operations. Additionally,
    as we have seen throughout this book, we can use SQL operations to perform ETL
    and analytics. Apache Spark was designed to cache large amounts of data in-memory
    to speed up algorithms that require several passes of the data. For example, algorithms
    that require several iterations of **gradient descent** can run orders of magnitude
    faster if the datasets are cached in-memory.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark also comes with a number of very useful libraries for streaming,
    graph manipulation, and the machine learning ones that we have used throughout
    this book. We encourage you to explore these additional libraries as they are
    extremely high-quality and useful. Spark is unique in that it seamlessly integrates
    many well-developed libraries together, such as TensorFlow and `scikit-learn`.
    You can build excellent models with both of these tools, but they do not currently
    allow us to read and prepare data by parallelizing the work in a cluster like
    Spark does. In other words, Apache Spark provides the full stack of packages,
    from data ingestion to model generation. Some people refer to Spark as the operating
    system for big data. Often, data scientists and engineers use Spark to perform
    data preparation at scale and then use other tools, such as TensorFlow and SageMaker
    to build and deploy specialized models. In [Chapter 5](ccd8e969-f651-4fb9-8ef2-026286577e70.xhtml) ,
    *Customer Segmentation Using Clustering Algorithms,* we saw how we can smoothly
    integrate Apache Spark and SageMaker through the use of SageMaker Spark estimators.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Hive was born as a translator from SQL to MapReduce jobs. You can specify
    **Data Definition Language** (**DDL**) and **Data Manipulation Language** (**DML**)
    statements and work with SQL as if you were working on a standard database management
    system using Apache Hive. Many non-technical users that knew SQL could perform
    analytics at scale when Hive first appeared, which was one of the reasons for
    its popularity. What happens under the hood with Hive (and with Spark SQL) is
    that the SQL statement is parsed and a series of MapReduce jobs are constructed
    on the fly and run on the cluster to perform the declarative operation described
    by the SQL statement.
  prefs: []
  type: TYPE_NORMAL
- en: Presto
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Presto is a product developed by Facebook that also translates SQL into big
    data workloads but is tailored for interactive analytics. It is extremely fast
    and is specially optimized for when you have a large fact table and several smaller-dimension
    tables (such as a transaction and other joined tables, such as a product and clients).
    AWS provides a serverless alternative based on Presto, called Athena, which is
    great when your data is on S3\. Athena queries are charged based on how much data
    is scanned. For this reason, it has become extremely popular for big data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Apache HBase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HBase is a product similar to Google's Bigtable. Conceptually, it can be seen
    as a huge distributed key-value store. HBase is not as popular anymore due to
    the appearance of technologies such as AWS DynamoDB, which is serverless and,
    in our experience, more reliable. However, it can be a cost-effective way to store
    data when you need to access it through keys. For example, you could use HBase
    to store a custom model for each user (on the assumption that you have billions
    of users to justify it).
  prefs: []
  type: TYPE_NORMAL
- en: Yet Another Resource Negotiator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Hadoop also developed **Yet Another Resource Negotiator** (**YARN**),
    which is the fundamental tool with which EMR schedules and coordinates different
    applications. YARN is effectively the cluster manager behind EMR and is responsible
    for launching the necessary daemons on different machines. When you configure
    a cluster through EMR, you can specify the different applications that you want
    to run. Examples of such applications are Spark, HBase, and Presto. YARN is responsible
    for launching the necessary processes. In the case of Spark, YARN will launch
    Spark executors and drivers as needed. Each of these processes reports the necessary
    memory and CPU code consumption to YARN. That way, YARN can make sure that the
    cluster load is properly managed and not overloaded.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning EMR for different applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: in this section we will consider the aspects involved in tuning the clusters
    we use for machine learning. When you launch an EMR cluster, you can specify the
    different applications you want to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the applications available in EMR version 5.23.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/559d2782-2eb6-49c0-8f69-06dc67d60f14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Upon launching an EMR cluster, these are the most relevant items that need
    to be configured:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Applications**: Applications such as Spark).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware**: We covered this in [Chapter 10](06270fa5-1364-4ad2-b4a0-3522c1ef7bcd.xhtml),
    *Creating Clusters on AWS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use of the Glue Data Catalog**: We''ll cover this in the last section of
    this chapter, *Managing data pipelines with Glue*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software configuration**: These are properties that we can specify to configure
    application-specific properties. In the next section, *Configuring application
    properties,* we''ll show how to customize the behavior of Spark through specific
    properties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootstrap actions**: These are user-specific scripts (typically located in
    S3) that will run on every node of the cluster as it boots up. Bootstrap actions
    are useful, for example, when you want to install a specific package on all the
    machines of the cluster upon startup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Steps**: These are the different jobs that the user wants to run once the
    applications are up. For example, if we want to launch a cluster that runs a training
    job in Spark and then we want to shut down the cluster, we would specify a Spark
    job step and check the auto-terminate cluster after the last step is complete
    option. Such a use case is pertinent when we are launching a cluster programmatically
    (via the AWS API). Scheduled or event-driven AWS Lambda functions can use libraries
    such as `boto3` to launch clusters programmatically upon the occurrence of an
    event, or on a regular schedule. More information about AWS Lambda can be found
    at [https://docs.aws.amazon.com/lambda/](https://docs.aws.amazon.com/lambda/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring application properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding screenshot, you might have noticed that there is a space called
    **Software Settings** for customizing the configuration of different applications.
    There are different categories of configurations, named `classifications`, that
    allow you to override the default configuration of the different applications
    by changing the values for a chosen set of properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, we provide a very useful set of properties to
    configure Spark for two things: maximizing resource allocation and enabling the
    AWS Glue metastore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at the effect of each of these configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Maximize Resource Allocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you enable `maximizeResourceAllocation`, EMR and Spark will figure out
    how to configure Spark so as to use all of the available resources (for example,
    memory and CPU). The alternative is to manually configure properties such as the
    number of executors, Java heap space for each executor, and the number of cores
    (that is, threads) for each executor. If you choose to do this manually, you need
    to take great care not to exceed the available resources of the cluster (and also
    not to underuse the available hardware). We recommend having this setting always
    set by default.
  prefs: []
  type: TYPE_NORMAL
- en: The AWS Glue Catalog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Glue provides a service that is known as a Hive metastore. The purpose of
    this service is to keep track of all the data in our data lake by defining tables
    that describe the data. A data lake is typically hosted on S3 or HDFS. Any data
    that lies on these distributed filesystems, and has a tabular format, such as
    Parquet or CSV, can be added to the metastore. This does not copy or move the
    data; it is just a way of keeping a catalog of all our data. By configuring the
    `hive.metastore.client.factory.class` property in the cluster configuration, we
    allow Spark to use all the tables registered in the Glue Catalog. Additionally,
    Spark can also create a new table or modify the catalog through Spark SQL statements.
    In the next section, we will show a concrete example of how Glue is useful.
  prefs: []
  type: TYPE_NORMAL
- en: Managing data pipelines with Glue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data scientists and data engineers run different jobs to transform, extract,
    and load data into systems such as S3\. For example, we might have a daily job
    that processes text data and stores a table with the bag-of-words table representation
    that we saw in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes*. We might want to update the table each day to
    point to the latest available data. Upstream processes can then only rely on the
    table name to find and process the latest version of the data. If we do not catalog
    this data properly, it will be very hard to combine the different data sources
    or even to know where the data is located, which is where AWS Glue metastore comes
    in. Tables in Glue are grouped into databases. However, tables in different databases
    can be joined and referenced.
  prefs: []
  type: TYPE_NORMAL
- en: Creating tables with Glue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can access the Glue console on AWS by going to [https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=databases](https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=databases).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the console, create a new database, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03081b05-181c-4c95-a244-db0732623893.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the database is created, you can switch to the Athena AWS service and start
    creating tables from our data in S3 to run queries for analytics. The AWS Athena
    console can be accessed at [https://console.aws.amazon.com/athena/home](https://console.aws.amazon.com/athena/home).
  prefs: []
  type: TYPE_NORMAL
- en: Let's create a table in S3 for the Boston house prices dataset that we worked
    on in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting House
    Value with Regression Algorithms*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot we can see how the create table SQL statement will
    specify the name, format, and fields of the table from our CSV data located in
    S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2127dbe-4e33-4297-8098-b8f5b623f53c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note, that the location specifies a folder (not a file). In our case, we have
    a single `CSV` folder at `s3://mastering-ml-aws/chapter3/linearmodels/train/training-housing.csv`.
    However, we could have many CSVs on the same folder and all would be linked to
    the `house_prices` table we just created. Once we create the table, since the
    data is in S3, we can start querying our table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bb5d075-8139-44d4-b3a9-1cb5dd505239.png)'
  prefs: []
  type: TYPE_IMG
- en: Note how the data is tabulated correctly. This is because we have told Glue
    the right format and location of our data. Now we can run ultra-fast analytics
    using SQL with Presto-as-a-service through Athena.
  prefs: []
  type: TYPE_NORMAL
- en: We just performed a create table operation; however, often, we want to perform
    alter table commands to switch the underlying data behind a table to a more recent
    version. It's also very common to perform add-partition operations to incrementally
    add data to a table (such as new batches or dates). Partitions also help the query
    engine to filter the data more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Glue tables in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the table created is in Glue, it will also become available on every EMR
    Spark cluster (as long as we configure the `hive.metastore.client.factory.class`
    described in the previous section,  *Tuning EMR for different applications*).
    Let's launch an EMR cluster with the JupyterHub application enabled. The JupyterHub
    application is an alternative to the EMR notebooks feature we used throughout
    [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter
    Feeds with Naive Bayes*, to [Chapter 6](c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml), *Analyzing
    Visitor Patterns to Make Recommendations*. Consider using `JupyterHub` when you
    have a team of data scientists reusing the same cluster and running different
    notebooks. You can learn more on JupyterHub at [https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub.html](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows our cluster created with the Glue metastore
    enabled and `JupyterHub` as the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/086ac49e-8153-4744-9384-b694ffa4ac74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you click on the `JupyterHub` link, it will take you to an authentication
    page, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47db6713-f8da-4a3d-b2e2-8e5da74444ec.png)'
  prefs: []
  type: TYPE_IMG
- en: The default configuration of `JupyterHub` has a default user account with a
    username of `jovyan` and a password of `jupyter` available. The authentication
    can be customized through the EMR configuration if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we authenticate, we can start creating notebooks exactly as we did with
    EMR notebooks. In this case, we will create a `PySpark3` notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6931e08-4e0a-466a-90ec-8d8b017b3e58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, notebooks can use SparkMagic to interleave paragraphs in Python and SQL.
    Let''s look at the following notebook example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71792244-9d3d-44bc-b87d-c627d4f7ab15.png)'
  prefs: []
  type: TYPE_IMG
- en: The first paragraph runs a SQL on the table we just created through Glue/Athena
    through SparkMagic's `%%sql` magic (more on SparkMagic can be found at [https://github.com/jupyter-incubator/sparkmagic](https://github.com/jupyter-incubator/sparkmagic)).
    The second paragraph constructs a Spark DataFrame through a simple SQL statement
    that selects two fields from our table. The third paragraph runs a Spark job (that
    is, the describe command) over the Spark DataFrame we constructed. You will appreciate
    how easy it is to handle, integrate, and process data once we have properly cataloged
    it in our Glue metastore.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the main configuration parameters of EMR and how
    they can help us run many big data frameworks, such as Spark, Hive, and Presto.
    We also explored the AWS services of Athena and Glue as a way to catalog the data
    on our data lake so that we can properly synchronize our data pipelines. Finally,
    we demonstrated how Glue can also be used in EMR, with smooth integration for
    `JupyterHub` with SparkMagic.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter,* Deploying Models Built in AWS*, we will cover how to deploy
    machine learning models in different environments.
  prefs: []
  type: TYPE_NORMAL
