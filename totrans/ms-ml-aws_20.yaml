- en: Tuning Clusters for Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为机器学习调整集群
- en: Many data scientists and machine learning practitioners face the problem of
    scale when attempting to run ML data pipelines over big data. In this chapter,
    we will focus primarily on **Elastic MapReduce** (**EMR**), which is a very powerful
    tool for running very large machine learning jobs. There are many ways to configure
    EMR and not every setup works for every scenario. In this chapter, we will outline
    the main configurations of EMR and how each configuration works for different
    objectives. Additionally, we will present AWS Glue as a tool to catalog the results
    of our big data pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学家和机器学习实践者在尝试在大数据上运行ML数据管道时都会遇到规模问题。在本章中，我们将主要关注**弹性MapReduce**（**EMR**），这是一个运行非常大的机器学习作业的非常强大的工具。配置EMR有许多方法，并不是每个设置都适用于每个场景。在本章中，我们将概述EMR的主要配置以及每种配置如何针对不同的目标工作。此外，我们将介绍AWS
    Glue作为我们的大数据管道结果编目工具。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to the EMR architecture
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR架构简介
- en: Tuning EMR for different applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为不同应用调整EMR
- en: Managing data pipelines with Glue
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Glue管理数据管道
- en: Introduction to the EMR architecture
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EMR架构简介
- en: In [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting User
    Behavior with Tree-Based Methods*, we introduced EMR, which is an AWS service
    that allows us to run and scale Apache Spark, Hadoop, HBase, Presto, Hive, and
    other big data frameworks. These big data frameworks typically require a cluster
    of machines running specific software that are correctly configured so that the
    machines are able to communicate with each other. Let's look at the most commonly
    used products within EMR.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](af506fc8-f482-453e-8162-93a676b2e737.xhtml) *使用基于树的预测用户行为* 中，我们介绍了EMR，这是一个AWS服务，允许我们运行和扩展Apache
    Spark、Hadoop、HBase、Presto、Hive和其他大数据框架。这些大数据框架通常需要运行特定软件的机器集群，这些机器配置正确，以便机器能够相互通信。让我们看看EMR中最常用的产品。
- en: Apache Hadoop
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Hadoop
- en: 'Many applications, such as Spark and HBase, require Hadoop. The basic installation
    of Hadoop comes with two main services:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序，例如Spark和HBase，都需要Hadoop。Hadoop的基本安装包含两个主要服务：
- en: '**Hadoop Distributed Filesystem** (**HDFS**): This is a service that allows
    us to store large amounts of data (for example, files that cannot be stored on
    a single machine) across many servers. A NameNode server is responsible for indexing
    which blocks of which file are stored in which server. The blocks of each file
    are replicated across the cluster so that if a machine goes down, we don''t lose
    any information. DataNode servers are responsible for keeping and serving the
    data on each machine. Many other EMR services, such as Apache HBase, Presto, and
    Apache Spark, are able to use HDFS to read and write data. HDFS works well when
    you are using long-running clusters. For clusters that are launched just for the
    purpose of a single job (such as a training job), you should consider having the
    data storage in S3 instead.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop分布式文件系统**（**HDFS**）：这是一个允许我们在多个服务器上存储大量数据（例如，无法存储在单个机器上的文件）的服务。NameNode服务器负责索引哪个文件的哪些块存储在哪个服务器上。每个文件的块在集群中复制，这样如果一台机器出现故障，我们不会丢失任何信息。DataNode服务器负责在每个机器上保持和提供数据。许多其他EMR服务，如Apache
    HBase、Presto和Apache Spark，能够使用HDFS来读取和写入数据。当您使用长期运行的集群时，HDFS运行良好。对于仅为了执行单个作业（例如训练作业）而启动的集群，您应考虑使用S3进行数据存储。'
- en: '**MapReduce**: This framework was the basis for big data crunching for many
    years. By allowing users to specify two functions (a `map` function and a `reduce`
    function), many big data workloads were made possible. The map function is responsible
    for taking chunks of data and transforming them in a one-to-one fashion (for example,
    take the price of every transaction). The reduce function takes the output of
    the map function and aggregates it in some way (such as finding the average transaction
    price per region). MapReduce was designed so that the processing was done on the
    same machines that we store the HDFS file blocks on, to avoid sending large amounts
    of data over the network. This data locality principle proved to be very effective
    for running big data jobs on commodity hardware and with limited network speeds.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MapReduce**：这个框架多年来一直是大数据处理的基础。通过允许用户指定两个函数（一个 `map` 函数和一个 `reduce` 函数），许多大数据工作负载得以实现。map
    函数负责将数据块取出来并以一对一的方式进行转换（例如，获取每笔交易的价格）。reduce 函数接收 map 函数的输出并以某种方式聚合它（例如，找出每个地区的平均交易价格）。MapReduce
    被设计成在存储 HDFS 文件块的同一台机器上执行处理，以避免在网络中传输大量数据。这种数据本地性原则被证明对于在通用硬件上运行大数据作业以及有限的网络速度下运行大数据作业非常有效。'
- en: 'EMR allows you to create clusters with three types of nodes:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: EMR 允许您创建包含三种类型节点的集群：
- en: '**Master node**: This is unique in a cluster and is typically responsible for
    orchestrating work throughout other nodes in the cluster.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主节点**：这是集群中唯一的节点，通常负责协调集群中其他节点的作业。'
- en: '**Core nodes**: These kinds of nodes will host HDFS blocks and run a DataNode
    server, hence job tasks running on these nodes may take advantage of data locality.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心节点**：这类节点将托管 HDFS 块并运行 DataNode 服务器，因此在这些节点上运行的作业可以利用数据本地性。'
- en: '**Task nodes**: These nodes do not host HDFS blocks but can run arbitrary job
    tasks. Tasks running on these nodes will need to read data from filesystems hosted
    on other machines (for example, core nodes or S3 servers).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务节点**：这些节点不托管 HDFS 块，但可以运行任意作业任务。在这些节点上运行的作业将需要从其他机器上托管（例如，核心节点或 S3 服务器）的文件系统中读取数据。'
- en: Apache Spark
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark
- en: Apache Spark is one of the most popular big data frameworks. It extends the
    idea of MapReduce by allowing the user to specify additional high-level functions
    on top of the data. It can perform map and reduce functions but also supports
    filter, group, join, window functions, and many other operations. Additionally,
    as we have seen throughout this book, we can use SQL operations to perform ETL
    and analytics. Apache Spark was designed to cache large amounts of data in-memory
    to speed up algorithms that require several passes of the data. For example, algorithms
    that require several iterations of **gradient descent** can run orders of magnitude
    faster if the datasets are cached in-memory.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是最受欢迎的大数据框架之一。它通过允许用户在数据之上指定额外的函数来扩展 MapReduce 的概念。它不仅可以执行 map
    和 reduce 函数，还支持过滤、分组、连接、窗口函数以及许多其他操作。此外，正如我们在整本书中看到的那样，我们可以使用 SQL 操作来执行 ETL 和分析。Apache
    Spark 被设计用来在内存中缓存大量数据以加速需要多次遍历数据的算法。例如，需要多次迭代 **梯度下降** 的算法如果数据集在内存中缓存，可以运行得快得多。
- en: Apache Spark also comes with a number of very useful libraries for streaming,
    graph manipulation, and the machine learning ones that we have used throughout
    this book. We encourage you to explore these additional libraries as they are
    extremely high-quality and useful. Spark is unique in that it seamlessly integrates
    many well-developed libraries together, such as TensorFlow and `scikit-learn`.
    You can build excellent models with both of these tools, but they do not currently
    allow us to read and prepare data by parallelizing the work in a cluster like
    Spark does. In other words, Apache Spark provides the full stack of packages,
    from data ingestion to model generation. Some people refer to Spark as the operating
    system for big data. Often, data scientists and engineers use Spark to perform
    data preparation at scale and then use other tools, such as TensorFlow and SageMaker
    to build and deploy specialized models. In [Chapter 5](ccd8e969-f651-4fb9-8ef2-026286577e70.xhtml) ,
    *Customer Segmentation Using Clustering Algorithms,* we saw how we can smoothly
    integrate Apache Spark and SageMaker through the use of SageMaker Spark estimators.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 还附带了一些非常实用的库，用于流处理、图操作以及我们在本书中使用的机器学习库。我们鼓励你探索这些额外的库，因为它们质量极高且非常有用。Spark
    的独特之处在于它无缝地整合了许多成熟的库，例如 TensorFlow 和 `scikit-learn`。你可以使用这两个工具构建出色的模型，但它们目前不允许我们像
    Spark 那样通过在集群中并行化工作来读取和准备数据。换句话说，Apache Spark 提供了从数据摄入到模型生成的完整堆栈的包。有些人将 Spark
    称为大数据的操作系统。通常，数据科学家和工程师使用 Spark 进行大规模的数据准备，然后使用其他工具，如 TensorFlow 和 SageMaker 来构建和部署专门的模型。在
    [第 5 章](ccd8e969-f651-4fb9-8ef2-026286577e70.xhtml) 中，我们看到了如何通过使用 SageMaker Spark
    估计器来平滑地整合 Apache Spark 和 SageMaker。
- en: Apache Hive
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Hive
- en: Apache Hive was born as a translator from SQL to MapReduce jobs. You can specify
    **Data Definition Language** (**DDL**) and **Data Manipulation Language** (**DML**)
    statements and work with SQL as if you were working on a standard database management
    system using Apache Hive. Many non-technical users that knew SQL could perform
    analytics at scale when Hive first appeared, which was one of the reasons for
    its popularity. What happens under the hood with Hive (and with Spark SQL) is
    that the SQL statement is parsed and a series of MapReduce jobs are constructed
    on the fly and run on the cluster to perform the declarative operation described
    by the SQL statement.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hive 最初作为一个从 SQL 到 MapReduce 作业的翻译器诞生。你可以指定 **数据定义语言** (**DDL**) 和 **数据操作语言**
    (**DML**) 语句，并像使用 Apache Hive 一样在标准数据库管理系统上工作。当 Hive 首次出现时，许多了解 SQL 的非技术用户能够进行大规模的分析，这是其受欢迎的原因之一。Hive（以及
    Spark SQL）内部发生的事情是，SQL 语句被解析，并动态构建一系列 MapReduce 作业，在集群上运行以执行 SQL 语句描述的声明性操作。
- en: Presto
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Presto
- en: Presto is a product developed by Facebook that also translates SQL into big
    data workloads but is tailored for interactive analytics. It is extremely fast
    and is specially optimized for when you have a large fact table and several smaller-dimension
    tables (such as a transaction and other joined tables, such as a product and clients).
    AWS provides a serverless alternative based on Presto, called Athena, which is
    great when your data is on S3\. Athena queries are charged based on how much data
    is scanned. For this reason, it has become extremely popular for big data analytics.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Presto 是由 Facebook 开发的一个产品，它也把 SQL 转换为大数据工作负载，但专为交互式分析而定制。它非常快，并且特别针对当你有一个大型事实表和几个小维度表（如交易和其他连接表，如产品和客户）时进行了优化。AWS
    提供了一个基于 Presto 的无服务器替代方案，称为 Athena，当你的数据在 S3 上时，它非常出色。Athena 查询的收费基于扫描的数据量。因此，它已成为大数据分析中非常受欢迎的工具。
- en: Apache HBase
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache HBase
- en: HBase is a product similar to Google's Bigtable. Conceptually, it can be seen
    as a huge distributed key-value store. HBase is not as popular anymore due to
    the appearance of technologies such as AWS DynamoDB, which is serverless and,
    in our experience, more reliable. However, it can be a cost-effective way to store
    data when you need to access it through keys. For example, you could use HBase
    to store a custom model for each user (on the assumption that you have billions
    of users to justify it).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: HBase 是一个类似于 Google Bigtable 的产品。从概念上讲，它可以被视为一个巨大的分布式键值存储。由于 AWS DynamoDB 等技术的出现，HBase
    的受欢迎程度已经不再那么高，后者是无服务器的，根据我们的经验，更加可靠。然而，当你需要通过键访问数据时，它可能是一个成本效益高的存储数据的方式。例如，你可以使用
    HBase 存储每个用户的自定义模型（假设你有数十亿用户来证明这一点）。
- en: Yet Another Resource Negotiator
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一个资源协商者
- en: Apache Hadoop also developed **Yet Another Resource Negotiator** (**YARN**),
    which is the fundamental tool with which EMR schedules and coordinates different
    applications. YARN is effectively the cluster manager behind EMR and is responsible
    for launching the necessary daemons on different machines. When you configure
    a cluster through EMR, you can specify the different applications that you want
    to run. Examples of such applications are Spark, HBase, and Presto. YARN is responsible
    for launching the necessary processes. In the case of Spark, YARN will launch
    Spark executors and drivers as needed. Each of these processes reports the necessary
    memory and CPU code consumption to YARN. That way, YARN can make sure that the
    cluster load is properly managed and not overloaded.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop还开发了**另一个资源协调器**（**YARN**），这是EMR调度和协调不同应用的基本工具。YARN实际上是EMR背后的集群管理器，负责在不同的机器上启动必要的守护进程。当你通过EMR配置集群时，你可以指定你想要运行的不同应用。这类应用的例子包括Spark、HBase和Presto。YARN负责启动必要的进程。在Spark的情况下，YARN将根据需要启动Spark执行器和驱动器。这些进程将必要的内存和CPU消耗报告给YARN。这样，YARN可以确保集群负载得到适当管理，不会过载。
- en: Tuning EMR for different applications
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整EMR以适应不同的应用
- en: in this section we will consider the aspects involved in tuning the clusters
    we use for machine learning. When you launch an EMR cluster, you can specify the
    different applications you want to run.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考虑调整我们用于机器学习的集群所涉及到的方面。当你启动一个EMR集群时，你可以指定你想要运行的不同应用。
- en: 'The following screenshot shows the applications available in EMR version 5.23.0:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了EMR版本5.23.0中可用的应用：
- en: '![](img/559d2782-2eb6-49c0-8f69-06dc67d60f14.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/559d2782-2eb6-49c0-8f69-06dc67d60f14.png)'
- en: 'Upon launching an EMR cluster, these are the most relevant items that need
    to be configured:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动EMR集群后，以下是需要配置的最相关项目：
- en: '**Applications**: Applications such as Spark).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用**: 例如Spark应用。'
- en: '**Hardware**: We covered this in [Chapter 10](06270fa5-1364-4ad2-b4a0-3522c1ef7bcd.xhtml),
    *Creating Clusters on AWS*.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件**: 我们在[第10章](06270fa5-1364-4ad2-b4a0-3522c1ef7bcd.xhtml)中介绍了这一点，*在AWS上创建集群*。'
- en: '**Use of the Glue Data Catalog**: We''ll cover this in the last section of
    this chapter, *Managing data pipelines with Glue*).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用Glue数据目录**: 我们将在本章的最后部分介绍，*使用Glue管理数据管道*)。'
- en: '**Software configuration**: These are properties that we can specify to configure
    application-specific properties. In the next section, *Configuring application
    properties,* we''ll show how to customize the behavior of Spark through specific
    properties.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件配置**: 这些是我们可以指定以配置特定应用属性的属性。在下一节，*配置应用属性*中，我们将展示如何通过特定属性来定制Spark的行为。'
- en: '**Bootstrap actions**: These are user-specific scripts (typically located in
    S3) that will run on every node of the cluster as it boots up. Bootstrap actions
    are useful, for example, when you want to install a specific package on all the
    machines of the cluster upon startup.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引导操作**: 这些是用户特定的脚本（通常位于S3），在集群启动时会运行在每个节点上。引导操作在例如你希望在集群启动时在所有机器上安装特定软件包时非常有用。'
- en: '**Steps**: These are the different jobs that the user wants to run once the
    applications are up. For example, if we want to launch a cluster that runs a training
    job in Spark and then we want to shut down the cluster, we would specify a Spark
    job step and check the auto-terminate cluster after the last step is complete
    option. Such a use case is pertinent when we are launching a cluster programmatically
    (via the AWS API). Scheduled or event-driven AWS Lambda functions can use libraries
    such as `boto3` to launch clusters programmatically upon the occurrence of an
    event, or on a regular schedule. More information about AWS Lambda can be found
    at [https://docs.aws.amazon.com/lambda/](https://docs.aws.amazon.com/lambda/).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤**: 这些是在应用启动后用户想要运行的不同作业。例如，如果我们想要启动一个运行Spark训练作业的集群，然后我们想要关闭集群，我们就会指定一个Spark作业步骤，并在最后一步完成后选择自动终止集群选项。这种用例在通过AWS
    API程序化启动集群时是相关的。计划或事件驱动的AWS Lambda函数可以使用`boto3`等库在事件发生或定期计划时程序化地启动集群。有关AWS Lambda的更多信息，请参阅[https://docs.aws.amazon.com/lambda/](https://docs.aws.amazon.com/lambda/)。'
- en: Configuring application properties
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置应用属性
- en: In the preceding screenshot, you might have noticed that there is a space called
    **Software Settings** for customizing the configuration of different applications.
    There are different categories of configurations, named `classifications`, that
    allow you to override the default configuration of the different applications
    by changing the values for a chosen set of properties.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，您可能已经注意到有一个名为 **软件设置** 的空间，用于自定义不同应用程序的配置。有不同的配置类别，称为 `分类`，允许您通过更改所选属性集的值来覆盖不同应用程序的默认配置。
- en: 'In the following code block, we provide a very useful set of properties to
    configure Spark for two things: maximizing resource allocation and enabling the
    AWS Glue metastore:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们提供了一组非常有用的属性来配置 Spark，用于两个目的：最大化资源分配并启用 AWS Glue 元数据存储：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's look at the effect of each of these configurations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每个这些配置的效果。
- en: Maximize Resource Allocation
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大化资源分配
- en: When you enable `maximizeResourceAllocation`, EMR and Spark will figure out
    how to configure Spark so as to use all of the available resources (for example,
    memory and CPU). The alternative is to manually configure properties such as the
    number of executors, Java heap space for each executor, and the number of cores
    (that is, threads) for each executor. If you choose to do this manually, you need
    to take great care not to exceed the available resources of the cluster (and also
    not to underuse the available hardware). We recommend having this setting always
    set by default.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启用 `maximizeResourceAllocation` 时，EMR 和 Spark 将确定如何配置 Spark 以使用所有可用资源（例如，内存和
    CPU）。另一种选择是手动配置属性，如执行器的数量、每个执行器的 Java 堆空间以及每个执行器的核心数（即线程数）。如果您选择手动进行此操作，您需要非常小心，不要超过集群的可用资源（并且也不要未充分利用可用硬件）。我们建议始终默认设置此设置。
- en: The AWS Glue Catalog
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS Glue 目录
- en: AWS Glue provides a service that is known as a Hive metastore. The purpose of
    this service is to keep track of all the data in our data lake by defining tables
    that describe the data. A data lake is typically hosted on S3 or HDFS. Any data
    that lies on these distributed filesystems, and has a tabular format, such as
    Parquet or CSV, can be added to the metastore. This does not copy or move the
    data; it is just a way of keeping a catalog of all our data. By configuring the
    `hive.metastore.client.factory.class` property in the cluster configuration, we
    allow Spark to use all the tables registered in the Glue Catalog. Additionally,
    Spark can also create a new table or modify the catalog through Spark SQL statements.
    In the next section, we will show a concrete example of how Glue is useful.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue 提供了一种称为 Hive 元数据存储的服务。此服务的目的是通过定义描述数据的表来跟踪我们数据湖中的所有数据。数据湖通常托管在 S3 或
    HDFS 上。任何位于这些分布式文件系统上的数据，且具有表格格式，如 Parquet 或 CSV，都可以添加到元数据存储中。这不会复制或移动数据；它只是保持所有数据目录的一种方式。通过在集群配置中配置
    `hive.metastore.client.factory.class` 属性，我们允许 Spark 使用 Glue 目录中注册的所有表。此外，Spark
    还可以通过 Spark SQL 语句创建新表或修改目录。在下一节中，我们将展示 Glue 如何有用的具体示例。
- en: Managing data pipelines with Glue
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Glue 管理数据管道
- en: Data scientists and data engineers run different jobs to transform, extract,
    and load data into systems such as S3\. For example, we might have a daily job
    that processes text data and stores a table with the bag-of-words table representation
    that we saw in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes*. We might want to update the table each day to
    point to the latest available data. Upstream processes can then only rely on the
    table name to find and process the latest version of the data. If we do not catalog
    this data properly, it will be very hard to combine the different data sources
    or even to know where the data is located, which is where AWS Glue metastore comes
    in. Tables in Glue are grouped into databases. However, tables in different databases
    can be joined and referenced.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和数据工程师运行不同的作业来转换、提取和加载数据到系统，如 S3。例如，我们可能有一个每日作业处理文本数据，并存储一个包含我们[第 2 章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)中看到的词袋表示法的表，即
    *使用朴素贝叶斯分类 Twitter 流*。我们可能希望每天更新该表以指向最新的可用数据。上游过程可以仅依赖于表名来查找和处理数据的最新版本。如果我们没有正确地编目这些数据，将非常难以合并不同的数据源，甚至不知道数据在哪里，这就是
    AWS Glue 元数据存储发挥作用的地方。Glue 中的表被分组到数据库中。然而，不同数据库中的表可以连接和引用。
- en: Creating tables with Glue
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Glue 创建表
- en: You can access the Glue console on AWS by going to [https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=databases](https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=databases).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问[https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=databases](https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=databases)来访问AWS上的Glue控制台。
- en: 'In the console, create a new database, as shown in the following screenshot:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中，创建一个新的数据库，如下面的屏幕截图所示：
- en: '![](img/03081b05-181c-4c95-a244-db0732623893.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/03081b05-181c-4c95-a244-db0732623893.png)'
- en: Once the database is created, you can switch to the Athena AWS service and start
    creating tables from our data in S3 to run queries for analytics. The AWS Athena
    console can be accessed at [https://console.aws.amazon.com/athena/home](https://console.aws.amazon.com/athena/home).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了数据库，您就可以切换到Athena AWS服务，并开始从S3中的数据创建表以运行查询分析。AWS Athena控制台可以通过[https://console.aws.amazon.com/athena/home](https://console.aws.amazon.com/athena/home)访问。
- en: Let's create a table in S3 for the Boston house prices dataset that we worked
    on in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting House
    Value with Regression Algorithms*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在S3中为我们在[第3章](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml)，“使用回归算法预测房价”中工作的波士顿房价数据集创建一个表。
- en: 'In the following screenshot we can see how the create table SQL statement will
    specify the name, format, and fields of the table from our CSV data located in
    S3:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，我们可以看到创建表的SQL语句将指定来自S3中CSV数据的表名、格式和字段：
- en: '![](img/c2127dbe-4e33-4297-8098-b8f5b623f53c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c2127dbe-4e33-4297-8098-b8f5b623f53c.png)'
- en: 'Note, that the location specifies a folder (not a file). In our case, we have
    a single `CSV` folder at `s3://mastering-ml-aws/chapter3/linearmodels/train/training-housing.csv`.
    However, we could have many CSVs on the same folder and all would be linked to
    the `house_prices` table we just created. Once we create the table, since the
    data is in S3, we can start querying our table as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，位置指定了一个文件夹（而不是一个文件）。在我们的例子中，我们在`s3://mastering-ml-aws/chapter3/linearmodels/train/training-housing.csv`有一个单独的`CSV`文件夹。然而，我们可以在同一个文件夹中有许多CSV文件，并且所有这些都会链接到我们刚刚创建的`house_prices`表。一旦我们创建了表，由于数据在S3上，我们就可以开始如下查询我们的表：
- en: '![](img/3bb5d075-8139-44d4-b3a9-1cb5dd505239.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3bb5d075-8139-44d4-b3a9-1cb5dd505239.png)'
- en: Note how the data is tabulated correctly. This is because we have told Glue
    the right format and location of our data. Now we can run ultra-fast analytics
    using SQL with Presto-as-a-service through Athena.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意数据是如何正确分表的。这是因为我们告诉Glue了数据的正确格式和位置。现在我们可以通过Athena使用Presto-as-a-service通过SQL进行超快速分析。
- en: We just performed a create table operation; however, often, we want to perform
    alter table commands to switch the underlying data behind a table to a more recent
    version. It's also very common to perform add-partition operations to incrementally
    add data to a table (such as new batches or dates). Partitions also help the query
    engine to filter the data more effectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚执行了一个创建表的操作；然而，通常我们想要执行更改表命令来将表背后的数据切换到更近的版本。执行添加分区操作以增量地向表中添加数据（如新批次或日期）也是非常常见的。分区也有助于查询引擎更有效地过滤数据。
- en: Accessing Glue tables in Spark
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark中访问Glue表
- en: Once the table created is in Glue, it will also become available on every EMR
    Spark cluster (as long as we configure the `hive.metastore.client.factory.class`
    described in the previous section,  *Tuning EMR for different applications*).
    Let's launch an EMR cluster with the JupyterHub application enabled. The JupyterHub
    application is an alternative to the EMR notebooks feature we used throughout
    [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter
    Feeds with Naive Bayes*, to [Chapter 6](c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml), *Analyzing
    Visitor Patterns to Make Recommendations*. Consider using `JupyterHub` when you
    have a team of data scientists reusing the same cluster and running different
    notebooks. You can learn more on JupyterHub at [https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub.html](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub.html).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建的表在 Glue 中，它也将可在每个 EMR Spark 集群中可用（只要我们配置了前述章节中描述的 `hive.metastore.client.factory.class`，即
    *调整 EMR 以适应不同应用*）。让我们启动一个启用了 JupyterHub 应用的 EMR 集群。JupyterHub 应用是 [第 2 章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)，*使用朴素贝叶斯分类
    Twitter 流*，到 [第 6 章](c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml)，*分析访问模式以生成推荐*
    中使用的 EMR 笔记本功能的替代品。当您有一组数据科学家在重用同一集群并运行不同的笔记本时，请考虑使用 `JupyterHub`。您可以在 [https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub.html](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub.html)
    上了解更多关于 JupyterHub 的信息。
- en: 'The following screenshot shows our cluster created with the Glue metastore
    enabled and `JupyterHub` as the application:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了启用 Glue 元数据存储和 `JupyterHub` 作为应用的我们创建的集群：
- en: '![](img/086ac49e-8153-4744-9384-b694ffa4ac74.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/086ac49e-8153-4744-9384-b694ffa4ac74.png)'
- en: 'If you click on the `JupyterHub` link, it will take you to an authentication
    page, such as the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您点击 `JupyterHub` 链接，它将带您到一个认证页面，如下所示：
- en: '![](img/47db6713-f8da-4a3d-b2e2-8e5da74444ec.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/47db6713-f8da-4a3d-b2e2-8e5da74444ec.png)'
- en: The default configuration of `JupyterHub` has a default user account with a
    username of `jovyan` and a password of `jupyter` available. The authentication
    can be customized through the EMR configuration if needed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`JupyterHub` 的默认配置有一个默认用户账户，用户名为 `jovyan`，密码为 `jupyter`。如果需要，可以通过 EMR 配置自定义认证。'
- en: 'Once we authenticate, we can start creating notebooks exactly as we did with
    EMR notebooks. In this case, we will create a `PySpark3` notebook:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 认证后，我们可以像使用 EMR 笔记本一样开始创建笔记本。在这种情况下，我们将创建一个 `PySpark3` 笔记本：
- en: '![](img/f6931e08-4e0a-466a-90ec-8d8b017b3e58.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f6931e08-4e0a-466a-90ec-8d8b017b3e58.png)'
- en: 'Now, notebooks can use SparkMagic to interleave paragraphs in Python and SQL.
    Let''s look at the following notebook example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，笔记本可以使用 SparkMagic 在 Python 和 SQL 中交错段落。让我们看看以下笔记本示例：
- en: '![](img/71792244-9d3d-44bc-b87d-c627d4f7ab15.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/71792244-9d3d-44bc-b87d-c627d4f7ab15.png)'
- en: The first paragraph runs a SQL on the table we just created through Glue/Athena
    through SparkMagic's `%%sql` magic (more on SparkMagic can be found at [https://github.com/jupyter-incubator/sparkmagic](https://github.com/jupyter-incubator/sparkmagic)).
    The second paragraph constructs a Spark DataFrame through a simple SQL statement
    that selects two fields from our table. The third paragraph runs a Spark job (that
    is, the describe command) over the Spark DataFrame we constructed. You will appreciate
    how easy it is to handle, integrate, and process data once we have properly cataloged
    it in our Glue metastore.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第一段通过 Glue/Athena 通过 SparkMagic 的 `%%sql` 魔法在刚刚创建的表上运行 SQL（有关 SparkMagic 的更多信息，请参阅
    [https://github.com/jupyter-incubator/sparkmagic](https://github.com/jupyter-incubator/sparkmagic)）。第二段通过一个简单的
    SQL 语句从我们的表中选择两个字段来构建 Spark DataFrame。第三段在我们的 Spark DataFrame 上运行 Spark 作业（即 describe
    命令）。您将欣赏到，一旦我们在 Glue 元数据存储中正确编目了数据，处理、集成和处理数据是多么容易。
- en: Summary
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at the main configuration parameters of EMR and how
    they can help us run many big data frameworks, such as Spark, Hive, and Presto.
    We also explored the AWS services of Athena and Glue as a way to catalog the data
    on our data lake so that we can properly synchronize our data pipelines. Finally,
    we demonstrated how Glue can also be used in EMR, with smooth integration for
    `JupyterHub` with SparkMagic.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了 EMR 的主要配置参数以及它们如何帮助我们运行许多大数据框架，如 Spark、Hive 和 Presto。我们还探讨了 AWS 服务
    Athena 和 Glue，作为在数据湖中编目数据的方式，以便我们能够正确同步我们的数据管道。最后，我们展示了 Glue 如何在 EMR 中使用，以及 `JupyterHub`
    与 SparkMagic 的无缝集成。
- en: In the next chapter,* Deploying Models Built in AWS*, we will cover how to deploy
    machine learning models in different environments.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章* 在AWS中构建的模型部署*中，我们将介绍如何在不同的环境中部署机器学习模型。
