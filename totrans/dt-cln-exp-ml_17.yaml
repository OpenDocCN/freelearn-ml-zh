- en: '*Chapter 13*: Support Vector Machine Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第13章*：支持向量机分类'
- en: There are some similarities between support vector classification models and
    k-nearest neighbors models. They are both intuitive and flexible. However, support
    vector classification, due to the nature of the algorithm, scales better than
    k-nearest neighbor. Unlike logistic regression, it can handle nonlinear models
    rather easily. The strategies and issues with using support vector machines for
    classification are similar to those we discussed in [*Chapter 8*](B17978_08_ePub.xhtml#_idTextAnchor106),
    *Support Vector Regression*, when we used support vector machines for regression.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量分类模型和k最近邻模型之间有一些相似之处。它们都是直观且灵活的。然而，由于算法的性质，支持向量分类比k最近邻有更好的可扩展性。与逻辑回归不同，它可以很容易地处理非线性模型。使用支持向量机进行分类的策略和问题与我们讨论的相似，见[*第8章*](B17978_08_ePub.xhtml#_idTextAnchor106)，*支持向量回归*，当时我们使用支持向量机进行回归。
- en: One of the key advantages of **support vector classification** (**SVC**) is
    the ability it gives us to reduce model complexity without increasing our feature
    space. But it also provides multiple levers we can adjust to limit the possibility
    of overfitting. We can choose a linear model or select from several nonlinear
    kernels. We can use a regularization parameter, much as we did for logistic regression.
    With extensions, we can also use these same techniques to construct multiclass
    models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量分类**（**SVC**）的一个关键优势是它赋予我们减少模型复杂度的能力，同时不增加我们的特征空间。但它也提供了多个我们可以调整的杠杆，以限制过拟合的可能性。我们可以选择线性模型，或从几个非线性核中选择。我们可以使用正则化参数，就像我们在逻辑回归中所做的那样。通过扩展，我们还可以使用这些相同的技巧来构建多类模型。'
- en: 'We will explore the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: Key concepts for SVC
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVC的关键概念
- en: Linear SVC models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性SVC模型
- en: Nonlinear SVM classification models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性SVM分类模型
- en: SVMs for multiclass classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类分类的SVM
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We will stick to the pandas, NumPy, and scikit-learn libraries in this chapter.
    All code in this chapter was tested with scikit-learn versions 0.24.2 and 1.0.2\.
    The code that displays the decision boundaries needs scikit-learn version 1.1.1
    or later.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将坚持使用pandas、NumPy和scikit-learn库。本章中的所有代码都使用scikit-learn版本0.24.2和1.0.2进行了测试。显示决策边界的代码需要scikit-learn版本1.1.1或更高版本。
- en: Key concepts for SVC
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVC的关键概念
- en: We can use **support vector machines** (**SVMs**) to find a line or curve to
    separate instances by class. When classes can be discriminated by a line, they
    are said to be **linearly separable**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**支持向量机**（**SVMs**）来找到一条线或曲线，通过类别来分离实例。当类别可以通过一条线来区分时，它们被称为**线性可分**。
- en: There may, however, be many possible linear classifiers, as we can see in *Figure
    13.1*. Each line successfully discriminates between the two classes, represented
    by dots and squares, using the two features x1 and x2\. The key difference is
    in how the lines would classify new instances, represented by the transparent
    rectangle. Using the line closest to the squares would cause the transparent rectanglez
    to be classified as a dot. Using either of the other two lines would classify
    it as a square.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们在*图13.1*中可以看到的，可能有许多可能的线性分类器。每条线都成功地使用两个特征x1和x2来区分由点表示的两个类别，关键的区别在于这些线如何对新的实例进行分类，这些新实例由透明矩形表示。使用离正方形最近的线会将透明矩形分类为点。使用其他两条线中的任何一条都会将其分类为正方形。
- en: '![Figure 13.1 – Three possible linear classifiers ](img/B17978_13_001.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图13.1 – 三种可能的线性分类器](img/B17978_13_001.jpg)'
- en: Figure 13.1 – Three possible linear classifiers
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 – 三种可能的线性分类器
- en: 'When a linear discriminant is very close to training instances, as is the case
    with two of the lines in *Figure 13.2*, there is a greater risk of misclassifying
    new instances. We want a line that gives us the maximum margin between classes;
    one that is furthest away from border data points for each class. That is the
    middle line in *Figure 13.1*, but it can be seen more clearly in *Figure 13.2*:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当线性判别器非常接近训练实例时，就像*图13.2*中的两条线一样，就有更大的风险将新实例分类错误。我们希望得到一个能够给出最大类别间间隔的线；一个离每个类别的边界数据点最远的线。那就是*图13.1*中的中间线，但在*图13.2*中可以看得更清楚：
- en: '![Figure 13.2 – SVM classification and maximum margin ](img/B17978_13_002.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图13.2 – SVM分类和最大间隔](img/B17978_13_002.jpg)'
- en: Figure 13.2 – SVM classification and maximum margin
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 – SVM分类和最大间隔
- en: The bold line splits the maximum margin and is referred to as the decision boundary.
    The border data points for each class are known as the support vectors.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 粗体线分割了最大间隔，被称为决策边界。每个类别的边界数据点被称为支持向量。
- en: We use SVM to find the linear discriminant with the maximum margin between classes.  It
    does this by finding an equation representing a margin that can be maximized,
    where the margin is the distance between a data point and the separating hyperplane.
    With two features, as in *Figure 13.2*, that hyperplane is just a line. However,
    this can be generalized to feature spaces with more dimensions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用支持向量机来寻找类别之间具有最大间隔的线性判别式。它是通过找到一个可以最大化的间隔的方程来实现的，其中间隔是数据点到分离超平面的距离。在具有两个特征的情况下，如*图13.2*所示，该超平面只是一条线。然而，这可以推广到具有更多维度的特征空间。
- en: With data points such as those in *Figure 13.2*, we can use what is known as
    **hard margin classification** without problems; that is, we can be strict about
    all observations for each class being on the correct side of the decision boundary.
    But what if our data points look like those in *Figure 13.3*? Here, there is a
    square very close to the dots. The hard margin classifier is the left line, giving
    us quite tiny margins.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像*图13.2*中的数据点，我们可以使用所谓的**硬间隔分类**而不会出现问题；也就是说，我们可以对每个类别的所有观察值在决策边界的正确一侧非常严格。但如果我们数据点的样子像*图13.3*中的那些呢？在这里，有一个方形非常接近点。硬间隔分类器是左侧的线，给我们非常小的间隔。
- en: '![Figure 13.3 – SVMs with hard and soft margins ](img/B17978_13_003.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图13.3 – 带有硬间隔和软间隔的支持向量机](img/B17978_13_003.jpg)'
- en: Figure 13.3 – SVMs with hard and soft margins
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 – 带有硬间隔和软间隔的支持向量机
- en: If we use **soft margin classification** instead, we get the line to the right.
    Soft margin classification relaxes the constraint that all instances have to be
    correctly separated. As is the case with the data in *Figure 13.3*, allowing for
    a small number of misclassifications in the training data can give us a larger
    margin. We ignore the wayward square and get a decision boundary represented by
    the soft margin line.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用**软间隔分类**，则得到右侧的线。软间隔分类放宽了所有实例都必须正确分离的约束。正如*图13.3*中的数据所示，允许训练数据中有少量错误分类可以给我们更大的间隔。我们忽略偏离的方形，得到由软间隔线表示的决策边界。
- en: The amount of relaxation of the constraint is determined by the *C* hyperparameter.
    The larger the value of *C*, the greater the penalty for margin violations. Not
    surprisingly, models with larger *C* values are more prone to overfitting. *Figure
    13.4* illustrates how the margin changes with values of *C*. At *C = 1*, the penalty
    for misclassification is low, giving us a much greater margin than when *C* is
    100\. Even at a *C* of 100, however, some margin violation still happens.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 约束放宽的程度由*C*超参数决定。*C*的值越大，对间隔违规的惩罚就越大。不出所料，具有较大*C*值的模型更容易过拟合。*图13.4*说明了间隔如何随着*C*值的改变而变化。在*C
    = 1*时，错误分类的惩罚较低，给我们比*C*为100时更大的间隔。然而，即使在*C*为100的情况下，仍然会发生一些间隔违规。
- en: '![Figure 13.4 – Soft margins at different C values ](img/B17978_13_004.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图13.4 – 不同C值下的软间隔](img/B17978_13_004.jpg)'
- en: Figure 13.4 – Soft margins at different C values
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 – 不同C值下的软间隔
- en: As a practical matter, we almost always build our SVC models with soft margins.
    The default value for *C* in scikit-learn is 1.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，我们几乎总是用软间隔构建我们的SVC模型。scikit-learn中*C*的默认值是1。
- en: Nonlinear SVM and the kernel trick
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性支持向量机和核技巧
- en: We have not yet fully addressed the issue of linear separability with SVC. For
    simplicity, it is helpful to return to a classification problem involving two
    features. Let’s say a plot of two features against a categorical target looks
    like the illustration in *Figure 13.5*. The target has two possible values, represented
    by the dots and squares. x1 and x2 are numeric and have negative values.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未完全解决SVC的线性可分性问题。为了简单起见，回到一个涉及两个特征的分类问题是有帮助的。假设两个特征与分类目标的关系图看起来像*图13.5*中的插图。目标有两个可能的值，由点和正方形表示。x1和x2是数值，具有负值。
- en: '![Figure 13.5 – Class labels not linearly separable with two features ](img/B17978_13_005.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图13.5 – 使用两个特征无法线性分离的类别标签](img/B17978_13_005.jpg)'
- en: Figure 13.5 – Class labels not linearly separable with two features
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5 – 使用两个特征无法线性分离的类别标签
- en: 'What can we do in a case like this to identify a margin between the classes?
    It is often the case that a margin can be identified at a higher dimension. In
    this example, we can use a polynomial transformation, as illustrated in *Figure
    13.6*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们如何识别类之间的边界？通常情况下，在更高的维度中可以识别出边界。在这个例子中，我们可以使用图13.6中所示的多项式变换：
- en: '![Figure 13.6 – Using polynomial transformation to establish the margin ](img/B17978_13_006.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图13.6 – 使用多项式变换建立边界](img/B17978_13_006.jpg)'
- en: Figure 13.6 – Using polynomial transformation to establish the margin
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6 – 使用多项式变换建立边界
- en: There is now a third dimension, which is the sum of the squares of x1 and x2\.
    The dots are all higher than the squares. This is similar to how we used polynomial
    transformation with linear regression.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有一个第三维度，它是x1和x2平方和的总和。点都高于平方。这与我们使用多项式变换进行线性回归的方式相似。
- en: One drawback of this approach is that we can quickly end up with too many features
    for our model to perform well. This is where the **kernel trick** comes in very
    handy. SVC can use a kernel function to expand the feature space implicitly without
    actually creating more features. This is done by creating a vector of values that
    can be used to fit a nonlinear margin.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点之一是我们可能会迅速拥有太多特征，以至于模型无法良好地执行。这就是**核技巧**大显身手的地方。SVC可以使用核函数隐式地扩展特征空间，而不实际创建更多特征。这是通过创建一个可以用来拟合非线性边界的值向量来实现的。
- en: 'While this allows us to fit a polynomial transformation like the hypothetical
    one illustrated in *Figure 13.6*, the most frequently used kernel function with
    SVC is the **radial basis function** (**RBF**). RBF is popular because it is faster
    than other common kernel functions and because it can be used with the gamma hyperparameter
    for additional flexibility. The equation for the RBF kernel is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这允许我们拟合一个类似于图13.6中假设的假设多项式变换，但SVC中最常用的核函数是**径向基函数**（RBF）。RBF之所以受欢迎，是因为它比其他常见的核函数更快，并且可以使用伽马超参数提供额外的灵活性。RBF核的方程如下：
- en: '![](img/B17978_13_0011.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_13_0011.jpg)'
- en: Here, ![](img/B17978_13_002.png) and ![](img/B17978_13_003.png) are data points.
    Gamma, ![](img/B17978_13_004.png), determines the amount of influence of each
    point. With high values of gamma, points have to be very close to each other to
    be grouped together. At very high values of gamma, we start to see islands of
    points.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17978_13_002.png) 和 ![](img/B17978_13_003.png) 是数据点。伽马值，![](img/B17978_13_004.png)，决定了每个点的影响力大小。当伽马值较高时，点必须非常接近才能被分组在一起。在伽马值非常高的情况下，我们开始看到点的岛屿。
- en: Of course, what is a high value for gamma, or of *C*, depends partly on our
    data. A good approach is to create visualizations of decision boundaries at different
    values for gamma and *C* before doing much modeling. This will give us a sense
    of whether or not we are underfitting or overfitting at different hyperparameter
    values. We will plot decision boundaries at different values of gamma and *C*
    in this chapter.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，伽马值或*C*的高值取决于我们的数据。一个好的方法是，在大量建模之前，创建不同伽马值和*C*值的决策边界可视化。这将让我们了解在不同的超参数值下，我们是否过度拟合或欠拟合。在本章中，我们将绘制不同伽马值和*C*值的决策边界。
- en: Multiclass classification with SVC
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVC的多类分类
- en: All of our discussion about SVC so far has centered on binary classification.
    Fortunately, all of the key concepts that apply to SVMs for binary classification
    also apply to classification when our target has more than two possible values.
    We transform the multiclass problem into a binary classification problem by modeling
    it as a **one-versus-one**, or a **one-versus-rest** problem.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们关于支持向量机（SVC）的所有讨论都集中在二元分类上。幸运的是，适用于二元分类支持向量机的所有关键概念也适用于我们的目标值超过两个的可能值时的分类。我们将多类问题建模为**一对一**或**一对余**问题，从而将其转化为二元分类问题。
- en: '![Figure 13.7 – Multiclass SVC options ](img/B17978_13_007.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图13.7 – 多类SVC选项](img/B17978_13_007.jpg)'
- en: Figure 13.7 – Multiclass SVC options
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7 – 多类SVC选项
- en: One-versus-one classification is easy to illustrate in a three-class example,
    as shown on the left side of *Figure 13.7*. A decision boundary is estimated between
    each class and each of the other classes. For example, the dotted line is the
    decision boundary for the dot class versus the square class. The solid line is
    the decision boundary between the dots and the ovals.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在三类示例中，一对一分类很容易说明，如图13.7的左侧所示。每个类别与每个其他类别之间估计一个决策边界。例如，虚线是点类与正方形类之间的决策边界。实线是点与椭圆形之间的决策边界。
- en: With one-versus-rest classification, a decision boundary is constructed between
    each class and those instances that are not of that class. This is illustrated
    on the right side of *Figure 13.7*. The solid line is the decision boundary between
    the dots and the instances that are not dots (those that are squares or ovals).
    The dotted and double lines are the decision boundaries for the squares versus
    the rest and the ovals versus the rest of the instances, respectively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在一对一分类中，每个类别与不属于该类别的实例之间构建一个决策边界。这如图13.7的右侧所示。实线是点与不是点（即正方形或椭圆形）的实例之间的决策边界。虚线和双线分别是正方形与剩余实例和椭圆形与剩余实例之间的决策边界。
- en: We can construct both linear and nonlinear SVC models using either one-versus-one
    or one-versus-rest classification. We can also specify values for *C* to construct
    soft margins. However, the construction of more decision boundaries with each
    of these techniques claims greater computational resources than SVC for binary
    classification. If we have a large number of observations, many features, and
    more than a couple of parameters to tune, we will likely need very good system
    resources to get timely results.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一对一或一对一分类来构建线性和非线性SVC模型。我们还可以指定*C*的值来构建软边界。然而，使用这些技术中的每一个构建更多的决策边界需要比二分类SVC更多的计算资源。如果我们有大量的观察结果、许多特征和多个参数需要调整，我们可能需要非常好的系统资源才能及时获得结果。
- en: 'The three-class example hides one thing that is different about one-versus-one
    and one-versus-rest classifiers. With three classes, they use the same number
    of classifiers (three), but the number of classifiers increases relatively rapidly
    with one-versus-one. The number of classifiers will always be equal to the number
    of class values with one-versus-rest, whereas, with one-versus-one, it is equal
    to the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 三类示例隐藏了一个关于一对一和一对一分类器不同之处。对于三个类别，它们使用相同数量的分类器（三个），但随着一对一的增加，分类器的数量相对迅速地增加。在一对一分类中，分类器的数量始终等于类别值的数量，而在一对一分类中，它等于以下：
- en: '![](img/B17978_13_0051.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_13_0051.jpg)'
- en: Here, *S* is the number of classifiers and *N* is the cardinality (the number
    of class values) of the target. So, with a cardinality of 4, one-versus-rest needs
    4 classifiers, and one-versus-one uses 6.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*S*是分类器的数量，*N*是目标的目标值（类别值）的基数。因此，基数是4时，一对一分类需要4个分类器，而一对一分类使用6个。
- en: We explore multiclass SVC models in the last section of this chapter, but let’s
    start with a relatively straightforward linear model to see SVC in action. There
    are two things to keep in mind when doing the preprocessing for an SVC model.
    First, SVC is sensitive to the scale of features, so we will need to address that
    before fitting our model. Second, if we are using hard margins or high values
    for *C*, outliers might have a large effect on our model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的最后部分探讨了多类SVC模型，但让我们从一个相对简单的线性模型开始，看看SVC的实际应用。在为SVC模型进行预处理时，有两个需要注意的事项。首先，SVC对特征的规模很敏感，因此在我们拟合模型之前需要解决这一点。其次，如果我们使用硬边界或高*C*值，异常值可能会对我们的模型产生很大的影响。
- en: Linear SVC models
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性SVC模型
- en: We can often get good results by using a linear SVC model. When we have more
    than two features, there is no easy way to visualize whether our data is linearly
    separable or not. We often decide on linear or nonlinear based on hyperparameter
    tuning. For this section, we will assume we can get good performance with a linear
    model and soft margins.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常可以通过使用线性SVC模型获得良好的结果。当我们有超过两个特征时，没有简单的方法来可视化我们的数据是否线性可分。我们通常根据超参数调整来决定是线性还是非线性。在本节中，我们将假设我们可以通过线性模型和软边界获得良好的性能。
- en: We will work with data on **National Basketball Association** (**NBA**) games
    in this section. The dataset has statistics from each NBA game from the 2017/2018
    season through the 2020/2021 season. This includes the home team, whether the
    home team won, the visiting team, shooting percentages for visiting and home teams,
    turnovers, rebounds, and assists by both teams, and a number of other measures.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将处理关于**美国职业篮球联赛**（**NBA**）比赛的数据。数据集包含了从2017/2018赛季到2020/2021赛季每场NBA比赛的统计数据。这包括主队、主队是否获胜、客队、客队和主队的投篮命中率、失误、篮板和助攻，以及许多其他指标。
- en: Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: NBA game data is available for download for the public at [https://www.kaggle.com/datasets/wyattowalsh/basketball](https://www.kaggle.com/datasets/wyattowalsh/basketball).
    This dataset has game data starting with the 1946/1947 NBA season. It uses `nba_api`
    to pull stats from [nba.com](http://nba.com). That API is available at [https://github.com/swar/nba_api](https://github.com/swar/nba_api).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: NBA比赛数据可在[https://www.kaggle.com/datasets/wyattowalsh/basketball](https://www.kaggle.com/datasets/wyattowalsh/basketball)供公众下载。该数据集从1946/1947赛季的NBA赛季开始。它使用`nba_api`从[nba.com](http://nba.com)获取统计数据。该API可在[https://github.com/swar/nba_api](https://github.com/swar/nba_api)找到。
- en: 'Let’s build a linear SVC model:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个线性SVC模型：
- en: 'We start by loading the familiar libraries. The only new modules are `LinearSVC`
    and `DecisionBoundaryDisplay`. We will use `DecisionBoundaryDisplay` to show the
    boundaries of a linear model:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载熟悉的库。唯一的新模块是`LinearSVC`和`DecisionBoundaryDisplay`。我们将使用`DecisionBoundaryDisplay`来显示线性模型的边界：
- en: '[PRE0]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are ready to load the NBA game data. We just have a little cleaning to do.
    A small number of observations have missing values for our target, `WL_HOME`,
    whether the home team won. We remove those observations. We convert the `WL_HOME`
    feature to a `0` and `1` feature.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好加载NBA比赛数据。我们只需做一些清理工作。少数观测值的目标`WL_HOME`（主队是否获胜）有缺失值。我们移除这些观测值。我们将`WL_HOME`特征转换为`0`和`1`特征。
- en: 'There is not much of a problem with a class imbalance here. This will save
    us some time later:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，类别不平衡的问题并不大。这将在以后为我们节省一些时间：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s organize our features by data type:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们按数据类型组织我们的特征：
- en: '[PRE2]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s look at some descriptive statistics. (I have omitted some features from
    the printout to save space.) We will need to scale these features since they have
    very different ranges. There are no missing values but we will generate some when
    we assign missings to extreme values:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看一些描述性统计。 （为了节省空间，我已经从打印输出中省略了一些特征。）我们需要缩放这些特征，因为它们的范围差异很大。没有缺失值，但当我们为极端值分配缺失值时，我们将生成一些缺失值：
- en: '[PRE3]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We should also review the correlations of the features:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该回顾一下特征的相关性：
- en: '[PRE4]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This produces the following plot:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 13.8 – Heat map of NBA game statistics correlations ](img/B17978_13_008.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图13.8 – NBA比赛统计数据相关性热图](img/B17978_13_008.jpg)'
- en: Figure 13.8 – Heat map of NBA game statistics correlations
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8 – NBA比赛统计数据相关性热图
- en: Several features are correlated with the target, including the field goal percentage
    of the home team (`FG_PCT_HOME`) and defensive rebounds of the home team (`DREB_HOME`).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特征与目标相关，包括主队的投篮命中率（`FG_PCT_HOME`）和主队的防守篮板球（`DREB_HOME`）。
- en: There is also correlation among the features. For example, the field goal percentage
    of the home team (`FG_PCT_HOME`) and the 3-point field goal percentage of the
    home team (`FG3_PCT_HOME`) are positively correlated, not surprisingly. Also,
    rebounds of the home team (`REB_HOME`) and defensive rebounds of the home team
    (`DREB_HOME`) are likely too closely correlated for any model to disentangle their
    impact.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 特征之间也存在相关性。例如，主队的投篮命中率（`FG_PCT_HOME`）和主队的3分投篮命中率（`FG3_PCT_HOME`）呈正相关，这并不令人意外。此外，主队的篮板球（`REB_HOME`）和防守篮板球（`DREB_HOME`）可能过于紧密地相关，以至于任何模型都无法分离它们的影响。
- en: 'Next, we create training and testing DataFrames:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建训练和测试数据框：
- en: '[PRE5]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We need to set up our column transformations. For the numeric columns, we check
    for outliers and scale the data. We one-hot encode the one categorical feature,
    `SEASON`. We will use these transformations later with the pipeline for our grid
    search:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要设置列转换。对于数值列，我们检查异常值并缩放数据。我们将一个分类特征`SEASON`进行独热编码。我们将在网格搜索中使用这些转换：
- en: '[PRE6]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Before constructing our model, let’s look at a decision boundary from a linear
    SVC model. We base the boundary on two features correlated with the target: the
    field goal percentage of the home team (`FG_PCT_HOME`) and defensive rebounds
    of the home team (`DREB_HOME`).'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建我们的模型之前，让我们看看一个线性SVC模型的决策边界。我们基于与目标相关的两个特征来设置边界：主队的投篮命中率(`FG_PCT_HOME`)和主队的防守篮板(`DREB_HOME`)。
- en: 'We create a function, `dispbound`, which will use the `DecisionBoundaryDisplay`
    module to show the boundary. This module is available with scikit-learn versions
    1.1.1 or later. `DecisionBoundaryDisplay` needs a model to fit, two features,
    and target values:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个函数`dispbound`，它将使用`DecisionBoundaryDisplay`模块来显示边界。这个模块在scikit-learn版本1.1.1或更高版本中可用。`DecisionBoundaryDisplay`需要一个模型来拟合，两个特征和目标值：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This produces the following plot:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 13.9 – Decision boundary for a two-feature linear SVC model ](img/B17978_13_009.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图13.9 – 双特征线性SVC模型的决策边界](img/B17978_13_009.jpg)'
- en: Figure 13.9 – Decision boundary for a two-feature linear SVC model
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9 – 双特征线性SVC模型的决策边界
- en: We get a pretty decent linear boundary with just the two features. That is great
    news, but let’s do a more carefully constructed model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只使用两个特征就得到了一个相当不错的线性边界。这是个好消息，但让我们构建一个更精心设计的模型。
- en: 'To build our model, we first instantiate a linear SVC object and set up recursive
    feature elimination. We then add the column transformation, the feature selection,
    and the linear SVC to a pipeline and fit it:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了构建我们的模型，我们首先实例化一个线性SVC对象并设置递归特征消除。然后我们将列转换、特征选择和线性SVC添加到管道中并拟合它：
- en: '[PRE8]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s see what features were selected from our recursive feature elimination.
    We need to first get the column names after the one-hot encoding. We can then
    use the `get_support` method of the `rfecv` object to get the features that were
    selected. (You will get a deprecated warning regarding `get_feature_names` if
    you are using scikit-learn versions 1 or later. You can use `get_feature_names_out`
    instead, though that will not work with earlier versions of scikit-learn.):'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看从我们的递归特征消除中选择了哪些特征。我们首先需要获取一元编码后的列名。然后我们可以使用`rfecv`对象的`get_support`方法来获取选定的特征。（如果你使用的是scikit-learn版本1或更高版本，你会得到一个关于`get_feature_names`的弃用警告。不过，你可以使用`get_feature_names_out`代替，尽管这不会与scikit-learn的早期版本兼容。）
- en: '[PRE9]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We should look at the coefficients. Coefficients for each of the selected columns
    can be accessed with the `coef_` attribute of the `linearsvc` object. Perhaps
    not surprisingly, the shooting percentages of the home team (`FG_PCT_HOME`) and
    the away team (`FG_PCT_AWAY`) are the most important positive and negative predictors
    of the home team winning. The next most important features are the number of turnovers
    of the away and home teams:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该看看系数。对于每个选定的列的系数可以通过`linearsvc`对象的`coef_`属性来访问。也许并不令人惊讶，主队的投篮命中率(`FG_PCT_HOME`)和客队的投篮命中率(`FG_PCT_AWAY`)是主队获胜的最重要正负预测因子。接下来最重要的特征是客队和主队的失误次数：
- en: '[PRE10]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s take a look at the predictions. Our model predicts the home team winning
    very well:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看预测结果。我们的模型在预测主队获胜方面做得很好：
- en: '[PRE11]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We should confirm that these metrics are not a fluke by doing some cross-validation.
    We use repeated stratified k folds for our validation, indicating that we want
    seven folds and 10 iterations. We get pretty much the same results as we did during
    the previous step:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该通过进行交叉验证来确认这些指标不是偶然的。我们使用重复的有放回分层k折来进行验证，这意味着我们想要7个折和10次迭代。我们得到的结果与之前步骤中的结果几乎相同：
- en: '[PRE12]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We have been using the default value of `C` of `1` so far. We can try to identify
    a better value for `C` with a randomized grid search:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用`C`的默认值`1`。我们可以尝试使用随机网格搜索来识别一个更好的`C`值：
- en: '[PRE13]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The best `C` value is 2.02 and the best accuracy score is 0.9316.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的`C`值是2.02，最佳的准确度得分是0.9316。
- en: 'Let’s take a closer look at the scores for each of the 20 times we ran the
    grid search. Each score is the average accuracy score across 10 folds. We actually
    get pretty much the same score regardless of the `C` value:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们仔细看看20次网格搜索中每次的得分。每个得分是10个折的准确度得分的平均值。实际上，无论`C`值如何，我们得到的分数都差不多：
- en: '[PRE14]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s now look at some of the predictions. Our model does well across the board,
    but not any better than the initial model:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们现在看看一些预测结果。我们的模型在各个方面都做得很好，但并没有比初始模型做得更好：
- en: '[PRE15]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s also look at a confusion matrix:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也看看一个混淆矩阵：
- en: '[PRE16]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This produces the following plot:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 13.10 – Confusion matrix for wins by the home team ](img/B17978_13_010.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图13.10 – 主队胜负的混淆矩阵](img/B17978_13_010.jpg)'
- en: Figure 13.10 – Confusion matrix for wins by the home team
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10 – 主队胜负的混淆矩阵
- en: Our model largely predicts home team wins and losses correctly. Tuning the value
    of `C` did not make much of a difference, as we get pretty much the same accuracy
    regardless of the `C` value.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在很大程度上正确预测了主队的胜负。调整`C`的值并没有带来太大的变化，因为我们几乎无论`C`值如何都能获得相同的准确率。
- en: Note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You may have noticed that we are using the accuracy metric more often with the
    NBA games data than with the heart disease and machine failure data that we have
    worked with in previous chapters. We focused more on sensitivity with that data.
    There are two reasons for that. First, accuracy is a more compelling measure when
    classes are closer to being balanced for reasons we discussed in detail in [*Chapter
    6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing for Model Evaluation*.
    Second, in predicting heart disease and machine power failure, we are biased towards
    sensitivity, as the cost of a false negative is higher than that of a false positive
    in those domains. For predicting NBA games, there is no such bias.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我们在处理NBA比赛数据时比在之前章节中处理的心脏病和机器故障数据时更频繁地使用准确率指标。我们更关注那个数据的敏感性。这有两个原因。首先，当类别几乎平衡时，准确率是一个更有说服力的度量标准，正如我们在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)“准备模型评估”中详细讨论的那样。其次，在预测心脏病和机器功率故障时，我们倾向于敏感性，因为那些领域中的假阴性成本高于假阳性。而对于预测NBA比赛，则没有这样的偏见。
- en: One advantage of linear SVC models is how easy they are to interpret. We are
    able to look at coefficients, which helps us make sense of the model and communicate
    the basis of our predictions to others. It nonetheless can be helpful to confirm
    that we do not get better results with a nonlinear model. We will do that in the
    next section.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 线性SVC模型的一个优点是它们很容易解释。我们能够查看系数，这有助于我们理解模型并与其他人沟通我们预测的基础。尽管如此，确认我们不会使用非线性模型获得更好的结果也是有帮助的。我们将在下一节中这样做。
- en: Nonlinear SVM classification models
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性SVM分类模型
- en: Although nonlinear SVC is more complicated conceptually than linear SVC, as
    we saw in the first section of this chapter, running a nonlinear model with scikit-learn
    is relatively straightforward. The main difference from a linear model is that
    we need to do a fair bit more hyperparameter tuning. We have to specify values
    for `C`, for `gamma`, and for the kernel we want to use.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然非线性SVC在概念上比线性SVC更复杂，正如我们在本章第一节中看到的，使用scikit-learn运行非线性模型相对简单。与线性模型的主要区别是我们需要进行相当多的超参数调整。我们必须指定`C`、`gamma`的值以及我们想要使用的核函数。
- en: 'While there are theoretical reasons for hypothesizing that some hyperparameter
    values might work better than others for a given modeling challenge, we usually
    resolve those values empirically, that is, with hyperparameter tuning. We try
    that in this section with the same NBA games data that we used in the previous
    section:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有一些理论上的理由可以假设对于特定的建模挑战，某些超参数值可能比其他值更有效，但我们通常通过经验方法（即超参数调整）来解决这个问题。我们将在本节中使用与上一节相同的NBA比赛数据来尝试这样做：
- en: 'We load the same libraries that we used in the previous section. We also import
    the `LogisticRegression` module. We will use that with a feature selection wrapper
    method later:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载了上一节中使用的相同库。我们还导入了`LogisticRegression`模块。我们稍后将会使用该模块与特征选择包装器方法结合：
- en: '[PRE17]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We import the `nbagames` module, which has the code that loads and preprocesses
    the NBA games data. This is just a copy of the code that we ran in the previous
    section to prepare the data for modeling. There is no need to repeat those steps
    here.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入了`nbagames`模块，其中包含加载和预处理NBA比赛数据的代码。这仅仅是我们在上一节中运行以准备建模数据的代码的副本。在这里没有必要重复那些步骤。
- en: 'We also import the `dispbound` function we used in the previous section to
    display decision boundaries. We copied that code into a file called `displayfunc.py`
    in the `helperfunctions` subfolder of the current directory:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还导入了上一节中使用的`dispbound`函数来显示决策边界。我们将那段代码复制到了当前目录下的`helperfunctions`子目录中，文件名为`displayfunc.py`：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We use the `nbagames` module to get the training and testing data:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`nbagames`模块来获取训练和测试数据：
- en: '[PRE19]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Before constructing a model, let’s look at the decision boundaries for a couple
    of different kernels with two features: the field goal percentage of the home
    team (`FG_PCT_HOME`) and defensive rebounds of the home team (`DREB_HOME`). We
    start with the `rbf` kernel, using different values for `gamma` and `C`:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建模型之前，让我们看看具有两个特征（主队的投篮命中率 `FG_PCT_HOME` 和主队的防守篮板 `DREB_HOME`）的几个不同核的决策边界。我们首先使用
    `rbf` 核，并使用不同的 `gamma` 和 `C` 值：
- en: '[PRE20]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Running this a few different ways produces the following plots:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以几种不同的方式运行此操作会产生以下图表：
- en: '![Figure 13.11 – Decision boundaries with the rbf kernel and different gamma
    and C values ](img/B17978_13_011.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.11 – 使用 rbf 核和不同 gamma 和 C 值的决策边界](img/B17978_13_011.jpg)'
- en: Figure 13.11 – Decision boundaries with the rbf kernel and different gamma and
    C values
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11 – 使用 rbf 核和不同 gamma 和 C 值的决策边界
- en: At values for `gamma` and `C` near the default, we see some bending of the decision
    boundary to accommodate a few wayward points in the loss class. These are instances
    where the home team lost despite having very high defensive rebound totals. With
    the `rbf` kernel, two of these instances are now correctly classified. There are
    also a couple of instances with a high home team field goal percentage but low
    home team defensive rebounds, which are now correctly classified. However, there
    is not much change overall in our predictions compared with the linear model from
    the previous section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `gamma` 和 `C` 的值接近默认值时，我们看到决策边界有一些弯曲，以适应损失类中的几个偏离的点。这些是主队尽管有很高的防守篮板总数却输掉比赛的情况。使用
    `rbf` 核，其中两个这样的实例现在被正确分类。还有一些主队投篮命中率很高但防守篮板很低的实例，现在也被正确分类。然而，与上一节中的线性模型相比，我们的预测整体上并没有太大变化。
- en: But this changes significantly if we increase values for `C` or `gamma`. Recall
    that higher values of `C` increase the penalty for misclassification. This leads
    to boundaries that wind around instances more.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们增加 `C` 或 `gamma` 的值，这种变化会显著。回想一下，`C` 的较高值会增加误分类的惩罚。这导致边界围绕实例旋转。
- en: Increasing `gamma` to `30` causes substantial overfitting. High values of `gamma`
    mean that data points have to be very close to each other to be grouped together.
    This results in decision boundaries closely tied to small numbers of instances,
    sometimes just one instance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `gamma` 增加到 `30` 会导致严重的过度拟合。`gamma` 的高值意味着数据点必须非常接近才能被分组在一起。这导致决策边界紧密地与少数实例相关联，有时甚至只有一个实例。
- en: 'We can also show the boundaries for a polynomial kernel. We will keep the `C`
    value at the default to focus on the effect of changing the number of degrees:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以展示多项式核的边界。我们将保持默认的 `C` 值，以关注改变度数的影响：
- en: '[PRE21]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Running this a couple of different ways produces the following plots:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以几种不同的方式运行此操作会产生以下图表：
- en: '![Figure 13.12 – Decision boundaries with polynomial kernel and different degrees
    ](img/B17978_13_012.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.12 – 使用多项式核和不同度数的决策边界](img/B17978_13_012.jpg)'
- en: Figure 13.12 – Decision boundaries with polynomial kernel and different degrees
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 – 使用多项式核和不同度数的决策边界
- en: We can see some bending of the decision boundary at higher degree levels to
    handle a couple of unusual instances. There is not much overfitting here, but
    not really much improvement in our predictions either.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在较高度数级别上决策边界的某些弯曲，以处理几个不寻常的实例。这里并没有过度拟合，但我们的预测也没有真正得到很大改善。
- en: This at least hints at what to expect when we construct the model. We should
    try some nonlinear models but there is a good chance that they will not lead to
    much improvement over the linear model we used in the previous section.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这至少暗示了当我们构建模型时可以期待什么。我们应该尝试一些非线性模型，但有很大可能性它们不会比我们在上一节中使用的线性模型带来太多改进。
- en: 'Now, we are ready to set up the pipeline that we will use for our nonlinear
    SVC. Our pipeline will do the column transformation and a recursive feature elimination.
    We use logistic regression for the feature selection:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已准备好设置我们将用于非线性 SVC 的管道。我们的管道将执行列转换和递归特征消除。我们使用逻辑回归进行特征选择：
- en: '[PRE22]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We create a dictionary to use for our hyperparameter tuning. This dictionary
    is structured somewhat differently from other dictionaries we have used for this
    purpose. That is because certain hyperparameters only work with certain other
    hyperparameters. For example, `gamma` does not work with a linear kernel:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个字典用于我们的超参数调整。这个字典的结构与我们用于此目的的其他字典略有不同。这是因为某些超参数只能与某些其他超参数一起使用。例如，`gamma`不能与线性核一起使用：
- en: '[PRE23]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You may have noticed that one of the kernels we will be using is linear, and
    wonder how this is different from the linear SVC module we used in the previous
    section. `LinearSVC` will often converge faster, particularly with large datasets.
    It does not use the kernel trick. We will also likely get different results as
    the optimization is different in several ways.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们将使用的一个核是线性的，并想知道这与我们在上一节中使用的线性SVC模块有何不同。`LinearSVC`通常会更快地收敛，尤其是在大型数据集上。它不使用核技巧。我们可能也会得到不同的结果，因为优化在几个方面都不同。
- en: 'Now we are ready to fit an SVC model. The best model is actually one with a
    linear kernel:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好拟合一个SVC模型。最佳模型实际上是一个线性核的模型：
- en: '[PRE24]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let’s take a closer look at the hyperparameters selected and the associated
    accuracy scores. We can get the 20 randomly chosen hyperparameter combinations
    from the `params` list from the grid object’s `cv_results_` dictionary. We can
    get the mean test score from that same dictionary.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看选定的超参数和相关的准确率分数。我们可以从网格对象的`cv_results_`字典中获取`params`列表中的20个随机选择的超参数组合。我们也可以从同一个字典中获取平均测试分数。
- en: 'We sort by accuracy score in descending order. Linear kernels outperform polynomial
    and `rbf` kernels, though not substantially better than polynomial at `3`, `4`,
    and `5` degrees. `rbf` kernels perform particularly poorly:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按准确率分数降序排序。线性核优于多项式核和`rbf`核，尽管在`3`、`4`和`5`度上并不比多项式核显著更好。`rbf`核的表现尤其糟糕：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We use the pandas `json_nomalize` method to handle the somewhat messy hyperparameter
    combinations we pull from the `params` list. It is messy because different hyperparameters
    are available depending on the kernel used. This means that the 20 dictionaries
    in the `params` list will have different keys. For example, the polynomial kernels
    will have values for degrees. The linear and `rbf` kernels will not.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用pandas的`json_normalize`方法来处理我们从`params`列表中提取的有些混乱的超参数组合。这是因为不同的超参数取决于所使用的核。这意味着`params`列表中的20个字典将具有不同的键。例如，多项式核将具有度数的值。线性核和`rbf`核则没有。
- en: 'We can access the support vectors via the `best_estimator_` attribute. There
    are 625 support vectors *holding up* the decision boundary:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过`best_estimator_`属性访问支持向量。有625个支持向量*支撑*着决策边界：
- en: '[PRE26]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, we can take a look at the predictions. Not surprisingly, we do not
    get better results than we got with the linear SVC model that we ran in the last
    section. I say not surprisingly because the best model was found to be a model
    with a linear kernel:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以看一下预测结果。不出所料，我们没有比上一节中运行的线性SVC模型得到更好的结果。我说不出所料，因为最佳模型被发现是一个线性核的模型：
- en: '[PRE27]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Although we have not improved upon our model from the previous section, it was
    still a worthwhile exercise to experiment with some nonlinear models. Indeed,
    this is often how we discover whether we have data that can be successfully separated
    linearly. This is typically difficult to visualize and so we rely on hyperparameter
    tuning to tell us which kernel classifies our data best.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们没有改进上一节中的模型，但尝试一些非线性模型仍然是一项值得的练习。事实上，我们通常就是这样发现我们是否有可以成功线性分离的数据。这通常很难可视化，所以我们依赖于超参数调整来告诉我们哪个核最适合我们的数据。
- en: This section and the previous one demonstrate the key techniques for using SVMs
    for binary classification. Much of what we have done so far applies to multiclass
    classification as well. We will take a look at SVC modeling strategies when our
    target has more than two values in the next section.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本节和上一节展示了使用SVM进行二类分类的关键技术。我们迄今为止所做的大部分内容也适用于多类分类。在下一节中，当我们的目标值超过两个时，我们将探讨SVC建模策略。
- en: SVMs for multiclass classification
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类分类的SVM
- en: All of the same concerns that we had when we used SVC for binary classification
    apply when we are doing multiclass classification. We need to determine whether
    the classes are linearly separable, and if not, which kernel will yield the best
    results. As discussed in the first section of this chapter, we also need to decide
    whether that classification is best modeled as one-versus-one or one-versus-rest.
    One-versus-one finds decision boundaries that separate each class from each of
    the other classes. One-versus-rest finds decision boundaries that distinguish
    each class from all other instances. We try both approaches in this section.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行多类分类时，所有我们在使用SVC进行二类分类时遇到的问题都适用。我们需要确定类别是否线性可分，如果不是，哪个核将产生最佳结果。正如本章第一节所讨论的，我们还需要决定这种分类是否最好建模为一对一或一对多。一对一找到将每个类别与其他每个类别分开的决策边界。一对多找到将每个类别与所有其他实例区分开的决策边界。我们在本节中尝试这两种方法。
- en: We will work with the machine failure data that we worked with in previous chapters.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们在前几章中使用过的机器故障数据。
- en: Note
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset on machine failure is available for public use at [https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification).
    There are 10,000 observations, 12 features, and two possible targets. One is binary:
    the machine failed or did not. The other has types of failure. The instances in
    this dataset are synthetic, generated by a process designed to mimic machine failure
    rates and causes.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关于机器故障的数据集可以在[https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification)上公开使用。有10,000个观测值，12个特征，以及两个可能的目标。一个是二元的：机器故障或未故障。另一个是故障类型。这个数据集中的实例是合成的，由一个旨在模拟机器故障率和原因的过程生成。
- en: 'Let’s build a multiclass SVC model:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个多类SVC模型：
- en: 'We start by loading the same libraries that we have been using in this chapter:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载本章中一直在使用的相同库：
- en: '[PRE28]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will load the machine failure type dataset and take a look at its structure.
    There is a mixture of character and numeric data. There are no missing values:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加载机器故障类型数据集并查看其结构。这里有字符和数值数据的混合。没有缺失值：
- en: '[PRE29]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s look at a few observations:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看一些观测值：
- en: '[PRE30]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let’s also look at the distribution of the target. We have a significant class
    imbalance, so we will need to deal with that in some way:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也看看目标值的分布。我们有显著的类别不平衡，所以我们需要以某种方式处理这个问题：
- en: '[PRE31]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can save ourselves some trouble later by creating a numeric code for failure
    type, which we will use rather than the character value. We do not need to put
    this into a pipeline since we are not introducing any data leakage in the conversion:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过为故障类型创建一个数字代码来节省一些麻烦，我们将使用这个数字代码而不是字符值。我们不需要将其放入管道中，因为我们没有在转换中引入任何数据泄露：
- en: '[PRE32]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We should also look at some descriptive statistics. We will need to scale the
    features:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该查看一些描述性统计。我们需要对特征进行缩放：
- en: '[PRE33]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s now create training and testing DataFrames. We should also use the `stratify`
    parameter to ensure an equal distribution of target values in our training and
    testing data:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们创建训练和测试数据框。我们还应该使用`stratify`参数来确保训练和测试数据中目标值的分布均匀：
- en: '[PRE34]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We set up the column transformations we need to run. For the numeric columns,
    we set outliers to the median and then scale the values. We do one-hot-encoding
    of the one categorical feature, `machinetype`. It has `H`, `M`, and `L` values
    for high, medium, and low quality:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了需要运行的列转换。对于数值列，我们将异常值设置为中位数，然后缩放值。我们对一个分类特征`machinetype`进行了一元编码。它有`H`、`M`和`L`值，分别代表高质量、中质量和低质量：
- en: '[PRE35]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we set up a pipeline with the column transformation and the SVC instance.
    We set the `class_weight` parameter to `balanced` to deal with class imbalance.
    This applies a weight that is inversely related to the frequency of the target
    class:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们设置一个包含列转换和SVC实例的管道。我们将`class_weight`参数设置为`balanced`以处理类别不平衡。这会应用一个与目标类别频率成反比的权重：
- en: '[PRE36]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We only have a handful of features in this case so we will not worry about feature
    selection. (We might still be concerned about features that are highly correlated,
    but that is not an issue with this dataset.)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们只有少量特征，所以我们不会担心特征选择。（我们可能仍然会担心高度相关的特征，但在这个数据集中这不是一个问题。）
- en: 'We create a dictionary with the hyperparameter combinations to use with a grid
    search. This is largely the same as the dictionary we used in the previous section,
    except we have added a decision function shape key. This will cause the grid search
    to try both one-versus-one (`ovo`) and one-versus-rest (`ovr`) classification:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个字典，包含用于网格搜索的超参数组合。这基本上与我们在上一节中使用的字典相同，只是我们添加了一个决策函数形状键。这将导致网格搜索尝试一对一（`ovo`）和一对多（`ovr`）分类：
- en: '[PRE37]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we are ready to run the randomized grid search. We will base our scoring
    on the area under the ROC curve. The best hyperparameters include the one-versus-one
    decision function and the `rbf` kernel:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好运行随机网格搜索。我们将基于ROC曲线下的面积来评分。最佳超参数包括一对一决策函数和`rbf`核：
- en: '[PRE38]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let’s see the score for each iteration. In addition to the best model that
    we saw in the previous step, there are several other hyperparameter combinations
    that have scores that are nearly as high. One-versus-rest with a linear kernel
    does nearly as well as the best-performing model:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看每次迭代的分数。除了我们在上一步中看到的最佳模型外，还有几个其他超参数组合的分数几乎一样高。使用线性核的一对多几乎与表现最好的模型一样好：
- en: '[PRE39]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We should take a look at the confusion matrix:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该看一下混淆矩阵：
- en: '[PRE40]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This produces the following plot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 13.13 – Confusion matrix for machine failure type prediction ](img/B17978_13_013.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图13.13 – 预测机器故障类型的混淆矩阵](img/B17978_13_013.jpg)'
- en: Figure 13.13 – Confusion matrix for machine failure type prediction
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.13 – 预测机器故障类型的混淆矩阵
- en: 'Let’s also do a classification report. We do not get great scores for sensitivity
    for most classes, though our model does predict heat and overstrain failures pretty
    well:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也做一个分类报告。尽管我们的模型在预测热和过载故障方面做得相当不错，但对于大多数类别，我们并没有获得很高的敏感性分数：
- en: '[PRE41]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: When modeling targets such as machine failure types that have a high class imbalance,
    we are often more concerned with metrics other than accuracy. This is partly determined
    by our domain knowledge. Avoiding false negatives may be more important than avoiding
    false positives. Doing a thorough check on a machine too early is definitely preferable
    to doing it too late.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当建模目标，如具有高类别不平衡的机器故障类型时，我们通常更关心除了准确性之外的指标。这部分取决于我们的领域知识。避免假阴性可能比避免假阳性更重要。过早地对机器进行彻底检查肯定比过晚进行更好。
- en: The 96 to 97 percent weighted precision, recall (sensitivity), and f1 scores
    do not provide a good sense of the performance of our model. They mainly reflect
    the large class imbalance and the fact that it is very easy to predict no machine
    failure. The much lower macro averages (which are just simple averages across
    classes) indicate that our model struggles to predict some types of machine failure.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 96%到97%的加权精确度、召回率（敏感性）和f1分数并不能很好地反映我们模型的表现。它们主要反映了类别不平衡很大，以及预测没有机器故障非常容易的事实。远低于宏观平均值（这些只是类别间的简单平均值）表明，我们的模型在预测某些类型的机器故障方面存在困难。
- en: This example illustrates that it is relatively easy to extend SVC to models
    that have targets with more than two values. We can specify whether we want to
    use one-versus-one or one-versus-rest classification. The one-versus-rest approach
    can be faster when the number of classes is above three since there will be fewer
    classifiers trained.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了将SVC扩展到具有多于两个值的目标的模型相对容易。我们可以指定是否想要使用一对一或一对多分类。当类别数量超过三个时，一对一方法可能会更快，因为将训练更少的分类器。
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the different strategies for implementing SVC.
    We used linear SVC (which does not use kernels), which can perform very well when
    our classes are linearly separable. We then examined how to use the kernel trick
    to extend SVC to cases where the classes are not linearly separable. Finally,
    we used one-versus-one and one-versus-rest classification to handle targets with
    more than two values.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了实现SVC的不同策略。我们使用了线性SVC（不使用核），当我们的类别是线性可分时，它可以表现得非常好。然后我们检查了如何使用核技巧将SVC扩展到类别不可分的情况。最后，我们使用一对一和一对多分类来处理多于两个值的目标。
- en: SVC is an exceptionally useful technique for binary and multiclass classification.
    It can handle both straightforward and complicated relationships between features
    and the target. There are few supervised learning problems for which SVMs should
    not at least be considered. However, it is not very efficient with very large
    datasets.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: SVC是一种特别有用的二分类和多分类技术。它能够处理特征与目标之间的简单和复杂关系。对于几乎所有的监督学习问题，SVMs至少都应该被考虑。然而，它在处理非常大的数据集时效率并不高。
- en: In the next chapter, we will explore another popular and flexible classification
    algorithm, Naive Bayes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一种流行且灵活的分类算法，朴素贝叶斯。
