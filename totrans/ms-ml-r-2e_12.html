<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Time Series and Causality</h1>
            </header>

            <article>
                
<div class="packt_quote">"An economist is an expert who will know tomorrow why the things he predicted yesterday didn't happen today."<br/>
                                                                                                        - Laurence J. Peter</div>
<p>A univariate time series is where the measurements are collected over a standard measure of time, which could be by the minute, hour, day, week, or month. What makes the time series problematic over the other data is that the order of the observations probably matters. This dependency of order can cause the standard analysis methods to produce an unnecessarily high bias or variance.</p>
<p>It seems that there is a paucity of literature on machine learning and time series data. This is unfortunate as so much of real-world data involves a time component. Furthermore, time series analysis can be quite complicated and tricky. I would say that if you haven't seen a time series analysis done incorrectly, you haven't been looking close enough.</p>
<p>Another aspect involving time series that is often neglected is causality. Yes, we don't want to confuse correlation with causation, but in time series analysis, one can apply the technique of Granger causality in order to determine if causality, statistically speaking, exists.</p>
<p>In this chapter, we will apply time series/econometric techniques to identify univariate forecast models, vector autoregression models, and finally, Granger causality. After completing the chapter, you may not be a complete master of the time series analysis, but you will know enough to perform an effective analysis and understand the fundamental issues to consider when building time series models and creating predictive models (forecasts).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Univariate time series analysis</h1>
            </header>

            <article>
                
<p>We will focus on two methods to analyze and forecast a single time series: <strong>exponential smoothing</strong> and <strong>autoregressive integrated moving average</strong> (<strong>ARIMA</strong>) models. We will start by looking at exponential smoothing models.</p>
<p>Like moving average models, exponential smoothing models use weights for past observations. But unlike moving average models, the more recent the observation the more weight it is given relative to the later ones. There are three possible smoothing parameters to estimate: the overall smoothing parameter, a trend parameter, and smoothing parameter. If no trend or seasonality is present, then these parameters become null.</p>
<p>The smoothing parameter produces a forecast with the following equation:</p>
<p><em>Yt+1 = α(Yt) + (1 – α)Yt-1 + (1-α)2Yt-2 +…, where 0 &lt; α ≤ 1</em></p>
<p>In this equation, <em>Y<sub>t</sub></em> is the value at the time T, and alpha (<em>α</em>) is the smoothing parameter. Algorithms optimize the alpha (and other parameters) by minimizing the errors, for example, <strong>sum of squared error</strong> (<strong>SSE</strong>) or <strong>mean squared error</strong> (<strong>MSE</strong>).</p>
<p>The forecast equation along with trend and seasonality equations, if applicable, will be as follows:</p>
<ul>
<li>The forecast, where <em>A</em> is the preceding smoothing equation and <em>h</em> is the number of forecast periods, <em>Y<sub>t+h</sub> = A + hB<sub>t</sub> + S<sub>t</sub></em></li>
</ul>
<ul>
<li>The trend equation,  <em>B<sub>t</sub> = β(A<sub>t</sub> – A<sub>t-1</sub>) + (1 – β)B<sub>t-1</sub></em></li>
<li>The seasonality, where <em>m</em> is the number of seasonal periods, <em>S<sub>t</sub> = Ω(Y<sub>t</sub> – A<sub>t-1</sub> – B<sub>t-1</sub>) + (1 - Ω)S<sub>t-m</sub></em></li>
</ul>
<p>This equation is referred to as the <strong>Holt-Winters Method</strong>. The forecast equation is additive in nature with the trend as linear. The method also allows the inclusion of a dampened trend and multiplicative seasonality, where the seasonality proportionally increases or decreases over time. In my experience, the Holt-Winter's Method provides the best forecasts, even better than the ARIMA models. I have come to this conclusion on having to update long-term forecasts for hundreds of time series based on monthly data, and in roughly 90 percent of the cases, Holt-Winters produced minimal forecast error. Additionally, you don't have to worry about the assumption of stationarity as in an ARIMA model. Stationarity is where the time series has a constant mean, variance, and correlation between all the time periods. Having said this, it is still important to understand the ARIMA models as there will be situations where they have the best performance.</p>
<p>Starting with the autoregressive model, the value of <em>Y</em> at time <em>T</em> is a linear function of the prior values of <em>Y</em>. The formula for an autoregressive lag-1 model <em>AR(1)</em>, is <em>Yt = constant + ΦYt-1 + Et</em>. The critical assumptions for the model are as follows:</p>
<ul>
<li><em>Et</em> denotes the errors that are identically and independently distributed with a mean zero and constant variance</li>
<li>The errors are independent of <em>Yt</em></li>
<li><em>Yt, Yt-1, Yt-n...</em> is stationary, which means that the absolute value of <em>Φ</em> is less than one</li>
</ul>
<p>With a stationary time series, you can examine <strong>Autocorrelation Function</strong> (<strong>ACF</strong>). The ACF of a stationary series gives correlations between <em>Yt</em> and <em>Yt-h</em> for <em>h = 1, 2...n</em>. Let's use R to create an <em>AR(1)</em> series and plot it. In doing so, we will also look at the capabilities of the <kbd>ggfortify</kbd> package, which acts as a wrapper around <kbd>ggplot2</kbd> functions:</p>
<pre>
<strong>    &gt; library(ggfortify)  </strong> <br/><br/><strong>    &gt; set.seed(123)</strong><br/>    <br/>    <strong>&gt; ar1 &lt;- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)</strong><br/>    <br/>    <strong>&gt; autoplot(ar1, main = "AR1")</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="276" width="387" class="image-border" src="assets/image_11_01.png"/></div>
<p>Now, let's examine <kbd>ACF</kbd>:</p>
<pre>
    <strong>&gt; autoplot(acf(ar1, plot = F), main = "AR1 - ACF")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="281" width="394" class="image-border" src="assets/image_11_02.png"/></div>
<p>The <strong>ACF</strong> plot shows the correlations exponentially decreasing as the <strong>Lag</strong> increases. The dotted blue lines indicate the confidence bands of a significant correlation. Any line that extends above the high or below the low band is considered significant. In addition to ACF, one should also examine <strong>Partial Autocorrelation Function</strong> (<strong>PACF</strong>). PACF is a conditional correlation, which means that the correlation between <em>Yt</em> and <em>Yt-h</em> is conditional on the observations that come between the two. One way to intuitively understand this is to think of a linear regression model and its coefficients. Let's assume that you have <em>Y = B0 + B1X1</em> versus <em>Y = B0 + B1X1 + B2X2</em>. The relationship of <em>X</em> to <em>Y</em> in the first model is linear with a coefficient, but in the second model, the coefficient will be different because of the relationship between <em>Y</em> and <em>X2</em> now being accounted for as well. Note that in the following <kbd>PACF</kbd> plot, the partial autocorrelation value at lag-1 is identical to the autocorrelation value at lag-1, as this is not a conditional correlation:</p>
<pre>
    <strong>&gt; autoplot(pacf(ar1, plot = F), main = "AR1 - PACF")</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="277" width="389" class="image-border" src="assets/image_11_03.png"/></div>
<p>We can safely make the assumption that the series is stationary from the appearance of the preceding time series plot. We'll look at a couple of statistical tests in the practical exercise to ensure that the data is stationary, but most of the time, the eyeball test is sufficient. If the data is not stationary, then it is possible to detrend the data by taking its differences. This is the Integrated (I) in ARIMA. After differencing, the new series becomes <em>ΔYt = Yt - Yt-1</em>. One should expect a first-order difference to achieve stationarity, but on some occasions, a second-order difference may be necessary. An ARIMA model with <em>AR(1)</em> and <em>I(1)</em> would be annotated as (1,1,0).</p>
<p>The MA stands for moving average. This is not the simple moving average as the 50-day moving average of a stock price, it's rather a coefficient that is applied to the errors. The errors are, of course, identically and independently distributed with a mean zero and constant variance. The formula for an <em>MA(1)</em> model is <em>Yt = constant + Et + ΘEt-1</em>. As we did with the <em>AR(1)</em> model, we can build an <em>MA(1)</em> in R, as follows:</p>
<pre>
    <strong>&gt; set.seed(123)</strong><br/>    <br/>    <strong>&gt; ma1 &lt;- arima.sim(list(order = c(0, 0, 1), ma = -0.5), n = 200)</strong><br/>    <br/>    <strong>&gt; autoplot(ma1, main = "MA1")</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="322" width="452" class="image-border" src="assets/image_11_04.png"/></div>
<p>The <kbd>ACF</kbd> and <kbd>PACF</kbd> plots are a bit different from the <em>AR(1)</em> model. Note that there are some rules of thumb while looking at the plots in order to determine whether the model has AR and/or MA terms. They can be a bit subjective; so I will leave it to you to learn these heuristics, but trust R to identify the proper model. In the following plots, we will see a significant correlation at lag-1 and two significant partial correlations at lag-1 and lag-2:</p>
<pre>
    <strong>&gt; autoplot(acf(ma1, plot = F), main = "MA1 - ACF")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="290" width="406" class="image-border" src="assets/image_11_05.png"/></div>
<p>The preceding figure is the <kbd>ACF</kbd> plot, and now, we will see the <kbd>PACF</kbd> plot:</p>
<pre>
    <strong>&gt; autoplot(pacf(ma1, plot = F), main = "MA1 - PACF")</strong>
</pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="264" width="370" class="image-border" src="assets/image_11_06.png"/></div>
<p>With the ARIMA models, it is possible to incorporate seasonality, including the autoregressive, integrated, and moving average terms. The nonseasonal ARIMA model notation is commonly (p,d,q). With seasonal ARIMA, assume that the data is monthly, then the notation would be (p,d,q) x (P,D,Q)12, with the '12' in the notation taking the monthly seasonality into account. In the packages that we will use, R will automatically identify whether the seasonality should be included; if so, the optimal terms will be included as well.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Understanding Granger causality</h1>
            </header>

            <article>
                
<p>Imagine you are asked a question such as, "What is the relationship between the amount of new prescriptions and total prescriptions for medicine X?". You know that these are measured monthly, so what could you do to understand that relationship, given that people believe that new scripts will drive up total scripts. Or, how about testing the hypothesis that commodity prices, in particular copper, is a leading indicator of stock market prices in the US? Well, with two sets of time series data, <em>x</em> and <em>y</em>, Granger causality is a method that attempts to determine whether one series is likely to influence a change in the other. This is done by taking different lags of one series and using this to model the change in the second series. To accomplish this, we will create two models that will predict <em>y</em>, one with only the past values of <em>y</em> (<kbd>Ω</kbd>) and the other with the past values of <em>y</em> and <em>x</em> (<kbd>π</kbd>). The models are as follows, where <em>k</em> is the number of lags in the time series:</p>
<div class="CDPAlignCenter CDPAlign"><img height="45" width="401" class="image-border" src="assets/image_11_07.png"/></div>
<p>The RSS are then compared and <kbd>F-test</kbd> is used to determine whether the nested model <kbd>(Ω)</kbd> is adequate enough to explain the future values of <em>y</em> or whether the full model (<kbd>π</kbd>) is better. <kbd>F-test</kbd> is used to test the following null and alternate hypotheses:</p>
<ul>
<li><em>H0</em>: <kbd>αi = 0</kbd> for each <em>i</em> <kbd>∊[1,k]</kbd>, no Granger causality</li>
<li><em>H1</em>: <kbd>αi ≠ 0</kbd> for at least one <em>i</em> <kbd>∊[1,k]</kbd>, Granger causality</li>
</ul>
<p>Essentially, we are trying to determine whether we can say that statistically, <em>x</em> provides more information about the future values of <em>y</em> than the past values of <em>y</em> alone. In this definition, it is clear that we are not trying to prove actual causation; only that the two values are related by some phenomenon. Along these lines, we must also run this model in reverse in order to verify that <em>y</em> does not provide information about the future values of <em>x</em>. If we find that this is the case, it is likely that there is some exogenous variable, say <em>Z</em>, that needs to be controlled or would possibly be a better candidate for the Granger causation. To avoid spurious results, the method should be applied to a stationary time series. Note that research papers are available that discuss the techniques nonlinear models use, but this is outside of the scope for this book; however, we will examine it from a non-stationary standpoint. There is an excellent introductory paper that revolves around the age-old conundrum of the chicken and the egg (Thurman, 1988).</p>
<p>There are a couple of different ways to identify the proper lag structure. Naturally, one can use brute force and ignorance to test all the reasonable lags, one at a time. One may have a rational intuition based on domain expertise or perhaps prior research that exists to guide the lag selection. If not, then <strong>Vector Autoregression</strong> (<strong>VAR</strong>) can be applied to identify the lag structure with the lowest information criterion, such as <strong>Aikake's Information Criterion</strong> (<strong>AIC</strong>) or <strong>Final Prediction Error</strong> (<strong>FPE</strong>). For simplicity, here is the notation for the VAR models with two variables, and this incorporates only one lag for each variable. This notation can be extended for as many variables and lags as appropriate.</p>
<ul>
<li><em>Y = constant<sub>1</sub> + B<sub>11</sub>Y<sub>t-1</sub> + B<sub>12</sub>Y<sub>t-1</sub> + e<sub>1</sub></em></li>
<li><em>X = constant<sub>1</sub> + B2<sub>1</sub>Y<sub>t-1</sub> + B2<sub>2</sub>Y<sub>t-1</sub> + e2</em></li>
</ul>
<p>In R, this process is quite simple to implement as we will see in the following practical problem.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<div class="packt_quote">The planet isn't going anywhere. We are! We're goin' away.<br/>
                                                                         - Philosopher and comedian, George Carlin</div>
<p>Climate change is happening. It always has and will, but the big question, at least from a political and economic standpoint, is I the climate change man-made? I will use this chapter to put econometric time series modeling to the test to try and learn whether carbon emissions cause, statistically speaking, climate change, and in particular, rising temperatures. Personally, I would like to take a neutral stance on the issue, always keeping in mind the tenets that Mr. Carlin left for us in his teachings on the subject.</p>
<p>The first order of business is to find and gather the data. For temperature, I chose the <strong>HadCRUT4</strong> annual median temperature time series, which is probably the gold standard. This data is compiled by a cooperative effort of the Climate Research Unit of the University of East Anglia and the Hadley Centre at the UK's Meteorological Office. A full discussion of how the data is compiled and modeled is available at <a href="http://www.metoffice.gov.uk/hadobs/index.html"><span class="URLPACKT">http://www.metoffice.gov.uk/hadobs/index.html</span></a>.</p>
<p>The data that we will use is provided as an annual anomaly, which is calculated as the difference of the median annual surface temperature for a given time period versus the average of the reference years (1961-1990). The annual surface temperature is an ensemble of the temperatures collected globally and blended from the <strong>CRUTEM4</strong> surface air temperature and <strong>HadSST3</strong> sea-surface datasets. This data has come under attack as biased and unreliable: <a href="http://www.telegraph.co.uk/comment/11561629/Top-scientists-start-to-examine-fiddled-global-warming-figures.html"><span class="URLPACKT">http://www.telegraph.co.uk/comment/11561629/Top-scientists-start-to-examine-fiddled-global-warming-figures.html</span></a>. This is way outside of our scope of effort here, so we must accept and utilize this data as it is. I've pulled the data from 1919 March, 1958 through 2013 to match our CO2 data.</p>
<p>Global CO2 emission estimates can be found at the <strong>Carbon Dioxide Information Analysis Center</strong> (<strong>CDIAC</strong>) of the US Department of Energy at the following website:  <a href="http://cdiac.ornl.gov/">http://cdiac.ornl.gov/</a></p>
<p>I've placed the data in a <kbd>.csv</kbd> file (<kbd>climate.csv</kbd>) for you to download and store in your working directory: <a href="https://github.com/datameister66/data/">https://github.com/datameister66/data/</a></p>
<p>Let's load it and examine the structure:</p>
<pre>
    <strong>&gt; climate &lt;- read.csv("climate.csv", stringsAsFactors = F)<br/><br/>    &gt; str(climate)<br/>     'data.frame': 95 obs. of 3 variables:<br/>     $ Year: int 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 ...<br/>     $ CO2 : int 806 932 803 845 970 963 975 983 1062 1065 ...<br/>     $ Temp: num -0.272 -0.241 -0.187 -0.301 -0.272 -0.292 -0.214 <br/>       -0.105 -0.208  -0.206 ...</strong>
</pre>
<p>Finally, we will put this in a time series structure, specifying the start and end years:</p>
<pre>
    <strong>&gt; climate &lt;- ts(climate[, 2:3], frequency = 12,<br/>       start = 1919, end = 2013)  <br/>    <br/>    &gt; head(climate)<br/>         CO2   Temp<br/>    [1,] 806 -0.272<br/>    [2,] 932 -0.241<br/>    [3,] 803 -0.187<br/>    [4,] 845 -0.301<br/>    [5,] 970 -0.272<br/>    [6,] 963 -0.292       </strong>
</pre>
<p>With our data loaded and put in time series structures, we can now begin to understand and further prepare it for analysis.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding and preparation</h1>
            </header>

            <article>
                
<p>Two packages are required for this effort, so ensure they are installed on your system:</p>
<pre>
    <strong>&gt; library(forecast)</strong><br/>    <br/>    <strong>&gt; library(tseries)</strong>
</pre>
<p>Let's start out with plots of the two time series:</p>
<pre>
    <strong>&gt; autoplot(climate)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="188" width="329" class="image-border" src="assets/image_11_08-1.png"/></div>
<p>It appears that CO2 levels really started to increase after World War II and a rapid rise in temperature anomalies in the mid-1970s. There does not appear to be any obvious outliers, and variation over time appears constant. Using the standard procedure, we can see that the two series are highly correlated, as follows:</p>
<pre>
    <strong>&gt; cor(climate)</strong><br/>    <strong>           CO2      Temp<br/>    CO2  1.0000000 0.8404215<br/>    Temp 0.8404215 1.0000000</strong>
</pre>
<p>As discussed earlier, this is nothing to jump for joy about as it proves absolutely nothing. We will look for the structure by plotting <kbd>ACF</kbd> and <kbd>PACF</kbd> for both series:</p>
<pre>
    <strong>&gt; autoplot(acf(climate[, 2], plot = F), main="Temp ACF")</strong>
</pre>
<p>The output of the preceding code snippet is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="216" width="378" class="image-border" src="assets/image_11_09-1.png"/></div>
<p>This code gives us the <kbd>PACF</kbd> plot for temperature:</p>
<pre>
<strong>    &gt; autoplot(pacf(climate[, 2], plot = F), main = "Temp PACF")</strong>
</pre>
<p><span>The output of the preceding code snippet is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="201" width="352" class="image-border" src="assets/image_11_10.png"/></div>
<p><span>This code gives us the <kbd>ACF</kbd> plot for <kbd>CO2</kbd>:</span></p>
<pre>
<strong>    &gt; autoplot(acf(climate[, 1], plot = F), main = "CO2 ACF")</strong>
</pre>
<p><span>The output of the preceding code snippet is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="267" width="468" class="image-border" src="assets/image_11_11-2.png"/></div>
<p><span>This code gives us the <kbd>PACF</kbd> plot for <kbd>CO2</kbd>:</span></p>
<pre>
<strong>    &gt; autoplot(pacf(climate[, 1], plot = F), main = "CO2 PACF")</strong>
</pre>
<p><span>The output of the preceding code snippet is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="270" width="473" class="image-border" src="assets/image_11_12-1.png"/></div>
<p>With the slowly decaying ACF patterns and rapidly decaying PACF patterns, we can assume that these series are both autoregressive, although temp appears to have some significant MA terms. Next, let's have a look at <strong>Cross Correlation Function</strong> (<strong>CCF</strong>). Note that we put our <em>x</em> before our <em>y</em> in the function:</p>
<pre>
    <strong>&gt; ccf(climate[, 1], climate[, 2], main = "CCF")</strong>
</pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="249" width="436" class="image-border" src="assets/image_11_13-1.png"/></div>
<p><strong>CCF</strong> shows us the correlation between the temperature and lags of CO2.  If the negative lags of the <em>x</em> variable have a high correlation, we can say that <em>x</em> leads <em>y</em>. If the positive lags of <em>x</em> have a high correlation, we say that <em>x</em> lags <em>y</em>. Here, we can see that CO2 is both a leading and lagging variable. For our analysis, it is encouraging that we see the former, but odd that we see the latter. We will see during the VAR and Granger causality analysis whether this will matter or not.</p>
<p>Additionally, we need to test whether the data is stationary. We can prove this with the <strong>Augmented Dickey-Fuller</strong> (<strong>ADF</strong>) test available in the <kbd>tseries</kbd> package, using the <kbd>adf.test()</kbd> function, as follows:</p>
<pre>
    <strong>&gt; adf.test(climate[, 1])<br/><br/>           Augmented Dickey-Fuller Test<br/><br/>    data: climate[, 1]<br/>    Dickey-Fuller = -1.1519, Lag order = 4, p-value =<br/>    0.9101<br/>    alternative hypothesis: stationary<br/><br/>    &gt; adf.test(climate[, 2])<br/><br/>           Augmented Dickey-Fuller Test<br/><br/>    data: climate[, 2]<br/>    Dickey-Fuller = -1.8106, Lag order = 4, p-value =<br/>    0.6546<br/>    alternative hypothesis: stationary</strong>
</pre>
<p>For both series, we have insignificant <kbd>p-values</kbd>, so we cannot reject the null and conclude that they are not stationary.</p>
<p>Having explored the data, let's begin the modeling process, starting with the application of univariate techniques to the temperature anomalies.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>For the modeling and evaluation step, we will focus on three tasks. The first is to produce a univariate forecast model applied to just the surface temperature. The second is to develop a regression model of the surface temperature based on itself and CO2 levels, using that output to inform our work on whether CO2 levels Granger cause the surface temperature anomalies.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Univariate time series forecasting</h1>
            </header>

            <article>
                
<p>With this task, the objective is to produce a univariate forecast for the surface temperature, focusing on choosing either a Holt linear trend model or an ARIMA model. We will train the models and determine their predictive accuracy on an out-of-time test set, just like we've done in other learning endeavors. The following code creates the temperature subset and then the train and test sets, starting after WWII:</p>
<pre>
    <strong>&gt; temp &lt;- climate[, 2]<br/><br/>    &gt; temp &lt;- climate[, 2]<br/><br/>    &gt; train &lt;- window(temp, start = 1946, end = 2003)<br/><br/>    &gt; test &lt;- window(temp, start = 2004)</strong>
</pre>
<p class="packt_figure">To build our smoothing model, we will use the <kbd>holt()</kbd> function found in the <kbd>forecast</kbd> package. We will build two models, one with and one without a damped trend. In this function, we will need to specify the time series, number of forecast periods as <em>h = ...</em>, method to select the initial state values, either <kbd>"optimal"</kbd> or <kbd>"simple"</kbd>, and whether we want a damped trend. Specifying <kbd>"optimal"</kbd>, the algorithm will find optimal initial starting values along with the smoothing parameters, while <kbd>"simple"</kbd> calculates starting values using the first few observations. Now, in the <kbd>forecast</kbd> package, you can use the <kbd>ets()</kbd> function, which will find all the optimal parameters. However, in our case, let's stick with <kbd>holt()</kbd> so that we can compare methods. Let's try the <kbd>holt</kbd> model without a damped trend, as follows:</p>
<pre>
<strong>     &gt; fit.holt &lt;- holt(train, h = 10, initial = "optimal")</strong>
</pre>
<p>Plot the <kbd>forecast</kbd> and see how well it performed out of sample with the following code:</p>
<pre>
    <strong>&gt; plot(forecast(fit.holt))</strong><br/>    <br/>    <strong>&gt; lines(test, type = "o")<br/><br/></strong>
</pre>
<p>The output of the preceding code is as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="191" width="335" class="image-border" src="assets/image_11_14-1.png"/></div>
<p>Looking at the plot, it seems that this forecast is showing a slight linear uptrend. Let's have a go by including the <kbd>damped</kbd> trend, as follows:</p>
<pre>
    <strong>&gt; fit.holtd &lt;- holt(train, h = 10, initial = "optimal", damped = <br/>      TRUE)<br/><br/>    &gt; plot(forecast(fit.holtd),main = "Holt Damped")</strong><br/><br/><strong>    &gt; lines(test, type = "o")<br/></strong>
</pre>
<p><span>The output of the preceding code is as follows: </span></p>
<div class="CDPAlignCenter CDPAlign"><img height="216" width="378" class="image-border" src="assets/image_11_15-1.png"/></div>
<p>Lastly, in univariate analysis, we develop an <kbd>ARIMA</kbd> model, using <kbd>auto.arima()</kbd>, which is also from the <kbd>forecast</kbd> package. There are many options that you can specify in the function, or you can just include your time series data and it will find the best <kbd>ARIMA</kbd> fit:</p>
<pre>
    <strong>&gt; fit.arima &lt;- auto.arima(train)<br/>    &gt; summary(fit.arima)<br/>    Series: train <br/>    ARIMA(0,1,1) with drift <br/><br/>    Coefficients:<br/>            ma1  drift<br/>        -0.6949 0.0094<br/>    s.e. 0.1041 0.0047</strong>
</pre>
<p>The abbreviated output shows that the model selected is an MA = 1, I = 1, or <kbd>ARIMA(0,1,1)</kbd> with drift (equivalent to an intercept term). We can examine the plot of its performance on the <kbd>test</kbd> data in the same fashion as before:</p>
<pre>
<strong>    &gt; plot(forecast(fit.arima, h = 10))</strong><br/>    <br/><strong>    &gt; lines(test, type="o")<br/></strong>
</pre>
<p class="packt_figure"><span>The output of the preceding code is as follows: </span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="217" width="380" class="image-border" src="assets/image_11_16-1.png"/></div>
<p>This is very similar to the <kbd>holt</kbd> method with no damped trend. We can score each model to find the one that provides the lowest error, mean absolute percentage error (MAPE), with the following code:</p>
<pre>
<strong>    &gt; mapeHOLT &lt;- sum(abs((test - fit.holt$mean)/test))/10</strong><br/><strong><br/>    &gt; mapeHOLT</strong><br/><strong>    [1] 0.105813</strong><br/><strong><br/>    &gt; mapeHOLTD &lt;- sum(abs((test - fit.holtd$mean)/test))/10</strong><br/><strong><br/>    &gt; mapeHOLTD</strong><br/><strong>    [1] 0.2220256</strong><br/><strong><br/>    &gt; mapeARIMA &lt;- sum(abs((test - forecast(fit.arima, h = <br/>      10)$mean)/test))/10</strong><br/><strong><br/>    &gt; mapeARIMA</strong><br/><strong>    [1] 0.1034813<br/></strong>
</pre>
<p>The forecast error is slightly less with the ARIMA 0,1,1 versus the <kbd>holt</kbd> methods, and clearly, the damped trend model performed the worst.</p>
<p>With the statistical and visual evidence, it seems that the best choice for a univariate forecast model is the ARIMA model. Interestingly, in the first edition using annual data, the Holt method with a damped trend had the best accuracy.</p>
<p>With this, we've completed the building of a univariate forecast model for the surface temperature anomalies, and now we will move on to the next task of seeing if CO2 levels cause these anomalies.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Examining the causality</h1>
            </header>

            <article>
                
<p>For this chapter, this is where I think the rubber meets the road and we will separate causality from mere correlation, well, statistically speaking anyway. This is not the first time that this technique has been applied to the problem. Triacca (2005) found no evidence to suggest that atmospheric CO2 Granger caused the surface temperature anomalies. On the other hand, Kodra (2010) concluded that there is a causal relationship, but put forth the caveat that their data was not stationary even after a second-order differencing. While this effort will not settle the debate, it will hopefully inspire you to apply the methodology in your personal endeavors. The topic at hand certainly provides an effective training ground to demonstrate the Granger causality. </p>
<p>Our plan here is to first demonstrate spurious linear regression where the residuals suffer from autocorrelation, also known as serial correlation. Then, we will examine two different approaches to Granger causality. The first will be the traditional methods, where both series are stationary. Then, we will look at the method demonstrated by Toda and Yamamoto (1995), which applies the methodology to the raw data or, as it is sometimes called, the "levels".</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Linear regression</h1>
            </header>

            <article>
                
<p>Let's get started with the spurious regression then, which I have seen implemented in the real world far too often. Here we simply build a linear model and examine the results:</p>
<pre>
<strong>    &gt; fit.lm &lt;- lm(Temp ~ CO2, data = climate)<br/><br/>    &gt; summary(fit.lm)<br/><br/>    Call:<br/>    lm(formula = Temp ~ CO2, data = climate)<br/><br/>    Residuals:<br/>         Min       1Q  Median      3Q     Max <br/>    -0.36411 -0.08986 0.00011 0.09475 0.28763 <br/><br/>    Coefficients:<br/>                  Estimate  Std. Error  t value     Pr(&gt;|t|) <br/>    (Intercept) -2.430e-01   2.357e-02   -10.31   &lt;2e-16 ***<br/>    CO2          7.548e-05   5.047e-06    14.96   &lt;2e-16 ***<br/>    ---<br/>    Signif. codes: <br/>    0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>    Residual standard error: 0.1299 on 93 degrees of freedom<br/>    Multiple R-squared: 0.7063, Adjusted R-squared: 0.7032 <br/>    F-statistic: 223.7 on 1 and 93 DF, p-value: &lt; 2.2e-16</strong>
</pre>
<p>Notice how everything is significant, and we have an adjusted R-squared of 0.7. OK, they are highly correlated but this is all meaningless as discussed by Granger and Newbold (1974). Again, I have seen results like these presented in meetings with many people with advanced degrees, and I had to be the bad guy and challenge the results.</p>
<p>We can plot the serial correlation, starting with a time series plot of the residuals, which produce a clear pattern:</p>
<pre>
<strong>    &gt; plot.ts(fit.lm$residuals)</strong>
</pre>
<p>The output of the preceding code is as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img height="266" width="466" class="image-border" src="assets/image_11_17-1.png"/></div>
<p>Then we create an ACF plot showing significant autocorrelation out to lag 10:</p>
<pre>
<strong>    &gt; acf(fit.lm$residuals)</strong>
</pre>
<div class="CDPAlignCenter CDPAlign"><img height="262" width="459" class="image-border" src="assets/image_11_18-1.png"/></div>
<p>You can test for autocorrelation by performing the <kbd>Durbin-Watson test</kbd>. The null hypothesis in the test is no autocorrelation exists:</p>
<pre>
<strong>    &gt; dwtest(fit.lm)</strong><br/><br/><strong>    Durbin-Watson test</strong><br/><br/><strong>    data: fit.lm</strong><br/><strong>    DW = 0.77425, p-value = 4.468e-12<br/>    alternative hypothesis: true autocorrelation is greater than 0</strong>
</pre>
<p>From examining the plots, it comes as no surprise that we can safely reject the null hypothesis of no autocorrelation. The simple way to deal with autocorrelation is to incorporate lagged variables of the dependent time series and/or to make all the data stationary. We will do that next using vector autoregression to identify the appropriate lag structure to incorporate in our causality efforts. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Vector autoregression</h1>
            </header>

            <article>
                
<p>We've seen in the preceding section that temperature is stationary and CO2 requires a first order difference. Another simple way to show this is with the <kbd>forecast</kbd> package's  <kbd>ndiffs()</kbd> function. It provides an output that spells out the minimum number of differences needed to make the data stationary. In the function, you can specify which test out of the three available ones you would like to use: <strong>Kwiatkowski, Philips, Schmidt &amp; Shin</strong> (<strong>KPSS</strong>), <strong>Augmented Dickey Fuller</strong> (<strong>ADF</strong>), or <strong>Philips-Peron</strong> (<strong>PP</strong>). I will use ADF in the following code, which has a null hypothesis that the data is not stationary:</p>
<pre>
<strong>    &gt; ndiffs(climate[, 1], test = "adf")</strong><br/><strong>    [1] 1</strong><br/><br/><strong>    &gt; ndiffs(climate[, 2], test = "adf")</strong><br/><strong>    [1] 1<br/></strong>
</pre>
<p>We see that both require a first order difference to become stationary. To get started, we will create the difference. Then, we will complete the traditional approach, where both series are stationary. Let's also load our packages for this exercise:</p>
<pre>
<strong>    &gt; library(vars)</strong><br/><br/><strong>    &gt; library(aod)</strong><br/><br/><strong>    &gt; climateDiff &lt;- diff(climate)<br/><br/>    &gt; climateDiff &lt;- window(climateDiff, start = 1946)<br/><br/>    &gt; head(climateDiff)<br/>         CO2     Temp<br/>    [1,]  78   -0.099<br/>    [2,] 154    0.034<br/>    [3,]  77    0.001<br/>    [4,] -50   -0.035<br/>    [5,] 211   -0.100<br/>    [6,] 137    0.121</strong><strong><br/></strong>
</pre>
<p>It is now a matter of determining the optimal lag structure based on the information criteria using vector autoregression. This is done with the <kbd>VARselect</kbd> function in the <kbd>vars</kbd> package. You only need to specify the data and number of lags in the model using <kbd>lag.max = x</kbd> in the function. Let's use a maximum of 12 lags:</p>
<pre>
    <strong>&gt; lag.select &lt;- VARselect(climateDiff, lag.max = 12)<br/><br/>    &gt; lag.select$selection<br/>    AIC(n) HQ(n) SC(n) FPE(n) <br/>         5     1     1      5</strong>
</pre>
<p>We called the information criteria using <kbd>lag$selection</kbd>. Four different criteria are provided including <strong>AIC</strong>, <strong>Hannan-Quinn Criterion</strong> (<strong>HQ</strong>), <strong>Schwarz-Bayes Criterion</strong> (<strong>SC</strong>), and <strong>FPE</strong>. Note that AIC and SC are covered in <a href="e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Linear Regression - The Blocking and Tackling of Machine Learning</em>, so I will not go over the criterion formulas or differences here. If you want to see the actual results for each lag, you can use <kbd>lag$criteria</kbd>. We can see that <kbd>AIC</kbd> and <kbd>FPE</kbd> have selected lag <span><span>5</span></span> and HQ and SC lag 1 as the optimal structure to a <kbd>VAR</kbd> model. It seems to make sense that the 5-year lag is the one to use. We will create that model using the <kbd>var()</kbd> function. I'll let you try it with lag 1:</p>
<pre>
    <strong>&gt; fit1 &lt;- VAR(climateDiff, p = 5)</strong>
</pre>
<p>The summary results are quite lengthy as it builds two separate models and would take up probably two whole pages. What I provide is the abbreviated output showing the results with temperature as the prediction:</p>
<pre>
<strong>    &gt; summary(fit1)</strong><br/><strong>    Residual standard error: 0.1006 on 52 degrees of freedom<br/>    Multiple R-Squared: 0.4509, Adjusted R-squared: 0.3453 <br/>    F-statistic: 4.27 on 10 and 52 DF, p-value: 0.0002326</strong>
</pre>
<p>The model was significant with a resulting adjusted R-square of 0.35.</p>
<p>As we did in the previous section, we should check for serial correlation. Here, the <kbd>VAR</kbd> package provides the <kbd>serial.test()</kbd> function for multivariate autocorrelation. It offers several different tests, but let's focus on <kbd> Portmanteau Test</kbd>, and also please note that the DW test is for univariate series only. The null hypothesis is that autocorrelations are zero and the alternate is that they are not zero:</p>
<pre>
    <strong>&gt; serial.test(fit1, type = "PT.asymptotic")<br/><br/>        Portmanteau Test (asymptotic)<br/><br/>    data: Residuals of VAR object fit1<br/>    Chi-squared = 35.912, df = 44, p-value = 0.8021</strong>
</pre>
<p>With <kbd>p-value</kbd> at <kbd>0.3481</kbd>, we do not have evidence to reject the null and can say that the residuals are not autocorrelated. What does the test say with 1 lag?</p>
<p>To do the Granger causality tests in R, you can use either the <kbd>lmtest</kbd> package and the <kbd>Grangertest()</kbd> function or the <kbd>causality()</kbd> function in the <kbd>vars</kbd> package. I'll demonstrate the technique using <kbd>causality()</kbd>. It is very easy as you just need to create two objects, one for <kbd>x</kbd> causing <kbd>y</kbd> and one for <kbd>y</kbd> causing <kbd>x</kbd>, utilizing the <kbd><span><span>fit1</span></span></kbd> object previously created:</p>
<pre>
    <strong>&gt; x2y &lt;- causality(fit1, cause = "CO2")<br/><br/>    &gt; y2x &lt;- causality(fit1, cause = "Temp")</strong>
</pre>
<p>It is now just a simple matter to call the Granger test results:</p>
<pre>
    <strong>&gt; x2y$Granger</strong><br/>    <strong> <br/>         Granger causality H0: CO2_diff do not Granger-cause</strong><br/><strong>         climate2.temp</strong><br/><br/><strong>    data: VAR object fit1</strong><br/><strong>    F-Test = 2.2069, df1 = 5, df2 = 104, p-value = 0.05908</strong><br/><br/><strong>    &gt; y2x$Granger</strong><br/><br/><strong>         Granger causality H0: climate2.temp do not Granger-cause</strong><br/><strong>         CO2_diff</strong><br/><br/><strong>    data: VAR object fit1</strong><br/><strong>    F-Test = 0.66783, df1 = 5, df2 = 104, p-value = 0.6487<br/></strong>
</pre>
<p>The <kbd>p-value</kbd> value for <span><span>CO2 differences of</span></span> Granger causing <span><span>temperature</span></span> is <kbd>0.05908</kbd> and not significant in the other direction. So what does all this mean? The first thing we can say is that Y does not cause X. As for X causing Y, we cannot reject the null at the 0.05 significance level and therefore conclude that X does not Granger cause Y.  However, is that the relevant conclusion here? Remember, the p-value evaluates how likely the effect is if the null hypothesis is true. Also, remember that the test was never designed to be some binary Yay or Nay.  If this was a controlled experiment, it would be unlikely for us to hesitate to say we had insufficient evidence to reject the null, like the <strong>Food and Drug Administration</strong> (<strong>FDS</strong>) would do for a phase 3 clinical trial.  Since this study is based on observational data, I believe we can say that it is highly probable that "CO2 emissions Granger cause Surface Temperature Anomalies". But, there is a lot of room for criticism on that conclusion. I mentioned upfront the controversy around the quality of the data.  The thing that still concerns me is what year to start the analysis.  I chose 1945 because it looked about right; you could say I applied <em>proc eyeball</em> in SAS terminology.  What year is chosen has a dramatic impact on the analysis, changing the lag structure and also leading to insignificant <kbd>p-values</kbd>.</p>
<p>However, we still need to model the original CO2 levels using the alternative Granger causality technique. The process to find the correct number of lags is the same as before, except we do not need to make the data stationary:</p>
<pre>
<strong>    &gt; climateLevels &lt;- window(climate, start = 1946)<br/><br/>    &gt; level.select &lt;- VARselect(climateLevels, lag.max = 12)<br/><br/>    &gt; level.select$selection<br/>    AIC(n) HQ(n) SC(n) FPE(n) <br/>        10     1     1     6 </strong>
</pre>
<p>Let's try the lag 6 structure and see whether we can achieve significance, remembering to add one extra lag to account for the integrated series. A discussion on the technique and why it needs to be done is available at <a href="http://davegiles.blogspot.de/2011/04/testing-for-granger-causality.html">http://davegiles.blogspot.de/2011/04/testing-for-granger-causality.html</a>:</p>
<pre>
<strong>      fit2 &lt;- VAR(climateLevels, p = 7)   <br/>    &gt; serial.test(fit2, type = "PT.asymptotic")<br/><br/>           Portmanteau Test (asymptotic)<br/><br/>    data: Residuals of VAR object fit2<br/>    Chi-squared = 35.161, df = 36, p-value = 0.5083</strong>
</pre>
<p>Now, to determine Granger causality for X causing Y, you conduct a Wald test, where the coefficients of X and only X are 0 in the equation to predict Y, remembering to not include the extra coefficients that account for integration in the test.</p>
<p>The Wald test in R is available in the <kbd>aod</kbd> package we've already loaded. We need to specify the coefficients of the full model, its variance-covariance matrix, and the coefficients of the causative variable.</p>
<div class="packt_tip">The coefficients for Temp that we need to test in the VAR object consist of a range of even numbers from 2 to 12, while the coefficients for CO2 are odd from 1 to 11. Instead of using c(2, 4, 6, and so on) in our function, let's create an object with base R's <kbd>seq()</kbd> function.</div>
<p>First, let's see how CO2 does Granger causing temperature:</p>
<pre>
<strong>    &gt; CO2terms &lt;- seq(1, 11, 2)<br/><br/>    &gt; Tempterms &lt;- seq(2, 12, 2)</strong>
</pre>
<p>We are now ready to run the <kbd>wald</kbd> test, described in the following code:</p>
<pre>
<strong>    &gt; wald.test(b = coef(fit2$varresult$Temp),<br/>    Sigma = vcov(fit2$varresult$Temp),<br/>    Terms = c(CO2terms))<br/>       Wald test:<br/>       ----------<br/><br/>    Chi-squared test:<br/>    X2 = 11.5, df = 6, P(&gt; X2) = 0.074</strong>
</pre>
<p>How about that? We are close to the magical 0.05 <kbd>p-value</kbd>. Let's test the other direction causality with the following code:</p>
<pre>
<strong>    &gt; wald.test(b = coef(fit2$varresult$CO2),<br/>    Sigma = vcov(fit2$varresult$CO2),<br/>    Terms = c(Tempterms))<br/>       Wald test:<br/>       ----------<br/><br/>    Chi-squared test:<br/>    X2 = 3.9, df = 6, P(&gt; X2) = 0.69</strong>
</pre>
<p>The last thing to show here is how to use a vector autoregression to produce a forecast. A <kbd>predict</kbd> function is available, so let's <kbd>autoplot()</kbd> it for a 25-year period and see what happens:</p>
<pre>
    <strong>&gt; autoplot(predict(fit2, n.ahead = 25, ci = 0.95))</strong>
</pre>
<div class="CDPAlignCenter CDPAlign"><img height="235" width="441" class="image-border" src="assets/image_11_19.png"/></div>
<p>It seems dark days lie ahead, perhaps, to coin a phrase, "Winter is Coming" from the popular TV series Game of Thrones. Fine by me as my investment and savings plan for a long time has consisted of canned goods and ammunition. What else am I supposed to do, ride a horse to work? I'll do that the day Al Gore does. In the meantime, I am going to work on my suntan.</p>
<p> If nothing else, I hope it has stimulated your thinking on how to apply the technique to your own real-world problems or maybe even to examine the climate change data in more detail. There should be a high bar when it comes to demonstrating causality, and Granger causality is a great tool for assisting in that endeavor.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, the goal was to discuss how important the element of time is in the field of machine learning and analytics, to identify the common traps when analyzing the time series, and demonstrate the techniques and methods to work around these traps. We explored both the univariate and bivariate time series analyses for global temperature anomalies and human carbon dioxide emissions. Additionally, we looked at Granger causality to determine whether we can say, statistically speaking, that atmospheric CO2 levels cause surface temperature anomalies. We discovered that the p-values are higher than 0.05 but less than 0.10 for Granger causality from CO2 to temperature. It does show that Granger causality is an effective tool in investigating causality in machine learning problems. In the next chapter, we will shift gears and take a look at how to apply learning methods to textual data.</p>
<p>Additionally, keep in mind that in time series analysis, we just skimmed the surface. I encourage you to explore other techniques around changepoint detection, decomposition of time series, nonlinear forecasting, and many others. Although not usually considered part of the machine learning toolbox, I believe you will find it an invaluable addition to yours.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>