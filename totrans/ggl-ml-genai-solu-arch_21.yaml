- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: 'Bringing It All Together: Building ML Solutions with Google Cloud and Vertex
    AI'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将一切整合：使用 Google Cloud 和 Vertex AI 构建机器学习解决方案
- en: You’ve finally made it! This is the last chapter in our book. In this chapter,
    we will bring together the topics we’ve learned in this book by building a couple
    of example solutions that combine multiple concepts and Google Cloud services
    we discussed throughout the book. This chapter focuses mainly on building solutions,
    so it will be short on textual content, and the primary activities will be outlined
    in the Jupyter Notebook files accompanying the chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你终于做到了！这是我们书籍的最后一章。在本章中，我们将通过构建几个示例解决方案来整合本书中学到的主题，这些解决方案结合了我们在整本书中讨论的多个概念和
    Google Cloud 服务。本章主要关注构建解决方案，因此文本内容将相对较少，主要活动将在章节附带的 Jupyter Notebook 文件中概述。
- en: 'We will build two solutions in the chapter: the first will be a deeper dive
    into **retrieval-augmented generation** (**RAG**), and the second will be an end-to-end
    solution that combines traditional **machine learning** (**ML**), MLOps, and generative
    AI.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将构建两个解决方案：第一个将更深入地探讨**检索增强生成**（**RAG**），第二个将是一个端到端解决方案，它结合了传统的**机器学习**（**ML**）、MLOps
    和生成式 AI。
- en: 'The topics in this chapter are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主题如下：
- en: Building a RAG implementation piece by piece
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐步构建 RAG 实现组件
- en: An example business use case and reference architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个示例商业用例和参考架构
- en: Building and implementing the use case
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和实施用例
- en: Recap and next steps
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾与下一步
- en: Before we dive into the architectural details, we need to perform a quick prerequisite
    step to set up the required environment settings in our Google Cloud project.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨架构细节之前，我们需要执行一个快速先决步骤，以在我们的 Google Cloud 项目中设置所需的环境设置。
- en: Prerequisites
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先决条件
- en: This section describes the steps we need to perform to set up our environment
    for the implementation steps later in this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了我们需要执行的步骤，以便为本章后面的实施步骤设置我们的环境。
- en: Note
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146) of this book, we created
    a local directory in our Cloud Shell environment and cloned our GitHub repository
    into that directory. If you did not perform those steps, please reference those
    instructions now, in the section named *Create a directory and clone our GitHub
    repository* in [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的[*第 4 章*](B18143_04.xhtml#_idTextAnchor146)中，我们在 Cloud Shell 环境中创建了一个本地目录，并将我们的
    GitHub 仓库克隆到该目录中。如果您没有执行这些步骤，请现在参考名为 *创建目录和克隆我们的 GitHub 仓库* 的[*第 4 章*](B18143_04.xhtml#_idTextAnchor146)中的说明。
- en: When you have ensured that the GitHub repository for this book has been cloned
    into the local directory in your Cloud Shell environment, continue with the prerequisite
    steps in this section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当您确保本书的 GitHub 仓库已克隆到您的 Cloud Shell 环境中的本地目录后，继续执行本节中的先决步骤。
- en: Using the Google Cloud Shell
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Google Cloud Shell
- en: We will use the Google Cloud Shell to perform the activities in this section.
    You can access it by clicking the Cloud Shell symbol in the top-right corner of
    the Google Cloud console screen, as shown in *Figure 18**.1*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Google Cloud Shell 来执行本节的活动。您可以通过点击 Google Cloud 控制台屏幕右上角的 Cloud Shell
    图标来访问它，如*图 18.1*所示。
- en: '![Figure 18.1: Activating the Cloud Shell](img/B18143_18_1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.1：激活 Cloud Shell](img/B18143_18_1.jpg)'
- en: 'Figure 18.1: Activating the Cloud Shell'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.1：激活 Cloud Shell
- en: 'Open the Google Cloud Shell as shown in *Figure 18**.1*, and perform the following
    steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 18.1*所示打开 Google Cloud Shell，并执行以下步骤：
- en: 'Change the directory to the location at which the code for this chapter is
    stored:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目录更改为存储本章代码的位置：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Run the `chapter-18-prereqs.sh` script:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `chapter-18-prereqs.sh` 脚本：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you are prompted to authorize the Cloud Shell, click **Authorize**.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您被提示授权 Cloud Shell，请点击**授权**。
- en: 'The script performs the following steps on our behalf:'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该脚本代表我们执行以下步骤：
- en: Grant the **Eventarc Event Receiver** IAM role to our project’s default Compute
    Engine service account (our Jupyter Notebook, Cloud Functions, and other components
    use this service account). If you want to use a service account other than the
    default Compute Engine service account, you would need to grant the **Eventarc
    Event Receiver** IAM role to that service account instead.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 授予我们的项目默认 Compute Engine 服务帐户（我们的 Jupyter Notebook、Cloud Functions 和其他组件使用此服务帐户）**Eventarc
    事件接收器** IAM 角色。如果您想使用除默认 Compute Engine 服务帐户之外的其他服务帐户，则需要将**Eventarc 事件接收器** IAM
    角色授予该服务帐户。
- en: Grant the **Service Account Token Creator** IAM role to our project’s Pub/Sub
    service agent. This is required for when we use Pub/Sub later in this chapter.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 授予我们的项目 Pub/Sub 服务代理的**服务账户令牌创建者** IAM 角色。这在我们在本章后面使用 Pub/Sub 时是必需的。
- en: Now that the prerequisites have been completed, we can continue with the main
    content of the chapter. The first solution we will focus on is building a RAG
    implementation piece by piece.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有先决条件都已完成，我们可以继续本章的主要内容。我们将首先关注的是逐步构建一个 RAG 实现。
- en: Building a RAG implementation piece by piece
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逐步构建 RAG 实现
- en: In [*Chapter 17*](B18143_17.xhtml#_idTextAnchor408), we implemented a RAG solution
    using Vertex AI Search. I explained that Vertex AI Search makes the process very
    easy for us because it abstracts away the steps in the process, such as chunking
    and embedding our content, and it performs all of those steps for us behind the
    scenes. There are also popular frameworks, such as LlamaIndex, that help to simplify
    RAG implementations, and Vertex AI has also launched a grounding service (Vertex
    AI Grounding) that you can use to ground responses from a generative model with
    results from Google Search or with your own data (using the aforementioned Vertex
    AI Search solution). In this section, we will dive deeper into the process and
    build a RAG solution, piece by piece. Before we dive into the solution architecture,
    we’ll cover some of the concepts that had been abstracted away in [*Chapter 17*](B18143_17.xhtml#_idTextAnchor408),
    most notably regarding **tokens** and **chunks**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第17章*](B18143_17.xhtml#_idTextAnchor408)中，我们使用 Vertex AI Search 实现了一个 RAG
    解决方案。我解释说，Vertex AI Search 使整个过程变得非常简单，因为它抽象掉了过程中的步骤，例如对内容进行块化和嵌入，并在幕后为我们执行所有这些步骤。还有一些流行的框架，如
    LlamaIndex，有助于简化 RAG 实现，Vertex AI 还推出了一个基础服务（Vertex AI Grounding），您可以使用它将来自生成模型的响应与
    Google 搜索的结果或您自己的数据（使用上述 Vertex AI Search 解决方案）进行关联。在本节中，我们将更深入地探讨这个过程，并逐步构建一个
    RAG 解决方案。在我们深入解决方案架构之前，我们将介绍一些在[*第17章*](B18143_17.xhtml#_idTextAnchor408)中抽象掉的概念，特别是关于**标记**和**块**的概念。
- en: Tokens and chunks
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记和块
- en: In this section, we’ll discuss the concepts of tokens and chunks, beginning
    with tokens.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论标记和块的概念，从标记开始。
- en: Tokens
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记
- en: LLMs generally work with tokens rather than words. For example, when dealing
    with text, a token often represents subsections of words, and tokenization can
    be done in different ways, such as breaking text up by characters, or using subword-based
    tokenization (the word “unbelievable” could be split into subwords such as “un,”
    “believe,” and “able”).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 通常与标记而不是单词一起工作。例如，在处理文本时，标记通常表示单词的子部分，标记化可以以不同的方式进行，例如通过字符分割文本，或使用基于子词的标记化（例如，“unbelievable”这个词可以分解成子词如“un”，“believe”，“able”）。
- en: The exact size and definition of a token can vary based on different tokenization
    methods, models, languages, and other factors, but a general rule of thumb for
    English text using subword tokenizers is around four characters per token on average.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 标记的确切大小和定义会根据不同的标记化方法、模型、语言和其他因素而有所不同，但使用子词标记化器对英文文本进行标记时，平均每个标记大约有四个字符。
- en: Next, let’s discuss the concept of chunks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一下“块”的概念。
- en: Chunks
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 块
- en: When creating embeddings, we usually break a document into chunks and then create
    embeddings of those chunks. Again, this can be done in different ways, using different
    tools. In the Jupyter Notebook file accompanying this chapter, we use Google Cloud
    Document AI to break our documents into chunks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建嵌入时，我们通常将文档分解成块，然后为这些块创建嵌入。同样，这可以通过不同的方式完成，使用不同的工具。在本章附带的 Jupyter Notebook
    文件中，我们使用 Google Cloud Document AI 将我们的文档分解成块。
- en: For this purpose, one of the parameters we need to specify for our Document
    AI processor is the chunk size to use, which is measured (in this case) by the
    number of tokens per chunk. You may need to experiment with the value of this
    parameter to find the chunk size that works best for your use case (e.g., based
    on the length and structure of the document sections). We generally want our chunks
    to capture some level of semantic granularity, but there are trade-offs in terms
    of this granularity. For example, smaller chunks can capture more granular semantic
    context and provide more precise search results but can be less efficient (computationally)
    to process. We also need to ensure chunk sizes are within the input length limits
    of the embedding model we’re using in order to avoid possible truncation. A good
    practice is to start with a moderate chunk size and adjust it based on how well
    it fits our needs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，我们需要为我们的Document AI处理器指定的一个参数是使用的块大小，这在此情况下是通过每个块的标记数来衡量的。你可能需要尝试这个参数的值，以找到最适合你用例的块大小（例如，基于文档部分的长度和结构）。我们通常希望我们的块能够捕捉到一定程度的语义粒度，但在这种粒度方面存在权衡。例如，较小的块可以捕捉到更细粒度的语义上下文并提供更精确的搜索结果，但可能在处理上效率较低。我们还需要确保块大小在所使用的嵌入模型的输入长度限制内，以避免可能的截断。一个好的做法是从一个适中的块大小开始，并根据它如何满足我们的需求进行调整。
- en: Fortunately, Document AI can automatically handle chunking based on layout,
    even if you don’t specify a preconfigured chunk size, which can be helpful if
    you don’t know what chunk size to use.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Document AI可以自动根据布局处理块分割，即使你没有指定预配置的块大小，这在你不知道使用什么块大小时可能很有帮助。
- en: Now that we’ve covered those concepts, let’s dive into the architecture of the
    RAG implementation we will build in this chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了这些概念，让我们深入了解本章我们将构建的RAG实现的架构。
- en: Example RAG solution architecture
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例RAG解决方案架构
- en: In this section, we will review the architecture of our example RAG implementation.
    The proposed solution architecture is shown in *Figure 18**.2*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾我们示例RAG实现的架构。所提出的解决方案架构显示在*图18.2*中。
- en: '![Figure 18.2: RAG solution piece by piece on Google Cloud](img/B18143_18_2.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图18.2：在Google Cloud上逐步展示的RAG解决方案](img/B18143_18_2.jpg)'
- en: 'Figure 18.2: RAG solution piece by piece on Google Cloud'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.2：在Google Cloud上逐步展示的RAG解决方案
- en: You can see that between some of the steps depicted in *Figure 18**.2*, **Google
    Cloud Storage** (**GCS**) is used to store the inputs and outputs of each step,
    but those intermediary processes are omitted to make the diagram more readable.
    Also, when we implement this solution in the Jupyter Notebook that accompanies
    this chapter, the notebook is the application/user that coordinates each of the
    steps in the overall process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在*图18.2*中的一些步骤之间，**Google Cloud Storage**（**GCS**）被用来存储每个步骤的输入和输出，但这些中间过程被省略以使图表更易于阅读。此外，当我们在这个章节伴随的Jupyter
    Notebook中实现此解决方案时，笔记本是协调整个过程中每个步骤的应用程序/用户。
- en: 'The steps in the solution depicted in *Figure 18**.2* are described as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.2*中展示的解决方案步骤描述如下：'
- en: Our documents, which are stored in GCS, are sent to Google Cloud Document AI
    for chunking. As the name suggests, the chunking process breaks the documents
    into chunks, which are smaller sections of the document. This is required in order
    to create standard-sized chunks that serve as inputs to the embedding process
    that comes in the next step. The size of the chunks is configurable in Document
    AI, and further details on this process are described later.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们存储在GCS中的文档被发送到Google Cloud Document AI进行块分割。正如其名所示，块分割过程将文档分割成块，这些块是文档的较小部分。这是为了创建标准大小的块，作为下一步嵌入过程的输入。块的大小在Document
    AI中是可配置的，关于此过程的更多细节将在稍后描述。
- en: The chunks are then sent to the Google Cloud text embedding LLM to create embeddings
    for the chunks. The resulting embeddings are stored in GCS, alongside their respective
    chunks (this step is omitted from the diagram).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些块随后被发送到Google Cloud文本嵌入LLM以创建块的嵌入。生成的嵌入存储在GCS中，与各自的块一起（此步骤在图中省略）。
- en: We create a Vertex AI Vector Search index, and the embeddings are ingested from
    GCS to the Vertex AI Vector Search index (the GCS intermediary step is omitted
    from the diagram).
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个Vertex AI Vector Search索引，并将嵌入从GCS摄入到Vertex AI Vector Search索引中（图中省略了GCS中间步骤）。
- en: Next, the application/user asks a question that relates to the contents of our
    documents. The question is sent as a query to the Google Cloud text embedding
    LLM to be embedded/vectorized.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，应用程序/用户提出一个问题，该问题与我们的文档内容相关。该问题作为查询发送到 Google Cloud 文本嵌入 LLM 进行嵌入/向量化。
- en: The vectorized query is then used as input in a request to Vertex AI Vector
    Search, which searches our index to find similar embeddings. Remember that the
    embeddings represent an element of semantic meaning, so similar embeddings have
    similar meanings. This is how we can perform a semantic search to find embeddings
    that are similar to our query.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将向量化的查询用作请求 Vertex AI 向量搜索的输入，以搜索我们的索引以找到相似的嵌入。记住，嵌入代表语义意义的一个元素，因此相似的嵌入具有相似的意义。这就是我们如何执行语义搜索以找到与我们的查询相似的嵌入。
- en: Next, we take the embeddings returned from our Vertex AI Vector Search query
    and find the chunks in GCS that relate to those embeddings (remember that *Step
    2* in our solution created a stored association of chunks and embeddings).
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从 Vertex AI 向量搜索查询返回的嵌入中找到与这些嵌入相关的 GCS 中的块（记住，我们解决方案中的**步骤 2**创建了一个块和嵌入的存储关联）。
- en: Now, it’s finally time to send a prompt to Gemini. The retrieved document chunks
    from *Step 6* serve as the context for the prompt. This helps Gemini respond to
    our prompt based on the relevant content from our documents and not just from
    its pre-trained knowledge.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，终于到了向 Gemini 发送提示的时候了。从**步骤 6**检索到的文档块作为提示的上下文。这有助于 Gemini 根据我们文档中的相关内容来响应我们的提示，而不仅仅是基于其预训练的知识。
- en: Gemini responds to the prompt.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Gemini 对提示做出响应。
- en: Now that we’ve walked through the steps in the process, let’s go ahead and implement
    this solution!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经走过了这个过程中的步骤，我们就来实施这个解决方案吧！
- en: Building the RAG implementation on Google Cloud
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Google Cloud 上构建 RAG 实现
- en: To build our solution, open the Jupyter Notebook file that accompanies this
    chapter, and perform the activities described in that file. We can use the same
    Vertex AI Workbench instance that we created in [*Chapter 14*](B18143_14.xhtml#_idTextAnchor348)
    for this purpose. Please open JupyterLab on that notebook instance. In the directory
    explorer on the left side of the screen, navigate to the `Chapter-18` directory
    and open the `rag.ipynb` notebook file. You can choose Python (local) as the kernel.
    Again, you can run each cell in the notebook by selecting the cell and pressing
    *Shift* + *Enter* on your keyboard. In addition to the relevant code, the notebook
    file contains markdown text that describes what the code is doing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建我们的解决方案，请打开与本章一起提供的 Jupyter Notebook 文件，并执行该文件中描述的活动。我们可以使用在[*第 14 章*](B18143_14.xhtml#_idTextAnchor348)中创建的相同的
    Vertex AI Workbench 实例来完成此目的。请在笔记本实例上打开 JupyterLab。在屏幕左侧的目录浏览器中，导航到 `Chapter-18`
    目录并打开 `rag.ipynb` 笔记本文件。您可以选择 Python (local) 作为内核。同样，您可以通过选择单元格并在键盘上按 *Shift*
    + *Enter* 来运行笔记本中的每个单元格。除了相关代码外，笔记本文件还包含描述代码正在做什么的 markdown 文本。
- en: When you have completed the steps in the notebook, you will have officially
    built your own RAG solution—nice work!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当您完成笔记本中的步骤后，您将正式构建自己的 RAG 解决方案——干得好！
- en: Note that in this case, we are using documents stored in GCS as our source of
    truth, but we could also use other data, such as data stored in BigQuery.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，我们使用存储在 GCS 中的文档作为我们的真相来源，但我们也可以使用其他数据，例如存储在 BigQuery 中的数据。
- en: Document citation
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 文档引用
- en: 'For congruity and comparability, we will use one of the same reference documents
    that we used in our RAG implementation in [*Chapter 17*](B18143_17.xhtml#_idTextAnchor408),
    cited as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持一致性和可比性，我们将使用我们在[*第 17 章*](B18143_17.xhtml#_idTextAnchor408)中 RAG 实现中使用的相同参考文档之一，如下所示：
- en: '*Hila Zelicha, Jieping Yang, Susanne M Henning, Jianjun Huang, Ru-Po Lee, Gail
    Thames, Edward H Livingston, David Heber, and Zhaoping Li, 2024\. Effect of cinnamon
    spice on continuously monitored glycemic response in adults with prediabetes:
    a 4-week randomized controlled crossover* *trial. DOI:https://doi.org/10.1016/j.ajcnut.2024.01.008*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*Hila Zelicha, Jieping Yang, Susanne M Henning, Jianjun Huang, Ru-Po Lee, Gail
    Thames, Edward H Livingston, David Heber, and Zhaoping Li, 2024\. Effect of cinnamon
    spice on continuously monitored glycemic response in adults with prediabetes:
    a 4-week randomized controlled crossover* *trial. DOI:https://doi.org/10.1016/j.ajcnut.2024.01.008*'
- en: Next, we will build a broader solution that brings together many topics from
    throughout this entire book. I’ll begin by explaining the use case.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个更广泛的解决方案，将本书中许多主题汇集在一起。我将首先解释用例。
- en: An example business use case and reference architecture
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个示例商业用例和参考架构
- en: 'We will focus on a **computer vision** (**CV**) use case to identify objects
    in images. Extending the CV use case we implemented in [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371),
    we will build our CV model via an MLOps pipeline that incorporates the majority
    of the main topics from the earlier chapters in this book, such as the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注一个**计算机视觉**（**CV**）用例，用于识别图像中的对象。扩展我们在[*第15章*](B18143_15.xhtml#_idTextAnchor371)中实现的CV用例，我们将通过一个MLOps管道构建我们的CV模型，该管道结合了本书早期章节中的大多数主要主题，例如以下内容：
- en: Data preparation, cleaning, and transformation
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备、清理和转换
- en: Model training, deployment, inference, and evaluation
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练、部署、推理和评估
- en: The novel approach we are using here is to utilize generative AI to generate
    the data that will be used to test and evaluate our model. Before we start building,
    I’ll present additional context on why I chose to include this use case as an
    example.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里采用的新颖方法是利用生成式AI生成用于测试和评估我们模型的数据。在我们开始构建之前，我将介绍为什么我选择将这个用例作为示例的额外背景。
- en: Additional background on our use case
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于我们用例的额外背景
- en: 'Throughout this book, one topic has come up more frequently than any other:
    data is often the most important factor in training high-quality ML models. Without
    adequate data, your ML project will not succeed. We’ve discussed that getting
    access to sufficient data often proves difficult, and this challenge continues
    to become ever more prevalent as the average size of models increases (today’s
    models with trillions of parameters require enormous amounts of data). We also
    discussed that there’s a limited amount of data in the world, but one mechanism
    that could help address this challenge is the creation of synthetic data and the
    use of generative AI models to generate new data. The use case I outline in this
    section combines all of the steps in the traditional ML model development life
    cycle and extends the process to include generating data that can be used to test
    our model. In the next subsection, I’ll describe the architecture for this solution.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，有一个主题比其他任何主题出现得更频繁：数据往往是训练高质量机器学习模型最重要的因素。没有足够的数据，你的机器学习项目将不会成功。我们讨论过获取足够的数据往往很困难，而且随着模型平均规模的增加，这个挑战正变得越来越普遍（今天的模型拥有万亿级别的参数，需要巨大的数据量）。我们还讨论过世界上数据是有限的，但一个可以帮助解决这个挑战的机制是创建合成数据和使用生成式AI模型来生成新数据。本节中我概述的使用案例结合了传统机器学习模型开发生命周期中的所有步骤，并将过程扩展到包括生成可用于测试我们模型的测试数据。在下一小节中，我将描述这个解决方案的架构。
- en: The reference architecture
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考架构
- en: This section outlines the components of the solution architecture we will build.
    In addition to Vertex AI components, the reference architecture brings other Google
    Cloud services into scope to build the overall solution, such as Google Cloud
    Functions, Pub/Sub, Eventarc, and Imagen. We originally introduced and described
    all of those Google Cloud services in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059)
    of this book. If you are not familiar with them, I recommend refreshing your knowledge
    from that chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了我们将要构建的解决方案架构的组件。除了Vertex AI组件外，参考架构还将其他Google Cloud服务纳入范围，以构建整体解决方案，例如Google
    Cloud Functions、Pub/Sub、Eventarc和Imagen。我们最初在本书的[*第3章*](B18143_03.xhtml#_idTextAnchor059)中介绍了并描述了所有这些Google
    Cloud服务。如果您不熟悉它们，我建议您从那一章中复习相关知识。
- en: Our solution begins with an MLOps pipeline that will train and deploy our CV
    model, which I will describe in detail shortly. The resulting model is then used
    in a broader architecture to implement the overall solution. I’ll begin by outlining
    the MLOps pipeline.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决方案的起点是一个MLOps管道，该管道将训练和部署我们的CV模型，我将在稍后详细描述。生成的模型随后被用于更广泛的架构中，以实现整体解决方案。我将首先概述MLOps管道。
- en: A note on the dataset (CIFAR-10)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据集（CIFAR-10）的说明
- en: 'In our model training and evaluation code, we’re using something called the
    **CIFAR-10** (**Canadian Institute For Advanced Research, 10 classes**) dataset,
    which is a commonly used benchmark dataset for CV and image classification. It
    contains 60,000 color images belonging to ten different classes: Airplane, Automobile,
    Bird, Cat, Deer, Dog, Frog, Horse, Ship, and Truck. Each image has a fixed size
    of 32x32 pixels, and the images are stored in the RGB color space, meaning each
    pixel is represented by three values corresponding to the red, green, and blue
    color channels. It’s important to keep these details in mind when we want to send
    generated data to our model at inference time.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型训练和评估代码中，我们使用一个叫做**CIFAR-10**（**加拿大高级研究研究所，10个类别**）的数据集，这是一个CV和图像分类中常用的基准数据集。它包含属于十个不同类别的60,000个彩色图像：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。每个图像具有固定的32x32像素大小，图像存储在RGB颜色空间中，这意味着每个像素由三个值表示，分别对应红色、绿色和蓝色颜色通道。当我们想在推理时将生成数据发送到我们的模型时，这些细节非常重要。
- en: The CIFAR-10 dataset is so commonly used for benchmarking CV models that it
    is included in the built-in datasets module in Tensorflow/Keras. This means that
    we can use it in our code by simply importing that module rather than needing
    to download the data from an external source.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10数据集因其常用于CV模型的基准测试而被广泛使用，它包含在Tensorflow/Keras的内置数据集模块中。这意味着我们可以通过简单地导入该模块来使用它，而无需从外部源下载数据。
- en: Our MLOps pipeline
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的MLOps流水线
- en: This section will walk through the steps implemented in our MLOps pipeline,
    as depicted in *Figure 18**.3*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍我们MLOps流水线中实现的步骤，如图*图18.3*所示。
- en: '![Figure 18.3: MLOps pipeline for CV model](img/B18143_18_3.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图18.3：CV模型的MLOps流水线](img/B18143_18_3.jpg)'
- en: 'Figure 18.3: MLOps pipeline for CV model'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.3：CV模型的MLOps流水线
- en: 'In *Figure 18**.3*, the process works as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图18.3*中，过程如下所示：
- en: The first step in our pipeline—the model training step—is invoked. While the
    MLOps pipeline we built for our tabular Titanic dataset in [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288)
    started with distinct data preprocessing steps using Serverless Spark in Dataproc,
    in our pipeline in this chapter, the data ingestion and preparation steps are
    handled directly in the code of our model training job. Also, as noted, in this
    case, we are using the built-in CIFAR-10 image dataset in Tensorflow/Keras rather
    than fetching a dataset from an external source. Vertex AI Pipelines starts the
    model training process by submitting a model training job to the Vertex AI training
    service.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们流水线中的第一步——模型训练步骤被调用。虽然我们为表格形式的Titanic数据集在[*第11章*](B18143_11.xhtml#_idTextAnchor288)中构建的MLOps流水线从使用Serverless
    Spark在Dataproc中的独立数据预处理步骤开始，但在本章的流水线中，数据摄取和准备步骤直接在我们的模型训练作业代码中处理。此外，正如所注，在这种情况下，我们使用Tensorflow/Keras内置的CIFAR-10图像数据集，而不是从外部源获取数据集。Vertex
    AI流水线通过向Vertex AI训练服务提交模型训练作业来启动模型训练过程。
- en: In order to execute our custom training job, the Vertex AI training service
    fetches our custom Docker container from Google Artifact Registry.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了执行我们的自定义训练作业，Vertex AI训练服务从Google Artifact Registry获取我们的自定义Docker容器。
- en: When our model has been trained, the trained model artifacts are saved in GCS.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们的模型训练完成后，训练好的模型工件将被保存在GCS中。
- en: The model training job status is complete.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型训练作业状态已完成。
- en: The next step in our pipeline—the model import step—is invoked. This is an intermediate
    step that prepares the model metadata to be referenced in later components of
    our pipeline. The relevant metadata in this case consists of the location of the
    model artifacts in GCS and the specification of the Docker container image in
    Google Artifact Registry that will be used to serve our model.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们流水线中的下一步——模型导入步骤被调用。这是一个中间步骤，它准备模型元数据，以便在流水线的后续组件中引用。在这种情况下，相关的元数据包括模型工件在GCS中的位置以及用于服务我们的模型的Google
    Artifact Registry中Docker容器镜像的规范。
- en: The next step in our pipeline—the model upload step—is invoked. This step references
    the metadata from the model import step.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们流水线中的下一步——模型上传步骤被调用。这一步骤引用了模型导入步骤中的元数据。
- en: The model metadata is used to register the model in the Vertex AI Model Registry.
    This makes it easy to deploy our model for serving traffic in Vertex AI.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型元数据被用于在Vertex AI模型注册表中注册模型。这使得在Vertex AI中部署我们的模型以处理服务流量变得容易。
- en: The model upload job status is complete.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型上传作业状态已完成。
- en: The next step in our pipeline—the endpoint creation step—is invoked.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们流水线中的下一步——端点创建步骤被调用。
- en: An endpoint is created in the Vertex AI prediction service. This endpoint will
    be used to host our model.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Vertex AI预测服务中创建了一个端点。这个端点将用于托管我们的模型。
- en: The endpoint creation job status is complete.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 端点创建作业状态已完成。
- en: The next step in our pipeline—the model deployment step—is invoked.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们流程中的下一步——模型部署步骤被调用。
- en: Our model is deployed to our endpoint in the Vertex AI prediction service. This
    step references the metadata of the endpoint that has just been created by our
    pipeline, as well as the metadata of our model in the Vertex AI Model Registry.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的模型已部署到Vertex AI预测服务中的端点。这一步骤引用了我们流程刚刚创建的端点元数据，以及Vertex AI模型注册表中我们的模型元数据。
- en: The model deployment job status is complete.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型部署作业状态已完成。
- en: Now that we’ve walked through the steps in our model training and deployment
    pipeline, I will begin to outline the broader solution architecture we will build
    in this chapter, of which our MLOps pipeline is just a subset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经走过了模型训练和部署流程的步骤，我将开始概述本章中我们将构建的更广泛的解决方案架构，其中我们的MLOps流程只是其中的一部分。
- en: The end-to-end solution
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端到端解决方案
- en: We will build an event-driven architecture, which is a popular pattern for developing
    serverless solutions intended to be implemented automatically in response to an
    event or a set of events. This is the end-to-end solution that incorporates the
    majority of the topics we’ve covered throughout this book, as depicted in *Figure
    18**.4*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个事件驱动架构，这是开发旨在自动响应事件或一系列事件的无服务器解决方案的流行模式。这是包含本书中我们讨论的大多数主题的端到端解决方案，如**图18.4**所示。
- en: '![Figure 18.4: The end-to-end solution](img/B18143_18_4.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图18.4：端到端解决方案](img/B18143_18_4.jpg)'
- en: 'Figure 18.4: The end-to-end solution'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4：端到端解决方案
- en: In *Figure 18**.4*, our MLOps pipeline is simplified in the top-left corner
    of the diagram. It still implements all of the same steps we discussed in the
    previous section, but the diagram is simplified so we can focus our discussion
    on the broader, end-to-end solution. In this context, the MLOps pipeline is represented
    as a single step in the overall process.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图18.4**中，我们的MLOps流程在图的上左角被简化。它仍然实现了我们在上一节中讨论的所有相同步骤，但图被简化了，这样我们就可以将讨论的重点放在更广泛的端到端解决方案上。在这种情况下，MLOps流程在整体过程中被表示为一个单独的步骤。
- en: Notice that Eventarc features prominently in our solution, as it is the primary
    mechanism for orchestrating the steps in our event-based architecture. In the
    Jupyter Notebook file that accompanies this chapter, I will explain in more detail
    exactly how Eventarc is being configured behind the scenes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Eventarc在我们的解决方案中占有一席之地，因为它是编排基于事件架构中步骤的主要机制。在伴随本章的Jupyter Notebook文件中，我将更详细地解释Eventarc在幕后是如何配置的。
- en: 'The following set of steps describes the architecture implemented in *Figure
    18**.4*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了*图18.4*中实现的架构：
- en: Our MLOps pipeline trains and deploys our CV model.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的MLOps流程训练并部署我们的CV模型。
- en: When the MLOps pipeline completes, it publishes a message to a Pub/Sub topic
    we created for that purpose.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当MLOps流程完成时，它将消息发布到我们为此目的创建的Pub/Sub主题。
- en: Eventarc detects that a message has been published to the Pub/Sub topic.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Eventarc检测到已向Pub/Sub主题发布了一条消息。
- en: Eventarc triggers the Cloud Function we’ve created to generate an image.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Eventarc触发我们创建的用于生成图像的Cloud Function。
- en: The code in our image generation function makes a call to the Imagen API with
    a prompt to generate an image containing one of the types of objects our model
    was trained to recognize (a type of object supported by the CIFAR-10 dataset).
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们图像生成函数中的代码调用Imagen API，并带有提示以生成包含模型训练以识别的一种对象类型的图像（CIFAR-10数据集支持的对象类型）。
- en: Imagen generates an image and returns it to our function.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Imagen生成一个图像并将其返回到我们的函数中。
- en: Our function stores the new image in GCS.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的功能将新图像存储在GCS中。
- en: GCS emits an event indicating that a new object has been uploaded to our bucket.
    Eventarc detects this event.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GCS发出一个事件，指示已将新对象上传到我们的存储桶。Eventarc检测到这个事件。
- en: Eventarc invokes our next Cloud Function and passes the GCS event metadata to
    our function. This metadata includes details such as the identifiers of the bucket
    and the object in question.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Eventarc调用我们的下一个Cloud Function，并将GCS事件元数据传递给我们的函数。这些元数据包括诸如存储桶和对象的标识符等详细信息。
- en: Our prediction function takes the details regarding the bucket and the object
    in question from the event metadata and uses those details to fetch the newly
    created object (i.e., the newly generated image from Imagen).
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的预测函数从事件元数据中获取有关存储桶和所涉及对象的详细信息，并使用这些详细信息来获取新创建的对象（即，从 Imagen 生成的新的图像）。
- en: Our prediction function then performs some preprocessing on the image to transform
    it into a format that is expected by our model (i.e., similar to the format of
    the CIFAR-10 data the model was trained on). Our function then sends the transformed
    data as a prediction request to the Vertex AI endpoint that hosts our model.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们预测函数随后对图像进行一些预处理，将其转换为模型期望的格式（即，类似于模型训练时所使用的 CIFAR-10 数据的格式）。然后，我们的函数将转换后的数据作为预测请求发送到托管我们模型的
    Vertex AI 端点。
- en: Our model predicts what type of object is in the image, and sends a prediction
    response to our Cloud Function.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的模型预测图像中包含的对象类型，并将预测响应发送到我们的 Cloud Function。
- en: Our Cloud Function saves the prediction response in GCS.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的 Cloud Function 将预测响应保存到 GCS。
- en: When the process has been completed, you can view the generated image and the
    resulting prediction from our model in GCS.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当过程完成后，您可以在 GCS 中查看生成的图像和模型的结果预测。
- en: Notice that all of the steps in the solution are implemented automatically and
    without the need to provision any servers. This is a fully serverless, event-driven
    solution architecture.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，解决方案中的所有步骤都是自动实现的，无需配置任何服务器。这是一个完全无服务器、事件驱动的解决方案架构。
- en: An interesting side effect of this solution is that, although the primary intention
    is to test our newly trained model on generated data, this solution could also
    be applied to the inverse use case. That is, if we are confident that our model
    has been trained effectively and provides consistently accurate results, we could
    use it to evaluate the quality of the generated data. For example, if our model
    predicts that the generated data contains a particular type of object with a probability
    of 99.8%, we can interpret this as a reflection of the quality of the generated
    data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案的一个有趣的副作用是，尽管主要目的是在生成数据上测试我们新训练的模型，但此解决方案也可以应用于逆向用例。也就是说，如果我们对我们的模型已经有效训练并提供了持续准确的结果有信心，我们可以使用它来评估生成数据的质量。例如，如果我们的模型预测生成数据包含特定类型的对象，概率为
    99.8%，我们可以将其解释为生成数据质量的反映。
- en: Now that we’ve discussed the various steps in the process, let’s start building
    it!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了过程中的各个步骤，让我们开始构建它！
- en: Building and implementing the use case on Google Cloud
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Google Cloud 上构建和实施用例
- en: To build our solution, open the Jupyter Notebook file that accompanies this
    chapter, and perform the activities described in that file. We can use the same
    Vertex AI Workbench instance that we created in [*Chapter 14*](B18143_14.xhtml#_idTextAnchor348)
    for this purpose. Please open JupyterLab on that notebook instance. In the directory
    explorer on the left side of the screen, navigate to the `Chapter-18` directory
    and open the `end-to-end-mlops-genai.ipynb` notebook file. You can choose Python
    (local) as the kernel. Again, you can run each cell in the notebook by selecting
    the cell and pressing *Shift* + *Enter* on your keyboard. In addition to the relevant
    code, the notebook file contains markdown text that describes what the code is
    doing.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建我们的解决方案，请打开本章附带的 Jupyter Notebook 文件，并执行该文件中描述的活动。我们可以使用在 [*第 14 章*](B18143_14.xhtml#_idTextAnchor348)
    中创建的相同的 Vertex AI Workbench 实例来完成此目的。请在笔记本实例上打开 JupyterLab。在屏幕左侧的目录浏览器中，导航到 `Chapter-18`
    目录并打开 `end-to-end-mlops-genai.ipynb` 笔记本文件。您可以选择 Python (local) 作为内核。同样，您可以通过选择单元格并在键盘上按
    *Shift* + *Enter* 来运行笔记本中的每个单元格。除了相关代码外，笔记本文件还包含描述代码正在做什么的 Markdown 文本。
- en: If you have followed all of the practical steps and successfully built the solution,
    then it’s time to give yourself a good pat on the back. Let’s take a moment to
    reflect on what you have done here. You have successfully built and executed an
    MLOps pipeline that trains and deploys a Keras convolutional neural network implementing
    a CV use case. You have then built a completely serverless, event-driven solution
    architecture using a combination of many Google Cloud services. That is absolutely
    something to be very proud of!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经遵循了所有实际步骤并成功构建了解决方案，那么是时候给自己一个热烈的掌声了。让我们花点时间反思一下你所做的事情。你成功构建并执行了一个MLOps管道，该管道训练和部署了一个实现CV用例的Keras卷积神经网络。然后，你使用多种Google
    Cloud服务构建了一个完全无服务器、事件驱动的解决方案架构。这绝对是一件值得骄傲的事情！
- en: As we approach the end of this chapter and book, let’s summarize what we’ve
    learned and discuss how to keep learning in this space.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们接近本章和本书的结尾时，让我们总结一下我们学到了什么，并讨论如何在这个领域继续学习。
- en: Recap and next steps
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述和下一步
- en: In this chapter, we combined the majority of the important concepts that we’ve
    covered throughout this book and explained how they all tie together. We did this
    by building solutions that incorporated many of the topics and Google Cloud products
    we’ve discussed.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们结合了本书中涵盖的大多数重要概念，并解释了它们是如何相互关联的。我们通过构建包含我们讨论的许多主题和Google Cloud产品的解决方案来实现这一点。
- en: First, you built a RAG implementation, piece by piece, focusing on the combination
    of various generative AI concepts in that process, such as using Google Cloud
    Document AI to break a document into chunks in a manner that preserves the hierarchical
    structure of the original document. This solution also included using the Google
    Cloud text embedding LLM to create embeddings for the document chunks and using
    Vertex AI Vector Search to store and index those embeddings. You used the resulting
    solution to implement a question-answering use case with Gemini, grounding the
    answers in the contents of the document. To do this, you used the Google Cloud
    text embedding LLM to create embeddings of the prompted question and used those
    embeddings to perform a semantic similarity search in Vertex AI Vector Search
    to find similar embeddings relating to specific chunks from the document. Those
    chunks were then provided as context when sending the request to Gemini.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你逐步构建了一个RAG实现，在这个过程中，你专注于结合各种生成式AI概念，例如使用Google Cloud Document AI将文档分割成块，同时保留原始文档的层次结构。这个解决方案还包括使用Google
    Cloud文本嵌入LLM为文档块创建嵌入，并使用Vertex AI Vector Search存储和索引这些嵌入。你使用这个解决方案来实现了一个与Gemini结合的问题回答用例，将答案基于文档内容。为此，你使用Google
    Cloud文本嵌入LLM创建提示问题的嵌入，并使用这些嵌入在Vertex AI Vector Search中执行语义相似度搜索，以找到与文档特定块相关的相似嵌入。然后，将这些块作为上下文提供，当向Gemini发送请求时。
- en: Next, you built a solution that combined many topics from this entire book,
    including all of the steps in a traditional model development life cycle, as well
    as generative AI and broader solution architecture concepts. This resulted in
    a fully automated, serverless, event-driven solution that incorporated many Google
    Cloud services, such as Google Cloud Functions, Pub/Sub, Eventarc, and Imagen.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你构建了一个结合了本书中许多主题的解决方案，包括传统模型开发生命周期的所有步骤，以及生成式AI和更广泛解决方案架构的概念。这导致了一个完全自动化、无服务器、事件驱动的解决方案，其中包含了多个Google
    Cloud服务，如Google Cloud Functions、Pub/Sub、Eventarc和Imagen。
- en: We’ve certainly covered a lot of topics in this book, and the learning journey
    never ends. Let’s wrap up by discussing how you can continue your learning journey
    in the fields of Google Cloud solutions architecture, ML, and generative AI.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中确实涵盖了大量的主题，学习之旅永远不会结束。让我们通过讨论如何在Google Cloud解决方案架构、机器学习和生成式AI领域继续你的学习之旅来结束这一部分。
- en: Next steps
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: 'The technology fields we’ve discussed in this book continue to evolve at an
    accelerating pace, with new frameworks and patterns emerging daily, and generative
    AI is arguably the fastest-evolving field of them all. For example, in addition
    to the popular LangChain framework we’ve discussed in this book, which can also
    be implemented as a managed experience in the Vertex AI Reasoning Engine, extensions
    of those frameworks, such as LangGraph, LangSmith, and many others continue to
    emerge, providing additional functionality and flexibility for building generative
    AI solutions and applications. Due to the accelerating pace of development and
    overall excitement in this space, new learning resources will continue to emerge.
    In addition to the GitHub repository associated with this book, many other example
    repositories provide valuable tutorials on how to implement various patterns.
    Perhaps the most valuable of these is, of course, the official Vertex AI example
    repository, which can be accessed at the following link:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本书所讨论的技术领域正以越来越快的速度发展，每天都有新的框架和模式出现，其中生成式AI可以说是发展最快的领域之一。例如，除了本书中讨论的流行LangChain框架，它还可以在Vertex
    AI推理引擎中实现为托管体验之外，这些框架的扩展，如LangGraph、LangSmith以及许多其他扩展也不断涌现，为构建生成式AI解决方案和应用提供了额外的功能和灵活性。由于这一领域的发展速度不断加快，以及整体上的兴奋情绪，新的学习资源将持续出现。除了与本书相关的GitHub仓库之外，许多其他示例仓库提供了关于如何实现各种模式的宝贵教程。其中最有价值的当然是官方Vertex
    AI示例仓库，您可以通过以下链接访问：
- en: '[https://github.com/GoogleCloudPlatform/vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/GoogleCloudPlatform/vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples)'
- en: 'I also find the information available at [langchain.com](http://langchain.com)
    to be a useful learning resource, and I highly recommend referencing the example
    architecture use cases in the Google Cloud Solutions Center, accessible at the
    following link:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我还发现[langchain.com](http://langchain.com)上提供的信息是一个有用的学习资源，我强烈推荐您参考Google Cloud解决方案中心中的示例架构用例，您可以通过以下链接访问：
- en: '[https://solutions.cloud.google.com/](https://solutions.cloud.google.com/)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://solutions.cloud.google.com/](https://solutions.cloud.google.com/)'
- en: As we wrap up, remember that the practical exercises and notebooks we used throughout
    this book create resources on Google Cloud, which can incur costs. For that reason,
    I recommend reviewing each of the practical steps we performed in this book and
    ensuring that all resources are deleted if you no longer plan to use them. The
    GitHub repository associated with this book contains steps for deleting resources
    such as models, endpoints, and data stores. For other types of resources, please
    refer to the Google Cloud documentation to ensure that the relevant resources
    are deleted.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束之前，请记住，本书中使用的实际练习和笔记本在Google Cloud上创建了资源，这可能会产生费用。因此，我建议您回顾本书中执行的所有实际步骤，并确保如果您不再计划使用它们，则删除所有资源。与本书相关的GitHub仓库包含删除模型、端点和数据存储等资源的方法。对于其他类型的资源，请参阅Google
    Cloud文档，以确保删除相关资源。
- en: I write this final section with a sense of sadness that our journey together
    is coming to an end, and I want to thank you for embarking on this journey with
    me. If you’ve made it this far in the book, then you have gained expert knowledge
    in AI/ML, generative AI, Google Cloud, and solutions architecture, and you now
    know a lot more about all of those topics than most people in the world! Congratulations,
    and well done! I wish you the best of luck in your future AI adventures, and I
    hope you will use your knowledge and skills to achieve wonderful things!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我在撰写这个最后一部分时，感到一种悲伤，因为我们共同的经历即将结束，我想感谢您与我一起踏上这段旅程。如果您已经阅读到本书的这一部分，那么您已经获得了AI/ML、生成式AI、Google
    Cloud和解决方案架构的专家知识，并且您现在对这些主题的了解比世界上大多数人都多！恭喜您，做得好！我祝愿您在未来的AI冒险中一切顺利，并希望您能运用您的知识和技能取得美好的成就！
