- en: '18'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bringing It All Together: Building ML Solutions with Google Cloud and Vertex
    AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’ve finally made it! This is the last chapter in our book. In this chapter,
    we will bring together the topics we’ve learned in this book by building a couple
    of example solutions that combine multiple concepts and Google Cloud services
    we discussed throughout the book. This chapter focuses mainly on building solutions,
    so it will be short on textual content, and the primary activities will be outlined
    in the Jupyter Notebook files accompanying the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will build two solutions in the chapter: the first will be a deeper dive
    into **retrieval-augmented generation** (**RAG**), and the second will be an end-to-end
    solution that combines traditional **machine learning** (**ML**), MLOps, and generative
    AI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a RAG implementation piece by piece
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example business use case and reference architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and implementing the use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recap and next steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we dive into the architectural details, we need to perform a quick prerequisite
    step to set up the required environment settings in our Google Cloud project.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section describes the steps we need to perform to set up our environment
    for the implementation steps later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146) of this book, we created
    a local directory in our Cloud Shell environment and cloned our GitHub repository
    into that directory. If you did not perform those steps, please reference those
    instructions now, in the section named *Create a directory and clone our GitHub
    repository* in [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146).
  prefs: []
  type: TYPE_NORMAL
- en: When you have ensured that the GitHub repository for this book has been cloned
    into the local directory in your Cloud Shell environment, continue with the prerequisite
    steps in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Google Cloud Shell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the Google Cloud Shell to perform the activities in this section.
    You can access it by clicking the Cloud Shell symbol in the top-right corner of
    the Google Cloud console screen, as shown in *Figure 18**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.1: Activating the Cloud Shell](img/B18143_18_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.1: Activating the Cloud Shell'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Google Cloud Shell as shown in *Figure 18**.1*, and perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the directory to the location at which the code for this chapter is
    stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the `chapter-18-prereqs.sh` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you are prompted to authorize the Cloud Shell, click **Authorize**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The script performs the following steps on our behalf:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Grant the **Eventarc Event Receiver** IAM role to our project’s default Compute
    Engine service account (our Jupyter Notebook, Cloud Functions, and other components
    use this service account). If you want to use a service account other than the
    default Compute Engine service account, you would need to grant the **Eventarc
    Event Receiver** IAM role to that service account instead.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Grant the **Service Account Token Creator** IAM role to our project’s Pub/Sub
    service agent. This is required for when we use Pub/Sub later in this chapter.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the prerequisites have been completed, we can continue with the main
    content of the chapter. The first solution we will focus on is building a RAG
    implementation piece by piece.
  prefs: []
  type: TYPE_NORMAL
- en: Building a RAG implementation piece by piece
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 17*](B18143_17.xhtml#_idTextAnchor408), we implemented a RAG solution
    using Vertex AI Search. I explained that Vertex AI Search makes the process very
    easy for us because it abstracts away the steps in the process, such as chunking
    and embedding our content, and it performs all of those steps for us behind the
    scenes. There are also popular frameworks, such as LlamaIndex, that help to simplify
    RAG implementations, and Vertex AI has also launched a grounding service (Vertex
    AI Grounding) that you can use to ground responses from a generative model with
    results from Google Search or with your own data (using the aforementioned Vertex
    AI Search solution). In this section, we will dive deeper into the process and
    build a RAG solution, piece by piece. Before we dive into the solution architecture,
    we’ll cover some of the concepts that had been abstracted away in [*Chapter 17*](B18143_17.xhtml#_idTextAnchor408),
    most notably regarding **tokens** and **chunks**.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens and chunks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll discuss the concepts of tokens and chunks, beginning
    with tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs generally work with tokens rather than words. For example, when dealing
    with text, a token often represents subsections of words, and tokenization can
    be done in different ways, such as breaking text up by characters, or using subword-based
    tokenization (the word “unbelievable” could be split into subwords such as “un,”
    “believe,” and “able”).
  prefs: []
  type: TYPE_NORMAL
- en: The exact size and definition of a token can vary based on different tokenization
    methods, models, languages, and other factors, but a general rule of thumb for
    English text using subword tokenizers is around four characters per token on average.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss the concept of chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Chunks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When creating embeddings, we usually break a document into chunks and then create
    embeddings of those chunks. Again, this can be done in different ways, using different
    tools. In the Jupyter Notebook file accompanying this chapter, we use Google Cloud
    Document AI to break our documents into chunks.
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, one of the parameters we need to specify for our Document
    AI processor is the chunk size to use, which is measured (in this case) by the
    number of tokens per chunk. You may need to experiment with the value of this
    parameter to find the chunk size that works best for your use case (e.g., based
    on the length and structure of the document sections). We generally want our chunks
    to capture some level of semantic granularity, but there are trade-offs in terms
    of this granularity. For example, smaller chunks can capture more granular semantic
    context and provide more precise search results but can be less efficient (computationally)
    to process. We also need to ensure chunk sizes are within the input length limits
    of the embedding model we’re using in order to avoid possible truncation. A good
    practice is to start with a moderate chunk size and adjust it based on how well
    it fits our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Document AI can automatically handle chunking based on layout,
    even if you don’t specify a preconfigured chunk size, which can be helpful if
    you don’t know what chunk size to use.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered those concepts, let’s dive into the architecture of the
    RAG implementation we will build in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Example RAG solution architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will review the architecture of our example RAG implementation.
    The proposed solution architecture is shown in *Figure 18**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.2: RAG solution piece by piece on Google Cloud](img/B18143_18_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.2: RAG solution piece by piece on Google Cloud'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that between some of the steps depicted in *Figure 18**.2*, **Google
    Cloud Storage** (**GCS**) is used to store the inputs and outputs of each step,
    but those intermediary processes are omitted to make the diagram more readable.
    Also, when we implement this solution in the Jupyter Notebook that accompanies
    this chapter, the notebook is the application/user that coordinates each of the
    steps in the overall process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps in the solution depicted in *Figure 18**.2* are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Our documents, which are stored in GCS, are sent to Google Cloud Document AI
    for chunking. As the name suggests, the chunking process breaks the documents
    into chunks, which are smaller sections of the document. This is required in order
    to create standard-sized chunks that serve as inputs to the embedding process
    that comes in the next step. The size of the chunks is configurable in Document
    AI, and further details on this process are described later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The chunks are then sent to the Google Cloud text embedding LLM to create embeddings
    for the chunks. The resulting embeddings are stored in GCS, alongside their respective
    chunks (this step is omitted from the diagram).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a Vertex AI Vector Search index, and the embeddings are ingested from
    GCS to the Vertex AI Vector Search index (the GCS intermediary step is omitted
    from the diagram).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the application/user asks a question that relates to the contents of our
    documents. The question is sent as a query to the Google Cloud text embedding
    LLM to be embedded/vectorized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The vectorized query is then used as input in a request to Vertex AI Vector
    Search, which searches our index to find similar embeddings. Remember that the
    embeddings represent an element of semantic meaning, so similar embeddings have
    similar meanings. This is how we can perform a semantic search to find embeddings
    that are similar to our query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we take the embeddings returned from our Vertex AI Vector Search query
    and find the chunks in GCS that relate to those embeddings (remember that *Step
    2* in our solution created a stored association of chunks and embeddings).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, it’s finally time to send a prompt to Gemini. The retrieved document chunks
    from *Step 6* serve as the context for the prompt. This helps Gemini respond to
    our prompt based on the relevant content from our documents and not just from
    its pre-trained knowledge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gemini responds to the prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we’ve walked through the steps in the process, let’s go ahead and implement
    this solution!
  prefs: []
  type: TYPE_NORMAL
- en: Building the RAG implementation on Google Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build our solution, open the Jupyter Notebook file that accompanies this
    chapter, and perform the activities described in that file. We can use the same
    Vertex AI Workbench instance that we created in [*Chapter 14*](B18143_14.xhtml#_idTextAnchor348)
    for this purpose. Please open JupyterLab on that notebook instance. In the directory
    explorer on the left side of the screen, navigate to the `Chapter-18` directory
    and open the `rag.ipynb` notebook file. You can choose Python (local) as the kernel.
    Again, you can run each cell in the notebook by selecting the cell and pressing
    *Shift* + *Enter* on your keyboard. In addition to the relevant code, the notebook
    file contains markdown text that describes what the code is doing.
  prefs: []
  type: TYPE_NORMAL
- en: When you have completed the steps in the notebook, you will have officially
    built your own RAG solution—nice work!
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this case, we are using documents stored in GCS as our source of
    truth, but we could also use other data, such as data stored in BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: Document citation
  prefs: []
  type: TYPE_NORMAL
- en: 'For congruity and comparability, we will use one of the same reference documents
    that we used in our RAG implementation in [*Chapter 17*](B18143_17.xhtml#_idTextAnchor408),
    cited as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hila Zelicha, Jieping Yang, Susanne M Henning, Jianjun Huang, Ru-Po Lee, Gail
    Thames, Edward H Livingston, David Heber, and Zhaoping Li, 2024\. Effect of cinnamon
    spice on continuously monitored glycemic response in adults with prediabetes:
    a 4-week randomized controlled crossover* *trial. DOI:https://doi.org/10.1016/j.ajcnut.2024.01.008*'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will build a broader solution that brings together many topics from
    throughout this entire book. I’ll begin by explaining the use case.
  prefs: []
  type: TYPE_NORMAL
- en: An example business use case and reference architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will focus on a **computer vision** (**CV**) use case to identify objects
    in images. Extending the CV use case we implemented in [*Chapter 15*](B18143_15.xhtml#_idTextAnchor371),
    we will build our CV model via an MLOps pipeline that incorporates the majority
    of the main topics from the earlier chapters in this book, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation, cleaning, and transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training, deployment, inference, and evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The novel approach we are using here is to utilize generative AI to generate
    the data that will be used to test and evaluate our model. Before we start building,
    I’ll present additional context on why I chose to include this use case as an
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Additional background on our use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Throughout this book, one topic has come up more frequently than any other:
    data is often the most important factor in training high-quality ML models. Without
    adequate data, your ML project will not succeed. We’ve discussed that getting
    access to sufficient data often proves difficult, and this challenge continues
    to become ever more prevalent as the average size of models increases (today’s
    models with trillions of parameters require enormous amounts of data). We also
    discussed that there’s a limited amount of data in the world, but one mechanism
    that could help address this challenge is the creation of synthetic data and the
    use of generative AI models to generate new data. The use case I outline in this
    section combines all of the steps in the traditional ML model development life
    cycle and extends the process to include generating data that can be used to test
    our model. In the next subsection, I’ll describe the architecture for this solution.'
  prefs: []
  type: TYPE_NORMAL
- en: The reference architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section outlines the components of the solution architecture we will build.
    In addition to Vertex AI components, the reference architecture brings other Google
    Cloud services into scope to build the overall solution, such as Google Cloud
    Functions, Pub/Sub, Eventarc, and Imagen. We originally introduced and described
    all of those Google Cloud services in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059)
    of this book. If you are not familiar with them, I recommend refreshing your knowledge
    from that chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Our solution begins with an MLOps pipeline that will train and deploy our CV
    model, which I will describe in detail shortly. The resulting model is then used
    in a broader architecture to implement the overall solution. I’ll begin by outlining
    the MLOps pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: A note on the dataset (CIFAR-10)
  prefs: []
  type: TYPE_NORMAL
- en: 'In our model training and evaluation code, we’re using something called the
    **CIFAR-10** (**Canadian Institute For Advanced Research, 10 classes**) dataset,
    which is a commonly used benchmark dataset for CV and image classification. It
    contains 60,000 color images belonging to ten different classes: Airplane, Automobile,
    Bird, Cat, Deer, Dog, Frog, Horse, Ship, and Truck. Each image has a fixed size
    of 32x32 pixels, and the images are stored in the RGB color space, meaning each
    pixel is represented by three values corresponding to the red, green, and blue
    color channels. It’s important to keep these details in mind when we want to send
    generated data to our model at inference time.'
  prefs: []
  type: TYPE_NORMAL
- en: The CIFAR-10 dataset is so commonly used for benchmarking CV models that it
    is included in the built-in datasets module in Tensorflow/Keras. This means that
    we can use it in our code by simply importing that module rather than needing
    to download the data from an external source.
  prefs: []
  type: TYPE_NORMAL
- en: Our MLOps pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will walk through the steps implemented in our MLOps pipeline,
    as depicted in *Figure 18**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.3: MLOps pipeline for CV model](img/B18143_18_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.3: MLOps pipeline for CV model'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 18**.3*, the process works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step in our pipeline—the model training step—is invoked. While the
    MLOps pipeline we built for our tabular Titanic dataset in [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288)
    started with distinct data preprocessing steps using Serverless Spark in Dataproc,
    in our pipeline in this chapter, the data ingestion and preparation steps are
    handled directly in the code of our model training job. Also, as noted, in this
    case, we are using the built-in CIFAR-10 image dataset in Tensorflow/Keras rather
    than fetching a dataset from an external source. Vertex AI Pipelines starts the
    model training process by submitting a model training job to the Vertex AI training
    service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to execute our custom training job, the Vertex AI training service
    fetches our custom Docker container from Google Artifact Registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When our model has been trained, the trained model artifacts are saved in GCS.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The model training job status is complete.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next step in our pipeline—the model import step—is invoked. This is an intermediate
    step that prepares the model metadata to be referenced in later components of
    our pipeline. The relevant metadata in this case consists of the location of the
    model artifacts in GCS and the specification of the Docker container image in
    Google Artifact Registry that will be used to serve our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step in our pipeline—the model upload step—is invoked. This step references
    the metadata from the model import step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model metadata is used to register the model in the Vertex AI Model Registry.
    This makes it easy to deploy our model for serving traffic in Vertex AI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model upload job status is complete.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next step in our pipeline—the endpoint creation step—is invoked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An endpoint is created in the Vertex AI prediction service. This endpoint will
    be used to host our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The endpoint creation job status is complete.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next step in our pipeline—the model deployment step—is invoked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our model is deployed to our endpoint in the Vertex AI prediction service. This
    step references the metadata of the endpoint that has just been created by our
    pipeline, as well as the metadata of our model in the Vertex AI Model Registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model deployment job status is complete.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we’ve walked through the steps in our model training and deployment
    pipeline, I will begin to outline the broader solution architecture we will build
    in this chapter, of which our MLOps pipeline is just a subset.
  prefs: []
  type: TYPE_NORMAL
- en: The end-to-end solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will build an event-driven architecture, which is a popular pattern for developing
    serverless solutions intended to be implemented automatically in response to an
    event or a set of events. This is the end-to-end solution that incorporates the
    majority of the topics we’ve covered throughout this book, as depicted in *Figure
    18**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.4: The end-to-end solution](img/B18143_18_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.4: The end-to-end solution'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 18**.4*, our MLOps pipeline is simplified in the top-left corner
    of the diagram. It still implements all of the same steps we discussed in the
    previous section, but the diagram is simplified so we can focus our discussion
    on the broader, end-to-end solution. In this context, the MLOps pipeline is represented
    as a single step in the overall process.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that Eventarc features prominently in our solution, as it is the primary
    mechanism for orchestrating the steps in our event-based architecture. In the
    Jupyter Notebook file that accompanies this chapter, I will explain in more detail
    exactly how Eventarc is being configured behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following set of steps describes the architecture implemented in *Figure
    18**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: Our MLOps pipeline trains and deploys our CV model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the MLOps pipeline completes, it publishes a message to a Pub/Sub topic
    we created for that purpose.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eventarc detects that a message has been published to the Pub/Sub topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eventarc triggers the Cloud Function we’ve created to generate an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code in our image generation function makes a call to the Imagen API with
    a prompt to generate an image containing one of the types of objects our model
    was trained to recognize (a type of object supported by the CIFAR-10 dataset).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Imagen generates an image and returns it to our function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our function stores the new image in GCS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GCS emits an event indicating that a new object has been uploaded to our bucket.
    Eventarc detects this event.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eventarc invokes our next Cloud Function and passes the GCS event metadata to
    our function. This metadata includes details such as the identifiers of the bucket
    and the object in question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our prediction function takes the details regarding the bucket and the object
    in question from the event metadata and uses those details to fetch the newly
    created object (i.e., the newly generated image from Imagen).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our prediction function then performs some preprocessing on the image to transform
    it into a format that is expected by our model (i.e., similar to the format of
    the CIFAR-10 data the model was trained on). Our function then sends the transformed
    data as a prediction request to the Vertex AI endpoint that hosts our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our model predicts what type of object is in the image, and sends a prediction
    response to our Cloud Function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our Cloud Function saves the prediction response in GCS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the process has been completed, you can view the generated image and the
    resulting prediction from our model in GCS.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that all of the steps in the solution are implemented automatically and
    without the need to provision any servers. This is a fully serverless, event-driven
    solution architecture.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting side effect of this solution is that, although the primary intention
    is to test our newly trained model on generated data, this solution could also
    be applied to the inverse use case. That is, if we are confident that our model
    has been trained effectively and provides consistently accurate results, we could
    use it to evaluate the quality of the generated data. For example, if our model
    predicts that the generated data contains a particular type of object with a probability
    of 99.8%, we can interpret this as a reflection of the quality of the generated
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve discussed the various steps in the process, let’s start building
    it!
  prefs: []
  type: TYPE_NORMAL
- en: Building and implementing the use case on Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build our solution, open the Jupyter Notebook file that accompanies this
    chapter, and perform the activities described in that file. We can use the same
    Vertex AI Workbench instance that we created in [*Chapter 14*](B18143_14.xhtml#_idTextAnchor348)
    for this purpose. Please open JupyterLab on that notebook instance. In the directory
    explorer on the left side of the screen, navigate to the `Chapter-18` directory
    and open the `end-to-end-mlops-genai.ipynb` notebook file. You can choose Python
    (local) as the kernel. Again, you can run each cell in the notebook by selecting
    the cell and pressing *Shift* + *Enter* on your keyboard. In addition to the relevant
    code, the notebook file contains markdown text that describes what the code is
    doing.
  prefs: []
  type: TYPE_NORMAL
- en: If you have followed all of the practical steps and successfully built the solution,
    then it’s time to give yourself a good pat on the back. Let’s take a moment to
    reflect on what you have done here. You have successfully built and executed an
    MLOps pipeline that trains and deploys a Keras convolutional neural network implementing
    a CV use case. You have then built a completely serverless, event-driven solution
    architecture using a combination of many Google Cloud services. That is absolutely
    something to be very proud of!
  prefs: []
  type: TYPE_NORMAL
- en: As we approach the end of this chapter and book, let’s summarize what we’ve
    learned and discuss how to keep learning in this space.
  prefs: []
  type: TYPE_NORMAL
- en: Recap and next steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we combined the majority of the important concepts that we’ve
    covered throughout this book and explained how they all tie together. We did this
    by building solutions that incorporated many of the topics and Google Cloud products
    we’ve discussed.
  prefs: []
  type: TYPE_NORMAL
- en: First, you built a RAG implementation, piece by piece, focusing on the combination
    of various generative AI concepts in that process, such as using Google Cloud
    Document AI to break a document into chunks in a manner that preserves the hierarchical
    structure of the original document. This solution also included using the Google
    Cloud text embedding LLM to create embeddings for the document chunks and using
    Vertex AI Vector Search to store and index those embeddings. You used the resulting
    solution to implement a question-answering use case with Gemini, grounding the
    answers in the contents of the document. To do this, you used the Google Cloud
    text embedding LLM to create embeddings of the prompted question and used those
    embeddings to perform a semantic similarity search in Vertex AI Vector Search
    to find similar embeddings relating to specific chunks from the document. Those
    chunks were then provided as context when sending the request to Gemini.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you built a solution that combined many topics from this entire book,
    including all of the steps in a traditional model development life cycle, as well
    as generative AI and broader solution architecture concepts. This resulted in
    a fully automated, serverless, event-driven solution that incorporated many Google
    Cloud services, such as Google Cloud Functions, Pub/Sub, Eventarc, and Imagen.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve certainly covered a lot of topics in this book, and the learning journey
    never ends. Let’s wrap up by discussing how you can continue your learning journey
    in the fields of Google Cloud solutions architecture, ML, and generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The technology fields we’ve discussed in this book continue to evolve at an
    accelerating pace, with new frameworks and patterns emerging daily, and generative
    AI is arguably the fastest-evolving field of them all. For example, in addition
    to the popular LangChain framework we’ve discussed in this book, which can also
    be implemented as a managed experience in the Vertex AI Reasoning Engine, extensions
    of those frameworks, such as LangGraph, LangSmith, and many others continue to
    emerge, providing additional functionality and flexibility for building generative
    AI solutions and applications. Due to the accelerating pace of development and
    overall excitement in this space, new learning resources will continue to emerge.
    In addition to the GitHub repository associated with this book, many other example
    repositories provide valuable tutorials on how to implement various patterns.
    Perhaps the most valuable of these is, of course, the official Vertex AI example
    repository, which can be accessed at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/GoogleCloudPlatform/vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples)'
  prefs: []
  type: TYPE_NORMAL
- en: 'I also find the information available at [langchain.com](http://langchain.com)
    to be a useful learning resource, and I highly recommend referencing the example
    architecture use cases in the Google Cloud Solutions Center, accessible at the
    following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://solutions.cloud.google.com/](https://solutions.cloud.google.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: As we wrap up, remember that the practical exercises and notebooks we used throughout
    this book create resources on Google Cloud, which can incur costs. For that reason,
    I recommend reviewing each of the practical steps we performed in this book and
    ensuring that all resources are deleted if you no longer plan to use them. The
    GitHub repository associated with this book contains steps for deleting resources
    such as models, endpoints, and data stores. For other types of resources, please
    refer to the Google Cloud documentation to ensure that the relevant resources
    are deleted.
  prefs: []
  type: TYPE_NORMAL
- en: I write this final section with a sense of sadness that our journey together
    is coming to an end, and I want to thank you for embarking on this journey with
    me. If you’ve made it this far in the book, then you have gained expert knowledge
    in AI/ML, generative AI, Google Cloud, and solutions architecture, and you now
    know a lot more about all of those topics than most people in the world! Congratulations,
    and well done! I wish you the best of luck in your future AI adventures, and I
    hope you will use your knowledge and skills to achieve wonderful things!
  prefs: []
  type: TYPE_NORMAL
