- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous chapters, we worked with distributions of single variables. Now
    we will discuss the relationships between variables. In this chapter and the next
    chapter, we will investigate the relationship between two or more variables using
    linear regression. In this chapter, we will discuss simple linear regression within
    the framework of `statsmodels`, and finally, we will address the scenarios of
    serial correlation and model validation. As highlighted, our main topics in this
    chapter follow this framework:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression using OLS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coefficients of correlation and determination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Required model assumptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test for significance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling model errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple linear regression using OLS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will study one of the simplest machine learning models – simple linear regression.
    We will provide its overview within the context of OLS, where the objective is
    to minimize the sum of the square of errors. It is a straightforward concept related
    to a dependent variable (quantitative response) y and its independent variable
    x, where their relationship can be drawn as a straight line, approximately. Mathematically,
    a simple linear regression model can be written in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: y = β 0 + β 1 x + ϵ
  prefs: []
  type: TYPE_NORMAL
- en: Here, β 0 is the intercept term and β 1 is the slope of the linear model. The
    error term is denoted as ϵ in the preceding linear model. We can see that in an
    ideal case where the error term is zero, β 0 represents the value of the dependent
    variable y at x = 0\. Within the range of the independent variable x, β 1 represents
    the increase in the outcome y corresponding to a unit change in x. In literature,
    the independent variable x can be called the explanatory variable, predictor,
    input, or feature and the dependent variable y can be called the response variable,
    output, or target. The question is raised, “If we have a sample from a dataset
    (x i, y i) with i = 1,2, … , n points), how do we determine a line of best fit
    using the intercept term and the slope?” By estimating these terms, we will fit
    the best line through the data to show the linear relationship between the independent
    variable and the dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The relationship between x and y – Line of best fit](img/B18945_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – The relationship between x and y – Line of best fit
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding plot example, the blue points are actual values representing
    the relationship between the independent variable x and the dependent variable
    y. The red line is the line of best fit through all these data points. Our objective
    now is to formulate the predicted line (the line of best fit):'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ y  = ˆ β 0 + ˆ β 1x
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a closer look at the relationship between the predicted values and the
    true values. In order to find the line of best fit, we will minimize the vertical
    distances, meaning that we will minimize the errors between the points and the
    line through the data. In other words, we will minimize the sum of square errors,
    which is computed by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Σ i=1 n  e i 2
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Errors between data points and the line through the data](img/B18945_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Errors between data points and the line through the data
  prefs: []
  type: TYPE_NORMAL
- en: Here,
  prefs: []
  type: TYPE_NORMAL
- en: e i = y i − ˆ y i,
  prefs: []
  type: TYPE_NORMAL
- en: 'where y i is the observed value and ˆ y i is the predicted value of the response
    variable y for the i th observation. The sum of the square of errors is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: S = Σ i=1 n  (y i − ˆ β 0 − ˆ β 1 x i) 2
  prefs: []
  type: TYPE_NORMAL
- en: 'To minimize S, we will take the partial derivatives with respect to ˆ β 0 and
    ˆ β 1\. Then, we see:'
  prefs: []
  type: TYPE_NORMAL
- en: ∂ S _ ∂ ˆ β 0  = − 2Σ(y i − ˆ β 0 − ˆ β 1 x i) = 0,
  prefs: []
  type: TYPE_NORMAL
- en: ∂ S _ ∂ ˆ β 1  = − 2Σ x i(y i − ˆ β 0 − ˆ β 1 x i) = 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, it is easy to see that:'
  prefs: []
  type: TYPE_NORMAL
- en: nˆ β 0 + (Σ i=1 n  x i)ˆ β 1 = Σ i=1 n  y i,
  prefs: []
  type: TYPE_NORMAL
- en: (Σ i=1 n  x i)ˆ β 0 + (Σ i=1 n  x i 2)ˆ β 1 = Σ i=1 n  x i y i,
  prefs: []
  type: TYPE_NORMAL
- en: 'This implies:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ β 0 =  Σ i=1 n  y i _ n  − ˆ β 1  Σ i=1 n  x i _ n ,
  prefs: []
  type: TYPE_NORMAL
- en: Σ i=1 n  x i Σ i=1 n  y i _ n  −  (Σ i=1 n  x i) 2 _ n ˆ β 1 + (Σ i=1 n  x i 2)ˆ β 1
    = Σ i=1 n  x i y i.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because  _ y  = Σ i=1 n  y i _ n  and  _ x  = Σ i=1 n  x i _ n , we can rewrite
    the slope and intercept as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ β 0 =  _ y  − ˆ β 1 _ x ,
  prefs: []
  type: TYPE_NORMAL
- en: ˆ β 1 =  Σ(x i −  _ x )(y i −  _ y )  ____________  Σ (x i −  _ x ) 2 .
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, we can implement the following code to find ˆ β 0 and ˆ β 1 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Coefficients of correlation and determination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss two related notions – coefficients of correlation
    and coefficients of determination.
  prefs: []
  type: TYPE_NORMAL
- en: Coefficients of correlation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A coefficient of correlation is a measure of the statistical linear relationship
    between two variables and can be computed using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: r =  1 _ n − 1 Σ i=1 n (x i −  ‾ x  _ s x )(y i −  ‾ y  _ s y )
  prefs: []
  type: TYPE_NORMAL
- en: The reader can go here – [https://shiny.rit.albany.edu/stat/corrsim/](https://shiny.rit.albany.edu/stat/corrsim/)
    – to simulate the correlation relationship between two variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Simulated bivariate distribution](img/B18945_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Simulated bivariate distribution
  prefs: []
  type: TYPE_NORMAL
- en: By observing the scatter plots, we can see the direction and the strength of
    the linear relationship between the two variables and their outliers. If the direction
    is positive (r>0), then both variables increase or decrease together.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Simulated bivariate distribution](img/B18945_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Simulated bivariate distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'If the direction is negative, then one variable increases while the other decreases.
    We illustrate this in the following scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Simulated bivariate distribution](img/B18945_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Simulated bivariate distribution
  prefs: []
  type: TYPE_NORMAL
- en: The coefficient of correlation exists as a value between -1 and 1\. When r =1,
    we have a perfect positive correlation, and when r = -1, we have a perfect negative
    correlation. Observe that the value of the coefficient of correlation does not
    change if, for example, the variable x and the variable y are switched or if all
    values of either variable are linearly scaled. In general, correlation does not
    imply causation.
  prefs: []
  type: TYPE_NORMAL
- en: Coefficients of determination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The coefficient of determination is just r 2, where r is the coefficient of
    correlation, which is a proportion of the variation in the variable y, explained
    by the variable x. The value of r 2 is between 0 and 1, with the linear relationship
    between the two variables becoming stronger as its value approaches 1\. The value
    of r 2 can also be computed using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: r 2 = 1 −  S S Res _ S S Tot
  prefs: []
  type: TYPE_NORMAL
- en: Here, S S Res is the sum of the square of errors and S S Totis the total sum
    of the square of the difference between the actual value and the average value.
    In `statsmodels`, from the output of the OLS regression results, the value of
    determination can be obtained. This output is discussed in the next sections of
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Required model assumptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like the parametric tests we discussed in [*Chapter 4*](B18945_04.xhtml#_idTextAnchor070)*,
    Parametric Tests*, linear regression is a parametric method and requires certain
    assumptions to be met for the results to be valid. For linear regression, there
    are four assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: A linear relationship between variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The normality of the residuals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The homoscedasticity of the residuals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Independent samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss each of these assumptions individually.
  prefs: []
  type: TYPE_NORMAL
- en: A linear relationship between the variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When thinking about fitting a linear model to data, our first consideration
    should be whether the model is appropriate for the data. When working with two
    variables, the relationship between the variables should be assessed with a scatter
    plot. Let’s look at an example. Three scatter plots are shown in *Figure 6**.6*.
    The data is plotted, and the actual function used to generate the data is drawn
    over the data points. The leftmost plot shows data exhibiting a linear relationship.
    The middle plot shows data exhibiting a quadratic relationship. The rightmost
    plot shows two uncorrelated variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Three scatter plots depicting three distinct relationships between
    two variables](img/B18945_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Three scatter plots depicting three distinct relationships between
    two variables
  prefs: []
  type: TYPE_NORMAL
- en: Of the example data in *Figure 6**.6*, a linear model would only be appropriate
    for the leftmost data. In this example, we have the benefit of knowing the actual
    relationship of the two variables, but, in real problems, you will have to determine
    whether a linear model is appropriate. Once we have assessed whether a linear
    model is appropriate and fitted a linear model, we can assess the two assumptions
    that depend on the model residuals.
  prefs: []
  type: TYPE_NORMAL
- en: Normality of the residuals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous two chapters, we discussed normality at length and looked at
    several examples of data that are normally distributed and several examples that
    were not normally distributed. In this case, we are making the same assessment,
    but it is for the model residuals rather than for the variables. In fact, unlike
    the previous parametric methods, the *variables themselves do not need to be normally
    distributed, only the residuals*. Let’s look at an example. We will fit a linear
    model on two variables, each of which exhibits a skewed distribution. As can be
    seen in *Figure 6**.7*, even though each variable has a skewed distribution, the
    variables are linearly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Distributions of X, Y, and their scatter plots](img/B18945_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Distributions of X, Y, and their scatter plots
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the residuals from the model fit. The residuals from the
    model fit are shown in *Figure 6**.8*. As we expect, the residuals from this model
    fit appear to be normally distributed. This model appears to meet the assumption
    of normally distributed residuals.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Residuals from model fit](img/B18945_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Residuals from model fit
  prefs: []
  type: TYPE_NORMAL
- en: With the assumption of normally distributed residuals verified, let’s move on
    to the assumption of homoscedasticity of the residuals.
  prefs: []
  type: TYPE_NORMAL
- en: Homoscedasticity of the residuals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not only should the residuals be normally distributed, but the residuals should
    also have constant variance over the linear model. **Homoscedasticity** refers
    to the residuals having equal scatter. Conversely, residuals that do exhibit equal
    spread are said to be **heteroscedastic**. Homoscedasticity is generally assessed
    by plotting the residuals against the model predictions with a scatter plot. The
    residuals should appear to be randomly distributed with a mean of 0 and equally
    dispersed along the *x* axis. *Figure 6**.9* shows a scatter plot of the model
    residuals against the predicted value of *Y*. These residuals appear to exhibit
    homoscedasticity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Scatter plot of model residuals versus prediction](img/B18945_06_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Scatter plot of model residuals versus prediction
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few common ways the homoscedasticity of the residuals could be
    violated:'
  prefs: []
  type: TYPE_NORMAL
- en: The residuals show a systematic change in variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is an extreme outlier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two examples of these violations are shown in *Figure 6**.10*. The left plot
    shows an extreme outlier, and the right plot shows non-constant variance in the
    residuals. In both cases, the model assumptions are violated.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Example residuals that exhibit poor behavior](img/B18945_06_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Example residuals that exhibit poor behavior
  prefs: []
  type: TYPE_NORMAL
- en: When the residuals show a systematic pattern, it may be an indication that another
    type of model would be more appropriate, or it may be helpful to apply a transformation
    to one or both variables. When an extreme outlier is present, it may be worth
    verifying and investigating the data point to ensure it should be included in
    the analysis. In the next section, we will discuss how extreme outliers impact
    the model fit.
  prefs: []
  type: TYPE_NORMAL
- en: Sample independence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have discussed sample independence in previous chapters. There are no graphics
    or statistics that can be used to determine whether samples are independent. *Assessing
    sample independence requires careful analysis of the sampling method and the populations
    from which the samples are drawn*. For example, a common type of data that violates
    the independence assumption is time-series data. Time-series data is a type of
    data that is sampled over time, making it serially correlated. We will discuss
    analysis methods for time-series data in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed the linear regression model assumptions. In the
    next section, we will discuss how to validate a linear model, which will again
    utilize the model residuals.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for significance and validating models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to this point in the chapter, we have discussed the concepts of the OLS approach
    to linear regression modeling; the coefficients in a linear model; the coefficients
    of correlation and determination; and the assumptions required for modeling with
    linear regression. We will now begin our discussion on testing for significance
    and model validation.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for significance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To test for significance, let us load `statsmodels` macrodata data set so we
    can build a model that tests the relationship between real gross private domestic
    investment, `realinv`, and real private disposable income, `realdpi`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Least squares regression requires a constant coefficient in order to derive
    the `statsmodels’` `add_constant` function here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The input data does not need to be normally distributed for least squares regression.
    However, it is assumed that the residuals of the model are normally distributed.
    However, it is useful to see the spread of the data to get an understanding of
    their statistics, such as the mean, median, and range, and whether there are outliers
    present. If the residual analysis after the modeling suggests there may be some
    issues, having inspected the model variables will help the analyst understand
    potential root causes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Visualizing the distributions of the realinv and realdpi variables](img/B18945_06_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Visualizing the distributions of the *realinv* and *realdpi* variables
  prefs: []
  type: TYPE_NORMAL
- en: When visualizing the relationship between `realinv` and `realdpi`, we can see
    a strong linear relationship, but also a potential serial correlation in the data
    as there appears to be a somewhat **cyclical oscillation** in the data, which
    becomes stronger toward the extreme values, seen in the upper right of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Visualizing the relationship between the realinv and realdpi
    variables](img/B18945_06_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Visualizing the relationship between the *realinv* and *realdpi*
    variables
  prefs: []
  type: TYPE_NORMAL
- en: 'We will analyze the potential serial correlation seen in the preceding plot
    after a few more steps. First, let us fit the input variable and coefficient to
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will want to print out a summary of the model performance so we can
    begin to gauge the significance of the terms involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Referring to the results in *Figure 6**.13*, we can see that the `const`, nor
    the input variable, `realinv`, contains zero. Therefore, in addition to the **significant
    p-values** for these variables, we can conclude they are significant contributors
    to the target at the coefficients provided. Now that we have confirmed there is
    statistical significance for both the intercept and the input variable and their
    coefficients, we want to know the level of correlation to the target they represent.
    We can see the **R-squared** statistic is 0.944\. Because the two variables are
    the constant and the input variable, this R-squared statistic explains only the
    input variable.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared versus adjusted r-squared
  prefs: []
  type: TYPE_NORMAL
- en: For multivariate regression, we would look at the adjusted r-squared value,
    which we can also see is the same – in the univariate/simple regression case –
    as the r-squared value.
  prefs: []
  type: TYPE_NORMAL
- en: The **correlation of determination** – also called goodness of fit – of 0.944
    means real gross private domestic investment explains 94.4% of the variance in
    real private disposable income.
  prefs: []
  type: TYPE_NORMAL
- en: We can see based on the table that the intercept and the `realinv` variable
    are both significant in predicting the target with very low p-values. We can also
    see that neither of the 95% confidence intervals contains 0, which backs up the
    relevancy of their p-values. However, the confidence interval for the constant
    (intercept) suggests there may be a risk of uncertainty in the model and that
    more variables may be needed to reduce that uncertainty and improve model performance.
    Stated differently, we could say a higher level of uncertainty in the constant
    indicates the model has variance left to be explained and that the current model
    is overly biased.
  prefs: []
  type: TYPE_NORMAL
- en: Validating models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Durbin-Watson test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is important to acknowledge that based on the fact that data was observed
    over a 50-year period and random sampling was not performed, it is possible the
    residuals will have a **serial correlation**. We observe a possibility of serial
    correlation in the **Durbin-Watson test** statistic of 0.095, which suggests the
    residuals exhibit positive autocorrelation. The Durbin-Watson statistic tests
    whether there is autocorrelation at the first time lag. This means the Durbin-Watson
    tests whether the change in values between each data point is correlated with
    the value of the previous data point. If that correlation exists, it is considered
    **lag-one autocorrelation**.
  prefs: []
  type: TYPE_NORMAL
- en: Durbin-Watson test statistic
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, a Durbin-Watson test statistic below roughly 2.0 (many consider
    1.5 to 2.0 to be reasonable) is considered a flag for positive autocorrelation,
    while a value above roughly 2.0 (many consider 2.0 to 2.5 to be reasonable) indicates
    possible negative autocorrelation. Procedurally, a table lookup is required to
    identify the critical values against which the test statistic should be measured.
    The process follows the same steps for any hypothesis test with respect to comparing
    the test statistic against the critical value. The Durbin-Watson test is typically
    interpreted as a one-tailed test as inference from a two-tailed test is not particularly
    useful. A version of the Durbin-Watson table can be found at [https://www.real-statistics.com/statistics-tables/durbin-watson-table/](https://www.real-statistics.com/statistics-tables/durbin-watson-table/).
    This table produces the critical values for **positive autocorrelation**. To find
    the critical values for **negative autocorrelation**, the positive critical values
    can be subtracted from 4.
  prefs: []
  type: TYPE_NORMAL
- en: Although we have a Durbin-Watson statistic that suggests serial correlation
    may be present in the data, we will confirm significance by comparing the Durbin-Watson
    statistic to its corresponding positive autocorrelation `statsmodels`’ OLS regression,
    we find the bounds for the `k=1` input variable to be `[1.758, 1.779]`. A value
    within this range should be interpreted as **inconclusive**. A value less than
    the lower bound indicates the presence of enough evidence to reject the null hypothesis
    and thus conclude there is statistically significant positive autocorrelation.
    A value greater than the upper bound indicates there is not enough evidence to
    reject the null hypothesis and thus there is no autocorrelation. Based on the
    results of our model, we can conclude with a 95% level of confidence that there
    is evidence of positive serial correlation in our residuals.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – OLS regression model output for realdpi](img/B18945_06_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – OLS regression model output for *realdpi*
  prefs: []
  type: TYPE_NORMAL
- en: At this point, an analyst may want to consider addressing the serial correlation
    before moving forward. However, for the purpose of following the process of end-to-end
    model validation, we will move forward to the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnostic plots for analyzing model errors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned, y i = β 0 + β 1 x i + e i, where in regression, β 0 + β 1 x i
    is the predicted mean for y i given x i and e i is the error term – also called
    the residual term, which is calculated as the observed value for a data point
    minus the predicted value for that data point. The residuals in least squares
    linear regression must approximate a normal distribution centered around the mean
    with a standard deviation. The method for deriving the residuals for a fitted
    model is to subtract the predicted value from the actual value for each observation.
    Four common visualizations for inspecting the fit of a least squares regression
    model are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Residuals versus fitted plots**, used to inspect the requirement of a linear
    relationship between the input and target variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantile plots**, used to inspect the assumption of residuals being normally
    distributed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale-location plots**, used to inspect the assumption of homoscedasticity,
    or the homogenous distribution of variation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residuals versus leverage influence plots**, which help identify the impact
    outliers may have on model fit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To produce these plots, we can use the following Python code implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The following output contains four common diagnostic plots.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Linear regression diagnostic plots](img/B18945_06_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Linear regression diagnostic plots
  prefs: []
  type: TYPE_NORMAL
- en: Residuals versus fitted
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One requirement of least squares regression is a **linear relationship** between
    the input and target variables. Suffice it to say a strongly correlated relationship
    exists between the input and target variables. This plot helps the analyst assess
    the linearity between the two.
  prefs: []
  type: TYPE_NORMAL
- en: We should expect the fitted line – representing the model – to be horizontal,
    representing the strength of the linear relationship between the input and target
    variable across all data points. The residuals should be roughly evenly spread
    around the line in reasonable proximity. If these attributes cannot be confirmed,
    a non-linear relationship may exist between the two variables. A transformation
    of at least one of the variables – if the variable is skewed – may help resolve
    this type of issue. However, it could also be that a non-parametric model is more
    appropriate, which is often the case when categorical features – or encoded categorical
    features – are present.
  prefs: []
  type: TYPE_NORMAL
- en: Our model produced a plot that does not display evenly spread residuals forming
    around a horizontal line. Rather, the line is concaved down, which suggests a
    polynomial or non-parametric regression may be more appropriate. Additionally,
    the residuals form a pattern toward the right end of the line that suggests there
    may be some serial correlation in the data as it exhibits some sinusoidal behavior.
    We were able to identify the presence of serial correlation using the Durbin-Watson
    statistic. This is likely due to the fact the data was not randomly sampled and
    was collected over a long period of time.
  prefs: []
  type: TYPE_NORMAL
- en: QQ plot of residuals
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Assessment of normality for the QQ plot follows the same procedure as assessing
    normality using the QQ plot in [*Chapter 4*](B18945_04.xhtml#_idTextAnchor070)*,
    Parametric Tests*. The primary purpose of this plot is to assess the required
    assumption of the **normal distribution of errors**. Minor deviation of the residuals
    from the 45-degree line in the plot is normal, but extreme skewness or deviation
    from the line may indicate a poor fit. Such a scenario could indicate poor statistical
    power. In the event of skewness, data transformation – such as a log transformation
    – could be useful to resolve the issue. Dropping outliers may be another suitable
    solution. However, rather than dropping outliers outright, it is advisable to
    ensure there was not an error in data handling that could be resolved to make
    the outliers conform.
  prefs: []
  type: TYPE_NORMAL
- en: In the QQ plot for our model’s residuals, we can see some moderate left-side
    skewness and extreme right-side skewness, which suggests the three values in the
    right tail could be extreme outliers. Aside from this, we can assume that overall,
    the residual error is approximately normally distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Scale-location
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The scale-location plot of the square root of the standardized residuals helps
    analysts use visual inspection to determine whether there is **homoscedasticity**
    – or *non-constant* variance – of the residuals, which is required for a least
    squares regression model. The plot visualizes the square root of the absolute
    value of the standardized residuals. Analyzing homoscedasticity can sometimes
    be useful for identifying potential issues related to **sample independence**,
    another requirement of least-squares regression. However, the Durbin-Watson test
    should be given more weight for testing this assumption.
  prefs: []
  type: TYPE_NORMAL
- en: In assessing our model, we can see the line has some deviation from being perfectly
    horizontal. Additionally, the residuals, while seeming to have a large amount
    of constant variance, nonetheless at times exhibit patterns, which conflicts with
    the behavior of homoscedasticity. Furthermore, there appear to be three very notable
    outliers. Based on this information, we can reasonably assume a risk of heteroscedasticity
    – or *constant* variance – exists within the model’s residuals.
  prefs: []
  type: TYPE_NORMAL
- en: Residuals versus leverage influence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Assessing the residuals versus leverage influence is another approach to assessing
    model fit. In addition to gauging leverage, this plot also shows when **Cook’s
    distance** is beyond a reasonable level.
  prefs: []
  type: TYPE_NORMAL
- en: '**Leverage** is used to identify the distance of an individual point from all
    other points. High leverage for a residual likely means the corresponding data
    point is an outlier strongly influencing the model to fit less approximately to
    the overall data and instead give more weight to that specific value. Residuals
    should be between -2 and 2 to not be considered potential outliers. Values between
    +/-2.5 to 3 suggest data points are extreme outliers. Overall, this plot is useful
    for separating outliers that have no significant negative impact on the model
    from the ones that do.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cook’s distance** is a measurement that uses both an observation’s leverage
    and its residual value. As the leverage is higher, its calculated Cook’s distance
    is higher. In general, a Cook’s distance value greater than 0.5 means a residual
    has leverage that is negatively impacting the model through over-representation
    – essentially, an outlier. Cook’s distance is included in the code and flags the
    residuals that have distances greater than 0.5\. Cook’s distance is particularly
    useful because removing outliers may not have a significant impact on the model.
    However, if the outlier has a high Cook’s distance value, that is a strong indication
    that resolving the outlier **will benefit** the model. Cook’s distance is calculated
    for each data point by removing it from the model and calculating the difference
    in error divided by the mean squared error multiplied by the number of model coefficients
    plus one. Its formulation follows:'
  prefs: []
  type: TYPE_NORMAL
- en: D i =  ∑ j=1 n  (ŷ j − ŷ j(i)) 2  ___________  (p + 1)σ ̂ 2
  prefs: []
  type: TYPE_NORMAL
- en: where ŷ j is the *j*th estimate of the response and ŷ j(i) is the value of the
    *j*th fitted value with the *i*th value removed. *p* is the number of coefficients
    in the model and σ ̂ 2 is the variance of the residuals for all observations,
    including those removed, also called the squared error.
  prefs: []
  type: TYPE_NORMAL
- en: In the plot for our model, we can observe three extreme outliers. However, we
    can also see no value of Cook’s distance is greater than 0.5\. As with the scale-location
    plot, the standardized residuals should follow even distribution around a smoothed
    line that is horizontal, which indicates a constant variance. We can see from
    our model’s output that this is not the case.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing issues with residuals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alone, each plot can be considered enough to negate the validity of a model.
    However, it is useful to consider all plots since a model may still be useful
    if there is only some deviance from expectations in the diagnostic plots. Some
    approaches to resolving issues with residuals are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Investigating and resolving any data collection issues that may produce outliers
    or data inconsistencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing outliers outright
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving the sampling approach using methods such as using random or stratified
    sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing data transformations, such as a log or square root transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Including additional input variables that can help explain the target variable
    or any serial correlation that may be present
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling serial correlation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If uncertain of the presence of serial correlation in the residuals, a useful
    next step is to analyze a **Partial Autocorrelation Function** (**PACF**) plot
    to assess whether serial correlation exists in the model at a significant level,
    which could explain issues with the model’s residuals.
  prefs: []
  type: TYPE_NORMAL
- en: ACF and PACF
  prefs: []
  type: TYPE_NORMAL
- en: The **PACF** provides a partial correlation between the value of a point and
    previous points, called lags, by controlling for the lags between. Controlling
    the lags between is where the term “partial” comes from. If a specific point at
    lag zero in a dataset is correlated strongly with the value of the third lagging
    point, there would be a significant correlation with a lag of 3, but not a lag
    of 2 or 1\. In time-series modeling, the PACF is used to directly derive the autoregressive
    orders, denoted as *AR(n)* where *n* is the lag. The **Autocorrelation Function**
    (**ACF**), which is used for determining moving average orders, denoted as *MA(n),*
    considers the correlation between data points at lag zero and all previous lagging
    points without controlling for the relationships between them. Stated alternatively,
    autocorrelation in an ACF plot at lag 3 will provide autocorrelation across points
    0 through 3 whereas correlation in a PACF plot at lag 3 will provide autocorrelation
    only between point 0 and point 3, excluding the impact of values at lags 1 and
    2.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can execute the following code to generate the PACF plot with a 95% confidence
    interval using the most recent 50 data points, ordered by index positioning –
    in this case, by date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 6.15 – Partial autocorrelation plot for the model residuals](img/B18945_06_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Partial autocorrelation plot for the model residuals
  prefs: []
  type: TYPE_NORMAL
- en: In the PACF plot in *Figure 6**.15*, we see very strong and significant correlations
    at lags 0 and 1\. We observe a low level of significant correlation (approximately
    -0.25) for lags 2 and 36, extending beyond the 95% confidence interval. What this
    tells us is we might be better off modeling the dataset with a first-order autoregressive
    (**AR(1)**) time-series model than a linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Answering regression questions with a least squares method when there is a presence
    of significant autocorrelation creates inherit risk. There are methods to adjust
    the results of a model when there is autocorrelation detected, such as taking
    a **first-order difference** – or another type of low-pass filter - or applying
    the Cochrane-Orcutt, Hildreth-Lu, or Yule-Walker adjustments. However, it is ideal
    to perform a time-series analysis, as the results of that type of modeling are
    more robust to time-dependent patterns. We will introduce time series in [*Chapter
    10*](B18945_10.xhtml#_idTextAnchor160), *Introduction to Time Series*, where we
    discuss simple and complex methods for approximating time series to produce more
    useful results.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider a first-order difference of the data. Regarding *Figure 6**.16*,
    we first look at the ACF plot and original data for the input variable, `realdpi`.
    We can see an exponentially growing line (original data) that displays an exponentially
    dampening ACF plot. This ACF behavior is typical for **trended** data that often
    benefits from **first-order differencing**. The low autoregressive ordering identified
    in the PACF indicates this may be sufficient for a regression model to proceed.
    Had the ACF shown different behavior, such as sinusoidal (seasonal) autocorrelation,
    or the PACF shown more partial autocorrelation, we would have evidence against
    a first-order difference being a sufficient solution for resolving serial correlation
    and proceeding with least squares regression. ACFs and PACFs for least squares
    regression should ideally have no significantly correlated lags. However, when
    the ACF exhibits this behavior and the PACF shows no significance, or when the
    level of correlation is very low, regression may perform well. It is preferable
    to see no patterns or peaks in either plot. Regardless, in either case, model
    error should be assessed, in addition to the autocorrelation of the residuals,
    to determine if least squares regression performs as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `diff()` `numpy` function, we take a first-order, low-pass difference
    and plot the ACFs, PACFs, and line plots for the original and differenced data.
    We can see a significant reduction in autocorrelation as a result. As an aside,
    the post-differencing autocorrelation suggests an **autoregressive moving average**
    (**ARMA**) model of AR order 1 and MA order 1 may be better suited than least-squares
    regression. However, we will ignore this for the chapter and continue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Plots for the realinv variable before and after first-order
    differencing](img/B18945_06_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Plots for the *realinv* variable before and after first-order
    differencing
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us data that we may more suitably model using regression. However,
    when using differencing as a method to resolve serial correlation in a least squares
    regression model, we must also differentiate the input variable. Observe the input
    variable, `realinv`, before and after transformation. Here in *Figure 6**.17*,
    we can see similar behavior as with `realdpi`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Plots for the realdpi variable before and after first-order
    differencing](img/B18945_06_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Plots for the *realdpi* variable before and after first-order
    differencing
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now build a new regression model using the differenced data. It is important
    to note first-order differencing removed one data point from the dataset so one
    value must also be removed from the constant in the design matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The output of this model is notably different. We now see an r-squared value
    of `0.045`. Looking at the differenced data, we can see there is a different variance
    in the differenced data for disposable income than there is for investment. This
    means that although the `realinv` variable is useful for predicting `realdpi`
    as it appears significant, there is a lot more influencing real disposable gross
    personal income. We observe that the new Durbin-Watson test statistic is now `2.487`.
    Using the critical values from the table lookup earlier, `[1.758, 1.779]`, we
    see the Durbin-Watson statistic is now above the upper limit. `[2.221, 2.242]`.
    Because the new statistic, `2.487`, is greater than `2.221`, we can confirm there
    is now negative autocorrelation in the model’s residuals. We can see based on
    the PACF plot, as well as the proximity to the critical value range, the autocorrelation
    is much less significant than before, but still present. It could be argued the
    autocorrelation of the residuals is now small enough to be considered resolved,
    however; as noted earlier, a Durbin-Watson statistic less than 2.5 can be considered
    normal. Nonetheless, least-squares regression remains risky compared to time-series
    modeling to generate our predictions. We begin an in-depth overview of this topic
    in [*Chapter 10*](B18945_10.xhtml#_idTextAnchor160)*, Introduction to* *Time Series*.
  prefs: []
  type: TYPE_NORMAL
- en: Let us assume, however, that for the sake of moving through the steps of regression
    model validation, we have no autocorrelation in our data. The next step will be
    to test our model.
  prefs: []
  type: TYPE_NORMAL
- en: Model validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assuming an analyst has developed a model that produces strong results and appears
    useful based on the previously mentioned plots, the next step is to test the model.
    One conventional approach is to perform a train-and-test split on the data used
    for the original model where we fit all data. In the following code, we run a
    split with the train dataset having 75% of the data and the test dataset having
    25%. The purpose is to assess whether there is a significant difference in performance
    between the two, as well as compared to the original model. The process follows
    the same steps as earlier. Acceptable differences are up to the analyst to decide,
    but in addition to assessing the differences between the metrics already discussed,
    the analyst should also note the coefficients for the slope and input variable,
    as these will be used to predict the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we split the data. We use the `shuffle` argument to *randomly* shuffle
    the data so that if there is some order to the data, such as it being ordered
    by time, the data will be split randomly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we build a model using the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we build a model using the testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The idea here is that the two models should produce similar results on two different
    partitions of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method for validating the model is to compare a metric against a naïve
    model. Let us use the **mean absolute error** (**MAE**). With this model metric,
    we could compare a naïve linear model’s errors where the error is each data point
    minus the average to the model’s predictions. A useful model MAE would need to
    be lower than the naïve model’s MAE. Here, we use the fit model to predict the
    inputs, then compare the model’s predictions to the naïve model’s predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Below we can observe the MAE from the *trained* model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see the MAE from the *naïve* model, which makes no assumptions
    other than that the mean will continue to hold the true values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can see the model compares favorably against the naïve model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method that is popular to use is testing on a holdout dataset. For
    this, a model is constructed and trained on the training data. Then, using that
    model, we apply it to the test data. We would then compare the metric to that
    of the training data. Using the training model’s MAE of 438, we compare it to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the error is lower for the test data than for the train data.
    However, since the train and test data are assumed to have different values and
    thus means, we should reassess that the naïve model still produces a higher MAE
    for the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here we can see the model does provide better MAE than the naïve model at approximately
    the same rate on the test data as on the train data. Therefore, we expect that
    the linear regression model is better than the naïve model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed an overview of simple linear regression between
    one explanatory variable and one response variable. The topics we covered include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The OLS method for simple linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coefficients of correlation and determination and their calculations and significance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The assumptions required for least squares regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods of analysis for model and parameter significance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We looked closely at the concept of the square of error and how the sum of squared
    errors is meaningful for building and validating linear regression models. Then,
    we walked through the four pertinent assumptions required to make linear regression
    a stable solution. After, we provided an overview of four diagnostic plots and
    their interpretations with respect to assessing the presence of various issues
    related to heteroscedasticity, linearity, outliers, and serial correlation. We
    then walked through an example of using the ACF and PACF to assess serial correlation
    and an example of using first-order differencing to remove serial correlation
    constraints from data and build an OLS model using the differenced data. Finally,
    we provided methods for testing and validating least square regression models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will extend this concept to include more than one explanatory
    variable, a technique called multiple linear regression. We will also discuss
    various topics related to multiple linear regression, such as variable selection
    and regularization.
  prefs: []
  type: TYPE_NORMAL
