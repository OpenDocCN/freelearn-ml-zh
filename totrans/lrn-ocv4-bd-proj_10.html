<html><head></head><body>
        

                            
                    <h1 class="header-title">Developing Segmentation Algorithms for Text Recognition</h1>
                
            
            
                
<p>In the previous chapters, we learned about a wide range of image processing techniques such as thresholding, contours descriptors, and mathematical morphology. In this chapter, we will discuss common problems that you may face while dealing with scanned documents, such as identifying where the text is or adjusting its rotation. We will also learn how to combine techniques presented in the previous chapters to solve those problems. By the end of this chapter, we will have segmented regions of text that can be sent to an <strong>optical character recognition</strong> (<strong>OCR</strong>) library.</p>
<p>By the end of this chapter, you should be able to answer the following questions:</p>
<ul>
<li>What kind of OCR applications exists?</li>
<li>What are the common problems while writing an OCR application?</li>
<li>How do I identify regions of documents?</li>
<li>How do I deal with problems like skewing and other elements in the middle of the text?</li>
<li>How do I use Tesseract OCR to identify my text?</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>This chapter requires familiarity with the basic C++ programming language. All of the code that's used in this chapter can be downloaded from the following GitHub link:<a href="https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_10">https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_10</a>. The code can be executed on any operating system, though it has only been tested on Ubuntu.</p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2KIoJFX">http://bit.ly/2KIoJFX</a></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introducing optical character recognition</h1>
                
            
            
                
<p>Identifying text in an image is a very popular application for computer vision. This process is commonly called <strong>optical character recognition</strong>, and is divided as follows:</p>
<ul>
<li><strong>Text preprocessing and segmentation</strong>: During this step, the computer must deal with image noise, and rotation (skewing), and identify what areas are candidate text.</li>
<li><strong>Text identification</strong>: This is the process of identifying each letter in text. Although this is also a computer vision topic, we will not show how you to do this in this book purely using OpenCV. Instead, we will show you how to use the Tesseract library to do this step, since it was integrated in OpenCV 3.0. If you are interested in learning how to do what Tesseract does by yourself, take a look at Packt's <em>Mastering OpenCV</em> book, which presents a chapter on car plate recognition.</li>
</ul>
<p>The preprocessing and segmentation phase can vary greatly depending on the source of the text. Let's take a look at common situations where preprocessing is done:</p>
<ul>
<li><strong>Production OCR applications with a scanner</strong>: This is a very reliable source of text. In this scenario, the background of the image is usually white and the document is almost aligned with the scanner margins. The content that's being scanned contains basically text, with almost no noise. This kind of application relies on simple preprocessing techniques that can adjust text quickly and maintain a fast scanning pace. When writing production OCR software, it is common to delegate the identification of important text regions to the user, and create a quality pipeline for text verification and indexing.</li>
<li><strong>Scanning text in a casually taken picture or in a video</strong>: This is a much more complex scenario, since there's no indication of where the text can be. This scenario is called <strong>scene text recognition</strong>, and OpenCV 4.0 contains a contrib library to deal with it. We will cover this in <a href="e39c0201-3568-4793-911b-9af5d1883d66.xhtml">Chapter 11</a>, <em>Text Recognition with Tesseract</em>. Usually, the preprocessor will use texture analysis techniques to identify the text patterns.</li>
<li><strong>Creating a production quality OCR for historical texts</strong>: Historical texts are also scanned, but they have several additional problems, such as noise that's created by the old paper color and the use of ink. Other common problems are decorated letters and specific text fonts, and low contrast content that's created by ink that is erased over time. It's not uncommon to write specific OCR software for the documents at hand.</li>
<li><strong>Scanning maps</strong>, <strong>diagrams</strong>, <strong>and charts</strong>: Maps, diagrams, and charts pose an especially difficult scenario since the text is usually in any orientation and in the middle of image content. For example, city names are often clustered, and ocean names often follow country shore contour lines. Some charts are heavily colored, with text appearing in both clear and dark tones.</li>
</ul>
<p>OCR application strategies also vary according to the objective of the identification. Will it be used for a full text search? Or should the text be separated into logical fields to index a database with information for a structured search?</p>
<p>In this chapter, we will focus on preprocessing scanned text, or text that's been photographed by a camera. We'll consider that the text is the main purpose of the image, such as in a photographed piece of paper or card, for example, in this parking ticket:</p>
<div><img class="alignnone size-full wp-image-473 image-border" src="img/33c9ec1a-b339-43aa-ba68-b3a2425be2a6.png" style="width:14.67em;height:19.83em;"/></div>
<p>We'll try to remove common noise, deal with text rotation (if any), and crop the possible text regions. While most OCR APIs already do these things automatically – and probably with state-of-the-art algorithms—it is still worth knowing how things happen under the hood. This will allow you to better understand most OCR APIs parameters and will give you better knowledge about the potential OCR problems you may face.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preprocessing stage</h1>
                
            
            
                
<p>Software that identifies letters does so by comparing text with previously recorded data. Classification results can be improved greatly if the input text is clear, if the letters are in a vertical position, and if there's no other elements, such as images sent to the classification software. In this section, we'll learn how to adjust text by using <strong>preprocessing</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Thresholding the image</h1>
                
            
            
                
<p>We usually start preprocessing by thresholding the image. This eliminates all color information. Most OpenCV functions consider information to be written in white, and the background to be black. So, let's start by creating a threshold function to match this criteria:</p>
<pre>#include opencv2/opencv.hpp; 
#include vector; 
 
using namespace std; 
using namespace cv; 
 
Mat binarize(Mat input)  
{   
   //Uses otsu to threshold the input image 
   Mat binaryImage; 
   cvtColor(input, input, COLOR_BGR2GRAY); 
   threshold(input, binaryImage, 0, 255, THRESH_OTSU); 
 
   //Count the number of black and white pixels 
   int white = countNonZero(binaryImage); 
   int black = binaryImage.size().area() - white; 
 
   //If the image is mostly white (white background), invert it 
   return white black ? binaryImage : ~binaryImage; 
}</pre>
<p>The <kbd>binarize</kbd> function applies a threshold, similar to what we did in <a href="ceaab6b4-2f4a-45e4-9f5d-2544c75bd405.xhtml">Chapter 4</a>, <em>Delving into Histogram and Filters</em>. But here, we will use the Otsu method by passing <kbd>THRESH_OTSU</kbd> in the fourth parameter of the function. The Otsu method maximizes inter-class variance. Since a threshold creates only two classes (the black and white pixels), this is the same as minimizing the intraclass variance. This method works using the image histogram. Then, it iterates through all the possible threshold values and calculates the spread for the pixel values for each side of the threshold, that is, the pixels that are either in the background or in the foreground of the image. The purpose of this process is to find the threshold value where the sum of both spreads are at their minimum.</p>
<p>After the thresholding is done, the function counts how many white pixels are in the image. The black pixels are simply the total number of pixels in the image, given by the image area, minus the white pixel count. Since text is usually written over a plain background, we will verify whether there are more white pixels than there are black pixels. In this case, we are dealing with black text over a white background, so we will invert the image for further processing.</p>
<p>The result of the thresholding process with the parking ticket image is as follows:</p>
<div><img class="alignnone size-full wp-image-474 image-border" src="img/49e0e74e-2cf2-436f-9f97-bdfabcace97c.png" style="width:47.00em;height:24.33em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Text segmentation</h1>
                
            
            
                
<p>The next step is to find where the text is located and extract it. There are two common strategies for this:</p>
<ul>
<li><strong>Using connected component analysis</strong>: Searching groups of connected pixels in the image. This will be the technique that will be used in this chapter.</li>
<li><strong>Use classifiers to search for a previously trained letter texture pattern</strong>: with texture features such as <strong>Haralick </strong>features, wavelet transforms are often used. Anther option is to identify <strong>maximally stable extremal regions</strong> (<strong>MSER</strong>s) in this task. This approach is more robust for text in a complex background and will be studied in <a href="e39c0201-3568-4793-911b-9af5d1883d66.xhtml">Chapter 11</a>, <em>Text Recognition with Tesseract</em>. You can read about Haralick features at his own website, which can be found at <a href="http://haralick.org/journals/TexturalFeatures.pdf">http://haralick.org/journals/TexturalFeatures.pdf</a>.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating connected areas</h1>
                
            
            
                
<p>If you take a closer look at the image, you'll notice that the letters are always together in blocks, formed by text paragraphs. That leaves us with the question, how do we detect and remove these blocks?</p>
<p>The first step is to make these blocks even more evident. We can do this by using the dilation morphological operator. Recall from <a href="58a72603-be5a-465f-aa7b-fc8ab1aae596.xhtml">Chapter 8</a>, <em>Video Surveillance</em>, <em>Background Modeling</em>, <em>and Morphological Operations</em>, that dilation makes the image elements thicker. Let's look at a small code snippet that does the trick:</p>
<pre>auto kernel = getStructuringElement(MORPH_CROSS, Size(3,3)); 
Mat dilated; 
dilate(input, dilated, kernel, cv::Point(-1, -1), 5); 
imshow("Dilated", dilated); </pre>
<p>In the preceding code, we start by creating a 3 x 3 cross kernel that will be used in the morphological operation. Then, we apply dilation five times, centered on this kernel. The exact kernel size and number of times vary according to the situation. Just make sure that the values glue all of the letters in the same line together.</p>
<p>The result of this operation is presented in the following screenshot:</p>
<div><img class="alignnone size-full wp-image-475 image-border" src="img/3e7f8e70-3d29-4e4d-9a76-15df7d9db8e7.png" style="width:35.67em;height:37.17em;"/></div>
<p>Notice that we now have huge white blocks. They match exactly with each paragraph of text, and also match with other non-textual elements, like images or the border noise.</p>
<p>The ticket image that comes with the code is a low resolution image. OCR engines usually work with high resolution images (200 or 300 DPI), so it may be necessary to apply dilation more than five times.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Identifying paragraph blocks</h1>
                
            
            
                
<p>The next step is to perform connect component analysis to find blocks that correspond with paragraphs. OpenCV has a function for this, which we previously used in <a href="b788527c-5892-4547-8add-0864ccbd3f95.xhtml">Chapter 5</a>, <em>Automated Optical Inspection</em>, <em>Object Segmentation</em>, <em>and Detection</em>. This is the <kbd>findContours</kbd> function:</p>
<pre>vector;vector;Point;contours; 
findContours(dilated, contours, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);  </pre>
<p>In the first parameter, we pass our dilated image. The second parameter is the vector of detected contours. Then, we use the option to retrieve only external contours and to use simple approximation. The image contours are presented as follows. Each tone of gray represents a different contour:</p>
<div><img class="alignnone size-full wp-image-476 image-border" src="img/77e6041a-d703-4078-ab93-1e24f2a6c85e.png" style="width:32.58em;height:33.83em;"/></div>
<p>The last step is to identify the minimum rotated bounding rectangle of each contour. OpenCV provides a handy function for this operation called <kbd>minAreaRect</kbd>. This function receives a vector of arbitrary points and returns a <kbd>RoundedRect</kbd> containing the bounding box. This is also a good opportunity to discard unwanted rectangles, that is, rectangles that are obviously not text. Since we are making software for OCR, we'll assume that the text contains a group of letters. With this assumption, we'll discard text in the following situations:</p>
<ul>
<li>The rectangle width or size is too small, that is, smaller than 20 pixels. This will help discard border noises and other small artifacts.</li>
<li>The rectangle of the image has a width/height proportion smaller than two. That is, rectangles that resemble a square, such as the image icons, or are much taller, will also be discarded.</li>
</ul>
<p>There's a little caveat in the second condition. Since we are dealing with rotated bounding boxes, we must test whether the bounding box angle is smaller than -45 degrees. If it is, the text is vertically rotated, so the proportion that we must take into account is height/width.</p>
<p>Let's check this out by looking at the following code:</p>
<pre>//For each contour 

vector;RotatedRect; areas; 
for (const auto&amp; contour : contours)  
{   
   //Find it's rotated rect 
   auto box = minAreaRect(contour); 
 
   //Discard very small boxes 
   if (box.size.width 20 || box.size.height 20) 
         continue; 
 
   //Discard squares shaped boxes and boxes  
   //higher than larger 
   double proportion = box.angle -45.0 ? 
         box.size.height / box.size.width :  
         box.size.width / box.size.height; 
 
   if (proportion 2)  
         continue; 
 
   //Add the box 
   areas.push_back(box); 
}</pre>
<p>Let's see which boxes this algorithm selected:</p>
<div><img class="alignnone size-full wp-image-477 image-border" src="img/740e6bb3-bd1c-4304-b490-fb7e2711ce0e.png" style="width:67.00em;height:66.33em;"/></div>
<p>That's certainly a good result!</p>
<p>We should notice that the algorithm described in step 2, in the preceding code, will also discard single letters. This is not a big issue since we are creating an OCR preprocessor, and single symbols are usually meaningless with context information; one example of such a case is the page numbers. The page numbers will be discarded with this process since they usually appear alone at the bottom of the page, and the size and proportion of the text will also be disturbed. But this will not be a problem, since after the text passes through the OCR, you will end up with a huge amount of text files with no page division at all.</p>
<p>We'll place all of this code in a function with the following signature:</p>
<pre>vector RotatedRect; findTextAreas(Mat input)</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Text extraction and skewing adjustment</h1>
                
            
            
                
<p>Now, all we must do is extract the text and adjust the text skew. This is done by the <kbd>deskewAndCrop</kbd> function, as follows:</p>
<pre>Mat deskewAndCrop(Mat input, const RotatedRect&amp; box) 
{ 
   double angle = box.angle;      
   auto size = box.size; 
 
   //Adjust the box angle 
   if (angle -45.0)  
   { 
        angle += 90.0;<br/>         std::swap(size.width, size.height);         
   } 
    
   //Rotate the text according to the angle 
   auto transform = getRotationMatrix2D(box.center, angle, 1.0); 
   Mat rotated; 
   warpAffine(input, rotated, transform, input.size(), INTER_CUBIC); 
 
   //Crop the result 
   Mat cropped; 
   getRectSubPix(rotated, size, box.center, cropped); 
   copyMakeBorder(cropped,cropped,10,10,10,10,BORDER_CONSTANT,Scalar(0)); 
   return cropped; 
}</pre>
<p>First, we start by reading the desired region angle and size. As we saw earlier, the angle may be less than -45 degrees. This means that the text is vertically aligned, so we must add 90 degrees to the rotation angle and switch the width and height properties. Next, we need to rotate the text. First, we start by creating a 2D affine transformation matrix that describes the rotation. We do so by using the <kbd>getRotationMatrix2D</kbd> OpenCV function. This function takes three parameters:</p>
<ul>
<li><strong>CENTER</strong>: The central position of the rotation. The rotation will pivot around this center. In our case, we use the box center.</li>
<li><strong>ANGLE</strong>: The rotation angle. If the angle is negative, the rotation will occur in a clockwise direction.</li>
<li><strong>SCALE</strong>: The isotropic scale factor. We will use <kbd>1.0</kbd> since we want to keep the box's original scale untouched.</li>
</ul>
<p>The rotation itself is made by using the <kbd>warpAffine</kbd> function. This function takes four mandatory arguments:</p>
<ul>
<li><strong>SRC</strong>: The input <kbd>mat</kbd> array to be transformed.</li>
<li><strong>DST</strong>: The destination <kbd>mat</kbd> array.</li>
<li><strong>M</strong>: A transformation matrix. This matrix is a 2 x 3 affine transformation matrix. This may be a translation, scale, or rotation matrix. In our case, we will just use the matrix we recently created.</li>
<li><strong>SIZE</strong>: The size of the output image. We will generate an image that's the same size as our input image.</li>
</ul>
<p>The following are another three optional arguments:</p>
<ul>
<li><strong>FLAGS</strong>: These indicate how the image should be interpolated. We use <kbd>BICUBIC_INTERPOLATION</kbd> for better quality. The default is <kbd>LINEAR_INTERPOLATION</kbd>.</li>
<li><strong>BORDER</strong>: Border mode. We use the default, <kbd>BORDER_CONSTANT</kbd>.</li>
<li><strong>BORDER VALUE</strong>: The color of the border. We use the default, which is black. Then, we use the <kbd>getRectSubPix</kbd> function. After we rotate our image, we need to crop the rectangle area of our bounding box. This function takes four mandatory arguments and one optional argument, and returns the cropped image:
<ul>
<li><strong>IMAGE</strong>: The image to crop.</li>
<li><strong>SIZE</strong>: A <kbd>cv::Size</kbd> object describing the width and height of the box to be cropped.</li>
<li><strong>CENTER</strong>: The central pixel of the area to be cropped. Notice that since we rotated around the center, this point is conveniently the same.</li>
<li><strong>PATCH</strong>: The destination image.</li>
<li><strong>PATCH_TYPE</strong>: The depth of the destination image. We use the default value, representing the same depth of the source image.</li>
</ul>
</li>
</ul>
<p>The final step is done by the <kbd>copyMakeBorder</kbd> function. This function adds a border around the image. This is important, since the classification stage usually expects a margin around the text. The function parameters are very simple: the input and output images, the border thickness at the top, bottom, left, and right, the border type, and the color of the new border.</p>
<p>For the card image, the following images will be generated:</p>
<div><img class="alignnone size-full wp-image-478 image-border" src="img/4fb5fee5-8e34-40b6-b20c-2544729a0efc.png" style="width:42.08em;height:31.92em;"/></div>
<p>Now, it's time to put every function together. Let's present the main method that does the following:</p>
<ul>
<li>Loads the ticket image</li>
<li>Calls our binarization function</li>
<li>Find all text regions</li>
<li>Shows each region in a window</li>
</ul>
<p>We will present the main method as follows:</p>
<pre>int main(int argc, char* argv[])  
{ 
   //Loads the ticket image and binarize it 
   auto ticket = binarize(imread("ticket.png"));     
   auto regions = findTextAreas(ticket); 
    
   //For each region 
   for (const auto&amp; region : regions) { 
         //Crop  
         auto cropped = deskewAndCrop(ticket, region); 
 
         //Show 
         imshow("Cropped text", cropped); 
         waitKey(0);  
         destroyWindow("Border Skew"); 
   } 
} </pre>
<p>For the complete source code, take a look at the <kbd>segment.cpp</kbd> file that comes with this book.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Tesseract OCR on your operating system</h1>
                
            
            
                
<p>Tesseract is an open source OCR engine that was originally developed by Hewlett-Packard Laboratories Bristol and Hewlett-Packard Co. All of its code is licensed under the Apache License and hosted on GitHub at <a href="https://github.com/tesseract-ocr">https://github.com/tesseract-ocr</a>. It is considered one of the most accurate OCR engines available: it can read a wide variety of image formats and can convert text written in more than 60 languages. In this session, we will teach you how to install Tesseract on Windows or Mac. Since there's lots of Linux distributions, we will not teach you how to install it on this operating system. Normally, Tesseract offers installation packages in your package repository, so, before compiling Tesseract yourself, just search for it there.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Tesseract on Windows</h1>
                
            
            
                
<p>Tesseract uses the <strong>C++ Archive Network</strong> (<strong>CPPAN</strong>) as its dependency manager. To install Tesseract, follow these steps.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Building the latest library</h1>
                
            
            
                
<ol>
<li>Download the latest CPPAN client from <a href="https://cppan.org/client/">https://cppan.org/client/</a>.</li>
<li>In the command line, run<br/>
<kbd>cppan --build pvt.cppan.demo.google.tesseract.tesseract-master</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up Tesseract in Visual Studio</h1>
                
            
            
                
<ol>
<li>Set up <kbd>vcpkg</kbd>,the Visual C++ Package Manager, at <a href="https://github.com/Microsoft/vcpkg">https://github.com/Microsoft/vcpkg</a>.</li>
<li>For a 64-bit compilation, use <kbd>vcpkg install tesseract:x64-windows</kbd>. You may also add <kbd>--head</kbd> for the master branch.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Static linking</h1>
                
            
            
                
<p>It's also possible to static link (<a href="https://github.com/tesseract-ocr/tesseract/wiki/Compiling#static-linking">https://github.com/tesseract-ocr/tesseract/wiki/Compiling#static-linking</a>) Tesseract in your project. This will avoid <kbd>dlls</kbd> to be packaged with your executable files. To do this, use <kbd>vcpkg</kbd>, like we did previously, with the following command for a 32-bit installation:</p>
<pre>vcpkg install tesseract:x86-windows-static</pre>
<p>Alternatively, you can use the following command for a 64-bit installation:</p>
<pre>vckpg install tesseract:x64-windows-static</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Tesseract on Mac</h1>
                
            
            
                
<p>The easiest way to install Tesseract OCR on Mac is using <strong>Homebrew</strong>. If you don't have Homebrew installed, just go to Homebrew's site (<a href="http://brew.sh/">http://brew.sh/</a>), open your console, and run the <strong>Ruby script</strong> that is on the front page. You may be required to type in your administrator password.</p>
<p>After Homebrew is installed, just type in the following:</p>
<pre><strong>brew install tesseract</strong></pre>
<p>The English language is already included in this installation. If you want to install other language packs, just run the following command:</p>
<pre><strong>brew install tesseract --all-languages </strong></pre>
<p>This will install all of the language packs. Then, just go to the Tesseract installation directory and delete any unwanted languages. Homebrew usually installs stuff in the <kbd>/usr/local/</kbd> directory.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using the Tesseract OCR library</h1>
                
            
            
                
<p>While Tesseract OCR is already integrated with OpenCV 3.0, it's still worth studying its API since it allows for finer grained control over Tesseract parameters. This integration will be studied in <a href="e39c0201-3568-4793-911b-9af5d1883d66.xhtml">Chapter 11</a>, <em>Text Recognition with Tesseract</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating an OCR function</h1>
                
            
            
                
<p>We'll change the previous example to work with Tesseract. Start by adding <kbd>tesseract/baseapi.h</kbd> and <kbd>fstream</kbd> to the <kbd>include</kbd> list:</p>
<pre>#include opencv2/opencv.hpp; 
#include tesseract/baseapi.h; 
 
#include vector; 
#include fstream; </pre>
<p>Then, we'll create a global <kbd>TessBaseAPI</kbd> object that represents our Tesseract OCR engine:</p>
<pre>tesseract::TessBaseAPI ocr; </pre>
<p>The <kbd>ocr</kbd> engine is completely self-contained. If you want to create a multi-threaded piece of OCR software, just add a different <kbd>TessBaseAPI</kbd> object in each thread, and the execution will be fairly thread-safe. You just need to guarantee that file writing is not done over the same file, otherwise you'll need to guarantee safety for this operation.</p>
<p>Next, we will create a function called <strong>identify text </strong>(<kbd>identifyText</kbd>) that will run the <kbd>ocr</kbd>:</p>
<pre>const char* identifyText(Mat input, const char* language = "eng")  
{   
   ocr.Init(NULL, language, tesseract::OEM_TESSERACT_ONLY);     
   ocr.SetPageSegMode(tesseract::PSM_SINGLE_BLOCK); 
   ocr.SetImage(input.data, input.cols, input.rows, 1, input.step); 
    
   const char* text = ocr.GetUTF8Text(); 
   cout  "Text:"  endl; 
   cout  text  endl; 
   cout  "Confidence: "  ocr.MeanTextConf() endl; 
    
    // Get the text     
   return text; 
} </pre>
<p>Let's explain this function line-by-line. In the first line, we start by initializing <kbd>tesseract</kbd>. This is done by calling the <kbd>Init</kbd> function. This function has the following signature:</p>
<pre>int Init(const char* datapath, const char* language, <br/> OcrEngineMode oem)</pre>
<p>Let's explain each parameter:</p>
<ul>
<li><kbd>datapath</kbd>: This is the path to the root directory of <kbd>tessdata</kbd> files. The path must end with a backslash <kbd>/</kbd> character. The <kbd>tessdata</kbd> directory contains the language files that you installed. Passing <kbd>NULL</kbd> to this parameter will make <kbd>tesseract</kbd> search its installation directory, which is the location that this folder is normally present in. It's common to change this value to <kbd>args[0]</kbd> when deploying an application, and include the <kbd>tessdata</kbd> folder in your application path.</li>
<li><kbd>language</kbd>: This is a three letter word for the language code (for example, eng for English, por for Portuguese, or hin for Hindi). Tesseract supports loading multiple language codes by using the <kbd>+</kbd> sign. Therefore, passing <kbd>eng+por</kbd> will load both the English and Portuguese languages. Of course, you can only use languages you have previously installed, otherwise the loading process will fail. A language config file may specify that two or more languages must be loaded together. To prevent that, you may use a tilde <kbd>~</kbd>. For example, you can use <kbd>hin+~eng</kbd> to guarantee that English is not loaded with Hindi, even if it is configured to do so.</li>
<li><kbd>OcrEngineMode</kbd>: These are the OCR algorithms that will be used. It can have one of the following values:
<ul>
<li><kbd>OEM_TESSERACT_ONLY</kbd>: Uses just <kbd>tesseract</kbd>. It's the fastest method, but it also has less precision.</li>
<li><kbd>OEM_CUBE_ONLY</kbd>: Uses the Cube engine. It's slower, but more precise. This will only work if your language was trained to support this engine mode. To check if that's the case, look for <kbd>.cube</kbd> files for your language in the <kbd>tessdata</kbd> folder. The support for English language is guaranteed.</li>
<li><kbd>OEM_TESSERACT_CUBE_COMBINED</kbd>: This combines both Tesseract and Cube to achieve the best possible OCR classification. This engine has the best accuracy and the slowest execution time.</li>
<li><kbd>OEM_DEFAULT</kbd>: This infers the strategy based on the language config file, command-line config file or, in the absence of both, uses <kbd>OEM_TESSERACT_ONLY</kbd>.</li>
</ul>
</li>
</ul>
<p>It's important to emphasize that the <kbd>Init</kbd> function can be executed many times. If a different language or engine mode is provided, Tesseract will clear the previous configuration and start again. If the same parameters are provided, Tesseract is smart enough to simply ignore the command. The <kbd>init</kbd> function returns <kbd>0</kbd> in case of success and <kbd>-1</kbd> in case of failure.</p>
<p>Our program will then proceed by setting the page segmentation mode:</p>
<pre>ocr.SetPageSegMode(tesseract::PSM_SINGLE_BLOCK); </pre>
<p>There are several segmentation modes available:</p>
<ul>
<li><kbd>PSM_OSD_ONLY</kbd>: Using this mode, Tesseract will just run its preprocessing algorithms to detect orientation and script detection.</li>
<li><kbd>PSM_AUTO_OSD</kbd>: This tells Tesseract to do automatic page segmentation with orientation and script detection.</li>
<li><kbd>PSM_AUTO_ONLY</kbd>: This does page segmentation, but avoids doing orientation, script detection, or OCR.</li>
<li><kbd>PSM_AUTO</kbd>: This does page segmentation and OCR, but avoids doing orientation or script detection.</li>
<li><kbd>PSM_SINGLE_COLUMN</kbd>: This assumes that the text of variable sizes is displayed in a single column.</li>
<li><kbd>PSM_SINGLE_BLOCK_VERT_TEXT</kbd>: This treats the image as a single uniform block of vertically aligned text.</li>
<li><kbd>PSM_SINGLE_BLOCK</kbd>: This assumes a single block of text, and is the default configuration. We will use this flag since our preprocessing phase guarantees this condition.</li>
<li><kbd>PSM_SINGLE_LINE</kbd>: Indicates that the image contains only one line of text.</li>
<li><kbd>PSM_SINGLE_WORD</kbd>: Indicates that the image contains just one word.</li>
<li><kbd>PSM_SINGLE_WORD_CIRCLE</kbd>: Informs us that the image is a just one word disposed in a circle.</li>
<li><kbd>PSM_SINGLE_CHAR</kbd>: Indicates that the image contains a single character.</li>
</ul>
<p>Notice that Tesseract already has <strong>deskewing</strong> and text segmentation algorithms implemented, just like most OCR libraries do. But it's interesting to know of such algorithms since you may provide your own preprocessing phase for specific needs. This allows you to improve text detection in many cases. For example, if you are creating an OCR application for old documents, the default threshold used by Tesseract may create a dark background. Tesseract may also be confused by borders or severe text skewing.</p>
<p>Next, we call the <kbd>SetImage</kbd> method with the following signature:</p>
<pre>void SetImage(const unsigned char* imagedata, int width, <br/> int height, int bytes_per_pixel, int bytes_per_line);</pre>
<p>The parameters are almost self-explanatory, and most of them can be read directly from our <kbd>Mat</kbd> object:</p>
<ul>
<li><kbd>data</kbd>: A raw byte array containing image data. OpenCV contains a function called <kbd>data()</kbd> in the <kbd>Mat</kbd> class that provides a direct pointer to the data.</li>
<li><kbd>width</kbd>: Image width.</li>
<li><kbd>height</kbd>: Image height.</li>
<li><kbd>bytes_per_pixel</kbd>: Number of bytes per pixel. We are using <kbd>1</kbd>, since we are dealing with a binary image. If you want the code to be more generic, you could also use the <kbd>Mat::elemSize()</kbd> function, which provides the same information.</li>
<li><kbd>bytes_per_line</kbd>: Number of bytes in a single line. We are using the <kbd>Mat::step</kbd> property since some images add trailing bytes.</li>
</ul>
<p>Then, we call <kbd>GetUTF8Text</kbd> to run the recognition itself. The recognized text is returned, encoded with UTF8 and without BOM. Before returning it, we also print some debug information.</p>
<p><kbd>MeanTextConf</kbd> returns a confidence index, which may by a number from <kbd>0</kbd> to <kbd>100</kbd>:</p>
<pre>   auto text = ocr.GetUTF8Text(); 
   cout  "Text:"  endl; 
   cout  text  endl; 
   cout  "Confidence: "  ocr.MeanTextConf()  endl; </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Sending the output to a file</h1>
                
            
            
                
<p>Let's change our main method to send the recognized output to a file. We do this by using a standard <kbd>ofstream</kbd>:</p>
<pre>int main(int argc, char* argv[])  
{ 
   //Loads the ticket image and binarize it 
   Mat ticket = binarize(imread("ticket.png"));     
   auto regions = findTextAreas(ticket); 
 
   std::ofstream file;  
   file.open("ticket.txt", std::ios::out | std::ios::binary); 
 
   //For each region 
   for (const auto&amp; region : regions) { 
         //Crop  
         auto cropped = deskewAndCrop(ticket, region); 
         auto text = identifyText(cropped, "por"); 
          
         file.write(text, strlen(text)); 
         file endl; 
   } 
    
   file.close(); 
} </pre>
<p>The following line opens the file in binary mode:</p>
<pre>file.open("ticket.txt", std::ios::out | std::ios::binary); </pre>
<p>This is important since Tesseract returns text encoded in UTF-8, taking into account special characters that are available in Unicode. We also write the output directly using the following command:</p>
<pre>file.write(text, strlen(text)); </pre>
<p>In this sample, we called the <kbd>identify</kbd> function using Portuguese as an input language (this is the language the ticket was written in). You may use another photo, if you like.</p>
<p>The complete source file is provided in the <kbd>segmentOcr.cpp</kbd> file, which comes with this book.</p>
<div><kbd>ticket.png</kbd> is a low resolution image, since we imagined you would want to display a window with the image while studying this code. For this image, the Tesseract results are rather poor. If you want to test with a higher resolution image, the code for this book provides you with a <kbd>ticketHigh.png</kbd> image. To test with this image, change the dilation repetitions to <kbd>12</kbd> and the minimum box size from <kbd>20</kbd> to <kbd>60</kbd>. You'll get a much higher confidence rate (about 87%), and the resulting text will be almost fully readable. The <kbd>segmentOcrHigh.cpp</kbd> file contains these modifications.</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we presented a brief introduction to OCR applications. We saw that the preprocessing phase of such systems must be adjusted according to the type of document we are planning to identify. We have learned about common operations while preprocessing text files, such as thresholding, cropping, skewing, and text region segmentation. Finally, we learned how to install and use Tesseract OCR to convert our image into text.</p>
<p>In the next chapter, we'll use a more sophisticated OCR technique to identify text in a casually taken picture or video –a situation known as scene text recognition. This is a much more complex scenario, since the text can be anywhere, in any font, and with different illuminations and orientations. There can even be no text at all! We'll also learn how to use the OpenCV 3.0 text contribution module, which is fully integrated with Tesseract.</p>


            

            
        
    </body></html>