- en: Chapter 11. Stereo Vision and 3D Reconstruction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn about stereo vision and how we can reconstruct
    the 3D map of a scene. We will discuss epipolar geometry, depth maps, and 3D reconstruction.
    We will learn how to extract 3D information from stereo images and build a point
    cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: What is stereo correspondence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is epipolar geometry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a depth map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to extract 3D information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build and visualize the 3D map of a given scene
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is stereo correspondence?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we capture images, we project the 3D world around us on a 2D image plane.
    So technically, we only have 2D information when we capture those photos. Since
    all the objects in that scene are projected onto a flat 2D plane, the depth information
    is lost. We have no way of knowing how far an object is from the camera or how
    the objects are positioned with respect to each other in the 3D space. This is
    where stereo vision comes into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: Humans are very good at inferring depth information from the real world. The
    reason is that we have two eyes positioned a couple of inches from each other.
    Each eye acts as a camera and we capture two images of the same scene from two
    different viewpoints, that is, one image each using the left and right eyes. So,
    our brain takes these two images and builds a 3D map using stereo vision. This
    is what we want to achieve using stereo vision algorithms. We can capture two
    photos of the same scene using different viewpoints, and then match the corresponding
    points to obtain the depth map of the scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is stereo correspondence?](img/B04554_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we capture the same scene from a different angle, it will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is stereo correspondence?](img/B04554_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there is a large amount of movement in the positions of the
    objects in the image. If you consider the pixel coordinates, the values of the
    initial position and final position will differ by a large amount in these two
    images. Consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is stereo correspondence?](img/B04554_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we consider the same line of distance in the second image, it will look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is stereo correspondence?](img/B04554_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The difference between **d1** and **d2** is large. Now, let''s bring the box
    closer to the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is stereo correspondence?](img/B04554_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s move the camera by the same amount as we did earlier, and capture
    the same scene from this angle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is stereo correspondence?](img/B04554_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the movement between the positions of the objects is not much.
    If you consider the pixel coordinates, you will see that the values are close
    to each other. The distance in the first image would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is stereo correspondence?](img/B04554_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we consider the same line of distance in the second image, it will be as
    shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is stereo correspondence?](img/B04554_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The difference between **d3** and **d4** is small. We can say that the absolute
    difference between **d1** and **d2** is greater than the absolute difference between
    **d3** and **d4**. Even though the camera moved by the same amount, there is a
    big difference between the apparent distances between the initial and final positions.
    This happens because we can bring the object closer to the camera; the apparent
    movement decreases when you capture two images from different angles. This is
    the concept behind stereo correspondence: we capture two images and use this knowledge
    to extract the depth information from a given scene.'
  prefs: []
  type: TYPE_NORMAL
- en: What is epipolar geometry?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before discussing epipolar geometry, let''s discuss what happens when we capture
    two images of the same scene from two different viewpoints. Consider the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see how it happens in real life. Consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s capture the same scene from a different viewpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Our goal is to match the keypoints in these two images to extract the scene
    information. The way we do this is by extracting a matrix that can associate the
    corresponding points between two stereo images. This is called the **fundamental
    matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in the camera figure earlier, we can draw lines to see where they
    meet. These lines are called **epipolar lines**. The point at which the epipolar
    lines converge is called epipole. If you match the keypoints using SIFT, and draw
    the lines towards the meeting point on the left image, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following are the matching feature points in the right image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The lines are epipolar lines. If you take the second image as the reference,
    they will appear as shown in the next image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following are the matching feature points in the first image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s important to understand epipolar geometry and how we draw these lines.
    If two frames are positioned in 3D, then each epipolar line between the two frames
    must intersect the corresponding feature in each frame and each of the camera
    origins. This can be used to estimate the pose of the cameras with respect to
    the 3D environment. We will use this information later on, to extract 3D information
    from the scene. Let''s take a look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what happens if we use the **SURF** feature extractor. The lines
    in the left image will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following are the matching feature points in the right image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you take the second image as the reference, you will see something like
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These are the matching feature points in the first image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is epipolar geometry?](img/B04554_11_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Why are the lines different as compared to SIFT?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SURF detects a different set of feature points, so the corresponding epipolar
    lines differ as well. As you can see in the images, there are more feature points
    detected when we use SURF. Since we have more information than before, the corresponding
    epipolar lines will also change accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Building the 3D map
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we are familiar with epipolar geometry, let''s see how to use it to
    build a 3D map based on stereo images. Let''s consider the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the 3D map](img/B04554_11_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first step is to extract the disparity map between the two images. If you
    look at the figure, as we go closer to the object from the cameras along the connecting
    lines, the distance decreases between the points. Using this information, we can
    infer the distance of each point from the camera. This is called a depth map.
    Once we find the matching points between the two images, we can find the disparity
    by using epipolar lines to impose epipolar constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the 3D map](img/B04554_11_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we capture the same scene from a different position, we get the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the 3D map](img/B04554_11_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we reconstruct the 3D map, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the 3D map](img/B04554_11_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Bear in mind that these images were not captured using perfectly aligned stereo
    cameras. That''s the reason the 3D map looks so noisy! This is just to demonstrate
    how we can reconstruct the real world using stereo images. Let''s consider an
    image pair captured using stereo cameras that are properly aligned. Following
    is the left view image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the 3D map](img/B04554_11_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next is the corresponding right view image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the 3D map](img/B04554_11_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you extract the depth information and build the 3D map, it will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the 3D map](img/B04554_11_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s rotate it to see if the depth is right for the different objects in
    the scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the 3D map](img/B04554_11_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You need a software called **MeshLab** to visualize the 3D scene. We'll discuss
    about it soon. As we can see in the preceding images, the items are correctly
    aligned according to their distance from the camera. We can intuitively see that
    they are arranged in the right way, including the tilted position of the mask.
    We can use this technique to build many interesting things.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how to do it in OpenCV-Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To visualize the output, you need to download MeshLab from [http://meshlab.sourceforge.net](http://meshlab.sourceforge.net).
  prefs: []
  type: TYPE_NORMAL
- en: Just open the `output.ply` file using MeshLab and you'll see the 3D image. You
    can rotate it to get a complete 3D view of the reconstructed scene. Some of the
    alternatives to MeshLab are Sketchup on OS X and Windows, and Blender on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about stereo vision and 3D reconstruction. We discussed
    how to extract the fundamental matrix using different feature extractors. We learned
    how to generate the disparity map between two images, and use it to reconstruct
    the 3D map of a given scene.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss augmented reality, and how we can
    build a cool application where we overlay graphics on top of real world objects
    in a live video.
  prefs: []
  type: TYPE_NORMAL
