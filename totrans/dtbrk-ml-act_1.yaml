- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting Started with This Book and Lakehouse Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Give me six hours to chop down a tree, and I will spend the first four sharpening
    the axe.”
  prefs: []
  type: TYPE_NORMAL
- en: – Abraham Lincoln
  prefs: []
  type: TYPE_NORMAL
- en: We will start with a basic overview of how **Databrick**’s **Data Intelligence
    Platform** (**DI**) is an open platform on a **lakehouse** architecture and the
    advantages of this in developing **machine learning** (**ML**) applications. For
    brevity, we will use terms such as *Data Intelligence Platform* and *Databricks*
    interchangeably throughout the book. This chapter will introduce the different
    projects and associated datasets we’ll use throughout the book. Each project intentionally
    highlights a function or component of the DI Platform. Use the example projects
    as hands-on lessons for each platform element we cover. We progress through these
    projects in the last section of each chapter – namely, applying our learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what you will learn in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The components of the Data Intelligence Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of the Databricks Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The components of the Data Intelligence Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Data Intelligence Platform allows your entire organization to leverage
    your data and AI. It’s built on a lakehouse architecture to provide an open, unified
    foundation for all data and governance layers. It is powered by a **Data Intelligence
    Engine**, which understands the context of your data. For practical purposes,
    let’s talk about the components of the Databricks Data Intelligence Platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – The components of the Databricks Data Intelligence Platform](img/B16865_01_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – The components of the Databricks Data Intelligence Platform
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check out the following list with the descriptions of the items in the
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Delta Lake**: The data layout within the Data Intelligence Platform is automatically
    optimized based on common data usage patterns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unity Catalog**: A unified governance model to secure, manage, and share
    your data assets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Intelligence Engine**: This uses AI to enhance the platform’s capabilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Databricks AI**: ML tools to support end-to-end ML solutions and **generative
    AI** capabilities, including creating, tuning, and serving LLMs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Delta live tables**: Enables automated data ingestion and **data quality**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workflows**: A fully integrated orchestration service to automate, manage,
    and monitor multi-task workloads, queries, and pipelines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Databricks SQL (DBSQL)**: An SQL-first interface, similar to how you would
    interact with a data warehouse, and with functionality such as text-to-SQL, which
    lets you use natural language to generate queries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have our elements defined, let’s discuss how they help us achieve
    our ML goals.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of the Databricks Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Databricks’ implementation of a lakehouse architecture is unique. Databricks’
    foundation is built on a Delta-formatted data lake that Unity Catalog governs.
    Therefore, it combines a data lake’s scalability and cost-effectiveness with a
    data warehouse’s governance. This means not only are table-level permissions managed
    through **access control lists** (**ACLs**) but file and object-level access are
    also regulated. This change in architecture from a data lake and/or a data warehouse
    to a unified platform is ideal – a lakehouse facilitates a wide range of new use
    cases for analytics, business intelligence, and data science projects across an
    organization. See the *Introduction to Data Lakes* blog post in the *Further reading*
    section for more information on lakehouse benefits.
  prefs: []
  type: TYPE_NORMAL
- en: This section will discuss the importance of open source frameworks and two critical
    advantages they provide – transparency and flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Open source features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How open source features relate to the Data Intelligence Platform is unique.
    This uniqueness lies in the concepts of openness and transparency, often referred
    to as the “glass box” approach by Databricks. It means that when you use the platform
    to create assets, there’s no inscrutable black box that forces you to depend on
    a specific vendor for usage, understanding, or storage. A genuinely open lakehouse
    architecture uses open data file formats to make accessing, sharing, and removing
    your data simple. Databricks has optimized the managed version of Apache Spark
    to leverage the open data format Delta (which we’ll cover in more detail shortly).
    This is one of the reasons why the Delta format is ideal for most use cases. However,
    nothing stops you from using something such as the CSV or Parquet format. Furthermore,
    Databricks introduced **Delta Lake Universal Format** (**Delta Lake UniForm**)
    to easily integrate with other file formats such as Iceberg or Hudi. For more
    details, check out the *Further reading* section at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1**.2* illustrates the coming together of data formats with UniForm.'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 1.2 – Delta Lake UniForm makes consuming Hudi and Iceberg file formats\
    \ as easy as consumin\uFEFFg Delta](img/B16865_01_2.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Delta Lake UniForm makes consuming Hudi and Iceberg file formats
    as easy as consuming Delta
  prefs: []
  type: TYPE_NORMAL
- en: The ability to use third-party and open source software fuels rapid innovation.
    New advances in data processing and ML can be quickly tested and integrated into
    your workflow. In contrast, proprietary systems often have longer wait times for
    vendors to incorporate updates. Waiting for a vendor to capitalize on open source
    innovation may seem rare, but it is the rule rather than the exception. This is
    especially true for data science. The speed of software and algorithmic advances
    is incredible. Evidence of this frantic pace of innovation can be seen daily on
    the Hugging Face community website. Developers share libraries and models on Hugging
    Face; hundreds of libraries are updated daily on the site alone.
  prefs: []
  type: TYPE_NORMAL
- en: Delta, Spark, the Pandas API on Spark (see *Figure 1**.3*), and MLflow are notable
    examples of consistent innovation, largely driven by their transparency as open
    source projects. We mention these specifically because they were all initially
    created by either the founders of Databricks or company members following its
    formation.
  prefs: []
  type: TYPE_NORMAL
- en: ML developers benefit significantly from this transparency, as it provides them
    with unparalleled flexibility, easy integration, and robust support from the open
    source community – all without the overhead of maintaining an open source full
    stack.
  prefs: []
  type: TYPE_NORMAL
- en: Starting development as a contractor using Databricks is super-fast compared
    to when companies require a fresh development environment to be set up. Some companies
    require a service request to install Python libraries. This can be a productivity
    killer for data scientists. In Databricks, many of your favorite libraries are
    pre-installed and ready to use, and of course, you can easily install your own
    libraries as well.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there is a large and vibrant community of Databricks users. The
    Databricks community website is an excellent resource to ask and answer questions
    about anything related to Databricks. We’ve included a link in the *Further reading*
    section at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – The pandas API on Spark](img/B16865_01_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – The pandas API on Spark
  prefs: []
  type: TYPE_NORMAL
- en: The pandas API on Spark is nearly identical syntax to standard pandas, making
    distributed computing with Spark easier to learn for those who have written pandas
    code in Python
  prefs: []
  type: TYPE_NORMAL
- en: While continuing with a focus on transparency, let’s move on to Databricks **AutoML**.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks AutoML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Databricks refers to its AutoML solution as a **glass box**. This terminology
    highlights the fact that there is nothing hidden from the user. This feature in
    the Data Intelligence Platform leverages an open source library, Hyperopt, in
    conjunction with Spark for hyperparameter tuning. It intelligently explores different
    model types in addition to optimizing the parameters in a distributed fashion.
    The use of Hyperopt allows each run within the AutoML experiment to inform the
    next run, reducing the overall number of runs needed to reach an optimal solution
    compared to a grid search. Each run in the experiment has an associated notebook
    with the code for the model. This method increases productivity, reduces unnecessary
    computing, and lets scientists perform experiments instead of writing boilerplate
    code. Once AutoML has converged on the algorithmically optimal solution, there
    is a “best notebook” for the best scoring model. We’ll expand on AutoML in several
    chapters throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Reusability and reproducibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As data scientists, transparency is especially important. We do not trust black
    box models. How do you use them without understanding them? A model is only as
    good as the data going in. In addition to not trusting the models, black boxes
    create concerns about our research’s reproducibility and model drivers’ explainability.
  prefs: []
  type: TYPE_NORMAL
- en: When we create a model, who does it belong to? Can we get access to it? Can
    we tweak, test, and, most importantly, reuse it? The amount of time put into the
    model’s creation is not negligible. Databricks AutoML gives you everything to
    explain, reproduce, and reuse the models it creates. In fact, you can take the
    model code or model object and run it on a laptop or wherever. This open source,
    glass-box, reproducible, and reusable methodology is our kind of open.
  prefs: []
  type: TYPE_NORMAL
- en: Open file formats give you flexibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flexibility is also an essential aspect of the Databricks platform, so let’s
    dive into the file format Delta, an open source project that makes it easy to
    adapt to many different use cases. For those familiar with Parquet, you can think
    of Delta as Parquet-plus – Delta files are Parquet files with a transaction log.
    The transaction log is a game changer. The increased reliability and optimizations
    make Delta the foundation of Databricks’ lakehouse architecture. The data lake
    side of the lakehouse is vital to data science, streaming, and unstructured and
    semi-structured data formats. Delta has also made the warehouse side possible.
    There are entire books on Delta; see the *Further reading* section for some examples.
    We are focusing on the fact that it is an open file format with key features that
    support building data products.
  prefs: []
  type: TYPE_NORMAL
- en: Integration and control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having an open file format is essential to maintain ownership of your data.
    Not only do you want to be able to read, alter, and open your data files, but
    you also want to keep them in your cloud tenant. Maintaining control over your
    data is possible in the Databricks Data Intelligence Platform. There is no need
    to put the data files into a proprietary format or lock them away in a vendor’s
    cloud. Take a look at *Figure 1**.4* to see how Delta is part of the larger ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – The Delta Kernel connection ecosystem](img/B16865_01_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – The Delta Kernel connection ecosystem
  prefs: []
  type: TYPE_NORMAL
- en: The Delta Kernel introduces a fresh approach, offering streamlined, focused,
    and reliable APIs that abstract away the intricacies of the Delta protocol. By
    simply updating the Kernel version, connector developers can seamlessly access
    the latest Delta features without needing to modify any code.
  prefs: []
  type: TYPE_NORMAL
- en: Time-travel versioning in Delta
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The freedom and flexibility of open file formats make it possible to integrate
    with new and existing external tooling. Delta Lake, in particular, offers unique
    support to create data products thanks to features such as time-travel versioning,
    exceptional speed, and the ability to update and merge changes. Time travel, in
    this context, refers to the capability of querying different versions of your
    data table, allowing you to revisit the state of the table before your most recent
    changes or transformations (see *Figure 1**.5*). The more obvious use is to back
    up after making a mistake rather than writing out multiple copies of the table
    as a safety measure. A possibly less obvious use for time travel is reproducible
    research. You can access the data your model was trained on in the previous week
    without creating an additional copy of the data. Throughout the book, we will
    detail features of the Data Intelligence Platform you can use to facilitate reproducible
    research. The following figure shows you how the previous version of a table,
    relative to a timestamp or a version number, can be queried.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 1.5 – A code example of the querying techniques to view previous versi\uFEFF\
    ons of a table](img/B16865_01_5.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – A code example of the querying techniques to view previous versions
    of a table
  prefs: []
  type: TYPE_NORMAL
- en: The speed of Databricks’ optimized combination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let us discuss the speed of Databricks’ lakehouse architecture. In November
    2021, Databricks set a new world record for the gold standard performance benchmark
    for data warehousing. The Barcelona Computing Group shared their research supporting
    this finding. This record-breaking speed resulted from the Databricks’ engines
    (Spark and Photon) paired with Delta (see the *Databricks Sets Official Data Warehousing
    Performance Record* link in the *Further* *reading* section).
  prefs: []
  type: TYPE_NORMAL
- en: The additional benefits of Delta
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Delta’s impressive features include **change data feed** (**CDF**), **change
    data capture** (**CDC**), and **schema evolution**. Each plays a specific role
    in data transformation in support of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with Delta’s CDF capability, it is exactly what it sounds like – a
    feed of the changed data. Let’s say you have a model looking for fraud, and that
    model needs to know how many transaction requests have occurred in the last 10
    minutes. It is not feasible to rewrite the entire table each time a value for
    an account needs to be updated. The feature value, or in this case, the number
    of transactions that occurred in the last 10 minutes, needs to be updated only
    when the value has changed. The use of CDF in this example enables updates to
    be passed to an **online feature store**; see *Chapters 5* and *6* for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s talk about change data capture, a game-changer in the world
    of data management. Unlike traditional filesystems, CDC in Delta has been purposefully
    designed to handle data updates efficiently. Let’s take a closer look at CDC and
    explore its capabilities through two practical scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario 1 – effortless record updates**: Picture a scenario involving Rami,
    one of your customers. He initially made a purchase in Wisconsin but later relocated
    to Colorado, where he continued to make purchases. In your records, it’s essential
    to reflect Rami’s new address in Colorado. Here’s where Delta’s CDC shines. It
    effortlessly updates Rami’s customer record without treating him as a new customer.
    CDC excels at capturing and applying updates seamlessly, ensuring data integrity
    without any hassles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario 2 – adapting to evolving data sources**: Now, consider a situation
    where your data source experiences unexpected changes, resulting in adding a new
    column containing information about your customers. Let’s say this new column
    provides insights into the colors of items purchased by customers. This is valuable
    data that you wouldn’t want to lose. Delta’s CDC, combined with its schema evolution
    feature, comes to the rescue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schema evolution, explored in depth in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123),
    enables Delta to gracefully adapt to schema changes without causing any disruptions.
    When dealing with a new data column, Delta smoothly incorporates this information,
    ensuring your data remains up to date while retaining its full historical context.
    This ensures that you can leverage valuable insights for both present and future
    analyses.
  prefs: []
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is heavily project-based. Each chapter starts with an overview of
    the important concepts and Data Intelligence Platform features that will prepare
    you for the main event – the *Applying our learning* sections. Every *Applying
    our learning* section has a *Technical requirements* section so that you know
    what technical resources you will need, in addition to your Databricks workspace
    and GitHub repository, to complete the project work in the respective chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the technical requirements needed to get started with the hands-on
    examples used throughout this book:'
  prefs: []
  type: TYPE_NORMAL
- en: We use Kaggle for two of our datasets. If you do not already have an account,
    you will need to create one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout the book, we will refer to code in GitHub. Create an account if you
    do not already have one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting to know your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are four main projects that progress sequentially throughout the book.
    In each subsequent chapter, the code will expand upon the code in previous chapters.
    We chose these projects to highlight a variety of Data Intelligence Platform features
    across different ML projects. Specifically, we include streaming data into your
    lakehouse architecture, forecasting sales, building a **deep learning** (**DL**)
    model for computer vision, and building a chatbot using **Retrieval Augmented
    Generation** (**RAG**) techniques. Read through the descriptions of each project
    to get an idea of what it will cover. If some of the concepts and features are
    unfamiliar, don’t worry! We’ll explain them in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Project – streaming transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This first project is a data solution for the streaming transactions dataset
    we will generate. The transaction data will include information such as the customer
    ID and transaction time, which we’ll use to simulate transactions streaming in
    real time; see the sample data in *Figure 1**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – A sample of the synthetic streaming transactions data](img/B16865_01_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – A sample of the synthetic streaming transactions data
  prefs: []
  type: TYPE_NORMAL
- en: Our goal with this project is to demonstrate how flexible the Data Intelligence
    Platform is compared to proprietary data warehouses of the past, which were more
    rigid for data ingestion. Additionally, we want to highlight important Databricks
    capabilities such as **Spark Structured Streaming**, **Auto Loader**, schema evolution,
    Delta Live Tables, and **Lakehouse Monitoring**.
  prefs: []
  type: TYPE_NORMAL
- en: When we generate the transactions, we also generate a label based on statistical
    distributions (note that the label is random and only used for learning purposes).
    This is the label we will be predicting. Our journey includes generating transaction
    records as multiline JSON files, formatting the files to a Delta table, creating
    a streaming feature for our ML model, wrapping a `pyfunc`) with the preprocessing
    steps, and deploying the model wrapper via a workflow. Take a look through the
    project pipeline in *Figure 1**.7* to understand how we’ll progress through this
    project.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – The project pipeline for the streaming transactions project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_01_7.jpg)![Figure 1.7 – The project pipeline for the streaming
    transactions project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_01_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.7 – The project pipeline for the streaming transactions project
  prefs: []
  type: TYPE_NORMAL
- en: That concludes the streaming transaction project explanation. Next, we will
    look at the forecasting project.
  prefs: []
  type: TYPE_NORMAL
- en: Project – Favorita sales forecasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a typical forecasting project. Our dataset is hosted on the Kaggle website
    (see the *Further reading* section). We will use the data to build a model to
    predict the total sales amount for a family of goods at a specific Favorita store
    in Ecuador. The data includes train, test, and supplementary data. This project
    will use Databricks’ AutoML for data exploration and to create a baseline model.
    Take a look through the project pipeline in *Figure 1**.8* to understand how we’ll
    progress through this project. The store sales dataset is a rich time-series dataset,
    and we encourage you to build on the project framework we provide using your favorite
    time-series library.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – The project pipeline for the Favorita store sales project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_01_9.jpg)![Figure 1.8 – The project pipeline for the Favorita
    store sales project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_01_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.8 – The project pipeline for the Favorita store sales project
  prefs: []
  type: TYPE_NORMAL
- en: That concludes the forecasting project explanation. Next, we will look at the
    DL project.
  prefs: []
  type: TYPE_NORMAL
- en: Project – multilabel image classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This project is a DL data solution that uses another Kaggle dataset. We will
    use these datasets images to fine-tune a deep-learning model, using PyTorch and
    Lightning to predict a corresponding label. We will implement MLflow code for
    experiment and model tracking, Spark for fast training and **inference**, and
    Delta for data version control. We will deploy the model as we would for a real-time
    scenario by creating a model wrapper, similar to the wrapper we use for the streaming
    transactions project. Take a look through the project pipeline in *Figure 1**.9*
    to understand how we’ll progress through this project.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – The project pipeline for the multilabel image classification
    project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_01_11.jpg)![Figure 1.9 – The project pipeline for the multilabel
    image classification project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_01_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.9 – The project pipeline for the multilabel image classification project
  prefs: []
  type: TYPE_NORMAL
- en: That concludes the image classification project explanation. Next, we will look
    at the chatbot project.
  prefs: []
  type: TYPE_NORMAL
- en: Project – a retrieval augmented generation chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This project is a RAG chatbot. The dataset we use comes from the *arXiv* website.
    We have selected a few research articles about the impact of generative AI on
    humans and labor. We will download and store them in a volume in [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073).
    After we download the PDF documents, we will extract and prepare the text through
    a process of chunking and tokenization, creating embeddings of the documents to
    be referenced in the chatbot. We will use Databricks Vector Search to store the
    embeddings. Then, we will use the new Foundation Model API to generate answers
    when text is retrieved. The final bot will be deployed as an application using
    **Databricks Model Serving**. This example allows you to build a chatbot from
    start to finish! Take a look through the project pipeline in *Figure 1**.10* to
    understand how we’ll progress through this project.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – The project pipeline for the chatbot project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_01_13.jpg)![Figure 1.10 – The project pipeline for the chatbot
    project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_01_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.10 – The project pipeline for the chatbot project
  prefs: []
  type: TYPE_NORMAL
- en: The four projects are hands-on examples that can be implemented on Databricks.
    *Databricks ML in Action* provides best practices and recommendations from an
    ML perspective based on our experiences and supplements online documentation.
    All the code and solutions presented in this book have been developed and tested
    on the full version of Databricks. However, we understand that accessibility matters.
    There is also a free community version of the Databricks Data Intelligence Platform
    available, enabling everyone to follow along with the examples to a certain point
    before considering an upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced you to *Databricks ML in Action*. We emphasized
    that the Databricks Data Intelligence Platform is designed with openness, flexibility,
    and tooling freedom in mind, which greatly accelerates productivity. Additionally,
    we’ve given you a sneak peek at the projects and the associated datasets that
    will be central to this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve gained a foundational understanding of the Data Intelligence
    Platform, it’s time to take the next step. In the upcoming chapter, we’ll guide
    you through setting up your environment and provide instructions on downloading
    the project data. This will prepare you for the practical, hands-on ML experiences
    that lie ahead in this journey.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s test ourselves on what we’ve learned by going through the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How will you use this book? Do you plan to go cover to cover or pick certain
    sections out? Have you chosen sections of interest?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We covered why transparency in modeling is critical to success. How does Databricks’
    glass-box approach to AutoML support this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Databricks has developed a new way of uniting the open data formats, called
    UniForm. Which data formats does UniForm unite?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delta is the foundation of the lakehouse architecture. What is one of the benefits
    of using Delta?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the main advantage of using the Delta file format for large-scale data
    processing over simple Parquet files in Databricks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After putting thought into the questions, compare your answers to ours:'
  prefs: []
  type: TYPE_NORMAL
- en: We cannot answer this question, but we hope you learn something you can use
    in your career soon!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The glass-box approach supports transparency by providing the code run for each
    run in the experiment and the best run, thus enabling reusability and reproducibility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apache Iceberg, Apache Hudi, and Linux Foundation Delta Lake (an open source/unmanaged
    version of Delta).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are several. Here are a few:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open protocol (no vendor lock-in)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Speed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Change data capture
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Time travel
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: While Parquet also provides columnar storage and has efficient read/write operations,
    its lack of ACID transaction capabilities distinguishes Delta Lake.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced vital technologies. Look at these resources
    to go deeper into the areas that interest you most:'
  prefs: []
  type: TYPE_NORMAL
- en: 'YouTube video – *Introduction to Databricks Data Intelligence* *Platform*:
    [https://youtu.be/E885Ld3N2As?si=1NPg85phVH8RhayO](https://youtu.be/E885Ld3N2As?si=1NPg85phVH8RhayO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introduction to Data* *Lakes*: [https://www.databricks.com/discover/data-lakes](https://www.databricks.com/discover/data-lakes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*5 Steps to a Successful Data Lakehouse* by Bill Inmon, father of the data
    warehouse: [https://www.databricks.com/resources/ebook/building-the-data-lakehouse](https://www.databricks.com/resources/ebook/building-the-data-lakehouse)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delta Lake: Up & Running* by O’Reilly: [https://www.databricks.com/resources/ebook/delta-lake-running-oreilly](https://www.databricks.com/resources/ebook/delta-lake-running-oreilly)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delta Lake: The Definitive* *Guide*: [https://www.oreilly.com/library/view/delta-lake-the/9781098151935/](https://www.oreilly.com/library/view/delta-lake-the/9781098151935/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Comparing Apache Spark and* *Databricks*: [https://www.databricks.com/spark/comparing-databricks-to-apache-spark](https://www.databricks.com/spark/comparing-databricks-to-apache-spark)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks* *MLflow*: [https://www.databricks.com/product/managed-mlflow](https://www.databricks.com/product/managed-mlflow)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Community Edition* *FAQ*: [https://www.databricks.com/product/faq/community-edition#:~:text=What%20is%20the%20difference%20between,ODBC%20integrations%20for%20BI%20analysis](https://www.databricks.com/product/faq/community-edition#:~:text=What%20is%20the%20difference%20between,ODBC%20integrations%20for%20BI%20analysis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delta 2.0 - The Foundation of your Data Lakehouse is* *Open*: [https://delta.io/blog/2022-08-02-delta-2-0-the-foundation-of-your-data-lake-is-open/](https://delta.io/blog/2022-08-02-delta-2-0-the-foundation-of-your-data-lake-is-open/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delta Lake* *Integrations*: [https://delta.io/integrations/](https://delta.io/integrations/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delta vs* *Iceberg*: [https://databeans-blogs.medium.com/delta-vs-iceberg-performance-as-a-decisive-criteria-add7bcdde03d](https://databeans-blogs.medium.com/delta-vs-iceberg-performance-as-a-decisive-criteria-add7bcdde03d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*UniForm*: [https://www.databricks.com/blog/delta-uniform-universal-format-lakehouse-interoperability](https://www.databricks.com/blog/delta-uniform-universal-format-lakehouse-interoperability)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delta Kernel: Simplifying Building Connectors for* *Delta*: [https://www.databricks.com/dataaisummit/session/delta-kernel-simplifying-building-connectors-delta/](https://www.databricks.com/dataaisummit/session/delta-kernel-simplifying-building-connectors-delta/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Community* *Website*: [https://community.cloud.databricks.com](https://community.cloud.databricks.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Podcast: Delta Lake Discussions with Denny* *Lee*: [https://open.spotify.com/show/6YvPDkILtWfnJNTzJ9HsmW?si=214eb7d808d84aa4](https://open.spotify.com/show/6YvPDkILtWfnJNTzJ9HsmW?si=214eb7d808d84aa4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Sets Official Data Warehousing Performance* *Record*: [https://www.databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html](https://www.databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LightGBM*: [https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)
    [https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kaggle* | *Store* *Sales*: [https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kaggle* | *Multi Label Image* *Classification*: [https://www.kaggle.com/datasets/meherunnesashraboni/multi-label-image-classification-dataset](https://www.kaggle.com/datasets/meherunnesashraboni/multi-label-image-classification-dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Store Sales - Time Series* *Forecasting*: [Kaggle.com/competitions/store-sales-time-series-forecasting/overview](http://Kaggle.com/competitions/store-sales-time-series-forecasting/overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*arXiv* *website*: [https://arxiv.org](https://arxiv.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
