<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer215">
			<h1 id="_idParaDest-153"><em class="italic"><a id="_idTextAnchor152"/>Chapter 9</em>: Building ML Models Using Azure Machine Learning</h1>
			<p>In the previous chapters, we learned about datasets, preprocessing, feature extraction, and pipelines in Azure Machine Learning. In this chapter, we will use the knowledge we have gained so far to create and train a powerful tree-based ensemble classifier.</p>
			<p>First, we will look behind the scenes of popular ensemble classifiers such as <strong class="bold">random forest</strong>, <strong class="bold">XGBoost</strong>, and <strong class="bold">LightGBM</strong>. These classifiers perform extremely well in practical real-world scenarios, and all are based on decision trees under the hood. By understanding their main benefits, you will be able to spot problems that can be solved with ensemble decision tree classifiers easily.</p>
			<p>We will also learn the difference between <strong class="bold">gradient boosting</strong> and <strong class="bold">random forest</strong> and what makes these tree ensembles useful for practical applications. Both techniques help to overcome the main weaknesses of decision trees and can be applied to many different classification and regression problems.</p>
			<p>Finally, we will train a LightGBM classifier on a sample dataset using all the techniques we have learned so far. We will write a training script that automatically logs all parameters, evaluation metrics, and figures, and is configurable with command-line arguments. We will schedule the training script on an Azure Machine Learning training cluster.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Working with tree-based ensemble classifiers</li>
				<li>Training an ensemble classifier model using LightGBM</li>
			</ul>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor153"/>Technical requirements</h1>
			<p>In this chapter, we will use the following Python libraries and versions to create decision tree-based ensemble classifiers:</p>
			<ul>
				<li><strong class="source-inline">azureml-core 1.34.0 </strong></li>
				<li><strong class="source-inline">azureml-sdk 1.34.0 </strong></li>
				<li><strong class="source-inline">lightgbm 3.2.1 </strong></li>
				<li><strong class="source-inline">numpy 1.19.5 </strong></li>
				<li><strong class="source-inline">pandas 1.3.2 </strong></li>
				<li><strong class="source-inline">scikit-learn 0.24.2 </strong></li>
				<li><strong class="source-inline">seaborn 0.11.2 </strong></li>
				<li><strong class="source-inline">matplotlib 3.4.3 </strong></li>
			</ul>
			<p>Similar to previous chapters, you can execute this code using either a local Python interpreter or a notebook environment hosted in Azure Machine Learning.</p>
			<p>All code examples in this chapter can be found in the GitHub repository for this book: <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter09">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter09</a>.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor154"/>Working with tree-based ensemble classifiers</h1>
			<p>Supervised tree-based ensemble classification<a id="_idIndexMarker1156"/> and regression techniques have proven very successful in many practical real-world applications in recent years. Hence, they are widely used today in various applications, including fraud detection, recommendation engines, tagging engines, and many more. All your favorite mobile and desktop operating systems, Office programs, and audio or video streaming services make heavy use of them every day.</p>
			<p>Therefore, in this section, we will dive into the main reasons for their popularity and performance, both for training and scoring. If you are an expert on traditional ML algorithms and know the difference between boosting and bagging, you might as well jump right to the next section, <em class="italic">Training an ensemble classifier model using LightGBM</em>, where we put the theory into practice.</p>
			<p>We will first look at decision trees, a very simple technique that is decades old. We encourage you to follow along even with the simple methods as they build the foundation of today's state-of-the-art classical supervised ML approaches. We will also explore the advantages of tree-based classifiers in detail to help you understand the differences between a classical approach and a deep learning-based ML model.</p>
			<p>A single decision tree<a id="_idIndexMarker1157"/> also has a lot of disadvantages associated with it and is therefore used only in ensemble models and never as an individual model. We will take a closer look at the disadvantages of individual decision trees later in this section. Afterwards, we will discover methods for combining multiple weak individual trees into a single strong ensemble classifier that builds upon the strengths of tree-based approaches and transforms them into what they are today—powerful multi-purpose supervised ML models that are integrated into almost every off-the-shelf ML platform.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor155"/>Understanding a simple decision tree</h2>
			<p>Let's first discuss what a <strong class="bold">decision tree</strong> is and how it works. A decision tree<a id="_idIndexMarker1158"/> estimator is a supervised ML approach that learns to approximate a function with multiple nested <strong class="source-inline">if/else</strong> statements. This function can be a continuous regressor function or a decision boundary function. Hence, like many other ML approaches, decision trees can be used for learning both regression and classification problems.</p>
			<p>From the preceding description, we can immediately spot a few important<a id="_idIndexMarker1159"/> advantages of decision trees:</p>
			<ul>
				<li>One is the flexibility to work on different data distributions, data types (for example, numerical and categorical data), and ML problems (such as classification or regression).</li>
				<li>Another advantage and one of the reasons they compete with more complicated models is their interpretability. Tree-based models and ensembles can be visualized and even printed out on paper to explain the decision (output) from a prediction.</li>
				<li>The third advantage lies in their practical use for training performance, model size, and validity. Integrating a pre-trained decision tree into a desktop, web, or mobile application is a lot less complex and a lot faster than a deep learning approach.<p class="callout-heading">Important Note</p><p class="callout">Please note that we don't intend to sell tree-based ensembles as the solution to every ML problem and to downplay the importance of deep learning approaches. We rather want to make you aware of the strengths of traditional approaches in this chapter so you can evaluate the right approach for your problem.</p></li>
			</ul>
			<p>The following figure shows an example<a id="_idIndexMarker1160"/> of a decision tree used to decide whether a person is fit or not:</p>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="image/B17928_09_01.jpg" alt="Figure 9.1 – A simple decision tree " width="1650" height="759"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – A simple decision tree</p>
			<p><em class="italic">Figure 9.1</em> is an example of a trained decision tree, where we can score the model by simply walking through each node and arriving at a class label at the leaf of the tree.</p>
			<h3>Advantages of a decision tree</h3>
			<p>Decision tree-based ML models are extremely popular due to their strengths when working on real-world applications where data comes in all forms and shapes and is messy, biased, and incomplete. These are the key advantages<a id="_idIndexMarker1161"/> of decision trees:</p>
			<ul>
				<li>They support a wide range of applications.</li>
				<li>They require little data preparation.</li>
				<li>The enable interpretability of the model.</li>
				<li>They provide fast training and fast scoring.</li>
			</ul>
			<p>First, let's focus on the <em class="italic">flexibility</em> of decision trees, which is one of their major strengths as opposed to many other classical/statistical ML approaches. While the general framework is very flexible<a id="_idIndexMarker1162"/> and supports <em class="italic">classification</em> and <em class="italic">regression</em>, as well as <em class="italic">multi-output problems</em>, it gained a lot of popularity because it can handle both numerical and categorical data out of the box. Thanks to nested <strong class="source-inline">if-else</strong> trees, it can also handle nominal categories as well as NULL or missing values in data. Decision trees are popular because they don't require massive preprocessing and data cleansing beforehand.</p>
			<p>While data preparation and cleaning are important steps in every ML pipeline, it's still nice to have a framework that naturally supports categorical input data out of the box. Some ensemble tree-based classifiers are built on top of this advantage, for example, <strong class="bold">CatBoost</strong>—a gradient boosted trees implementation<a id="_idIndexMarker1163"/> from Yandex Research with native support for categorical data.</p>
			<p>Another important advantage of tree-based models, especially from a business perspective, is the <em class="italic">interpretability</em> of the model. Unlike other ML approaches, the output of a decision tree classifier model is not a huge parametric decision boundary function. Trained deep learning models often generate a model with more than 100 million parameters and hence behave like a black box—especially for business decision makers. While it is possible to gain insights and reason about the activations in deep learning models, it's usually very hard to reason about the effect of an input parameter on the output variable.</p>
			<p>Interpretability is where tree-based approaches shine. In contrast to many other traditional ML approaches (such as SVM, logistic regression, or deep learning), a decision tree is a non-parametric model and therefore, doesn't use parameters to describe the function to be learned. It uses a nested decision tree that can be plotted, visualized, and printed out on paper. This allows decision makers to understand every decision (output) of a tree-based classification model—it may require a lot of paper, but it is always possible.</p>
			<p>While speaking about interpretability, we need to mention another important aspect of decision trees: the decision tree model implicitly develops a notion of <em class="italic">feature importance</em> during the training process. This is a very useful output of a trained decision tree model that we can use to rank features for preprocessing, without requiring to first clean the data.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">While feature importance can also be measured with other ML approaches, for example, linear regression, they usually require a cleaned and normalized dataset as input. Many other ML approaches, such as SVM or deep learning, don't develop a measure of feature importance for the individual input dimensions.</p>
			<p>Decision tree-based approaches<a id="_idIndexMarker1164"/> excel at this as they internally create each individual split (decision) based on an importance criterion. This results in an inherent understanding of how and which feature dimensions are important to the final model.</p>
			<p>Let's look at another great advantage of decision trees. Decision trees have many practical benefits over traditional statistical models derived from the non-parametric approach. Tree-based models generally yield good results on a wide variety of input distributions and even work well when the model assumptions are violated. On top of that, the size of the trained tree is small compared to deep learning approaches, and inference/scoring is fast.</p>
			<h3>Disadvantages of a decision tree</h3>
			<p>As everything in life<a id="_idIndexMarker1165"/> comes with advantages and disadvantages, the same is true for decision trees. There are quite a few severe disadvantages associated with individual decision trees that should make you avoid a single decision tree classifier in your ML pipeline. The main weakness of a single decision tree is that the tree is fitted on all training samples and, hence, is very likely to <em class="italic">overfit</em>. The reason for this is that the model itself tends to build complex <strong class="source-inline">if-else</strong> trees to model a continuous function.</p>
			<p>Another important point is that finding the optimal decision tree even<a id="_idIndexMarker1166"/> for simple concepts is an <strong class="bold">NP-hard problem</strong> (also known as a <strong class="bold">nondeterministic polynomial time-hard problem</strong>). Therefore, it is solved through heuristics<a id="_idIndexMarker1167"/> and the resulting single decision is usually not the optimal one.</p>
			<p>Overfitting is bad – very bad – and leads to a serious complication in ML. Once a model overfits, it doesn't generalize well and hence has very poor performance on unseen data. Therefore, predictions for new inputs will yield results that are worse than those measured during training. Another related problem is that tiny changes in the training data or the order of training samples can lead to very different nested trees and hence, the training convergence is unstable. Single decision trees are extremely prone to overfitting. On top of that, a single decision tree is very likely to be biased toward the class with the largest number of samples in your training data.</p>
			<p>You can overcome the disadvantages <a id="_idIndexMarker1168"/>of single trees, such as overfitting, instability, and non-optimal trees, by combining multiple decision trees through<a id="_idIndexMarker1169"/> bagging and boosting to an <strong class="bold">ensemble model</strong>. There are also many<a id="_idIndexMarker1170"/> tree-based optimizations, including <strong class="bold">tree pruning</strong>, to improve generalization. Popular models that use these<a id="_idIndexMarker1171"/> techniques include <strong class="bold">random forests</strong> and <strong class="bold">gradient boosted trees</strong>, which overcome<a id="_idIndexMarker1172"/> most of the problems of a single decision tree while keeping most of their benefits. We will look at these two methods in the next section.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Some more fundamental disadvantages sometimes crop up even with tree-based ensemble methods that are worth mentioning. Due to the nature of decision trees, tree-based models have difficulties learning complicated functions, such as the XOR problem. For these problems, it's better to use non-linear parametric models, such as neural networks and deep learning approaches.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor156"/>Combining classifiers with bagging</h2>
			<p>One key disadvantage of a single decision<a id="_idIndexMarker1173"/> tree is overfitting to training<a id="_idIndexMarker1174"/> data and, hence, poor generalization performance and instability from small changes in the training data. A <strong class="bold">bagging</strong> (also called <em class="italic">bootstrap aggregation</em>) classifier uses the simple<a id="_idIndexMarker1175"/> concept of combining multiple<a id="_idIndexMarker1176"/> independent models into an <strong class="bold">ensemble model</strong> trained on a subset of the training<a id="_idIndexMarker1177"/> data to overcome this exact problem. The subsets are built by randomly picking samples from the training dataset with replacements. The output of the individual models is either selected through a majority vote for classification or mean aggregation for regression problems.</p>
			<p>By combining independent models, we can reduce the variance of the combined model without increasing the bias and thereby greatly improve generalization. However, there is another benefit to training multiple individual models: parallelization. Since each individual model uses a random subset of the training data, the training process can easily be parallelized and trained on multiple compute nodes. Therefore, bagging is a popular technique when training a large number of tree-based classifiers on a large dataset.</p>
			<p>The following <em class="italic">Figure 9.2</em> shows how each classifier<a id="_idIndexMarker1178"/> is trained independently on the same<a id="_idIndexMarker1179"/> training data—each model uses a random subset with replacements. The combination of all individual models makes up the ensemble model.</p>
			<div>
				<div id="_idContainer209" class="IMG---Figure">
					<img src="image/B17928_09_02.jpg" alt="Figure 9.2 – Bagging " width="1650" height="921"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Bagging</p>
			<p>Bagging can be used to combine any ML model; however, it is often used with tree-based classifiers as they suffer most from overfitting. The idea of <strong class="bold">random forest</strong> builds on top of the bagging<a id="_idIndexMarker1180"/> method combined with a random subset of features for each split (decision). When a feature is selected at random, the optimal threshold for the split is computed such that a certain <em class="italic">information criterion</em> is optimized (usually <strong class="bold">GINI</strong> or <strong class="bold">information gain</strong>). Hence, the random<a id="_idIndexMarker1181"/> forest uses a random subset<a id="_idIndexMarker1182"/> of the training data, random feature selection, and an optimal threshold for the split.</p>
			<p>Random forests are widely used for their simple decision tree-based model combined with much better generalization and easy parallelization. Another benefit of taking a random subset of features is that this technique also works well with very high-dimensional inputs. Hence, when dealing with classical ML approaches, random forests are often used for large-scale tree ensembles.</p>
			<p>Another popular tree-based bagging technique is the <strong class="bold">extra-trees</strong> (short for <strong class="bold">extremely randomized trees</strong>) algorithm, which adds another<a id="_idIndexMarker1183"/> randomization step on the dimension split. For each split, thresholds are drawn<a id="_idIndexMarker1184"/> at random and the best one is selected for that decision. Hence, in addition<a id="_idIndexMarker1185"/> to random features, the extra-trees algorithm also uses random split thresholds to further improve generalization.</p>
			<p>The following <em class="italic">Figure 9.3</em> shows how all tree ensemble techniques are used for inferencing. Each tree computes an individual score while the result of each tree is aggregated to yield the result:</p>
			<div>
				<div id="_idContainer210" class="IMG---Figure">
					<img src="image/B17928_09_03.jpg" alt="Figure 9.3 – Majority voting " width="1650" height="958"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – Majority voting</p>
			<p>You can find tree-based bagging ensembles such as random forest, and sometimes also extra-trees, in many popular ML libraries, such as scikit-learn, Spark MLlib, ML.NET, and many others.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor157"/>Optimizing classifiers with boosting rounds</h2>
			<p>In many problems<a id="_idIndexMarker1186"/> in computer science, we can replace a random<a id="_idIndexMarker1187"/> greedy approach with a more complex but more optimal approach. The same holds<a id="_idIndexMarker1188"/> true for tree ensembles and builds the foundation for <strong class="bold">boosted tree ensembles</strong>.</p>
			<p>The basic idea behind boosting is the following:</p>
			<ol>
				<li>We start to train an individual model on the whole training dataset.</li>
				<li>Then we compute the predictions of the model on the training dataset and start weighting training samples that yield a wrong result higher.</li>
				<li>Next, we train another decision tree using the weighted training set. We then combine both decision trees into an ensemble and predict the output classes for the weighted training set. We then further increase the weights on the wrongly classified training samples of the combined model for the next boosting round.</li>
				<li>We continue this algorithm until a stopping criterion is reached.</li>
			</ol>
			<p>The following <em class="italic">Figure 9.4</em> shows how the training<a id="_idIndexMarker1189"/> error using boosting optimization decreases<a id="_idIndexMarker1190"/> each iteration (boosting round) with the addition of a new tree:</p>
			<div>
				<div id="_idContainer211" class="IMG---Figure">
					<img src="image/B17928_09_04.jpg" alt="Figure 9.4 – Boosting " width="1650" height="896"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Boosting</p>
			<p>The first boosting algorithm was <strong class="bold">AdaBoost</strong>, which combined multiple<a id="_idIndexMarker1191"/> weak models into an ensemble by fitting it on a weighted training set that adapts each iteration through a learning rate. The notion<a id="_idIndexMarker1192"/> of this approach was to add individual<a id="_idIndexMarker1193"/> trees that focus on predicting something the previous trees couldn't predict.</p>
			<p>One particular successful<a id="_idIndexMarker1194"/> technique of boosting is <strong class="bold">gradient boosted trees</strong> (or <strong class="bold">gradient boosting</strong>). In gradient boosting, you<a id="_idIndexMarker1195"/> combine the gradient descent optimization technique with boosting in order to generalize boosting to an arbitrary loss function. Now, instead of tuning the dataset samples using weights, we can compute the gradient of the loss function and select the optimal weights—the ones that minimize the loss function—during each iteration. Thanks to the usage of optimization, this technique yields very good results, adding to the existing advantages of decision trees.</p>
			<p>Gradient boosted tree-based ensembles are included in many popular ML libraries such as scikit-learn, Spark MLlib, and others. However, some individual implementations, such as XGBoost and LightGBM, have gained quite a lot of popularity and are available as standalone libraries and as plugins for scikit-learn and Spark.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor158"/>Training an ensemble classifier model using LightGBM</h1>
			<p>Both random forest and gradient boosted<a id="_idIndexMarker1196"/> trees are powerful ML techniques<a id="_idIndexMarker1197"/> due to the simplicity of decision trees and the benefits of combining multiple classifiers. In this example, we will use the popular LightGBM library from Microsoft to implement both techniques on a test dataset. LightGBM is a framework for gradient boosting that incorporates multiple tree-based learning algorithms.</p>
			<p>For this section, we will follow a typical best-practice approach using Azure Machine Learning and perform the following steps:</p>
			<ol>
				<li value="1">Register the dataset in Azure.</li>
				<li>Create a remote compute cluster.</li>
				<li>Implement a configurable training script.</li>
				<li>Run the training script on the compute cluster.</li>
				<li>Log and collect the dataset, parameters, and performance.</li>
				<li>Register the trained model.</li>
			</ol>
			<p>Before we start<a id="_idIndexMarker1198"/> with this exciting approach, we'll take a quick<a id="_idIndexMarker1199"/> look at why we chose LightGBM as a tool for training bagged and boosted tree ensembles.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor159"/>LightGBM in a nutshell</h2>
			<p>LightGBM uses many optimizations<a id="_idIndexMarker1200"/> of classical tree-based ensemble techniques to provide<a id="_idIndexMarker1201"/> excellent performance on both categorical<a id="_idIndexMarker1202"/> and continuous features. The latter is profiled using a histogram-based approach and converted into discrete bins of optimal splits, which reduces memory consumption and speeds up training. This makes LightGBM faster and more memory efficient than other boosting libraries that use pre-sorted algorithms for computing splits, and hence is a great choice for large datasets.</p>
			<p>Another optimization in LightGBM is that trees are grown vertically, leaf after leaf, whereas other similar libraries grow trees horizontally, layer after layer. In a leaf-wise algorithm, the newly added leaf always has the largest decrease in loss. This means that these algorithms tend to achieve less loss compared to level-wise algorithms. However, greater depth also results in overfitting, and therefore you must carefully tune the maximum depth of each tree. Overall, LightGBM produces great results using default parameters on a large set of applications.</p>
			<p>In <a href="B17928_07_ePub.xhtml#_idTextAnchor112"><em class="italic">Chapter 7</em></a>, <em class="italic">Advanced Feature Extraction with NLP</em>, we learned a lot about categorical feature embedding and extracting semantic meanings from textual features. We looked at common techniques for embedding nominal categorical variables, such as label encoding and one-hot encoding, and others. However, to optimize the split criterion in tree-based learners for categorical variables, there are better encodings to produce optimal splits. Therefore, we don't encode categorical variables at all in this section, but simply tell LightGBM which of the variables used are categorical.</p>
			<p>One last thing to mention is that LightGBM can take advantage of GPU acceleration, and training can be parallelized both in a data-parallel or model-parallel way. We will learn more about distributed training in <a href="B17928_12_ePub.xhtml#_idTextAnchor189"><em class="italic">Chapter 12</em></a>, <em class="italic">Distributed Machine Learning on Azure</em>. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">LightGBM is a great choice for a tree-based ensemble model, especially for very large datasets.</p>
			<p>We will use LightGBM with the <strong class="source-inline">lgbm</strong> namespace throughout this book. We can then call different methods<a id="_idIndexMarker1203"/> from the namespace directly<a id="_idIndexMarker1204"/> by typing four characters less—a best-practice approach among data scientists in Python. Let's see a simple example:</p>
			<p class="source-code">import lightgbm as lgbm</p>
			<p class="source-code"># Construct a LGBM dataset</p>
			<p class="source-code">lgbm.Dataset(..)</p>
			<p class="source-code"># Train a LGBM predictor</p>
			<p class="source-code">clf = lgbm.train(..)</p>
			<p>What is interesting to note is that all algorithms are trained via the <strong class="source-inline">lgbm.train()</strong> method and we use different parameters to specify the algorithm, application type, and loss function, as well as additional hyperparameters for each algorithm. LightGBM supports multiple decision tree-based ensemble models for bagging and boosting. These are the algorithm options that you can choose from, along with their names, to identify them for the boosting parameter:</p>
			<ul>
				<li><strong class="source-inline">gbdt</strong>: Traditional gradient boosting decision tree</li>
				<li><strong class="source-inline">rf</strong>: Random forest</li>
				<li><strong class="source-inline">dart</strong>: Dropouts meet multiple additive regression trees</li>
				<li><strong class="source-inline">goss</strong>: Gradient-based one-side sampling</li>
			</ul>
			<p>The first<a id="_idIndexMarker1205"/> two options, namely, <em class="italic">gradient boosting decision tree</em> (<strong class="source-inline">gbdt</strong>), which is the default choice of LightGBM, and <em class="italic">random forest</em> (<strong class="source-inline">rf</strong>), are classical implementations<a id="_idIndexMarker1206"/> of the boosting and bagging techniques, explained in the first section of this chapter, with LightGBM<a id="_idIndexMarker1207"/>-specific optimizations. The other two techniques, <em class="italic">dropouts meet multiple additive regression trees</em> (<strong class="source-inline">dart</strong>) and <em class="italic">gradient-based one-side sampling</em> (<strong class="source-inline">goss</strong>), are specific to LightGBM<a id="_idIndexMarker1208"/> and provide more optimizations for better results in a trade-off for training speed.</p>
			<p>The objective parameter—which is one of the most<a id="_idIndexMarker1209"/> important parameters—specifies the application<a id="_idIndexMarker1210"/> type of the model, and hence the ML problem you're trying to solve. In LightGBM, you have the following standard options, which are similar to most other decision tree-based ensemble algorithms:</p>
			<ul>
				<li><strong class="source-inline">regression</strong>: For predicting continuous target variables</li>
				<li><strong class="source-inline">binary</strong>: For binary classification tasks</li>
				<li><strong class="source-inline">multiclass</strong>: For multiclass classification problems</li>
			</ul>
			<p>Besides the standard choices, you can also choose between the following more specific objectives: <strong class="source-inline">regression_l1</strong>, <strong class="source-inline">huber</strong>, <strong class="source-inline">fair</strong>, <strong class="source-inline">poisson</strong>, <strong class="source-inline">quantile</strong>, <strong class="source-inline">mape</strong>, <strong class="source-inline">gamma</strong>, <strong class="source-inline">cross_entropy</strong>, and many others.</p>
			<p>Directly related to the objective parameter of the model is the choice of loss function to measure and optimize the training performance. Here, too, LightGBM gives us the default options that are also available in most other boosting libraries, which we can specify via the metric parameter:</p>
			<ul>
				<li><strong class="source-inline">mae</strong>: Mean absolute error</li>
				<li><strong class="source-inline">mse</strong>: Mean squared error</li>
				<li><strong class="source-inline">binary_logloss</strong>: Loss for binary classification</li>
				<li><strong class="source-inline">multi_logloss</strong>: Loss for multi-classification</li>
			</ul>
			<p>Apart from these loss metrics, other metrics are supported as well, such as <strong class="source-inline">rmse</strong>, <strong class="source-inline">quantile</strong>, <strong class="source-inline">mape</strong>, <strong class="source-inline">huber</strong>, <strong class="source-inline">fair</strong>, <strong class="source-inline">poisson</strong>, and many others. In our classification scenario, we will choose the <strong class="source-inline">dart</strong> algorithm with the <strong class="source-inline">binary</strong> objective and <strong class="source-inline">binary_logloss</strong> metric.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can also use LightGBM as a scikit-learn estimator. To do so, call the <strong class="source-inline">LGBMModel</strong>, <strong class="source-inline">LGBMClassifier</strong>, or <strong class="source-inline">LGBMRegressor</strong> model from the <strong class="source-inline">lightgbm</strong> namespace. However, the latest features are typically only available through the LightGBM interface.</p>
			<p>Now, knowing<a id="_idIndexMarker1211"/> how to use<a id="_idIndexMarker1212"/> LightGBM, we can start with the implementation of the data preparation and authoring script.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor160"/>Preparing the data</h2>
			<p>In this section, we will read and prepare<a id="_idIndexMarker1213"/> the data and register the cleaned data as a new dataset<a id="_idIndexMarker1214"/> in Azure Machine Learning. This will allow us to access the data from any compute target connected with the workspace without the need to manually copy data around, mount disks, or set up connections to datastores. This was discussed in detail in <a href="B17928_04_ePub.xhtml#_idTextAnchor071"><em class="italic">Chapter 4</em></a>, <em class="italic">Ingesting Data and Managing Datasets</em>. All the setup, scheduling, and operations will be done from an authoring environment—a <em class="italic">Jupyter notebook</em>.</p>
			<p>For the classification<a id="_idIndexMarker1215"/> example, we will use the <em class="italic">Titanic dataset</em>, a popular dataset for ML practitioners to predict the binary survival probability (<em class="italic">survived</em> or <em class="italic">not survived</em>) for each passenger on the Titanic. The features of this dataset describe the passengers and contain the following attributes: passenger ID, class, name, sex, age, number of siblings or spouse on the ship, number of children or parents on the ship, ticket identification number, fare, cabin number, and embarked port.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The details about this dataset, as well as the complete preprocessing pipeline, can be found in the source code that comes with this book.</p>
			<p>Without knowing any more details, we'll roll up our sleeves<a id="_idIndexMarker1216"/> and set up the workspace<a id="_idIndexMarker1217"/> and start experimentation:</p>
			<ol>
				<li value="1">We import <strong class="source-inline">Workspace</strong> and <strong class="source-inline">Experiment</strong> from <strong class="source-inline">azureml.core</strong> and specify the name <strong class="source-inline">titanic-lgbm</strong> for this experiment:<p class="source-code">from azureml.core import Workspace, Experiment</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">exp = Experiment(workspace=ws, name="titanic-lgbm")</p></li>
				<li>Next, we load the dataset using pandas, and start cleaning and preprocessing the data:<p class="source-code">import pandas as pd</p><p class="source-code"># Read the data</p><p class="source-code">df = pd.read_csv('data/titanic.csv')</p><p class="source-code"># Prepare the data</p><p class="source-code">df.drop(['PassengerId'], axis=1, inplace=True)</p><p class="source-code">df.loc[df['Sex'] == 'female', 'Sex'] = 0</p><p class="source-code">df.loc[df['Sex'] == 'male', 'Sex'] = 1</p><p class="source-code">df['Sex'] = df['Sex'].astype('int8')</p><p class="source-code">embarked_encoder = LabelEncoder()</p><p class="source-code">embarked_encoder.fit(df['Embarked'].fillna('Null'))</p><p class="source-code">df['Embarked'].fillna('Null', inplace=True)</p><p class="source-code">df['Embarked'] = embarked_encoder.transform(</p><p class="source-code">    df['Embarked'])</p><p class="source-code">df.drop(['Name', 'Ticket', 'Cabin'],</p><p class="source-code">    axis=1,</p><p class="source-code">    inplace=True)</p></li>
			</ol>
			<p>In the preceding example, we load the data from a CSV file, remove unused columns, replaced the values of the <strong class="source-inline">Sex</strong> feature with labels <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, and encode the categorical values of the <strong class="source-inline">Embarked</strong> features with labels.</p>
			<ol>
				<li value="3">Next, we write<a id="_idIndexMarker1218"/> a small utility function, <strong class="source-inline">df_to_dataset()</strong>, which will help us to store pandas DataFrames and register and persist them<a id="_idIndexMarker1219"/> as Azure datasets, in order to reuse them with ease anywhere in the Azure Machine Learning environment:<p class="source-code">def df_to_dataset(ws, df, name):</p><p class="source-code">    datastore = ws.get_default_datastore()</p><p class="source-code">    dataset = Dataset.Tabular.register_pandas_dataframe(</p><p class="source-code">        df, datastore, name)</p><p class="source-code">    return dataset</p></li>
			</ol>
			<p>First, we retrieve a reference to the default datastore of our ML workspace—this is the Azure Blob storage that was created when we first set up the workspace. Then, we use a helper function to upload the dataset to this default datastore and reference it as a tabular dataset.</p>
			<ol>
				<li value="4">Next, we use the newly created helper function to register the pandas DataFrame as a dataset with the name <strong class="source-inline">titanic_cleaned</strong>:<p class="source-code"># Register the data</p><p class="source-code">df_to_dataset(ws, df, 'titanic_cleaned')</p></li>
				<li>Once the dataset is registered in Azure, it can be accessed anywhere in the Azure Machine Learning workspace. If we now<a id="_idIndexMarker1220"/> go to the UI and click on the <strong class="bold">Datasets</strong> menu, we will<a id="_idIndexMarker1221"/> find the <strong class="source-inline">titanic_cleaned</strong> dataset. In the UI, we can also easily inspect and preview the data, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer212" class="IMG---Figure">
					<img src="image/B17928_09_05.jpg" alt="Figure 9.5 – Titanic dataset " width="1466" height="990"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Titanic dataset</p>
			<p>One thing worth mentioning is that we will first encode categorical variables to integers using label encoding, but later tell LightGBM which variables contain categorical information in the numeric columns. This will help LightGBM to treat these columns differently when computing the histogram and optimal parameter splits.</p>
			<p>The benefit of having the dataset registered is that we can now simply pass the data to a training script or access it from any Python interpreter from within Azure Machine Learning Let's continue with the training example and create a training and execution environment for LightGBM.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor161"/>Setting up the compute cluster and execution environment</h2>
			<p>Before we can start training<a id="_idIndexMarker1222"/> the LightGBM classifier, we need to set up our training cluster<a id="_idIndexMarker1223"/> and a training environment with all the required Python libraries. For this chapter, we choose a CPU cluster with up to four nodes of the type <strong class="source-inline">STANDARD_D2_V2</strong>:</p>
			<ol>
				<li value="1">Let's write a small helper function that lets us retrieve or create a training cluster with a specified name and configuration. We take advantage of <strong class="source-inline">ComputeTargetException</strong>, which is thrown if a cluster with a specified name was not found:<p class="source-code">def get_aml_cluster(ws, cluster_name,</p><p class="source-code">                    vm_size='STANDARD_D2_V2',</p><p class="source-code">                    max_nodes=4):</p><p class="source-code">    try:</p><p class="source-code">        cluster = ComputeTarget(</p><p class="source-code">             workspace=ws, name=cluster_name)</p><p class="source-code">    except ComputeTargetException:</p><p class="source-code">        config = AmlCompute.provisioning_configuration(</p><p class="source-code">            vm_size=vm_size, max_nodes=max_nodes)</p><p class="source-code">        cluster = ComputeTarget.create(</p><p class="source-code">            ws, cluster_name, config)</p><p class="source-code">    return cluster </p></li>
			</ol>
			<p>We have already seen the ingredients of this script in the previous chapters, where we called <strong class="source-inline">AmlCompute.provisioning_configuration()</strong> to provision a new cluster. It is extremely helpful that you can define all your infrastructure within your authoring environment.</p>
			<ol>
				<li value="2">Let's retrieve<a id="_idIndexMarker1224"/> or create a new training<a id="_idIndexMarker1225"/> cluster:<p class="source-code"># Create or get training cluster</p><p class="source-code">aml_cluster = get_aml_cluster(ws, </p><p class="source-code">                              cluster_name="cpu-cluster")</p><p class="source-code">aml_cluster.wait_for_completion(show_output=True)</p></li>
				<li>Next, we want to do the same for our training environment and Python configuration. We implement a small <strong class="source-inline">get_run_config()</strong> function to return a remote execution environment with a Python configuration. This will be used to configure all the required Python packages for the training script:<p class="source-code">def get_run_config(target, packages=None):</p><p class="source-code">    packages = packages or []</p><p class="source-code">    packages += ['azureml-defaults']</p><p class="source-code">    config = RunConfiguration()</p><p class="source-code">    config.target = target</p><p class="source-code">    config.environment.python.conda_dependencies = \</p><p class="source-code">        <strong class="bold">CondaDependencies.create(pip_packages=packages)</strong></p><p class="source-code">    return config</p></li>
			</ol>
			<p>In the preceding script, we define <strong class="source-inline">RunConfiguration</strong> with the required packages for Azure Machine Learning such as <strong class="source-inline">azureml-defaults</strong>, and custom Python packages.</p>
			<ol>
				<li value="4">Next, we use this function to configure a Python image with all the required <strong class="source-inline">pip</strong> packages, including <strong class="source-inline">lightgbm</strong>:<p class="source-code"># Create a remote run configuration</p><p class="source-code">lgbm_config = get_run_config(aml_cluster, [</p><p class="source-code">    'numpy', 'pandas', 'matplotlib', 'seaborn',</p><p class="source-code">    'scikit-learn', 'joblib', <strong class="bold">'lightgbm'</strong></p><p class="source-code">])</p></li>
			</ol>
			<p>The two functions used in the preceding snippets are very useful. The longer you work with Azure Machine Learning, the more abstractions you will build to easily interact with the Azure Machine Learning service.</p>
			<p>Using the custom run configuration<a id="_idIndexMarker1226"/> and custom Python packages, Azure Machine Learning <a id="_idIndexMarker1227"/>will set up a Docker image and automatically register it in the <em class="italic">container registry</em>, as soon as we schedule a job using this run configuration. Let's first construct the training script and then schedule it on the cluster.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor162"/>Building a LightGBM classifier</h2>
			<p>Now that we have the dataset<a id="_idIndexMarker1228"/> ready, and we've set up the environment and cluster for the training of the LightGBM classification model, we can set up the training script. The code from the preceding section was written in a Jupyter notebook. The following code in this section will now be written and stored in a Python file called <strong class="source-inline">train_lgbm.py</strong>. We will start building the classifier using the following steps:</p>
			<ol>
				<li value="1">First, we configure the run and extract the workspace configuration from the run. This should already look familiar as we have done this for almost every script that we have been scheduling on Azure Machine Learning so far:<p class="source-code">from azureml.core import Dataset, Run</p><p class="source-code">run = Run.get_context()</p><p class="source-code">ws = run.experiment.workspace</p></li>
				<li>Next, we set up an argument parser to parse command-line parameters into LightGBM parameters. We start<a id="_idIndexMarker1229"/> with a handful of parameters but could easily add all available parameters and default values:<p class="source-code">parser.add_argument('--data', type=str)</p><p class="source-code">parser.add_argument('--boosting', type=str)</p><p class="source-code">parser.add_argument('--learning-rate', type=float)</p><p class="source-code">parser.add_argument('--drop-rate', type=float)</p><p class="source-code">args = parser.parse_args()</p><p class="callout-heading">Important Note</p><p class="callout">We recommend making your training scripts configurable. Use <strong class="source-inline">argparse</strong> to define datasets, input parameters, and default values. If you stick to this convention, all your model parameters will automatically be tracked in your Azure Machine Learning experiment. Another benefit is that you will later be able to tune the hyperparameters without changing a line of code in your training script.</p></li>
				<li>Then, we can reference the cleaned dataset from the input argument and load it to memory using the <strong class="source-inline">to_pandas_dataframe()</strong> method:<p class="source-code"># Get a dataset by id</p><p class="source-code">dataset = Dataset.get_by_id(ws, id=args.data)</p><p class="source-code"># Load a TabularDataset into pandas DataFrame</p><p class="source-code">df = dataset.to_pandas_dataframe()</p></li>
				<li>Having loaded the dataset as a pandas DataFrame, we can now start splitting the training data into training and validation sets. We will also split the target variable, <strong class="source-inline">Survived</strong>, from the training dataset into its own variable:<p class="source-code">y = df.pop('Survived')</p><p class="source-code"># Split into training and testing set </p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(</p><p class="source-code">    df, y, test_size=0.2, random_state=42) </p></li>
				<li>Next, we tell LightGBM about categorical features, which are already transformed into numeric variables, but need special treatment to compute the optimal split values:<p class="source-code">categories = ['Alone', 'Sex', 'Pclass', 'Embarked']</p></li>
				<li>Next, we create<a id="_idIndexMarker1230"/> the actual LightGBM training and test sets from the pandas DataFrames:<p class="source-code"># Create training set</p><p class="source-code">train_data = lgbm.Dataset(data=X_train, label=y_train, </p><p class="source-code">    categorical_feature=categories, free_raw_data=False)</p><p class="source-code"># Create testing set</p><p class="source-code">test_data = lgbm.Dataset(data=X_test, label=y_test,</p><p class="source-code">    categorical_feature=categories, free_raw_data=False)</p></li>
			</ol>
			<p>In contrast to scikit-learn, we cannot work directly with pandas DataFrames in LightGBM but need to use a wrapper class, <strong class="source-inline">lgbm.Dataset</strong>. This will give us access to all required optimizations and features, such as distributed training, optimization for sparse data, and meta-information about categorical features.</p>
			<ol>
				<li value="7">Having parsed the command-line arguments, we pass them into a parameter dictionary, which will then be passed to the LightGBM training method:<p class="source-code">lgbm_params = {</p><p class="source-code">    'application': 'binary',</p><p class="source-code">    'metric': 'binary_logloss',</p><p class="source-code">    'learning_rate': args.learning_rate,</p><p class="source-code">    'boosting': args.boosting,</p><p class="source-code">    'drop_rate': args.drop_rate,</p><p class="source-code">}</p></li>
				<li>All parameters that are passed through command-line arguments are automatically logged in Azure Machine Learning. However, if you want programmatic access to the model parameters or to display them in the experiment overview in Azure Machine Learning, we can log them in the <a id="_idIndexMarker1231"/>experiment. This will attach all the parameters to each run and make them available as parameter values in Azure Machine Learning. This means that we can later sort and filter the experiment runs by model parameters:<p class="source-code">for k, v in params.items():</p><p class="source-code">    run.log(k, v)</p></li>
			</ol>
			<p>Gradient boosting is an iterative optimization approach with a variable number of iterations and an optional early stopping criterion. Therefore, we also want to log all metrics for each iteration of the training script. Throughout this book, we will use a similar technique for all ML frameworks—namely, using a callback function that logs all available metrics to your Azure Machine Learning workspace. Let's write such a function using LightGBM's specification for custom callbacks.</p>
			<ol>
				<li value="9">Here, we create a callback object, which iterates over all the evaluation results and logs them for the run:<p class="source-code">def azure_ml_callback(run):</p><p class="source-code">    def callback(env):</p><p class="source-code">        if env.evaluation_result_list:</p><p class="source-code">            for data_name, eval_name, result, _ in \</p><p class="source-code">                env.evaluation_result_list:</p><p class="source-code">                run.log("%s (%s)" % (eval_name, </p><p class="source-code">                                     data_name), result)</p><p class="source-code">    callback.order = 10</p><p class="source-code">    return callback </p></li>
				<li>After we have set the parameters for the LightGBM predictor, we can configure the training and validation<a id="_idIndexMarker1232"/> procedure using the <strong class="source-inline">lgbm.train()</strong> method. We need to supply all arguments, parameters, and callbacks:<p class="source-code">clf = <strong class="bold">lgbm.train</strong>(train_set=train_data,</p><p class="source-code">                 params=lgbm_params,</p><p class="source-code">                 valid_sets=[train_data, test_data], </p><p class="source-code">                 valid_names=['train', 'val'],</p><p class="source-code">                 num_boost_round=args.num_boost_round,</p><p class="source-code">                 callbacks = [<strong class="bold">azure_ml_callback</strong>(run)])</p></li>
			</ol>
			<p>What's great about the preceding code is that by supplying the generic callback function, all training and validation scores will be logged to Azure automatically. Hence, we can follow the training iterations in real time, either in the UI or via the API—for example, inside a Jupyter widget that automatically collects all run information.</p>
			<ol>
				<li value="11">In order to evaluate the final training score, we use the trained classifier to predict a couple of default classification scores, such as <strong class="source-inline">accuracy</strong>, <strong class="source-inline">precision</strong>, and <strong class="source-inline">recall</strong>, as well as the combined <strong class="source-inline">f1</strong> score:<p class="source-code">y_pred = clf.predict(X_test)</p><p class="source-code">run.log("accuracy (test)", accuracy_score(y_test, </p><p class="source-code">                                          y_pred))</p><p class="source-code">run.log("precision (test)", precision_score(y_test, </p><p class="source-code">                                            y_pred))</p><p class="source-code">run.log("recall (test)", recall_score(y_test, y_pred))</p><p class="source-code">run.log("f1 (test)", f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>We could already run the script and see all the metrics and the performance of the model in Azure. But this was just the start – we want more!</p>
			<ol>
				<li value="12">Let's compute feature importance and track a plot of it and run it in Azure Machine Learning. We can do this in a few lines of code:<p class="source-code">fig = plt.figure()</p><p class="source-code">ax = plt.subplot(111)</p><p class="source-code">lgbm.plot_importance(clf, ax=ax)</p><p class="source-code">run.log_image("feature importance", plot=fig)</p></li>
			</ol>
			<p>Once this snippet<a id="_idIndexMarker1233"/> is added to the training script, each training run will also store a feature importance plot. This is helpful to see how different metrics influence feature importance.</p>
			<ol>
				<li value="13">There is one more step we would like to add. Whenever the training script runs, we want to upload the trained model and register it in the model registry. By doing so, we can later take any training run and manually or automatically deploy the model to a container service. However, this can only be done by saving the training artifacts of each run:<p class="source-code">import joblib</p><p class="source-code">joblib.dump(clf, 'outputs/lgbm.pkl')</p><p class="source-code">run.upload_file('lgbm.pkl', 'outputs/lgbm.pkl')</p><p class="source-code">run.register_model(model_name='lgbm_titanic', </p><p class="source-code">    model_path='lgbm.pkl')</p></li>
			</ol>
			<p>In the preceding snippet, we use the <strong class="source-inline">joblib</strong> package that originally was part of scikit-learn to save the classifier to disk. We then register the exported model as a LightGBM model in Azure Machine Learning.</p>
			<p>That's it – we have written the whole training script. It's not extremely long, it's not super-complicated. The trickiest part is understanding how to pick some of the parameters of LightGBM and understanding gradient boosting in general—and that's why we dedicated the first half of the chapter to that topic. Let's now fire up the cluster and submit the training script.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor163"/>Scheduling the training script on the Azure Machine Learning cluster</h2>
			<p>We are logically jumping<a id="_idIndexMarker1234"/> back to the authoring environment – the Jupyter notebook. The code<a id="_idIndexMarker1235"/> from the previous section is stored as a <strong class="source-inline">train_lgbm.py</strong> file, and we'll now get ready to submit it to the cluster. One great thing is that we made the training script configurable via command-line arguments, so we can tune the base parameters of the LightGBM model using CLI arguments. In the following steps, we will configure the authoring script to execute the training process:</p>
			<ol>
				<li value="1">Let's define the parameters for this model—we will use <strong class="source-inline">dart</strong>, with a standard learning rate of <strong class="source-inline">0.01</strong> and a dropout rate of <strong class="source-inline">0.15</strong>. We also pass the dataset as a named parameter to the training script:<p class="source-code">script_params = [</p><p class="source-code">  '--data', ds.as_named_input('titanic'),</p><p class="source-code">  '--boosting', 'dart',</p><p class="source-code">  '--learning-rate', '0.01',</p><p class="source-code">  '--drop-rate', '0.15',</p><p class="source-code">]</p></li>
			</ol>
			<p>We specified the boosting method, <strong class="source-inline">dart</strong>. As we learned in the previous section, this technique performs very well but is not extremely performant and is a bit slower than the other options—<strong class="source-inline">gbdt</strong>, <strong class="source-inline">rf</strong>, and <strong class="source-inline">goss</strong>.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">This is also the same way that hyperparameters are passed by <strong class="source-inline">HyperOpt</strong>—the hyperparameter tuning tool in Azure Machine Learning—to the training script. We will learn a lot more about this in <a href="B17928_11_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 11</em></a>, <em class="italic">Hyperparameter Tuning and Automated Machine Learning</em>.</p>
			<ol>
				<li value="2">Next, we can pass the parameters to <strong class="source-inline">ScriptRunConfig</strong> and kick off the training script:<p class="source-code">from azureml.core import ScriptRunConfig</p><p class="source-code">src = ScriptRunConfig(</p><p class="source-code">    source_directory=os.getcwd(),</p><p class="source-code">    script='train_lightgbm.py',</p><p class="source-code">    run_config= lgbm_config</p><p class="source-code">    arguments=script_params)</p></li>
			</ol>
			<p>In the preceding code, we specify the file of our classifier, which is stored relative to the current<a id="_idIndexMarker1236"/> authoring script. Azure Machine Learning<a id="_idIndexMarker1237"/> will upload the training script to the default datastore and make it available on all cluster nodes that run the script.</p>
			<ol>
				<li value="3">Finally, let's submit the run configuration and execute the training script:<p class="source-code">from azureml.widgets import RunDetails</p><p class="source-code">run = exp.submit(src)</p><p class="source-code">RunDetails(run).show()</p></li>
			</ol>
			<p>The <strong class="source-inline">RunDetails</strong> method gives us an interactive widget with real-time logs of the remote computing service. We can see the cluster getting initialized and scaled up, the Docker images getting built and registered, and ultimately, also the training script logs.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If you prefer other methods over an interactive Jupyter widget, you can also trail the logs using <strong class="source-inline">run.wait_for_completion(show_output=True)</strong> or <strong class="source-inline">print(run.get_portal_url())</strong> to get the URL to the experiment to run in Azure.</p>
			<ol>
				<li value="4">Let's now switch over to the Azure Machine Learning UI and look for the run in the experiment. Once we click on it, we can navigate to the <strong class="bold">Metrics</strong> section and find an overview of all our logged metrics. You can see in the following <em class="italic">Figure 9.6</em> how metrics that are logged<a id="_idIndexMarker1238"/> multiple times with the same<a id="_idIndexMarker1239"/> name get converted into vectors and displayed as line charts:</li>
			</ol>
			<div>
				<div id="_idContainer213" class="IMG---Figure">
					<img src="image/B17928_09_06.jpg" alt="Figure 9.6 – Validation loss " width="1012" height="621"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Validation loss</p>
			<p>Then, click on the <strong class="bold">Images</strong> section. When we do so, we are presented with the feature importance figure that we created in the training script. The following <em class="italic">Figure 9.7</em> shows how this looks in the Azure Machine Learning UI:</p>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="image/B17928_09_07.jpg" alt="Figure 9.7 – Feature importance " width="809" height="411"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Feature importance</p>
			<p>We saw how you can train a LightGBM classifier in Azure Machine Learning, taking advantage of an autoscaling Azure Machine Learning<a id="_idIndexMarker1240"/> compute cluster. Logging metrics, figures, and parameters<a id="_idIndexMarker1241"/> keeps all information about the training run in a single place. Together with saving snapshots of the training script, outputs, logs, and the trained model, this is invaluable for any professional, large-scale ML project.</p>
			<p>What you should remember from this chapter is that gradient boosted trees are a very performant and scalable classical ML approach, with many great libraries, and support for distributed learning and GPU acceleration. LightGBM is one alternative offered by Microsoft that is well embedded in both the Microsoft and open source ecosystem. If you are looking for a classical, fast, and understandable ML model, our advice is to go with LightGBM.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor164"/>Summary</h1>
			<p>In this chapter, you learned how to build a classical ML model in Azure Machine Learning.</p>
			<p>You learned about decision trees, a popular technique for various classification and regression problems. The main strengths of decision trees are that they require little data preparation as they work well on categorical data and different data distributions. Another important benefit is their interpretability, which is especially important for business decisions and users. This helps you to understand when a decision tree-based ensemble predictor is appropriate to use.</p>
			<p>However, we also learned about a set of weaknesses, especially regarding overfitting and poor generalization. Luckily, tree-based ensemble techniques such as bagging (bootstrap aggregation) and boosting help to overcome these problems. While bagging has popular methods such as random forests that parallelize very well, boosting, especially gradient boosting, has efficient implementations, including XGBoost and LightGBM.</p>
			<p>You implemented and trained a decision tree-based classifier in Azure Machine Learning using the LightGBM library. LightGBM is developed at Microsoft and delivers great performance and training time through a couple of optimizations. These optimizations help LightGBM to keep a small memory footprint, even for larger datasets, and yield better losses with fewer iterations. You used Azure Machine Learning not only to execute your training script but also to track your model's training performance and the final classifier.</p>
			<p>In the following chapter, we will take a look at some popular deep learning techniques and how to train them using Azure Machine Learning.</p>
		</div>
	</div>
</div>
</body></html>