<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Google Machine Learning APIs</h1>
                </header>
            
            <article>
                
<p>As seen in the previous chapter, machine learning is used in a wide variety of applications. However, a few applications are easy to build, while a few are very hard to build, especially for a user who is less familiar with machine learning. Some of the applications that we are going to discuss in this chapter fall in the hard to build category, as the process of building a machine learning model for these applications is data intensive, resource intensive, and requires a lot of knowledge in the field.</p>
<p>In this chapter, we will go over five machine learning APIs provided by Google (as of March 2018). These APIs are meant to be used out of the box, as RESTful APIs. For each service mentioned in the following, we will show what type of application can benefit from it, and how to interpret the returned results:</p>
<ul>
<li>Vision has a label detection, OCR, face detection and emotions, logo,and landmark</li>
<li>Speech means speech-to-text</li>
<li>NLP has entities, sentiment, and POS</li>
<li>Translation</li>
<li>Video intelligence</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Vision API</h1>
                </header>
            
            <article>
                
<p>The Vision API lets us build quite a few applications related to vision:</p>
<ul>
<li>Detecting labels in an image</li>
<li>Detecting the text in an image</li>
<li>Face detection</li>
<li>Emotion detection</li>
<li>Logo detection</li>
<li>Landmark detection</li>
</ul>
<p class="mce-root">Before we dive into building applications using the preceding, let's get a quick understanding of how they might be built, using face emotion detection as an example.</p>
<p>The process of detecting emotions involves:</p>
<ol>
<li>Collecting a huge set of images</li>
<li>Hand-labeling images with the emotion that is likely represented in the image</li>
<li>Training a <strong>convolutional neural network</strong> (<strong>CNN</strong>) (to be discussed in future chapters) to classify the emotion, based on an image as input</li>
</ol>
<p>While the preceding steps are heavily resource intensive (as we would need a lot of humans to collect and hand-label images), there are multiple other ways to obtain face emotion detection. We are not sure how Google is collecting and labeling images, but we will now consider the API that Google has built for us, so that, if we want to classify images into the emotions they represent, we can make use of that API.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Enabling the API</h1>
                </header>
            
            <article>
                
<p>Before we start building applications, we first have to enable the API, as follows:</p>
<ol>
<li>Search for the <span class="packt_screen">Google Cloud Vision API</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c0b69895-ef96-4500-836c-892128798f1b.png"/></div>
<ol start="2">
<li>Enable the <span class="packt_screen">Google Cloud Vision API</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-856 image-border" src="assets/3f0fd833-0aba-4b4a-a7d6-da1cfd61f563.png" style=""/></div>
<ol start="3">
<li>Once you click on <span class="packt_screen">ENABLE,</span> the API will be enabled for the project (that is, <span class="packt_screen">My First Project</span>), as seen in the preceding screenshot.</li>
<li>Fetch credentials for the API:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-857 image-border" src="assets/8af83174-c263-4cab-843e-e61d707c8774.png" style=""/></div>
<ol start="5">
<li>Click on <span class="packt_screen">Service account key</span> after clicking on <span class="packt_screen">Create credentials</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f0c5cd49-93fb-44db-aae9-12e910f22ad4.png" style=""/></div>
<ol start="6">
<li>Click on <span class="packt_screen">New service account</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-863 image-border" src="assets/e34a91c8-b9d0-4e6e-a48f-6d17ae69ff82.png" style=""/></div>
<ol start="7">
<li>Enter a service account name (in my case, <kbd>kish-gcp</kbd>) and <span class="packt_screen">Select a role</span> as the project <span class="packt_screen">Owner</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9a1e919d-a00a-4897-9b8d-fe2001b1b8f8.png" style=""/></div>
<ol start="8">
<li>Click on <span class="packt_screen">C</span><span class="packt_screen">reate</span> to save the JSON file of keys.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Opening an instance</h1>
                </header>
            
            <article>
                
<p>In order to open an instance, click on <span class="packt_screen">VM instances</span><span class="packt_screen"><span class="packt_screen">, </span></span>as shown in the screenshot that follows, and then click on the <span class="packt_screen">Activate google cloud shell</span> icon:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-841 image-border" src="assets/f8df15b9-f8a2-4a24-af2b-ec2d45251d15.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-840 image-border" src="assets/2370fe86-0ccd-48aa-8185-38b28125c7b9.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an instance using Cloud Shell</h1>
                </header>
            
            <article>
                
<p>Once we click on the cloud shell icon, we create an instance as follows:</p>
<ol>
<li>An instance is created by specifying the following code:</li>
</ol>
<pre style="padding-left: 60px">datalab create --no-create-repository &lt;instance name&gt;</pre>
<ol start="2">
<li>In the <span class="packt_screen">Cloud Shell</span>, the preceding code looks as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-865 image-border" src="assets/7b24116c-0aa3-4f1f-9458-d605dbadc306.png" style=""/></div>
<ol start="3">
<li>Once you have keyed in the responses for all of the prompts, you need to <span class="packt_screen">Change port</span> to <kbd>8081</kbd> to access Datalab, which is done as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/710044dc-2640-47d0-93b1-8ddbc07c34f8.png" style=""/></div>
<ol start="4">
<li>Once you click on <span class="packt_screen">Change port</span> you will get a window as follows. Enter <kbd>8081</kbd> and click on <span class="packt_screen">CHANGE AND PREVIEW</span> to open Datalab:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a2160b6a-880a-4a66-8c76-313dd4705a96.png" style=""/></div>
<ol start="5">
<li>This will open up Datalab, which has functionalities that enable us to write all types of commands: <kbd>bash</kbd>, <kbd>bigquery</kbd>, <kbd>python</kbd>, and so on.</li>
</ol>
<p class="mce-root">Now that the requirements are set up, let's fetch/install the requirements for the API:</p>
<ol>
<li>Accessing the API keys in the previous section, we have downloaded the required keys. Now, let's upload the <kbd>.json</kbd> file to Datalab by clicking on the <span class="packt_screen">Upload</span> button:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-858 image-border" src="assets/fba08f3a-b025-40eb-825f-6d338fcae55e.png" style=""/></div>
<ol start="2">
<li>Once the <kbd>.json</kbd> file is uploaded, you should be able to access it through Datalab from here:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e4a25f51-1121-4c68-a153-369213f94930.png" style=""/></div>
<ol start="3">
<li>Open a notebook; you can open a notebook in Datalab by clicking on the <span class="packt_screen">Notebook</span> tab, as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-843 image-border" src="assets/1c1dbb14-1143-4f17-a662-b9a1d269b821.png" style=""/></div>
<ol start="4">
<li>To install <kbd>google-cloud</kbd>, once you open the <span class="packt_screen">Notebook</span>, change the kernel from <span class="packt_screen">python2</span> to <span class="packt_screen">python3</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-847 image-border" src="assets/2c842bd5-f941-4f47-a91c-1a54681a9ff2.png" style=""/></div>
<ol start="5">
<li>Install the <kbd>google-cloud</kbd> package, as follows:</li>
</ol>
<pre style="padding-left: 60px">%bash<br/>pip install google-cloud</pre>
<ol start="6">
<li>Once <kbd>google-cloud</kbd> is installed, make sure that the <kbd>.json</kbd> file uploaded earlier is accessible in the current Python environment, by specifying the following:</li>
</ol>
<pre style="padding-left: 60px">import os<br/>os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/datalab/google-<br/>api.json"</pre>
<ol start="7">
<li>In order to upload an image of interest, we will look at transferring a file from the local machine into the <span class="packt_screen">bucket,</span> and from the <span class="packt_screen">bucket</span> to Datalab.</li>
</ol>
<ol start="8">
<li>Search for <kbd>bucket</kbd> in the Google Cloud:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-866 image-border" src="assets/6337862a-a63e-4cf7-b6c8-6d66838c8e77.png" style=""/></div>
<ol start="9">
<li>Now, name the bucket and create it:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/34631cf2-357f-435a-b990-77457e90d3fd.png" style=""/></div>
<ol start="10">
<li>Click on <span class="packt_screen">Upload</span> <strong>files</strong> to upload relevant files from the local machine to the bucket.</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/32220fa6-257a-444a-b7d2-1d24631e8a8f.jpg" style=""/></div>
<ol start="11">
<li>Once the file is uploaded to the bucket, fetch it from Datalab, as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4ebd04af-3b8c-499f-873f-b44a33315867.png" style=""/></div>
<ol start="12">
<li>Now, you should notice that <kbd>11.jpg</kbd> is accessible in Datalab.</li>
</ol>
<p>Now that the image to analyze is accessible in Datalab, let's understand the ways to leverage the Cloud Vision API to understand images better:</p>
<ol start="1">
<li>Import the relevant packages:</li>
</ol>
<pre style="padding-left: 60px">from google.cloud import vision</pre>
<p style="padding-left: 60px">The preceding code snippet makes sure that the methods available in Vision are accessible in the current session.</p>
<ol start="2">
<li>Invoke the service that performs <span class="packt_screen">Google Cloud Vision</span> API detection tasks (<span>such as face, landmark, logo, label, and text detection)</span> over client images—<kbd>ImageAnnotator</kbd>:</li>
</ol>
<pre style="padding-left: 60px">client = vision.ImageAnnotatorClient()</pre>
<ol start="3">
<li>Verify that the image is uploaded per expectation:</li>
</ol>
<pre style="padding-left: 60px">import matplotlib.pyplot as plt<br/>import matplotlib.image as mpimg<br/>%matplotlib inline<br/>img=mpimg.imread('/content/datalab/11.jpg')<br/>plt.axis('off')<br/>plt.imshow(img)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/eda014fe-a165-4cbb-b709-d7743b54d0f0.png" style=""/></div>
<ol start="4">
<li>Invoke the <kbd>face_detection</kbd> method to fetch the relevant details of the image, as follows:</li>
</ol>
<pre style="padding-left: 60px">response = client.face_detection({'source' : {'image_uri': "gs://kish-<br/>bucket/11.jpg"},})</pre>
<ol start="5">
<li>The responses to image annotations are as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-850 image-border" src="assets/b1b03a99-5a4c-43b2-8d2a-60f8a586d63d.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-855 image-border" src="assets/59629615-6973-44ba-9585-fbf796eba63e.png" style=""/></div>
<ol start="6">
<li>Now that we have run our method to detect faces in the image, let's look at the output - <kbd>response</kbd>. The output of <kbd>response</kbd> is a set of attributes, as described previously:</li>
</ol>
<pre style="padding-left: 60px">response</pre>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-852 image-border" src="assets/f2c8d0a0-778a-421f-9a5a-e2e31d1f90e2.png" style=""/></div>
<p>The following are the few more points explained in detail:</p>
<ul>
<li><strong>Bounding polygon</strong>: The bounding polygon is around the face. The coordinates of the bounding box are in the original image's scale, as returned in <kbd>ImageParams</kbd>. The bounding box is computed to frame the face in accordance with human expectations. It is based on the landmarker results. Note that one or more <em>x</em> and/or <em>y</em> coordinates may not be generated in the <kbd>BoundingPoly</kbd> (the polygon will be unbounded) if only a partial face appears in the image to be annotated.</li>
<li><strong>Face detection bounding polygon</strong>: The <kbd>fd_bounding_poly</kbd> bounding polygon is tighter than the <kbd>BoundingPoly</kbd>, and encloses only the skin part of the face. Typically, it is used to eliminate the face from any image analysis that detects the amount of skin visible in an image.</li>
<li><strong>Landmarks</strong>: Detected face landmarks.</li>
</ul>
<p>There are few more terms explained in the following points:</p>
<ul>
<li><kbd>roll_angle</kbd>: Roll angle, which indicates the amount of clockwise/anticlockwise rotation of the face, relative to the image . The range is [-180,180].</li>
<li><kbd>pan_angle</kbd>: Yaw angle, which indicates the leftward/rightward angle that the face is pointing, relative to the vertical plane perpendicular to the image. The range is [-180,180].</li>
<li><kbd>tilt_angle</kbd>: Pitch angle, which indicates the upwards/downwards angle that the face is pointing, relative to the image's horizontal plane. The range is [-180,180].</li>
<li><kbd>detection_confidence</kbd>: C<span>onfidence associated with the detection.</span></li>
<li><kbd>landmarking_confidence</kbd>: <span>C</span><span>onfidence associated with the landmarking.</span></li>
<li><kbd>joy_likelihood</kbd>: L<span>ikelihood</span><span> associated with the joy.</span></li>
<li><kbd>sorrow_likelihood</kbd>: <span>L</span><span>ikelihood</span><span> associated with the sorrow.</span></li>
<li><kbd>anger_likelihood</kbd>:<span> </span><span>L</span><span>ikelihood</span><span> associated with the anger.</span></li>
<li><kbd>surprise_likelihood</kbd>:<span> </span><span>L</span><span>ikelihood</span><span> associated with the surprise. </span></li>
<li><kbd>under_exposed_likelihood</kbd>:<span> </span><span>L</span><span>ikelihood</span><span> associated with the exposed. </span></li>
<li><kbd>blurred_likelihood</kbd>:<span> </span><span>L</span><span>ikelihood</span><span> associated with the blurred.</span></li>
<li><kbd>headwear_likelihood</kbd>:<span> </span><span>L</span><span>ikelihood</span><span> associated with the headwear. </span></li>
</ul>
<p>Face landmarks would further provide the locations of eyes, noses, lips, ears and so on.</p>
<p>We should be able to make a boundary box around the face identified.</p>
<p>The output of <kbd>face_annotations</kbd> is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e8b59b5a-4ea1-4e12-ba0b-a3c54a431b69.png" style=""/></div>
<p>From the preceding code, we should be able to understand the coordinates of the bounding box. In the code that follows, we calculate the starting point of the bounding box, and the corresponding width and height of the bounding box. Once the calculation is done, we superimpose the rectangle over the original image:</p>
<pre>import matplotlib.patches as patches<br/>import numpy as np<br/>fig,ax = plt.subplots(1)<br/><br/># Display the image<br/>ax.imshow(img)<br/><br/># Create a Rectangle patch<br/>x_width = np.abs(response.face_annotations[0].bounding_poly.vertices[1].x-<br/>  response.face_annotations[0].bounding_poly.vertices[0].x)<br/>y_height = np.abs(response.face_annotations[0].bounding_poly.vertices[1].y-<br/>  response.face_annotations[0].bounding_poly.vertices[3].y)<br/><br/>rect =<br/> patches.Rectangle((response.face_annotations[0].bounding_poly.vertices[0].x,<br/> response.face_annotations[0].bounding_poly.vertices[0].y),<br/>                         x_width,y_height,linewidth=5,edgecolor='y',facecolor='none')<br/><br/># Add the patch to the Axes<br/>ax.add_patch(rect)<br/>plt.axis('off')<br/>plt.show()</pre>
<p>The output of the preceding code is the image with a bounding box around the face, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-846 image-border" src="assets/1b10cd5c-cc20-4c35-9236-765efdb5b670.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Label detection</h1>
                </header>
            
            <article>
                
<p>In the previous code snippet, we used the <kbd>face_detection</kbd> method to fetch the various coordinates.</p>
<p>In order to understand the label of the image, we will be using the <kbd>label_detection</kbd> method in place of <kbd>face_detection</kbd>, as follows:</p>
<pre>response_label = client.label_detection({'source' : {'image_uri': "gs://kish-<br/>bucket/11.jpg"},})</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/202f4ad6-4b36-410b-aabe-88695abbd757.png" style=""/></div>
<p>The output of label detection is a collection of labels, along with the scores associated with each label.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text detection</h1>
                </header>
            
            <article>
                
<p>The text in an image can be identified by using the <kbd>text_detection</kbd> method, as follows:</p>
<pre>response_text = client.text_detection({'source' : {'image_uri': "gs://kish-<br/>bucket/11.jpg"},})</pre>
<p>The output of <kbd>response_text</kbd> is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ad3a9fa3-3b57-4f05-a1d0-9136cc27f118.png" style=""/></div>
<p>Note that the output of the <kbd>text_detection</kbd> method is the bounding box of the various text that is present in the image.</p>
<p>Also, note that the description of <kbd>text_annotations</kbd> provides the text detected in the image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logo detection</h1>
                </header>
            
            <article>
                
<p>Vision services also enable us to recognize the logo in an image by using the <kbd>logo_detection</kbd> method.</p>
<p>In the following code, you can see that we are able to detect the logo of <kbd>wikipedia</kbd> by passing the URL of the image's location, as follows:</p>
<pre>response = client.logo_detection({'source' : {'image_uri':<br/>"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Wikipedia-logo-v2-<br/>en.svg/135px-Wikipedia-logo-v2-en.svg.png"},})</pre>
<p>The output of the <kbd>logo_detection</kbd> method is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6b346970-15c3-4286-8f53-ac12b3f5753c.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Landmark detection</h1>
                </header>
            
            <article>
                
<p>Note that, in the preceding lines of code, we have specified the URL of the image location in the <kbd>logo_detection</kbd> method, and it resulted in a description of the predicted logo, and also the confidence score associated with it.</p>
<p>Similarly, any landmark located in an image can be detected by using the <kbd>landmark_detection</kbd> method, as follows:</p>
<pre>response = client.landmark_detection({'source' : {'image_uri': <br/> "https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/<br/>  Taj_Mahal_%28Edited%29.jpeg/250px-Taj_Mahal_%28Edited%29.jpeg"},})</pre>
<p>The output of the <kbd>landmark_detection</kbd> method is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a58efb19-07d8-4363-9f9c-37bb2458c7b1.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloud Translation API</h1>
                </header>
            
            <article>
                
<p>The Cloud Translation API provides a simple, programmatic interface for translating an arbitrary string into any supported language, using state-of-the-art neural machine translation. The Translation API is highly responsive, so websites and applications can integrate with the Translation API for fast, dynamic translation of source text from the source language to a target language (for example, French to English). Language detection is also available for cases in which the source language is unknown.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Enabling the API</h1>
                </header>
            
            <article>
                
<p>For us to be able to use Google cloud translation services, we need to enable, which is done as follows:</p>
<ol>
<li>In order to enable the <span class="packt_screen">Google Cloud Translation API</span>, search for the API in the console:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-854 image-border" src="assets/b9192c37-46b7-4d04-a53a-9e9018296cb1.png" style=""/></div>
<ol start="2">
<li>Enable the <span class="packt_screen">Google Cloud Translation API</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-844 image-border" src="assets/c6f06a96-ad9e-4b17-9fd2-9dc7ffa6f099.png" style=""/></div>
<ol start="3">
<li>Once the Translation API is enabled, the next step is to create credentials to access the API. However, note that if you have already created credentials for one API, they can be used for any other API. Let's go ahead and initialize our instance using Cloud Shell:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-849 image-border" src="assets/5152c3b3-6194-4621-89c8-e542b9653cea.png" style=""/></div>
<ol start="4">
<li>Once the instance starts, we will open Datalab on port <kbd>8081</kbd>. We provide a path to the location of the <kbd>api-key</kbd> file as follows:</li>
</ol>
<pre style="padding-left: 60px">import os<br/>os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/datalab/google-api.json"</pre>
<ol start="5">
<li>The various methods to <kbd>translate</kbd> are imported by using the following statement:</li>
</ol>
<pre style="padding-left: 60px">from google.cloud import translate</pre>
<ol start="6">
<li>Create a <kbd>client</kbd> object that creates a connection to the Cloud Translation service, as follows:</li>
</ol>
<pre style="padding-left: 60px">client = translate.Client()</pre>
<p class="mce-root">The <span class="packt_screen">Google Cloud Translation API</span> has three supported methods, and they are <kbd>get_languages()</kbd>, <kbd>detect_language()</kbd>, and <kbd>translate()</kbd>:</p>
<ul>
<li>The <kbd>client.get_languages()</kbd> method gives us a list of all of the available languages, and also their shorthand notations, as follows:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fff7307a-0a48-4a08-ae02-46c9dffce6fc.png" style=""/></div>
<ul>
<li>The <kbd>client.detect_language()</kbd> method detects the language that the text is written in:</li>
</ul>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/da018e49-6ab0-40a0-a710-b1c4db37929f.png"/></div>
<p style="padding-left: 60px">Note that in the preceding method, we have given two texts—one in Spanish, and the other in English. The preceding output represents the language of the text, along with the confidence associated with the detection of the language.</p>
<ul>
<li>The <kbd>client.translate()</kbd> method detects the source language and translates the text into English (by default), as follows:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/53851a88-716c-4b72-9094-fe2ff2ab68db.png"/></div>
<ul>
<li>The <kbd>client.translate()</kbd> method also gives us an option to specify the target language to which a text needs to be translated, as follows:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/76b2504a-a437-471b-ba5d-01d31127dfcd.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Natural Language API</h1>
                </header>
            
            <article>
                
<p>The Google Cloud Natural Language API reveals the structure and meaning of text by offering powerful machine learning models in an easy-to-use REST API. You can use it to extract information about people, places, events, and much more, that are mentioned in text documents, news articles, or blog posts. You can also use it to understand the sentiment about your product on social media, or to parse intent from customer conversations happening in a call center or a messaging app. You can analyze the text uploaded in your request, or integrate it with with your document storage on Google Cloud storage.</p>
<p>The <span class="packt_screen">Cloud Natural Language API</span> can be found by searching for it in your console, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-848 image-border" src="assets/a3f55923-2eb1-4136-8ca5-83284d52ef7e.png" style=""/></div>
<p>The <span class="packt_screen">Cloud Natural Language API</span> is enabled in the resulting page:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-859 image-border" src="assets/6e269f60-6e7e-421e-a5c1-d11aeb89a1af.png" style=""/></div>
<p class="mce-root">Similar to the Translation API, we do not have to create credentials for this API if at least one API is already enabled.</p>
<p class="mce-root">Natural language processing can be useful in extracting the sentiments associated with various text.</p>
<p class="mce-root">Sentiment analysis inspects the given text and identifies the prevailing emotional opinion within the text, to determine a writer's attitude as positive, negative, or neutral. Sentiment analysis is performed through the <kbd>analyzeSentiment</kbd> method.</p>
<p class="mce-root">In the following example, let's understand how to identify the sentiment of a statement:</p>
<ol>
<li>Import the relevant packages:</li>
</ol>
<pre style="padding-left: 60px">from google.cloud import language</pre>
<ol start="2">
<li>Initialize the class corresponding to the language service:</li>
</ol>
<pre style="padding-left: 60px">client = language.LanguageServiceClient()</pre>
<p>The <span class="packt_screen">Google Natural Language API</span> has the following supported methods:</p>
<ul>
<li><kbd>analyzeEntities</kbd></li>
<li><kbd>analyzeSentiment</kbd></li>
<li><kbd>analyzeEntitySentiment</kbd></li>
<li><kbd>annotateText</kbd></li>
<li><kbd>classifyText</kbd></li>
</ul>
<p>Each method uses a <kbd>Document</kbd> for representing text. Let's explore the <kbd>analyzeSentiment</kbd> method in the following example:</p>
<pre>text="this is a good text"<br/>from google.cloud.language_v1 import types<br/>document = types.Document(<br/>        content=text,<br/>        type='PLAIN_TEXT')<br/>sentiment = client.analyze_sentiment(document).document_sentiment<br/>sentiment.score</pre>
<p>Note that we have converted the input text into a <kbd>Document</kbd> type, and then analyzed the sentiment of the document.</p>
<p>The output of the sentiment score reflects the probability of a text being positive; the closer the score is to one, the more positive the statement is.</p>
<p>Similarly, one could pass on an HTML file, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f48ffe89-6faa-4933-8c74-485e248c3fa1.png" style=""/></div>
<p>Files that are stored in a Google Cloud bucket can also be referenced, by changing the content to <kbd>gcs_content_uri</kbd>, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-842 image-border" src="assets/2f8354ee-51fb-4f8e-a652-096019b87500.png" style=""/></div>
<p>The <kbd>analyze_entities()</kbd>method finds named entities (that is, proper names) in the text. This method returns an <kbd>AnalyzeEntitiesResponse</kbd>:</p>
<pre>document = language.types.Document(content='Michelangelo Caravaggio, Italian    painter, is known for "The Calling of Saint Matthew".'<br/>                                   ,type='PLAIN_TEXT') <br/>response = client.analyze_entities(document=document)<br/><br/>for entity in response.entities:<br/>  print('name: {0}'.format(entity.name)) </pre>
<p>The output of the preceding loop is the named entities present in the document's content, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/251c1f51-1758-40d2-b0e9-e1fc626ea55e.png" style=""/></div>
<p>We can also extract the part of speech of each of the words in the given text by using the <kbd>analyze_syntax</kbd> method, as follows:</p>
<ol>
<li>Tokenize the document into the corresponding words that constitute the text:</li>
</ol>
<pre style="padding-left: 60px">tokens = client.analyze_syntax(document).tokens<br/>tokens[0].text.content<br/># The preceding output is u'Michelangelo'</pre>
<ol start="2">
<li>The parts of speech of a <kbd>token</kbd> can then be extracted, as follows:</li>
</ol>
<pre style="padding-left: 60px">pos_tag = ('UNKNOWN', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM','PRON', 'PRT', 'PUNCT', 'VERB', 'X', 'AFFIX')<br/>for token in tokens:print(u'{}: {}'.format(pos_tag[token.part_of_speech.tag],<br/>                               token.text.content))</pre>
<p>The output of the preceding code is:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9abf115f-6c7d-4ac1-b8ed-4ce4059c4b33.png" style=""/></div>
<p>Note that the majority of the words are classified into the right parts of speech.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speech-to-text API</h1>
                </header>
            
            <article>
                
<p>The Google Cloud Speech API enables developers to convert audio to text, by applying powerful neural network models in an easy-to-use API. The API recognizes over 110 languages and variants. One can transcribe the text of users dictating to an application's microphone, enable command-and-control through voice, or transcribe audio files, among many use cases.</p>
<p>In order to enable the speech to text API, search for it in the console, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-853 image-border" src="assets/7751f4e4-0b32-4450-a80c-d80227d2600f.png" style=""/></div>
<p>In the resulting web page, enable the API, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-860 image-border" src="assets/b74baeec-4435-4ffb-9edc-c72f0988558f.png" style=""/></div>
<p>Similar to the APIs mentioned in the previous sections, credentials obtained for one API can be replicated for the other Google APIs. So, we don't have to create credentials separately for the speech to text API.</p>
<p>Once the API is enabled, let's start the Cloud Shell and Datalab, as we did in the previous sections.</p>
<p>In the following code, we transcribe a small audio file into text:</p>
<ol>
<li>Import the relevant packages and the API key:</li>
</ol>
<pre style="padding-left: 60px">from google.cloud import speech<br/>import os<br/>os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/datalab/google-api.json"<br/>from google.cloud.speech import enums<br/>from google.cloud.speech import types</pre>
<ol start="2">
<li>Invoke the speech service, as follows:</li>
</ol>
<pre style="padding-left: 60px">client = speech.SpeechClient()</pre>
<ol start="3">
<li>We can specify the audio that we want to convert, as follows:</li>
</ol>
<pre style="padding-left: 60px">audio = types.RecognitionAudio(uri='gs://kish-bucket/how_are_you.flac')</pre>
<div class="packt_infobox">Note that <strong>Free Lossless Audio Codec</strong> (<strong>FLAC</strong>).</div>
<p>An audio file (<kbd>.wav</kbd>) can be converted to a <kbd>.flac</kbd> file by using the converter located at <a href="https://audio.online-convert.com/convert-to-flac" target="_blank">https://audio.online-convert.com/convert-to-flac</a>.</p>
<p>The file is located in the bucket we created earlier. We specify the audio configuration, as follows:</p>
<pre>config = types.RecognitionConfig(<br/>encoding=enums.RecognitionConfig.AudioEncoding.FLAC,<br/>sample_rate_hertz=16000,<br/>language_code='en-US')</pre>
<p>A response is obtained by passing the <kbd>audio</kbd> content, as well as the configuration specified:</p>
<pre>response = client.recognize(config, audio)</pre>
<p>The results can now be accessed, as follows:</p>
<pre>for result in response.results: <br/>  print(result)</pre>
<p>The output for this is:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/82460a32-f922-4b33-a502-babcfed47d56.png" style=""/></div>
<p>The <kbd>recognize</kbd> method works when the input audio file is a short (&lt;1 minute) duration audio.</p>
<p>If the <kbd>audio</kbd> file is longer in duration, the method to be used is <kbd>long_running_recognize</kbd>:</p>
<pre>operation = client.long_running_recognize(config, audio)</pre>
<p>The <kbd>result</kbd> can then be accessed by specifying the following:</p>
<pre>response = operation.result(timeout=90)</pre>
<p>Finally, the transcription and the confidence can be obtained by printing the response results, as was done previously.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Video Intelligence API</h1>
                </header>
            
            <article>
                
<p><span class="packt_screen"><span><span>The</span></span> Cloud Video Intelligence API</span> makes videos searchable and discoverable, by extracting metadata with an easy-to-use REST API. You can now search every moment of every video file in your catalog. It quickly annotates videos stored in Google Cloud storage, and helps you to identify key entities (nouns) within your videos and when they occur.</p>
<p>The <span class="packt_screen">Cloud Video Intelligence API</span> can be searched for and enabled as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-862 image-border" src="assets/fd03c537-4531-4616-b22e-ce7363923d6a.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-861 image-border" src="assets/5a9ffecc-7966-4fd5-9ae7-f5d114eb5a1e.png" style=""/></div>
<p>We import the required packages and add the path to the <kbd>api-key</kbd>, as follows:</p>
<pre>from google.cloud import videointelligence<br/>import os<br/>os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/datalab/google-api.json"<br/>from google.cloud.speech import enums<br/>from google.cloud.speech import types</pre>
<p>The method <kbd>features</kbd> enables us to specify the type of content that we want to detect in a video. The features available are as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/54d48021-a4fb-496e-8b98-10cb80940baa.png" style=""/></div>
<p>Let's go ahead and detect labels in the video of interest to us:</p>
<pre>features = [videointelligence.enums.Feature.LABEL_DETECTION]</pre>
<p>We specify the <kbd>config</kbd> and context of the video, as follows:</p>
<pre>mode = videointelligence.enums.LabelDetectionMode.SHOT_AND_FRAME_MODE<br/>config = videointelligence.types.LabelDetectionConfig(<br/>    label_detection_mode=mode)<br/>context = videointelligence.types.VideoContext(<br/>    label_detection_config=config)</pre>
<p>The video then needs to be passed from Cloud storage, as follows:</p>
<pre>path="gs://kish-bucket/Hemanvi_video.mp4"<br/>operation = video_client.annotate_video(<br/>        path, features=features, video_context=context)</pre>
<p>The result of the <kbd>annotate_video</kbd> method is accessed as follows:</p>
<pre>result = operation.result(timeout=90)</pre>
<p>The annotation results for a video can be obtained at the:</p>
<ul>
<li>Video segment level</li>
<li>Video shot level</li>
<li>Frame level</li>
</ul>
<p>Results at the segment level, after looping through each of the various segment label annotations, can be obtained as follows:</p>
<pre>segment_labels = result.annotation_results[0].segment_label_annotations<br/>for i, segment_label in enumerate(segment_labels):<br/>    print('Video label description: {}'.format(<br/>        segment_label.entity.description))<br/>    for category_entity in segment_label.category_entities:<br/>        print('\tLabel category description: {}'.format(<br/>            category_entity.description))<br/><br/>    for i, segment in enumerate(segment_label.segments):<br/>        start_time = (segment.segment.start_time_offset.seconds +<br/>                      segment.segment.start_time_offset.nanos / 1e9)<br/>        end_time = (segment.segment.end_time_offset.seconds +<br/>                    segment.segment.end_time_offset.nanos / 1e9)<br/>        positions = '{}s to {}s'.format(start_time, end_time)<br/>        confidence = segment.confidence<br/>        print('\tSegment {}: {}'.format(i, positions))<br/>        print('\tConfidence: {}'.format(confidence))<br/>    print('\n')</pre>
<p>The output of the preceding code is:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9f133d48-607a-4b76-90f0-abfa9876765b.png" style=""/></div>
<p>Similarly, results at the shot level can be obtained as follows:</p>
<pre>shot_labels = result.annotation_results[0].shot_label_annotations<br/>for i, shot_label in enumerate(shot_labels):<br/>    print('Shot label description: {}'.format(<br/>        shot_label.entity.description))<br/>    for category_entity in shot_label.category_entities:<br/>        print('\tLabel category description: {}'.format(<br/>            category_entity.description))<br/><br/>    for i, shot in enumerate(shot_label.segments):<br/>        start_time = (shot.segment.start_time_offset.seconds +<br/>                      shot.segment.start_time_offset.nanos / 1e9)<br/>        end_time = (shot.segment.end_time_offset.seconds +<br/>                    shot.segment.end_time_offset.nanos / 1e9)<br/>        positions = '{}s to {}s'.format(start_time, end_time)<br/>        confidence = shot.confidence<br/>        print('\tSegment {}: {}'.format(i, positions))<br/>        print('\tConfidence: {}'.format(confidence))<br/>    print('\n')</pre>
<p>The output of the preceding lines of code is:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8670e457-6af0-4c19-b37d-e62ff23ad8a2.png" style=""/></div>
<p>Finally, the result at the frame level can be obtained as follows:</p>
<pre>frame_labels = result.annotation_results[0].frame_label_annotations<br/>for i, frame_label in enumerate(frame_labels):<br/>    print('Frame label description: {}'.format(<br/>        frame_label.entity.description))<br/>    for category_entity in frame_label.category_entities:<br/>        print('\tLabel category description: {}'.format(<br/>            category_entity.description))<br/><br/>    # Each frame_label_annotation has many frames,<br/>    # here we print information only about the first frame.<br/>    frame = frame_label.frames[0]<br/>    time_offset = (frame.time_offset.seconds +<br/>                   frame.time_offset.nanos / 1e9)<br/>    print('\tFirst frame time offset: {}s'.format(time_offset))<br/>    print('\tFirst frame confidence: {}'.format(frame.confidence))<br/>    print('\n')</pre>
<p>The output of the preceding lines of code is:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8a83da6e-44f2-4ad7-aa58-b6284fc4791d.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we went through the major machine learning APIs that Google provides: vision, translate, NLP, speech, and video intelligence. We have learned how the various methods in each of the APIs enable us to replicate deep learning results, without having to code from scratch.</p>
<p> </p>


            </article>

            
        </section>
    </body></html>