["```py\nimport numpy as np\n\nwidth = 15\nheight = 5\n\ny_final = width - 1\nx_final = height - 1\n\ny_wells = [0, 1, 3, 5, 5, 6, 7, 9, 10, 11, 12, 14]\nx_wells = [3, 1, 2, 0, 4, 3, 1, 3, 1, 2, 4, 1]\n\ny_prizes = [0, 3, 4, 6, 7, 8, 9, 12]\nx_prizes = [2, 4, 3, 2, 1, 4, 0, 2]\n\nstandard_reward = -0.1\ntunnel_rewards = np.ones(shape=(height, width)) * standard_reward\n\ndef init_tunnel_rewards():\n    for x_well, y_well in zip(x_wells, y_wells):\n        tunnel_rewards[x_well, y_well] = -5.0\n\n    for x_prize, y_prize in zip(x_prizes, y_prizes):\n        tunnel_rewards[x_prize, y_prize] = 1.0\n\n    tunnel_rewards[x_final, y_final] = 5.0\n\ninit_tunnel_rewards()\n```", "```py\nimport numpy as np\n\nnb_actions = 4\nmax_steps = 1000\nalpha = 0.25\nlambd = 0.6\ngamma = 0.95\n\ntunnel_values = np.zeros(shape=(height, width))\neligibility_traces = np.zeros(shape=(height, width))\npolicy = np.random.randint(0, nb_actions, size=(height, width)).astype(np.uint8)\n```", "```py\nimport numpy as np\n\nxy_grid = np.meshgrid(np.arange(0, height), np.arange(0, width), sparse=False)\nxy_grid = np.array(xy_grid).T.reshape(-1, 2)\n\nxy_final = list(zip(x_wells, y_wells)) + list(zip(x_prizes, y_prizes))\nxy_final.append([x_final, y_final])\n\nxy_start = []\n\nfor x, y in xy_grid:\n    if (x, y) not in xy_final:\n        xy_start.append([x, y])\n\nxy_start = np.array(xy_start)\n\ndef starting_point():\n    xy = np.squeeze(xy_start[np.random.randint(0, xy_start.shape[0], size=1)])\n    return xy[0], xy[1]\n```", "```py\nimport numpy as np\n\ndef is_final(x, y):\n    if (x, y) in zip(x_wells, y_wells) or (x, y) == (x_final, y_final):\n        return True\n    return False\n\ndef episode():\n    (i, j) = starting_point()\n    x = y = 0\n\n    e = 0\n\n    state_history = [(i, j)]\n\n    init_tunnel_rewards()\n    total_reward = 0.0\n\n    while e < max_steps:\n        e += 1\n\n        action = policy[i, j]\n\n        if action == 0:\n            if i == 0:\n                x = 0\n            else:\n                x = i - 1\n            y = j\n\n        elif action == 1:\n            if j == width - 1:\n                y = width - 1\n            else:\n                y = j + 1\n            x = i\n\n        elif action == 2:\n            if i == height - 1:\n                x = height - 1\n            else:\n                x = i + 1\n            y = j\n\n        else:\n            if j == 0:\n                y = 0\n            else:\n                y = j - 1\n            x = i\n\n        reward = tunnel_rewards[x, y]\n        total_reward += reward\n\n        td_error = reward + (gamma * tunnel_values[x, y]) - tunnel_values[i, j]\n        eligibility_traces[i, j] += 1.0\n\n        for sx, sy in state_history:\n            tunnel_values[sx, sy] += (alpha * td_error * eligibility_traces[sx, sy])\n            eligibility_traces[sx, sy] *= (gamma * lambd)\n\n        if is_final(x, y):\n            break\n        else:\n            i = x\n            j = y\n\n            state_history.append([x, y])\n\n            tunnel_rewards[x_prizes, y_prizes] *= 0.85\n\n    return total_reward\n\ndef policy_selection():\n    for i in range(height):\n        for j in range(width):\n            if is_final(i, j):\n                continue\n\n            values = np.zeros(shape=(nb_actions, ))\n\n            values[0] = (tunnel_rewards[i - 1, j] + (gamma * tunnel_values[i - 1, j])) if i > 0 else -np.inf\n            values[1] = (tunnel_rewards[i, j + 1] + (gamma * tunnel_values[i, j + 1])) if j < width - 1 else -np.inf\n            values[2] = (tunnel_rewards[i + 1, j] + (gamma * tunnel_values[i + 1, j])) if i < height - 1 else -np.inf\n            values[3] = (tunnel_rewards[i, j - 1] + (gamma * tunnel_values[i, j - 1])) if j > 0 else -np.inf\n\n            policy[i, j] = np.argmax(values).astype(np.uint8)\n```", "```py\nn_episodes = 5000\n\ntotal_rewards = []\n\nfor _ in range(n_episodes): \n    e_reward = episode()\n    total_rewards.append(e_reward)\n    policy_selection()\n```", "```py\nimport numpy as np\n\ntunnel_values = np.zeros(shape=(height, width))\n\ngamma = 0.99\nalpha = 0.25\nrho = 0.001\n```", "```py\nimport numpy as np\n\nnb_actions = 4\n\npolicy_importances = np.zeros(shape=(height, width, nb_actions))\n\ndef get_softmax_policy():\n    softmax_policy = policy_importances - np.amax(policy_importances, axis=2, keepdims=True)\n    return np.exp(softmax_policy) / np.sum(np.exp(softmax_policy), axis=2, keepdims=True)\n```", "```py\nimport numpy as np\n\ndef select_action(epsilon, i, j):\n    if np.random.uniform(0.0, 1.0) < epsilon:\n        return np.random.randint(0, nb_actions)\n\n    policy = get_softmax_policy()\n    return np.argmax(policy[i, j])\n\ndef action_critic_episode(epsilon):\n    (i, j) = starting_point()\n    x = y = 0\n\n    e = 0\n\n    while e < max_steps:\n        e += 1\n\n        action = select_action(epsilon, i, j)\n\n        if action == 0:\n            if i == 0:\n                x = 0\n            else:\n                x = i - 1\n            y = j\n\n        elif action == 1:\n            if j == width - 1:\n                y = width - 1\n            else:\n                y = j + 1\n            x = i\n\n        elif action == 2:\n            if i == height - 1:\n                x = height - 1\n            else:\n                x = i + 1\n            y = j\n\n        else:\n            if j == 0:\n                y = 0\n            else:\n                y = j - 1\n            x = i\n\n        reward = tunnel_rewards[x, y]\n        td_error = reward + (gamma * tunnel_values[x, y]) - tunnel_values[i, j]\n\n        tunnel_values[i, j] += (alpha * td_error)\n        policy_importances[i, j, action] += (rho * td_error)\n\n        if is_final(x, y):\n            break\n        else:\n            i = x\n            j = y\n```", "```py\nn_episodes = 50000\nn_exploration = 30000\n\nfor t in range(n_episodes):\n    epsilon = 0.0\n\n    if t <= n_exploration:\n        epsilon = 1.0 - (float(t) / float(n_exploration))\n\n    action_critic_episode(epsilon)\n```", "```py\nimport numpy as np\n\nnb_actions = 4\n\nQ = np.zeros(shape=(height, width, nb_actions))\n\nx_start = 0\ny_start = 0\n\nmax_steps = 2000\nalpha = 0.25\n```", "```py\nimport numpy as np\n\ndef is_final(x, y):\n    if (x, y) in zip(x_wells, y_wells) or (x, y) == (x_final, y_final):\n        return True\n    return False\n\ndef select_action(epsilon, i, j):\n    if np.random.uniform(0.0, 1.0) < epsilon:\n        return np.random.randint(0, nb_actions)\n    return np.argmax(Q[i, j])\n\ndef sarsa_step(epsilon):\n    e = 0\n\n    i = x_start\n    j = y_start\n\n    while e < max_steps:\n        e += 1\n\n        action = select_action(epsilon, i, j)\n\n        if action == 0:\n            if i == 0:\n                x = 0\n            else:\n                x = i - 1\n            y = j\n\n        elif action == 1:\n            if j == width - 1:\n                y = width - 1\n            else:\n                y = j + 1\n            x = i\n\n        elif action == 2:\n            if i == height - 1:\n                x = height - 1\n            else:\n                x = i + 1\n            y = j\n\n        else:\n            if j == 0:\n                y = 0\n            else:\n                y = j - 1\n            x = i\n\n        action_n = select_action(epsilon, x, y)\n        reward = tunnel_rewards[x, y]\n\n        if is_final(x, y):\n            Q[i, j, action] += alpha * (reward - Q[i, j, action])\n            break\n\n        else:\n            Q[i, j, action] += alpha * (reward + (gamma * Q[x, y, action_n]) - Q[i, j, action])\n\n            i = x\n            j = y\n```", "```py\nn_episodes = 20000\nn_exploration = 15000\n\nfor t in range(n_episodes):\n    epsilon = 0.0\n\n    if t <= n_exploration:\n        epsilon = 1.0 - (float(t) / float(n_exploration))\n\n    sarsa_step(epsilon)\n```", "```py\nimport numpy as np\n\ndef q_step(epsilon):\n    e = 0\n\n    i = x_start\n    j = y_start\n\n    while e < max_steps:\n        e += 1\n\n        action = select_action(epsilon, i, j)\n\n        if action == 0:\n            if i == 0:\n                x = 0\n            else:\n                x = i - 1\n            y = j\n\n        elif action == 1:\n            if j == width - 1:\n                y = width - 1\n            else:\n                y = j + 1\n            x = i\n\n        elif action == 2:\n            if i == height - 1:\n                x = height - 1\n            else:\n                x = i + 1\n            y = j\n\n        else:\n            if j == 0:\n                y = 0\n            else:\n                y = j - 1\n            x = i\n\n        reward = tunnel_rewards[x, y]\n\n        if is_final(x, y):\n            Q[i, j, action] += alpha * (reward - Q[i, j, action])\n            break\n\n        else:\n            Q[i, j, action] += alpha * (reward + (gamma * np.max(Q[x, y])) - Q[i, j, action])\n\n            i = x\n            j = y\n```", "```py\nn_episodes = 5000\nn_exploration = 3500\n\nfor t in range(n_episodes):\n    epsilon = 0.0\n\n    if t <= n_exploration:\n        epsilon = 1.0 - (float(t) / float(n_exploration))\n\n    q_step(epsilon)\n```", "```py\nimport numpy as np\n\nwidth = 5\nheight = 5\nnb_actions = 4\n\ny_final = width - 1\nx_final = height - 1\n\ny_wells = [0, 1, 3, 4]\nx_wells = [3, 1, 2, 0] \n\nstandard_reward = -0.1\ntunnel_rewards = np.ones(shape=(height, width)) * standard_reward\n\nfor x_well, y_well in zip(x_wells, y_wells):\n    tunnel_rewards[x_well, y_well] = -5.0\n\ntunnel_rewards[x_final, y_final] = 5.0\n```", "```py\nimport numpy as np\n\ndef reset_tunnel():\n    tunnel = np.zeros(shape=(height, width), dtype=np.float32)\n\n    for x_well, y_well in zip(x_wells, y_wells):\n        tunnel[x_well, y_well] = -1.0\n\n    tunnel[x_final, y_final] = 0.5\n\n    return tunnel\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential()\n\nmodel.add(Dense(8, input_dim=width * height))\nmodel.add(Activation('tanh'))\n\nmodel.add(Dense(4))\nmodel.add(Activation('tanh'))\n\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='mse')\n```", "```py\nimport numpy as np\n\ndef train(state, q_value):\n    model.train_on_batch(np.expand_dims(state.flatten(), axis=0), np.expand_dims(q_value, axis=0))\n\ndef get_Q_value(state):\n    return model.predict(np.expand_dims(state.flatten(), axis=0))[0]\n\ndef select_action_neural_network(epsilon, state):\n    Q_value = get_Q_value(state)\n\n    if np.random.uniform(0.0, 1.0) < epsilon:\n        return Q_value, np.random.randint(0, nb_actions)\n\n    return Q_value, np.argmax(Q_value)\n```", "```py\nimport numpy as np\n\nxy_grid = np.meshgrid(np.arange(0, height), np.arange(0, width), sparse=False)\nxy_grid = np.array(xy_grid).T.reshape(-1, 2)\n\nxy_final = list(zip(x_wells, y_wells))\nxy_final.append([x_final, y_final])\n\nxy_start = []\n\nfor x, y in xy_grid:\n    if (x, y) not in xy_final:\n        xy_start.append([x, y])\n\nxy_start = np.array(xy_start)\n\ndef starting_point():\n    xy = np.squeeze(xy_start[np.random.randint(0, xy_start.shape[0], size=1)])\n    return xy[0], xy[1]\n```", "```py\nimport numpy as np\n\ndef is_final(x, y):\n    if (x, y) in zip(x_wells, y_wells) or (x, y) == (x_final, y_final):\n        return True\n    return False\n\ndef q_step_neural_network(epsilon, initial_state): \n    e = 0\n    total_reward = 0.0\n\n    (i, j) = starting_point()\n\n    prev_value = 0.0\n    tunnel = initial_state.copy()\n    tunnel[i, j] = 1.0\n\n    while e < max_steps:\n        e += 1\n\n        q_value, action = select_action_neural_network(epsilon, tunnel)\n\n        if action == 0:\n            if i == 0:\n                x = 0\n            else:\n                x = i - 1\n            y = j\n\n        elif action == 1:\n            if j == width - 1:\n                y = width - 1\n            else:\n                y = j + 1\n            x = i\n\n        elif action == 2:\n            if i == height - 1:\n                x = height - 1\n            else:\n                x = i + 1\n            y = j\n\n        else:\n            if j == 0:\n                y = 0\n            else:\n                y = j - 1\n            x = i\n\n        reward = tunnel_rewards[x, y]\n        total_reward += reward\n\n        tunnel_n = tunnel.copy()\n        tunnel_n[i, j] = prev_value\n        tunnel_n[x, y] = 1.0\n\n        prev_value = tunnel[x, y]\n\n        if is_final(x, y):\n            q_value[action] = reward\n            train(tunnel, q_value)\n            break\n\n        else:\n            q_value[action] = reward + (gamma * np.max(get_Q_value(tunnel_n)))\n            train(tunnel, q_value)\n\n            i = x\n            j = y\n\n            tunnel = tunnel_n.copy()\n\n    return total_reward\n```", "```py\nn_episodes = 10000\nn_exploration = 7500\n\ntotal_rewards = []\n\nfor t in range(n_episodes):\n    tunnel = reset_tunnel()\n\n    epsilon = 0.0\n\n    if t <= n_exploration:\n        epsilon = 1.0 - (float(t) / float(n_exploration))\n\n    t_reward= q_step_neural_network(epsilon, tunnel)\n    total_rewards.append(t_reward)\n```", "```py\nimport numpy as np\n\ntrajectories = []\ntunnels_c = []\n\nfor i, j in xy_start:\n    tunnel = reset_tunnel()\n\n    prev_value = 0.0\n\n    trajectory = [[i, j, -1]]\n\n    tunnel_c = tunnel.copy()\n    tunnel[i, j] = 1.0\n    tunnel_c[i, j] = 1.0\n\n    final = False\n    e = 0\n\n    while not final and e < max_steps:\n        e += 1\n\n        q_value = get_Q_value(tunnel)\n        action = np.argmax(q_value)\n\n        if action == 0:\n            if i == 0:\n                x = 0\n            else:\n                x = i - 1\n            y = j\n\n        elif action == 1:\n            if j == width - 1:\n                y = width - 1\n            else:\n                y = j + 1\n            x = i\n\n        elif action == 2:\n            if i == height - 1:\n                x = height - 1\n            else:\n                x = i + 1\n            y = j\n\n        else:\n            if j == 0:\n                y = 0\n            else:\n                y = j - 1\n            x = i\n\n        trajectory[e - 1][2] = action\n        trajectory.append([x, y, -1])\n\n        tunnel[i, j] = prev_value\n\n        prev_value = tunnel[x, y]\n\n        tunnel[x, y] = 1.0\n        tunnel_c[x, y] = 1.0\n\n        i = x\n        j = y\n\n        final = is_final(x, y)\n\n    trajectories.append(np.array(trajectory))\n    tunnels_c.append(tunnel_c)\n\ntrajectories = np.array(trajectories) \n```"]