<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Failures of Banks - Univariate Analysis</h1>
                </header>
            
            <article>
                
<p>In recent years, big data and machine learning have become increasingly popular in many areas. It is generally believed that the greater the number of variables there are, the more accurate a classifier becomes. However, this is not always true.</p>
<p>In this chapter, we will reduce the number of variables in the dataset by analyzing the individual predictive power of each variable and using different alternatives.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Feature selection algorithm</li>
<li>Filter method </li>
<li>Wrapper method</li>
<li>Embedded methods</li>
<li>Dimensionality reduction</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature selection algorithm</h1>
                </header>
            
            <article>
                
<p>In this real-world case of predicting the failure of banks, we have a high number of variables or financial ratios to train a classifier, so we would expect to obtain a great predictive model. With this in mind, <span>why would we want to select alternate variables and reduce their number</span>?</p>
<p>Well, in some cases, increasing the dimensionality of the problem by adding new features could reduce the performance of our model. This is called the <strong>curse of dimensionality</strong> problem.</p>
<p>According to this problem, the fact of adding more features or increasing the dimensionality of our feature space will require collecting more data. In this sense, the new observations we need to collect have to grow exponentially quickly to maintain the learning process and to avoid overfitting.</p>
<p>This problem is commonly observed in <span><span>cases</span></span> in which the ratio between the number of variables and the observations in our data is not very high.</p>
<p>Feature selection is also useful for identifying and removing unneeded, irrelevant, and redundant variables from data and for reducing the complexity of the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature selection classes</h1>
                </header>
            
            <article>
                
<p>Three general classes of feature selection algorithms can be found in many machine learning guidelines. These include the following:</p>
<ul>
<li><strong>Filter methods</strong>: <span>In this method, the variables are selected according to the correlation with a target variable. Thus, it is measured by th</span><span>e ability of each variable to explain the target of the model. These methods are especially useful when the number of variables is high and they help to avoid overfitting. As a drawback, it is worth mentioning that, in spite of being non-predictive individually, and measured in a univariate way, the variables can become predictive when combined with other variables. To summarize, these methods do not consider the relationships between variables.</span></li>
<li><strong>Wrapper methods</strong>: <span>Wrapper methods evaluate subsets of variables to detect the possible interactions between variables. In wrapper methods, several combinations of variables are used in a predictive model, and a score is given to each combination according to the model accuracy. Consequently, it is possible to avoid irrelevant combinations. Nevertheless, these methods are very time consuming if the number of variables is large, and there is always a risk of overfitting, especially if the number of observations is low.</span></li>
<li><strong>Embedded methods</strong>: <span>Finally, </span><span>features that best contribute to improving the model accuracy are learned and remembered during the training process by the embedded methods. <strong>Regularization</strong> is one such feature selection method. They are also known as <strong>penalization methods</strong> because they impose constraints on the optimization parameters that usually get models with a fewer number of variables. Lasso, elastic net, and Ridge regression are the most common regularization methods. </span>Other examples of embedded feature selection algorithms include Lasso, elastic net, and Ridge regression algorithms. We will look at these models in more detail later.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Filter methods</h1>
                </header>
            
            <article>
                
<p>Let’s start with a filter method to reduce the number of variables in a first step. For that, we will measure the predictive power or the ability of a variable to classify our target variable individually and <span>correctly</span><span>.</span></p>
<p>In this case, we try to find variables that differentiate correctly between solvent and non-solvent banks. To measure the predictive power of a variable, we use a metric named <strong>Information Value</strong> (<strong>IV</strong>).</p>
<p>Specifically, given a grouped variable in <em>n</em> groups, each with a certain distribution of good banks and bad banks—or in our case, solvent and non-solvent banks—the information value for that predictor can be calculated as follows:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/238254df-559f-4acc-b4cc-5e7c7bfb3b94.png" style="width:37.92em;height:4.17em;"/></div>
<p>The IV statistic is generally interpreted depending on its value:</p>
<ul>
<li><strong>&lt; 0.02</strong>: The variable of analysis does not accurately separate the classes in the target variable</li>
<li><strong>0.02 to 0.1</strong>: The variable has a weak relationship with the target</li>
<li><strong>0.1 to 0.3</strong>: The variable displays a medium-strength relationship</li>
<li><strong> &gt; 0.3</strong>: The variable is a good predictor of the target</li>
</ul>
<p>According to this value, this variable is itself very predictive. Therefore, this variable could be useful to use in our model. Let’s see an example <span>calculation of <kbd>IV</kbd> f</span><span>or a variable</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-691 image-border" src="assets/211876f5-808f-43cb-b855-e1ad082758e3.png" style=""/></div>
<p>In the preceding table, we can see the calculation of the information value of the <kbd>UBPRE006</kbd> variable, which represents the sum of provisions for loans and losses divided by the total assets of a bank.</p>
<p>Broadly speaking, when a loan is granted, part of it must be provisioned in case the credit becomes delinquent; that is, banks make two types of provisions in their income statement to cover the so-called credit risk: the generic ones, which are made at the time the loan is granted, and the specific ones, which cover the unpaid credits.</p>
<p>In theory, the higher the ratio, the more likely a bank is to fail, because, if the level of provisions is high, this is an indication that the credit quality of its loans will be lower.</p>
<p>Remember that, in our sample, the percentage of failed banks is 4.70%. In this example, the <kbd>UBPRE006</kbd> variable has been grouped into four categories and an additional category measuring the level of missing values. This can be seen in the <kbd>BadRate</kbd> column as the ratio of failed banks that have a value in this ratio that is lower than 0.5487%. <span>This is very low, representing only 0.80% of banks in this group</span>. As this ratio increases, the ratio of failed banks is higher as well. Additionally, there are no banks with missing values in this ratio.</p>
<p>The value appearing in the first group of this table is calculated according to this equation:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/b796ccf0-b1f5-4206-8b47-9a334aa76700.png" style="width:21.17em;height:2.75em;"/></div>
<p>The sum of all the values in the <kbd>IV</kbd> column can be found in row <kbd>6</kbd> of this table in the <kbd>3.2803 </kbd>column.</p>
<p>According to this value, this variable is itself very predictive. Therefore, it could be useful to use this variable in our model.</p>
<p><span><span>On the other hand, <strong>Weight of Evidence</strong> (<strong>WoE</strong>) is a metric that is very closely related to the information value.</span></span> This metric is also included in the preceding table. WoE is calculated as follows:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/01c6b3c7-47fc-41d3-89e9-4d833b89d755.png" style="width:15.75em;height:2.58em;"/></div>
<p>In fact, the WoE equation is a part of the IV metric. The value of WoE will be 0 if the odds of the <em>Goods <span>to B</span>ads </em>ratio is equal to 1.</p>
<p>If the percentage of <em>Bads</em> in a group is greater than the percentage of <em>Goods</em>, the odds ratio will be less than 1 and the WoE will be a negative number; if the number of <em>Goods</em> is greater than the <em>Bads</em> in a group, the WoE value will be a positive number.</p>
<p>In general, positive values of WoE indicates that banks placed in the group are more solvent than the average of total banks in the sample. On the other hand, the <span><span>higher </span></span>the negative value, the riskier the banks in this group are.</p>
<p class="mce-root"/>
<p>We are going to calculate the information value for each variable in our training set. The <kbd>smbinning</kbd> package is very useful for this purpose.</p>
<p>As already seen, one important step is grouping variables, which is done automatically in this package.</p>
<p>We will carry out two different experiments. Thus, the information value will be calculated for the training set before and after the missing imputation. We will discuss the reason behind these experiments later in this section.</p>
<p>This package assumes that the target variable should take the value of 1 if a bank is solvent and 0 otherwise, which is t<span>he exact opposite of</span> what we did in previous steps. So, the first step consists of reversing the target values:</p>
<pre>aux_original&lt;-train<br/>aux_original$Defaultf&lt;-as.numeric(as.character(aux_original$Default))<br/>aux_original$Defaultf&lt;-ifelse(aux_original$Default==1,0,1)<br/>aux_nomiss&lt;-train_nomiss<br/>aux_nomiss$Defaultf&lt;-as.numeric(as.character(aux_nomiss$Default))<br/>aux_nomiss$Defaultf&lt;-ifelse(aux_nomiss$Default==1,0,1)</pre>
<p>Next, we run the following code (this is a very time-consuming process):</p>
<pre>library(smbinning)<br/>table_iv&lt;-matrix("NA",0,5)<br/>table_iv&lt;-data.frame(table_iv)<br/>colnames(table_iv)&lt;-c("Char","IV_original","Process_original","IV_nomiss","Process_nomiss")<br/> <br/> for (var in 1:length(aux_original[,2:1408]))<br/> {<br/> variable&lt;-colnames(aux_original)[var+1]<br/> aux_original2&lt;-aux_original[,c(variable,"Defaultf")]<br/> aux_nomiss2&lt;-aux_nomiss[,c(variable,"Defaultf")]<br/> temp1&lt;-smbinning.sumiv(aux_original2, "Defaultf")<br/> temp2&lt;-smbinning.sumiv(aux_nomiss2, "Defaultf")<br/> colnames(temp1)&lt;-c("Char","IV_original","Process_original")<br/> colnames(temp2)&lt;-c("Char","IV_nomiss","Process_nomiss")<br/> temp2$Char&lt;-NULL<br/> temp1&lt;-cbind(temp1,temp2)<br/> table_iv&lt;-rbind(table_iv,temp1)<br/> }</pre>
<p>The previous code creates a table where information values will be stored (<kbd>table_iv</kbd>). For each variable in the <kbd>train</kbd> dataset, the information value is then calculated using the <kbd>smbinning.sumiv</kbd> function.</p>
<p class="mce-root"/>
<p>Once the process finishes, a backup of the workspace is created:</p>
<pre>save.image("Data8.RData")</pre>
<p>Let’s look at the results:</p>
<pre>head(table_iv)<br/> ## Char IV_original      Process_original IV_nomiss<br/> ## 1 UBPR1795      2.6138    Numeric binning OK    2.6138<br/> ## 2 UBPR4635      2.5253    Numeric binning OK    2.5253<br/> ## 3 UBPRC233          NA No significant splits        NA<br/> ## 4 UBPRD582          NA    Uniques values &lt; 5        NA<br/> ## 5 UBPRE386          NA No significant splits        NA<br/> ## 6 UBPRE388      0.5853    Numeric binning OK    0.5622<br/> ##          Process_nomiss<br/> ## 1    Numeric binning OK<br/> ## 2    Numeric binning OK<br/> ## 3 No significant splits<br/> ## 4    Uniques values &lt; 5<br/> ## 5 No significant splits<br/> ## 6    Numeric binning OK</pre>
<p>In this table, we have the information value of each variable in the training sample before and after missing imputation.</p>
<p>Behind the information value, a column informs us about the calculation status. Different messages could be displayed here as follows:</p>
<ul>
<li><kbd>Numeric binning OK</kbd>: The information value has been calculated correctly and at least two different groups can be differentiated in the variable to discriminate between <em>good</em> and <em>bad</em> banks.</li>
<li><kbd>No significant splits</kbd>: The information value is not calculated because the variable is not able to discriminate against the future solvency of a bank. Different groups of banks are not detected.</li>
<li><kbd>Uniques values &lt; 5</kbd>: A variable has fewer than five different values. In this case, the information value is not calculated.</li>
</ul>
<p>Results are different depending on whether the missing values have previously <span>been</span><span> </span><span>treated or whether the missing values are included in the example:</span></p>
<pre>table(table_iv$Process_original)<br/> ##<br/> ##    Numeric binning OK No significant splits    Uniques values &lt; 5<br/> ##                   522                   807                    78<br/>table(table_iv$Process_nomiss)<br/> ##<br/> ##    Numeric binning OK No significant splits    Uniques values &lt; 5<br/> ##                   539                   790                    78</pre>
<p>The <kbd>smbinning</kbd> package considers the missing values as a new category that, in some cases, could contain relevant information or be more related with one class in our target variable.</p>
<p>It is important to be aware that imputation of missing variables implies a kind of restriction on your data.</p>
<p>Let’s check the impact of this simple decision in terms of predictive power. Differences of <kbd>IV</kbd> existing, or not missing values, are calculated as follows:</p>
<pre>diff_iv&lt;-table_iv[complete.cases(table_iv) &amp; table_iv$Process_original=="Numeric binning OK" &amp;table_iv$Process_nomiss=="Numeric binning OK" ,]<br/> <br/>diff_iv$diff&lt;-(diff_iv$IV_nomiss - diff_iv$IV_original)<br/>hist(diff_iv$diff, border=TRUE , col=rgb(0.8,0.2,0.8,0.7) , main="")</pre>
<p>Let's look at the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-692 image-border" src="assets/c50ea91c-8653-480e-9ada-b7713b9310a9.png" style=""/></div>
<p>Here are the summary statistics:</p>
<pre>summary(diff_iv$diff)<br/> ##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.<br/> ## -2.64110 -0.03052  0.00000 -0.05247  0.00000  0.22570</pre>
<p>According to the summary statistics, on average, missing imputation reduces the predictive power or information value of variables by 5.247%. In some cases, variables increase the predictive power as well.</p>
<p>We can classify the predictive power of variables depending on its information value. We will consider the thresholds explained previously to define whether a variable displays a strong, medium, or weak predictive power:</p>
<pre>table_iv$IV_Category&lt;-ifelse(table_iv$IV_nomiss &gt;= 0.3, "1:Strong", ifelse(table_iv$IV_nomiss &gt;= 0.1, "2:Medium","3:Weak"))<br/><br/>table(table_iv$IV_Category)<br/> ##<br/> ## 1:Strong 2:Medium   3:Weak<br/> ##      358      114       67</pre>
<p>In this step, we can remove variables with a low predictive power. Thus, from more than a thousand variables we have in the dataset, we will only select variables classified as <kbd>Strong</kbd> or <kbd>Medium</kbd>:</p>
<pre>table_iv&lt;-table_iv[complete.cases(table_iv) &amp; table_iv$IV_Category != "3:Weak",]</pre>
<p>Despite there being limitations, we will use data where missing values have been previously treated:</p>
<pre>train&lt;-train_nomiss<br/>test&lt;-test_nomiss</pre>
<p>Normally, the use of univariate transformations on the variables that may be part of a multivariate model improve its discriminatory power.</p>
<p>Variable transformation is helpful for mitigating the effect that outliers produce over the model development and for capturing non-linear relationships between the variable and the target.</p>
<p>One of the most common practices in credit risk consists of the transformation of variables to a different category and then to assign to each category the value of its corresponding WoE. Let’s see an example of this. Again, the <kbd>smbinning</kbd> package is used.</p>
<p class="mce-root"/>
<p>For this, we first need to convert the target to the opposite values we have due to package restrictions:</p>
<pre>train$Defaultf&lt;-as.numeric(as.character(train$Default))<br/>train$Defaultf&lt;-ifelse(train$Default==1,0,1)<br/> <br/>test$Defaultf&lt;-as.numeric(as.character(test$Default))<br/>test$Defaultf&lt;-ifelse(test$Default==1,0,1)</pre>
<p>We apply the <kbd>smbinning</kbd> function to the <kbd>UBPRD486</kbd> variable or <kbd>Tier One Leverage Capital</kbd>. The <kbd>Tier One</kbd> ratio represents the core measure of a bank’s financial strength from a regulator’s point of view. The higher the ratio, the higher the solvency and strength of a bank.</p>
<p>First, we analyze the distribution of this variable for failed and non-failed banks:</p>
<pre>boxplot(train$UBPRD486~train$Default, horizontal=T, frame=F, col="lightgray",main="Tier One Leverage Ratio Distribution")</pre>
<p>Here's the tier-one ratio distribution plot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-693 image-border" src="assets/4405e573-41f5-4cbe-ad8c-11880830eeea.png" style=""/></div>
<p>In general, failed banks display lower values in this ratio according to the previous screenshot. Applying the <kbd>smbinning</kbd> function, an object is created and variables are categorized.</p>
<p>Some graphical analysis can be carried out as follows:</p>
<pre>smbinning.plot(result,option="dist")</pre>
<p>The following screenshot describes the categorization very well:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-694 image-border" src="assets/786c28b5-2a19-492c-8ac8-ccc97482ced3.png" style=""/></div>
<p>The 74.6% of banks displays a <span>higher</span><span> </span><span>value than 8.16 in this variable . Let’s look at the percentage of failed banks by group:</span></p>
<pre>smbinning.plot(result,option="badrate")</pre>
<p>The resulting plot below, shows that high bad rates are observed in banks where value of this ratio takes lower or equal values than <strong>5.6</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-695 image-border" src="assets/da3fd1e1-1ee7-4de5-80f8-85dc9d64a8c4.png" style=""/></div>
<p>As commented, the previous screenshot shows that the lower the value of the ratio, the higher the number of failed banks. This variable is very predictive because it is easy to find groups <span><span>by</span></span> collecting a high number of failed banks. Consequently, the 74.8% of banks included in the first group went bankrupt. It is also possible to plot the <kbd>WoE</kbd> value for each group running the following code:</p>
<pre>smbinning.plot(result,option="WoE")</pre>
<p>The preceding code provides the following screenshot where weight of evidence values are shown:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-696 image-border" src="assets/7ad480a8-0ebb-4317-b893-8ea6adf70e45.png" style=""/></div>
<p>For some credit applications, as the development of scoring models, it is a very common practice to replace the value of original values of each ratio to the corresponding weight of evidence value as shown in the preceding plot. For example, values lower or equal than 5.6 will be replaced by the value -4.1. Thus, WoE variables are then used to train the model, using logistic regression, the most common approach.</p>
<p>The <kbd>smbinning</kbd> package also helps to transform original variables to their corresponding groups. In my experience, I didn’t find much evidence that WoE transformation really improves the performance of a model. So, in this case, we are not going to transform our variables.</p>
<p>Weak variables are removed as well:</p>
<pre>relevant_vars&lt;-as.vector(table_iv$Char)<br/>relevant_vars&lt;-c("ID_RSSD","Default","Defaultf", relevant_vars)<br/>train&lt;-train[,relevant_vars]<br/>test&lt;-test[,relevant_vars]</pre>
<p>The workspace is then saved as follows:</p>
<pre>save.image("Data9.RData")</pre>
<p>We can continue filtering variables. At this point, the dimensions of our dataset are these:</p>
<pre>dim(train)<br/> ## [1] 7091  465</pre>
<p>When facing regression or classification problems, some models perform better if highly correlated attributes are removed. Correlations can be obtained as follows:</p>
<pre>correlations &lt;- cor(train[,4:ncol(train)])</pre>
<p>The <kbd>findCorrelation</kbd> function that belongs to the caret package performs a search on the correlation matrix and outputs vectors that contain integers. These integers correspond to columns which, if removed, can reduce the pairwise correlation. This function thus looks for attributes that have a higher level of correlation. </p>
<p>It works by considering the absolute values of pairwise correlated columns. It removes the variables that have the largest mean absolute, which is calculated by comparing variables having a high correlation:</p>
<pre>## Loading required package: lattice<br/>highlyCorrelated &lt;- data.frame("Char"=findCorrelation(correlations,      cutoff=0.75,names = TRUE))</pre>
<p class="mce-root">The cut-off option is the threshold for selecting which variables are highly correlated:</p>
<pre><br/>correlated_vars&lt;-as.vector(highlyCorrelated$Char)<br/>non_correlated_vars&lt;-!(colnames(train) %in% correlated_vars)<br/> <br/>train&lt;-train[,non_correlated_vars]<br/>test&lt;-test[,non_correlated_vars]</pre>
<p>A total of <kbd>262</kbd> variables remain in the dataset:</p>
<pre>ncol(train)<br/> #262</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wrapper methods</h1>
                </header>
            
            <article>
                
<p>As stated at the beginning of this section, <strong>wrapper methods</strong> evaluate subsets of variables to detect the possible interactions between variables being a step ahead of the filter methods.</p>
<p>In wrapper methods, several combinations of variables are used in a predictive model and a score is given to each combination according to the model accuracy.</p>
<p>In wrapper methods, a classifier is iteratively trained with multiple combinations of variables acting as a black box, for which the only output is a ranking of important features.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boruta package</h1>
                </header>
            
            <article>
                
<p>One of the most known wrapper packages in R is called <kbd>Boruta</kbd>. This package is mainly based on the algorithm of <strong>random forests</strong>.</p>
<p>Although this algorithm will be explained in more detail later in the book, Boruta, proposed by Breiman in 2001, is a tool that mines data and generates many decision trees on the samples and combines them by majority voting. The purpose of random forests creating different decision trees is to acquire the best possible classifications from different classes of data. </p>
<p>One example of a successful implementation of random forests is in credit card fraud detection systems.</p>
<p>In the <kbd>Boruta</kbd> package, randomized variables are created using multiple combinations of other variables in the dataset.</p>
<p>New variables are then combined with the original variables and they train a different random forest. The importance of the different features is obtained by comparing the importance of random variables with the importance of the original variables.</p>
<p>Only variables with a higher importance than that of the randomized variables are considered important. The <kbd>Boruta</kbd> package is very time consuming if the number of variables is high, especially because the algorithm creates even more variables to rank its features.</p>
<p>Let’s launch the <kbd>Boruta</kbd> algorithm in R. First, a <kbd>seed</kbd> is established to make the exercise reproducible:</p>
<pre>set.seed(123)</pre>
<p>An auxiliary table is then created from the train dataset and no relevant variables are removed:</p>
<pre>aux&lt;-train<br/>aux$`ID_RSSD`&lt;-NULL<br/>aux$Defaultf&lt;-NULL</pre>
<p>Finally, the <kbd>Boruta</kbd> algorithm is launched (this is very time consuming and can take over an hour):</p>
<pre>library(Boruta)<br/>wrapper &lt;- Boruta(Default ~. , data = aux, doTrace = 2,maxRuns = 100)</pre>
<p>When the <kbd>wrapper</kbd> <span>object</span><span> is printed, it provides the significance of features in the dataset.</span></p>
<p>The <kbd>Boruta</kbd> algorithm comes to a conclusion about any of the variables we have in our database:</p>
<pre>print(wrapper)<br/> ## Boruta performed 99 iterations in 1.15968 hours.<br/> ##  85 attributes confirmed important: UBPR2150, UBPR7402, UBPRA222,<br/> ## UBPRD488, UBPRD646 and 80 more;<br/> ##  139 attributes confirmed unimportant: UBPR0071, UBPR1590,<br/> ## UBPR1616, UBPR1658, UBPR1661 and 134 more;<br/> ##  35 tentative attributes left: UBPR2366, UBPR3816, UBPRE083,<br/> ## UBPRE085, UBPRE140 and 30 more;</pre>
<p>Many variables are classified as important and unimportant, but in other cases, variables are assigned in a tentative category:</p>
<pre>table(wrapper$finalDecision)<br/> ##<br/> ## Tentative Confirmed  Rejected<br/> ##        35        85       139</pre>
<p>Tentative features have an importance that is so close to their best random features. In such cases, <kbd>Boruta</kbd> is not able<span> to make a confident decision with regard to the default number of random forest iterations.</span></p>
<p>Let’s make a workspace backup before moving ahead:</p>
<pre>save.image("Data10.RData")</pre>
<p>Through the <kbd>TentativeRoughFix</kbd> function, it is possible to make a decision on the tentative variables. For this, the median feature Z-score with the median Z-score of the most important random feature is compared and a decision is made:</p>
<pre>wrapper &lt;- TentativeRoughFix(wrapper)<br/>print(wrapper)<br/> ## Boruta performed 99 iterations in 1.15968 hours.<br/> ## Tentatives roughfixed over the last 99 iterations.<br/> ##  108 attributes confirmed important: UBPR2150, UBPR3816, UBPR7402,<br/> ## UBPRA222, UBPRD488 and 103 more;<br/> ##  151 attributes confirmed unimportant: UBPR0071, UBPR1590,<br/> ## UBPR1616, UBPR1658, UBPR1661 and 146 more;</pre>
<p>Consequently, and according to this package, our train sample will be reduced to only <kbd>99</kbd> variables.</p>
<p><kbd>Boruta</kbd> is not the only wrapper approach. The <kbd>caret</kbd> package also includes a wrapper filter. In this case, the algorithm is called <strong>Recursive Feature Elimination</strong> (<strong>RFE</strong>).</p>
<p>In this algorithm, first a model is trained using all independent variables and the importance of features is calculated. The less important variables (<em>n</em>) are removed from the sample and the model is trained again. This step is repeated several times until all variables are used. In each iteration, the performance of the model is assessed. Top variables in the model with the best performance are determined as the best predictors.</p>
<p>In this algorithm, apart from random forests (<kbd>rfFuncs</kbd>), there are many models that can be used for training, such as the following:</p>
<ul>
<li>Linear regression, the <kbd>lmFuncs</kbd> function</li>
<li>Naive Bayes function, <kbd>nbFuncs</kbd></li>
<li>Bagged trees function, <kbd>treebagFuncs</kbd></li>
</ul>
<p>Let’s see how this algorithm can be used in R:</p>
<ol>
<li>First, fix a <kbd>seed</kbd> to obtain the same results:</li>
</ol>
<pre style="padding-left: 60px">library(caret)<br/>set.seed(1234)</pre>
<ol start="2">
<li>Convert the target variable to a <kbd>factor</kbd>. Thus, the algorithm is used for classification. If not, it is assumed to be a regression problem:</li>
</ol>
<pre style="padding-left: 60px">aux$Default&lt;-as.factor(aux$Default)</pre>
<ol start="3">
<li>Finally, run the algorithm. Random forest is chosen as a classifier with a 10-fold validation (this execution is also time consuming):</li>
</ol>
<pre style="padding-left: 60px">rfe_control &lt;- rfeControl(functions=rfFuncs, method='cv', number=10)<br/>recursive &lt;- rfe(aux[,2:260], aux[,1], rfeControl=rfe_control)</pre>
<p>If the <kbd>recursive</kbd> object is printed, the most important variables are displayed:</p>
<pre>print(recursive, top=10)<br/> ##<br/> ## Recursive feature selection<br/> ##<br/> ## Outer resampling method: Cross-Validated (10 fold)<br/> ##<br/> ## Resampling performance over subset size:<br/> ##<br/> ##  Variables Accuracy  Kappa AccuracySD KappaSD Selected<br/> ##          4   0.9848 0.8224   0.005833 0.06490        <br/> ##          8   0.9866 0.8451   0.004475 0.04881        <br/> ##         16   0.9884 0.8685   0.005398 0.06002        <br/> ##        259   0.9886 0.8659   0.004019 0.04617        *<br/> ##<br/> ## The top 10 variables (out of 259):<br/> ##    UBPRD488, UBPRE626, UBPRE217, UBPRE170, UBPRE392, UBPRE636, UBPRE883, UBPRE394, UBPRE370, UBPRE074<br/>plot(recursive, type=c("g", "o"), cex = 1.0)</pre>
<p>The following output will be obtained:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-697 image-border" src="assets/346afcb8-0154-4c9e-aff0-7c33ea6ec316.png" style=""/></div>
<p>Let's get the ranking of all the variables:</p>
<pre>head(predictors(recursive))<br/> ##   [1] "UBPRD488" "UBPRE626" "UBPRE217" "UBPRE170" "UBPRE392"           "UBPRE636<br/>head(recursive$resample, 10)<br/> ##    Variables  Accuracy     Kappa .cell1 .cell2 .cell3 .cell4 Resample<br/> ## 4        259 0.9915374 0.8988203    675      1      5     28   Fold01<br/> ## 8        259 0.9915374 0.8988203    675      1      5     28   Fold02<br/> ## 12       259 0.9830748 0.7976887    672      3      9     25   Fold03<br/> ## 16       259 0.9887165 0.8690976    673      3      5     28   Fold04<br/> ## 20       259 0.9929577 0.9169746    676      0      5     29   Fold05<br/> ## 24       259 0.9901269 0.8801237    675      1      6     27   Fold06<br/> ## 28       259 0.9873061 0.8590115    671      5      4     29   Fold07<br/> ## 32       259 0.9859155 0.8314180    674      2      8     26   Fold08<br/> ## 36       259 0.9929478 0.9169536    675      1      4     29   Fold09<br/> ## 40       259 0.9816384 0.7903799    669      6      7     26   Fold10</pre>
<p>As you can see, we successfully got the rankings of the variables. Nevertheless, the user has to select specifically how many final variables will be included in the final model.</p>
<p>In this case, only the resulting variables after the execution of the <kbd>Boruta</kbd> package are considered:</p>
<pre> predictors&lt;-data.frame("decision"=wrapper$finalDecision)<br/> <br/> predictors&lt;-cbind("variable"=row.names(predictors),predictors)<br/> <br/> predictors&lt;-                    as.vector(predictors[predictors$decision=="Confirmed","variable"])<br/> <br/> train&lt;-train[,c('ID_RSSD','Default',predictors)]<br/> <br/> test&lt;-test[,c('ID_RSSD','Default',predictors)]</pre>
<p>Our sample has been reduced:</p>
<pre>ncol(train)<br/> ## [1] 110<br/>save.image("Data11.RData")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Embedded methods</h1>
                </header>
            
            <article>
                
<p>The main difference between filter and wrapper approaches is that in filter approaches, such as <strong>embedded methods</strong>, you cannot separate the learning and feature selection parts. </p>
<p>Regularization methods are the most common type of embedded feature selection methods.</p>
<p>In classification problems such as this one, the logistic regression method cannot handle the multi-collinearity problem, which <span>occurs when variables are very correlated. When</span> the number of observations is not much larger than the number of variables of covariates, <em>p</em>, then there can be a lot of variability. Consequently, <span><span>this variability</span></span> could even increase the likelihood by simply adding more parameters, resulting in overfitting.</p>
<p>If variables are highly correlated or if collinearity exists, we expect the model parameters and variance to be inflated. The high variance is because of the wrongly specified model that has redundant predictors.</p>
<p>In order to solve these limitations, some approaches have emerged: Ridge regression, Lasso, and elastic net are the most common approaches.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ridge regression</h1>
                </header>
            
            <article>
                
<p>In a <strong>Ridge regression</strong>, the size of the regression coefficients is penalized based on the L2 norm:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/89739147-b061-437a-892f-ee9cfe957ab2.png" style="width:17.83em;height:4.00em;"/></div>
<p>Here, <em>L(B|y,x)</em> represents the likelihood of the logistic regression, and λ is the tuning parameter that serves to control the relative impact of these two terms on the regression coefficient estimates.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A limitation of Ridge regression</h1>
                </header>
            
            <article>
                
<p>Ridge regression includes all the predictors in the final model. However, it usually displays problems in the model interpretation when the number of variable <em>p</em> is large.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Lasso </h1>
                </header>
            
            <article>
                
<p><strong>Lasso</strong> represents another alternative of regularization and it overcomes the disadvantage of Ridge regression, reducing the number of predictors in the final model. This time, it penalizes the size of the regression coefficients using an L1 penalty:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/b37470cd-3557-47fc-9fca-c1d7116f0779.png" style="width:18.17em;height:3.92em;"/></div>
<p>When the λ is sufficiently large, it forces some of the coefficient estimates to be exactly equal to zero, obtaining more parsimonious models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Limitations of Lasso</h1>
                </header>
            
            <article>
                
<p>Sometimes, Lasso also displays an important weakness: If the number of covariates, <em>p</em>, is much larger than the number of observations, the number of selected variables are bounded by the number of observations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elastic net</h1>
                </header>
            
            <article>
                
<p><strong>Elastic net</strong> tries to overcome Ridge and Lasso models and performs well when variables are highly correlated. </p>
<p><span>Elastic net trains a model using all the variables, but it also tries to combine the strengths of two previously used approaches (Ridge and Lasso regressions). Consequently, elastic net penalizes the size of the regression coefficients based on both the L1 norm and the L2 norm as follows:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/4769f16a-a0f7-40c4-a110-66301c782c53.png" style="width:27.08em;height:4.25em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Drawbacks of elastic net</h1>
                </header>
            
            <article>
                
<p>Elastic net entails the selection of values λ<sub>1</sub> and λ<sub>2</sub> as being critical to a good performance of the model. These parameters are commonly obtained by cross-validation techniques. From these methods, Lasso and elastic net are usually used for feature selection. At the moment, we have 96 variables in our dataset; we decided not to reduce the number of variables.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimensionality reduction</h1>
                </header>
            
            <article>
                
<p><strong>Dimensionality projection</strong>, or feature projection, consists of converting data in a high-dimensional space to a space of fewer dimensions.</p>
<p>High dimensionality increases the computational complexity <span>substantially,</span><span> and </span><span>could </span><span>even increase the risk of overfitting.</span></p>
<p><strong><span>Dimensionality reduction</span></strong> techniques are useful for featuring selection as well. In this case, variables are converted into other new variables through different combinations. These combinations extract and summarize the relevant information from a complex database with fewer variables.</p>
<p>Different algorithms exist, with the following being the most important:</p>
<ul>
<li><strong>Principal Component Analysis</strong> (<strong>PCA</strong>)</li>
<li>Sammon mapping</li>
<li><strong>Singular value decomposition</strong> (<strong>SVD</strong>)</li>
<li>Isomap</li>
<li><strong>Local linear embedding</strong> (<strong>LLE</strong>)</li>
<li>Laplacian eigenmaps</li>
<li><strong>t-distributed Stochastic Neighbor Embedding</strong> (<strong>t-SNE</strong>)</li>
</ul>
<p>Although dimensionality reduction is not very common in cases such as failure prediction models or credit risk, we will see an example of this in our data. </p>
<p>We will also see the application of PCA and t-SNE, which are the most used algorithms.</p>
<p>PCA is a method for extracting important variables on a dataset by making linear transformations of variables. Thus, we can define a principal component as a normalized linear combination of the original variables.</p>
<p>The first principal component is the linear combination of variables that captures the maximum variance in the dataset. The larger the variability captured in the first component, the larger the information captured by the component. The first component best summarizes the largest information of our data <span>i</span><span>n just one line</span><span>. The second and subsequent principal components are also linear combinations of original variables that capture the remaining variance in the data.</span></p>
<p>PCA is also used when variables are highly correlated. One of the main properties of this method is that correlations among different components are zero.</p>
<p>Let’s see the implementation in R. For that, we use the <kbd>prcomp</kbd> function included in the <kbd>rstat</kbd> package:</p>
<pre>pca &lt;- prcomp(train[,3:ncol(train)], retx=TRUE, center=TRUE, scale=TRUE)</pre>
<p>Before implementing the PCA approach, variables should be standardized. This means that we should see to it that the variables have a mean that should be equal to zero and that they should have a standard deviation equal to 1.</p>
<p>This can be done using the <kbd>scale</kbd> and <kbd>center</kbd> options as parameters in the same function:</p>
<pre>names(pca)<br/>## [1] "sdev"     "rotation" "center"   "scale"    "x"</pre>
<p>The <kbd>center</kbd> and <kbd>scale</kbd> vectors contain the mean and standard deviation of the variables we have used.</p>
<p>The rotation measure returns the principal components. We obtain the same number of principal components as the variables that we have in the sample.</p>
<p>Let’s print how these components look. For example, the first rows of the four first components are shown here:</p>
<pre>pca$rotation[1:10,1:4]<br/> ##                  PC1          PC2         PC3          PC4<br/> ## UBPRE395 -0.05140105  0.027212743  0.01091903 -0.029884263<br/> ## UBPRE543  0.13068409 -0.002667109  0.03250766 -0.010948699<br/> ## UBPRE586  0.13347952 -0.013729338  0.02583513 -0.030875234<br/> ## UBPRFB60  0.17390861 -0.042970061  0.02813868  0.016505787<br/> ## UBPRE389  0.07980840  0.069097429  0.08331793  0.064870471<br/> ## UBPRE393  0.08976446  0.115336263  0.02076018 -0.012963786<br/> ## UBPRE394  0.16230020  0.119853462  0.07177180  0.009503902<br/> ## UBPRE396  0.06572403  0.033857693  0.07952204 -0.005602078<br/> ## UBPRE417 -0.06109615 -0.060368186 -0.01204455 -0.155802734<br/> ## UBPRE419  0.08178735  0.074713474  0.11134947  0.069892907</pre>
<p>Each of these components explains a proportion of the total variance. The proportion of variance explained by each component is easy to compute as follows:</p>
<ol>
<li>Let's first calculate the variance of each component:</li>
</ol>
<pre style="padding-left: 60px">pca_variances =pca$sdev^2</pre>
<ol start="2">
<li>Then each variance is divided by the sum of the component variances:</li>
</ol>
<pre style="padding-left: 60px">prop_var_explained &lt;- pca_variances/sum(pca_variances)<br/> <br/>head(prop_var_explained,10)<br/><br/> ##  [1] 0.10254590 0.06510543 0.04688792 0.04055387 0.03637036          0.03576523<br/> ##  [7] 0.02628578 0.02409343 0.02305206 0.02091978</pre>
<p>The first principal component explains about 10% of the variance. The second component explains 6% of the variance, and so on.</p>
<p>We can observe both the total variances and their contribution graphically using this code:</p>
<pre>plot(pca, type = "l",main = " Variance of Principal components")</pre>
<p>The preceding code generates the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-698 image-border" src="assets/9e00eae0-a9c4-46cc-b17e-8b11b1f820fc.png" style=""/></div>
<p>Let's run the code to plot variance:</p>
<pre>plot(prop_var_explained, xlab = "Principal Component",<br/>              ylab = "Proportion of Variance Explained",<br/>              type = "b")</pre>
<p>The preceding code generates the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-699 image-border" src="assets/d21336c7-d632-43f1-aa71-820cb4f64b19.png" style=""/></div>
<p class="mce-root"/>
<p>The previous screenshots are helpful to determine what the number of variables or principal components are that explain an important part of the total variance.</p>
<p>Consequently, these components could be used for modeling instead of using the full list of variables. It is interesting to plot the cumulative explained variance:</p>
<pre>plot(cumsum(prop_var_explained), xlab = "Principal Component",<br/> ylab = "Cumulative Proportion of Variance Explained",<br/> type = "b")</pre>
<p><span>The preceding code generates the following plot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-700 image-border" src="assets/41c891c7-457f-4e7c-b943-1d69f6347675.png" style=""/></div>
<p>According to the preceding screenshot, the first 20 components explain about 60% of the total variance in our dataset.</p>
<p>We could opt to use these 20 components to create our model. This approach is not commonly <span>used</span><span> </span><span>in credit risk models, so we will not use these transformations.</span></p>
<p>Nevertheless, it is important to assess what our dataset looks like. In the following screenshot, we show a graphical representation of our data using the first two components.</p>
<p>Moreover, we classify each bank in the graph in accordance with its corresponding target variable. This time, we use the <kbd>ggfortify</kbd> package:</p>
<pre>library(ggfortify)<br/> <br/>train$Default&lt;-as.factor(train$Default)<br/>autoplot(pca, data = train, colour = 'Default'</pre>
<p>This screenshot shows the classified graph of the failed and non-failed banks:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-701 image-border" src="assets/5696ea9d-b8e8-42bf-82fd-b12d5d35a8d6.png" style=""/></div>
<p>It is quite interesting to see only two components. Although these components only explain about 17% of total variance, failed and non-failed banks are to some extent differentiated between.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimensionality reduction technique</h1>
                </header>
            
            <article>
                
<p>You should consider that a principal component assumes the linear transformations of variables, but there are other non-linear dimensionality reduction techniques.</p>
<p>For me, one of the most interesting techniques is the t-SNE developed by Laurens van der Maaten, who says this:</p>
<div class="packt_quote">"As a sanity check, try running PCA on your data to reduce it to two dimensions. If this also gives bad results, then maybe there is not very much nice structure in your data in the first place. If PCA works well but t-SNE doesn’t, I am fairly sure you did something wrong."</div>
<p>Let’s see an example of how t-SNE is applied on our dataset. As usual, it is recommended that you fix a <kbd>seed</kbd>:</p>
<pre>set.seed(1234)</pre>
<p class="mce-root"/>
<p>We will need to use the <kbd>Rtsne</kbd> package. This package contains the <kbd>Rtsne</kbd> function that performs the algorithm. The most important parameters are as follows:</p>
<ul>
<li><kbd>pca</kbd>: This establishes whether a principal component analysis is carried out before running t-SNE .</li>
<li><kbd>perplexity</kbd>: This is a measure for information (defined as 2 to the power of the Shannon entropy). The <kbd>p<span>erplexity</span></kbd> <span>parameter establishes the number of the nearest neighbors in each observation. This parameter is useful to the algorithm, as it enables it to find a balance between local and global relations in the observations in your data.</span></li>
</ul>
<p>The code to run the algorithm is as follows:</p>
<pre>library(Rtsne)<br/> <br/>tsne= Rtsne(as.matrix(train[,3:ncol(train)]), check_duplicates=TRUE, pca=TRUE, perplexity=75, theta=0.5, dims=2,max_iter = 2000,verbose=TRUE)</pre>
<p>It takes a few minutes for this to finish running. Additional information on how the algorithm works is also included in the package documentation and its references.</p>
<p>In general, the complete dataset is reduced to only two vectors:</p>
<pre>tsne_vectors = as.data.frame(tsne$Y)<br/> <br/> head(tsne_vectors)<br/> ##           V1          V2<br/> ## 1  -4.300888 -14.9082526<br/> ## 2   4.618766  44.8443129<br/> ## 3  21.554283   3.2569812<br/> ## 4  45.518532   0.7150365<br/> ## 5  12.098218   4.9833460<br/> ## 6 -14.510530  31.7903585</pre>
<p>Let’s plot our train dataset according to its vectors:</p>
<pre>ggplot(tsne_vectors, aes(x=V1, y=V2)) +<br/>   geom_point(size=0.25) +<br/>   guides(colour=guide_legend(override.aes=list(size=6))) +<br/>   xlab("") + ylab("") +<br/>   ggtitle("t-SNE") +<br/>   theme_light(base_size=20) +<br/>   theme(axis.text.x=element_blank(),<br/>         axis.text.y=element_blank()) +<br/>   scale_colour_brewer(palette = "Set2")</pre>
<p>The preceding code generates the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-702 image-border" src="assets/ef4fb2f9-bb1f-4913-b9c5-af0e5a1b1a01.png" style=""/></div>
<p>Now let's plot it again, giving a color to each target value and failed and non-failed banks:</p>
<pre>plot(tsne$Y, t='n', main="tsne",xlab="Vector X",ylab="Vector y")<br/> text(tsne$Y, labels=as.vector(train$Default), col=c('red', 'blue')[as.numeric(train$Default)])</pre>
<p>The preceding code generates the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-703 image-border" src="assets/def33c8a-bb4c-40a3-a0fa-adfe8b72603b.png" style=""/></div>
<p>We can see that many failed banks are placed in the same part of the resulting bi-dimensional map. Nevertheless, one of the main weaknesses of t-SNE is the black-box type nature of the algorithm. It is not possible to make inferences on additional data based on the results, which does not happen using PCA.</p>
<p>t-SNE is mainly used for exploratory data analysis and it is also used as an input for clustering algorithms.</p>
<p>In this real case, and trying to be accurate with analysis and processes used in credit risk, we will ignore the results of PCA and t-SNE, and we will continue with our original dimensionality.</p>
<p>Once we have selected the most predictive variables, we will try to combine them using different algorithms. The aim is to develop a model with the highest accuracy to predict the future insolvency of banks.</p>
<p>Before continuing, let’s save the workspace:</p>
<pre>rm(list=setdiff(ls(), c("Model_database","train","test","table_iv")))<br/> <br/>save.image("~/Data12.RData")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we saw how univariate analysis reduced the sample space of our problem data and analyzed the data. Consequently, in the next chapter, we will see how these variables can be combined to obtain an accurate model, where several algorithms will be tested.</p>


            </article>

            
        </section>
    </body></html>