<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer140">
<h1 class="chapter-number" id="_idParaDest-110"><a id="_idTextAnchor157"/>8</h1>
<h1 id="_idParaDest-111"><a id="_idTextAnchor158"/>Neural Networks and Deep Neural Networks</h1>
<p>In <a href="B18714_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Support Vector Machines</em>, we saw that <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>) can <a id="_idIndexMarker639"/>be used to classify tweets for emotions based on the words that they contain. Is there anything else that we can use that simply looks at the words that are present? In this chapter, we will consider the use of neural networks for this purpose. Neural networks<a id="_idIndexMarker640"/> are a way of carrying out computations by assigning weights to a network of nodes and propagating an initial set of values through the network until the output nodes are reached. The values of the output nodes will then be a representation of the result of the computation. When neural networks were introduced in the 1940s, they were intended as a model of the way that the human brain carries out computation<a id="_idTextAnchor159"/><a id="_idTextAnchor160"/><a id="_idTextAnchor161"/>s (Hebb, 1949<a id="_idTextAnchor162"/>) (McCulloch &amp; Pitts, 1943). This kind of network is no longer taken seriously as a model of the human brain, but the results that can sometimes be achieved this way can be very impressive,  particularly when the relationship between the inputs and outputs is hard to determine. A typical neural network<a id="_idIndexMarker641"/> has an <strong class="bold">input layer</strong> of nodes, a set<a id="_idIndexMarker642"/> of <strong class="bold">hidden layers</strong>, and an <strong class="bold">output layer</strong>, with<a id="_idIndexMarker643"/> connections usually linking nodes in one layer with nodes in either the same layer or <span class="No-Break">the next.</span></p>
<p>We will start by looking at the use of simple neural networks with no hidden layers, and we will investigate the effect of varying several relevant parameters. As with the algorithms from <em class="italic">Chapters 6</em> and <em class="italic">7</em>, the standard application of neural networks aims to produce a single value for each input set of features; however, as with the Naïve Bayes algorithm from <a href="B18714_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Naive Bayes </em>it does this by assigning a score to each potential output label, and hence we can easily adapt it to the case where a tweet can have any number of labels. By the end of this chapter, you’ll have a clear understanding of how neural networks carry out computations and how adding hidden layers to a network allows it to compute functions that cannot be computed with a single hidden layer. You will also understand how they can be used to assign labels to tweets in <span class="No-Break">our datasets.</span></p>
<p>In this chapter, we’ll cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Single-layer neural networks and their use <span class="No-Break">as classifiers</span></li>
<li>Multi-layer neural networks and their use <span class="No-Break">as classifiers</span></li>
</ul>
<h1 id="_idParaDest-112"><a id="_idTextAnchor163"/>Single-layer neural networks</h1>
<p>A <a id="_idIndexMarker644"/>neural network, in general, consists of a set of nodes, organized in<a id="_idIndexMarker645"/> layers, with connections between them. A <strong class="bold">simple neural network</strong> (<strong class="bold">SNN</strong>) simply has an<a id="_idIndexMarker646"/> input layer that <a id="_idIndexMarker647"/>corresponds to the features that the classification is to be based on, and an output layer that corresponds to the possible outcomes. In the simplest case, where we just want to know whether something belongs to a specified category, there will be just one output node, but in our case, where we have multiple possible outcomes, we will have multiple output nodes. An SNN looks something like <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<img alt="Figure 8.1 – A single-layer neural network where the inputs are words and the outputs are emotions" height="847" src="image/B18714_08_01.jpg" width="1580"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – A single-layer neural network where the inputs are words and the outputs are emotions</p>
<p>The links between nodes each have a weight and every node that’s not in the input layer has a bias. The weights and the bias are essentially the same as the weights and the constant term in the <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/18.png" style="vertical-align:-0.012em;height:0.676em;width:15.431em" width="643"/> equation, which we used to define the separating hyperplane in <a href="B18714_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Support </em><span class="No-Break"><em class="italic">Vector Machines.</em></span></p>
<p>Applying <a id="_idIndexMarker648"/>such a network to a tweet that needs to be classified is very simple: just multiply the weights associated with each word in the tweet (each <strong class="bold">active input node</strong>), take<a id="_idIndexMarker649"/> the sum of those, and add it to the bias for the connected node: if this is positive, then set it as the activation for the connected node; otherwise, set the activation to 0. Training a network of this kind is more challenging. The basic idea is that you look at the output nodes. If an output node is doing what the training data says it should, then there is nothing to be done (after all, if all the output nodes did what they should on all the training data, then the classifier would be trained as well as possible). If it is not, then there must be something wrong with the connections leading into it. There are two possibilities: the node is on when it should be off, or it is off when it should be on. Suppose it is on when it should be off. The only reason <a id="_idIndexMarker650"/>for it to be on is if the sum of the weights on the links from active nodes that lead into it is greater than its threshold, so to stop it from turning on, the weights on those links should all be <span class="No-Break">decreased slightly.</span></p>
<p>Similarly, if a node is off when it should be on, then the weights on active nodes that lead into it should be increased slightly. Note that in both cases, it is the links from active nodes that are adjusted – inactive nodes cannot contribute to turning a node that they are connected to on, so changing the weights on the links from them has no effect. Exactly how much the weights should be increased or decreased, and when this should happen, has substantial effects on the accuracy of the results and the time taken to get them. If you change the weights too much, the process may fail to converge or it may converge on a sub-optimal configuration; if you change them too little, then convergence can take a very long time. This process of gradually changing the weights to drive the network to reproduce the training data is known<a id="_idIndexMarker651"/> as <strong class="bold">gradient descent</strong>, reflecting the fact that the aim is to move the network downhill in the space of weights and thresholds to obtain the minimum <span class="No-Break">overall error.</span></p>
<p>In the original presentations of neural networks, this process <a id="_idIndexMarker652"/>was <strong class="bold">back-propagated</strong> through the network so that the weights on connections leading into the layer before the output layer were also adjusted per their overall contribution to the output, and the weights on the layer before that, and so on until the input layer<a id="_idTextAnchor164"/> was reached (Rumelhart et al., 1986). Doing this could be very slow with networks with many hidden layers, with very small changes – sometimes vanishingly small changes – happening in the early layers. The use of neural networks was therefore restricted to quite shallow networks until it was realized that you could train a network with <em class="italic">N</em> hidden layers by training one with N-1 layers and adding another layer and fine-tuning the resul<a id="_idTextAnchor165"/>ting network (Hinton et al., 2006). This meant that you could train a network with, say, three hidden layers by training one with no hidden layers and then adding a new layer just before the output layer and retraining this network (which has one hidden layer), then adding another new layer just before the output layers and retraining this network (which has two hidden layers), and then adding another new layer, retraining with this one (which has three hidden layers). This strategy makes it feasible to train complex networks based on the assumption that the generalizations captured by the early layers are robust so that <a id="_idIndexMarker653"/>errors in later ones will not have a major effect on the <span class="No-Break">early ones.</span></p>
<p>There are<a id="_idIndexMarker654"/> numerous strategies for implementing the training algorithm (there is only one way to do the actual application once a network has been trained), and numerous implementations of the training algorithm and the machinery for applying a trained network to a task. For very large datasets, using an implementation that can run in parallel can be a good idea, but the sparse nature of our data means that the <strong class="source-inline">sklearn.neural_network.MLPClassifier</strong> implementation runs in a reasonable amount of time. We will not try out every possible combination of features for every dataset. As with SVMs (but more so), there are countless settings and parameters to play with, and it is easy to get diverted into trying variations in the hope of getting a few percentage points of improvement. We will look at the effect of some of the more significant choices, but we will concentrate mainly on considering the way that features are used rather than on the minutiae of the training regime. We will start by considering an SNN – that is, a neural network with just an input layer and an output layer, as in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
<p>An SNN is just a <strong class="bold">deep neural network</strong> (<strong class="bold">DNN</strong>) with <a id="_idIndexMarker655"/>no hidden layers. We start by specifying the class of <strong class="source-inline">DNNCLASSIFIER</strong> as a subclass of <strong class="source-inline">SVMCLASSIFIER</strong> (they have several shared properties, which we can exploit by doing this). If we specify that the default is that <strong class="source-inline">DNNCLASSIFIER</strong> should have no hidden layers, then we have a constructor for SNNs. The initialization code follows the same pattern as for the other <strong class="source-inline">SKLEARNCLASSIFIER</strong> (including <strong class="source-inline">SVMCLASSIFIER</strong>) – use <strong class="source-inline">readTrainingData</strong> to read the training data and put it into the standard format and then invoke the <strong class="source-inline">sklearn</strong> implementation of<a id="_idIndexMarker656"/> the classification algorithm and fit it to the <span class="No-Break">training data:</span></p>
<pre class="source-code">
class DNNCLASSIFIER(svmclassifier.SVMCLASSIFIER):    def __init__(self, train=None,
                 args={"hiddenlayers":()}):
       args0 = ARGS({"N":500, "wthreshold":5,
                      "useDF": False,
                      "max_iter":sys.maxsize,
                      "solver":"sgd", "alpha": 1e-5})
       for k in args:
            args0[k] = args[k]
       self.readTrainingData(train, N=args0["N"])
       # making a multi-layer classifier requires a
       # lot of parameters to be set
       self.clsf = MLPClassifier(solver=args["solver"],
                   alpha=args["alpha"],
                   max_iter=args["max_iter"],
                   hidden_layer_sizes=args["hiddenlayers"],
                   random_state=1)
       self.clsf.fit(self.matrix, self.values)</pre>
<p>We are <a id="_idIndexMarker657"/>using the <strong class="source-inline">sklearn.neural_network.MLPClassifier</strong> package from <strong class="source-inline">sklearn</strong>. This package takes a large number of parameters that control the shape of the network and the ways that the weights are calculated and used. As ever, we are not going to carry out experiments to see how varying these parameters affects performance on our tasks. Our goal is to see how well the basic algorithm works for us, so we will largely use the default values for these parameters. Once we have ascertained how well the algorithm works in general, it may be worth tweaking the parameters, but since, as with all these algorithms, the performance depends to a large extent on the nature of the dataset, this is something that can be left <span class="No-Break">for later.</span></p>
<p>As with all the classifiers so far, the constructor trains the model: with the sklearn-based ones, this always involves using <strong class="source-inline">readTrainingData</strong> to convert the data into its standard form, making a model of the specified type, and calling <strong class="source-inline">self.clsf.fit(self.matrix, self.values)</strong> to train it. Applying the trained model involves applying the <strong class="source-inline">applyToTweets</strong> method, which is inherited from the abstract <strong class="source-inline">BASECLASSIFIER</strong> class from <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Sentiment Lexicons and </em><span class="No-Break"><em class="italic">Vector-Space Models</em></span><span class="No-Break">.</span></p>
<p>Trying it out <a id="_idIndexMarker658"/>on our datasets, we get the following. (The results for CARER were obtained by training on 70K of the full 440K available. Training neural networks is considerably slower than other algorithms. We will look at the relationships between training size, accuracy, and time later, but for now, just note that the accuracy on<a id="_idIndexMarker659"/> the CARER dataset seems to have started to level off after about 70K, so we can use that for comparison with the <span class="No-Break">other algorithms):</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-8">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Precision</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Recall</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Micro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Macro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Jaccard</strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.902</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.902</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.902</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.902</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.822</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.648</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.275</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.386</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.388</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.239</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">WASSA-EN</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.837</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.837</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.837</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.837</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.720</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">CARER-EN</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.901</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.901</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.901</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.901</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.820</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">IMDB-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.885</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.885</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.885</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.885</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.793</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.670</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.670</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.670</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.670</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.504</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.596</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.260</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.362</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.370</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.221</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">KWT.M-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.035</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.126</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.055</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.034</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.028</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.541</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.472</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.504</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.409</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.337</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.484</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.290</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.362</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.361</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.221</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Simple neural network applied to the standard datasets</p>
<p>The scores in the preceding table for the two big datasets are the best so far, but the others are all slightly worse than what we achieved using Naïve Bayes and SVMs. The obvious way to improve the performance of this algorithm is to use a DNN. DNNs have been shown to have better performance than SNNs on many tasks, and it is reasonable to expect that they will help here. There are, however, a huge number of options to choose from when you start using networks with hidden layers, and it is worth looking at what the non-hidden layer version is doing with the data it was supplied with before trying to add hidden layers. Do we want one hidden layer that is half the size of the input layer? Do we want 50 hidden layers, each of which is of size 15? Given that training a neural network with hidden layers can be very slow, it is a good idea to think about what we want the hidden layers to do before we start doing <span class="No-Break">any experiments.</span></p>
<p>We will start<a id="_idIndexMarker660"/> by looking at the effects of varying parameters, such as<a id="_idIndexMarker661"/> the size of the training data, the number of input features, and the number of iterations. Training even a no-hidden- layers neural network can be quite slow (see <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.3</em>), and it is worth looking at how changes to the training data affect the time for training and the accuracy: if we find that there is a way of reducing training time while maintaining a reasonable level of performance, then it may be worth using that rather than the full unrestricted <span class="No-Break">training set.</span></p>
<p>There are three obvious things we can <span class="No-Break">look at:</span></p>
<ul>
<li>How does the accuracy and training time vary with the size of the <span class="No-Break">training set?</span></li>
<li>How does the accuracy and training time vary with the number of input features (that <span class="No-Break">is, words)?</span></li>
<li>How does the accuracy and training time vary with the number <span class="No-Break">of iterations?</span></li>
</ul>
<p>We will start by looking at how accuracy (reported as Jaccard score) and training time vary with the size of the training set. The following graph plots these for the CARER dataset (which is the largest of our datasets) with the other factors held constant (only use the 10K most frequent words, do at most <span class="No-Break">1K iterations):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer100">
<img alt="Figure 8.3 – Jaccard score and training time in seconds versus training size with the CARER-EN dataset" height="559" src="image/B18714_08_03.jpg" width="1000"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Jaccard score and training time in seconds versus training size with the CARER-EN dataset</p>
<p>It is clear that the<a id="_idIndexMarker662"/> Jaccard score levels off after about 40K <a id="_idIndexMarker663"/>tweets, while the training time seems to be on an upward trend. It is not easy to fit a curve to the Jaccard plot – a polynomial one will inevitably begin to trend downwards, and a logarithmic one will inevitably increase to above 1 at some point – however, a simple inspection should give you a reasonable idea of the point at which adding extra data will stop producing useful increases <span class="No-Break">in performance.</span></p>
<p>The next thing to vary is the size of the dictionary. Since the input layer consists of the words that appear in the tweets, removing infrequent words may speed things up without having too much effect <span class="No-Break">on accuracy:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<img alt="Figure 8.4 – Jaccard score and training time in seconds versus dictionary size with the CARER-EN dataset" height="593" src="image/B18714_08_04.jpg" width="1000"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Jaccard score and training time in seconds versus dictionary size with the CARER-EN dataset</p>
<p>The CARER-EN dataset contains 16.7K words, but the Jaccard score flattens out at somewhere <a id="_idIndexMarker664"/>between 1K and 2K. Since training<a id="_idIndexMarker665"/> time does increase more or less linearly as the number of input features increases, it is worth checking for the point at which adding new words has little effect on <span class="No-Break">the accuracy.</span></p>
<p>The third thing that we can vary is the number of iterations. Neural network training involves making a series of adjustments to the weights and thresholds until no further improvement is achievable. The more iterations that we carry out, the longer training takes, but the accuracy tends to start to flatten out before the best possible result is found. The following chart shows how the training time and the Jaccard score vary as the number of iterations increases for SEM4-EN. There were no further improvements after 1,800 iterations for this dataset, so we stopped the plot at this point. Unsurprisingly, the training time goes up linearly with the number of iterations, whereas the Jaccard score starts to level off at around <span class="No-Break">1,400 iterations:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer102">
<img alt="Figure 8.5 – Jaccard score and training time versus iterations with the SEM4-EN dataset" height="570" src="image/B18714_08_05.jpg" width="1000"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Jaccard score and training time versus iterations with the SEM4-EN dataset</p>
<p>Varying <a id="_idIndexMarker666"/>the size of the training data, the number of input<a id="_idIndexMarker667"/> features, and the number of iterations affects the scores and the training time. While you are developing a model and are trying out different combinations of parameters, settings, and preprocessing steps, it is certainly worth doing some preliminary investigations to find a set of values for these factors at which the Jaccard score appears to be leveling off. But in the end, you just have to grit your teeth and train the model using a large amount of training data, a large dictionary, and a large number <span class="No-Break">of iterations.</span></p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor166"/>Multi-layer neural networks</h1>
<p>We have seen that if we are prepared to wait, using an SNN can produce, at least in some cases, better results than any of the previous algorithms. For a lot of problems, adding extra hidden layers can produce better results than networks with just an input layer and an output layer. Will this help with our <span class="No-Break">current task?</span></p>
<p>SNNs<a id="_idIndexMarker668"/> compute very similar information to that calculated by Naïve Bayes and SVMs. The links between input and output nodes carry information about how strongly the input nodes (that is, words) are correlated to the output nodes (that is, emotions) and how the biases roughly carry information about how likely the given output is. The following tables show the links between several common words and emotions after <a id="_idIndexMarker669"/>training on the <span class="No-Break">CARER </span><span class="No-Break"><a id="_idIndexMarker670"/></span><span class="No-Break">dataset:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-5">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">anger</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">fear</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">joy</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">love</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">sadness</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">surprise</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">the</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.036</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.065</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.031</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.046</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.015</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.036</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">sorrow</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.002</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.028</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.098</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.098</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.063</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.020</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">scared</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.356</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">1.792</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.683</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.283</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.562</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.057</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">happy</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.090</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.161</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.936</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.332</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.191</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.156</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">disgusting</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.048</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.014</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.031</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.045</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.020</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.000</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">and</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.001</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.033</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.014</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.015</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.031</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.022</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">adoring</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.054</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.034</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.110</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.218</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.085</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.007</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">irritated</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">1.727</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.249</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.558</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.183</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.621</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.124</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">kisses</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.004</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.041</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.041</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.120</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.038</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.001</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Links between words and emotions in the CARER-EN dataset</p>
<p>The following table displays the words with the strongest and weakest connections to <span class="No-Break">the emotions:</span></p>
<table class="No-Table-Style" id="table003-4">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">anger</span></p>
</td>
<td class="No-Table-Style">
<p>offended, greedy, rushed, resentful, selfish ... passionate, supporting, strange, <span class="No-Break">amazing, weird</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">fear</span></p>
</td>
<td class="No-Table-Style">
<p>unsure, reluctant, shaky, insecure, vulnerable ... shocked, supporting, <span class="No-Break">sweet, stressed</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">joy</span></p>
</td>
<td class="No-Table-Style">
<p>smug, sincere, invigorated, joyful, positive ... helpless, agitated, weird, <span class="No-Break">strange, overwhelmed</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">love</span></p>
</td>
<td class="No-Table-Style">
<p>horny, sympathetic, gentle, naughty, liked ... amazing, overwhelmed, hated, <span class="No-Break">strange, weird</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">sadness</span></p>
</td>
<td class="No-Table-Style">
<p>burdened, homesick, disturbed, rotten, guilty ... sweet, agitated, <span class="No-Break">weird, strange</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">surprise</span></p>
</td>
<td class="No-Table-Style">
<p>impressed, shocked, amazed, surprised, curious ... feelings, don, very, <span class="No-Break">being, or</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Strongest and weakest words for each emotion in the CARER-EN dataset</p>
<p>Given <a id="_idIndexMarker671"/>a set of input words (a tweet!), the neural network <a id="_idIndexMarker672"/>calculates the sum of the links from those words to each emotion and compares it with the threshold (different implementations of neural networks carry <a id="_idIndexMarker673"/>out slightly different calculations: the one used here, <strong class="bold">rectified linear activati<a id="_idTextAnchor167"/>on</strong> (Fukushima, 1969), calculates the weighted sum of the inputs and the bias but then sets it to zero if it is negative). This is very similar to what all the other algorithms do – SVMs also calculate a weighted sum of the inputs but do not reset negative outcomes to zero; the lexicon-based algorithms also just calculate a weighted sum but since none of the weights are negative, the total cannot be less than zero, so there is no need to reset them. Naïve Bayes combines the conditional probabilities of the various observed events to produce an overall probability. What they all have in common is that a single word <em class="italic">always</em> makes the same contribution to the total. This may not always <span class="No-Break">be true:</span></p>
<ul>
<li>There are words whose sole job is to change the meaning of other words. Consider the word <em class="italic">happy</em>. This word is linked to <strong class="bold">joy</strong> rather than to any of the <span class="No-Break">other emotions:</span></li>
</ul>
<table class="No-Table-Style _idGenTablePara-1" id="table004-4">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">anger</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">fear</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">joy</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">sadness</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">happy</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.077</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.159</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.320</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.048</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Links between happy and the four emotions from SEM4-EN</p>
<p>Tweets <a id="_idIndexMarker674"/>where <em class="italic">happy</em> appears next to <em class="italic">not</em>, however, do<a id="_idIndexMarker675"/> not <span class="No-Break">express joy:</span></p>
<p><em class="italic">This is kind of screwed up , but my brother is about to join the police academy . . . . and I ‘ m not happy about . And I ‘m not the </em><span class="No-Break"><em class="italic">only one.</em></span></p>
<p><em class="italic">Yay bmth canceled Melbourne show fanpoxytastic just lost a days pay and hotel fees not happy atm # sad # </em><span class="No-Break"><em class="italic">angry</em></span></p>
<p><em class="italic"> I was just put on hold for 20 minutes till I hung up . # not happy # terribleservice # unhappy @ virginmedia I should have stayed . . .</em></p>
<p>The presence of <em class="italic">not</em> changes the meaning of these tweets so that they express something other <span class="No-Break">than joy.</span></p>
<p>Not all words that affect the meanings of other words are as easy to identify as <em class="italic">not</em>, particularly in informal texts where abbreviations such as <em class="italic">don’t</em> and <em class="italic">can’t</em> are very common, but there are certainly others that do something like this. It is also important to note that the word whose meaning is being affected by the modifying term may not be adjacent <span class="No-Break">to it.</span></p>
<ul>
<li>Some words form compounds that express meanings that are not straightforwardly related to the meanings of the individual words in isolation. We saw this with Chinese compounds earlier, but even English words can do this. Using pointwise mutual information to find compounds (as in <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, Sentiment Lexicons and Vector Space Models</em>), we find that <em class="italic">supporting cast</em> and <em class="italic">sweet potatoes</em> occur more often in the CARER-EN dataset than you would expect given the distributions of the individual words – that is, these terms may be viewed as compounds. The weights for the individual words are given in the following table, with <em class="italic">supporting</em> and <em class="italic">sweet</em> both having strong links to <strong class="bold">love</strong> and slightly weaker links to <strong class="bold">joy</strong>. Neither of the compound words would be expected to have these links – there is nothing particularly lovely or joyous about sweet potatoes! It is not possible to capture the fact that these words make a different contribution to the overall emotional charge of the texts containing them when they co-occur with <em class="italic">cast</em> and <em class="italic">potatoes</em> using an SNN or <a id="_idIndexMarker676"/>indeed<a id="_idIndexMarker677"/> any of the <span class="No-Break">earlier algorithms:</span></li>
</ul>
<table class="No-Table-Style _idGenTablePara-1" id="table005-4">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold"> </strong><span class="No-Break"><strong class="bold">anger</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"> </strong><span class="No-Break"><strong class="bold">fear</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"> </strong><span class="No-Break"><strong class="bold">joy</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"> </strong><span class="No-Break"><strong class="bold">love</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">sadness</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">surprise</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">supporting</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.183</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.154</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.220</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.515</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.319</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.043</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">cast</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.015</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.017</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.012</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.003</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.006</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.009</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">sweet</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.177</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.187</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.207</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.553</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.371</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.079</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">potatoes</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.009</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.003</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.004</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.003</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.020</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.019</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Weights for individual words that can occur as compounds</p>
<ul>
<li>Some words are simply ambiguous, with one interpretation carrying one emotional charge and another carrying a different one (or none). It is extremely difficult to detect that a word is ambiguous simply by looking at texts that contain it, and even more difficult to detect how many interpretations it has, and even if you do know how many interpretations a word has, you still have to decide which one is intended in a given text. So, inferring what emotional charge each interpretation has and then deciding which interpretation is intended is more or less impossible. However, in some cases, we can see, as with the cases of compounds previously, that two words are cooccurring unexpectedly often, and in such cases, we can be reasonably sure that the same interpretations are intended in each case. <em class="italic">Feel like</em> and <em class="italic">looks like</em>, for instance, occur more often in the SEM4-EN data than they should: both of these could be ambiguous, with the different meanings carrying different emotional charges. But it seems very likely that in each occurrence of <em class="italic">feel like</em> the same interpretations of <em class="italic">feel</em> and <em class="italic">like</em> are intended – as it happens, the interpretation of <em class="italic">like</em> in these phrases is not the one that is closely linked <span class="No-Break">to </span><span class="No-Break"><strong class="bold">love</strong></span><span class="No-Break">.</span></li>
</ul>
<p>All the <a id="_idIndexMarker678"/>algorithms that we have seen so far, including SNNs, treat the<a id="_idIndexMarker679"/> contributions made by individual words atomistically – they all compute a score for each word for each emotion, and they then combine the scores using some fairly simple arithmetical calculation. Therefore, they <em class="italic">cannot</em> be sensitive to the issues <span class="No-Break">raised here.</span></p>
<p>Adding extra layers to our neural network will enable us to handle these phenomena. The simplest demonstration of how adding layers will allow a neural network to compute something that cannot be dealt with by an SNN involves the XOR function, where we have two inputs and we want to get a response if one, but not both, of the inputs <span class="No-Break">is on.</span></p>
<p>This cannot be done with an SNN. We will explore the reasons for this and the way that DNNs overcome this limitation by considering a set of made-up tweets consisting solely of the words <em class="italic">love</em>, <em class="italic">like</em>, <em class="italic">hate</em>, and <em class="italic">shock</em> and the emotions <em class="italic">anger</em>, <em class="italic">joy</em>, and <em class="italic">surprise</em>, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table006-4">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">ID</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">tweet</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">joy</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">anger</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">surprise</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">love</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">like</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">love like</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">hate</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">shock</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">love</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>7</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">like</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>8</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">love like</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>9</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">hate</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">10</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">shock</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Straightforward training data</p>
<p>If we train an SNN<a id="_idIndexMarker680"/> on this data, we will get the <span class="No-Break">following </span><span class="No-Break"><a id="_idIndexMarker681"/></span><span class="No-Break">network:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer103">
<img alt="Figure 8.10 – An SNN for surprise, anger, and joy, with straightforward training data" height="792" src="image/B18714_08_10.jpg" width="1580"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – An SNN for surprise, anger, and joy, with straightforward training data</p>
<p>The strongest link from <em class="italic">hate</em> is to <strong class="bold">anger</strong>, the strongest link from <em class="italic">shock</em> is to <strong class="bold">surprise</strong>, and the strongest links from <em class="italic">love</em> and <em class="italic">like</em> are to <strong class="bold">joy</strong>. So, if a tweet consists of one of these words, it will trigger the appropriate emotion. If a tweet contains both <em class="italic">love</em> and <em class="italic">like</em>, it will also trigger <strong class="bold">joy</strong>, but the training data says nothing about what should happen if a tweet consists of, for instance, <em class="italic">shock</em> and <em class="italic">like</em> or <em class="italic">shock</em> and <em class="italic">hate</em>. Looking at the network, we can see that <em class="italic">hate</em> votes quite strongly for <strong class="bold">anger</strong> and <em class="italic">shock</em> votes by about the same amount for <strong class="bold">surprise</strong>, but that <em class="italic">shock</em> votes much more strongly <em class="italic">against</em> <strong class="bold">anger</strong> than <em class="italic">hate</em> does against <strong class="bold">surprise</strong>. So, overall, <em class="italic">shock</em> and <em class="italic">hate</em> vote for <strong class="bold">surprise</strong>. There is nothing meaningful going on here: the network is initialized with random values, and these spill over into random decisions about configurations of features that have not been seen in the <span class="No-Break">training data.</span></p>
<p>As <a id="_idIndexMarker682"/>noted previously, our SNN carries out essentially the same<a id="_idIndexMarker683"/> operations as an SVM: if the weights on the connections between a set of input nodes, <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" height="41" src="image/19.png" style="vertical-align:-0.340em;height:0.988em;width:3.527em" width="147"/>, and an output node, <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:math&gt;" height="28" src="image/20.png" style="vertical-align:-0.015em;height:0.679em;width:0.707em" width="29"/>, are <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" height="46" src="image/21.png" style="vertical-align:-0.383em;height:1.097em;width:5.341em" width="223"/><span class="_-----MathTools-_Math_Variable"><img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" height="46" src="image/22.png" style="vertical-align:-0.390em;height:1.104em;width:2.694em" width="112"/></span> and the bias for the output node is <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" height="32" src="image/23.png" style="vertical-align:-0.065em;height:0.779em;width:1.881em" width="78"/>, then if the input values for <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" height="41" src="image/24.png" style="vertical-align:-0.340em;height:0.988em;width:3.193em" width="133"/> are <span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base">)</span>, then the excitation of the output node is determined by  <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" height="46" src="image/25.png" style="vertical-align:-0.390em;height:1.104em;width:16.778em" width="699"/><span class="_-----MathTools-_Math_Variable"><img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" height="32" src="image/26.png" style="vertical-align:-0.065em;height:0.779em;width:1.399em" width="58"/></span> – the excitation of the output node will be <img alt="&lt;mml:math  &gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="27" src="image/27.png" style="vertical-align:-0.012em;height:0.646em;width:0.479em" width="20"/> if this sum is negative and on to some degree if it is positive. <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" height="46" src="image/28.png" style="vertical-align:-0.390em;height:1.104em;width:18.600em" width="775"/> determines a hyperplane that divides points into two classes, <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:math&gt;" height="28" src="image/29.png" style="vertical-align:-0.015em;height:0.679em;width:0.707em" width="29"/> or <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" height="32" src="image/30.png" style="vertical-align:-0.065em;height:0.779em;width:2.689em" width="112"/>, just like the coefficients <span class="No-Break">in SVMs.</span></p>
<p>But that means that an SNN cannot classify if the classes are <em class="italic">not</em> linearly separable. The classic example here is the XOR function – that is, examples where each of two features in isolation denotes a specific class but the two together do not – that is, <em class="italic">XOR(0, 0)=0</em>, <em class="italic">XOR(0, 1)=1</em>, <em class="italic">XOR(1, 0)=1</em>, and <em class="italic">XOR(1, 1)=0</em>. It is easy enough to draw this function and show that it looks as though no line separates the 0 and 1 cases. In <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.11</em>, the red circles (at (0, 0) and (1, 1)) represent the cases where XOR is 0, and the blue diamonds (at (1, 0) and (0, 1)) represent where XOR <span class="No-Break">is 1:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<img alt="Figure 8.11 – XOR – the blue diamonds and red circles cannot be separated by a straight line" height="183" src="image/B18714_08_11.jpg" width="187"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – XOR – the blue diamonds and red circles cannot be separated by a straight line</p>
<p>It looks as though it would be impossible to draw a line that divides the blue diamonds and red circles – that is, these two classes look as though they are not <span class="No-Break">linearly separable.</span></p>
<p>For formal proof of this, assume that there is such a line. It will have an equation such as <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="38" src="image/31.png" style="vertical-align:-0.257em;height:0.921em;width:9.233em" width="385"/> where, for any point that is above the line, <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="38" src="image/32.png" style="vertical-align:-0.257em;height:0.921em;width:9.227em" width="384"/> , and for any point below the line, <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="38" src="image/33.png" style="vertical-align:-0.257em;height:0.921em;width:9.104em" width="379"/> (if B is positive and vice versa if B <span class="No-Break">is negative).</span></p>
<p>Given our<a id="_idIndexMarker684"/> four points, let’s assume that the red circles are both <a id="_idIndexMarker685"/>below the line and the blue diamonds are above it and B is positive. Then, for the red circle at (0, 0), we would have <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/34.png" style="vertical-align:-0.015em;height:0.679em;width:2.626em" width="109"/>, since putting 0 for each of <em class="italic">x</em> and <em class="italic">y</em> would give us <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/35.png" style="vertical-align:-0.015em;height:0.679em;width:9.207em" width="384"/>), which makes C&lt;0.  Similarly, for the red circle at (1, 1), substituting 1 for each of <em class="italic">x</em> and <em class="italic">y</em> would give us <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/36.png" style="vertical-align:-0.015em;height:0.679em;width:6.029em" width="251"/>, for the blue diamond at (1, 0) substituting 1 for <em class="italic">x</em> and 0 for <em class="italic">y</em> would give us  <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/37.png" style="vertical-align:-0.015em;height:0.679em;width:4.366em" width="182"/>, and for the blue diamond at (1, 0), we would get  <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/38.png" style="vertical-align:-0.015em;height:0.679em;width:4.285em" width="179"/>.</p>
<p>From <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/39.png" style="vertical-align:-0.015em;height:0.679em;width:2.620em" width="109"/> and <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/40.png" style="vertical-align:-0.015em;height:0.679em;width:4.358em" width="182"/>, we get <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/41.png" style="vertical-align:-0.012em;height:0.675em;width:2.648em" width="110"/>, and likewise from <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/42.png" style="vertical-align:-0.015em;height:0.679em;width:2.619em" width="109"/> and <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/43.png" style="vertical-align:-0.015em;height:0.679em;width:4.271em" width="178"/>, we get <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/44.png" style="vertical-align:-0.012em;height:0.660em;width:2.570em" width="107"/>. But then <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:math&gt;" height="28" src="image/45.png" style="vertical-align:-0.015em;height:0.679em;width:11.673em" width="487"/>, so since <img alt="&lt;mml:math  &gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="32" src="image/46.png" style="vertical-align:-0.065em;height:0.779em;width:9.704em" width="404"/>, then <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/47.png" style="vertical-align:-0.015em;height:0.679em;width:6.433em" width="268"/>, contradicting the observation that <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="28" src="image/48.png" style="vertical-align:-0.015em;height:0.679em;width:6.065em" width="253"/> for the point (1, 1). We do have to consider the possibility that the blue diamonds are both above the line and the red circles are below, or that B is negative, but an exactly parallel argument rules these out in the same way – there is no way to draw a straight line that separates the blue diamonds and <span class="No-Break">red circles.</span></p>
<p>What does this mean for our task? Suppose we adjust our made-up data <span class="No-Break">as follows:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table007-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">ID</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">tweet</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">joy</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">anger</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Surprise</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">love</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">like</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">love like</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">hate</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">shock</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">love</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>7</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">like</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>8</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">love like</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>9</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">hate</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Difficult training data</p>
<p>The only <a id="_idIndexMarker686"/>change we have made is that we have made the <a id="_idIndexMarker687"/>tweets that contain <em class="italic">love</em> and <em class="italic">like</em> express <strong class="bold">anger</strong> rather than <strong class="bold">joy</strong>. This is much like the situation with XOR previously, where two features express one emotion when they appear in isolation and a different one when they appear together. There is no exact parallel to the (0, 0) point from XOR, but in the cases where neither was present, then the target is either <strong class="bold">anger</strong> (if the tweet was just the word <em class="italic">hate</em>) or <strong class="bold">surprise</strong> (if the tweet was just the word <em class="italic">shock</em>) – that is, when neither <em class="italic">love</em> nor <em class="italic">like</em> is present, then the tweets do not <span class="No-Break">express </span><span class="No-Break"><strong class="bold">joy</strong></span><span class="No-Break">.</span></p>
<p>When we train on this data, we will find that the SNN cannot be relied on to find weights that assign the right emotions. Sometimes it does, sometimes not. The problem is not that there is no set of weights that will assign the correct labels. Over a run of 10 folds of 90% training/10% testing, weights that split the data correctly were found in two cases, but in eight cases, the classifier assigned the wrong emotion to tweets containing both <em class="italic">like</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">love</em></span><span class="No-Break">.</span></p>
<p>In the incorrectly trained network shown here, the scores for joy, anger, and surprise for tweets containing both these words were 0.35, -0.25, and -3.99, with joy the clear winner. The data <em class="italic">is</em> linearly separable since the correctly trained classifier does separate the data into the right classes by using hyperplanes defined by the connection weights and biases; however, the gradient descent process can easily get stuck in local minima, doing the best it can with the single-word tweets but unable to find the correct weights for the <span class="No-Break">compound ones:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<img alt="Figure 8.13 – Correctly trained network" height="792" src="image/B18714_08_13.jpg" width="1580"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Correctly trained network</p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<img alt="Figure 8.14 – Incorrectly trained SNN" height="792" src="image/B18714_08_14.jpg" width="1580"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Incorrectly trained SNN</p>
<p>Therefore, we <a id="_idIndexMarker688"/>have two<a id="_idIndexMarker689"/> kinds <span class="No-Break">of problems:</span></p>
<ul>
<li>If the data is not linearly separable, then no SNN can classify <span class="No-Break">it correctly</span></li>
<li>Even if it can be divided by a set of hyperplanes, SNNs can easily get stuck in local minima, doing the best they can with most of the data but unable to find the right weights for cases where words have a different effect when they occur together from the effect they would have <span class="No-Break">in isolation</span></li>
</ul>
<p>We can solve the first problem by adding extra layers. In order, for instance, to calculate XOR, you need a node in the network that is turned on when both the input nodes are on and has a negative link to the output. A simple feedforward network should have the nodes in the input layer connected to nodes in the first hidden layer and then nodes in the first hidden layer connected to nodes in the second hidden layer and so on until the nodes in the last hidden layer are connected to nodes in the output layer. You need at least one hidden layer with at least three nodes in it. However, as we have seen, networks can quite easily get stuck in local minima, and the smallest configuration that can be reliably trained to <a id="_idIndexMarker690"/>recognize XOR has a single hidden layer with <a id="_idIndexMarker691"/><span class="No-Break">five nodes:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer137">
<img alt="Figure 8.15 – DNN trained to classify tweets with either happy or like, but not both, as joy" height="988" src="image/B18714_08_15.jpg" width="1423"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – DNN trained to classify tweets with either happy or like, but not both, as joy</p>
<p>If <em class="italic">like</em> is on but <em class="italic">happy</em> is off, then the second hidden node will be on with a score of 1.22 (i.e. -0.766+1.988), which will propagate through to the output node as -2.96 (1.22*-2.46). This will then be added to the bias of the output node to produce -2.15. If <em class="italic">happy</em> is on but <em class="italic">like</em> is not, then the second and fourth hidden nodes will be on, with scores of 0.76 and 1.23, which will propagate through to the output node as -0.766*0.76 (for the second hidden node) + -1.309*1.23, which when added to the bias for the output node becomes -2.66. If both the input nodes are on, then none of the hidden nodes will be, so the score at the output node is just its own bias – that is, 0.81. For networks with just one output, the standard logistic function used for interpreting the final score treats negative numbers as being on and positive ones as off, so this network classifies tweets that contain just <em class="italic">like</em> or <em class="italic">happy</em> as expressing joy and ones that contain both as not <span class="No-Break">expressing it.</span></p>
<p>Adding hidden units<a id="_idIndexMarker692"/> will let the network recognize significant<a id="_idIndexMarker693"/> combinations of input features as being non-compositional, in that the effect of the combination is not just the cumulative effect of the features themselves. We can also see that if you do not have enough hidden features, then the training process can get stuck in local minima – although you <em class="italic">can</em> compute XOR using just three features in the hidden layer, it is very difficult to train such a network for this task <a id="_idTextAnchor168"/>(see (Minsky &amp; Papert, 1969) for further discussion of this issue). This is not just a matter of not having enough data, or of not allowing the network to train for long enough. Networks with hidden layers with three nodes converge very quickly (after about 10 or 12 epochs) and ones with four just take a few hundred. We can also add extra layers – networks with two hidden layers with four and three nodes each can also solve this problem and typically converge slightly more quickly than ones with one hidden layer with <span class="No-Break">five nodes:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<img alt="Figure 8.16 – Network with two hidden layers for solving XOR" height="910" src="image/B18714_08_16.jpg" width="1580"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Network with two hidden layers for solving XOR</p>
<p>The problem<a id="_idIndexMarker694"/> is that setting the initial weights and biases <a id="_idIndexMarker695"/>at random in small networks nearly always leaves you in an area of the search space where you will end up in a local minimum. Using larger networks, on the other hand, nearly always produces networks that can solve the problem since there will be nodes that are in the right part of the search space that can be given increasing significance, but they take much longer to train. So, the key task is to find the appropriate number of <span class="No-Break">hidden units.</span></p>
<p>The role of a hidden unit is to find words that feed differently into the output nodes when they occur in isolation and when they occur in combinations with other words. The two key parameters here seem likely to be the number of input features (that is, the number of distinct words in the data) and the number of output classes (that is, the number of emotions). If there are more words in the lexicon, then there are more possible combinations of words, which might mean that having more words requires more hidden nodes. If there are more output classes, then there are more places where having combinations of words might <span class="No-Break">be helpful.</span></p>
<p>Given that there are many thousands of words in the lexicons for our datasets, but only four to eleven emotions, it seems sensible to start by investigating the effect of relating the number of hidden nodes to the number of emotions. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.17</em> shows what happens when we have a hidden layer with 0.5 times as many nodes as there are emotions, 1 times as many, or 1.5 times <span class="No-Break">as many:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<img alt="Figure 8.17 – Jaccard score versus the number of hidden nodes = F*number of emotions, F from 0.5 to 5" height="536" src="image/B18714_08_17.jpg" width="1000"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.17 – Jaccard score versus the number of hidden nodes = F*number of emotions, F from 0.5 to 5</p>
<p>For the three <a id="_idIndexMarker696"/>datasets for which we obtained quite good<a id="_idIndexMarker697"/> results with an SNN, the effect of adding a hidden layer with a moderate number of nodes is substantial. The original scores have been repeated here for ease <span class="No-Break">of reference:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table008-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Precision</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Recall</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Micro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Macro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Jaccard</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.902</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.902</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.902</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.902</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.822</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.648</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.275</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.386</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.388</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.239</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">WASSA-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.837</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.837</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.837</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.837</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.720</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">CARER-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.901</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.901</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.901</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.901</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.820</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – Simple neural network applied to the standard English datasets</p>
<p>The original Jaccard score for CARER-EN was 0.77, which is equivalent to around 0.87 accuracy; when we add a hidden layer with half as many nodes as the number of emotions in CARER (that is, with just three nodes in the hidden layer since CARER has six emotions), we get a better score (Jaccard 0.79, accuracy 0.89) than in the original, and then as we increase the number of hidden nodes to 6, 9, 12, we get a very gradual improvement, up to the point where the score seems to have flattened out and maybe even started <span class="No-Break">to overtrain.</span></p>
<p>A similar, but more marked, pattern occurs with SEM4-EN and WASSA-EN. In these cases, the score starts fairly low when we only have half as many nodes in the hidden layer as we have emotions (that is, just two for both of these), but then leaps up considerably higher than the original scores as soon as we have as many nodes in the hidden layer as we have emotions and then flattens out at around Jaccard 0.875 (accuracy 0.93) for SEM4-EN and Jaccard 0.81 (accuracy 0.9) for WASSA-EN. In general, it looks as though adding a hidden layer with a modest number of nodes produces some improvement over the basic neural network with no hidden units, but experiments with more hidden layers or a single hidden layer with more nodes suggest that these improvements are fairly limited. This is likely to be because hidden layers look for non-compositional combinations of words. There are two possible reasons why this has <span class="No-Break">limited effects:</span></p>
<ul>
<li>There simply are not all that many cases of words whose emotional weight changes when they co-occur with <span class="No-Break">specific partners</span></li>
<li>Where there are such combinations, their frequency in the data is not enough to <a id="_idIndexMarker698"/>override their<a id="_idIndexMarker699"/> <span class="No-Break">normal interpretation</span></li>
</ul>
<p>It may be that using much more training data makes using networks with multiple or large hidden layers more effective, but with modest-sized datasets, doing so has comparatively <span class="No-Break">little effect.</span></p>
<h1 id="_idParaDest-114"><a id="_idTextAnchor169"/>Summary</h1>
<p>In this chapter, we looked at using neural networks for our task of identifying the emotions expressed in informal communications such as tweets. We examined the way that the lexicon for the datasets is used as the nodes in the input layer and looked at how the weights associated with individual words reflect the emotional significance of those words. We considered simple neural networks with no hidden layers and also slightly deeper ones with a single hidden layer with slightly more nodes than the set of output nodes – the performance of the neural network flattened out once the hidden layer contained 1.5 to 2 times as many nodes as the output layer, so there seemed <span class="No-Break">little point.</span></p>
<p>The highest-scoring algorithms for the various datasets are now <span class="No-Break">as follows:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table009-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold">LEX (</strong><span class="No-Break"><strong class="bold">unstemmed)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">LEX (</strong><span class="No-Break"><strong class="bold">stemmed)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">CP (</strong><span class="No-Break"><strong class="bold">stemmed)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">NB  (</strong><span class="No-Break"><strong class="bold">multi)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">SVM    (</strong><span class="No-Break"><strong class="bold">single)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">MULTI-SVM</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">SNN    (</strong><span class="No-Break"><strong class="bold">single)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">DNN</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.503</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.497</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.593</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.778</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.845</span></p>
</td>
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break">0.829</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.847*</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.347</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.348</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.353</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.267</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.224</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.385*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.242</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.246</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">WASSA-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.445</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.437</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.505</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.707</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.770*</strong></span></p>
</td>
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break">0.737</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.752</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">CARER-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.350</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.350</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.395</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.774</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.770</span></p>
</td>
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.820*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.804</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">IMDB-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.722</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.667</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.722</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.740</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.736</span></p>
</td>
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.793*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.793*</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.506</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.509</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.513</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.532*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.514</span></p>
</td>
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break">0.504</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.444</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.378</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.386*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.382</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.274</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.216</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.340</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.221</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.207</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">KWT.M-AR</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.687*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.663</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.666</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.507</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.631</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.341</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.028</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.026</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-ES</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.425*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.420</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.177</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.331</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.412</span></p>
</td>
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break">0.337</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.343</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.269</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.271</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.278*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.255</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.226</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.268</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.221</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.222</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.19 –  Scores for the best algorithms so far</p>
<p>Neural networks produced the best results in 4 of the 10 datasets, but the simple lexical algorithms are still the best for the multi-label datasets. The general lesson remains the same as it was at the end of <a href="B18714_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Support Vector Machines </em>– you shouldn’t just accept that there is a single best classification algorithm: do experiments, try out variations, and see for yourself what works best with <span class="No-Break">your data.</span></p>
<h1 id="_idParaDest-115"><a id="_idTextAnchor170"/>References</h1>
<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
<ul>
<li><a id="_idTextAnchor171"/>Fukushima, K. (1969). <em class="italic">Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements</em>. IEEE Transactions on Systems Science and Cybernetics, 5(4), <span class="No-Break">322–333. </span><a href="https://doi.org/10.1109/TSSC.1969.300225"><span class="No-Break">https://doi.org/10.1109/TSSC.1969.300225</span></a><span class="No-Break">.</span></li>
<li>Hebb, D. O. (1949). <em class="italic">The organization of behavior: A neuropsychological </em><span class="No-Break"><em class="italic">theory</em></span><span class="No-Break">. Wiley.</span></li>
<li>Hinton, G. E., Osindero, S., &amp; Teh, Y.-W. (2006). <em class="italic">A Fast Learning Algorithm for Deep Belief Nets</em>. Neural Comput., 18(7), <span class="No-Break">1527–1554. </span><a href="https://doi.org/10.1162/neco.2006.18.7.1527"><span class="No-Break">https://doi.org/10.1162/neco.2006.18.7.1527</span></a><span class="No-Break">.</span></li>
<li>McCulloch, W. S., &amp; Pitts, W. (1943). <em class="italic">A logical calculus of the ideas immanent in nervous activity</em>. The Bulletin of Mathematical Biophysics, 5(4), <span class="No-Break">115–133. </span><a href="https://doi.org/10.1007/BF02478259"><span class="No-Break">https://doi.org/10.1007/BF02478259</span></a><span class="No-Break">.</span></li>
<li>Minsky, M., &amp; Papert, S. (1969). <em class="italic">Perceptrons</em>. <span class="No-Break">MIT Press.</span></li>
<li>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). <em class="italic">Learning representations by back-propagating errors</em>. Nature, 323(6088), <span class="No-Break">533–536. </span><a href="https://doi.org/10.1038/323533a0"><span class="No-Break">https://doi.org/10.1038/323533a0</span></a><span class="No-Break">.</span></li>
</ul>
</div>
</div></body></html>