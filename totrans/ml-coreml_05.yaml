- en: Locating Objects in the World
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have limited ourselves to recognizing the single most dominant object
    within an image using a **convolutional neural network** (**CNN**). We have seen
    how a model can be trained to take in a image and extract a series of feature
    maps that are then fed into a **fully connected layer **to output a probability
    distribution of a set of classes. This is then interpreted to classify the object
    within the image, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/046899f6-8490-4d77-b184-fa56fff667b3.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we will build on this and explore how we can detect and locate
    multiple objects within a single image. We will start by building up our understanding
    of how this works and then walk through implementing a image search for a photo
    gallery application. This application allows the user to filter and sort images
    not only based on what objects are present in the image, but also on their position
    relative to one another (object composition). Along the way, we will also get
    hands-on experience with Core ML Tools, the tool set Apple released to support
    converting models from popular **machine learning** (**ML**) frameworks to Core
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by first understanding what it takes to detect multiple objects
    in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Object localization and object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the introduction of this chapter, we have already been introduced
    to the concepts behind object recognition using CNNs. For this case, we used a
    trained model to perform classification; it achieved this by learning a set of
    feature maps using convolutional layers that are fed into fully connected (or
    dense) layers and, finally, their output, through an activation layer which gave
    us the probability for each of the classes. The class was inferred by selecting
    the one with the largest probability.
  prefs: []
  type: TYPE_NORMAL
- en: Let's differentiate between object recognition, object localization, and object
    detection. Object recognition is the task of classifying the most dominant object
    in a image while object localization performs classification and predicts an object's
    bounding box. Object detection further extends this and allows multiple classes
    to be detected and located, and that's the topic of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is known as **object recognition** and is a classification problem,
    but here we don''t get the full picture (pun intended). What about the location
    of the detected object? That would be useful for increasing the perception capabilities
    of robotic systems or increasing the scope for intelligent interfaces, such as
    intelligent cropping and image enhancements. What about detecting multiple objects
    and their locations? The former, detecting the location of a single object, is
    known as **object localization,** while the later is generally known as **object
    detection**, illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2e938f9-2cd5-4c6f-8657-ee51140ad2e4.png)'
  prefs: []
  type: TYPE_IMG
- en: We will start by introducing object localization and then work our way to object
    detection. The concepts are complementary and the former can be seen as an extension
    of object recognition, which you are already familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: 'When training a model for classification, we tune the weights so that they
    achieve minimum loss for predicting a single class. For object localization, we
    essentially want to extend this to predict not only the class, but also the location
    of the recognized object. Let''s work through a concrete example to help illustrate
    the concept. Imagine we are training a model to recognize and locate a cat, dog,
    or person. For this, our model would need to output the probabilities for each
    class (cat, dog, or person) as we have already seen, and also their location.
    This can be described using the center *x* and *y* position and the width and
    height of the object. To simplify the task of training, we also include a value
    indicating whether an object exists or not. The following figure illustrates two
    input images and their associated outputs. Let''s assume here that our one-hot
    encoded classes are in the order of cat, dog, and person. That is, cat would be
    encoded as *(1,0,0)*, dog as *(0,1,0)*, and person as *(**0,0,1)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f10dbb68-c666-46d6-9ed2-3756bc9b113f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The structure of the output, shown in the preceding image, consists of these
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da6eafe8-786a-45b6-8466-bb1c4dc9ad83.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we have three classes, but this can be generalized to include any arbitrary
    number of classes. What is of note is that if there is no object detected (the
    first element in our output), then we ignore the remaining elements. The other
    important point to highlight is that the bounding box is described in units rather
    than absolute values.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a value of *0.5* for *b[x]* would indicate half of the width of
    the image, where the top left is **(0, 0)** and bottom right is **(1, 1)**, as
    illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50b813d6-eae3-463f-baa0-ce445442a090.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can borrow a lot of the structure of a typical CNN used for classification.
    Therein, the image is fed through a series of convolutional layers and their output,
    a feature vector, through a series of fully connected layers, before being squeezed
    through a softmax activation for multi-class classification (giving us the probability
    distribution across all classes). Instead of just passing the feature vector from
    the convolutional layers to a single set of fully connected layers, we can also
    pass them to a layer (or layers) for binary classification (the fourth element:
    object present or not) and another layer (or series of layers) for predicting
    the bounding box using regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of these modifications can be seen in the following figure, with
    the amendments in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45b9bfc5-e790-4474-aa98-6ae1f66c1a50.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a good start but, more often than not, our images consist of many objects.
    So, let's briefly describe how we can approach this problem.
  prefs: []
  type: TYPE_NORMAL
- en: We are now moving into object detection, where we are interested in detecting
    and locating multiple objects (of different classes) within a single image. So
    far, we have seen how we can detect a single object and its location from an image,
    so a logical progression from this is reshaping our problem around this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'By this, I mean we can use this or a similar approach, but rather than passing
    the full image, we can pass in cropped regions of the image; the regions are selected
    by sliding a window across the image, as follows (and in keeping with our cat
    theme):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbbdab03-082b-405d-9e22-396a48f3cd4d.png)'
  prefs: []
  type: TYPE_IMG
- en: This, for obvious reasons, is called the **sliding window detection algorithm**
    and should be familiar to those who have experience in computer vision (used in template
    matching, among many others). It's important to also emphasize the difference
    in training; in object localization, we trained the network by passing the full
    image along with the associated output vector (*b[x], b[y], b[w], h[y], p[c], c[1],
    c[2], c[3], ...*), while here the network is trained on **tightly cropped images**
    for each of the objects, which may occupy our window size.
  prefs: []
  type: TYPE_NORMAL
- en: For the inquisitive reader wondering how this algorithm detects objects that
    don't fit nicely into the window size, one approach could be to simply resize
    your image (both decreasing and increasing) or, similarly, to use different-sized
    windows, that is, a set of small, medium, and large windows.
  prefs: []
  type: TYPE_NORMAL
- en: This approach has two major drawbacks; the first is that it is computationally
    expensive, and the second is that it doesn't allow for very accurate bounding
    boxes due to the dependency on window sizes and stride size. The former can be
    resolved by rearchitecting the CNN so that it performs the sliding window algorithm
    in a single pass, but we are still left with inaccurate bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, for us, in 2015, J. Redmon, S. Divvala, R. Girshick, and A. Farhadi
    released their paper *You Only Look Once (YOLO): Unified, Real-Time Object Detection*.
    It describes an approach that requires just a single network capable of predicting
    bounding boxes and probabilities from a full image in a single pass. And because
    it is a unified pipeline, the whole process is efficient enough to be performed
    in real time, hence the network we use in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper *You Only Look Once: Unified, Real-Time Object Detection* is available
    here: [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's spend some time getting acquainted with the algorithm YOLO, where we will
    briefly look at the general concepts of the algorithm and interpreting the output
    to make use of it later in the example application for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the major differences of YOLO compared to the previous approaches we
    have discussed in this chapter is how the model is trained. Similar to the first,
    when object localization was introduced, the model was trained on a image and
    label pair, and the elements of the label consisted of (*b[x]*, *b[y]*, *b[w]*,
    *b[h]*, *p[c]*, *c[1]*, *c[2]*, ...), so too is the case with the YOLO network.
    But instead of training on the whole image, the image is broken down into a grid,
    with each cell having an associated label, as outlined before. In the next figure,
    you can see this. The grid illustrated here is a 3 x 3 for legibility; these are
    typically more dense, as you''ll soon see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80e6dea6-0a0c-4bf7-9caa-0a3c57addce3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When training, we feed in the image and the network is structured so that it
    outputs a **vector** (as shown before) for **each grid cell**. To make this more
    concrete, the following figure illustrates how some of these outputs would look
    for the cells within this grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0b85d59-a5c9-41fd-b971-dd44c3e5378a.png)'
  prefs: []
  type: TYPE_IMG
- en: This should look familiar to you as it is, as previously mentioned, very similar
    to the approach we first introduced for object localization. The only major difference
    is that it is performed on each grid cell as opposed to the whole image.
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that despite the object spanning across multiple cells, the
    training samples only label an object using a single cell (normally the cell in
    the center of the image), with the other encompassing cells having no object assigned
    to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on, let''s unpack the bounding box variables. In the preceding
    figure, I have entered approximate values for each of the objects; like object
    localization, these values are normalized between *0.0* and *1.0* for values that
    fall within the cell. But, unlike object localization, these values are **local**
    to the cell itself rather than the image. Using the first cat as an example, we
    can see that the central position is *0.6* in the *x* axis and *0.35* in the *y*
    axis; this can be interpreted as being at a position 60% along the *x* axis of
    the cell and 35% along the *y* axis of the cell. Because our bounding box extends
    beyond the cell, there are assigned values greater than one, as we saw in the
    preceding figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cd83249-1015-49b0-8408-ff2ad8748e31.png)'
  prefs: []
  type: TYPE_IMG
- en: Previously, we highlighted that the training sample only assigns a single cell
    per object but, given that we are running object detection and localization on
    each cell, it is likely that we will end up with multiple predictions. To manage
    this, YOLO uses something called **non-max suppression**, which we will introduce
    over the next few paragraphs, and you'll get a chance to implement it in the upcoming
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned before, because we are performing object detection and localization
    on each grid cell, it is likely that we will end up with multiple bounding boxes.
    This is shown in the following figure. To simplify the illustration, we will just
    concentrate on a single object, but this of course applies to all detected objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c42812f-b4cd-4d60-8543-2461ec466366.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous figure, we can see that the network has predicted and located
    the cat in three cells. For each cell, I have added a fictional confidence value,
    located at the top-right, to each of their associated bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: The **confidence value** is calculated by multiplying the object present probability
    (*p[c]*) with the most likely class, that is, the class with the highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in non-max suppression is simply to filter out predictions that
    don''t meet a certain threshold; for all intents and purposes, let''s set our
    object threshold to *0.3*. With this value, we can see that we filter out one
    of the predictions (with a confidence value of *0.2*), leaving us with just two,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbcb36de-1806-49f1-a12b-9f95bdd3a72c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next step, we iterate over all detected boxes, from the one with the
    largest confidence to the least, removing any other bounding boxes that occupy
    the same space. In the preceding figure, we can clearly see that both bounding
    boxes are essentially occupying the same area, but how do we determine this programmatically?
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine this, we calculate what is called the **intersection over union**
    (**IoU**) and test the returned value against a threshold. We calculate this,
    as the name implies, by dividing the **intersection area** of the two bounding
    boxes by their **union area**. A value of *1.0* tells us that the two bounding
    boxes occupy exactly the same space (a value you would get if you performed this
    calculation using a single bounding box with itself). Anything below this gives
    us a ratio of overlapped occupancy; a typical threshold is *0.5*. The following
    figure illustrates the intersection and union areas of our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/830b7548-9fcf-4473-a286-252b13806d68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Because these two bounding boxes would return a relatively high IoU, we would
    end up pruning the least probable one (the one with the lower confidence score)
    and end up with a single bounding box, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23a6b3ce-185a-4eee-9bdb-15093c7adb22.png)'
  prefs: []
  type: TYPE_IMG
- en: We repeat this process until we have iterated over all of the model's predictions.
    There is just one more concept to introduce before moving on to the example project
    for this chapter and starting to write some code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Up until this point, we have assumed (or have been constrained by the idea) that
    each cell is associated with either no object or a single object. But what about
    the case of objects overlapping, where two objects'' center positions occupy the
    same grid cell? To handle circumstances like this, the YOLO algorithm implements
    something called **anchor boxes**. Anchor boxes allow multiple objects to occupy
    a single grid cell given that their bounding shape differs. Let''s make this more
    concrete by explaining it visually. In the next figure, we have a image where
    the centers of two objects occupy the same grid cell. With our current output
    vector, we would need to label the cell as either a person or a bike, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46fb4511-7627-440b-a97e-394862e35787.png)'
  prefs: []
  type: TYPE_IMG
- en: The idea with anchor boxes is that we extend our output vector to include different
    variations of anchor boxes (illustrated here as two, but these can be of any number).
    This allows each cell to encode multiple objects so long as they have different
    bounding shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the previous figure, we see that we can use two anchor boxes, one for
    the person and the other for the bike, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2eb10ad6-92a7-4029-8aa6-d76c5f03b343.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With our anchor boxes now defined, we extend our vector output such that for
    each grid cell we can encode the output for both anchor boxes, as illustrated
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/803ec1ef-98a0-4285-847e-b5f0b5b1b1cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Each anchor box can be treated independently of each other output of the same
    cell and other cells; that is, we handle it exactly as we did previously, the
    only addition now being that we have more bounding boxes to process.
  prefs: []
  type: TYPE_NORMAL
- en: Just to clarify, anchor boxes are not constrained to a specific class even though
    some shapes are more suitable than others. They are typically generalized shapes
    found using some type of unsupervised learning algorithm, such as **k-means**, within
    an existing dataset to find the dominant shapes.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes all the concepts we need to understand for this chapter's example
    and what we will be implementing in the coming sections but, before we do, let's
    walk through converting a Keras model to Core ML using the Core ML Tools Python
    package.
  prefs: []
  type: TYPE_NORMAL
- en: Converting Keras Tiny YOLO to Core ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we discussed the concepts of the model and algorithm
    we will be using in this chapter. In this section, we will be moving one step
    closer to realizing the example project for this chapter by converting a trained
    Keras model of Tiny YOLO to Core ML using Apple's Core ML Tools Python package;
    but, before doing so, we will quickly discuss the model and the data it was trained
    on.
  prefs: []
  type: TYPE_NORMAL
- en: YOLO was conceived on a neural network framework called **darknet**, which is
    currently not supported by the default Core ML Tools package; fortunately, the
    authors of YOLO and darknet have made the architecture and weights of the trained
    model publicly available on their website at [https://pjreddie.com/darknet/yolov2/](https://pjreddie.com/darknet/yolov2/).
    There are a few variations of YOLO that have been trained on either the dataset
    from **Common Objects in Context **(**COCO**), which consists of 80 classes, or
    The PASCAL **Visual Object Classes** (**VOC**) Challenge 2007, which consists
    of 20 classes.
  prefs: []
  type: TYPE_NORMAL
- en: The official website can be found at [http://cocodataset.org](http://cocodataset.org),
    and *The PASCAL VOC Challenge 2007* at [http://host.robots.ox.ac.uk/pascal/VOC/index.html](http://host.robots.ox.ac.uk/pascal/VOC/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be using the Tiny version of YOLOv2 and the weights
    from the model trained on the *The PASCAL VOC Challenge 2007* dataset. The Keras
    model we will be using was modeled on the configuration file and weights available
    on the official site (link presented previously).
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we will be omitting a lot of details of the model and will instead
    provide the model in its diagrammatic form, shown next. We''ll then discuss some
    of the relevant parts before moving on to convert it into a Core ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68b12837-9f34-460f-b84f-72051d5898e0.png)'
  prefs: []
  type: TYPE_IMG
- en: The first thing to notice is the shape of the input and output; this indicates
    what our model will be expecting to be fed and what it will be outputting for
    us to use. As shown before, the input size is 416 x 416 x 3, which, as you might
    suspect, is a 416 x 416 RGB image. The output shape needs a little more explanation
    and it will become more apparent when we arrive at coding the example for this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output shape is 13 x 13 x 125\. The 13 x 13 tells us the size of the grid
    being applied, that is, the 416 x 416 image is broken into a grid of 13 x 13 cells,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7f5353d-1f9f-4c88-864f-09eaa71d0960.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As discussed previously, each cell has a 125*-*vector encoding of the probability
    of an object being present and, if so, the bounding box and probabilities across
    all 20 classes; visually, this is explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e83bddf8-2a30-4205-a45f-36d08e13191c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final point about the model that I want to highlight is its simplicity;
    the bulk of the network is made up of convolutional blocks consisting of a convolutional
    layer: **Batch Normalization**, **LeakyReLU** activation, and finally a **MaxPooling**
    layer. This progressively increases the filter size (depth) of the network until
    it has reached the desired grid size and then transforms the data using only the
    convolutional layer, **Batch Normalization**, and **LeakyReLU** activation, dropping
    the **MaxPooling**.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have introduced the terms batch normalization and leaky ReLU, which
    may be unfamiliar to some; here I will provide a brief description of each, starting
    with batch normalization. It's considered best practice to normalize the input
    layer before feeding it into a network. For example, we normally divide pixel
    values by 255 to force them into a range of 0 to 1\. We do this to make it easier
    for the network to learn by removing any large values (or large variance in values),
    which may cause our network to oscillate when adjusting the weights during training.
    Batch normalization performs this same adjustment for the outputs of the hidden
    layers rather than the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU is an activation function that sets anything below 0 to 0, that is, it
    doesn't allow non-positive values to propagate through the network. Leaky ReLU
    provides a less strict implementation of ReLU, allowing a small non-zero gradient
    to slip through when the neuron is not active.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes our brief overview of the model. You can learn more about it
    from the official paper *YOLO9000: Better, Faster, Stronger* by J. Redmon and
    A. Farhadi, available at [https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242).
    Let''s now turn our attention to converting the trained Keras model of the Tiny
    YOLO to Core ML.'
  prefs: []
  type: TYPE_NORMAL
- en: As alluded to in [Chapter 1](7d4f641f-7137-4a8a-ae6e-2bb0e2a6db5c.xhtml), *Introduction
    to Machine Learning*, Core ML is more of a suite of tools than a single framework.
    One part of this suite is the Core ML Tools Python package, which assists in converting
    trained models from other frameworks to Core ML for easy and rapid integration.
    Currently, official converters are available for Caffe, Keras, LibSVM, scikit-learn,
    and XGBoost, but the package is open source, with many other converters being
    made available for other popular ML frameworks, such as TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, the conversion process generates a model specification that is
    a machine-interpretable representation of the learning models and is used by Xcode
    to generate the Core ML models, consisting of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model description**: Encodes names and type information of the inputs and
    outputs of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model parameters**: The set of parameters required to represent a specific
    instance of the model (model weights/coefficients)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additional metadata**: Information about the origin, license, and author
    of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we present the most simplistic of flows, but we will revisit
    the Core ML Tools package in [Chapter 6](40971e0d-b260-42e1-a9fb-5c4a56b0ebb2.xhtml),
    *Creating Art with Style Transfer*, to see how to deal with custom layers.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid any complications when setting up the environment on your local or
    remote machine, we will leverage the free Jupyter cloud service provided by Microsoft.
    Head over to [https://notebooks.azure.com](https://notebooks.azure.com) and log
    in, or register if you haven't already.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once logged in, click on the Libraries menu link from the navigation bar, which
    will take you to a page containing a list of all your libraries, similar to what
    is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c8f12d6-2b5b-4c41-a724-6903324fa026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, click on the + New Library link to bring up the Create New Library dialog:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19bb5167-2674-4f8c-bc58-c8b64bab75b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Then click on the From GitHub tab and enter `https://github.com/packtpublishing/machine-learning-with-core-ml`
    in the GitHub repository field. After that, give your library a meaningful name
    and click on the Import button to begin the process of cloning the repository
    and creating the library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the library has been created, you will be redirected to the root; from
    here, click on the `Chapter5/Notebooks` folder to open up the relevant folder
    for this chapter. Finally, click on the Notebook `Tiny YOLO_Keras2CoreML.ipynb`.
    To help ensure that we are all on the same page (pun intended), here is a screenshot
    of what you should see after clicking on the `Chapter5/Notebooks` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d07e169-339e-41dd-904d-d5db7e83baff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With our Notebook now loaded, it''s time to walk through each of the cells
    to create our Core ML model; all of the required code exists and all that remains
    is executing each of the cells sequentially. To execute a cell, you can either
    use the shortcut keys *Shift* + *Enter* or click on the Run button in the toolbar
    (which will run the currently selected cell), as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c956b413-3944-4894-9ebb-915ccba65bfd.png)'
  prefs: []
  type: TYPE_IMG
- en: I will provide a brief explanation of what each cell does; ensure that you execute
    each cell as we walk through them so that we all end up with the converted model,
    which we will then download and use in the next section for our iOS project.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by ensuring that the Core ML Tools Python package is available in
    the environment, by running the following cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installed, we make the package available by importing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The model architecture and weights have been serialized and saved to the file
    `tinyyolo_voc2007_modelweights.h5`; in the following cell, we will pass this into
    the convert function of the Keras converter, which will return the converted Core
    ML model (if no errors occur). Along with the file, we also pass in values for
    the parameters `input_names`, `image_input_names`, `output_names`, and `image_scale`.
    The `input_names` parameter takes in a single string, or a list of strings for
    multiple inputs, and is used to explicitly set the names that will be used in
    the interface of the Core ML model to refer to the inputs of the Keras model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also pass this input name to the `image_input_names` parameter so that the
    converter treats the input as an image rather than an N-dimensional array. Similar
    to `input_names`, values passed to `output_names` will be used in the interface
    of the Core ML model to refer to the outputs of the Keras model. The last parameter,
    `image_scale`, allows us to add a scaling factor to our input before being passed
    to the model. Here, we are dividing each pixel by 255, which forces each pixel
    to be in a range of *0.0* to *1.0*, a typical preprocessing task when working
    with images. There are plenty more parameters available, allowing you to tune
    and tweak the inputs and outputs of your model. You can learn more about these
    at the official documentation site here at [https://apple.github.io/coremltools/generated/coremltools.converters.keras.convert.html](https://apple.github.io/coremltools/generated/coremltools.converters.keras.convert.html).
    In the next snippet, we perform the actual conversion using what we have just
    discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With reference to the converted model, `coreml_model`, we add metadata, which
    will be made available and displayed in Xcode''s ML model views:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to save our model; run the final cell to save the converted
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With our model now saved, we return to the previous tab showing the contents
    of the `Chapter5` directory and download the `tinyyolo_voc2007.mlmodel` file.
    We do so by either right-clicking on it and selecting the Download menu item,
    or by clicking on the Download toolbar item, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e089f81-195a-4c68-ac48-937d747bb77e.png)'
  prefs: []
  type: TYPE_IMG
- en: With our converted model in hand, it's now time to jump into Xcode and work
    through the example project for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Making it easier to find photos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will put our model to work in an intelligent search application;
    we'll start off by quickly introducing the application, giving us a clear vision
    of what we intend to build. Then, we'll work through implementing the functionality
    related to interpreting the model's output and search heuristic for the desired
    functionality. We will be omitting a lot of the usual iOS functionality so that
    we can stay focused on the intelligent aspect of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few years, we have seen a surge of intelligence being embedded
    in photo gallery applications, providing us with efficient ways of surfacing those
    cat photos hidden deep in hundreds (if not thousands) of photos we have accumulated
    over the years. In this section, we want to continue with this theme but push
    that level of intelligence a little bit further by taking advantage of the semantic
    information gained through object detection. Our users will be able to search
    for not only specific objects within a image, but also photos based on the objects
    and their relative positioning. For example, they can search for an image, or
    images, with two people standing side by side and in front of a car.
  prefs: []
  type: TYPE_NORMAL
- en: The user interface allows the user to draw the objects as they would like them
    positioned and their relative sizes. It will be our job in this section to implement
    the intelligence that returns relevant images based on this search criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next figure shows the user interface; the first two screenshots show the
    search screen, where the user can visually articulate what they are looking for.
    By using labeled bounding boxes, the user is able to describe what they are looking
    for, how they would like these objects arranged, and relative object sizes. The
    last two screenshots show the result of a search, and when expanded (last screenshot),
    the image will be overlaid with the detected objects and associated bounding boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2d18585-97a8-4c18-980f-5106f4e79979.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's start by taking a tour of the existing project before importing the model
    we have just converted and downloaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you haven''t already, pull down the latest code from the accompanying repository: 
    [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter5/Start/` and open the project
    `ObjectDetection.xcodeproj`. Once loaded, you will see the project for this chapter,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e23df0d7-0d32-4113-a52d-9d5d45500745.png)'
  prefs: []
  type: TYPE_IMG
- en: I will leave exploring the full project as an exercise for you, and I'll just
    concentrate on the files `PhotoSearcher.swift` and `YOLOFacade.swift` for this
    section. `PhotoSearcher.swift` is where we will implement the cost functions responsible
    for filtering and sorting the photos based on the search criteria and detected
    objects from `YOLOFacade.swift`, whose sole purpose is to wrap the Tiny YOLO model
    and implement the functionality to interpret its output. But before jumping into
    the code, let's quickly review the flow and data structures we will be working
    with.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram illustrates the general flow of the application; the user
    first defines the search criteria via `SearchViewController`, which is described
    as an array of normalized `ObjectBounds`. We'll cover more details on these later.
    When the user initiates the search (top-right search icon) these are passed to
    `SearchResultsViewController`, which delegates the task of finding suitable images
    to `PhotoSearcher`.
  prefs: []
  type: TYPE_NORMAL
- en: '`PhotoSearcher` proceeds to iterate through all of our photos, passing each
    of them through to `YOLOFacade` to perform object detection using the model we
    converted in the previous section. The results of these are passed back to `PhotoSearcher`,
    which evaluates the cost of each with respect to the search criteria and then
    filters and orders the results, before passing them back to `SearchResultsViewController`
    to be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d31059c4-70ff-44fe-b7bf-c737a34299ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each component communicates with the another using either the data object `ObjectBounds`
    or `SearchResult`. Because we will be working with them throughout the rest of
    this chapter, let''s quickly introduce them here, all of which are defined in
    the `DataObjects.swift` file. Let''s start with `ObjectBounds`, the structure
    shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As the name suggests, `ObjectBounds` is just that—it encapsulates the boundary
    of an object using the variables `origin` and `size`. The `object` itself is of
    type `DetectableObject`, which provides a structure to store both the class index
    and its associated label. It also provides a static array of objects that are
    available in our search, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`ObjectBounds` are used for both the search criteria defined by the user and
    search results returned by `YOLOFacade`; in the former, they describe where and
    which objects the user is interested in finding (search criteria), and the latter
    encapsulates the results from object detection.'
  prefs: []
  type: TYPE_NORMAL
- en: '`SearchResult` doesn''t get any more complex; it''s intended to encapsulate
    the result of a search with the addition of the image and cost, which is set during
    the cost evaluation stage (*step 8*), as shown in the previous diagram. For the
    complete code, the structure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It's worth noting that the `ObjectBounds` messages, in the previous diagram,
    annotated with the word **Normalized**, refer to the values being in unit values
    based on the source or target size; that is, an origin of *x = 0.5* and *y = 0.5*
    defines the center of the source image it was defined on. The reason for this
    to ensure that the bounds are invariant to changes in the images they are operating
    on. You will soon see that, before passing images to our model, we need to resize
    and crop to a size of 416 x 416 (the expected input to our model), but we need
    to transform them back to the original for rendering the results.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a better idea of what objects we will be consuming and generating;
    let's proceed with implementing the `YOLOFacade` and work our way up the stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the model we have just converted in the previous
    section; locate the downloaded `.mlmodel` file and drag it onto Xcode. Once imported,
    select it from the left-hand panel to inspect the metadata to remind ourselves
    what we need to implement. It should resemble this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c09a525c-cff6-494f-b272-cd1d0681949f.png)'
  prefs: []
  type: TYPE_IMG
- en: With our model now imported, let's walk through implementing the functionality
    `YOLOFacade` is responsible for; this includes preprocessing the image, passing
    it to our model for inference, and then parsing the model's output, including
    performing **non-max supression**. Select `YOLOFacade.swift` from the left-hand
    panel to bring up the code in the main window.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class is broken into three parts, via an extension, with the first including
    the variables and entry point; the second including the functionality for performing
    inference and parsing the models outputs; and the third part including the non-max
    supression algorithm we discussed at the start of this chapter. Let''s start at
    the beginning which currently looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `asyncDetectObjects` method is the entry point of the class and is called
    by `PhotoSearcher` for each image it receives from the Photos framework; when
    called, this method simply delegates the task to the method `detectObject` in
    the background and waits for the results, before passing them back to the caller
    on the main thread. I have annotated the class with `TODO` to help you keep focused.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by declaring the target size required by our model; this will
    be used for preprocessing of the input of our model and transforming the normalized
    bounds to those of the source image. Add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define properties of our model that are used during parsing of the
    output; these include grid size, number of classes, number of anchor boxes, and
    finally, the dimensions for each of the anchor boxes (each pair describes the
    width and height, respectively). Make the following amendments to your `YOLOFacade`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now implement the model property; in this example, we will take advantage
    of the Vision framework for handling the preprocessing. For this, we will need
    to wrap our model in an instance of `VNCoreMLModel` so that we can pass it into
    a `VNCoreMLRequest`; make the following amendments, as shown in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let's now turn our attention to the `detectObjects` method. It will be responsible
    for performing inference via `VNCoreMLRequest` and `VNImageRequestHandler`, passing
    the model's output to the `detectObjectsBounds` method (which we will come to
    next), and finally transforming the normalized bounds to the dimensions of the
    original (source) image.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will postpone the discussion around the Vision framework
    classes (`VNCoreMLModel`, `VNCoreMLRequest`, and `VNImageRequestHandler`) until
    the next chapter, where we will elaborate a little on what each does and how they
    work together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the `detectObjects` method of `YOLOFacade`, replace the comment `//
    TODO preprocess image and pass to model` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, we start by creating an instance of `VNCoreMLRequest`,
    passing in our model, which itself has been wrapped with an instance of `VNCoreMLModel`.
    This request performs the heavy lifting, including preprocessing (inferred by
    the model's metadata) and performing inference. We set its `imageCropAndScaleOption`
    property to `centerCrop`, which determines, as you might expect, how the image
    is resized to fit into the model's input. The request itself doesn't actually
    execute the task; this is the responsibility of `VNImageRequestHandler`, which
    we declare next by passing in our source image and then executing the request
    via the handler's `perform` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'If all goes to plan, we should expect to have the model''s output available
    via the request''s results property. Let''s move on to the last snippet for this
    method; replace the comment `// TODO pass models results to detectObjectsBounds(::)`
    and the following statement, `completionHandler(nil)`, with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We begin by trying to cast the results to an array of `VNCoreMLFeatureValueObservation`,
    a type of image analysis observation that provides key-value pairs. One of them
    is `multiArrayValue`, which we then pass to the `detectObjectsBounds` method to
    parse the output and return the detected objects and their bounding boxes. Once
    `detectObjectsBounds` returns, we map each of the results with the `ObjectBounds`
    method `transformFromCenteredCropping`, which is responsible for transforming
    the normalized bounds into the space of the source image. Once each of the bounds
    has been transformed, we call the completion handler, passing in the detected
    bounds.
  prefs: []
  type: TYPE_NORMAL
- en: The next two methods encapsulate the bulk of the YOLO algorithm and the bulk
    of the code for this class. Let's start with the `detectObjectsBounds` method,
    making our way through it in small chunks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method will receive an `MLMultiArray` with the shape of *(125, 13, 13)*;
    this will hopefully look familiar to you (although reversed) where the *(13, 13)*
    is the size of our grid and the 125 encodes five blocks (coinciding with our five
    anchor boxes) each containing the bounding box, the probability of an object being
    present (or not), and the probability distribution across 20 classes. For your
    convenience, I have again added the diagram illustrating this structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca1f312e-358e-4a58-98f1-80da52f41fc8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To improve performance, we will access the `MLMultiArray`''s raw data directly,
    rather than through the `MLMultiArray` subscript. Although having direct access
    gives us a performance boost, it does have a trade-off of requiring us to correctly
    calculate the index for each value. Let''s define the constants that we will use
    when calculating these indexes, as well as obtaining access to the raw data buffer
    and some arrays to store the intermediate results; add the following code within
    your `detectObjectsBounds` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned before, we start by defining constants of stride values for the
    grid, row, and column—each used to calculate the current value. These values are
    obtained through the `strides` property of `MLMultiArray`, which gives us the
    number of data elements in each dimension. In this case, this would be 125, 13,
    and 13 respectively. Next, we get a reference to the underlying buffer of the
    `MLMultiArray` and, finally, we create two arrays to store the bounds and associated
    confidence value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we want to iterate through the model''s output and process each of the
    grid cells and their subsequent anchor boxes independently; we do this by using
    three nested loops and then calculating the relevant index. Let''s do that by
    adding the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The important values here are `gridOffset` and `anchorBoxOffset`; `gridOffset`
    gives us the relevant offset for the specific grid cell (as the name implies),
    while `anchorBoxOffset` gives us the index of the associated anchor box. Now that
    we have these values, we can access each of the elements using the `[(anchorBoxOffset
    + INDEX_TO_VALUE) * gridStride + gridOffset` index, where `INDEX_TO_VALUE` is
    the relevant value within the anchor box vector we want to access, as illustrated
    in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f430c34-c32a-4b53-bfad-50114ad51c93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we know how to access each bounding box for each grid cell in our buffer,
    let''s use it to find the most probable class and put in our first test of ignoring
    any prediction if it doesn''t meet our threshold (defined as a method parameter
    with the default value of 0.3*:* `objectThreshold:Float = 0.3`). Add the following
    code, replacing the comment `// TODO calculate the confidence of each class, ignoring
    if under threshold`, as seen previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we first obtain the probability of an object
    being present and store it in the constant `confidence`. Then, we populate an
    array with the probabilities of all the classes, before applying a softmax across
    them all. This will squash the values so that the accumulated value of them equals
    *1.0,* essentially providing us with our probability distribution across all classes.
  prefs: []
  type: TYPE_NORMAL
- en: We then find the class index with the largest probability and multiply it with
    our `confidence` constant, which gives us the class confidence we will threshold
    against and use during non-max suppression, ignoring the prediction if it doesn't
    meet our threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before continuing with the procedure, I want to take a quick detour to highlight
    and explain a couple of the methods used in the preceding snippet, namely the
    `softmax` method and `argmax` property of the classes array. Softmax is a logistic
    function that essentially squashes a vector of numbers so that all values in the
    vector add up to 1; it''s an activation function commonly used when dealing with
    multi-class classification problems where the result is interpreted as the likelihood
    of each class, typically taking the class with the largest value as the predicted
    class (within a threshold). The implementation can be found in the `Math.swift`
    file, which makes use of the Accelerate framework to improve performance. The
    equation and implementation are shown here for completeness, but the details are
    omitted and left for you to explore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afcd28b0-c339-49fc-a38a-f254a0f0c909.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we use a slightly modified version of the equation shown previously; in
    practice, calculating the softmax values can be problematic if any of the values
    are very large. Applying an exponential operation on it will make it explode,
    and dividing any value by a huge value can cause arithmetic computation problems.
    To avoid this, it is often best practice to subtract the maximum value from all
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because there are quite a few functions for this operation, let''s build it
    up piece by piece, from the inside out. The following is the function that performs
    element-wise subtraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the function that computes the element-wise exponential for an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the function to perform summation on an array, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the last function used by the softmax function! This will be responsible
    for performing element-wise division for a given scalar, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we the softmax function (using the max trick as described previously):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to the preceding functions, it uses an extension property, `maxValue`,
    of Swift''s array class; this extension also includes the `argmax` property alluded
    to previously. So, we will present both together in the following snippet, found
    in the `Array+Extension.swift` file. Before presenting the code, just a reminder
    about the function of the `argmax` property—its purpose is to return the index
    of the largest value within the array, a common method available in the Python
    package NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now turn our attention back to the parsing of the model''s output and
    extracting the detected objects and associated bounding boxes. Within the loop,
    we now have a prediction we are somewhat confident with, having passed our threshold
    filter. The next task is to extract and transform the bounding box of the predicted
    object. Add the following code, replacing the line `// TODO obtain bounding box
    and transform to image dimensions`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We start by getting the first four values of from the grid cell's anchor box
    segment; this returns the center position and size relative to the grid. The next
    block is responsible for transforming these values from the grid coordinate system
    to the image coordinate system. For the center position, we pass the returned
    value through a `sigmoid` function, keeping it between *0.0 - 1.0*, and offset
    based on the relevant column (or row). Finally we divide it by the grid size (13).
    Similarly with the dimensions, we first get the associated anchor box, multiplying
    it by the exponential of the predicted dimension and then dividing it by the grid
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have done previously, I now present the implementations for the function
    `sigmoid` for reference, which can found in the `Math.swift` file. The equation
    is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1fb2ae26-e3be-4ffa-9f64-beffc326387d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The final chunk of code simply creates an instance `ObjectBounds`, passing
    in the transformed bounding box and the associated `DetectableObject` class (filtering
    on the class index). Add the following code, replacing the comment `// TODO create
    a ObjectBounds instance and store it in our array of candidates`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In addition to storing the `ObjectBounds`, we also store `confidence`, which
    will be used when we get to implementing non-max suppression.
  prefs: []
  type: TYPE_NORMAL
- en: 'This completes the functionality required within the nested loops; by the end
    of this process, we have an array populated with our candidate detected objects.
    Our next task will be to filter them. Near the end of the `detectObjectsBounds`
    method, add the following statement (outside any loops):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are simply returning the results from the `filterDetectedObjects`
    method, which we will now turn our attention to. The method has been blocked out
    but is vacant of functionality, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Our job will be to implement the non-max suppression algorithm; just to recap,
    the algorithm can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Order the detected boxes from most confident to least
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While valid boxes remain, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick the box with the highest confidence value (the top of our ordered array)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate through all the remaining boxes, discarding any with an IoU value greater
    than a predefined threshold
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s start by creating a clone of the confidence array passed into the method;
    we will use this to obtain an array of sorted indices, as well as to flag any
    boxes that are sufficiently overlapped by the preceding box. This is done by simply
    setting its confidence value to 0\. Add the following statement to do just this,
    along with creating the sorted array of indices, replacing the comment `// TODO
    implement Non-Max Suppression`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned previously, we start by cloning the confidence array, assigning
    it to the variable `detectionConfidence`. Then, we sort the indices in descending
    order and, finally, create an array to store the boxes we want to keep and return.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create the loops that embody the bulk of the algorithm, including
    picking the next box with the highest confidence and storing it in our `bestObjectsBounds`
    array. Add the following code, replacing the comment `// TODO iterate through
    each box`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Most of the code should be self-explanatory; what's worth noting is that within
    each loop, we test that the associated boxes confidence is greater than 0\. As
    mentioned before, we use this to indicate that an object has been discarded due
    to being sufficiently overlapped by a box with higher confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'What remains is calculating the IoU between `objectBounds` and `otherObjectBounds`,
    and invaliding `otherObjectBounds` if it doesn''t meet our IoU threshold, `nmsThreshold`.
    Replace the comment `// TODO calculate IoU and compare against our threshold`,
    with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are using a `CGRect` extension method, `computeIOU`, to handle the
    calculation. Let''s have a peek at this, implemented in the file `CGRect+Extension.swift`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to the existing `intersection` and `union` of the `CGRect` structure,
    this method is nice and concise.
  prefs: []
  type: TYPE_NORMAL
- en: One final thing to do before we finish with the `YOLOFacade` class as well as
    the YOLO algorithm is to return the results. At the bottom of the `filterDetectedObjects`
    method, return the array `bestObjectsBounds`; with that done, we can now turn
    our attention to the last piece of functionality before implementing our intelligent
    search photo application.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter does a good job highlighting that most of the effort integrating
    ML into your applications surrounds the **preprocessing** of the data before feeding
    it into the model and **interpreting** the output of the model. The **Vision framework**
    does a good job alleviating the preprocessing tasks, but there is still significant
    effort handling the output. Fortunately, no doubt because object detection is
    compelling for many applications, Apple has added a new observation type explicitly
    for object detection called **VNRecognizedObjectObservation**. Although we don't
    cover it here; I encourage you to review the official documentation [https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation](https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation).
  prefs: []
  type: TYPE_NORMAL
- en: 'The next piece of functionality is concerned with evaluating a cost on each
    of the returned detected objects with respect to the user''s search criteria;
    by this, I mean filtering and sorting the photos so that the results are relevant
    to what the user sought. As a reminder, the search criteria is defined by an array
    of `ObjectBounds`, collectively describing the objects the user wants within a
    image, their relative positions, as well as the sizes relative to each other and
    to the image itself. The following figure shows how the user defines their search
    within our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/334ca2eb-c83f-4a10-8aa7-a224e2a3e50e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we will implement only two of the four evaluations, but it should provide
    a sufficient base for you to implement the remaining two yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost evaluation is performed within the `PhotoSearcher` class once the
    `YOLOFacade` has returned the detected objects for all of the images. This code
    resides in the `asyncSearch` method (within the `PhotoSearcher.swift` file), highlighted
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`calculateCostForObjects` takes in the search criteria and results from the
    `YOLOFacade` and returns an array of `SearchResults` from the `detectObjects`
    with their cost properties set, after which they are filtered and sorted before
    being returned to the delegate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s jump into the `calculateCostForObjects` method and discuss what we mean
    by cost; the code of the method `calculateCostForObjects` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: A `SearchResult` incurs a cost each time it differs from the user's search criteria,
    meaning that the results with the least cost are those that better match the search
    criteria. We perform cost evaluation on four different heuristics; each method
    will be responsible for adding the calculated cost to each result. Here we will
    only implement `costForObjectPresences` and `costForObjectRelativePositioning`,
    leaving the remaining two as an exercise for your.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s jump straight in and start implementing the `costForObjectPresences`
    method; at the moment, it''s nothing more than a stub, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Before writing the code, let's quickly discuss what we are evaluating for. Maybe,
    a better name for this function would have been `costForDifference` as we not
    only want to assess that the image has objects declared in the search criteria,
    but also we equally want to increase the cost for additional objects. That is,
    if the user searches for just two dogs but a photo has three dogs or two dogs
    and a cat, we want to increase the cost for these additional objects such that
    we are favoring the one that is most similar to the search criteria (just two
    dogs).
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate this, we simply need to find the absolute difference between the
    two arrays; to do this, we first create a dictionary of counts for all classes
    in both `detectedObject` and `searchCriteria`. The directory''s key will be the
    object''s label and the corresponding value will be the count of objects within
    the array. The following figure illustrates these arrays and formula used to calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4ce80a3-a157-4be3-b033-2c3e76108094.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now implement it; add the following code to do this, replacing the comment
    `// TODO implement cost function for object presence`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, with our count dictionaries created and populated, it''s simply a matter
    of iterating over all available classes (using the items in `DetectableObject.objects`)
    and calculating the cost based on the absolute difference between the two. Add
    the following code, which does this, by replacing the comment `// TODO accumulate
    cost based on the difference`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The result of this is a cost that is larger for images that differ the most
    from the search criteria; the last thing worth noting is that the cost is multiplied
    by a weight before being returned (function parameter). Each evaluation method
    has a weight parameter which allows for easy tuning (during either design time
    or runtime) of the search, giving preference to one evaluation over another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next, and last, cost evaluation function we are going to implement is the
    method `costForObjectRelativePositioning`; the stub of this method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As we did before, let's quickly discuss the motivation behind this evaluation
    and how we plan to implement it. This method is used to favor items that match
    the composition of the user's search; this allows our search to surface images
    that closely resemble the arrangement the user is searching for. For example,
    the user may be looking for an image or images where two dogs are sitting next
    to each other, side by side, or they may want an image with two dogs sitting next
    to each other on a sofa.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are no doubt many approaches you could take for this, and it''s perhaps
    a use case for a neural network, but the approach taken here is the simplest I
    could think of to avoid having to explain complicated code; the algorithm used
    is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For each object (`a`) of type `ObjectBounds` within `searchCriteria`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the closest object (`b`) in proximity (still within `searchCriteria`)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a normalized direction vector from `a` to `b`
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the matching object `a'` (the same class) within the `detectedObject`
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Search all other objects (`b'`) in `detectedObject` that have the same class
    as `b`
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a normalized direction vector from `a'` to `b'`
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the dot product between the two vectors (angle); in this case, our
    vectors are `a->b` and `a'->b'`
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `a'` and `b'`, which have the lowest dot product, increment the cost by
    how much the angle differs from the search criteria and images
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Essentially, what we are doing is finding two matching pairs from the `searchCriteria`
    and `detectedObject` arrays, and calculating the cost based on the difference
    in the angles.
  prefs: []
  type: TYPE_NORMAL
- en: A direction vector of two objects is calculated by subtracting one's position
    from the other and then normalizing it. The dot product can then be used on two
    (normalized) vectors to find their angle, where *1.0* would be returned if the
    vectors are pointing in the same direction, *0.0* if they are perpendicular, and
    *-1.0* if pointing in opposite directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure presents part of this process; we first find an object
    pair in close proximity within the search criteria. After calculating the dot
    product, we iterate over all the objects detected in the image and find the most
    suitable pair; "suitable" here means the same object type and the closest angle
    to the search criteria within the possible matching pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6556913-07e2-45cd-90be-1fe8e929762b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once comparable pairs are found, we calculate the cost based on the difference
    in angle, as we will soon see. But we are getting a little ahead of ourselves;
    we first need a way to find the closest object. Let''s do this using a nested
    function we can call within our `costForObjectRelativePositioning` method. Add
    the following code, replacing the comment `// TODO implement cost function for
    relative positioning`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function will be used to find the closest object, given an array
    of `ObjectBounds` and index of the object we are searching against. From there,
    it simply iterates over all of the items in the array, returning the one that
    is, well, closest.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our helper function now implemented, let''s create the loop that will
    inspect the search item pair from the user''s search criteria. Append the following
    code to the `costForObjectRelativePositioning` method, replacing the comment `//
    TODO Iterate over all items in the searchCriteria array`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We start by searching for the closest object to the current object, jumping
    to the next item if nothing is found. Once we have our search pair, we proceed
    to calculate the direction by subtracting the first bound's center from its pair
    and normalizing the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to find all objects of both classes, whereby we will proceed to
    evaluate each of them to find the best match. Before that, let''s get all the
    classes with the index of `searchAClassIndex` and `searchBClassIndex`; add the
    following code, replacing the comment `// TODO Find matching pair`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are unable to find a matching pair, we continue to the next item, knowing
    that a cost has already been added for the mismatch in objects of both arrays.
    Next, we iterate over all pairs. For each pair, we calculate the normalized direction
    vector and then the dot product against our `searchDirection` vector, taking the
    one that has the closest dot product (closest in angle). Add the following code
    in place of the comment `// TODO Search for the most suitable pair`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Similar to what we did with our search pair, we calculate the direction vector
    by subtracting the pair's center positions and then normalize the result. Then,
    with the two vectors `searchDirection` and `detectedDirection`, we calculate the
    dot product, keeping reference to it if it is the first or lowest dot product
    so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is just one last thing we need to do for this method, and this project.
    But before doing so, let''s take a little detour and look at a couple of extensions
    made to `CGPoint`, specifically the `dot` and `normalize` used previously. You
    can find these extensions in the `CGPoint+Extension.swift` file. As I did previously,
    I will list the code for reference rather than describing the details, most of
    which we have already touched upon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, back to the `costForObjectRelativePositioning` method to finish our method
    and project. Our final task is to add to the cost; this is done simply by subtracting
    the stored `closestDotProduct` from `1.0` (remembering that we want to increase
    the cost for larger differences where the dot product of two normalized vectors
    pointing in the same direction is `1.0`) and ensuring that the value is positive
    by wrapping it in an `abs` function. Let''s do that now; add the following code,
    replacing the comment `// TODO add cost`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'With that done, we have finished this method, and the coding for this chapter.
    Well done! It''s time to test it out; build and run the project to see your hard
    work in action. Shown here are a few searches and their results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02d7b7ca-c6d5-44b5-86b0-a4b7242dc890.png)'
  prefs: []
  type: TYPE_IMG
- en: Although the YOLO algorithm is performant and feasible for near real-time use,
    our example is far from optimized and unlikely to perform well on large sets of
    photos. With the release of Core ML 2, Apple provides one avenue we can use to
    make our process more efficient. This will be the topic of the next section before
    wrapping up.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing with batches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the moment, our process involves iterating over each photo and performing
    inference on each one individually. With the release of Core ML 2, we now have
    the option to create a batch and pass this batch to our model for inference. As
    with efficiencies gained with economies of scale, here, we also gain significant
    improvements; so let's walk through adapting our project to process our photos
    in a single batch rather than individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s work our way up the stack, starting in our `YOLOFacade` class and moving
    up to the `PhotoSearcher`. For this we will be using our model directly rather
    than proxying through Vision, so our first task is to replace the `model` property
    of our `YOLOFacade` class with the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s rewrite the `detectObjects` method to handle an array of photos
    rather than a single instance; because this is where most of the changes reside,
    we will start from scratch. So, go ahead and delete the method from your `YOLOFacade`
    class and replace it with the following stub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: I have made the changes to the signature of the method boldand listed our remaining
    tasks. The first is to create an array of `MLFeatureProvider`. If you recall from
    [Chapter 3](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml), *Recognizing Objects
    in the World*, when we import a Core ML model into Xcode, it generates interfaces
    for the model, its input and output. The input and output are subclasses of `MLFeatureProvider`,
    so here we want to create an array of `tinyyolo_voc2007Input`, which can be instantiated
    with instances of `CVPixelBuffer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create this, we will transform the array of photos passed into the method,
    including the required preprocessing steps (resizing to 416 x 416). Replace the
    comment `// TODO batch items (array of MLFeatureProvider)` with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: For reasons of simplicity and readability, we are omitting any kind of error
    handling; obviously, in production you want to handle exceptions appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform inference on a batch, we need to have our input conform to the `MLBatchProvider`
    interface. Fortunately, Core ML provides a concrete implementation that conveniently
    wraps array. Let''s do this now; replace the comment `// TODO Wrap our items in
    an instance of MLArrayBatchProvider` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform inference, it''s simply a matter of calling the `predictions` method
    on our model; as usual, replace the comment `// TODO Perform inference on the
    batch` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'What we get back is an instance of `MLBatchProvider` (if successful); this
    is more or less a collection of results for each of our samples (inputs). We can
    access an specific result via the batch providers `features(at: Int)` method,
    which returns an instance of `MLFeatureProvider` (in our case, an in `tinyyolo_voc2007Output`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we simply process each result as we had done before to obtain the most
    salient; replace the comment `// TODO (As we did before) Process the outputs of
    the model` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The only difference here, than before, is that we are iterating over a batch
    of outputs rather than a single one. The last thing we need to do is call the
    handler; replace the comment `// TODO Return results via the callback handler`
    with the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This now completes the changes required to our `YOLOFacade` class; let's jump
    into the `PhotoSearcher` and make the necessary, and final, changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The big change here is that we now need to pass in all photos at once rather
    than passing each one individually. Locate the `detectObjects` method and replace
    its body with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Same code but organised a little differently to handle batch inputs and output
    from and to the `YOLOFacade` class. Now is a good time to build, deploy and run
    the application; paying particular attention to efficiencies gained from adapting
    batch inference. When you return; we will conclude this chapter with a quick summary.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the concept of object detection, comparing it
    with object recognition and object localization. While the other two are limited
    to a single dominant object, object detection allows multi-object classification,
    including predicting their bounding boxes. We then spent some time introducing
    one particular algorithm, YOLO, before getting acquainted with Apple's Core ML
    Tools Python package, walking through converting a trained Keras model to Core
    ML. Once we had the model in hand, we moved on to implementing YOLO in Swift with
    the goal of creating an intelligent search application.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this being a fairly lengthy chapter, I hope you found it valuable and
    gained deeper intuition into how deep neural networks learn and understand images
    and how they can be applied in novel ways to create new experiences. It's helpful
    to remind ourselves that using the same architecture, we can create devise new
    applications by simply swapping the data we train it on. For example, you could
    train this model on a dataset of hands and their corresponding bounding boxes
    to create a more immersive **augmented reality** (**AR**) experience by allowing
    the user to interact with digital content through touch.
  prefs: []
  type: TYPE_NORMAL
- en: But for now, let's continue our journey of understanding Core ML and explore
    how else we can apply it. In the next chapter, you will see how the popular Prisma
    creates those stunning photos with style transform.
  prefs: []
  type: TYPE_NORMAL
