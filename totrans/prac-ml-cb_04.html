<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Model Selection and Regularization"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Model Selection and Regularization</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Shrinkage methods - calories burned per day</li><li class="listitem" style="list-style-type: disc">Dimension reduction methods - Delta's Aircraft Fleet</li><li class="listitem" style="list-style-type: disc">Principal component analysis - understanding world cuisine</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec22"/>Introduction</h1></div></div></div><p>
<span class="strong"><strong>Subset selection</strong></span>: The use of labeled examples to induce a model that classifies objects into a finite set of known classes is one of the main challenges of supervised classification in machine learning. Vectors of numeric or nominal features are used to describe the various examples. In the feature subset selection problem, a learning algorithm is faced with the problem of selecting some subset of features upon which to focus its attention, while ignoring the rest.</p><p>When fitting a linear regression model, a subset of variables that best describe the data are of interest. There are a number of different ways the best subset, applying a number of different strategies, can be adopted when searching for a variables set. If there are <span class="emphasis"><em>m</em></span> variables and the best regression model consists of <span class="emphasis"><em>p</em></span> variables, <span class="emphasis"><em>p≤m</em></span>, then a more general approach to pick the best subset might be to try all possible combinations of <span class="emphasis"><em>p</em></span> variables and select the model that fits the data the best. </p><p>However, there are <span class="emphasis"><em>m! p!(m−p)! </em></span>possible combinations, which increases with the increase in the value of <span class="emphasis"><em>m</em></span>, for example, <span class="emphasis"><em>m = 20</em></span> and <span class="emphasis"><em>p = 4</em></span> gives 4,845 possible combinations. In addition, through the usage of fewer features, we may reduce the cost of acquiring the data and improve the comprehensibility of the classification model.</p><p>
<span class="strong"><strong>Shrinkage methods: </strong></span>Shrinkage regression refers to shrinkage methods of estimation or prediction in regression situations; useful when there is multi co-linearity among the regressors. In cases where the dataset is small compared to the number of co-variables studied, shrinkage techniques may improve predictions. The common shrinkage methods are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Linear shrinkage factor--shrinks all coefficients with the same factor</li><li class="listitem" style="list-style-type: disc">Ridge regression--penalized maximum likelihood, penalty factor is added to the likelihood function such that coefficients are shrunk individually according to the variance of each co-variable</li><li class="listitem" style="list-style-type: disc">Lasso--shrinks some coefficients to zero by setting a constraint on the sum of the absolute values of the coefficients of standardized co-variables</li></ul></div><p>Shrinkage methods retain a subset of the predictors, while discarding the rest. The subset selection produces a model that is interpretable and produces possibly lower prediction-errors than the full model, while not reducing the prediction error of the full model. Shrinkage methods more continuous and don't suffer as much from high variability. When there are many correlated variables in a linear regression model, their coefficients are poorly determined and exhibit high variance.</p><p>
<span class="strong"><strong>Dimension reduction methods: </strong></span>One of the significant challenges across a wide variety of information-processing fields, including pattern recognition, data compression, machine learning, and database navigation, is manifold learning. The measured data vectors are high-dimensional and, in many cases, the data lies near a lower-dimensional manifold. The main challenges of high-dimensional data are that it is multiple; it indirectly measures the underlying source, which typically cannot be directly measured. Dimensionality reduction may also be seen as the process of deriving a set of degrees of freedom, which can be used to reproduce most of the variability of a dataset.</p></div></div>
<div class="section" title="Shrinkage methods - calories burned per day"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec23"/>Shrinkage methods - calories burned per day</h1></div></div></div><p>In order to compare the metabolic rate of humans, the concept of <span class="strong"><strong>basal metabolic rate</strong></span> (<span class="strong"><strong>BMR</strong></span>) is critical, in a clinical context, as a means of determining thyroid status in humans. The BMR of mammals varies with body mass, with the same allometric exponent as field metabolic rate, and with many physiological and biochemical rates. Fitbit, as a device, uses BMR and activities performed during the day to estimate calories burned throughout the day.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec21"/>Getting ready</h2></div></div></div><p>In order to perform shrinkage methods, we shall be using a dataset collected from Fitbit and a calories-burned dataset.</p><div class="section" title="Step 1 - collecting and describing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec54"/>Step 1 - collecting and describing data</h3></div></div></div><p>The dataset titled <code class="literal">fitbit_export_20160806.csv</code> which is in CSV format shall be used. The dataset is in standard format. There are 30 rows of data and 10 variables. The numeric variables are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Calories Burned</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Steps</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Distance</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Floors</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Minutes Sedentary</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Minutes Lightly Active</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Minutes Fairly Active</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ExAng</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Minutes Very Active</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Activity Calories</code></li></ul></div><p>The non-numeric variables are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Date</code></li></ul></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec22"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec55"/>Step 2 - exploring data</h3></div></div></div><p>As the first step, the following packages need to be loaded:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; install.packages("glmnet")</strong></span>
<span class="strong"><strong>    &gt; install.packages("dplyr")</strong></span>
<span class="strong"><strong>    &gt; install.packages("tidyr")</strong></span>
<span class="strong"><strong>    &gt; install.packages("ggplot2")</strong></span>
<span class="strong"><strong>    &gt; install.packages("caret")</strong></span>
<span class="strong"><strong>    &gt; install.packages("boot")</strong></span>
<span class="strong"><strong>    &gt; install.packages("RColorBrewer")</strong></span>
<span class="strong"><strong>    &gt; install.packages("Metrics")</strong></span>
<span class="strong"><strong>    &gt; library(dplyr)</strong></span>
<span class="strong"><strong>    &gt; library(tidyr)</strong></span>
<span class="strong"><strong>    &gt; library(ggplot2)</strong></span>
<span class="strong"><strong>    &gt; library(caret)</strong></span>
<span class="strong"><strong>    &gt; library(glmnet)</strong></span>
<span class="strong"><strong>    &gt; library(boot)</strong></span>
<span class="strong"><strong>    &gt; library(RColorBrewer)</strong></span>
<span class="strong"><strong>    &gt; library(Metrics)</strong></span>
</pre><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>Version info: Code for this page was tested in R version 3.3.0 (2016-05-03)</p></div></div><p>Let's explore the data and understand the relationships among the variables. We'll begin by importing the csv data file named <code class="literal">fitbit_export_20160806.csv</code>. We will be saving the data to the <code class="literal">fitbit_details</code> frame:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; fitbit_details &lt;- read.csv("https://raw.githubusercontent.com/ellisp/ellisp.github.io/source/data/fitbit_export_20160806.csv", </strong></span>
<span class="strong"><strong>    + skip = 1, stringsAsFactors = FALSE) %&gt;%</strong></span>
<span class="strong"><strong>    + mutate(</strong></span>
<span class="strong"><strong>    + Calories.Burned = as.numeric(gsub(",", "", Calories.Burned)),</strong></span>
<span class="strong"><strong>    + Steps = as.numeric(gsub(",", "", Steps)),</strong></span>
<span class="strong"><strong>    + Activity.Calories = as.numeric(gsub(",", "", Activity.Calories)),</strong></span>
<span class="strong"><strong>    + Date = as.Date(Date, format = "%d/%m/%Y")</strong></span>
<span class="strong"><strong>    + )</strong></span>
</pre><p>Storing the <code class="literal">fitbit_details</code> data frame to the <code class="literal">fitbit</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; fitbit &lt;- fitbit_details</strong></span>
</pre><p>Printing data frame <code class="literal">fitbit</code>. The <code class="literal">head()</code> function returns the first part of the <code class="literal">fitbit</code> data frame. The <code class="literal">fitbit</code> data frame is passed as an input parameter:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; head(fitbit)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_001.jpg" alt="Step 2 - exploring data"/></div><p>
</p><p>Setting <code class="literal">Activity.Calories</code> and <code class="literal">Date</code> values as NULL:</p><pre class="programlisting">
<span class="strong"><strong>&gt; fitbit$Activity.Calories &lt;- NULL</strong></span>
</pre><pre class="programlisting">
<span class="strong"><strong>&gt; fitbit$Date &lt;- NULL</strong></span>
</pre><p>Scaling coefficients to calories per thousand steps. The result is then set to the <code class="literal">fitbit$Steps</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; fitbit$Steps &lt;- fitbit$Steps / 1000</strong></span>
</pre><p>Printing the <code class="literal">fitbit$Steps</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; fitbit$Steps</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_002.jpg" alt="Step 2 - exploring data"/></div><p>
</p><p>Exploring all the candidate variables. Function for calculating correlation coefficients:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; panel_correlations &lt;- function(x, y, digits = 2, prefix = "", cex.cor, ...){</strong></span>
<span class="strong"><strong>    # combining multiple plots into one overall graph</strong></span>
<span class="strong"><strong>    + usr &lt;- par("usr")</strong></span>
<span class="strong"><strong>    + on.exit(par(usr))</strong></span>
<span class="strong"><strong>    + par(usr = c(0, 1, 0, 1))</strong></span>
<span class="strong"><strong>    # computing the absolute value</strong></span>
<span class="strong"><strong>    + r &lt;- abs(cor(x, y))</strong></span>
<span class="strong"><strong>    # Formatting object </strong></span>
<span class="strong"><strong>    + txt &lt;- format(c(r, 0.123456789), digits = digits)[1]</strong></span>
<span class="strong"><strong>    + txt &lt;- paste0(prefix, txt)</strong></span>
<span class="strong"><strong>    + if(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)</strong></span>
<span class="strong"><strong>    + text(0.5, 0.5, txt, cex = cex.cor * r)</strong></span>
<span class="strong"><strong>    + }</strong></span>
</pre><p>Producing a matrix of scatterplots. The 
<code class="literal">pairs()</code>function produces the scatter plots in matrix form. <code class="literal">fitbit</code> is the dataset for scatter plots. Distance can be calculated almost exactly directly from <code class="literal">Steps</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pairs(fitbit[ , -1], lower.panel = panel_correlations, main = "Pairwise Relationship - Fitbit's Measured Activities")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_003.jpg" alt="Step 2 - exploring data"/></div><p>
</p><p>Printing <code class="literal">fitbit data frame</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; ggplot(fitbit, aes(x = Distance / Steps)) + geom_rug() + geom_density() +ggtitle("Stride Length Reverse- Engineered from Fitbit Data", subtitle = "Not all strides identical, due to rounding or other jitter")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_004.jpg" alt="Step 2 - exploring data"/></div><p>
</p></div><div class="section" title="Step 3 - building the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec56"/>Step 3 - building the model</h3></div></div></div><p>Building ordinary least squares estimation with Steps as the sole explanatory variable and <code class="literal">Calories.Burned</code> as the response variable. <code class="literal">lm()</code> as a function is used to fit linear models. <code class="literal">Calories.Burned ~ Steps</code> is the formula, while <code class="literal">fitbit</code> is the data frame. The result is then stored in the moderate data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; moderate &lt;- lm(Calories.Burned ~ Steps, data = fitbit)</strong></span>
</pre><p>Printing the <code class="literal">moderate</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; moderate</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_005.jpg" alt="Step 3 - building the model"/></div><p>
</p><p>Rounding off the values of the <code class="literal">moderate</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; round(coef(moderate))</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_006.jpg" alt="Step 3 - building the model"/></div><p>
</p><p>Plotting the predicted calories with the residuals from the model used. The <code class="literal">plot()</code> function is a generic function for plotting R objects. The <code class="literal">moderate</code> data frame is passed as a function value. The <code class="literal">bty</code> parameter determines the type of box drawn about plots:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(moderate, which = 1, bty = "l", main = "Predicted Calories compared with Residuals")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_007.jpg" alt="Step 3 - building the model"/></div><p>
</p><p>Checking the partial autocorrelation function of residuals. <code class="literal">pacf()</code> is used for partial autocorrelations. <code class="literal">resid()</code> as a function computes the difference between the observed data of the dependent variable. <code class="literal">moderate</code> is passed as a data frame to the <code class="literal">resid()</code> function, to compute the difference between the observed data of the dependent variable:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pacf(resid(moderate), main = "Partial Autocorrelation of residuals from single variable regression")</strong></span>
</pre><p>The <code class="literal">grid()</code> function adds the grids to the plotted data:</p><pre class="programlisting">
<span class="strong"><strong>&gt; grid()</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_008.jpg" alt="Step 3 - building the model"/></div><p>
</p></div><div class="section" title="Step 4 - improving the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec57"/>Step 4 - improving the model</h3></div></div></div><p>Predicting daily calories based on all seven explanatory variables. Fitting the model to multiple samples at different values of alpha, using the fit model to predict the out of bag points from the original sample that weren't in the re-sample. It is about creating the balance between the extremes of ridge regression and lasso estimation by choosing an appropriate value of alpha.</p><p>Creating the matrix X by standardizing. The <code class="literal">as.matrix()</code> function turns <code class="literal">fitbit[ , -1]</code> that is, apart from date column into matrix:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; X &lt;- as.matrix(fitbit[ , -1])</strong></span>
</pre><p>Printing the <code class="literal">X</code> data frame. The <code class="literal">head()</code> function returns the first part of the <code class="literal">X</code> data frame. The <code class="literal">X</code> data frame is passed as an input parameter:</p><pre class="programlisting">
<span class="strong"><strong>&gt; head(X)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_009.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Creating the vector <code class="literal">Y</code> by standardizing:</p><pre class="programlisting">
<span class="strong"><strong>&gt; Y &lt;- fitbit$Calories.Burned</strong></span>
</pre><p>Printing the <code class="literal">Y</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; Y</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_010.jpg" alt="Step 4 - improving the model"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(123)</strong></span>
</pre><p>Generating regular sequences:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; alphas &lt;- seq(from = 0, to  = 1, length.out = 10)</strong></span>
<span class="strong"><strong>    &gt; res &lt;- matrix(0, nrow = length(alphas), ncol = 6)</strong></span>
</pre><p>Creating five repeats of each CV run:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; for(i in 1:length(alphas)){</strong></span>
<span class="strong"><strong>    + for(j in 2:6){</strong></span>
<span class="strong"><strong>    # k-fold cross-validation for glmnet</strong></span>
<span class="strong"><strong>    + cvmod &lt;- cv.glmnet(X, Y, alpha = alphas[i])</strong></span>
<span class="strong"><strong>    + res[i, c(1, j)] &lt;- c(alphas[i], sqrt(min(cvmod$cvm)))</strong></span>
<span class="strong"><strong>    + }</strong></span>
<span class="strong"><strong>    + }</strong></span>
</pre><p>Creating the dataset to be used. The <code class="literal">data.frame()</code> function is used to create data frames based on a tightly coupled set of variables. These variables share the properties of matrices:</p><pre class="programlisting">
<span class="strong"><strong>&gt; res &lt;- data.frame(res)</strong></span>
</pre><p>Printing the <code class="literal">res</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; res</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_011.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Creating a vector of <code class="literal">average_rmse</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; res$average_rmse &lt;- apply(res[ , 2:6], 1, mean)</strong></span>
</pre><p>Printing the <code class="literal">res$average_rmse</code> vector:</p><pre class="programlisting">
<span class="strong"><strong>&gt; res$average_rmse</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_012.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Arranging the <code class="literal">res$average_rmse</code> in ascending order. The result is then stored in the <code class="literal">res</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; res &lt;- res[order(res$average_rmse), ]</strong></span>
</pre><p>Printing the <code class="literal">res</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; res</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_013.jpg" alt="Step 4 - improving the model"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>    &gt; names(res)[1] &lt;- "alpha"</strong></span>
<span class="strong"><strong>    &gt; res %&gt;%</strong></span>
<span class="strong"><strong>    + select(-average_rmse) %&gt;%</strong></span>
<span class="strong"><strong>    + gather(trial, rmse, -alpha) %&gt;%</strong></span>
<span class="strong"><strong>    + ggplot(aes(x = alpha, y = rmse)) +</strong></span>
<span class="strong"><strong>    + geom_point() +</strong></span>
<span class="strong"><strong>    + geom_smooth(se = FALSE) +</strong></span>
<span class="strong"><strong>    + labs(y = "Root Mean Square Error") +</strong></span>
<span class="strong"><strong>    + ggtitle("Cross Validation best RMSE for differing values of alpha")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_014.jpg" alt="Step 4 - improving the model"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; bestalpha &lt;- res[1, 1]</strong></span>
</pre><p>Printing the  <code class="literal">bestalpha</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; bestalpha</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_04_015.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Comparing ordinary least squares equivalents with the estimated values of the eight coefficients (seven explanatory variables plus an intercept) by using elastic net.</p><p>Determining lambda at best value of alpha. Computing k-fold cross-validation for <code class="literal">glmnet</code> by calling the <code class="literal">cv.glmnet()</code> function:</p><pre class="programlisting">
<span class="strong"><strong>&gt; crossvalidated &lt;- cv.glmnet(X, Y, alpha = bestalpha)</strong></span>
</pre><p>Creating the model. <code class="literal">glmnet()</code> fits a generalized linear model via penalized maximum likelihood. The regularization path is computed for the lasso or <code class="literal">elasticnet</code> penalty at a grid of values for the regularization parameter lambda. <code class="literal">X</code> is the input matrix, while <code class="literal">Y</code> is the response variable. <code class="literal">alpha</code> is the <code class="literal">elasticnet</code> mixing parameter, with 0 ≤ α ≤ 1:</p><pre class="programlisting">
<span class="strong"><strong>&gt; moderate1 &lt;- glmnet(X, Y, alpha = bestalpha)</strong></span>
</pre><p>Building an ordinary least squares estimation, with <code class="literal">fitbit</code> as the sole explanatory variable and <code class="literal">Calories.Burned</code> as the response variable. <code class="literal">lm()</code> as a function is used to fit linear models. <code class="literal">Calories.Burned ~ Steps</code> is the formula, while <code class="literal">fitbit</code> is the data frame. The result is then stored in the <code class="literal">OLSmodel</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; OLSmodel &lt;- lm(Calories.Burned ~ ., data = fitbit)</strong></span>
</pre><p>Printing the <code class="literal">OLSmodel</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; OLSmodel</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_016.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Comparing ordinary least squares equivalents with the estimated values of the eight coefficients (seven explanatory variables plus an intercept). The result is then stored in the <code class="literal">coeffs</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; coeffs &lt;- data.frame(original = coef(OLSmodel), </strong></span>
<span class="strong"><strong>       + shrunk = as.vector(coef(moderate1, s = crossvalidated$lambda.min)),</strong></span>
<span class="strong"><strong>       + very.shrunk = as.vector(coef(moderate1, s = crossvalidated$lambda.1se)))</strong></span>
</pre><p>Printing the <code class="literal">coeffs</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; coeffs</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_017.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Rounding off the values of the <code class="literal">moderate</code> data frame to three significant digits:</p><pre class="programlisting">
<span class="strong"><strong>&gt; round(coeffs, 3)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_018.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Creating the model. <code class="literal">glmnet()</code> fits a generalized linear model via a penalized maximum likelihood:</p><pre class="programlisting">
<span class="strong"><strong>&gt; moderate2 &lt;- glmnet(X, Y, lambda = 0)</strong></span>
</pre><p>Printing the <code class="literal">moderate2</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; moderate2</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_019.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Rounding off the values to three significant digits:</p><pre class="programlisting">
<span class="strong"><strong>&gt; round(data.frame("elastic, lambda = 0" = as.vector(coef(moderate2)), "lm" = coef(OLSmodel), check.names = FALSE), 3)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_020.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Creating the model. <code class="literal">glmnet()</code>fits a generalized linear model via a penalized maximum likelihood after eliminating the distance column:</p><pre class="programlisting">
<span class="strong"><strong>&gt; moderate3 &lt;- glmnet(X[ , -2], Y, lambda = 0)</strong></span>
</pre><p>Printing the <code class="literal">moderate3</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; moderate3</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_021.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Building ordinary least squares estimation <code class="literal">Y ~ X[ , -2]</code> is the formula. The result is then stored in the <code class="literal">moderate4</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; moderate4 &lt;- lm(Y ~ X[ , -2])</strong></span>
</pre><p>Printing the <code class="literal">moderate4</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; moderate4</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_022.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Rounding off the values to three significant digits:</p><pre class="programlisting">
<span class="strong"><strong>&gt; round(data.frame("elastic, lambda = 0" = as.vector(coef(moderate3)), "lm" = coef(moderate4), check.names = FALSE), 3)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_023.jpg" alt="Step 4 - improving the model"/></div><p>
</p></div><div class="section" title="Step 5 - comparing the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec58"/>Step 5 - comparing the model</h3></div></div></div><p>Comparing the predictive strength of different models by using bootstrapping, where the modeling approach is applied to bootstrap re-samples of the data. The estimate model is then used to predict the full, original dataset.</p><p>Function to feed to boot that does elastic modeling:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; modellingfucn1 &lt;- function(data, i){</strong></span>
<span class="strong"><strong>    + X &lt;- as.matrix(data[i , -1])</strong></span>
<span class="strong"><strong>    + Y &lt;- data[i , 1]</strong></span>
<span class="strong"><strong>    # k-fold cross-validation for glmnet</strong></span>
<span class="strong"><strong>    + crossvalidated &lt;- cv.glmnet(X, Y, alpha = 1, nfolds = 30)</strong></span>
<span class="strong"><strong>    # Fitting a generalized linear model via penalized maximum likelihood</strong></span>
<span class="strong"><strong>    + moderate1 &lt;- glmnet(X, Y, alpha = 1)</strong></span>
<span class="strong"><strong>    # Computing the root mean squared error</strong></span>
<span class="strong"><strong>    + rmse(predict(moderate1, newx = as.matrix(data[ , -1]), s =     crossvalidated$lambda.min), data[ , 1])</strong></span>
<span class="strong"><strong>    + }</strong></span>
</pre><p>Generating an R bootstrap replica of a statistic applied to data. <code class="literal">fitbit</code> is the dataset, <code class="literal">statistic = modellingfucn1</code> is the function, which, when applied to <code class="literal">fitbit</code>, returns a vector containing the statistics of interest. <code class="literal">R = 99</code>  indicates the number of bootstrap replicates:</p><pre class="programlisting">
<span class="strong"><strong>&gt; elastic_boot &lt;- boot(fitbit, statistic = modellingfucn1, R = 99)</strong></span>
</pre><p>Printing the <code class="literal">elastic_boot</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; elastic_boot</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_024.jpg" alt="Step 5 - comparing the model"/></div><p>
</p><p>Function to feed to boot that does OLS modeling:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; modellingOLS &lt;- function(data, i){</strong></span>
<span class="strong"><strong>    + mod0 &lt;- lm(Calories.Burned ~ Steps, data = data[i, ])</strong></span>
<span class="strong"><strong>    + rmse(predict(moderate, newdata = data), data[ , 1])</strong></span>
<span class="strong"><strong>    + }</strong></span>
</pre><p>Generating an R bootstrap replica of a statistic applied to data. <code class="literal">fitbit</code> is the dataset, <code class="literal">statistic = modellingOLS</code> is the function, which, when applied to <code class="literal">fitbit</code>, returns a vector containing the statistics of interest. <code class="literal">R = 99</code>  indicates the number of bootstrap replicates:</p><pre class="programlisting">
<span class="strong"><strong>&gt; lmOLS_boot &lt;- boot(fitbit, statistic = modellingOLS, R = 99)</strong></span>
</pre><p>Printing the <code class="literal">lmOLS_boot</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; lmOLS_boot</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_025.jpg" alt="Step 5 - comparing the model"/></div><p>
</p><p>Generating an R bootstrap replica of a statistic applied to data. <code class="literal">fitbit</code> is the dataset, <code class="literal">statistic = modellingfucn2</code> is the function, which, when applied to <code class="literal">fitbit</code>, returns a vector containing the statistics of interest. <code class="literal">R = 99</code>  indicates the number of bootstrap replicates:</p><pre class="programlisting">
<span class="strong"><strong>&gt; lm_boot &lt;- boot(fitbit, statistic = modellingfucn2, R = 99)</strong></span>
</pre><p>Printing the <code class="literal">lm_boot</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; lm_boot</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_026.jpg" alt="Step 5 - comparing the model"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>    &gt; round(c("elastic modelling" = mean(elastic_boot$t), </strong></span>
<span class="strong"><strong>      + "OLS modelling" = mean(lm_boot$t),</strong></span>
<span class="strong"><strong>      + "OLS modelling, only one explanatory variable" = mean(lmOLS_boot$t)), 1)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_027.jpg" alt="Step 5 - comparing the model"/></div><p>
</p><p>Refitting the model with scaled variables.</p><p>Creating the model. <code class="literal">glmnet()</code> fits a generalized linear model via a penalized maximum likelihood.</p><pre class="programlisting">
<span class="strong"><strong>    &gt; ordering &lt;- c(7,5,6,2,1,3,4)</strong></span>
<span class="strong"><strong>    &gt; par(mar = c(5.1, 4.1, 6.5, 1), bg = "grey90")</strong></span>
<span class="strong"><strong>    &gt; model_scaled &lt;- glmnet(scale(X), Y, alpha = bestalpha)</strong></span>
<span class="strong"><strong>    &gt; the_palette &lt;- brewer.pal(7, "Set1")</strong></span>
<span class="strong"><strong>    &gt; plot(model_scaled, xvar = "dev", label = TRUE, col = the_pallete, lwd = 2, main = "Increasing contribution of different explanatory variablesnas penalty for including them is relaxed")</strong></span>
<span class="strong"><strong>    &gt; legend("topleft", legend = colnames(X)[ordering], text.col = the_palette[ordering], lwd = 2, bty = "n", col = the_palette[ordering])</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_028.jpg" alt="Step 5 - comparing the model"/></div><p>
</p></div></div></div>
<div class="section" title="Dimension reduction methods - Delta's Aircraft Fleet"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec24"/>Dimension reduction methods - Delta's Aircraft Fleet</h1></div></div></div><p>Fleet planning is a part of the strategic planning process for any airline company. Fleet is the total number of aircraft that an airline operates, as well as the specific aircraft types that comprise the total fleet. Airline selection criteria for aircraft acquisition are based on technical/performance characteristics, economic and financial impact, environmental regulations and constraints, marketing considerations, and political realities. Fleet composition is a critical long-term strategic decision for an airline company. Each aircraft type has different technical performance characteristics, for example, the capacity to carry the payload over a maximum flight distance or range. It affects financial position, operating costs, and especially the ability to serve specific routes.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec23"/>Getting ready</h2></div></div></div><p>In order to perform dimension reduction we shall be using a dataset collected on Delta Airlines Aircraft Fleet.</p><div class="section" title="Step 1 - collecting and describing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec59"/>Step 1 - collecting and describing data</h3></div></div></div><p>The dataset titled <code class="literal">delta.csv</code> shall be used. The dataset is in standard format. There are 44 rows of data and 34 variables.</p></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec24"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec60"/>Step 2 - exploring data</h3></div></div></div><p>The first step is to load the following packages:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; install.packages("rgl")</strong></span>
<span class="strong"><strong>    &gt; install.packages("RColorBrewer")</strong></span>
<span class="strong"><strong>    &gt; install.packages("scales")</strong></span>
<span class="strong"><strong>    &gt; library(rgl)</strong></span>
<span class="strong"><strong>    &gt; library(RColorBrewer)</strong></span>
<span class="strong"><strong>    &gt; library(scales)</strong></span>
</pre><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>Version info: Code for this page was tested in R version 3.3.2 (2016-10-31)</p></div></div><p>Let's explore the data and understand the relationships among the variables. We'll begin by importing the csv data file named <code class="literal">delta.csv</code>. We will be saving the data to the delta frame:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; delta &lt;- read.csv(file="d:/delta.csv", header=T, sep=",", row.names=1)</strong></span>
</pre><p>Exploring the internal structure of the <code class="literal">delta</code> data frame. The <code class="literal">str()</code> function displays the internal structure of the data frame. The details passed as an R object to the <code class="literal">str()</code> function:</p><pre class="programlisting">
<span class="strong"><strong>&gt; str(delta)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_029.jpg" alt="Step 2 - exploring data"/></div><p>
</p><p>Exploring the intermediary quantitative variables related to the aircraft's physical characteristics: Accommodation, Cruising Speed, Range, Engines, Wing Span, Tail Height, and <code class="literal">Length.Scatter</code> plot matrix. The <code class="literal">plot()</code> function is a generic function for plotting Robjects. The <code class="literal">delta[,16:22]</code> data frame is passed as a function value:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(delta[,16:22], main = "Aircraft Physical Characteristics", col = "red")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_030.jpg" alt="Step 2 - exploring data"/></div><p>
</p><p>There is a positive correlation between all these variables as all of them are related to the aircraft's overall size.</p></div><div class="section" title="Step 3 - applying principal components analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec61"/>Step 3 - applying principal components analysis</h3></div></div></div><p>Visualizing a high-dimensional dataset, such as the number of engines. Applying principle components analysis to data. The <code class="literal">princomp()</code> function performs principal components analysis on the <code class="literal">delta</code> datamatrix. The result is <code class="literal">principal_comp_analysis</code>, which is an object of class <code class="literal">princomp</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; principal_comp_analysis &lt;- princomp(delta)</strong></span>
</pre><p>Printing the <code class="literal">principal_comp_analysis</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; principal_comp_analysis</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_031.jpg" alt="Step 3 - applying principal components analysis"/></div><p>
</p><p>Plotting <code class="literal">principal_comp_analysis</code> data:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(principal_comp_analysis, main ="Principal Components Analysis of Raw Data", col ="blue")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_032.jpg" alt="Step 3 - applying principal components analysis"/></div><p>
</p><p>It is demonstrable that the first principal component has a standard deviation, which accounts for over 99.8% of the variance in the data.</p><p>Printing loadings of principal components analysis. The <code class="literal">loadings()</code> function uses the <code class="literal">principal_comp_analysis</code> principal components-analysis data object as input:</p><pre class="programlisting">
<span class="strong"><strong>&gt; loadings(principal_comp_analysis)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_033.jpg" alt="Step 3 - applying principal components analysis"/></div><p>
</p><p>Looking at the first column of loadings, it is clear that the first principle component is just the range, in miles. The scale of each variable in the dataset is different.</p><p>Plotting variance on regular scaling. <code class="literal">barplot()</code> plots both vertical and horizontal bars. <code class="literal">sapply()</code> is a wrapper function that returns a list of the same length as <code class="literal">delta.horiz=T</code> signifies a logical value that the bars are to be drawn horizontally, with the first at the bottom:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; mar &lt;- par()$mar</strong></span>
<span class="strong"><strong>    &gt; par(mar=mar+c(0,5,0,0))</strong></span>
<span class="strong"><strong>    &gt; barplot(sapply(delta, var), horiz=T, las=1, cex.names=0.8, main = "Regular Scaling of Variance", col = "Red", xlab = "Variance")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_034.jpg" alt="Step 3 - applying principal components analysis"/></div><p>
</p><p>Plotting variance on a logarithmic scale. <code class="literal">barplot()</code> plots both vertical and horizontal bars:</p><pre class="programlisting">
<span class="strong"><strong>&gt; barplot(sapply(delta, var), horiz=T, las=1, cex.names=0.8, log='x', main = "Logarithmic  Scaling of Variance", col = "Blue", xlab = "Variance")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_035.jpg" alt="Step 3 - applying principal components analysis"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; par(mar=mar)</strong></span>
</pre></div><div class="section" title="Step 4 - scaling the data"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec62"/>Step 4 - scaling the data</h3></div></div></div><p>The scaling of <code class="literal">delta</code> data is useful under certain circumstances, since the variables span different ranges. <code class="literal">scale()</code>as a function centers and/or scales the columns of the <code class="literal">delta</code> matrix. The result is then stored in the <code class="literal">delta2</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; delta2 &lt;- data.frame(scale(delta))</strong></span>
</pre><p>Verifying whether the variance is uniform:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(sapply(delta2, var), main = "Variances Across Different Variables", ylab = "Variances")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_036.jpg" alt="Step 4 - scaling the data"/></div><p>
</p><p>The variance is now constant across variables.</p><p>Applying principal components to the scaled data <code class="literal">delta2</code>. The <code class="literal">princomp()</code> function performs principal components analysis on the <code class="literal">delta2</code> datamatrix. The result is <code class="literal">principal_comp_analysis</code>, which is an object of class <code class="literal">princomp</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; principal_comp_analysis &lt;- princomp(delta2)</strong></span>
</pre><p>Plotting the <code class="literal">principal_comp_analysis</code> object:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(principal_comp_analysis, main ="Principal Components Analysis of Scaled Data", col ="red")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_037.jpg" alt="Step 4 - scaling the data"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(principal_comp_analysis, type='l', main ="Principal Components Analysis of Scaled Data")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_038.jpg" alt="Step 4 - scaling the data"/></div><p>
</p><p>The <code class="literal">summary()</code> function is used to produce summaries of the results of various model-fitting functions:</p><pre class="programlisting">
<span class="strong"><strong>&gt; summary(principal_comp_analysis)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_039.jpg" alt="Step 4 - scaling the data"/></div><p>
</p><p>Applying principal components to the scaled data <code class="literal">delta2</code>. The <code class="literal">prcomp()</code> function performs principal components analysis on the <code class="literal">delta2</code> datamatrix. The result is <code class="literal">principal_comp_analysis</code>, which is an object of class <code class="literal">prcomp</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; principal_comp_vectors &lt;- prcomp(delta2)</strong></span>
</pre><p>Creating a data frame of <code class="literal">principal_comp_vectors</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; comp &lt;- data.frame(principal_comp_vectors$x[,1:4])</strong></span>
</pre><p>Applying k means with <code class="literal">k = 4</code>. The <code class="literal">kmeans()</code>function performs k-means clustering on comp. <code class="literal">nstart=25</code> signifies the number of random sets to be chosen. <code class="literal">iter.max=1000</code> is the maximum number of iterations allowed:</p><pre class="programlisting">
<span class="strong"><strong>&gt; k_means &lt;- kmeans(comp, 4, nstart=25, iter.max=1000)</strong></span>
</pre><p>Creating a vector of nine contiguous colors:</p><pre class="programlisting">
<span class="strong"><strong>&gt; palette(alpha(brewer.pal(9,'Set1'), 0.5))</strong></span>
</pre><p>Plotting comp:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(comp, col=k_means$clust, pch=16)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_040.jpg" alt="Step 4 - scaling the data"/></div><p>
</p></div><div class="section" title="Step 5 - visualizing in 3D plot"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec63"/>Step 5 - visualizing in 3D plot</h3></div></div></div><p>Plotting in 3D <code class="literal">comp$PC1</code>, <code class="literal">comp$PC2</code>, <code class="literal">comp$PC3</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot3d(comp$PC1, comp$PC2, comp$PC3, col=k_means$clust)    </strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_041.jpg" alt="Step 5 - visualizing in 3D plot"/></div><p>
</p><p>Plotting in 3D <code class="literal">comp$PC1</code>, <code class="literal">comp$PC3</code>, <code class="literal">comp$PC4</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot3d(comp$PC1, comp$PC3, comp$PC4, col=k_means$clust)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_042.jpg" alt="Step 5 - visualizing in 3D plot"/></div><p>
</p><p>Examining the clusters in order of increasing size:</p><pre class="programlisting">
<span class="strong"><strong>&gt; sort(table(k_means$clust))</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_043.jpg" alt="Step 5 - visualizing in 3D plot"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; clust &lt;- names(sort(table(k_means$clust)))</strong></span>
</pre><p>Names as displayed in the first cluster:</p><pre class="programlisting">
<span class="strong"><strong>&gt; row.names(delta[k_means$clust==clust[1],])</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_044.jpg" alt="Step 5 - visualizing in 3D plot"/></div><p>
</p><p>Names as displayed in the second cluster:</p><pre class="programlisting">
<span class="strong"><strong>&gt; row.names(delta[k_means$clust==clust[2],])</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_045.jpg" alt="Step 5 - visualizing in 3D plot"/></div><p>
</p><p>Names as displayed in the third cluster:</p><pre class="programlisting">
<span class="strong"><strong>&gt; row.names(delta[k_means$clust==clust[3],])</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_046.jpg" alt="Step 5 - visualizing in 3D plot"/></div><p>
</p><p>Names as displayed in the fourth cluster:</p><pre class="programlisting">
<span class="strong"><strong>&gt; row.names(delta[k_means$clust==clust[4],])</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_047.jpg" alt="Step 5 - visualizing in 3D plot"/></div><p>
</p></div></div></div>
<div class="section" title="Principal component analysis - understanding world cuisine"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec25"/>Principal component analysis - understanding world cuisine</h1></div></div></div><p>Food is a powerful symbol of who we are. There are many types of food identification, such as ethnic, religious, and class identifications. Ethnic food preferences become identity markers in the presence of gustatory foreigners, such as when one goes abroad, or when those foreigners visit the home shores.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec25"/>Getting ready</h2></div></div></div><p>In order to perform principal component analysis, we shall be using a dataset collected on the Epicurious recipe dataset.</p><div class="section" title="Step 1 - collecting and describing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec64"/>Step 1 - collecting and describing data</h3></div></div></div><p>The dataset titled <code class="literal">epic_recipes.txt</code> shall be used. The dataset is in standard format.</p></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec26"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec65"/>Step 2 - exploring data</h3></div></div></div><p>The first step is to load the following packages:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; install.packages("glmnet") </strong></span>
<span class="strong"><strong>    &gt; library(ggplot2)</strong></span>
<span class="strong"><strong>    &gt; library(glmnet)</strong></span>
</pre><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>Version info: Code for this page was tested in R version 3.3.2 (2016-10-31)</p></div></div><p>Let's explore the data and understand the relationships among the variables. We'll begin by importing the TXT data file named <code class="literal">epic_recipes.txt</code>. We will be saving the data to the <code class="literal">datafile</code> frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; datafile &lt;- file.path("d:","epic_recipes.txt")</strong></span>
</pre><p>Reading a file in table format and creating a data frame from it. <code class="literal">datafile</code> is the file name, which is passed as an input:</p><pre class="programlisting">
<span class="strong"><strong>&gt; recipes_data &lt;- read.table(datafile, fill=TRUE, col.names=1:max(count.fields(datafile)), na.strings=c("", "NA"), stringsAsFactors = FALSE)</strong></span>
</pre></div><div class="section" title="Step 3 - preparing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec66"/>Step 3 - preparing data</h3></div></div></div><p>Splitting the data into subsets. <code class="literal">aggregate()</code> splits <code class="literal">recipes_data[,-1]</code> and computes summary statistics. <code class="literal">recipes_data[,-1]</code>list of grouping elements, each as long as the variables in the data frame. The result is then stored in the <code class="literal">agg</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; agg &lt;- aggregate(recipes_data[,-1], by=list(recipes_data[,1]), paste, collapse=",")</strong></span>
</pre><p>Creating a vector, array, or list of values:</p><pre class="programlisting">
<span class="strong"><strong>&gt; agg$combined &lt;- apply(agg[,2:ncol(agg)], 1, paste, collapse=",")</strong></span>
</pre><p>Replacing all occurrence of patterns. <code class="literal">gsub()</code> as a function replaces each <code class="literal">,NA</code> with <code class="literal">""</code> after searching <code class="literal">agg$combined</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; agg$combined &lt;- gsub(",NA","",agg$combined)</strong></span>
</pre><p>Extracting the names of all cuisines:</p><pre class="programlisting">
<span class="strong"><strong>&gt; cuisines &lt;- as.data.frame(table(recipes_data[,1]))</strong></span>
</pre><p>Printing the cuisines data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; cuisines</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_048.jpg" alt="Step 3 - preparing data"/></div><p>
</p><p>Extracting the frequency of ingredients:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; ingredients_freq &lt;- lapply(lapply(strsplit(a$combined,","), table), as.data.frame) </strong></span>
<span class="strong"><strong>    &gt; names(ingredients_freq) &lt;- agg[,1]</strong></span>
</pre><p>Normalizing the frequency of ingredients:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; proportion &lt;- lapply(seq_along(ingredients_freq), function(i) {</strong></span>
<span class="strong"><strong>    + colnames(ingredients_freq[[i]])[2] &lt;- names(ingredients_freq)[i]</strong></span>
<span class="strong"><strong>    + ingredients_freq[[i]][,2] &lt;- ingredients_freq[[i]][,2]/cuisines[i,2] </strong></span>
<span class="strong"><strong>    + ingredients_freq[[i]]}</strong></span>
<span class="strong"><strong>    + )</strong></span>
</pre><p>List of 26 elements, one for each cuisine:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; names(proportion) &lt;- a[,1]</strong></span>
<span class="strong"><strong>    &gt; final &lt;- Reduce(function(...) merge(..., all=TRUE, by="Var1"), proportion)</strong></span>
<span class="strong"><strong>    &gt; row.names(final) &lt;- final[,1]</strong></span>
<span class="strong"><strong>    &gt; final &lt;- final[,-1]</strong></span>
<span class="strong"><strong>    &gt; final[is.na(final)] &lt;- 0</strong></span>
<span class="strong"><strong>    &gt; prop_matrix &lt;- t(final)</strong></span>
<span class="strong"><strong>    &gt; s &lt;- sort(apply(prop_matrix, 2, sd), decreasing=TRUE)</strong></span>
</pre><p>The <code class="literal">scale()</code> function centers and/or scales the columns of the <code class="literal">prop_matrix</code> matrix. The result is then stored in the <code class="literal">final_impdata</code> frame:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; final_imp &lt;- scale(subset(prop_matrix, select=names(which(s &gt; 0.1))))</strong></span>
</pre><p>Creating heatmap. <code class="literal">final_imp</code> is the data frame passed as an input. <code class="literal">trace="none"</code> signifies the character string, indicating whether a solid <code class="literal">"trace"</code> line should be drawn across rows or down columns, <code class="literal">"both"</code> or <code class="literal">"none"</code>. The <code class="literal">key=TRUE</code> value represents that a color-key should be shown:</p><pre class="programlisting">
<span class="strong"><strong>&gt; heatmap.2(final_imp, trace="none", margins = c(6,11), col=topo.colors(7), key=TRUE, key.title=NA, keysize=1.2, density.info="none")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_049.jpg" alt="Step 3 - preparing data"/></div><p>
</p></div><div class="section" title="Step 4 - applying principal components analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec67"/>Step 4 - applying principal components analysis</h3></div></div></div><p>Applying principle components analysis to data. <code class="literal">princomp()</code> performs principal components analysis on the <code class="literal">final_imp</code> datamatrix. The result is <code class="literal">pca_computation</code>, which is an object of class <code class="literal">princomp</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pca_computation &lt;- princomp(final_imp) </strong></span>
</pre><p>Printing the <code class="literal">pca_computation</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pca_computation</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_050.jpg" alt="Step 4 - applying principal components analysis"/></div><p>
</p><p>Producing a biplot. <code class="literal">pca_computation</code> is an object of class <code class="literal">princomp</code>. <code class="literal">pc.biplot=TRUE</code> means it is a principal component biplot:</p><pre class="programlisting">
<span class="strong"><strong>&gt; biplot(pca_computation, pc.biplot=TRUE, col=c("black","red"), cex=c(0.9,0.8), xlim=c(-2.5,2.5), xlab="PC1, 39.7%", ylab="PC2, 24.5%")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_051.jpg" alt="Step 4 - applying principal components analysis"/></div><p>
</p></div></div></div></body></html>