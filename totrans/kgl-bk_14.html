<html><head></head><body>
  <div id="_idContainer362" class="Basic-Text-Frame">
    <h1 class="chapterNumber">12</h1>
    <h1 id="_idParaDest-182" class="chapterTitle">Simulation and Optimization Competitions</h1>
    <p class="normal"><strong class="keyWord">Reinforcement learning </strong>(<strong class="keyWord">RL</strong>) is<a id="_idIndexMarker975"/> an interesting case among the different branches of machine learning. On the one hand, it is quite demanding from a technical standpoint: various intuitions from supervised learning do not hold, and the associated mathematical apparatus is quite a bit more advanced; on the other hand, it is the easiest one to explain to an outsider or layperson. A simple analogy is teaching your pet (I am very intentionally trying to steer clear of the dogs versus cats debate) to perform tricks: you provide a treat for a trick well done, and refuse it otherwise.</p>
    <p class="normal">Reinforcement learning was a latecomer to the competition party on Kaggle, but the situation has changed in the last few years with the introduction of simulation competitions. In this chapter, we will describe this new and exciting part of the Kaggle universe. So far – at the time of writing – there have been four <strong class="keyWord">Featured</strong> competitions and two <strong class="keyWord">Playground</strong> ones; this list, while admittedly not extensive, allows us to give a broad overview.</p>
    <p class="normal">In this chapter, we will demonstrate solutions to the problems presented in several simulation competitions:</p>
    <ul>
      <li class="bulletList">We begin with <em class="italic">Connect X</em>.</li>
      <li class="bulletList">We follow with <em class="italic">Rock, Paper, Scissors</em>, where a dual approach to building a competitive agent is shown.</li>
      <li class="bulletList">Next, we demonstrate a solution based on multi-armed bandits to the <em class="italic">Santa </em>competition.</li>
      <li class="bulletList">We conclude with an overview of the remaining competitions, which are slightly outside the scope of this chapter.</li>
    </ul>
    <p class="normal">If reinforcement learning<a id="_idIndexMarker976"/> is a completely new concept for you, it is probably a good idea to get some basic understanding first. A very good way to start on the<a id="_idIndexMarker977"/> RL adventure is the Kaggle Learning course dedicated to this very topic in the context of Game AI (<a href="https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning"><span class="url">https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning</span></a>). The course introduces basic concepts such as agents and policies, also providing a (crash) introduction to deep reinforcement learning. All the examples in the course use the data from the Playground competition <em class="italic">Connect X</em>, in which the <a id="_idIndexMarker978"/>objective is to train an agent capable of playing a game of connecting checkers in a line (<a href="https://www.kaggle.com/c/connectx/overview"><span class="url">https://www.kaggle.com/c/connectx/overview</span></a>).</p>
    <p class="normal">On a more general level, it is worth pointing out that an important aspect of simulation and optimization competitions is the <strong class="keyWord">environment</strong>: due to the very nature of the problem, your solution needs to exhibit more dynamic characteristics than just submitting a set of numbers (as would be the case for “regular” supervised learning contests). A very informative and detailed description of the environment used in the simulation competitions can be found at <a href="https://github.com/Kaggle/kaggle-environments/blob/master/README.md"><span class="url">https://github.com/Kaggle/kaggle-environments/blob/master/README.md</span></a>.</p>
    <h1 id="_idParaDest-183" class="heading-1">Connect X</h1>
    <p class="normal">In this section, we <a id="_idIndexMarker979"/>demonstrate how to approach the simple problem of playing checkers using heuristics. While not a deep learning solution, it is our view that this bare-bones presentation of the concepts is much more useful for people without significant prior exposure to RL.</p>
    <p class="normal">If you are new to the concept of using AI for board games, the presentation by <em class="italic">Tom van de Wiele</em> (<a href="https://www.kaggle.com/tvdwiele"><span class="url">https://www.kaggle.com/tvdwiele</span></a>) is a resource worth exploring: <a href="https://tinyurl.com/36rdv5sa"><span class="url">https://tinyurl.com/36rdv5sa</span></a>.</p>
    <p class="normal">The objective of <em class="italic">Connect X</em> is to get a number (<em class="italic">X</em>) of your checkers in a row – horizontally, vertically, or diagonally – on the game board before your opponent. Players take turns dropping their checkers into one of the columns at the top of the board. This means each move may have the purpose of trying to win for you or trying to stop your opponent from winning.</p>
    <figure class="mediaobject"><img src="../Images/B17574_12_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.1: Connect X board</p>
    <p class="normal"><em class="italic">Connect X</em> was the <a id="_idIndexMarker980"/>first competition that introduced <strong class="keyWord">agents</strong>: instead of a<a id="_idIndexMarker981"/> static submission (or a Notebook that was evaluated against an unseen dataset), participants had to submit agents capable of playing the game against others. The evaluation proceeded in steps:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Upon uploading, a submission plays against itself to ensure it works properly.</li>
      <li class="numberedList">If this validation episode is successful, a skill rating is assigned, and the submission joins the ranks of all competitors.</li>
      <li class="numberedList">Each day, several episodes are played for each submission, and subsequently rankings are adjusted.</li>
    </ol>
    <p class="normal">With that setup in mind, let us proceed toward demonstrating how to build a submission<a id="_idIndexMarker982"/> for the <em class="italic">Connect X</em> competition. The code we present is for <em class="italic">X=4</em>, but can be easily adapted for other values or variable <em class="italic">X</em>.</p>
    <p class="normal">First, we install the Kaggle environments package:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install kaggle-environments --upgrade
</code></pre>
    <p class="normal">We define an environment in which our agent will be evaluated:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> kaggle_environments <span class="hljs-keyword">import</span> evaluate, make
env = make(<span class="hljs-string">"connectx"</span>, debug=<span class="hljs-literal">True</span>)
env.render()
</code></pre>
    <p class="normal">While a<a id="_idIndexMarker983"/> frequent impulse you might have is to try sophisticated methods, it is useful to start simple – as we will do here, by using simple heuristics. These are combined into a single function in the accompanying code, but for the sake of presentation, we describe them one at a time here.</p>
    <p class="normal">The first rule is checking whether either of the players has a chance to connect four checkers vertically and, if so, returning the position at which it is possible. We can achieve this by using a simple variable as our input argument, which can take on two possible values indicating which player opportunities are being analyzed:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> my_agent(observation, configuration):
    <span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> choice
<span class="hljs-comment">    # me:me_or_enemy=1, enemy:me_or_enemy=2</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">check_vertical_chance</span><span class="hljs-function">(</span><span class="hljs-params">me_or_enemy</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">7</span>):
            <span class="hljs-keyword">if</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">5</span>] == me_or_enemy \
            <span class="hljs-keyword">and</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">4</span>] == me_or_enemy \
            <span class="hljs-keyword">and</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">3</span>] == me_or_enemy \
            <span class="hljs-keyword">and</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">2</span>] == <span class="hljs-number">0</span>:
                <span class="hljs-keyword">return</span> i
            <span class="hljs-keyword">elif</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">4</span>] == me_or_enemy \
            <span class="hljs-keyword">and</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">3</span>] == me_or_enemy \
            <span class="hljs-keyword">and</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">2</span>] == me_or_enemy \
            <span class="hljs-keyword">and</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">1</span>] == <span class="hljs-number">0</span>:
                <span class="hljs-keyword">return</span> i
            <span class="hljs-keyword">elif</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">3</span>] == me_or_enemy \
            <span class="hljs-keyword">and</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">2</span>] == me_or_enemy \
            <span class="hljs-keyword">and</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">1</span>] == me_or_enemy \
            <span class="hljs-keyword">and</span> observation.board[i+<span class="hljs-number">7</span>*<span class="hljs-number">0</span>] == <span class="hljs-number">0</span>:
                <span class="hljs-keyword">return</span> i
        <span class="hljs-comment"># no chance</span>
        <span class="hljs-keyword">return</span> -<span class="hljs-number">99</span>
</code></pre>
    <p class="normal">We can define an analogous method for horizontal chances:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">check_horizontal_chance</span><span class="hljs-function">(</span><span class="hljs-params">me_or_enemy</span><span class="hljs-function">):</span>
        chance_cell_num = -<span class="hljs-number">99</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> [<span class="hljs-number">0</span>,<span class="hljs-number">7</span>,<span class="hljs-number">14</span>,<span class="hljs-number">21</span>,<span class="hljs-number">28</span>,<span class="hljs-number">35</span>]:
            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">4</span>):
                val_1 = i+j+<span class="hljs-number">0</span>
                val_2 = i+j+<span class="hljs-number">1</span>
                val_3 = i+j+<span class="hljs-number">2</span>
                val_4 = i+j+<span class="hljs-number">3</span>
                <span class="hljs-keyword">if</span> <span class="hljs-built_in">sum</span>([observation.board[val_1] == me_or_enemy, \
                        observation.board[val_2] == me_or_enemy, \
                        observation.board[val_3] == me_or_enemy, \
                        observation.board[val_4] == me_or_enemy]) == <span class="hljs-number">3</span>:
                    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> [val_1,val_2,val_3,val_4]:
                        <span class="hljs-keyword">if</span> observation.board[k] == <span class="hljs-number">0</span>:
                            chance_cell_num = k
                            <span class="hljs-comment"># bottom line</span>
                            <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">35</span>, <span class="hljs-number">42</span>):
                                <span class="hljs-keyword">if</span> chance_cell_num == l:
                                    <span class="hljs-keyword">return</span> l - <span class="hljs-number">35</span>
                            <span class="hljs-comment"># others</span>
                            <span class="hljs-keyword">if</span> observation.board[chance_cell_num+<span class="hljs-number">7</span>] != <span class="hljs-number">0</span>:
                                <span class="hljs-keyword">return</span> chance_cell_num % <span class="hljs-number">7</span>
        <span class="hljs-comment"># no chance</span>
        <span class="hljs-keyword">return</span> -<span class="hljs-number">99</span>
</code></pre>
    <p class="normal">We repeat<a id="_idIndexMarker984"/> the same approach for the diagonal combinations:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># me:me_or_enemy=1, enemy:me_or_enemy=2</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">check_slanting_chance</span><span class="hljs-function">(</span><span class="hljs-params">me_or_enemy, lag, cell_list</span><span class="hljs-function">):</span>
        chance_cell_num = -<span class="hljs-number">99</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> cell_list:
            val_1 = i+lag*<span class="hljs-number">0</span>
            val_2 = i+lag*<span class="hljs-number">1</span>
            val_3 = i+lag*<span class="hljs-number">2</span>
            val_4 = i+lag*<span class="hljs-number">3</span>
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">sum</span>([observation.board[val_1] == me_or_enemy, \
                    observation.board[val_2] == me_or_enemy, \
                    observation.board[val_3] == me_or_enemy, \
                    observation.board[val_4] == me_or_enemy]) == <span class="hljs-number">3</span>:
                <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> [val_1,val_2,val_3,val_4]:
                    <span class="hljs-keyword">if</span> observation.board[j] == <span class="hljs-number">0</span>:
                        chance_cell_num = j
                        <span class="hljs-comment"># bottom line</span>
                        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">35</span>, <span class="hljs-number">42</span>):
                            <span class="hljs-keyword">if</span> chance_cell_num == k:
                                <span class="hljs-keyword">return</span> k - <span class="hljs-number">35</span>
                        <span class="hljs-comment"># others</span>
                        <span class="hljs-keyword">if</span> chance_cell_num != -<span class="hljs-number">99</span> \
                        <span class="hljs-keyword">and</span> observation.board[chance_cell_num+<span class="hljs-number">7</span>] != <span class="hljs-number">0</span>:
                            <span class="hljs-keyword">return</span> chance_cell_num % <span class="hljs-number">7</span>
        <span class="hljs-comment"># no chance</span>
        <span class="hljs-keyword">return</span> -<span class="hljs-number">99</span>
</code></pre>
    <p class="normal">We can combine <a id="_idIndexMarker985"/>the logic into a single function checking the opportunities (playing the game against an opponent):</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">check_my_chances</span><span class="hljs-function">():</span>
        <span class="hljs-comment"># check my vertical chance</span>
        result = check_vertical_chance(my_num)
        <span class="hljs-keyword">if</span> result != -<span class="hljs-number">99</span>:
            <span class="hljs-keyword">return</span> result
        <span class="hljs-comment"># check my horizontal chance</span>
        result = check_horizontal_chance(my_num)
        <span class="hljs-keyword">if</span> result != -<span class="hljs-number">99</span>:
            <span class="hljs-keyword">return</span> result
        <span class="hljs-comment"># check my slanting chance 1 (up-right to down-left)</span>
        result = check_slanting_chance(my_num, <span class="hljs-number">6</span>, [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">10</span>,<span class="hljs-number">11</span>,<span class="hljs-number">12</span>,<span class="hljs-number">13</span>,<span class="hljs-number">17</span>,<span class="hljs-number">18</span>,<span class="hljs-number">19</span>,<span class="hljs-number">20</span>])
        <span class="hljs-keyword">if</span> result != -<span class="hljs-number">99</span>:
            <span class="hljs-keyword">return</span> result
        <span class="hljs-comment"># check my slanting chance 2 (up-left to down-right)</span>
        result = check_slanting_chance(my_num, <span class="hljs-number">8</span>, [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>,<span class="hljs-number">14</span>,<span class="hljs-number">15</span>,<span class="hljs-number">16</span>,<span class="hljs-number">17</span>])
        <span class="hljs-keyword">if</span> result != -<span class="hljs-number">99</span>:
            <span class="hljs-keyword">return</span> result
        <span class="hljs-comment"># no chance</span>
        <span class="hljs-keyword">return</span> -<span class="hljs-number">99</span>
</code></pre>
    <p class="normal">Those blocks constitute the basics of the logic. While a bit cumbersome to formulate, they are a useful exercise in converting an intuition into heuristics that can be used in an agent competing in a game.</p>
    <div class="note">
      <p class="normal">Please see the accompanying code in the repository for a complete definition of the agent in this example.</p>
    </div>
    <p class="normal">The performance of our newly defined agent can be evaluated against a pre-defined agent, for example, a random one:</p>
    <pre class="programlisting code"><code class="hljs-code">env.reset()
env.run([my_agent, <span class="hljs-string">"random"</span>])
env.render(mode=<span class="hljs-string">"ipython"</span>, width=<span class="hljs-number">500</span>, height=<span class="hljs-number">450</span>)
</code></pre>
    <p class="normal">The code above shows you how to set up a solution from scratch for a relatively simple problem (there is a reason why <em class="italic">Connect X</em> is a Playground and not a Featured competition). Interestingly, this simple problem can be handled with (almost) state-of-the-art methods<a id="_idIndexMarker986"/> like AlphaZero: <a href="https://www.kaggle.com/connect4alphazero/alphazero-baseline-connectx"><span class="url">https://www.kaggle.com/connect4alphazero/alphazero-baseline-connectx</span></a>.</p>
    <p class="normal">With the introductory example behind us, you should be ready to dive into the more elaborate (or in any case, not toy example-based) contests.</p>
    <h1 id="_idParaDest-184" class="heading-1">Rock-paper-scissors</h1>
    <p class="normal">It is no coincidence <a id="_idIndexMarker987"/>that several problems in simulation competitions refer to playing games: at varying levels of complexity, games offer an environment with clearly defined rules, naturally lending itself to the agent-action-reward framework. Aside from Tic-Tac-Toe, connecting checkers is one of the simplest examples of a competitive game. Moving up the difficulty ladder (of games), let’s have a look at <strong class="keyWord">rock-paper-scissors </strong>and how a Kaggle contest centered around this game could be approached.</p>
    <p class="normal">The idea of the <em class="italic">Rock, Paper, Scissors</em> competition (<a href="https://www.kaggle.com/c/rock-paper-scissors/code"><span class="url">https://www.kaggle.com/c/rock-paper-scissors/code</span></a>) was an<a id="_idIndexMarker988"/> extension of the basic rock-paper-scissors game (known as <em class="italic">roshambo</em> in some parts of the world): instead of the usual “best of 3” score, we use “best of 1,000.”</p>
    <p class="normal">We will describe two possible approaches to the problem: one rooted in the game-theoretic approach, and the other more focused on the algorithmic side.</p>
    <p class="normal">We begin with<a id="_idIndexMarker989"/> the <strong class="keyWord">Nash equilibrium</strong>. Wikipedia gives the definition of this as the solution to a non-cooperative game involving two or more players, where each player is assumed to know the equilibrium strategies of the others, and no player can obtain an advantage by changing only their own strategy.</p>
    <div class="note">
      <p class="normal">An excellent introduction to rock-paper-scissors in a game-theoretic framework can be found at <a href="https://www.youtube.com/watch?v=-1GDMXoMdaY"><span class="url">https://www.youtube.com/watch?v=-1GDMXoMdaY</span></a>.</p>
    </div>
    <p class="normal">Denoting our players as red and blue, each cell in the matrix of outcomes shows the result of a given combination of moves:</p>
    <figure class="mediaobject"><img src="../Images/B17574_12_02.png" alt="Obraz zawierający tekst, tablica wyników  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 12.2: Payoff matrix for rock-paper-scissors </p>
    <p class="normal">As an example, if <a id="_idIndexMarker990"/>both play Rock (the top-left cell), both gain 0 points; if blue plays Rock and red plays Paper (the cell in the second column of the first row), red wins – so red gains +1 point and blue has -1 point as a result.</p>
    <p class="normal">If we played each<a id="_idIndexMarker991"/> action with an equal probability of 1/3, then the opponent must do the same; otherwise, if they play Rock all the time, they will tie against Rock, lose against Paper, and win against Scissors – each with a probability of 1/3 (or one-third of the time). The expected reward, in this case, is 0, in which case we can change our strategy to Paper and win all the time. The same reasoning can be conducted for the strategy of Paper versus Scissors and Scissors versus Rock, for which we will not show you the matrix of outcomes due to redundancy.</p>
    <p class="normal">The remaining option in order to be in equilibrium is that both players need to play a random strategy – which is the<a id="_idIndexMarker992"/> Nash equilibrium. We can build a simple agent around this idea: </p>
    <pre class="programlisting code"><code class="hljs-code">%%writefile submission.py
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">nash_equilibrium_agent</span><span class="hljs-function">(</span><span class="hljs-params">observation, configuration</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>)
</code></pre>
    <div class="note">
      <p class="normal">The magic at the start (writing from a Notebook directly to a file) is necessary to satisfy the submission constraints of this particular competition.</p>
    </div>
    <p class="normal">How does our Nash agent perform against others? We can find out by evaluating the performance:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install -q -U kaggle_environments
<span class="hljs-keyword">from</span> kaggle_environments <span class="hljs-keyword">import</span> make
</code></pre>
    <div class="note">
      <p class="normal">At the time of writing, there is an error that pops up after this import (<strong class="screenText">Failure to load a module named ‘gfootball’</strong>); the official advice from Kaggle is to ignore it. In practice, it does not seem to have any impact on executing the code.</p>
    </div>
    <p class="normal">We start by <a id="_idIndexMarker993"/>creating the rock-paper-scissors environment and setting the limit to 1,000 episodes per simulation:</p>
    <pre class="programlisting code"><code class="hljs-code">env = make(
    <span class="hljs-string">"rps"</span>, 
    configuration={<span class="hljs-string">"episodeSteps"</span>: <span class="hljs-number">1000</span>}
)
</code></pre>
    <p class="normal">We will make use of a Notebook created in this competition that implemented numerous agents based on deterministic heuristics (<a href="https://www.kaggle.com/ilialar/multi-armed-bandit-vs-deterministic-agents"><span class="url">https://www.kaggle.com/ilialar/multi-armed-bandit-vs-deterministic-agents</span></a>) and import the code for the agents we compete against from there:</p>
    <pre class="programlisting code"><code class="hljs-code">%%writefile submission_copy_opponent.py
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">copy_opponent_agent</span><span class="hljs-function">(</span><span class="hljs-params">observation, configuration</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">if</span> observation.step &gt; <span class="hljs-number">0</span>:
        <span class="hljs-keyword">return</span> observation.lastOpponentAction
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>
<span class="hljs-comment"># nash_equilibrium_agent vs copy_opponent_agent</span>
env.run(
    [<span class="hljs-string">"submission.py"</span>, <span class="hljs-string">"submission_copy_opponent.py"</span>]
)
env.render(mode=<span class="hljs-string">"</span><span class="hljs-string">ipython"</span>, width=<span class="hljs-number">500</span>, height=<span class="hljs-number">400</span>)
</code></pre>
    <p class="normal">When<a id="_idIndexMarker994"/> we execute the preceding block and run the environment, we can watch an animated board for the 1,000 epochs. A snapshot looks like this:</p>
    <figure class="mediaobject"><img src="../Images/B17574_12_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.3: A snapshot from a rendered environment evaluating agent performance</p>
    <p class="normal">In supervised learning – both classification and regression – it is frequently useful to start approaching any problem with a simple benchmark, usually a linear model. Even though not state of the art, it can provide a useful expectation and a measure of performance. In reinforcement learning, a similar idea holds; an approach worth trying in this capacity is the multi-armed bandit, the simplest algorithm we can honestly call RL. In the next section, we demonstrate how this approach can be used in a simulation competition.</p>
    <h1 id="_idParaDest-185" class="heading-1">Santa competition 2020</h1>
    <p class="normal">Over the last few years, a <a id="_idIndexMarker995"/>sort of tradition has emerged on Kaggle: in early December, there is a Santa-themed competition. The actual algorithmic side varies from year to year, but for our purposes, the 2020 competition is an interesting <a id="_idIndexMarker996"/>case: <a href="https://www.kaggle.com/c/santa-2020"><span class="url">https://www.kaggle.com/c/santa-2020</span></a>.</p>
    <p class="normal">The setup was a <a id="_idIndexMarker997"/>classical <strong class="keyWord">multi-armed bandit</strong> (<strong class="keyWord">MAB</strong>) trying to maximize reward by taking repeated action on a vending machine, but with two extras:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Reward decay</strong>: At<a id="_idIndexMarker998"/> each step, the probability of obtaining a reward from a machine decreases by 3 percent.</li>
      <li class="bulletList"><strong class="keyWord">Competition</strong>: You are constrained not only by time (a limited number of attempts) but also by another player attempting to achieve the same objective. We mention this constraint mostly for the sake of completeness, as it is not crucial to incorporate explicitly in our demonstrated solution.</li>
    </ul>
    <div class="packt_tip">
      <p class="normal">For a good explanation of the methods for approaching the general MAB problem, the reader is referred to <a href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html"><span class="url">https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html</span></a>.</p>
    </div>
    <p class="normal">The solution we demonstrate is adapted from <a href="https://www.kaggle.com/ilialar/simple-multi-armed-bandit"><span class="url">https://www.kaggle.com/ilialar/simple-multi-armed-bandit</span></a>, code from <em class="italic">Ilia Larchenko</em> (<a href="https://www.kaggle.com/ilialar"><span class="url">https://www.kaggle.com/ilialar</span></a>). Our approach is based on successive updates to the distribution of reward: at each step, we generate a random number from a Beta distribution with parameters (<em class="italic">a+1</em>, <em class="italic">b+1</em>) where:</p>
    <ul>
      <li class="bulletList"><em class="italic">a</em> is the total reward from this arm (number of wins)</li>
      <li class="bulletList"><em class="italic">b</em> is the number of historical losses</li>
    </ul>
    <p class="normal">When we need to decide which arm to pull, we select the arm with the highest generated number and use it to generate the next step; our posterior distribution becomes a prior for the next step.</p>
    <p class="normal">The graph below shows the shape of a Beta distribution for different pairs of (<em class="italic">a</em>, <em class="italic">b</em>) values:</p>
    <figure class="mediaobject"><img src="../Images/B17574_12_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.4: Shape of the beta distribution density for different combinations of (a,b) parameters</p>
    <p class="normal">As you<a id="_idIndexMarker999"/> can see, initially, the distribution is flat (Beta(0,0) is uniform), but as we gather more information, it concentrates the probability mass around the mode, which means there is less uncertainty and we are more confident about our judgment. We can incorporate the competition-specific reward decay by decreasing the <em class="italic">a</em> parameter every time an arm is used.</p>
    <p class="normal">We begin the creation of our agent by writing a submission file. First, the necessary imports and variable initialization:</p>
    <pre class="programlisting code"><code class="hljs-code">%%writefile submission.py
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
bandit_state = <span class="hljs-literal">None</span>
total_reward = <span class="hljs-number">0</span>
last_step = <span class="hljs-literal">None</span>
</code></pre>
    <p class="normal">We define the class specifying an MAB agent. For the sake of reading coherence, we reproduce the entire code and include the explanations in comments within it:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">multi_armed_bandit_agent </span><span class="hljs-function">(</span><span class="hljs-params">observation, configuration</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">global</span> history, history_bandit
    step = <span class="hljs-number">1.0</span>         <span class="hljs-comment"># balance exploration / exploitation</span>
    decay_rate = <span class="hljs-number">0.97</span>  <span class="hljs-comment"># how much do we decay the win count after each call</span>
    
    <span class="hljs-keyword">global</span> bandit_state,total_reward,last_step
        
    <span class="hljs-keyword">if</span> observation.step == <span class="hljs-number">0</span>:
        <span class="hljs-comment"># initial bandit state</span>
        bandit_state = [[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(configuration[<span class="hljs-string">"banditCount"</span>])]
    <span class="hljs-keyword">else</span>:       
        <span class="hljs-comment"># updating bandit_state using the result of the previous step</span>
        last_reward = observation[<span class="hljs-string">"reward"</span>] - total_reward
        total_reward = observation[<span class="hljs-string">"reward"</span>]
        
        <span class="hljs-comment"># we need to understand who we are Player 1 or 2</span>
        player = <span class="hljs-built_in">int</span>(last_step == observation.lastActions[<span class="hljs-number">1</span>])
        
        <span class="hljs-keyword">if</span> last_reward &gt; <span class="hljs-number">0</span>:
            bandit_state[observation.lastActions[player]][<span class="hljs-number">0</span>] += last_reward * step
        <span class="hljs-keyword">else</span>:
            bandit_state[observation.lastActions[player]][<span class="hljs-number">1</span>] += step
        
        bandit_state[observation.lastActions[<span class="hljs-number">0</span>]][<span class="hljs-number">0</span>] = (bandit_state[observation.lastActions[<span class="hljs-number">0</span>]][<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>) * decay_rate + <span class="hljs-number">1</span>
        bandit_state[observation.lastActions[<span class="hljs-number">1</span>]][<span class="hljs-number">0</span>] = (bandit_state[observation.lastActions[<span class="hljs-number">1</span>]][<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>) * decay_rate + <span class="hljs-number">1</span>
    <span class="hljs-comment"># generate random number from Beta distribution for each agent and select the most lucky one</span>
    best_proba = -<span class="hljs-number">1</span>
    best_agent = <span class="hljs-literal">None</span>
    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(configuration[<span class="hljs-string">"banditCount"</span>]):
        proba = np.random.beta(bandit_state[k][<span class="hljs-number">0</span>],bandit_state[k][<span class="hljs-number">1</span>])
        <span class="hljs-keyword">if</span> proba &gt; best_proba:
            best_proba = proba
            best_agent = k
        
    last_step = best_agent
    <span class="hljs-keyword">return</span> best_agent  
</code></pre>
    <p class="normal">As you can see, the core logic of the function is a straightforward implementation of the MAB algorithm. An adjustment specific to our contest occurs with the <code class="inlineCode">bandit_state</code> variable, where we apply the decay multiplier.</p>
    <p class="normal">Similar to <a id="_idIndexMarker1000"/>the previous case, we are now ready to evaluate the performance of our agent in the contest environment. The code snippet below demonstrates how this can be implemented:</p>
    <pre class="programlisting code"><code class="hljs-code">%%writefile random_agent.py
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">random_agent</span><span class="hljs-function">(</span><span class="hljs-params">observation, configuration</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> random.randrange(configuration.banditCount)
<span class="hljs-keyword">from</span> kaggle_environments <span class="hljs-keyword">import</span> make
env = make(<span class="hljs-string">"mab"</span>, debug=<span class="hljs-literal">True</span>)
env.reset()
env.run([<span class="hljs-string">"random_agent.py"</span>, <span class="hljs-string">"submission.py"</span>])
env.render(mode=<span class="hljs-string">"ipython"</span>, width=<span class="hljs-number">800</span>, height=<span class="hljs-number">700</span>)
</code></pre>
    <p class="normal">We see something like this:</p>
    <figure class="mediaobject"><img src="../Images/B17574_12_05.png" alt="Obraz zawierający tekst, sprzęt elektroniczny, zrzut ekranu, wyświetlanie  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 12.5: Snapshot from a rendered environment evaluating agent performance</p>
    <p class="normal">In this <a id="_idIndexMarker1001"/>section, we demonstrated how a vintage multi-armed bandit algorithm can be utilized in a simulation competition on Kaggle. While useful as a starting point, this was not sufficient to qualify for the medal zone, where deep reinforcement learning approaches were more popular.</p>
    <p class="normal">We will follow up with a discussion of approaches based on other methods, in a diverse range of competitions.</p>
    <h1 id="_idParaDest-186" class="heading-1">The name of the game</h1>
    <p class="normal">Beyond the<a id="_idIndexMarker1002"/> relatively elementary games discussed above, simulation competitions involve more elaborate setups. In this section, we will briefly discuss those. The <a id="_idIndexMarker1003"/>first example is <em class="italic">Halite</em>, defined on the competition page (<a href="https://www.kaggle.com/c/halite"><span class="url">https://www.kaggle.com/c/halite</span></a>) in the following manner:</p>
    <blockquote class="packt_quote">
      <p class="quote">Halite [...] is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it’s up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.</p>
    </blockquote>
    <p class="normal">This is what <a id="_idIndexMarker1004"/>the game looks like:</p>
    <figure class="mediaobject"><img src="../Images/B17574_12_6.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.6: Halite game board</p>
    <p class="normal">Kaggle <a id="_idIndexMarker1005"/>organized two competitions around the game: a Playground edition (<a href="https://www.kaggle.com/c/halite-iv-playground-edition"><span class="url">https://www.kaggle.com/c/halite-iv-playground-edition</span></a>) as well as a regular Featured edition (<a href="https://www.kaggle.com/c/halite"><span class="url">https://www.kaggle.com/c/halite</span></a>). The classic reinforcement learning approach was less useful in this instance since, with an arbitrary number of units (ships/bases) and a dynamic opponent pool, the problem of credit assignment was becoming intractable for people with access to a “normal” level of computing resources.</p>
    <div class="packt_tip">
      <p class="normal">Explaining the problem of credit assignment in full generality is beyond the scope of this book, but an interested reader is encouraged to start with the Wikipedia entry (<a href="https://en.wikipedia.org/wiki/Assignment_problem"><span class="url">https://en.wikipedia.org/wiki/Assignment_problem</span></a>) and follow up with this excellent introductory article by Mesnard et al.: <a href="https://proceedings.mlr.press/v139/mesnard21a.html"><span class="url">https://proceedings.mlr.press/v139/mesnard21a.html</span></a>.</p>
    </div>
    <p class="normal">A description of the winning solution by <em class="italic">Tom van de Wiele</em> (<a href="https://www.kaggle.com/c/halite/discussion/183543"><span class="url">https://www.kaggle.com/c/halite/discussion/183543</span></a>) provides an excellent overview of the modified approach that proved successful in this instance (deep RL with independent credit assignment per unit).</p>
    <p class="normal">Another competition involving a<a id="_idIndexMarker1006"/> relatively sophisticated game was <em class="italic">Lux AI</em> (<a href="https://www.kaggle.com/c/lux-ai-2021"><span class="url">https://www.kaggle.com/c/lux-ai-2021</span></a>). In this competition, participants were tasked with designing agents to tackle a multi-variable optimization problem combining resource gathering and allocation, competing against other players. In addition, successful agents had to analyze the moves of their opponents and react accordingly. An interesting feature of this contest was the popularity<a id="_idIndexMarker1007"/> of a “meta” approach: <strong class="keyWord">imitation learning</strong> (<a href="https://paperswithcode.com/task/imitation-learning"><span class="url">https://paperswithcode.com/task/imitation-learning</span></a>). This is a fairly novel approach in RL, focused on learning a behavior policy from demonstration – without a specific model to describe the generation of state-action pairs. A competitive implementation of this idea (at the time of writing) is given by Kaggle user Ironbar (<a href="https://www.kaggle.com/c/lux-ai-2021/discussion/293911"><span class="url">https://www.kaggle.com/c/lux-ai-2021/discussion/293911</span></a>).</p>
    <p class="normal">Finally, no discussion of simulation competitions in Kaggle would be complete without the <em class="italic">Google Research Football with Manchester City F.C.</em> competition (<a href="https://www.kaggle.com/c/google-football/overview"><span class="url">https://www.kaggle.com/c/google-football/overview</span></a>). The motivation behind this contest was for researchers to explore AI agents’ ability to play in complex settings like football. The competition <strong class="screenText">Overview</strong> section formulates the problem thus:</p>
    <blockquote class="packt_quote">
      <p class="quote">The sport requires a balance of short-term control, learned concepts such as passing, and high-level strategy, which can be difficult to teach agents. A current environment exists to train and test agents, but other solutions may offer better results.</p>
    </blockquote>
    <p class="normal">Unlike some examples given above, this competition was dominated by reinforcement learning approaches:</p>
    <ul>
      <li class="bulletList">Team Raw Beast (3<sup class="superscript">rd</sup>) followed a methodology inspired by <em class="italic">AlphaStar</em>: <a href="https://www.kaggle.com/c/google-football/discussion/200709"><span class="url">https://www.kaggle.com/c/google-football/discussion/200709</span></a></li>
      <li class="bulletList">Salty Fish (2<sup class="superscript">nd</sup>) utilized a form of self-play: <a href="https://www.kaggle.com/c/google-football/discussion/202977"><span class="url">https://www.kaggle.com/c/google-football/discussion/202977</span></a></li>
      <li class="bulletList">The winners, WeKick, used a deep learning-based solution with creative feature engineering and reward structure adjustment: <a href="https://www.kaggle.com/c/google-football/discussion/202232"><span class="url">https://www.kaggle.com/c/google-football/discussion/202232</span></a></li>
    </ul>
    <p class="normal">Studying the solutions listed above is an excellent starting point to learn how RL can be utilized to solve this class of problems.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Firat_Gonen.png" alt=""/>
      </div>
      <p class="intervieweeName">Firat Gonen</p>
      <p class="normal"><a href="https://www.kaggle.com/frtgnn"><span class="url">https://www.kaggle.com/frtgnn</span></a></p>
      <p class="normal">For this chapter’s interview, we<a id="_idIndexMarker1008"/> spoke to Firat Gonen, a Triple Grandmaster in Datasets, Notebooks, and Discussion, and an HP Data Science Global Ambassador. He gives us his take on his Kaggle approach, and how his attitude has evolved over time.</p>
      <p class="interviewHeader">What’s your favorite kind of competition and why? In terms of technique and solving approaches, what is your specialty on Kaggle? </p>
      <p class="normal"><em class="italic">My favorite kind evolved over time. I used to prefer very generic tabular competitions where a nice laptop and some patience would suffice to master the trends. I felt like I used to be able to see the outlying trends between training and test sets pretty good. Over time, with being awarded the ambassadorship by Z by HP and my workstation equipment, I kind of converted myself towards more computer vision competitions, though I still have a lot to learn.</em></p>
      <p class="interviewHeader">How do you approach a Kaggle competition? How different is this approach to what you do in your day-to-day work? </p>
      <p class="normal"><em class="italic">I usually prefer to delay the modeling part for as long as I can. I like to use that time on EDAs, outliers, reading the forum, etc., trying to be patient. After I feel like I’m done with feature engineering, I try to form only benchmark models to get a grip on different architecture results. My technique is very similar when it comes to professional work as well. I find it useless to try to do the best in a huge amount of time; there has to be a balance between time and success.</em></p>
      <p class="interviewHeader">Tell us about a particularly challenging competition you entered, and what insights you used to tackle the task.</p>
      <p class="normal"><em class="italic">The competition hosted by François Chollet was extremely challenging; the very first competition to force us into AGI. I remember I felt pretty powerless in that competition, where I learned several new techniques. I think everybody did that while remembering data science is not just machine learning. Several other techniques like mixed integer programming resurfaced at Kaggle.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">Of course: I learned a lot of new techniques and stayed up to date thanks to Kaggle. I’m in a place in my career where my main responsibility lies mostly in management. That’s why Kaggle is very important to me for staying up to date in several things.</em></p>
      <p class="interviewHeader">How have you built up your portfolio thanks to Kaggle?</p>
      <p class="normal"><em class="italic">I believe the advantage was in a more indirect way, where people saw both practical skills (thanks to Kaggle) and more theoretical skills in my more conventional education qualifications.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">I think there are two things newcomers do wrong. The first one is having fear in entering a new competition, thinking that they will get bad scores and it will be registered. This is nonsense. Everybody has bad scores; it’s all about how much you devote to a new competition. The second one is that they want to get to the model-building stage ASAP, which is very wrong; they want to see their benchmark scores and then they get frustrated. I advise them to take their time in feature generation and selection, and also in EDA stages.</em></p>
      <p class="interviewHeader">What mistakes have you made in competitions in the past?</p>
      <p class="normal"><em class="italic">My mistakes are, unfortunately, very similar to new rookies. I got impatient in several competitions where I didn’t pay enough attention to early stages, and after some time you feel like you don’t have enough time to go back.</em></p>
      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis or machine learning?</p>
      <p class="normal"><em class="italic">I would recommend PyCaret for benchmarking to get you speed, and PyTorch for a model-building framework.</em></p>
      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition?</p>
      <p class="normal"><em class="italic">Exploratory data analysis and previous similar competition discussions.</em></p>
      <p class="interviewHeader">Do you use other competition platforms? How do they compare to Kaggle?</p>
      <p class="normal"><em class="italic">To be honest, I haven’t rolled the dice outside Kaggle, but I have had my share of them from a tourist perspective. It takes time to adjust to other platforms.</em></p>
    </div>
    <h1 id="_idParaDest-187" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we discussed simulation competitions, a new type of contest that is increasing in popularity. Compared to vision or NLP-centered ones, simulation contests involve a much broader range of methods (with somewhat higher mathematical content), which reflects the difference between supervised learning and reinforcement learning.</p>
    <p class="normal">This chapter concludes the technical part of the book. In the remainder, we will talk about turning your Kaggle Notebooks into a portfolio of projects and capitalizing on it to find new professional opportunities. </p>
    <h1 id="_idParaDest-188" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join the book’s Discord workspace for a monthly <em class="italic">Ask me Anything</em> session with the authors: </p>
    <p class="normal"><a href="https://packt.link/KaggleDiscord"><span class="url">https://packt.link/KaggleDiscord</span></a></p>
    <p class="normal"><img src="../Images/QR_Code40480600921811704671.png" alt=""/></p>
  </div>
</body></html>