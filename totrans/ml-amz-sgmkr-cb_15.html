<html><head></head><body>
		<div id="_idContainer240">
			<h1 id="_idParaDest-155"><em class="italic"><a id="_idTextAnchor176"/>Chapter 12</em>: DataRobot Python API</h1>
			<p>Users can access DataRobot's capabilities using DataRobot's Python client package. This lets us ingest data, create machine learning projects, make predictions from models, and manage models programmatically. It is easy to see the advantages that <strong class="bold">Application Programming Interfaces</strong> (<strong class="bold">APIs</strong>) offer users. The integrated use of Python and DataRobot lets us leverage the AutoML capabilities DataRobot presents, all while exploiting the programmatic flexibility and potential that Python possesses. </p>
			<p>In this chapter, we will use the DataRobot Python API to ingest data, create a project with models, evaluate the models, and make predictions against them. At a high level, we will cover the following topics: </p>
			<ul>
				<li>Accessing the DataRobot API </li>
				<li>Understanding the DataRobot Python client </li>
				<li><a id="_idTextAnchor177"/>Building models programmatically</li>
				<li>Making predictions programmatically </li>
			</ul>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor178"/>Technical requirements</h1>
			<p>For the analysis and modeling that will be carried out in this chapter, you will need access to the DataRobot software. Jupyter Notebook is crucial for this chapter as most of the interactions with DataRobot will be carried out from the console. Your Python version should be 2.7 or 3.4+. Now, let's look at the dataset that will be utilized in this chapter.</p>
			<p>Check out the following video to see the Code in Action at <a href="https://bit.ly/3wV4qx5">https://bit.ly/3wV4qx5</a>.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor179"/>Automobile Dataset</h2>
			<p>The automobile dataset <a id="_idIndexMarker627"/>can be accessed at the UCI Machine Learning Repository ( <a href="https://archive.ics.uci.edu/ml/datasets/Automobile">https://archive.ics.uci.edu/ml/datasets/Automobile</a>). Each row in this <a id="_idIndexMarker628"/>dataset represents a specific automobile. The features (columns) describe its characteristics, risk rating, and associated normalized losses. Even though it is a small dataset, it has many features that are numerical <a id="_idIndexMarker629"/>as well as categorical. Its features are described on its web page and the data is provided in<strong class="source-inline">.csv</strong> format.</p>
			<p class="callout-heading">Dataset Citation</p>
			<p class="callout">Dua, D. and Graff, C. (2019). UCI Machine Learning Repository (<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>). Irvine, CA: University of California, School of Information and Computer Science.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor180"/>Accessing the DataRobot API</h1>
			<p>The <a id="_idIndexMarker630"/>programmatic use of DataRobot enables data experts to leverage the platform's efficacies while having the flexibility associated with typical programming. With the API access of DataRobot, data from numerous sources can be integrated for analytic or modeling purposes. This capability is not only limited to the data that's ingested, but also the output of the outcome. For instance, API access makes it possible for a customer risk profiling model to get data from differing sources, such as Google BigQuery, local files, as well as AWS S3 buckets. And in a few lines of codes, the outcomes can update records on Salesforce, as well as those surfaced on PowerBI via a BigQuery table. The strength of this multiple data source integration capability is furthered as this enables the automated, scheduled, end-to-end periodic refresh of model outcomes. </p>
			<p>In this preceding case, it becomes possible for the client base to be rescored periodically. Regarding scoring data, the DataRobot platform can only score datasets that are less than 1 GB in size. When problems require huge datasets, the <strong class="bold">Batch Prediction API</strong> normally<a id="_idIndexMarker631"/> chunks up the data and scores them concurrently. For a dataset with hundreds of millions of rows, it is possible to set up an iterative job to chunk up the data and score it iteratively using the Batch Prediction API.</p>
			<p>In addition, the API access to DataRobot allows users to develop user-defined features that make commercial sense before analysis and those based on scored model outcomes. This makes the modeling process more robust as it allows human intelligence to be applied to outcomes. In the preceding client risk profiling case, it becomes possible to classify customers into risk categories for easier business decision making. Also, based on the explanations given, the next best actions could be developed. </p>
			<p>Furthermore, programmatic use of DataRobot allows users to configure differing visualizations as they deem fit. This also offers analysts a broader range of visual outcome types. The Seaborn and Matplotlib Python libraries offer a huge range of visualization <a id="_idIndexMarker632"/>types with differing configurations. This also allows certain data subgroups or splits to be visualized. Among other benefits, it becomes possible to even select certain aspects of the data to be visualized.</p>
			<p>One of the big advantages of accessing DataRobot using its API is the ability to create multiple projects iteratively. Two easy examples come to mind here. One approach to improving the outcomes of multi-class modeling is to use the one versus all modeling paradigm. This involves creating models for each of the classes. When scoring, all the models are used to score the data and for each row, the class with the highest score is attributed to the row. To bring this to life, let's assume we are building models to predict wheel drive types based on other <a id="_idIndexMarker633"/>characteristics. First, models are created for<a id="_idIndexMarker634"/> the three main types of wheel drives; that is, <strong class="bold">front-wheel drive</strong> (<strong class="bold">FWD</strong>), <strong class="bold">four-wheel drive</strong> (<strong class="bold">4WD</strong>), and <strong class="bold">rear-wheel drive</strong> (<strong class="bold">RWD</strong>). Data <a id="_idIndexMarker635"/>is then scored against all three models, and the model that presents each row with the highest prediction is assumed as the class the row belongs to.</p>
			<p>The model factory is another example where multiple model projects are integrated into a system so that each project builds models for a subgroup in the data. In some problems, data tends to be nested in that certain variables tend to govern the way models behave generally. A point in case is modeling the performance of students nested in class. These features, such as the class teacher for schools, tend to control the effect other exogenous variables have on the dependent variable. </p>
			<p>In the case of cars, their brands typically drive their prices. For instance, irrespective of how similar a Skoda is to an Audi, the Audi will most likely be more expensive. As such, when developing models for such a case, it is ideal to create models for each of the car brands. In the context of programmatically accessing DataRobot, the idea would be to run an iteration of the project for each of the car brands. </p>
			<p>In addition to<a id="_idIndexMarker636"/> creating and scoring DataRobot models programmatically, we will use Jupyter Notebook's <strong class="bold">Integrated Development Environment</strong> (<strong class="bold">IDE</strong>) to build projects for a case of one versus all and a model factory. However, before we can create projects with DataRobot using an API, certain identification processes must be covered. Let's have a look. </p>
			<p>To programmatically access DataRobot, users need to create an API key. This key is then used to access the platform from a client. To create an API key, open the <strong class="bold">Account</strong> menu at <a id="_idIndexMarker637"/>the top right-hand corner of the home page (see <em class="italic">Figure 12.1</em>). From there, access the <strong class="bold">Developer Tools</strong> window (see <em class="italic">Figure 12.1</em>):</p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="image/B17159_12_01.jpg" alt="Figure 12.1 – Accessing Developer Tools&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – Accessing Developer Tools</p>
			<p>After opening the <strong class="bold">Developer Tools</strong> window, click on <strong class="bold">Create New Key</strong> and enter the name of the new key. On saving the new key's name, the API key will be generated (see <em class="italic">Figure 12.2</em>). After this, the generated key is copied and secured. The API key, along with the endpoint, is necessary to establish a connection between the local machine and the DataRobot instance:</p>
			<div>
				<div id="_idContainer228" class="IMG---Figure">
					<img src="image/B17159_12_02.jpg" alt="Figure 12.2 – Creating an API key&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – Creating an API key</p>
			<p>The<a id="_idIndexMarker638"/> endpoint parameter is the URL of the DataRobot endpoint. <a href="https://app.datarobot.com/api/v2">https://app.datarobot.com/api/v2</a> is the default endpoint for the US cloud-based endpoint for its US and Japanese users. The EU-managed cloud endpoint is <a href="https://app.eu.datarobot.com/api/v2">https://app.eu.datarobot.com/api/v2</a>. VPC, on-premises, hybrid, or private users usually have their deployment endpoint as their DataRobot GUI root. To enhance security, these credentials are sometimes stored and accessed as <strong class="source-inline">.yaml</strong> files. These two credentials enable a connection between a computer and a DataRobot instance to use the DataRobot Python client.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor181"/>Using the DataRobot Python client </h1>
			<p>The <a id="_idIndexMarker639"/>Python programming language is one of the most popular programming<a id="_idIndexMarker640"/> languages used by data scientists. It is flexible yet powerful. Being able to integrate the AutoML capabilities of DataRobot and utilize the flexibility of Python offers data scientists various benefits, as we mentioned earlier. </p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor182"/>Programming in Python using the Jupyter IDE. </h2>
			<p>Now, let's explore the DataRobot Python client. </p>
			<p>To use<a id="_idIndexMarker641"/> the DataRobot Python client, Python must be version 2.7 or 3.4+. The most up-to-date version of DataRobot must be installed. For the cloud version, the <strong class="source-inline">pip</strong> command will install the most recent version of the <strong class="source-inline">DataRobot</strong> package. On Python, running <strong class="source-inline">!pip install datarobot</strong> should install the <strong class="source-inline">DataRobot</strong> package.</p>
			<p>Having installed the <strong class="source-inline">DataRobot</strong> package, the package has been imported. The <strong class="source-inline">Client</strong> method of the <strong class="source-inline">DataRobot</strong> package provides the much-needed connection to the DataRobot instance. As shown in <em class="italic">Figure 12.3</em>, the basic format for the <strong class="source-inline">Client</strong> method is as follows:</p>
			<p class="source-code">Import DataRobot as dr</p>
			<p class="source-code">dr.Client(endpoint= 'ENTER_THE_ENDPOINT_LINK', token = 'ENTER_YOUR_API TOKEN')</p>
			<p>In terms <a id="_idIndexMarker642"/>of data ingestion, data can be imported from different sources. This process is identical to normal data imports with Python. The local file installation is quite straightforward. Here, all you need is the API key and the file path. <em class="italic">Figure 12.3</em> presents the code for ingesting the automobile dataset. For the JDBC connection, to get data from platforms such as BigQuery and Snowflake, in addition to the API key, the identity of the data source object is required, as well as the user database's credentials – their usernames and passwords. The user database's credentials are provided by their organization's database administrators.</p>
			<p>In this section, we established how to access the credentials necessary to programmatically use DataRobot. We have also imported data programmatically. Naturally, conducting some analysis and modeling comes after ingesting data. In the next section, we will create machine learning models using the Python API.</p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor183"/>Building models programmatically</h1>
			<p>Now<a id="_idIndexMarker643"/> that we have imported the data, we will start building models programmatically. We will look at building the most basic models, then explore how to extract and visualize feature impact, before evaluating the performance of our models. Then, we will create more complex projects. Specifically, we will build one <a id="_idIndexMarker644"/>versus all <strong class="bold">multiclass</strong> classification models <a id="_idIndexMarker645"/>and <strong class="bold">model factories</strong>.</p>
			<p>To create a DataRobot project, we must use the DataRobot <strong class="source-inline">Project.start</strong> method. The basic format for this is importing the necessary libraries (DataRobot, in the following case). Thereafter, the access credentials are presented, as described in the previous section. It is at the point that the <strong class="source-inline">Project</strong> method is called. <strong class="source-inline">project_name</strong>, <strong class="source-inline">sourcedata</strong>, and <strong class="source-inline">target</strong> are the minimal parameters that are required by the <strong class="source-inline">Project</strong> method for projects to be created. The <strong class="source-inline">project_name</strong> parameter tells DataRobot the name to give the created project. <strong class="source-inline">sourcedata</strong> provides information regarding the location of the data that's required to create models. This could be a location or a Python object. Finally, <strong class="source-inline">target</strong> specifies the target variable for the models to be built, as shown here:</p>
			<p class="source-code">import datarobot as dr</p>
			<p class="source-code">dr.Client(endpoint= 'ENTER_THE_ENDPOINT_LINK', token = 'ENTER_YOUR_API TOKEN')</p>
			<p class="source-code">project = dr.Project.start(project_name = 'ENTER_PROJECT_NAME',</p>
			<p class="source-code">sourcedata='ENTER_DATA_LOCATION',</p>
			<p class="source-code">target='ENTER_YOUR_TARGET_VARIABLE')</p>
			<p>The <a id="_idIndexMarker646"/>basic format for creating projects was shown in the preceding section and illustrated in <em class="italic">Figure 12.3</em>. Once the models have been created, we can use the <strong class="source-inline">project.get_models</strong> method to get a list of them. This list of models is presented in order by their validation scores by default. For this example, we will be using the automobile dataset, which we used to build models in <a href="B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104"><em class="italic">Chapter 6</em></a>, <em class="italic">Model Building with DataRobot</em>. The project's name is <strong class="source-inline">autoproject_1</strong>. Here, the file's location is specifically stored in a pandas object called <strong class="source-inline">data</strong>. The target variable is <strong class="source-inline">price</strong>. Note that these parameters are case-sensitive:</p>
			<div>
				<div id="_idContainer229" class="IMG---Figure">
					<img src="image/B17159_12_03.jpg" alt="Figure 12.3 – Programmatically creating DataRobot models and extracting their lists&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – Programmatically creating DataRobot models and extracting their lists</p>
			<p>Once <a id="_idIndexMarker647"/>you've created the model, the <strong class="source-inline">get_models</strong> method is called to list the models. We can see that the best performing model is <strong class="source-inline">Gradient Boosted Greedy Trees Regressor (Least-Square Loss)</strong>. To evaluate this model, we need to extract its ID. To do so, we must create an object, <strong class="source-inline">best_model_01</strong>, to store the best-performing model. This metrics method is then called for this model. As shown in the following screenshot, the cross-validation RMSE for this model is <strong class="source-inline">2107.40</strong>:</p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/B17159_12_04.jpg" alt="Figure 12.4 – Programmatically evaluating DataRobot models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.4 – Programmatically evaluating DataRobot models</p>
			<p>To provide some insight into the price drivers, we need the feature impacts. These can be retrieved through the DataRobot API using the <strong class="source-inline">get_or_feature_impact</strong> method. To <a id="_idIndexMarker648"/>visualize the feature impacts for projects, we must define a function called <strong class="source-inline">plot_FI</strong> that takes in the model's name and chart title as parameters, gets the feature impacts, and then normalizes and plots them using Seaborn's bar plot method. Regarding the <strong class="source-inline">autoproject_1</strong> project, the following screenshot shows how to retrieve and present the feature impacts using the <strong class="source-inline">plot_FI</strong> function:</p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/B17159_12_05.jpg" alt="Figure 12.5 – Defining a function and extracting the feature impacts&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.5 – Defining a function and extracting the feature impacts</p>
			<p>Programmatic access to DataRobot furthers the benefits the platform offers. With programmatic access, you can take advantage of the iterative process within Python, and users can create multiple projects for the same dataset. Now, let's look at two ways to create multiple projects from the same dataset: <strong class="bold">multi-class</strong> classification and <strong class="bold">model factory</strong>.</p>
			<p>Multi-class classification<a id="_idIndexMarker649"/> involves classifying<a id="_idIndexMarker650"/> instances into more than two classes. It is possible to create a single project that classifies rows into either of these classes. Essentially, this is a model that classes rows into one of all the available classes. Another way to approach this problem involves building different models for the different classes. Within this approach, a model is built for each of the classes as a target. You can see how this can be executed using Python's iterative process; that is, by looping through all the target levels. The one versus all method is better for performing classification problems with more than two classes. </p>
			<p>Now, let's <a id="_idIndexMarker651"/>demonstrate how to use the one versus all method on the auto pricing project. Here, we will create price classes using the pandas <strong class="bold">quantile-base discretization</strong> function, <strong class="source-inline">qcut</strong>. <strong class="source-inline">qcut</strong> helps divide data into similarly<a id="_idIndexMarker652"/> sized bins. Using this function, we can divide our data into price classes – low to high. The following screenshot shows this price discretizing process and checking the distribution of cases across the classes:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="image/B17159_12_06.jpg" alt="Figure 12.6 – Price discretization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.6 – Price discretization</p>
			<p>Having created the classes, to allow for<a id="_idIndexMarker653"/> data <strong class="bold">leakages</strong>, we will drop the initial price variable. We will write a loop that builds models for each of the price classes. Perform the following steps:</p>
			<ol>
				<li>Turn the <strong class="source-inline">price_class</strong> variable into dummy variables.</li>
				<li>For each iteration, create a DataRobot project after a dummified price class name.</li>
				<li>For each iteration, we drop the <strong class="source-inline">price_class</strong> dummy level being modeled. This ensures that there are no leakages.</li>
				<li>For <a id="_idIndexMarker654"/>each iteration, we must build the models for a target variable dummy.</li>
				<li>After creating the projects, the top-performing model for each project is selected and stored in a dictionary:</li>
			</ol>
			<div>
				<div id="_idContainer233" class="IMG---Figure">
					<img src="image/B17159_12_07.jpg" alt="Figure 12.7 – Creating a one versus all classification suite of projects&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.7 – Creating a one versus all classification suite of projects</p>
			<p>This process involves creating projects with a suite of models with targets iterating through all the price classes. After creating the projects, the best model for each target class is selected using an iteration of all the projects with names starting with <strong class="source-inline">Auto</strong>, and then the top-performing model for each project. These best models are placed in a dictionary. </p>
			<p>It is sometimes recommended, if not ideal, to create different projects with a subset of the data. After selecting all the cases for the target variable, you must create a random subset of the data for each project creation iteration. In the auto pricing case, however, we were unable to explore this as the out-sample size was limiting.</p>
			<p>A <strong class="bold">model factory</strong> is a <a id="_idIndexMarker655"/>multi-level modeling system where a <a id="_idIndexMarker656"/>model is developed for a subgroup of cases. For instance, the price of a car might be heavily determined by its fuel type in that it becomes beneficial to build different models for each fuel type within the same system. Programmatically building model factories is somewhat like the one versus all approach to classification. Instead of projects being iteratively created for each of the unique target variable levels, as with the one versus all process, the model factory involves building models for each level of a predictor variable. The key steps in building the model factory process, which involves iterating through each of the unique variable levels, are as follows (<strong class="source-inline">fuel-type</strong>):</p>
			<ol>
				<li value="1">First, create and store a project.</li>
				<li>Select cases for the target variable (the influencer of interest). In this case, the variable is <strong class="source-inline">fuel-type</strong>. Here, this variable is selected, and differing levels of this variable are used to create DataRobot projects. In simple terms, this step involves, for instance, selecting all the rows with <strong class="source-inline">fuel-type</strong> set to <strong class="source-inline">gas</strong> as a subgroup.</li>
				<li>If necessary, define the evaluation metric. Here, we can alter aspects of the advanced options we encountered in <a href="B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104"><em class="italic">Chapter 6</em></a>, <em class="italic">Model Building with DataRobot</em>. Other advance options can be selected and altered.</li>
				<li>If necessary, set a data limit that a class will be deselected for (for instance, if the number of rows is less than 20 for that class). The importance of this step lies in the fact that some variable levels could have very low occurrences, so the sample size within the subgroup is small. Therefore, creating models out of these becomes a challenge. This step becomes the best place to drop such variable levels using the count of cases within the subgroup.</li>
				<li>All the models from all the projects are selected and stored in a dictionary.</li>
			</ol>
			<p>Some of these steps are evident in creating a model factor for the auto pricing problem (see <em class="italic">Figure 12.8</em>). Here, <strong class="source-inline">fuel-type</strong> is selected as the feature that projects are created on. In this case, only two projects are created: one for gas automobiles and another <a id="_idIndexMarker657"/>for diesel ones. Now that we've created the models, the next step is to collect the best-performing models for each <strong class="source-inline">fuel-type</strong>:</p>
			<div>
				<div id="_idContainer234" class="IMG---Figure">
					<img src="image/B17159_12_08.jpg" alt="Figure 12.8 – Creating model factories&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.8 – Creating model factories</p>
			<p>The efficacy <a id="_idIndexMarker658"/>of using one versus all multiclass classification models and model factories lies in their ability to fit models to each level of the target variable. This happens in an automated fashion and considers the sample validation, all the preprocessing steps, and the model training process. When data cardinality and volume are high, these approaches would mostly outperform typical modeling.</p>
			<p>For the model factory, multiple projects are created for the different levels of the feature of interest. To evaluate this, the best-performing model for each project is selected from the dictionary for all projects. This set of best models from all the projects is stored in another dictionary object. A <strong class="source-inline">for</strong> loop is then run across all the models of the dictionary to extract the performance of the model, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="image/B17159_12_09.jpg" alt="Figure 12.9 – Evaluating the performance of models with a model factory&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.9 – Evaluating the performance of models with a model factory</p>
			<p>Improved<a id="_idIndexMarker659"/> model performance is only one of the reasons you should use the one versus all multiclass classification models, as well as model factors. Sometimes, understanding the drivers is equally as important. Visualizing the feature importance for the different fuel types could present an interesting contrast in drivers. This means that different factors affect the prices of different fuel types. This could have a bearing on strategic decisions. As shown in the following screenshot, the Python API can be used to plot the feature impacts by leveraging chart functions from Seaborn and Matplotlib:</p>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="image/B17159_12_10.jpg" alt="Figure 12.10 – Feature impacts for the differing diesel and gas automobiles&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.10 – Feature impacts for the differing diesel and gas automobiles</p>
			<p>As we <a id="_idIndexMarker660"/>can see, there are some differences in the feature impacts for the automobile fuel types. While <strong class="source-inline">curb-weight</strong> seems to be an important driver, its effect is relatively more important for diesel vehicles. Similarly, for gas cars, the power that's generated by these automobiles, as typified by the <strong class="source-inline">engine_size</strong> and <strong class="source-inline">horsepower</strong> features, carries more importance in determining price than those of diesel cars. You can already see the effect such preliminary findings could have on decisions and how this could be applied to other commercial cases. Using feature importance to examine multiple models can also be applied in the case of one versus all classification problems. </p>
			<p>In this section, we created basic DataRobot projects using the Python API. After, we solved more complex problems by using multiple projects within a system. There, we created one versus all projects to solve multiclass classification problems and model factories to solve multi-level problems involving subgroups. We also explored feature impact and model evaluation. Having programmatically created models, we will now learn how to make predictions using these models. Specifically, we will learn how to deploy models, make predictions, extract explanations from models, and score large datasets through parallelization.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor184"/>Making predictions programmatically</h1>
			<p>The <a id="_idIndexMarker661"/>possibilities that programmatically using DataRobot presents are enormous. By using its API, models can be deployed and predictions can be made against them. Before making programmatical predictions within the production environment, models need to be deployed. DataRobot models are deployed using Portable Prediction Servers. These are Docker containers that can host machine learning models, which serve predictions and prediction explanations through a REST API. </p>
			<p>To deploy models, we can use the DataRobot package's <strong class="source-inline">deployment</strong> method. Here, we must provide a description, the DataRobot model's ID, as well as its label to create the deployments. A typical Python deployment script follows this format: </p>
			<p class="source-code">deployment = dr.Deployment.create_from_learning_model(</p>
			<p class="source-code">MODEL_ID, label='DEPLOYMENT_LABEL', description='DEPLOYMENT_DESCRIPTION',</p>
			<p class="source-code">    default_prediction_server_id=PREDICTION_SERVER_ID)</p>
			<p class="source-code">deployment</p>
			<p>As per this approach, the following screenshot shows how <strong class="source-inline">autoproject_1</strong>, which we created in the <em class="italic">Building models programmatically</em> section, can be deployed. Here, the model ID is <strong class="source-inline">best_model_1</strong>. We will label <strong class="source-inline">AutoBase Deployment</strong> with a description of <strong class="source-inline">Base Automobile Price Deployment</strong>:</p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/B17159_12_11.jpg" alt="Figure 12.11 – Deploying a model programmatically&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.11 – Deploying a model programmatically</p>
			<p>The deployment process can be iterated to enable those of more complex projects. For instance, with model factories, irrespective of the number of levels the differentiating variable has, with a single <strong class="source-inline">for</strong> loop, all the best models can be deployed to DataRobot. For each of the best models, a deployment is created, which is then used to score new data. The <a id="_idIndexMarker662"/>script for deploying the model factory for the automobile project, along with the fuel type as its differentiating variable, is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="image/B17159_12_12.jpg" alt="Figure 12.12 – Deploying models from a model factory&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.12 – Deploying models from a model factory</p>
			<p>Having deployed the models, predictions can be made against them. To make simple predictions within the development environment, we can use the <strong class="source-inline">DataRobot BatchPredictionJob.score_to_file</strong> method. To make predictions, this method requires the model ID, prediction data, and the location where the scored data will be stored. Here, we will use <strong class="source-inline">best_model_1</strong> to score the same model we used to develop the model, the <strong class="source-inline">df</strong> data object, and the location path, which specifies the prediction file path as <strong class="source-inline">./pred.csv</strong>. The <strong class="source-inline">passthrough_columns_set</strong> parameter specifies the columns from the original dataset that will be included in the predictions. Since this is set to <strong class="source-inline">'all'</strong>, all the columns are returned, as shown here: </p>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/B17159_12_13.jpg" alt="Figure 12.13 – Simple programmatic prediction&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.13 – Simple programmatic prediction</p>
			<p>These predictions comprise all the columns from the initial dataset, in addition to the predicted prices. There are cases where it is ideal to include rationales behind predictions. In such cases, the <strong class="source-inline">max_explanations</strong> parameter should be included in the job's configuration. This parameter sets the highest number of explanations to be provided for every data row.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor185"/>Summary</h1>
			<p>DataRobot provides us with a unique capability to rapidly develop models. With this platform, data scientists can combine the benefits of DataRobot and the flexibilities of open programming. In this chapter, we explored ways to access the credentials needed to programmatically use DataRobot. Using the Python client, we demonstrated ways in which data can be ingested and how basic projects can be created. We started building models for more complex problems. We created model factories as well as one versus all models. Finally, we demonstrated how models can be deployed and used to score data.</p>
			<p>One of the key advantages of programmatically using DataRobot is the ability to ingest data from numerous sources, score them, and store them in the relevant sources. This makes it possible to carry out end-to-end dataset scoring. It becomes possible for a system to be set up to score models periodically. With this comes numerous data quality and model monitoring concerns. The next chapter will focus on how to control the quality of the models and data on the DataRobot platform, as well as using the Python API.</p>
		</div>
	</body></html>