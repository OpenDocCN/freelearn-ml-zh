- en: '*Chapter 3*: Exploring Regression with TPOT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you'll get hands-on experience with automated regression modeling
    through three datasets. You will learn how to handle regression tasks with TPOT
    in an automated manner with tons of practical examples, tips, and advice.
  prefs: []
  type: TYPE_NORMAL
- en: We will go through essential topics such as dataset loading, exploratory data
    analysis, and basic data preparation first. Then, we'll get our hands dirty with
    TPOT. You will learn how to train models in an automated way and how to evaluate
    those models.
  prefs: []
  type: TYPE_NORMAL
- en: Before training models automatically, we will see how good performance can be
    obtained with basic models, such as linear regression. These models will serve
    as a baseline that TPOT needs to outperform.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying automated regression modeling to the fish market dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying automated regression modeling to the insurance dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying automated regression modeling to the vehicle dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this chapter, you will need a computer with Python and TPOT installed.
    The previous chapter demonstrated how to set up the environment from scratch for
    both standalone Python installation and installation through Anaconda. Refer to
    [*Chapter 2*](B16954_02_Final_SK_ePub.xhtml#_idTextAnchor036), *Deep Dive into
    TPOT*, for detailed instructions on environment setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the source code and datasets for this chapter here: [https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter03](https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter03)'
  prefs: []
  type: TYPE_NORMAL
- en: Applying automated regression modeling to the fish market dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section demonstrates how to apply machine learning automation with TPOT
    to a regression dataset. The section uses the fish market dataset ([https://www.kaggle.com/aungpyaeap/fish-market](https://www.kaggle.com/aungpyaeap/fish-market))
    for exploration and regression modeling. The goal is to predict the weight of
    a fish. You will learn how to load the dataset, visualize it, adequately prepare
    it, and how to find the best machine learning pipeline with TPOT:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is to load in the required libraries and load in the
    dataset. With regards to the libraries, you''ll need `numpy`, `pandas`, `m``atplotlib`,
    and `seaborn`. Additionally, the `rcParams` module is imported with `matplotlib`
    to tweak the plot stylings a bit. You can find the code for this step in the following
    block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s how the first couple of rows look (the result from calling the `head()`
    method):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.1 – First five rows of the fish market dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_001.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.1 – First five rows of the fish market dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Exploratory data analysis comes in next. It''s not a hard requirement for using
    TPOT, but you should always be aware of how your data looks. The first thing of
    interest is missing values. Here''s how to check for them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here''s the corresponding output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Count of missing values per column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_002.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.2 – Count of missing values per column
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, there are no missing values. This makes the data preparation
    process much easier and shorter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to check how the target variable is distributed. For this
    dataset, we are trying to predict `Weight`. Here''s the code for drawing a simple
    histogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here''s how the histogram looks:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Histogram of the target variable (Weight)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.3 – Histogram of the target variable (Weight)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Most of the fish are light, but there are a couple of heavy ones present. Let's
    explore species further to get a better grasp.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The following code prints how many instances of a specific species there are
    (the number and percentage of the total), and also prints average and standard
    deviation for every attribute. To be more precise, a subset of the original dataset
    is kept where the species equals the specified species. Afterward, the number
    of records, total percentage, mean, and standard deviation are printed for every
    column in the subset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This function is then called for every unique species:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the corresponding output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Feature exploration for every fish species'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.4 – Feature exploration for every fish species
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, let''s check for correlation between attributes. Correlation can be
    calculated only for numerical attributes. The following snippet shows you how
    to visualize a correlation matrix with the `seaborn` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the correlation matrix:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Correlation matrix of features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.5 – Correlation matrix of features
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can do more in the exploratory data analysis process, but we'll stop here.
    This book shows you how to build automated models with TPOT, so we should spend
    most of the time there.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There''s one step left to do before modeling, and that is data preparation.
    We can''t pass non-numerical attributes to the pipeline optimizer. We''ll convert
    them to dummy variables for simplicity''s sake and merge them with the original
    data afterward. Here''s the code for doing so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here''s how the dataset looks now:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.6 – First five rows of the fish market dataset after data preparation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.6 – First five rows of the fish market dataset after data preparation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, we deleted the `Species` column because it's not needed anymore.
    Let's begin with the modeling next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To start, we need to make a couple of imports and decide on the scoring strategy.
    TPOT comes with a couple of regression scoring metrics. The default one is `neg_mean_squared_error`.
    We can''t escape the negative metric, but we can at least make it be in the same
    units as the target variable is. It makes no sense to predict weight and keep
    track of errors in weight squared. That''s where **Root Mean Squared Error** (**RMSE**)
    comes into play. It is a simple metric that calculates the square root of the
    previously discussed mean squared error. Due to the square root operations, we''re
    tracking errors in the original units (weight) instead of squared units (weight
    squared). We will define it with the help of lambda functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next on the requirement list is the train test split. We will keep 75% of the
    data for training and evaluate on the rest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s how many instances are in the train and test sets, respectively:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Number of instances in the training and test sets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.7 – Number of instances in the training and test sets
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let''s make a model with the linear regression algorithm. This model
    is just a baseline that TPOT needs to outperform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the corresponding RMSE value for linear regression on the test set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.8 – RMSE score for the linear regression model (baseline)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_008.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.8 – RMSE score for the linear regression model (baseline)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The baseline model is wrong by 82 units of weight on average. Not bad, considering
    we have weights up to 1,500\.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let''s fit a TPOT pipeline optimization model. We will use our RMSE scorer
    and perform the optimization for 10 minutes. You can optimize for more time, but
    10 minutes should outperform the baseline model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the optimization has finished, here''s the output that''s shown in the
    console:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.9 – TPOT regressor output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.9 – TPOT regressor output
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here''s how to obtain the RMSE score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here is the corresponding output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.10 – RMSE score for TPOT optimized pipeline model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.10 – RMSE score for TPOT optimized pipeline model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Don''t worry about the minus sign before the number. The actual RMSE is 73.35
    units of weight. The TPOT model outperformed the baseline one. That''s all you
    need to know. TPOT gives us access to the best pipeline through the `fitted_pipeline_`
    attribute. Here''s how it looks:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Full TPOT pipeline'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.11 – Full TPOT pipeline
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As a final step, we can export the pipeline to a Python file. Here''s how:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s what the file looks like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Source code of the TPOT pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_03_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Source code of the TPOT pipeline
  prefs: []
  type: TYPE_NORMAL
- en: You can now use this file to make predictions on new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you've built your first automated machine learning pipeline
    with TPOT on a simple dataset. Most of the time, in practice, the steps you take
    will look similar. It's the data cleaning and preparation where things differ.
    Always make sure to prepare your dataset adequately before passing it to TPOT.
    Sure, TPOT does many things for you, but it can't turn garbage data into a usable
    model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you'll see how to apply TPOT to the medical insurance dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Applying automated regression modeling to the insurance dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section demonstrates how to apply an automated machine learning solution
    to a slightly more complicated dataset. You will use the medical insurance cost
    dataset ([https://www.kaggle.com/mirichoi0218/insurance](https://www.kaggle.com/mirichoi0218/insurance))
    to predict how much insurance will cost based on a couple of predictor variables.
    You will learn how to load the dataset, perform exploratory data analysis, how
    to prepare it, and how to find the best machine learning pipeline with TPOT:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the previous example, the first step is to load in the libraries and
    the dataset. We''ll need `numpy`, `pandas`, `m``atplotlib`, and `seaborn` to start
    with the analysis. Here''s how to import the libraries and load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first five rows are shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13 – First five rows of the insurance dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.13 – First five rows of the insurance dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We''ll continue with the exploratory data analysis. As with the previous example,
    we''ll first check for the number of missing values. Here''s the code for doing
    so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure shows counts of missing values per column:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Missing value counts per column for the insurance dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_014.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.14 – Missing value counts per column for the insurance dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, there are no missing values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We''re trying to predict the `charges` column with this dataset, so let''s
    quickly check what type of values we can expect there. A histogram seems like
    an easy enough option. Here''s the code needed for drawing one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here''s the resulting histogram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Distribution of the target variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_015.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.15 – Distribution of the target variable
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, values even go above $60,000.00\. Most of them are lower, so it will be
    interesting to see how the model will handle it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's dive deeper into the analysis and explore other variables. The goal is
    to see the average insurance costs for every categorical variable segment. We'll
    use the median as an average value, as it's less prone to outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The easiest way to approach this analysis is to make a function that makes
    a bar chart for the specified column. The following function will come in handy
    for this example and many others in the future. It calculates a median from a
    grouped dataset and visualizes a bar chart with a title, labels, a legend, and
    text on top of the bars. You can use this function in general to visualize medians
    of some variable after a grouping operation is performed. It''s best suited for
    categorical variables:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now use this function to visualize the median insurance cost for smokers
    and non-smokers. Here''s the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here''s the corresponding visualization:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Median insurance charges for smokers and non-smokers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_016.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.16 – Median insurance charges for smokers and non-smokers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, smokers pay an insurance fee several times higher than non-smokers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s make a similar-looking visualization for comparing median insurance
    costs between genders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see the visualization here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Median insurance charges between genders'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_017.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.17 – Median insurance charges between genders
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Not much of a difference here.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'But what will happen if we compare median insurance costs by the number of
    children? The following code snippet does just that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s how the costs are distributed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Median insurance charges by number of children'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_018.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.18 – Median insurance charges by number of children
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The insurance costs seem to go up until the fifth child. Maybe there aren't
    that many families with five children. Can you confirm that on your own?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'What about the region? Here''s the code for visualizing median insurance costs
    by region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The cost distribution per region is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Median insurance charges by region'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_019.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.19 – Median insurance charges by region
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The values don't differ that much.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We've made a decent amount of visualizations and explored the dataset. It's
    now time to prepare it and apply machine learning models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are a couple of things we need to do for this dataset to be machine learning
    ready. First, we'll have to remap string values to integers for the columns `sex`
    and `smoker`. Then, we'll need to create dummy variables for the `region` column.
    This step is necessary because TPOT can't understand raw textual data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s the code snippet that does the necessary preparation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calling the `head()` function results in the dataset shown in the following
    figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Insurance dataset after preparation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_020.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.20 – Insurance dataset after preparation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The dataset is now ready for predictive modeling. Before we do so, let''s check
    for variable correlations with the target variable. The following snippet draws
    the correlation matrix with annotations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The corresponding correlation matrix is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Insurance dataset correlation matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_021.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.21 – Insurance dataset correlation matrix
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next stop – predictive modeling.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As before, the first step is to make a train/test split. The following code
    snippet shows you how to do that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The number of training and testing instances is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Number of instances in train and test sets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_022.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.22 – Number of instances in train and test sets
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We''ll first make a baseline model with a linear regression algorithm. It will
    serve as something TPOT must outperform. You''ll find a code snippet for training
    a baseline model here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Coefficient of determination (R2) and root mean squared error (RMSE) values
    are shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.23 – R2 and RMSE for the linear regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_023.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.23 – R2 and RMSE for the linear regression model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On average, a simple linear regression model is wrong by $5,926.02\. This simple
    model captures 77% of the variance in the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can further explore the linear regression model's feature importance by examining
    the assigned weights (coefficients).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet prints the variable name and its corresponding coefficient:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Coefficients of a linear regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_024.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.24 – Coefficients of a linear regression model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the column with the largest coefficient is `smoker`. That makes
    sense, as it confirms our visualization made in the exploratory data analysis
    phase.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It's now time to bring in the big guns. We'll use the TPOT library to produce
    an automated machine learning pipeline. We'll optimize the pipeline for R2 score
    this time, but feel free to stick with RMSE or any other metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet imports the TPOT library, instantiates it, and fits
    the pipeline:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After 10 minutes, you should see the following output in your notebook:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.25 – TPOT score per generation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_025.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.25 – TPOT score per generation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The score on the training set started to increase in the last couple of generations.
    You'd likely get a slightly better model if you gave TPOT more time to train.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The R2 score on the test set can be obtained with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.26 – TPOT R2 score on the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_026.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.26 – TPOT R2 score on the test set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can obtain R2 and RMSE values for the test set manually. The following
    code snippet shows you how:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The corresponding scores are shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.27 – TPOT R2 and RMSE scores on the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_027.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.27 – TPOT R2 and RMSE scores on the test set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As the last step, we''ll export the optimized pipeline to a Python file. The
    following code snippet does it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Python code for the optimized pipeline is shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.28 – TPOT optimized pipeline for the insurance dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_03_028.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.28 – TPOT optimized pipeline for the insurance dataset
  prefs: []
  type: TYPE_NORMAL
- en: You can now use this file to make predictions on new, unseen data. It would
    be best to leave the pipeline to perform the optimization for as long as needed,
    but even 10 minutes was enough to produce good-quality models.
  prefs: []
  type: TYPE_NORMAL
- en: This section showed you how to build automated pipelines optimized for different
    metrics and with a bit more verbose output printed to the console. As you can
    see, the code for optimization is more or less identical. It's the data preparation
    that changes drastically from project to project, and that's where you'll spend
    most of your time.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you'll see how to apply TPOT to the vehicle dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Applying automated regression modeling to the vehicle dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section shows how to develop an automated machine learning model on the
    most complex dataset thus far. You will use the vehicle dataset ([https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho](https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho)),
    so download it if you haven't already. The goal is to predict the selling price
    based on the various predictors, such as year made and kilometers driven.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we won''t focus on exploratory data analysis. You can do that on
    your own if you''ve followed the last two examples. Instead, we''ll concentrate
    on dataset preparation and model training. There''s a lot of work required to
    transform this dataset into something ready for machine learning, so let''s get
    started right away:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, the first step is to load in the libraries and the dataset. The
    requirements are the same as with previous examples. You''ll need `numpy`, `p``andas`,
    `matplotlib`, and `seaborn`. Here''s how to import the libraries and load the
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calling the `head()` function displays the first five rows. You can see how
    they look in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.29 – First five rows of the vehicle dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_029.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.29 – First five rows of the vehicle dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The dataset has a lot of columns, and not all of them are shown in *Figure
    3.29*. The next step in the data preparation phase is to check for missing values.
    The following code snippet does that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.30 – Count of missing values for the vehicle dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_030.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.30 – Count of missing values for the vehicle dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Some of the values are missing, and we'll address this issue with the simplest
    approach – by removing them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Removing missing values might not always be the best option. You should always
    investigate why the values are missing and if they can (or should) be somehow
    filled. This book focuses on machine learning automation, so we won't do that
    here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s how you can drop the missing values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Executing the preceding code results in the following count:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.31 – Removing missing values from the vehicle dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_031.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.31 – Removing missing values from the vehicle dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are no missing values now, but that doesn''t mean we''re done with data
    preparation. Here''s the list of steps required to make this dataset suitable
    for machine learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the `transmission` column to an integer – 1 if *manual*, 0 otherwise.
    Also, rename the column to `is_manual`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Remap the `owner` column to integers. Check the `remap_owner()` function for
    further clarifications.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract car brand, mileage, engine, and max power from the corresponding attributes.
    The value of interest for all of the mentioned attributes is everything before
    the first space.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create dummy variables from the attributes `name`, `fuel`, and `seller_type`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Concatenate the original dataset with dummy variables and drop unnecessary attributes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for the `remap_owner()` function:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'And here is the code for performing all of the mentioned transformations:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'After applying the transformations, the dataset looks like this:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.32 – Prepared vehicle dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_03_032.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.32 – Prepared vehicle dataset
  prefs: []
  type: TYPE_NORMAL
- en: Data in this format can be passed to a machine learning algorithm. Let's do
    that next.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, we''ll start with the train test split. The following code snippet
    shows you how to perform it on this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see how many instances are in both sets in *Figure 3.33*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.33 – Number of instances in train and test sets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_033.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.33 – Number of instances in train and test sets
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, this is a much larger dataset than we had before.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We won''t use your standard metrics for evaluating regression models (R2 and
    RMSE) this time. We''ll use `scikit-learn` library, so we''ll have to implement
    it manually. Here''s how:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And now it''s time to make a baseline model. Once again, it will be a linear
    regression model, evaluated on the test set with R2 and MAPE metrics. Here''s
    the code for implementing the baseline model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The corresponding results are shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.34 – R2 and MAPE for the baseline model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_034.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.34 – R2 and MAPE for the baseline model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On average, the baseline model is wrong by 43%. It's a lot, but we have to start
    somewhere.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s take a look at the linear regression model coefficient to determine
    which features are important. Here''s the code for obtaining coefficients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here are the coefficients:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.35 – Baseline model coefficients'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_035.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.35 – Baseline model coefficients
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Just take a moment to appreciate how interpretable this is. The higher the year,
    the newer the car is, which results in a higher price. The more kilometers the
    vehicle has driven, the more the price decreases. It also looks like cars with
    automatic transmissions cost more. You get the point. Interpretability is something
    that linear regression offers. But it lacks accuracy. That's what TPOT will improve.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s fit a TPOT model next and optimize it for MAPE score. We''ll train the
    model for 10 minutes on every available CPU core (indicated by `n_jobs=-1`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output you''ll see after 10 minutes is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.36 – Output of a TPOT optimization process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_036.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.36 – Output of a TPOT optimization process
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It looks like 10 minutes wasn't nearly enough for TPOT to give its best.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The resulting pipeline is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.37 – Best fitted pipeline after 10 minutes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_03_037.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.37 – Best fitted pipeline after 10 minutes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'And now the moment of truth – did the MAPE decrease? Here''s the code to find
    out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.38 – R2 and MAPE for the TPOT optimized model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_03_038.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.38 – R2 and MAPE for the TPOT optimized model
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, TPOT decreased the error significantly and increased the goodness
    of fit (R2) simultaneously. Just as expected.
  prefs: []
  type: TYPE_NORMAL
- en: The final code-along section showed you how easy it is to train automated models
    on a more complex dataset. The procedure is more or less identical, depending
    on the metric you're optimizing for, but it's the data preparation phase that
    makes all the difference.
  prefs: []
  type: TYPE_NORMAL
- en: If you spend more time preparing and analyzing the data, and maybe removing
    some noisy data, you will get better results, guaranteed! That's mainly the case
    when a lot of columns contain text data. A lot of features can be extracted from
    there.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was the first purely hands-on chapter in the book. You've connected the
    theory from the previous chapters with practice. You've built not one, but three
    fully automated machine learning models. Without any kind of doubt, you should
    now be able to use TPOT to solve any type of regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: As with most things in data science and machine learning, 90% of the work boils
    down to data preparation. TPOT can make this percentage even higher because less
    time is spent designing and tweaking the models. Use this extra time wisely, and
    get yourself fully acquainted with the dataset. There's no way around it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you'll see how to build automated machine learning models
    for classification datasets. That chapter will also be entirely hands-on. Later,
    in [*Chapter 5*](B16954_05_Final_SK_ePub.xhtml#_idTextAnchor065), *Parallel Training
    with TPOT and Dask*, we'll combine both theory and practice.
  prefs: []
  type: TYPE_NORMAL
- en: Q&A
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which type of data visualization lets you explore the distribution of a continuous
    variable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain R2, RMSE, and MAPE metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you use a custom scoring function with TPOT? If yes, how?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is it essential to build baseline models first? Which algorithm is considered
    as a "baseline" for regression tasks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do the coefficients of a linear regression model tell you?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you use all CPU cores when training TPOT models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you use TPOT to obtain the Python code of the best pipeline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
