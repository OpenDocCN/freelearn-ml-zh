- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hybrid Deep Learning Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will talk about some of the hybrid deep learning techniques
    that combine the data-level ([*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*) and algorithm-level ([*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep Learning Techniques*) methods in some ways. This chapter
    contains some recent and more advanced techniques that can be challenging to implement,
    so it is recommended to have a good understanding of the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin with an introduction to graph machine learning, clarifying how
    graph models exploit relationships within data to boost performance, especially
    for minority classes. Through a side-by-side comparison of a **Graph Convolutional
    Network** (**GCN**), XGBoost, and MLP models, using an imbalanced social network
    dataset, we will highlight the superior performance of the GCN.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to explore strategies to tackle class imbalance in deep learning,
    examining techniques that manipulate data distribution and prioritize challenging
    examples. We will also go over techniques called hard example mining and minority
    class incremental rectification, which focus on improving model performance through
    prioritization of difficult instances and iterative enhancement of minority class
    representation, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: While a significant portion of our discussion will revolve around image datasets,
    notably the imbalanced MNIST, it’s crucial to understand the broader applicability
    of these techniques. For instance, our deep dive into graph machine learning won’t
    rely on MNIST. Instead, we’ll switch gears to a more realistic dataset from Facebook,
    offering a fresh perspective on handling imbalances in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Graph machine learning for imbalanced data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard example mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minority class incremental rectification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we will become familiar with some hybrid methods,
    enabling us to understand the core principles behind more complex techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `numpy`, `pandas`, `sklearn`, and `torch`. For graph machine learning, we will
    use the `torch_geometric` library as well. The code and notebooks for this chapter
    are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09).
    You can open the GitHub notebook using Google Colab by clicking on the **Open
    in Colab** icon at the top of the chapter’s notebook or by launching it from [https://colab.research.google.com](https://colab.research.google.com),
    using the GitHub URL of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Using graph machine learning for imbalanced data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see when graphs can be useful tools in machine learning,
    when to use graph ML models in general, and how they can be helpful on certain
    kinds of imbalanced datasets. We’ll also be exploring how graph ML models can
    outperform classical models such as XGBoost on certain imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs are incredibly versatile data structures that can represent complex relationships
    and structures, from social networks and web pages (think of links as edges) to
    molecules in chemistry (consider atoms as nodes and the bonds between them as
    edges) and various other domains. Graph models allow us to represent the relationships
    in data, which can be helpful to make predictions and gain insights, even for
    problems where the relationships are not explicitly defined.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graphs are the foundation of graph ML, so it’s important to understand them
    first. In the context of computer science, a graph is a collection of nodes (or
    vertices) and edges. Nodes represent entities, and edges represent relationships
    or interactions between these entities. For example, in a social network, each
    person can be a node, and the friendship between two people can be an edge.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs can be either directed (edges have a direction) or undirected (edges
    do not have a direction). They can also be weighted (edges have a value) or unweighted
    (edges do not have a value).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.1* shows a sample tabular dataset on the left and its corresponding
    graph representation on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Tabular data (left) contrasted with its visual graph representation
    (right)
  prefs: []
  type: TYPE_NORMAL
- en: The graph representation on the right emphasizes the relationships between various
    entities. In the tabular representation on the left, devices and their IP addresses
    are listed with connection details along with network bandwidth. The graphical
    representation on the right visually represents these connections, allowing easier
    comprehension of the network topology. Devices are nodes, connections are edges
    with bandwidth are the weights. Graphs provide a clearer view of interrelationships
    than tables, emphasizing network design insights.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will get an overview of how ML can be applied to
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Graph machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Graph Machine Learning** (**GML**) is a set of techniques that use the structure
    of a graph to extract features and make predictions. GML algorithms can leverage
    the rich information contained in the graph structure, such as the connections
    between nodes and the patterns of these connections, to improve model performance,
    especially on imbalanced data.'
  prefs: []
  type: TYPE_NORMAL
- en: Two popular neural network GML algorithms are GCNs and **Graph Attention Networks**
    (**GATs**). Both algorithms use the graph structure to aggregate information from
    a node’s neighborhood. However, they differ in how they weigh the importance of
    a node’s neighbors. GCN gives equal weight to all neighbors, while GAT uses attention
    mechanisms to assign different weights to different neighbors. We will limit our
    discussion to GCNs only in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with imbalanced data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In ML, when one class significantly outnumbers the others, the model may become
    biased toward the majority class, leading to poor performance on the minority
    class. This is problematic because, often, the minority class is the one of interest.
    For example, in fraud detection, the number of non-fraud cases significantly outnumbers
    the fraud cases, but it’s the fraud cases that we’re interested in detecting.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of GML, the structure of the graph can provide additional information
    that can help mitigate the effects of data imbalance. For example, minority class
    nodes might be more closely connected to each other than to majority class nodes.
    GML algorithms can leverage this structure to improve the performance of the minority
    class.
  prefs: []
  type: TYPE_NORMAL
- en: GCNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will briefly discuss the key ideas behind what GCNs are and how they work.
  prefs: []
  type: TYPE_NORMAL
- en: GCNs offer a unique method of processing structured data. Unlike standard neural
    networks that assume independent and identically distributed data, GCNs can operate
    over graph data, capturing dependencies and connections between nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The essence of GCNs is message passing, which can be broken down as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node messaging**: Each node in the graph sends out and receives messages
    through its edges to and from its neighbors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation**: Nodes aggregate these messages to gain a broader understanding
    of these local neighborhoods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In GCNs, full feature vectors are passed around instead of just the labels of
    the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of a GCN layer as a transformation step. The primary operations can be
    viewed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregate neighbors**: Nodes pull features from their neighbors, leading
    to an aggregation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural network transformation**: The aggregated feature set from the previous
    step then undergoes transformation via the neural network layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore GCNs using the example of photos on Facebook. Users upload photos,
    and our objective is to categorize these images as either spam or non-spam. This
    categorization is based on the image content as well as the IP address or user
    ID.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine we have a graph where each node is a Facebook photo, and two photos
    are connected if they were uploaded using either the same IP address or the same
    account.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we want to use the actual content of the photo (possibly a feature
    vector from a pre-trained CNN or some metadata) as a node attribute. Let’s assume
    that we have a 5-dimensional vector representing each photo’s features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – An image with a 5-dimensional feature embedding
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – graph creation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will create a graph where every vertex symbolizes a Facebook image. We will
    establish a link between two images if they were uploaded via an identical IP
    address or user ID.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – A diagram with images connected to each other if they share the
    IP or user ID
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – image representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Represent each image using a 5-dimensional vector. This can be accomplished
    by using image metadata, features derived from trained neural networks, or other
    techniques suitable for image data (*Figure 9**.2*).
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – a singular-layer GCN for image analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When a specific image is passed through a single-layer GCN, here is what will
    happen:'
  prefs: []
  type: TYPE_NORMAL
- en: We aggregate all the neighboring image’s feature vectors. The neighbors are
    images with matching IP addresses or user IDs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An average function is used to combine vectors. Let’s call the combined vector
    the neighborhood average vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The neighborhood average vector is multiplied using a weight matrix (of, say,
    size 5x1, as shown in *Figure 9**.4*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, an activation function is applied to the outcome to get a single value,
    which suggests the likelihood of the image being spam.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17259_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – How a single GCN layer works
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – a dual-layer GCN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multilayer GCNs, like traditional deep neural networks, can be stacked into
    multiple layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Raw node features feed into the first layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subsequent layer’s input is the previous layer’s output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With each added layer, GCNs grasp more extended neighborhood information. For
    instance, in a two-layer GCN, information can ripple two hops away in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: With a foundational understanding of graph ML and GCNs in place, we’re set to
    explore a case study. We’ll compare the performance of a graph model to other
    models, including a classical ML model, on an imbalanced graph dataset. Our aim
    is to determine whether graph models can outperform other models by leveraging
    the relationships between graph structures.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – the performance of XGBoost, MLP, and a GCN on an imbalanced dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the Facebook Page-Page dataset from the **PyTorch Geometric** (**PyG**)
    library, designed to create and train deep learning models on graphs and other
    irregular structures. This dataset comprises a large collection of social networks
    from Facebook, where nodes represent official Facebook pages and edges signify
    reciprocal likes between them. Each node is labeled with one of four categories:
    Politicians, Governmental Organizations, Television Shows, or Companies. The task
    is to predict these categories based on the node characteristics, which are derived
    from descriptions provided by the page owners.'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset serves as a challenging benchmark for graph neural network models
    due to its size and complexity. It was collected via the Facebook Graph API in
    November 2017 and focuses on multi-class node classification within the four aforementioned
    categories. You can find more about the dataset at [https://snap.stanford.edu/data/facebook-large-page-page-network.html](https://snap.stanford.edu/data/facebook-large-page-page-network.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing some of the common libraries and the Facebook dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The data object here is of type `torch_geometric.data`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some statistics about the graph data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s print the features and label in a tabular format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the features and labels contained in the `dfx` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **1** | **2** | **…** | **127** | **label** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | -0.262576 | -0.276483 | … | -0.223836 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | -0.262576 | -0.276483 | … | -0.128634 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -0.262576 | -0.265053 | … | -0.223836 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 22468 | -0.262576 | -0.276483 | … | -0.218148 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 22469 | -0.232275 | -0.276483 | … | -0.221275 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 9.1 – A dataset with each row showing feature values; the last column
    shows the label for each data point
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall printed result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These 127 features have been generated using the Doc2Vec technique from the
    page description text. These features act like an embedding vector for each Facebook
    page.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9**.5*, we visualize the dataset using Gephi, which is a graph visualization
    software:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – The Facebook Page-Page dataset Gephi visualization
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph contains both inter-category and intra-category connections, but
    the latter is more dominant, highlighting the mutual-like affinity within the
    same category. This leads to distinct clusters, offering a bird’s-eye view of
    the strong intra-category affiliations on Facebook. If we analyze the original
    dataset for various classes, their proportion is not so imbalanced. So, let’s
    add some imbalance by removing some nodes randomly (as shown in *Figure 9**.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Number of nodes in the** **original dataset** | **Number of
    nodes after removing** **some nodes** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 3,327 | 3,327 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6,495 | 1,410 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 6,880 | 460 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 5,768 | 256 |'
  prefs: []
  type: TYPE_TB
- en: Table 9.2 – The distribution of various classes in the dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what the distribution of data looks like after adding imbalance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – The distribution of various classes after adding imbalance
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s split the data into training and test sets by specifying their ranges
    of indices. In the `Data` object, we can specify this range to denote training
    and test sets, using masks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Training an XGBoost model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s set up a simple baseline using an XGBoost model on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create our train/test dataset using the masks we created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we train and evaluate on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the following accuracy value on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the PR curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This prints the PR curve (*Figure 9**.7*) and area for various classes, using
    the XGBoost model. The area for the most imbalanced class, 3, is the lowest, as
    expected.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – The PR curve using XGBoost
  prefs: []
  type: TYPE_NORMAL
- en: Training a MultiLayer Perceptron model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can set up another baseline using the simplest of deep learning models, the
    **MultiLayer Perceptron** (**MLP**). *Figure 9**.8* shows the PR curve for each
    class. Overall, the MLP did worse than XGBoost, but its performance on the most
    imbalanced class, 3, is better than that of XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – The PR curve using the MLP model
  prefs: []
  type: TYPE_NORMAL
- en: Training a GCN model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we switch to using a graph convolutional network, which is a generalization
    of the convolution layers in CNNs. A GCN, as we discussed previously, uses the
    structure of the graph to update the features of each node, based on its neighbors’
    features. In other words, each node gets to learn from its friends!
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step involves importing the required libraries. Here, we import `PyTorch`,
    the `GCNConv` module from PyG, and the `functional` module from PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `GraphConvolutionalNetwork` class is a representation of our model. The
    class inherits from PyTorch’s `nn.Module`. It contains an initializer, a forward
    function for forward propagation, a function to train the model, and a function
    to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the `__init__()` function, we initialize the layers of the model. Our model
    contains two **Graph Convolutional Network layers** (**GCNConv layers**). The
    dimensions of the input, hidden, and output layers are required as arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define a `forward()` function to perform forward propagation through
    the network. It takes the node features and edge index as input, applies the first
    GCN layer followed by a ReLU activation function, and then applies the second
    GCN layer. The function then applies a `log_softmax` activation function and returns
    the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train_model()` function trains the model. It takes in the data and the
    number of epochs as input. It sets the model to training mode and initializes
    the **negative log-likelihood loss** (**NLLLoss**) as the loss function, and Adam
    as the optimizer. It then runs a loop for the specified number of epochs to train
    the model. Within each epoch, it computes the output of the model, calculates
    the loss and accuracy, performs backpropagation, and updates the model parameters.
    It also calculates and prints the training and validation losses and accuracies
    every 20 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `evaluate_model` function is used to evaluate the model. It sets the model
    to evaluation mode and calculates the output of the model and the test accuracy.
    It returns the test accuracy and the output for the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We start the training process and then evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s print the PR curves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – The PR curve using the GCN model
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Table 9.3*, we compare the overall accuracy values as well as class-wise
    accuracy values of various models:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Accuracy %** | **MLP** | **XGBoost** | **GCN** |'
  prefs: []
  type: TYPE_TB
- en: '| Overall | 76.5 | 83.9 | **90.9** |'
  prefs: []
  type: TYPE_TB
- en: '| Class 0 | 84.9 | 95.2 | **96.6** |'
  prefs: []
  type: TYPE_TB
- en: '| Class 1 | 72.9 | 78.0 | **88.1** |'
  prefs: []
  type: TYPE_TB
- en: '| Class 2 | 33.3 | 57.1 | **71.4** |'
  prefs: []
  type: TYPE_TB
- en: '| Class 3 | 68.8 | 37.5 | **75.0** |'
  prefs: []
  type: TYPE_TB
- en: Table 9.3 – Class-wise accuracy values in % on the Facebook Page-Page network
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some insights from *Table 9.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall performance**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCN outshines both MLP and XGBoost with an overall accuracy of 90.9%. GCN is
    the best for this network data, excelling in all classes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Class-specific insights**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Class 0**: GCN and XGBoost do well on class 0.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classes 1–3**: GCN leads, while MLP and XGBoost struggle, especially in classes
    2 and 3\. Note in particular that on class 3, which had the fewest number of examples
    in the training data, GCN performed significantly better than others.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we compared the performance of the traditional ML algorithm of XGBoost
    and a basic MLP deep learning model with GCN, a graph ML model on an imbalanced
    dataset. The results showed that graph ML algorithms can outperform traditional
    algorithms, demonstrating the potential of graph ML to deal with imbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: The superior performance of graph ML algorithms can be attributed to their ability
    to leverage the structure of the graph. By aggregating information from a node’s
    neighborhood, graph ML algorithms can capture local and global patterns in data
    that traditional algorithms might miss.
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Graph ML at Uber and Grab
  prefs: []
  type: TYPE_NORMAL
- en: '🎯 **Problem** **being solved**:'
  prefs: []
  type: TYPE_NORMAL
- en: Both Uber and Grab aimed to tackle the complex issue of fraud across their diverse
    service offerings, ranging from ride-hailing to food delivery and financial services.
    Uber focused on collusion fraud [1], where groups of users work together to commit
    fraud. For instance, users can collaborate to take fake trips using stolen credit
    cards and then request chargebacks from the bank to get refunds for those illegitimate
    purchases. Grab aimed for a general fraud detection framework that could adapt
    to new patterns [2].
  prefs: []
  type: TYPE_NORMAL
- en: '⚖️ **Data** **imbalance issue**:'
  prefs: []
  type: TYPE_NORMAL
- en: Fraudulent activities were rare but diverse, creating a class imbalance problem.
    Both companies faced the challenge of adapting to new fraud patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '🎨 **Graph** **modeling strategy**:'
  prefs: []
  type: TYPE_NORMAL
- en: '• **Graph models**: Both companies employed **Relational Graph Convolutional
    Networks** (**RGCNs**) to capture complex relationships indicative of fraud. To
    determine whether an Uber user is fraudulent, Uber wanted to leverage not just
    the features of the target user but also the features of users connected to them
    within a defined network distance.'
  prefs: []
  type: TYPE_NORMAL
- en: '• **Semi-supervised learning**: Grab’s RGCN model was trained on a graph with
    millions of nodes and edges, where only a small percentage had labels. Tree-based
    models rely heavily on quality labels and feature engineering, while graph-based
    models need minimal feature engineering and excel in detecting unknown fraud,
    using graph structures.'
  prefs: []
  type: TYPE_NORMAL
- en: '📊**Real-world impact**:'
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based models proved to be effective in detecting both known and unknown
    fraud risks. They require less feature engineering and are less dependent on labels,
    making them a sustainable foundation to combat various types of fraud risks. However,
    Grab does not use RGCN for real-time model prediction due to latency concerns
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: '🛠 **Challenges** **and tips**:'
  prefs: []
  type: TYPE_NORMAL
- en: '• **Data pipeline and scalability**: Large graph sizes necessitated distributed
    training and prediction. Future work was needed to enhance real-time capabilities
    at Uber.'
  prefs: []
  type: TYPE_NORMAL
- en: '• **Batch real-time prediction**: For Grab, real-time graph updates were computationally
    intensive, making batch real-time predictions a viable solution.'
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, graph ML offers a promising approach to deal with imbalanced
    data, when the data either inherently has a graph structure or we think we can
    exploit the interconnectedness in the data. By leveraging the rich information
    contained in the graph structure, graph ML algorithms can improve model performance
    and provide more accurate and reliable predictions. As more data becomes available
    and graphs become larger and more complex, its potential to deal with imbalanced
    data will only continue to grow.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will shift our focus to a different strategy called
    hard example mining, which operates on the principle of prioritizing the most
    challenging examples in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Hard example mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hard example mining is a technique in deep learning that forces the model to
    pay more attention to these difficult examples, and to prevent overfitting to
    the majority of the samples that are easy to predict. To do this, hard example
    mining identifies and selects the most challenging samples in the dataset and
    then backpropagates the loss incurred only by those challenging samples. Hard
    example mining is often used in computer vision tasks such as object detection.
    Hard examples can be of two kinds:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hard positive examples** are the correctly labeled examples with low prediction
    scores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hard negative examples** are incorrectly labeled examples with high prediction
    scores, which are obvious mistakes made by the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term “mining” refers to the process of finding such examples that are “hard.”
    The idea of hard negative mining is not really new and is quite similar to the
    idea of **boosting**, on which the popular algorithms of boosted decision trees
    are based. The boosted decision trees essentially figure out the examples on which
    the model makes mistakes, and then a new model (called a weak learner) is trained
    on such “hard” examples.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with large datasets, processing all training data to identify difficult
    examples can be time-consuming. This motivates our exploration of the online version
    of hard example mining.
  prefs: []
  type: TYPE_NORMAL
- en: Online Hard Example Mining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In **Online Hard Example Mining** (**OHEM**) [3], the “hard” examples are figured
    out for each batch of the training cycle, where we take the *k* examples, which
    have the lowest value of the loss. We then backpropagate the loss for only those
    *k* examples during the training.
  prefs: []
  type: TYPE_NORMAL
- en: This way, the network focuses on the most difficult samples that have more information
    than the easy samples, and the model improves faster with less training data.
  prefs: []
  type: TYPE_NORMAL
- en: The OHEM technique, introduced in the paper by Shrivastava et al. [3], has been
    quite popular. It is a technique primarily used in object detection to improve
    model performance by focusing on challenging cases. It aims to efficiently select
    a subset of “hard” negative examples that are most informative to train the model.
    As an example, imagine we’re developing a facial recognition model, and our dataset
    consists of images with faces (positive examples) and images without faces (negative
    examples). In practice, we often encounter a large number of negative examples
    compared to a smaller set of positive ones. To make our training more efficient,
    it’s wise to select a subset of the most challenging negative examples that will
    be most informative for our model.
  prefs: []
  type: TYPE_NORMAL
- en: In our experiment, we found that online hard example mining did help the imbalanced
    MNIST dataset and improved our model’s performance on the most imbalanced classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the core implementation of the OHEM function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the `NLL_OHEM` class, we first computed the regular cross-entropy loss, and
    then we figured out the *k* smallest loss values. These *k* values denote the
    hardest *k* examples that the model had trouble with. We then only propagate those
    *k* loss values during the backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: As we did earlier in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235), *Algorithm-Level
    Deep Learning Techniques*, we will continue using the long-tailed version of the
    MNIST dataset (*Figure 9**.10*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – An imbalanced MNIST dataset showing the counts of each class
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9**.11*, we show the performance of OHEM loss when compared with
    cross-entropy loss after 20 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – A performance comparison of online hard example mining when compared
    with cross-entropy loss
  prefs: []
  type: TYPE_NORMAL
- en: It’s evident that the most significant improvements are observed for the classes
    with the highest level of imbalance. Though some research works [4] have tried
    to apply OHEM to general problems without much success, we think this is a good
    technique to be aware of in general.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will introduce our final topic of minority class
    incremental rectification.
  prefs: []
  type: TYPE_NORMAL
- en: Minority class incremental rectification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Minority class incremental rectification is a deep learning technique that boosts
    the representation of minority classes in imbalanced datasets using a **Class
    Rectification Loss** (**CRL**). This strategy dynamically adjusts to class imbalance,
    enhancing model performance by incorporating hard example mining and other methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique is based on the paper by Dong et al. [5][6]. Here are the main
    steps of the technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Class identification in** **each batch**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Binary classification**: We consider a class as a minority if it makes up
    less than 50% of the batch. The rest is the majority class.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class classification**: We define all minority classes as those that
    collectively account for no more than 50% of the batch. The remaining classes
    are treated as majority classes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute the class** **rectification loss**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Locate** **challenging samples**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Find hard positives**: We identify samples from the minority class that our
    model incorrectly assesses with low prediction scores.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Find hard negatives**: We locate samples from other (majority) classes that
    our model mistakenly assigns high prediction scores for the minority class.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Construct triplets**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use minority samples as anchors**: We use each sample from the minority class
    as an anchor for triplet formation.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create triplets**: We form triplets using an anchor sample, a hard positive,
    and a hard negative.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculate distances within triplets**: We define the distance (d) between
    matched (anchor and hard positive) and unmatched (anchor and hard negative) pairs
    as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: d(anchor, hard positive) = ∣ Prediction score of anchor − Prediction score of
    hard positive ∣
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d(anchor, hard negative) = Prediction score of anchor − Prediction score of
    hard negative
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Impose margin ranking**: We ensure the distance from the anchor to the hard
    negative is greater than the distance from the anchor to the hard positive, increased
    by a margin.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Formulate final** **loss function**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Class imbalance rectification**: We modify the standard cross-entropy loss
    to address the class imbalance by introducing a **CRL** term.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom loss calculation**: We use the formed triplets to compute an average
    sum of the defined distances.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss equation**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: L final = α × L CRL + (1 − α) × L CE
  prefs: []
  type: TYPE_NORMAL
- en: Here, L CRL is the CRL loss, L CE is the cross-entropy loss, and α is a hyperparameter
    dependent upon the amount of class imbalance in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – A comic illustrating the usage of triplet loss in class rectification
    loss
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the hard sample mining technique in minority class incremental rectification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The minority class incremental rectification technique uses the hard negative
    technique but with two customizations:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses only minority classes for hard mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses both hard positives and hard negatives for loss computation (triplet
    margin loss)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key highlight of the minority class incremental rectification technique
    in handling highly imbalanced datasets is that it uses the triplet margin loss
    on the minority class of the batch that it operates upon. This makes sure that
    the model incrementally optimizes the triplet loss for the minority class in every
    batch.
  prefs: []
  type: TYPE_NORMAL
- en: Our results on imbalanced MNIST data by using `ClassRectificationLoss` were
    relatively mediocre compared to the baseline model that employed cross-entropy
    loss. This performance difference could be due to the technique’s suitability
    for very large-scale training data, as opposed to a much smaller dataset such
    as MNIST, which we used here. Please find the complete notebook in the GitHub
    repo.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that the original authors of the paper applied this method
    to the CelebA face attribute dataset, which is extensive and multi-label as well
    as multi-class. *Table 9.4* presents the results from the paper, where they used
    a five-layer CNN as a baseline and compared CRL with oversampling, undersampling,
    and cost-sensitive techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attributes (****imbalance ratio)** | **Baseline (****five-layer CNN)**
    | **Over-sampling** | **Under-sampling** | **Cost-sensitive** | **CRL** |'
  prefs: []
  type: TYPE_TB
- en: '| Bald (1:43) | 93 | 92 | 79 | 93 | **99** |'
  prefs: []
  type: TYPE_TB
- en: '| Mustache (1:24) | 88 | 90 | 60 | 88 | **93** |'
  prefs: []
  type: TYPE_TB
- en: '| Gray hair (1:23) | 90 | 90 | 88 | 90 | **96** |'
  prefs: []
  type: TYPE_TB
- en: '| Pale skin (1:22) | 81 | 82 | 78 | 80 | **92** |'
  prefs: []
  type: TYPE_TB
- en: '| Double chin (1:20) | 83 | 84 | 80 | 84 | **89** |'
  prefs: []
  type: TYPE_TB
- en: Table 9.4 – A performance comparison of CRL on facial attribute recognition
    on the CelebA benchmark, using class-balanced accuracy (in %) (adapted from Dong
    et al. [6])
  prefs: []
  type: TYPE_NORMAL
- en: As is evident from the table, the CRL technique consistently outperforms other
    methods across various facial attributes, even in high imbalance scenarios. Specifically,
    for the **Bald** attribute, with a 1:43 imbalance ratio, CRL achieved a remarkable
    99% accuracy. Its effectiveness is also evident in attributes such as **Mustache**
    and **Gray hair**, where it surpassed the baseline by 5% and 6%, respectively.
    This demonstrates CRL’s superior ability to address class imbalances.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – A visual representation of CRL regularization in rectifying model
    biases from class-imbalanced training data
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the `ClassRectificationLoss` class provides a custom loss function
    that combines triplet loss and negative log-likelihood loss while also considering
    class imbalance in the dataset. This can be a useful tool to train models on imbalanced
    datasets where the minority class samples are of particular interest.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explored a few modern deep learning strategies to handle imbalanced
    data, including graph ML, hard example mining, and minority class incremental
    rectification. By blending data-level and algorithm-level techniques, and sometimes
    even transitioning a problem paradigm from tabular to graph-based data representation,
    we can effectively leverage challenging examples, improve the representation of
    less common classes, and advance our ability to manage data imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to graph ML and saw how it can be useful
    for certain imbalanced datasets. We trained and compared the performance of the
    GCN model with baselines of XGBoost and MLP on the Facebook page-page dataset.
    For certain datasets (including tabular ones), where we are able to leverage the
    rich and interconnected structure of the graph data, the graph ML models can beat
    even XGBoost models. As we continue to encounter increasingly complex and interconnected
    data, the importance and relevance of graph ML models will only continue to grow.
    Understanding and utilizing these algorithms can be invaluable in your arsenal.
  prefs: []
  type: TYPE_NORMAL
- en: We then went over a hard mining technique, where the “hard” examples with the
    lowest loss values are first identified. Then, the loss for only *k* such examples
    is backpropagated in order to force a model to focus on the minority class examples,
    which the model has the most trouble learning about. Finally, we deep-dived into
    another hybrid deep learning technique called minority class incremental rectification.
    This method employs triplet loss on examples that are mined using the online hard
    example mining technique. Because the minority class incremental rectification
    method combines hard sample mining from minority groups with a regularized objective
    function, known as CRL, it is considered a hybrid approach that combines both
    data-level and algorithm-level deep learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We hope this chapter equipped you with the confidence to extract key insights
    from new techniques and understand their main ideas, taken directly from research
    papers.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will talk about model calibration, its importance,
    and some of the popular techniques to calibrate ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apply triplet loss to the imbalanced MNIST dataset, and see whether the model’s
    performance is better than using the cross-entropy loss function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply minority class incremental rectification technique to the imbalanced datasets
    – CIFAR10-LT and CIFAR100-LT. For a reference implementation of this technique
    on the MNIST-LT dataset, you can refer to the accompanying GitHub notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Fraud Detection: Using Relational Graph Learning to Detect Collusion (**2021)*:
    [https://www.uber.com/blog/fraud-detection/](https://www.uber.com/blog/fraud-detection/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Graph for fraud detection (**2022)*: [https://engineering.grab.com/graph-for-fraud-detection](https://engineering.grab.com/graph-for-fraud-detection).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. Shrivastava, A. Gupta, and R. Girshick, *“Training Region-Based Object Detectors
    with Online Hard Example Mining,”* in 2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), Las Vegas, NV, USA, Jun. 2016, pp. 761–769: doi:
    10.1109/CVPR.2016.89.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Marius Schmidt-Mengin, Théodore Soulier, Mariem Hamzaoui, Arya Yazdan-Panah,
    Benedetta Bod-ini, et al. *“Online hard example mining vs. fixed oversampling
    strategy for segmentation of new multiple sclerosis lesions from longitudinal
    FLAIR MRI”*. Frontiers in Neuroscience, 2022, 16, pp.100405\. 10.3389/fnins.2022.1004050\.
    hal-03836922.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q. Dong, S. Gong, and X. Zhu, *“Class Rectification Hard Mining for Imbalanced
    Deep Learning,”* in 2017 IEEE International Conference on Computer Vision (ICCV),
    Venice, Oct. 2017, pp. 1869–1878\. doi: 10.1109/ICCV.2017.205.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q. Dong, S. Gong, and X. Zhu, *“Imbalanced Deep Learning by Minority Class
    Incremental Rectification.”* arXiv, Apr. 28, 2018\. Accessed: Jul. 26, 2022\.
    [Online]. Available: [http://arxiv.org/abs/1804.10851](http://arxiv.org/abs/1804.10851).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
