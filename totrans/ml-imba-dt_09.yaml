- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Hybrid Deep Learning Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ··åˆæ·±åº¦å­¦ä¹ æ–¹æ³•
- en: In this chapter, we will talk about some of the hybrid deep learning techniques
    that combine the data-level ([*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*) and algorithm-level ([*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep Learning Techniques*) methods in some ways. This chapter
    contains some recent and more advanced techniques that can be challenging to implement,
    so it is recommended to have a good understanding of the previous chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€äº›æ··åˆæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯ä»¥æŸç§æ–¹å¼ç»“åˆäº†æ•°æ®çº§ï¼ˆ[*ç¬¬7ç« *](B17259_07.xhtml#_idTextAnchor205)ï¼Œ*æ•°æ®çº§æ·±åº¦å­¦ä¹ æ–¹æ³•*ï¼‰å’Œç®—æ³•çº§ï¼ˆ[*ç¬¬8ç« *](B17259_08.xhtml#_idTextAnchor235)ï¼Œ*ç®—æ³•çº§æ·±åº¦å­¦ä¹ æŠ€æœ¯*ï¼‰æ–¹æ³•ã€‚æœ¬ç« åŒ…å«ä¸€äº›æœ€è¿‘ä¸”æ›´é«˜çº§çš„æŠ€æœ¯ï¼Œå¯èƒ½éš¾ä»¥å®ç°ï¼Œå› æ­¤å»ºè®®æ‚¨å¯¹å‰å‡ ç« æœ‰è‰¯å¥½çš„ç†è§£ã€‚
- en: We will begin with an introduction to graph machine learning, clarifying how
    graph models exploit relationships within data to boost performance, especially
    for minority classes. Through a side-by-side comparison of a **Graph Convolutional
    Network** (**GCN**), XGBoost, and MLP models, using an imbalanced social network
    dataset, we will highlight the superior performance of the GCN.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é¦–å…ˆä»‹ç»å›¾æœºå™¨å­¦ä¹ çš„åŸºç¡€ï¼Œé˜æ˜å›¾æ¨¡å‹å¦‚ä½•åˆ©ç”¨æ•°æ®ä¸­çš„å…³ç³»æ¥æå‡æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ•°ç±»åˆ«çš„åº”ç”¨ä¸­ã€‚é€šè¿‡å°†**å›¾å·ç§¯ç½‘ç»œ**ï¼ˆ**GCN**ï¼‰ã€XGBoostå’ŒMLPæ¨¡å‹è¿›è¡Œå¹¶æ’æ¯”è¾ƒï¼Œä½¿ç”¨ä¸å¹³è¡¡çš„ç¤¾ä¼šç½‘ç»œæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†çªå‡ºGCNçš„ä¼˜è¶Šæ€§èƒ½ã€‚
- en: We will continue to explore strategies to tackle class imbalance in deep learning,
    examining techniques that manipulate data distribution and prioritize challenging
    examples. We will also go over techniques called hard example mining and minority
    class incremental rectification, which focus on improving model performance through
    prioritization of difficult instances and iterative enhancement of minority class
    representation, respectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç»§ç»­æ¢ç´¢è§£å†³æ·±åº¦å­¦ä¹ ä¸­ç±»åˆ«ä¸å¹³è¡¡çš„ç­–ç•¥ï¼Œæ£€æŸ¥æ“çºµæ•°æ®åˆ†å¸ƒå’Œä¼˜å…ˆå¤„ç†æŒ‘æˆ˜æ€§ç¤ºä¾‹çš„æŠ€æœ¯ã€‚æˆ‘ä»¬è¿˜å°†ä»‹ç»ç§°ä¸ºç¡¬ç¤ºä¾‹æŒ–æ˜å’Œå°‘æ•°ç±»å¢é‡æ ¡æ­£çš„æŠ€æœ¯ï¼Œå®ƒä»¬åˆ†åˆ«é€šè¿‡ä¼˜å…ˆå¤„ç†å›°éš¾å®ä¾‹å’Œè¿­ä»£å¢å¼ºå°‘æ•°ç±»è¡¨ç¤ºæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚
- en: While a significant portion of our discussion will revolve around image datasets,
    notably the imbalanced MNIST, itâ€™s crucial to understand the broader applicability
    of these techniques. For instance, our deep dive into graph machine learning wonâ€™t
    rely on MNIST. Instead, weâ€™ll switch gears to a more realistic dataset from Facebook,
    offering a fresh perspective on handling imbalances in real-world scenarios.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬çš„å¤§éƒ¨åˆ†è®¨è®ºå°†å›´ç»•å›¾åƒæ•°æ®é›†å±•å¼€ï¼Œç‰¹åˆ«æ˜¯ä¸å¹³è¡¡çš„MNISTæ•°æ®é›†ï¼Œä½†ç†è§£è¿™äº›æŠ€æœ¯çš„æ›´å¹¿æ³›é€‚ç”¨æ€§è‡³å…³é‡è¦ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯¹å›¾æœºå™¨å­¦ä¹ çš„æ·±å…¥ç ”ç©¶ä¸ä¼šä¾èµ–äºMNISTã€‚ç›¸åï¼Œæˆ‘ä»¬å°†è½¬å‘æ¥è‡ªFacebookçš„æ›´çœŸå®çš„æ•°æ®é›†ï¼Œä¸ºå¤„ç†ç°å®åœºæ™¯ä¸­çš„ä¸å¹³è¡¡é—®é¢˜æä¾›æ–°çš„è§†è§’ã€‚
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Graph machine learning for imbalanced data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸å¹³è¡¡æ•°æ®çš„å›¾æœºå™¨å­¦ä¹ 
- en: Hard example mining
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¡¬ç¤ºä¾‹æŒ–æ˜
- en: Minority class incremental rectification
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°‘æ•°ç±»å¢é‡æ ¡æ­£
- en: By the end of this chapter, we will become familiar with some hybrid methods,
    enabling us to understand the core principles behind more complex techniques.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å°†ç†Ÿæ‚‰ä¸€äº›æ··åˆæ–¹æ³•ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿç†è§£æ›´å¤æ‚æŠ€æœ¯èƒŒåçš„æ ¸å¿ƒåŸç†ã€‚
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `numpy`, `pandas`, `sklearn`, and `torch`. For graph machine learning, we will
    use the `torch_geometric` library as well. The code and notebooks for this chapter
    are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09).
    You can open the GitHub notebook using Google Colab by clicking on the **Open
    in Colab** icon at the top of the chapterâ€™s notebook or by launching it from [https://colab.research.google.com](https://colab.research.google.com),
    using the GitHub URL of the notebook.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å‰å‡ ç« ç±»ä¼¼ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨å¸¸è§çš„åº“ï¼Œå¦‚`numpy`ã€`pandas`ã€`sklearn`å’Œ`torch`ã€‚å¯¹äºå›¾æœºå™¨å­¦ä¹ ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`torch_geometric`åº“ã€‚æœ¬ç« çš„ä»£ç å’Œç¬”è®°æœ¬å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼Œç½‘å€ä¸º[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09)ã€‚æ‚¨å¯ä»¥é€šè¿‡ç‚¹å‡»ç« èŠ‚ç¬”è®°æœ¬é¡¶éƒ¨çš„**åœ¨Colabä¸­æ‰“å¼€**å›¾æ ‡æˆ–åœ¨[https://colab.research.google.com](https://colab.research.google.com)ä½¿ç”¨ç¬”è®°æœ¬çš„GitHub
    URLæ¥æ‰“å¼€GitHubç¬”è®°æœ¬ã€‚
- en: Using graph machine learning for imbalanced data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å›¾æœºå™¨å­¦ä¹ å¤„ç†ä¸å¹³è¡¡æ•°æ®
- en: In this section, we will see when graphs can be useful tools in machine learning,
    when to use graph ML models in general, and how they can be helpful on certain
    kinds of imbalanced datasets. Weâ€™ll also be exploring how graph ML models can
    outperform classical models such as XGBoost on certain imbalanced datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨åœ¨æœºå™¨å­¦ä¹ ä¸­ä½•æ—¶å›¾å½¢æ˜¯æœ‰ç”¨çš„å·¥å…·ï¼Œä¸€èˆ¬ä½•æ—¶ä½¿ç”¨å›¾æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥åŠå®ƒä»¬åœ¨ç‰¹å®šç±»å‹çš„å¤±è¡¡æ•°æ®é›†ä¸Šå¦‚ä½•æœ‰æ‰€å¸®åŠ©ã€‚æˆ‘ä»¬è¿˜å°†æ¢è®¨å›¾æœºå™¨å­¦ä¹ æ¨¡å‹å¦‚ä½•åœ¨æŸäº›å¤±è¡¡æ•°æ®é›†ä¸Šä¼˜äºç»å…¸æ¨¡å‹ï¼Œå¦‚XGBoostã€‚
- en: Graphs are incredibly versatile data structures that can represent complex relationships
    and structures, from social networks and web pages (think of links as edges) to
    molecules in chemistry (consider atoms as nodes and the bonds between them as
    edges) and various other domains. Graph models allow us to represent the relationships
    in data, which can be helpful to make predictions and gain insights, even for
    problems where the relationships are not explicitly defined.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾å½¢æ˜¯æå…¶çµæ´»çš„æ•°æ®ç»“æ„ï¼Œå¯ä»¥è¡¨ç¤ºå¤æ‚çš„å…³ç³»å’Œç»“æ„ï¼Œä»ç¤¾äº¤ç½‘ç»œå’Œç½‘é¡µï¼ˆå°†é“¾æ¥è§†ä¸ºè¾¹ï¼‰åˆ°åŒ–å­¦ä¸­çš„åˆ†å­ï¼ˆå°†åŸå­è§†ä¸ºèŠ‚ç‚¹ï¼Œå®ƒä»¬ä¹‹é—´çš„é”®è§†ä¸ºè¾¹ï¼‰ä»¥åŠå„ç§å…¶ä»–é¢†åŸŸã€‚å›¾æ¨¡å‹ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¡¨ç¤ºæ•°æ®ä¸­çš„å…³ç³»ï¼Œè¿™å¯¹äºé¢„æµ‹å’Œæ´å¯ŸåŠ›æ˜¯æœ‰å¸®åŠ©çš„ï¼Œå³ä½¿å¯¹äºå…³ç³»æ²¡æœ‰æ˜ç¡®å®šä¹‰çš„é—®é¢˜ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: Understanding graphs
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è§£å›¾å½¢
- en: Graphs are the foundation of graph ML, so itâ€™s important to understand them
    first. In the context of computer science, a graph is a collection of nodes (or
    vertices) and edges. Nodes represent entities, and edges represent relationships
    or interactions between these entities. For example, in a social network, each
    person can be a node, and the friendship between two people can be an edge.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾å½¢æ˜¯å›¾æœºå™¨å­¦ä¹ çš„åŸºç¡€ï¼Œå› æ­¤é¦–å…ˆç†è§£å®ƒä»¬æ˜¯å¾ˆé‡è¦çš„ã€‚åœ¨è®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œå›¾æ˜¯ç”±èŠ‚ç‚¹ï¼ˆæˆ–é¡¶ç‚¹ï¼‰å’Œè¾¹ç»„æˆçš„ä¸€ä¸ªé›†åˆã€‚èŠ‚ç‚¹ä»£è¡¨å®ä½“ï¼Œè¾¹ä»£è¡¨è¿™äº›å®ä½“ä¹‹é—´çš„å…³ç³»æˆ–äº¤äº’ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªç¤¾äº¤ç½‘ç»œä¸­ï¼Œæ¯ä¸ªäººå¯ä»¥æ˜¯ä¸€ä¸ªèŠ‚ç‚¹ï¼Œä¸¤ä¸ªäººä¹‹é—´çš„å‹è°Šå¯ä»¥æ˜¯ä¸€æ¡è¾¹ã€‚
- en: Graphs can be either directed (edges have a direction) or undirected (edges
    do not have a direction). They can also be weighted (edges have a value) or unweighted
    (edges do not have a value).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾å½¢å¯ä»¥æ˜¯å®šå‘çš„ï¼ˆè¾¹æœ‰æ–¹å‘ï¼‰æˆ–éå®šå‘çš„ï¼ˆè¾¹æ²¡æœ‰æ–¹å‘ï¼‰ã€‚å®ƒä»¬ä¹Ÿå¯ä»¥æ˜¯æœ‰æƒé‡çš„ï¼ˆè¾¹æœ‰å€¼ï¼‰æˆ–æ— æƒé‡çš„ï¼ˆè¾¹æ²¡æœ‰å€¼ï¼‰ã€‚
- en: '*Figure 9**.1* shows a sample tabular dataset on the left and its corresponding
    graph representation on the right:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾9.1* å±•ç¤ºäº†ä¸€ä¸ªæ ·æœ¬è¡¨æ ¼æ•°æ®é›†åœ¨å·¦ä¾§åŠå…¶å¯¹åº”çš„å›¾å½¢è¡¨ç¤ºåœ¨å³ä¾§ï¼š'
- en: '![](img/B17259_09_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_09_01.jpg)'
- en: Figure 9.1 â€“ Tabular data (left) contrasted with its visual graph representation
    (right)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.1 â€“ è¡¨æ ¼æ•°æ®ï¼ˆå·¦ä¾§ï¼‰ä¸å…¶è§†è§‰å›¾å½¢è¡¨ç¤ºï¼ˆå³ä¾§ï¼‰è¿›è¡Œå¯¹æ¯”
- en: The graph representation on the right emphasizes the relationships between various
    entities. In the tabular representation on the left, devices and their IP addresses
    are listed with connection details along with network bandwidth. The graphical
    representation on the right visually represents these connections, allowing easier
    comprehension of the network topology. Devices are nodes, connections are edges
    with bandwidth are the weights. Graphs provide a clearer view of interrelationships
    than tables, emphasizing network design insights.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä¾§çš„å›¾å½¢è¡¨ç¤ºå¼ºè°ƒäº†å„ç§å®ä½“ä¹‹é—´çš„å…³ç³»ã€‚åœ¨å·¦ä¾§çš„è¡¨æ ¼è¡¨ç¤ºä¸­ï¼Œè®¾å¤‡å’Œå®ƒä»¬çš„IPåœ°å€ä»¥åŠç½‘ç»œå¸¦å®½çš„è¿æ¥ç»†èŠ‚è¢«åˆ—å‡ºã€‚å³ä¾§çš„å›¾å½¢è¡¨ç¤ºç›´è§‚åœ°è¡¨ç¤ºäº†è¿™äº›è¿æ¥ï¼Œä½¿å¾—ç½‘ç»œæ‹“æ‰‘ç»“æ„æ›´å®¹æ˜“ç†è§£ã€‚è®¾å¤‡æ˜¯èŠ‚ç‚¹ï¼Œè¿æ¥æ˜¯å¸¦æœ‰å¸¦å®½çš„è¾¹ï¼Œæƒé‡ã€‚å›¾å½¢æ¯”è¡¨æ ¼æä¾›äº†æ›´æ¸…æ™°çš„ç›¸äº’å…³ç³»è§†å›¾ï¼Œå¼ºè°ƒäº†ç½‘ç»œè®¾è®¡æ´å¯Ÿã€‚
- en: In the following section, we will get an overview of how ML can be applied to
    graphs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¦‚è¿°æœºå™¨å­¦ä¹ å¦‚ä½•åº”ç”¨äºå›¾å½¢ã€‚
- en: Graph machine learning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾æœºå™¨å­¦ä¹ 
- en: '**Graph Machine Learning** (**GML**) is a set of techniques that use the structure
    of a graph to extract features and make predictions. GML algorithms can leverage
    the rich information contained in the graph structure, such as the connections
    between nodes and the patterns of these connections, to improve model performance,
    especially on imbalanced data.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾æœºå™¨å­¦ä¹ **ï¼ˆ**GML**ï¼‰æ˜¯ä¸€ç»„ä½¿ç”¨å›¾çš„æ¶æ„æ¥æå–ç‰¹å¾å’Œè¿›è¡Œé¢„æµ‹çš„æŠ€æœ¯ã€‚GMLç®—æ³•å¯ä»¥åˆ©ç”¨å›¾ç»“æ„ä¸­åŒ…å«çš„ä¸°å¯Œä¿¡æ¯ï¼Œä¾‹å¦‚èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å’Œè¿™äº›è¿æ¥çš„æ¨¡å¼ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤±è¡¡æ•°æ®ä¸Šã€‚'
- en: Two popular neural network GML algorithms are GCNs and **Graph Attention Networks**
    (**GATs**). Both algorithms use the graph structure to aggregate information from
    a nodeâ€™s neighborhood. However, they differ in how they weigh the importance of
    a nodeâ€™s neighbors. GCN gives equal weight to all neighbors, while GAT uses attention
    mechanisms to assign different weights to different neighbors. We will limit our
    discussion to GCNs only in this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§æµè¡Œçš„ç¥ç»ç½‘ç»œGMLç®—æ³•æ˜¯GCNså’Œ**å›¾æ³¨æ„åŠ›ç½‘ç»œ**ï¼ˆ**GATs**ï¼‰ã€‚è¿™ä¸¤ç§ç®—æ³•éƒ½ä½¿ç”¨å›¾ç»“æ„ä»èŠ‚ç‚¹çš„é‚»åŸŸä¸­èšåˆä¿¡æ¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¦‚ä½•æƒè¡¡èŠ‚ç‚¹é‚»å±…çš„é‡è¦æ€§æ–¹é¢æœ‰æ‰€ä¸åŒã€‚GCNå¯¹æ‰€æœ‰é‚»å±…ç»™äºˆç›¸åŒçš„æƒé‡ï¼Œè€ŒGATä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ä¸ºä¸åŒçš„é‚»å±…åˆ†é…ä¸åŒçš„æƒé‡ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»…è®¨è®ºGCNsã€‚
- en: Dealing with imbalanced data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤„ç†ä¸å¹³è¡¡æ•°æ®
- en: In ML, when one class significantly outnumbers the others, the model may become
    biased toward the majority class, leading to poor performance on the minority
    class. This is problematic because, often, the minority class is the one of interest.
    For example, in fraud detection, the number of non-fraud cases significantly outnumbers
    the fraud cases, but itâ€™s the fraud cases that weâ€™re interested in detecting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå½“ä¸€ä¸ªç±»åˆ«æ˜¾è‘—å¤šäºå…¶ä»–ç±»åˆ«æ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼šåå‘å¤šæ•°ç±»åˆ«ï¼Œå¯¼è‡´å¯¹å°‘æ•°ç±»åˆ«çš„æ€§èƒ½ä¸ä½³ã€‚è¿™æ˜¯é—®é¢˜æ‰€åœ¨ï¼Œå› ä¸ºé€šå¸¸ï¼Œå°‘æ•°ç±»åˆ«æ‰æ˜¯æˆ‘ä»¬æ„Ÿå…´è¶£çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¬ºè¯ˆæ£€æµ‹ä¸­ï¼Œéæ¬ºè¯ˆæ¡ˆä¾‹çš„æ•°é‡æ˜¾è‘—å¤šäºæ¬ºè¯ˆæ¡ˆä¾‹ï¼Œä½†æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯æ£€æµ‹æ¬ºè¯ˆæ¡ˆä¾‹ã€‚
- en: In the context of GML, the structure of the graph can provide additional information
    that can help mitigate the effects of data imbalance. For example, minority class
    nodes might be more closely connected to each other than to majority class nodes.
    GML algorithms can leverage this structure to improve the performance of the minority
    class.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨GMLçš„èƒŒæ™¯ä¸‹ï¼Œå›¾çš„ç»“æ„å¯ä»¥æä¾›é¢å¤–çš„ä¿¡æ¯ï¼Œæœ‰åŠ©äºå‡è½»æ•°æ®ä¸å¹³è¡¡çš„å½±å“ã€‚ä¾‹å¦‚ï¼Œå°‘æ•°ç±»åˆ«çš„èŠ‚ç‚¹å¯èƒ½æ¯”å¤šæ•°ç±»åˆ«çš„èŠ‚ç‚¹å½¼æ­¤ä¹‹é—´æ›´ç´§å¯†åœ°è¿æ¥ã€‚GMLç®—æ³•å¯ä»¥åˆ©ç”¨è¿™ç§ç»“æ„æ¥æé«˜å°‘æ•°ç±»åˆ«çš„æ€§èƒ½ã€‚
- en: GCNs
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GCNs
- en: We will briefly discuss the key ideas behind what GCNs are and how they work.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç®€è¦è®¨è®ºGCNsèƒŒåçš„å…³é”®æ€æƒ³å’Œå®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: GCNs offer a unique method of processing structured data. Unlike standard neural
    networks that assume independent and identically distributed data, GCNs can operate
    over graph data, capturing dependencies and connections between nodes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GCNsæä¾›äº†ä¸€ç§ç‹¬ç‰¹çš„å¤„ç†ç»“æ„åŒ–æ•°æ®çš„æ–¹æ³•ã€‚ä¸å‡è®¾ç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®çš„æ ‡å‡†ç¥ç»ç½‘ç»œä¸åŒï¼ŒGCNså¯ä»¥åœ¨å›¾æ•°æ®ä¸Šæ“ä½œï¼Œæ•æ‰èŠ‚ç‚¹ä¹‹é—´çš„ä¾èµ–å’Œè¿æ¥ã€‚
- en: 'The essence of GCNs is message passing, which can be broken down as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GCNsçš„æœ¬è´¨æ˜¯æ¶ˆæ¯ä¼ é€’ï¼Œå¯ä»¥åˆ†è§£å¦‚ä¸‹ï¼š
- en: '**Node messaging**: Each node in the graph sends out and receives messages
    through its edges to and from its neighbors'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹æ¶ˆæ¯ä¼ é€’**ï¼šå›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹é€šè¿‡å…¶è¾¹å‘å…¶é‚»å±…å‘é€å’Œæ¥æ”¶æ¶ˆæ¯'
- en: '**Aggregation**: Nodes aggregate these messages to gain a broader understanding
    of these local neighborhoods'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èšåˆ**ï¼šèŠ‚ç‚¹å°†è¿™äº›æ¶ˆæ¯èšåˆèµ·æ¥ï¼Œä»¥è·å¾—å¯¹è¿™äº›å±€éƒ¨é‚»åŸŸæ›´å¹¿æ³›çš„ç†è§£'
- en: In GCNs, full feature vectors are passed around instead of just the labels of
    the nodes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨GCNsä¸­ï¼Œä¼ é€’çš„æ˜¯å®Œæ•´çš„ç‰¹å¾å‘é‡ï¼Œè€Œä¸æ˜¯ä»…ä»…èŠ‚ç‚¹çš„æ ‡ç­¾ã€‚
- en: 'Think of a GCN layer as a transformation step. The primary operations can be
    viewed as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å°†GCNå±‚è§†ä¸ºä¸€ä¸ªè½¬æ¢æ­¥éª¤ã€‚ä¸»è¦æ“ä½œå¯ä»¥çœ‹ä½œå¦‚ä¸‹ï¼š
- en: '**Aggregate neighbors**: Nodes pull features from their neighbors, leading
    to an aggregation'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èšåˆé‚»å±…**ï¼šèŠ‚ç‚¹ä»å…¶é‚»å±…é‚£é‡Œæå–ç‰¹å¾ï¼Œå¯¼è‡´èšåˆ'
- en: '**Neural network transformation**: The aggregated feature set from the previous
    step then undergoes transformation via the neural network layer'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¥ç»ç½‘ç»œè½¬æ¢**ï¼šå‰ä¸€æ­¥éª¤ä¸­èšåˆçš„ç‰¹å¾é›†éšåé€šè¿‡ç¥ç»ç½‘ç»œå±‚è¿›è¡Œè½¬æ¢'
- en: Letâ€™s explore GCNs using the example of photos on Facebook. Users upload photos,
    and our objective is to categorize these images as either spam or non-spam. This
    categorization is based on the image content as well as the IP address or user
    ID.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡Facebookä¸Šçš„ç…§ç‰‡ç¤ºä¾‹æ¥æ¢ç´¢GCNsã€‚ç”¨æˆ·ä¸Šä¼ ç…§ç‰‡ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†è¿™äº›å›¾åƒåˆ†ç±»ä¸ºåƒåœ¾é‚®ä»¶æˆ–éåƒåœ¾é‚®ä»¶ã€‚è¿™ç§åˆ†ç±»åŸºäºå›¾åƒå†…å®¹ä»¥åŠIPåœ°å€æˆ–ç”¨æˆ·IDã€‚
- en: Letâ€™s imagine we have a graph where each node is a Facebook photo, and two photos
    are connected if they were uploaded using either the same IP address or the same
    account.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æƒ³è±¡æˆ‘ä»¬æœ‰ä¸€ä¸ªå›¾ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€å¼ Facebookç…§ç‰‡ï¼Œå¦‚æœä¸¤å¼ ç…§ç‰‡æ˜¯ä½¿ç”¨ç›¸åŒçš„IPåœ°å€æˆ–ç›¸åŒçš„è´¦æˆ·ä¸Šä¼ çš„ï¼Œé‚£ä¹ˆè¿™ä¸¤å¼ ç…§ç‰‡å°±æ˜¯ç›¸è¿çš„ã€‚
- en: Letâ€™s say we want to use the actual content of the photo (possibly a feature
    vector from a pre-trained CNN or some metadata) as a node attribute. Letâ€™s assume
    that we have a 5-dimensional vector representing each photoâ€™s features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³è¦ä½¿ç”¨ç…§ç‰‡çš„å®é™…å†…å®¹ï¼ˆå¯èƒ½æ˜¯ä¸€ä¸ªä»é¢„è®­ç»ƒçš„CNNæˆ–æŸäº›å…ƒæ•°æ®ä¸­å¾—åˆ°çš„ç‰¹å¾å‘é‡ï¼‰ä½œä¸ºèŠ‚ç‚¹å±æ€§ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª5ç»´å‘é‡æ¥è¡¨ç¤ºæ¯å¼ ç…§ç‰‡çš„ç‰¹å¾ã€‚
- en: '![](img/B17259_09_02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_02.jpg)'
- en: Figure 9.2 â€“ An image with a 5-dimensional feature embedding
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 â€“ graph creation
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will create a graph where every vertex symbolizes a Facebook image. We will
    establish a link between two images if they were uploaded via an identical IP
    address or user ID.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 â€“ A diagram with images connected to each other if they share the
    IP or user ID
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 â€“ image representation
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Represent each image using a 5-dimensional vector. This can be accomplished
    by using image metadata, features derived from trained neural networks, or other
    techniques suitable for image data (*Figure 9**.2*).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 â€“ a singular-layer GCN for image analysis
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When a specific image is passed through a single-layer GCN, here is what will
    happen:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: We aggregate all the neighboring imageâ€™s feature vectors. The neighbors are
    images with matching IP addresses or user IDs.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An average function is used to combine vectors. Letâ€™s call the combined vector
    the neighborhood average vector.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The neighborhood average vector is multiplied using a weight matrix (of, say,
    size 5x1, as shown in *Figure 9**.4*).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, an activation function is applied to the outcome to get a single value,
    which suggests the likelihood of the image being spam.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17259_09_04.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 â€“ How a single GCN layer works
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 â€“ a dual-layer GCN
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multilayer GCNs, like traditional deep neural networks, can be stacked into
    multiple layers:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Raw node features feed into the first layer
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subsequent layerâ€™s input is the previous layerâ€™s output
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With each added layer, GCNs grasp more extended neighborhood information. For
    instance, in a two-layer GCN, information can ripple two hops away in the graph.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: With a foundational understanding of graph ML and GCNs in place, weâ€™re set to
    explore a case study. Weâ€™ll compare the performance of a graph model to other
    models, including a classical ML model, on an imbalanced graph dataset. Our aim
    is to determine whether graph models can outperform other models by leveraging
    the relationships between graph structures.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Case study â€“ the performance of XGBoost, MLP, and a GCN on an imbalanced dataset
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the Facebook Page-Page dataset from the **PyTorch Geometric** (**PyG**)
    library, designed to create and train deep learning models on graphs and other
    irregular structures. This dataset comprises a large collection of social networks
    from Facebook, where nodes represent official Facebook pages and edges signify
    reciprocal likes between them. Each node is labeled with one of four categories:
    Politicians, Governmental Organizations, Television Shows, or Companies. The task
    is to predict these categories based on the node characteristics, which are derived
    from descriptions provided by the page owners.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The dataset serves as a challenging benchmark for graph neural network models
    due to its size and complexity. It was collected via the Facebook Graph API in
    November 2017 and focuses on multi-class node classification within the four aforementioned
    categories. You can find more about the dataset at [https://snap.stanford.edu/data/facebook-large-page-page-network.html](https://snap.stanford.edu/data/facebook-large-page-page-network.html).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ•°æ®é›†ç”±äºå…¶è§„æ¨¡å’Œå¤æ‚æ€§ï¼Œæˆä¸ºå›¾ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ã€‚å®ƒäº2017å¹´11æœˆé€šè¿‡Facebook Graph APIæ”¶é›†ï¼Œå¹¶ä¸“æ³¨äºä¸Šè¿°å››ä¸ªç±»åˆ«ä¸­çš„å¤šç±»èŠ‚ç‚¹åˆ†ç±»ã€‚æ‚¨å¯ä»¥åœ¨[https://snap.stanford.edu/data/facebook-large-page-page-network.html](https://snap.stanford.edu/data/facebook-large-page-page-network.html)äº†è§£æ›´å¤šå…³äºè¯¥æ•°æ®é›†çš„ä¿¡æ¯ã€‚
- en: 'We start by importing some of the common libraries and the Facebook dataset:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå¯¼å…¥ä¸€äº›å¸¸ç”¨åº“å’ŒFacebookæ•°æ®é›†ï¼š
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The data object here is of type `torch_geometric.data`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ•°æ®å¯¹è±¡çš„æ•°æ®ç±»å‹ä¸º`torch_geometric.data`ã€‚
- en: 'Here are some statistics about the graph data:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›å…³äºå›¾æ•°æ®çš„ç»Ÿè®¡æ•°æ®ï¼š
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Letâ€™s print the features and label in a tabular format:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»¥è¡¨æ ¼æ ¼å¼æ‰“å°ç‰¹å¾å’Œæ ‡ç­¾ï¼š
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This prints the features and labels contained in the `dfx` DataFrame:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ‰“å°å‡ºåŒ…å«åœ¨`dfx` DataFrameä¸­çš„ç‰¹å¾å’Œæ ‡ç­¾ï¼š
- en: '|  | **1** | **2** | **â€¦** | **127** | **label** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | **1** | **2** | **â€¦** | **127** | **æ ‡ç­¾** |'
- en: '| 0 | -0.262576 | -0.276483 | â€¦ | -0.223836 | 0 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 0 | -0.262576 | -0.276483 | â€¦ | -0.223836 | 0 |'
- en: '| 1 | -0.262576 | -0.276483 | â€¦ | -0.128634 | 2 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 1 | -0.262576 | -0.276483 | â€¦ | -0.128634 | 2 |'
- en: '| 2 | -0.262576 | -0.265053 | â€¦ | -0.223836 | 1 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 2 | -0.262576 | -0.265053 | â€¦ | -0.223836 | 1 |'
- en: '| ... | ... | ... | ... | ... | ... |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... | ... |'
- en: '| 22468 | -0.262576 | -0.276483 | â€¦ | -0.218148 | 1 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 22468 | -0.262576 | -0.276483 | â€¦ | -0.218148 | 1 |'
- en: '| 22469 | -0.232275 | -0.276483 | â€¦ | -0.221275 | 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 22469 | -0.232275 | -0.276483 | â€¦ | -0.221275 | 0 |'
- en: Table 9.1 â€“ A dataset with each row showing feature values; the last column
    shows the label for each data point
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨9.1 â€“ æ¯è¡Œæ˜¾ç¤ºç‰¹å¾å€¼çš„æ•°æ®é›†ï¼›æœ€åä¸€åˆ—æ˜¾ç¤ºæ¯ä¸ªæ•°æ®ç‚¹çš„æ ‡ç­¾
- en: 'The overall printed result is as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰“å°çš„æ•´ä½“ç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These 127 features have been generated using the Doc2Vec technique from the
    page description text. These features act like an embedding vector for each Facebook
    page.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™127ä¸ªç‰¹å¾æ˜¯é€šè¿‡ä»é¡µé¢æè¿°æ–‡æœ¬ä¸­ä½¿ç”¨Doc2VecæŠ€æœ¯ç”Ÿæˆçš„ã€‚è¿™äº›ç‰¹å¾å°±åƒæ¯ä¸ªFacebooké¡µé¢çš„åµŒå…¥å‘é‡ã€‚
- en: 'In *Figure 9**.5*, we visualize the dataset using Gephi, which is a graph visualization
    software:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾9.5*ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Gephiï¼ˆä¸€ç§å›¾å½¢å¯è§†åŒ–è½¯ä»¶ï¼‰å¯è§†åŒ–æ•°æ®é›†ï¼š
- en: '![](img/B17259_09_05.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_05.jpg)'
- en: Figure 9.5 â€“ The Facebook Page-Page dataset Gephi visualization
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.5 â€“ Facebooké¡µé¢-é¡µé¢æ•°æ®é›†Gephiå¯è§†åŒ–
- en: 'The graph contains both inter-category and intra-category connections, but
    the latter is more dominant, highlighting the mutual-like affinity within the
    same category. This leads to distinct clusters, offering a birdâ€™s-eye view of
    the strong intra-category affiliations on Facebook. If we analyze the original
    dataset for various classes, their proportion is not so imbalanced. So, letâ€™s
    add some imbalance by removing some nodes randomly (as shown in *Figure 9**.6*):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å›¾åŒ…å«è·¨ç±»åˆ«å’Œç±»åˆ«å†…çš„è¿æ¥ï¼Œä½†åè€…æ›´ä¸ºçªå‡ºï¼Œçªæ˜¾äº†åŒä¸€ç±»åˆ«å†…çš„ç›¸äº’ç±»ä¼¼äº²å’ŒåŠ›ã€‚è¿™å¯¼è‡´äº†ä¸åŒçš„é›†ç¾¤ï¼Œæä¾›äº†å¯¹Facebookä¸Šå¼ºå¤§çš„ç±»åˆ«å†…å…³è”çš„é¸Ÿç°å›¾ã€‚å¦‚æœæˆ‘ä»¬åˆ†æåŸå§‹æ•°æ®é›†ä¸­å„ç§ç±»åˆ«çš„æ•°æ®ï¼Œå®ƒä»¬çš„æ¯”ä¾‹å¹¶ä¸é‚£ä¹ˆä¸å¹³è¡¡ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬é€šè¿‡éšæœºåˆ é™¤ä¸€äº›èŠ‚ç‚¹æ¥æ·»åŠ ä¸€äº›ä¸å¹³è¡¡ï¼ˆå¦‚å›¾*9.6*æ‰€ç¤ºï¼‰ï¼š
- en: '| **Class** | **Number of nodes in the** **original dataset** | **Number of
    nodes after removing** **some nodes** |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **ç±»åˆ«** | **åŸå§‹æ•°æ®é›†ä¸­èŠ‚ç‚¹çš„æ•°é‡** | **åˆ é™¤ä¸€äº›èŠ‚ç‚¹åçš„èŠ‚ç‚¹æ•°é‡** |'
- en: '| 0 | 3,327 | 3,327 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 3,327 | 3,327 |'
- en: '| 1 | 6,495 | 1,410 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6,495 | 1,410 |'
- en: '| 2 | 6,880 | 460 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 6,880 | 460 |'
- en: '| 3 | 5,768 | 256 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 5,768 | 256 |'
- en: Table 9.2 â€“ The distribution of various classes in the dataset
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨9.2 â€“ æ•°æ®é›†ä¸­å„ç§ç±»åˆ«çš„åˆ†å¸ƒ
- en: 'Here is what the distribution of data looks like after adding imbalance:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ ä¸å¹³è¡¡åæ•°æ®çš„åˆ†å¸ƒå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/B17259_09_06.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_06.jpg)'
- en: Figure 9.6 â€“ The distribution of various classes after adding imbalance
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.6 â€“ æ·»åŠ ä¸å¹³è¡¡åå„ç§ç±»åˆ«çš„åˆ†å¸ƒ
- en: 'Letâ€™s split the data into training and test sets by specifying their ranges
    of indices. In the `Data` object, we can specify this range to denote training
    and test sets, using masks:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡æŒ‡å®šç´¢å¼•çš„èŒƒå›´å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚åœ¨`Data`å¯¹è±¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ©ç æŒ‡å®šæ­¤èŒƒå›´ä»¥è¡¨ç¤ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Training an XGBoost model
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®­ç»ƒXGBoostæ¨¡å‹
- en: Letâ€™s set up a simple baseline using an XGBoost model on this dataset.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨è¯¥æ•°æ®é›†ä¸Šä½¿ç”¨XGBoostæ¨¡å‹è®¾ç½®ä¸€ä¸ªç®€å•çš„åŸºçº¿ã€‚
- en: 'First, letâ€™s create our train/test dataset using the masks we created earlier:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„æ©ç æ¥åˆ›å»ºæˆ‘ä»¬çš„è®­ç»ƒ/æµ‹è¯•æ•°æ®é›†ï¼š
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we train and evaluate on the data:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åœ¨æ•°æ®ä¸Šè®­ç»ƒå’Œè¯„ä¼°ï¼š
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This prints the following accuracy value on the test set:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨æµ‹è¯•é›†ä¸Šæ‰“å°å‡ºä»¥ä¸‹å‡†ç¡®åº¦å€¼ï¼š
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Letâ€™s plot the PR curve:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶PRæ›²çº¿ï¼š
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This prints the PR curve (*Figure 9**.7*) and area for various classes, using
    the XGBoost model. The area for the most imbalanced class, 3, is the lowest, as
    expected.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šæ‰“å°å‡ºä½¿ç”¨XGBoostæ¨¡å‹çš„å„ä¸ªç±»åˆ«çš„PRæ›²çº¿ï¼ˆ*å›¾9**.7*ï¼‰å’Œé¢ç§¯ã€‚æœ€ä¸å¹³è¡¡çš„ç±»åˆ«3çš„é¢ç§¯æœ€ä½ï¼Œæ­£å¦‚é¢„æœŸçš„é‚£æ ·ã€‚
- en: '![](img/B17259_09_07.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_07.jpg)'
- en: Figure 9.7 â€“ The PR curve using XGBoost
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.7 â€“ ä½¿ç”¨XGBoostçš„PRæ›²çº¿
- en: Training a MultiLayer Perceptron model
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¤šå±‚æ„ŸçŸ¥å™¨æ¨¡å‹
- en: We can set up another baseline using the simplest of deep learning models, the
    **MultiLayer Perceptron** (**MLP**). *Figure 9**.8* shows the PR curve for each
    class. Overall, the MLP did worse than XGBoost, but its performance on the most
    imbalanced class, 3, is better than that of XGBoost.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æœ€ç®€å•çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå³**å¤šå±‚æ„ŸçŸ¥å™¨**ï¼ˆ**MLP**ï¼‰æ¥è®¾ç½®å¦ä¸€ä¸ªåŸºçº¿ã€‚*å›¾9**.8*æ˜¾ç¤ºäº†æ¯ä¸ªç±»çš„PRæ›²çº¿ã€‚æ€»ä½“è€Œè¨€ï¼ŒMLPçš„è¡¨ç°ä¸å¦‚XGBoostï¼Œä½†åœ¨æœ€ä¸å¹³è¡¡çš„ç±»åˆ«3ä¸Šçš„è¡¨ç°ä¼˜äºXGBoostã€‚
- en: '![](img/B17259_09_08.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_08.jpg)'
- en: Figure 9.8 â€“ The PR curve using the MLP model
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.8 â€“ ä½¿ç”¨MLPæ¨¡å‹çš„PRæ›²çº¿
- en: Training a GCN model
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®­ç»ƒGCNæ¨¡å‹
- en: Finally, we switch to using a graph convolutional network, which is a generalization
    of the convolution layers in CNNs. A GCN, as we discussed previously, uses the
    structure of the graph to update the features of each node, based on its neighborsâ€™
    features. In other words, each node gets to learn from its friends!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åˆ‡æ¢åˆ°ä½¿ç”¨å›¾å·ç§¯ç½‘ç»œï¼Œè¿™æ˜¯CNNä¸­å·ç§¯å±‚çš„ä¸€ç§æ¨å¹¿ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„ï¼ŒGCNä½¿ç”¨å›¾çš„ç»“æ„æ ¹æ®å…¶é‚»å±…çš„ç‰¹å¾æ›´æ–°æ¯ä¸ªèŠ‚ç‚¹çš„ç‰¹å¾ã€‚æ¢å¥è¯è¯´ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥ä»å®ƒçš„æœ‹å‹é‚£é‡Œå­¦ä¹ ï¼
- en: 'The first step involves importing the required libraries. Here, we import `PyTorch`,
    the `GCNConv` module from PyG, and the `functional` module from PyTorch:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ¶‰åŠå¯¼å…¥æ‰€éœ€çš„åº“ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯¼å…¥`PyTorch`ã€PyGä¸­çš„`GCNConv`æ¨¡å—å’ŒPyTorchçš„`functional`æ¨¡å—ï¼š
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `GraphConvolutionalNetwork` class is a representation of our model. The
    class inherits from PyTorchâ€™s `nn.Module`. It contains an initializer, a forward
    function for forward propagation, a function to train the model, and a function
    to evaluate the model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`GraphConvolutionalNetwork`ç±»æ˜¯æˆ‘ä»¬æ¨¡å‹çš„è¡¨ç¤ºã€‚è¯¥ç±»ç»§æ‰¿è‡ªPyTorchçš„`nn.Module`ã€‚å®ƒåŒ…å«ä¸€ä¸ªåˆå§‹åŒ–å™¨ã€ä¸€ä¸ªç”¨äºå‰å‘ä¼ æ’­çš„å‰å‘å‡½æ•°ã€ä¸€ä¸ªè®­ç»ƒæ¨¡å‹çš„å‡½æ•°å’Œä¸€ä¸ªè¯„ä¼°æ¨¡å‹çš„å‡½æ•°ï¼š'
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the `__init__()` function, we initialize the layers of the model. Our model
    contains two **Graph Convolutional Network layers** (**GCNConv layers**). The
    dimensions of the input, hidden, and output layers are required as arguments.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`__init__()`å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åˆå§‹åŒ–æ¨¡å‹çš„å±‚ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŒ…å«ä¸¤ä¸ª**å›¾å·ç§¯ç½‘ç»œå±‚**ï¼ˆ**GCNConvå±‚**ï¼‰ã€‚è¾“å…¥ã€éšè—å’Œè¾“å‡ºå±‚çš„ç»´åº¦ä½œä¸ºå‚æ•°è¦æ±‚ã€‚
- en: 'Then, we define a `forward()` function to perform forward propagation through
    the network. It takes the node features and edge index as input, applies the first
    GCN layer followed by a ReLU activation function, and then applies the second
    GCN layer. The function then applies a `log_softmax` activation function and returns
    the result:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª`forward()`å‡½æ•°æ¥é€šè¿‡ç½‘ç»œæ‰§è¡Œå‰å‘ä¼ æ’­ã€‚å®ƒæ¥å—èŠ‚ç‚¹ç‰¹å¾å’Œè¾¹ç´¢å¼•ä½œä¸ºè¾“å…¥ï¼Œåº”ç”¨ç¬¬ä¸€ä¸ªGCNå±‚ï¼Œç„¶åæ˜¯ReLUæ¿€æ´»å‡½æ•°ï¼Œæ¥ç€åº”ç”¨ç¬¬äºŒä¸ªGCNå±‚ã€‚è¯¥å‡½æ•°éšååº”ç”¨`log_softmax`æ¿€æ´»å‡½æ•°å¹¶è¿”å›ç»“æœï¼š
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `train_model()` function trains the model. It takes in the data and the
    number of epochs as input. It sets the model to training mode and initializes
    the **negative log-likelihood loss** (**NLLLoss**) as the loss function, and Adam
    as the optimizer. It then runs a loop for the specified number of epochs to train
    the model. Within each epoch, it computes the output of the model, calculates
    the loss and accuracy, performs backpropagation, and updates the model parameters.
    It also calculates and prints the training and validation losses and accuracies
    every 20 epochs:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_model()`å‡½æ•°ç”¨äºè®­ç»ƒæ¨¡å‹ã€‚å®ƒæ¥å—æ•°æ®å’Œepochæ•°ä½œä¸ºè¾“å…¥ã€‚å®ƒå°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œå¹¶å°†**è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±**ï¼ˆ**NLLLoss**ï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼ŒAdamä½œä¸ºä¼˜åŒ–å™¨ã€‚ç„¶åå®ƒè¿è¡ŒæŒ‡å®šæ•°é‡çš„epochæ¥è®­ç»ƒæ¨¡å‹ã€‚åœ¨æ¯ä¸ªepochä¸­ï¼Œå®ƒè®¡ç®—æ¨¡å‹çš„è¾“å‡ºï¼Œè®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡ï¼Œæ‰§è¡Œåå‘ä¼ æ’­å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚å®ƒè¿˜ä¼šæ¯20ä¸ªepochè®¡ç®—å¹¶æ‰“å°è®­ç»ƒå’ŒéªŒè¯çš„æŸå¤±å’Œå‡†ç¡®ç‡ï¼š'
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `evaluate_model` function is used to evaluate the model. It sets the model
    to evaluation mode and calculates the output of the model and the test accuracy.
    It returns the test accuracy and the output for the test data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_model`å‡½æ•°ç”¨äºè¯„ä¼°æ¨¡å‹ã€‚å®ƒå°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå¹¶è®¡ç®—æ¨¡å‹çš„è¾“å‡ºå’Œæµ‹è¯•å‡†ç¡®ç‡ã€‚å®ƒè¿”å›æµ‹è¯•å‡†ç¡®ç‡å’Œæµ‹è¯•æ•°æ®çš„è¾“å‡ºã€‚'
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We start the training process and then evaluate the model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¼€å§‹è®­ç»ƒè¿‡ç¨‹ï¼Œç„¶åè¯„ä¼°æ¨¡å‹ï¼š
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produces the following output:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡ºï¼š
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Letâ€™s print the PR curves:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‰“å°PRæ›²çº¿ï¼š
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/B17259_09_09.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_09.jpg)'
- en: Figure 9.9 â€“ The PR curve using the GCN model
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.9 â€“ ä½¿ç”¨GCNæ¨¡å‹çš„PRæ›²çº¿
- en: 'In *Table 9.3*, we compare the overall accuracy values as well as class-wise
    accuracy values of various models:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*è¡¨9.3*ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å„ç§æ¨¡å‹çš„æ•´ä½“å‡†ç¡®ç‡ä»¥åŠæŒ‰ç±»åˆ«åˆ’åˆ†çš„å‡†ç¡®ç‡ï¼š
- en: '| **Accuracy %** | **MLP** | **XGBoost** | **GCN** |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| **å‡†ç¡®ç‡ç™¾åˆ†æ¯”** | **MLP** | **XGBoost** | **GCN** |'
- en: '| Overall | 76.5 | 83.9 | **90.9** |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| æ€»ä½“ | 76.5 | 83.9 | **90.9** |'
- en: '| Class 0 | 84.9 | 95.2 | **96.6** |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| ç±»åˆ« 0 | 84.9 | 95.2 | **96.6** |'
- en: '| Class 1 | 72.9 | 78.0 | **88.1** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| ç±»åˆ« 1 | 72.9 | 78.0 | **88.1** |'
- en: '| Class 2 | 33.3 | 57.1 | **71.4** |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ç±»åˆ« 2 | 33.3 | 57.1 | **71.4** |'
- en: '| Class 3 | 68.8 | 37.5 | **75.0** |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ç±»åˆ« 3 | 68.8 | 37.5 | **75.0** |'
- en: Table 9.3 â€“ Class-wise accuracy values in % on the Facebook Page-Page network
    dataset
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨9.3 â€“ åœ¨Facebooké¡µé¢-é¡µé¢ç½‘ç»œæ•°æ®é›†ä¸Šçš„ç±»åˆ«å‡†ç¡®ç‡ç™¾åˆ†æ¯”
- en: 'Here are some insights from *Table 9.3*:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€äº›æ¥è‡ª*è¡¨9.3*çš„è§è§£ï¼š
- en: '**Overall performance**:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ€»ä½“æ€§èƒ½**ï¼š'
- en: GCN outshines both MLP and XGBoost with an overall accuracy of 90.9%. GCN is
    the best for this network data, excelling in all classes.
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCNä»¥90.9%çš„æ•´ä½“å‡†ç¡®ç‡è¶…è¶Šäº†MLPå’ŒXGBoostã€‚GCNå¯¹äºè¿™ç§ç½‘ç»œæ•°æ®æ¥è¯´æ˜¯æœ€å¥½çš„ï¼Œåœ¨æ‰€æœ‰ç±»åˆ«ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚
- en: '**Class-specific insights**:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«ç‰¹å®šè§è§£**ï¼š'
- en: '**Class 0**: GCN and XGBoost do well on class 0.'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç±»åˆ« 0**ï¼šGCNå’ŒXGBooståœ¨ç±»åˆ«0ä¸Šè¡¨ç°è‰¯å¥½ã€‚'
- en: '**Classes 1â€“3**: GCN leads, while MLP and XGBoost struggle, especially in classes
    2 and 3\. Note in particular that on class 3, which had the fewest number of examples
    in the training data, GCN performed significantly better than others.'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç±»åˆ« 1â€“3**ï¼šGCNé¢†å…ˆï¼Œè€ŒMLPå’ŒXGBooståœ¨ç±»åˆ«2å’Œ3ä¸­è¡¨ç°ä¸ä½³ã€‚ç‰¹åˆ«æ˜¯è¦æ³¨æ„ï¼Œåœ¨è®­ç»ƒæ•°æ®ä¸­ç¤ºä¾‹æ•°é‡æœ€å°‘çš„ç±»åˆ«3ä¸Šï¼ŒGCNçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚'
- en: Here, we compared the performance of the traditional ML algorithm of XGBoost
    and a basic MLP deep learning model with GCN, a graph ML model on an imbalanced
    dataset. The results showed that graph ML algorithms can outperform traditional
    algorithms, demonstrating the potential of graph ML to deal with imbalanced data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†XGBoostè¿™ç§ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•ã€åŸºæœ¬çš„MLPæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸GCNï¼ˆå›¾æœºå™¨å­¦ä¹ æ¨¡å‹ï¼‰åœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œå›¾æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥è¶…è¶Šä¼ ç»Ÿç®—æ³•ï¼Œå±•ç¤ºäº†å›¾æœºå™¨å­¦ä¹ å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„æ½œåŠ›ã€‚
- en: The superior performance of graph ML algorithms can be attributed to their ability
    to leverage the structure of the graph. By aggregating information from a nodeâ€™s
    neighborhood, graph ML algorithms can capture local and global patterns in data
    that traditional algorithms might miss.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾æœºå™¨å­¦ä¹ ç®—æ³•ä¼˜è¶Šæ€§èƒ½çš„å½’å› äºå®ƒä»¬èƒ½å¤Ÿåˆ©ç”¨å›¾çš„ç»“æ„ã€‚é€šè¿‡ä»èŠ‚ç‚¹çš„é‚»åŸŸèšåˆä¿¡æ¯ï¼Œå›¾æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥æ•æ‰åˆ°ä¼ ç»Ÿç®—æ³•å¯èƒ½é”™è¿‡çš„æ•°æ®ä¸­çš„å±€éƒ¨å’Œå…¨å±€æ¨¡å¼ã€‚
- en: ğŸš€ Graph ML at Uber and Grab
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ ä¼˜æ­¥å’ŒGrabçš„å›¾æœºå™¨å­¦ä¹ 
- en: 'ğŸ¯ **Problem** **being solved**:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¯ **è§£å†³çš„é—®é¢˜**ï¼š
- en: Both Uber and Grab aimed to tackle the complex issue of fraud across their diverse
    service offerings, ranging from ride-hailing to food delivery and financial services.
    Uber focused on collusion fraud [1], where groups of users work together to commit
    fraud. For instance, users can collaborate to take fake trips using stolen credit
    cards and then request chargebacks from the bank to get refunds for those illegitimate
    purchases. Grab aimed for a general fraud detection framework that could adapt
    to new patterns [2].
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜æ­¥å’ŒGrabéƒ½æ—¨åœ¨è§£å†³å…¶å¤šæ ·åŒ–çš„æœåŠ¡ä¸­å¤æ‚çš„æ¬ºè¯ˆé—®é¢˜ï¼Œä»æ‰“è½¦åˆ°é€é¤å’Œé‡‘èæœåŠ¡ã€‚ä¼˜æ­¥ä¸“æ³¨äºå…±è°‹æ¬ºè¯ˆ[1]ï¼Œå³ç”¨æˆ·ç¾¤ä½“å…±åŒå®æ–½æ¬ºè¯ˆã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥åˆä½œä½¿ç”¨è¢«ç›—ä¿¡ç”¨å¡è¿›è¡Œè™šå‡è¡Œç¨‹ï¼Œç„¶åå‘é“¶è¡Œç”³è¯·é€€æ¬¾ä»¥è·å¾—é‚£äº›éæ³•è´­ä¹°çš„é€€æ¬¾ã€‚Grabæ—¨åœ¨å»ºç«‹ä¸€ä¸ªé€šç”¨çš„æ¬ºè¯ˆæ£€æµ‹æ¡†æ¶ï¼Œèƒ½å¤Ÿé€‚åº”æ–°çš„æ¨¡å¼[2]ã€‚
- en: 'âš–ï¸ **Data** **imbalance issue**:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: âš–ï¸ **æ•°æ®ä¸å¹³è¡¡é—®é¢˜**ï¼š
- en: Fraudulent activities were rare but diverse, creating a class imbalance problem.
    Both companies faced the challenge of adapting to new fraud patterns.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬ºè¯ˆæ´»åŠ¨è™½ç„¶ç½•è§ä½†ç§ç±»ç¹å¤šï¼Œé€ æˆäº†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚ä¸¤å®¶å…¬å¸éƒ½é¢ä¸´ç€é€‚åº”æ–°æ¬ºè¯ˆæ¨¡å¼æŒ‘æˆ˜ã€‚
- en: 'ğŸ¨ **Graph** **modeling strategy**:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¨ **å›¾å»ºæ¨¡ç­–ç•¥**ï¼š
- en: 'â€¢ **Graph models**: Both companies employed **Relational Graph Convolutional
    Networks** (**RGCNs**) to capture complex relationships indicative of fraud. To
    determine whether an Uber user is fraudulent, Uber wanted to leverage not just
    the features of the target user but also the features of users connected to them
    within a defined network distance.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: â€¢ **å›¾æ¨¡å‹**ï¼šä¸¤å®¶å…¬å¸éƒ½é‡‡ç”¨äº†**å…³ç³»å›¾å·ç§¯ç½‘ç»œ**ï¼ˆ**RGCNs**ï¼‰æ¥æ•æ‰æ¬ºè¯ˆçš„å¤æ‚å…³ç³»ã€‚ä¸ºäº†ç¡®å®šä¼˜æ­¥ç”¨æˆ·æ˜¯å¦æ¬ºè¯ˆï¼Œä¼˜æ­¥ä¸ä»…æƒ³åˆ©ç”¨ç›®æ ‡ç”¨æˆ·çš„ç‰¹å¾ï¼Œè¿˜æƒ³åˆ©ç”¨åœ¨å®šä¹‰çš„ç½‘ç»œè·ç¦»å†…ä¸ä¹‹ç›¸è¿çš„ç”¨æˆ·çš„ç‰¹å¾ã€‚
- en: 'â€¢ **Semi-supervised learning**: Grabâ€™s RGCN model was trained on a graph with
    millions of nodes and edges, where only a small percentage had labels. Tree-based
    models rely heavily on quality labels and feature engineering, while graph-based
    models need minimal feature engineering and excel in detecting unknown fraud,
    using graph structures.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: â€¢ **åŠç›‘ç£å­¦ä¹ **ï¼šGrabçš„RGCNæ¨¡å‹åœ¨ä¸€ä¸ªåŒ…å«æ•°ç™¾ä¸‡ä¸ªèŠ‚ç‚¹å’Œè¾¹çš„å›¾ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå…¶ä¸­åªæœ‰ä¸€å°éƒ¨åˆ†æœ‰æ ‡ç­¾ã€‚åŸºäºæ ‘çš„æ¨¡å‹ä¸¥é‡ä¾èµ–äºé«˜è´¨é‡çš„æ ‡ç­¾å’Œç‰¹å¾å·¥ç¨‹ï¼Œè€ŒåŸºäºå›¾çš„æ¨¡å‹éœ€è¦çš„ç‰¹å¾å·¥ç¨‹æœ€å°‘ï¼Œåœ¨æ£€æµ‹æœªçŸ¥æ¬ºè¯ˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåˆ©ç”¨å›¾ç»“æ„ã€‚
- en: 'ğŸ“Š**Real-world impact**:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“Š**å®é™…å½±å“**ï¼š
- en: Graph-based models proved to be effective in detecting both known and unknown
    fraud risks. They require less feature engineering and are less dependent on labels,
    making them a sustainable foundation to combat various types of fraud risks. However,
    Grab does not use RGCN for real-time model prediction due to latency concerns
    [2].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå›¾çš„æ¨¡å‹åœ¨æ£€æµ‹å·²çŸ¥å’ŒæœªçŸ¥æ¬ºè¯ˆé£é™©æ–¹é¢å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚å®ƒä»¬éœ€è¦çš„ç‰¹å¾å·¥ç¨‹è¾ƒå°‘ï¼Œä¸”å¯¹æ ‡ç­¾çš„ä¾èµ–æ€§è¾ƒä½ï¼Œè¿™ä½¿å¾—å®ƒä»¬æˆä¸ºå¯¹æŠ—å„ç§ç±»å‹æ¬ºè¯ˆé£é™©çš„å¯æŒç»­åŸºç¡€ã€‚ç„¶è€Œï¼Œç”±äºå»¶è¿Ÿé—®é¢˜ï¼ŒGrabæ²¡æœ‰ä½¿ç”¨RGCNè¿›è¡Œå®æ—¶æ¨¡å‹é¢„æµ‹[2]ã€‚
- en: 'ğŸ›  **Challenges** **and tips**:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ›  **æŒ‘æˆ˜** **å’ŒæŠ€å·§**ï¼š
- en: 'â€¢ **Data pipeline and scalability**: Large graph sizes necessitated distributed
    training and prediction. Future work was needed to enhance real-time capabilities
    at Uber.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: â€¢ **æ•°æ®ç®¡é“å’Œå¯æ‰©å±•æ€§**ï¼šå¤§å‹å›¾å¤§å°éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒå’Œé¢„æµ‹ã€‚åœ¨Uberï¼Œæœªæ¥å·¥ä½œéœ€è¦å¢å¼ºå®æ—¶èƒ½åŠ›ã€‚
- en: 'â€¢ **Batch real-time prediction**: For Grab, real-time graph updates were computationally
    intensive, making batch real-time predictions a viable solution.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: â€¢ **æ‰¹é‡å®æ—¶é¢„æµ‹**ï¼šå¯¹äºGrabæ¥è¯´ï¼Œå®æ—¶å›¾æ›´æ–°è®¡ç®—å¯†é›†ï¼Œä½¿å¾—æ‰¹é‡å®æ—¶é¢„æµ‹æˆä¸ºå¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚
- en: In conclusion, graph ML offers a promising approach to deal with imbalanced
    data, when the data either inherently has a graph structure or we think we can
    exploit the interconnectedness in the data. By leveraging the rich information
    contained in the graph structure, graph ML algorithms can improve model performance
    and provide more accurate and reliable predictions. As more data becomes available
    and graphs become larger and more complex, its potential to deal with imbalanced
    data will only continue to grow.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œå›¾æœºå™¨å­¦ä¹ ä¸ºå¤„ç†ä¸å¹³è¡¡æ•°æ®æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå½“æ•°æ®æœ¬èº«å…·æœ‰å›¾ç»“æ„ï¼Œæˆ–è€…æˆ‘ä»¬è®¤ä¸ºå¯ä»¥åˆ©ç”¨æ•°æ®ä¸­çš„ç›¸äº’å…³è”æ€§æ—¶ã€‚é€šè¿‡åˆ©ç”¨å›¾ç»“æ„ä¸­åŒ…å«çš„ä¸°å¯Œä¿¡æ¯ï¼Œå›¾æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æä¾›æ›´å‡†ç¡®ã€æ›´å¯é çš„é¢„æµ‹ã€‚éšç€æ•°æ®çš„å¢å¤šå’Œå›¾å˜å¾—æ›´å¤§ã€æ›´å¤æ‚ï¼Œå…¶å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„èƒ½åŠ›å°†åªä¼šæŒç»­å¢é•¿ã€‚
- en: In the following section, we will shift our focus to a different strategy called
    hard example mining, which operates on the principle of prioritizing the most
    challenging examples in our dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠé‡ç‚¹è½¬å‘å¦ä¸€ç§ç§°ä¸ºç¡¬ä¾‹æŒ–æ˜çš„ä¸åŒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºä¼˜å…ˆå¤„ç†æ•°æ®é›†ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„ç¤ºä¾‹çš„åŸåˆ™ã€‚
- en: Hard example mining
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¡¬ä¾‹æŒ–æ˜
- en: 'Hard example mining is a technique in deep learning that forces the model to
    pay more attention to these difficult examples, and to prevent overfitting to
    the majority of the samples that are easy to predict. To do this, hard example
    mining identifies and selects the most challenging samples in the dataset and
    then backpropagates the loss incurred only by those challenging samples. Hard
    example mining is often used in computer vision tasks such as object detection.
    Hard examples can be of two kinds:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡¬ä¾‹æŒ–æ˜æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€ç§æŠ€æœ¯ï¼Œå®ƒè¿«ä½¿æ¨¡å‹æ›´åŠ å…³æ³¨è¿™äº›å›°éš¾ç¤ºä¾‹ï¼Œå¹¶é˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆé‚£äº›å®¹æ˜“é¢„æµ‹çš„å¤§å¤šæ•°æ ·æœ¬ã€‚ä¸ºæ­¤ï¼Œç¡¬ä¾‹æŒ–æ˜è¯†åˆ«å¹¶é€‰æ‹©æ•°æ®é›†ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œç„¶åä»…å¯¹è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬è¿›è¡Œåå‘ä¼ æ’­æŸå¤±ã€‚ç¡¬ä¾‹æŒ–æ˜å¸¸ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚ç›®æ ‡æ£€æµ‹ã€‚ç¡¬ä¾‹å¯ä»¥åˆ†ä¸ºä¸¤ç§ï¼š
- en: '**Hard positive examples** are the correctly labeled examples with low prediction
    scores'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¡¬æ­£ä¾‹**æ˜¯æŒ‡é‚£äº›é¢„æµ‹åˆ†æ•°ä½ä½†æ ‡ç­¾æ­£ç¡®çš„ç¤ºä¾‹'
- en: '**Hard negative examples** are incorrectly labeled examples with high prediction
    scores, which are obvious mistakes made by the model'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¡¬è´Ÿä¾‹**æ˜¯æŒ‡é‚£äº›æ ‡ç­¾é”™è¯¯ä½†é¢„æµ‹åˆ†æ•°é«˜çš„ç¤ºä¾‹ï¼Œè¿™æ˜¯æ¨¡å‹çŠ¯çš„æ˜æ˜¾é”™è¯¯'
- en: The term â€œminingâ€ refers to the process of finding such examples that are â€œhard.â€
    The idea of hard negative mining is not really new and is quite similar to the
    idea of **boosting**, on which the popular algorithms of boosted decision trees
    are based. The boosted decision trees essentially figure out the examples on which
    the model makes mistakes, and then a new model (called a weak learner) is trained
    on such â€œhardâ€ examples.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: â€œæŒ–æ˜â€ä¸€è¯æŒ‡çš„æ˜¯å¯»æ‰¾è¿™äº›â€œå›°éš¾â€ç¤ºä¾‹çš„è¿‡ç¨‹ã€‚ç¡¬è´Ÿä¾‹æŒ–æ˜çš„æƒ³æ³•å®é™…ä¸Šå¹¶ä¸æ–°é¢–ï¼Œå¹¶ä¸”ä¸**æå‡**ï¼ˆboostingï¼‰çš„æƒ³æ³•éå¸¸ç›¸ä¼¼ï¼Œè€Œæå‡æ˜¯æµè¡Œçš„æå‡å†³ç­–æ ‘ç®—æ³•çš„åŸºç¡€ã€‚æå‡å†³ç­–æ ‘æœ¬è´¨ä¸Šç¡®å®šäº†æ¨¡å‹å‡ºé”™çš„åœ°æ–¹ï¼Œç„¶ååœ¨è¿™äº›â€œå›°éš¾â€ç¤ºä¾‹ä¸Šè®­ç»ƒä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼ˆç§°ä¸ºå¼±å­¦ä¹ å™¨ï¼‰ã€‚
- en: When dealing with large datasets, processing all training data to identify difficult
    examples can be time-consuming. This motivates our exploration of the online version
    of hard example mining.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ï¼Œå¤„ç†æ‰€æœ‰è®­ç»ƒæ•°æ®ä»¥è¯†åˆ«å›°éš¾ä¾‹å­å¯èƒ½å¾ˆè€—æ—¶ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬æ¢ç´¢ç¡¬æ ·æœ¬æŒ–æ˜çš„åœ¨çº¿ç‰ˆæœ¬ã€‚
- en: Online Hard Example Mining
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨çº¿ç¡¬æ ·æœ¬æŒ–æ˜
- en: In **Online Hard Example Mining** (**OHEM**) [3], the â€œhardâ€ examples are figured
    out for each batch of the training cycle, where we take the *k* examples, which
    have the lowest value of the loss. We then backpropagate the loss for only those
    *k* examples during the training.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**åœ¨çº¿ç¡¬æ ·æœ¬æŒ–æ˜**ï¼ˆ**OHEM**ï¼‰[3]ä¸­ï¼Œæ¯ä¸ªè®­ç»ƒå‘¨æœŸçš„æ‰¹æ¬¡éƒ½ä¼šç¡®å®šâ€œç¡¬â€çš„ä¾‹å­ï¼Œå…¶ä¸­æˆ‘ä»¬é€‰å–äº†*æœ€å°çš„k*ä¸ªä¾‹å­ï¼Œè¿™äº›ä¾‹å­å…·æœ‰æœ€ä½çš„æŸå¤±å€¼ã€‚ç„¶åæˆ‘ä»¬åªåœ¨è®­ç»ƒä¸­åå‘ä¼ æ’­è¿™äº›*æœ€å°çš„k*ä¸ªä¾‹å­çš„æŸå¤±ã€‚
- en: This way, the network focuses on the most difficult samples that have more information
    than the easy samples, and the model improves faster with less training data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œç½‘ç»œä¸“æ³¨äºæ¯”ç®€å•æ ·æœ¬å…·æœ‰æ›´å¤šä¿¡æ¯çš„æœ€å›°éš¾æ ·æœ¬ï¼Œå¹¶ä¸”æ¨¡å‹åœ¨è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ä¸‹æ›´å¿«åœ°æé«˜ã€‚
- en: The OHEM technique, introduced in the paper by Shrivastava et al. [3], has been
    quite popular. It is a technique primarily used in object detection to improve
    model performance by focusing on challenging cases. It aims to efficiently select
    a subset of â€œhardâ€ negative examples that are most informative to train the model.
    As an example, imagine weâ€™re developing a facial recognition model, and our dataset
    consists of images with faces (positive examples) and images without faces (negative
    examples). In practice, we often encounter a large number of negative examples
    compared to a smaller set of positive ones. To make our training more efficient,
    itâ€™s wise to select a subset of the most challenging negative examples that will
    be most informative for our model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Shrivastavaç­‰äºº[3]åœ¨è®ºæ–‡ä¸­ä»‹ç»çš„å¼€æºç¡¬ä¾‹å­æŒ–æ˜ï¼ˆOHEMï¼‰æŠ€æœ¯ç›¸å½“å—æ¬¢è¿ã€‚è¿™æ˜¯ä¸€ç§ä¸»è¦ç”¨äºç›®æ ‡æ£€æµ‹çš„æŠ€æœ¯ï¼Œé€šè¿‡å…³æ³¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ˆä¾‹æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚å®ƒçš„ç›®æ ‡æ˜¯é«˜æ•ˆåœ°é€‰æ‹©ä¸€ç»„â€œç¡¬â€çš„è´Ÿé¢ä¾‹å­ï¼Œè¿™äº›ä¾‹å­å¯¹è®­ç»ƒæ¨¡å‹æœ€æœ‰ä¿¡æ¯é‡ã€‚ä¾‹å¦‚ï¼Œæƒ³è±¡æˆ‘ä»¬æ­£åœ¨å¼€å‘ä¸€ä¸ªé¢éƒ¨è¯†åˆ«æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†ç”±å¸¦æœ‰é¢éƒ¨ï¼ˆæ­£ä¾‹ï¼‰çš„å›¾åƒå’Œæ²¡æœ‰é¢éƒ¨ï¼ˆè´Ÿä¾‹ï¼‰çš„å›¾åƒç»„æˆã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬ç»å¸¸é‡åˆ°æ¯”æ­£ä¾‹æ•°é‡å°‘å¾—å¤šçš„è´Ÿä¾‹ã€‚ä¸ºäº†ä½¿æˆ‘ä»¬çš„è®­ç»ƒæ›´æœ‰æ•ˆç‡ï¼Œé€‰æ‹©ä¸€ç»„æœ€å…·æŒ‘æˆ˜æ€§çš„è´Ÿé¢ä¾‹å­ï¼Œè¿™äº›ä¾‹å­å¯¹æˆ‘ä»¬çš„æ¨¡å‹æœ€æœ‰ä¿¡æ¯é‡æ˜¯æ˜æ™ºçš„ã€‚
- en: In our experiment, we found that online hard example mining did help the imbalanced
    MNIST dataset and improved our modelâ€™s performance on the most imbalanced classes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°åœ¨çº¿ç¡¬æ ·æœ¬æŒ–æ˜ç¡®å®æœ‰åŠ©äºä¸å¹³è¡¡çš„MNISTæ•°æ®é›†ï¼Œå¹¶æé«˜äº†æˆ‘ä»¬æ¨¡å‹åœ¨æœ€ä¸å¹³è¡¡çš„ç±»åˆ«ä¸Šçš„æ€§èƒ½ã€‚
- en: 'Here is the core implementation of the OHEM function:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯OHEMå‡½æ•°çš„æ ¸å¿ƒå®ç°ï¼š
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the `NLL_OHEM` class, we first computed the regular cross-entropy loss, and
    then we figured out the *k* smallest loss values. These *k* values denote the
    hardest *k* examples that the model had trouble with. We then only propagate those
    *k* loss values during the backpropagation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`NLL_OHEM`ç±»ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®—äº†å¸¸è§„çš„äº¤å‰ç†µæŸå¤±ï¼Œç„¶åç¡®å®šäº†*æœ€å°çš„k*ä¸ªæŸå¤±å€¼ã€‚è¿™äº›*æœ€å°çš„k*ä¸ªå€¼è¡¨ç¤ºæ¨¡å‹éš¾ä»¥å¤„ç†çš„*æœ€å°çš„k*ä¸ªä¾‹å­ã€‚ç„¶åæˆ‘ä»¬åªåœ¨åå‘ä¼ æ’­ä¸­ä¼ æ’­è¿™äº›*æœ€å°çš„k*ä¸ªæŸå¤±å€¼ã€‚
- en: As we did earlier in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235), *Algorithm-Level
    Deep Learning Techniques*, we will continue using the long-tailed version of the
    MNIST dataset (*Figure 9**.10*).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨[*ç¬¬8ç« *](B17259_08.xhtml#_idTextAnchor235)ä¸­æåˆ°çš„ï¼Œ*ç®—æ³•çº§æ·±åº¦å­¦ä¹ æŠ€æœ¯*ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨MNISTæ•°æ®é›†çš„é•¿å°¾ç‰ˆæœ¬ï¼ˆ*å›¾9.10*ï¼‰ã€‚
- en: '![](img/B17259_09_10.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_09_10.jpg)'
- en: Figure 9.10 â€“ An imbalanced MNIST dataset showing the counts of each class
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.10 â€“ ä¸€ä¸ªä¸å¹³è¡¡çš„MNISTæ•°æ®é›†ï¼Œæ˜¾ç¤ºäº†æ¯ä¸ªç±»çš„è®¡æ•°
- en: In *Figure 9**.11*, we show the performance of OHEM loss when compared with
    cross-entropy loss after 20 epochs.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾9.11*ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†OHEMæŸå¤±ä¸äº¤å‰ç†µæŸå¤±åœ¨20ä¸ªepochåçš„æ€§èƒ½ã€‚
- en: '![](img/B17259_09_11.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_09_11.jpg)'
- en: Figure 9.11 â€“ A performance comparison of online hard example mining when compared
    with cross-entropy loss
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.11 â€“ ä¸äº¤å‰ç†µæŸå¤±ç›¸æ¯”çš„åœ¨çº¿ç¡¬æ ·æœ¬æŒ–æ˜æ€§èƒ½æ¯”è¾ƒ
- en: Itâ€™s evident that the most significant improvements are observed for the classes
    with the highest level of imbalance. Though some research works [4] have tried
    to apply OHEM to general problems without much success, we think this is a good
    technique to be aware of in general.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œå¯¹äºä¸å¹³è¡¡ç¨‹åº¦æœ€é«˜çš„ç±»åˆ«ï¼Œè§‚å¯Ÿåˆ°çš„æ”¹è¿›æœ€ä¸ºæ˜¾è‘—ã€‚å°½ç®¡ä¸€äº›ç ”ç©¶å·¥ä½œ[4]è¯•å›¾å°†OHEMåº”ç”¨äºä¸€èˆ¬é—®é¢˜è€Œæ²¡æœ‰å–å¾—å¤ªå¤§æˆåŠŸï¼Œä½†æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå€¼å¾—æ³¨æ„çš„å¥½æŠ€æœ¯ã€‚
- en: In the following section, we will introduce our final topic of minority class
    incremental rectification.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»æˆ‘ä»¬å…³äºå°‘æ•°ç±»å¢é‡æ ¡æ­£çš„æœ€åä¸€ä¸ªä¸»é¢˜ã€‚
- en: Minority class incremental rectification
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°‘æ•°ç±»å¢é‡æ ¡æ­£
- en: Minority class incremental rectification is a deep learning technique that boosts
    the representation of minority classes in imbalanced datasets using a **Class
    Rectification Loss** (**CRL**). This strategy dynamically adjusts to class imbalance,
    enhancing model performance by incorporating hard example mining and other methods.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique is based on the paper by Dong et al. [5][6]. Here are the main
    steps of the technique:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '**Class identification in** **each batch**:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Binary classification**: We consider a class as a minority if it makes up
    less than 50% of the batch. The rest is the majority class.'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class classification**: We define all minority classes as those that
    collectively account for no more than 50% of the batch. The remaining classes
    are treated as majority classes.'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute the class** **rectification loss**:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Locate** **challenging samples**:'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Find hard positives**: We identify samples from the minority class that our
    model incorrectly assesses with low prediction scores.'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Find hard negatives**: We locate samples from other (majority) classes that
    our model mistakenly assigns high prediction scores for the minority class.'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Construct triplets**:'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use minority samples as anchors**: We use each sample from the minority class
    as an anchor for triplet formation.'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create triplets**: We form triplets using an anchor sample, a hard positive,
    and a hard negative.'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculate distances within triplets**: We define the distance (d) between
    matched (anchor and hard positive) and unmatched (anchor and hard negative) pairs
    as follows:'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: d(anchor, hard positive) = âˆ£ Prediction score of anchor âˆ’ Prediction score of
    hard positive âˆ£
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d(anchor, hard negative) = Prediction score of anchor âˆ’ Prediction score of
    hard negative
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Impose margin ranking**: We ensure the distance from the anchor to the hard
    negative is greater than the distance from the anchor to the hard positive, increased
    by a margin.'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Formulate final** **loss function**:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Class imbalance rectification**: We modify the standard cross-entropy loss
    to address the class imbalance by introducing a **CRL** term.'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom loss calculation**: We use the formed triplets to compute an average
    sum of the defined distances.'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss equation**:'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LÂ final = Î± Ã— LÂ CRL + (1 âˆ’ Î±) Ã— LÂ CE
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Here, LÂ CRL is the CRL loss, LÂ CE is the cross-entropy loss, and Î± is a hyperparameter
    dependent upon the amount of class imbalance in the dataset.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_12.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 â€“ A comic illustrating the usage of triplet loss in class rectification
    loss
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the hard sample mining technique in minority class incremental rectification
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The minority class incremental rectification technique uses the hard negative
    technique but with two customizations:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: It uses only minority classes for hard mining
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses both hard positives and hard negatives for loss computation (triplet
    margin loss)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key highlight of the minority class incremental rectification technique
    in handling highly imbalanced datasets is that it uses the triplet margin loss
    on the minority class of the batch that it operates upon. This makes sure that
    the model incrementally optimizes the triplet loss for the minority class in every
    batch.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Our results on imbalanced MNIST data by using `ClassRectificationLoss` were
    relatively mediocre compared to the baseline model that employed cross-entropy
    loss. This performance difference could be due to the techniqueâ€™s suitability
    for very large-scale training data, as opposed to a much smaller dataset such
    as MNIST, which we used here. Please find the complete notebook in the GitHub
    repo.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s worth noting that the original authors of the paper applied this method
    to the CelebA face attribute dataset, which is extensive and multi-label as well
    as multi-class. *Table 9.4* presents the results from the paper, where they used
    a five-layer CNN as a baseline and compared CRL with oversampling, undersampling,
    and cost-sensitive techniques.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attributes (****imbalance ratio)** | **Baseline (****five-layer CNN)**
    | **Over-sampling** | **Under-sampling** | **Cost-sensitive** | **CRL** |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| Bald (1:43) | 93 | 92 | 79 | 93 | **99** |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| Mustache (1:24) | 88 | 90 | 60 | 88 | **93** |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| Gray hair (1:23) | 90 | 90 | 88 | 90 | **96** |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| Pale skin (1:22) | 81 | 82 | 78 | 80 | **92** |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| Double chin (1:20) | 83 | 84 | 80 | 84 | **89** |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: Table 9.4 â€“ A performance comparison of CRL on facial attribute recognition
    on the CelebA benchmark, using class-balanced accuracy (in %) (adapted from Dong
    et al. [6])
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: As is evident from the table, the CRL technique consistently outperforms other
    methods across various facial attributes, even in high imbalance scenarios. Specifically,
    for the **Bald** attribute, with a 1:43 imbalance ratio, CRL achieved a remarkable
    99% accuracy. Its effectiveness is also evident in attributes such as **Mustache**
    and **Gray hair**, where it surpassed the baseline by 5% and 6%, respectively.
    This demonstrates CRLâ€™s superior ability to address class imbalances.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_13.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 â€“ A visual representation of CRL regularization in rectifying model
    biases from class-imbalanced training data
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the `ClassRectificationLoss` class provides a custom loss function
    that combines triplet loss and negative log-likelihood loss while also considering
    class imbalance in the dataset. This can be a useful tool to train models on imbalanced
    datasets where the minority class samples are of particular interest.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explored a few modern deep learning strategies to handle imbalanced
    data, including graph ML, hard example mining, and minority class incremental
    rectification. By blending data-level and algorithm-level techniques, and sometimes
    even transitioning a problem paradigm from tabular to graph-based data representation,
    we can effectively leverage challenging examples, improve the representation of
    less common classes, and advance our ability to manage data imbalance.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to graph ML and saw how it can be useful
    for certain imbalanced datasets. We trained and compared the performance of the
    GCN model with baselines of XGBoost and MLP on the Facebook page-page dataset.
    For certain datasets (including tabular ones), where we are able to leverage the
    rich and interconnected structure of the graph data, the graph ML models can beat
    even XGBoost models. As we continue to encounter increasingly complex and interconnected
    data, the importance and relevance of graph ML models will only continue to grow.
    Understanding and utilizing these algorithms can be invaluable in your arsenal.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: We then went over a hard mining technique, where the â€œhardâ€ examples with the
    lowest loss values are first identified. Then, the loss for only *k* such examples
    is backpropagated in order to force a model to focus on the minority class examples,
    which the model has the most trouble learning about. Finally, we deep-dived into
    another hybrid deep learning technique called minority class incremental rectification.
    This method employs triplet loss on examples that are mined using the online hard
    example mining technique. Because the minority class incremental rectification
    method combines hard sample mining from minority groups with a regularized objective
    function, known as CRL, it is considered a hybrid approach that combines both
    data-level and algorithm-level deep learning techniques.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: We hope this chapter equipped you with the confidence to extract key insights
    from new techniques and understand their main ideas, taken directly from research
    papers.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will talk about model calibration, its importance,
    and some of the popular techniques to calibrate ML models.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apply triplet loss to the imbalanced MNIST dataset, and see whether the modelâ€™s
    performance is better than using the cross-entropy loss function.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply minority class incremental rectification technique to the imbalanced datasets
    â€“ CIFAR10-LT and CIFAR100-LT. For a reference implementation of this technique
    on the MNIST-LT dataset, you can refer to the accompanying GitHub notebook.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Fraud Detection: Using Relational Graph Learning to Detect Collusion (**2021)*:
    [https://www.uber.com/blog/fraud-detection/](https://www.uber.com/blog/fraud-detection/).'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Graph for fraud detection (**2022)*: [https://engineering.grab.com/graph-for-fraud-detection](https://engineering.grab.com/graph-for-fraud-detection).'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. Shrivastava, A. Gupta, and R. Girshick, *â€œTraining Region-Based Object Detectors
    with Online Hard Example Mining,â€* in 2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), Las Vegas, NV, USA, Jun. 2016, pp. 761â€“769: doi:
    10.1109/CVPR.2016.89.'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Marius Schmidt-Mengin, ThÃ©odore Soulier, Mariem Hamzaoui, Arya Yazdan-Panah,
    Benedetta Bod-ini, et al. *â€œOnline hard example mining vs. fixed oversampling
    strategy for segmentation of new multiple sclerosis lesions from longitudinal
    FLAIR MRIâ€*. Frontiers in Neuroscience, 2022, 16, pp.100405\. 10.3389/fnins.2022.1004050\.
    hal-03836922.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q. Dong, S. Gong, and X. Zhu, *â€œClass Rectification Hard Mining for Imbalanced
    Deep Learning,â€* in 2017 IEEE International Conference on Computer Vision (ICCV),
    Venice, Oct. 2017, pp. 1869â€“1878\. doi: 10.1109/ICCV.2017.205.'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q. Dong, S. Gong, and X. Zhu, *â€œImbalanced Deep Learning by Minority Class
    Incremental Rectification.â€* arXiv, Apr. 28, 2018\. Accessed: Jul. 26, 2022\.
    [Online]. Available: [http://arxiv.org/abs/1804.10851](http://arxiv.org/abs/1804.10851).'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
