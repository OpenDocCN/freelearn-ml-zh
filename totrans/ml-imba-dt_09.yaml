- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Hybrid Deep Learning Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合深度学习方法
- en: In this chapter, we will talk about some of the hybrid deep learning techniques
    that combine the data-level ([*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*) and algorithm-level ([*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep Learning Techniques*) methods in some ways. This chapter
    contains some recent and more advanced techniques that can be challenging to implement,
    so it is recommended to have a good understanding of the previous chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一些混合深度学习技术，这些技术以某种方式结合了数据级（[*第7章*](B17259_07.xhtml#_idTextAnchor205)，*数据级深度学习方法*）和算法级（[*第8章*](B17259_08.xhtml#_idTextAnchor235)，*算法级深度学习技术*）方法。本章包含一些最近且更高级的技术，可能难以实现，因此建议您对前几章有良好的理解。
- en: We will begin with an introduction to graph machine learning, clarifying how
    graph models exploit relationships within data to boost performance, especially
    for minority classes. Through a side-by-side comparison of a **Graph Convolutional
    Network** (**GCN**), XGBoost, and MLP models, using an imbalanced social network
    dataset, we will highlight the superior performance of the GCN.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍图机器学习的基础，阐明图模型如何利用数据中的关系来提升性能，尤其是在少数类别的应用中。通过将**图卷积网络**（**GCN**）、XGBoost和MLP模型进行并排比较，使用不平衡的社会网络数据集，我们将突出GCN的优越性能。
- en: We will continue to explore strategies to tackle class imbalance in deep learning,
    examining techniques that manipulate data distribution and prioritize challenging
    examples. We will also go over techniques called hard example mining and minority
    class incremental rectification, which focus on improving model performance through
    prioritization of difficult instances and iterative enhancement of minority class
    representation, respectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续探索解决深度学习中类别不平衡的策略，检查操纵数据分布和优先处理挑战性示例的技术。我们还将介绍称为硬示例挖掘和少数类增量校正的技术，它们分别通过优先处理困难实例和迭代增强少数类表示来提高模型性能。
- en: While a significant portion of our discussion will revolve around image datasets,
    notably the imbalanced MNIST, it’s crucial to understand the broader applicability
    of these techniques. For instance, our deep dive into graph machine learning won’t
    rely on MNIST. Instead, we’ll switch gears to a more realistic dataset from Facebook,
    offering a fresh perspective on handling imbalances in real-world scenarios.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的大部分讨论将围绕图像数据集展开，特别是不平衡的MNIST数据集，但理解这些技术的更广泛适用性至关重要。例如，我们对图机器学习的深入研究不会依赖于MNIST。相反，我们将转向来自Facebook的更真实的数据集，为处理现实场景中的不平衡问题提供新的视角。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Graph machine learning for imbalanced data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不平衡数据的图机器学习
- en: Hard example mining
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬示例挖掘
- en: Minority class incremental rectification
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少数类增量校正
- en: By the end of this chapter, we will become familiar with some hybrid methods,
    enabling us to understand the core principles behind more complex techniques.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将熟悉一些混合方法，使我们能够理解更复杂技术背后的核心原理。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `numpy`, `pandas`, `sklearn`, and `torch`. For graph machine learning, we will
    use the `torch_geometric` library as well. The code and notebooks for this chapter
    are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09).
    You can open the GitHub notebook using Google Colab by clicking on the **Open
    in Colab** icon at the top of the chapter’s notebook or by launching it from [https://colab.research.google.com](https://colab.research.google.com),
    using the GitHub URL of the notebook.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，我们将继续使用常见的库，如`numpy`、`pandas`、`sklearn`和`torch`。对于图机器学习，我们将使用`torch_geometric`库。本章的代码和笔记本可在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09)。您可以通过点击章节笔记本顶部的**在Colab中打开**图标或在[https://colab.research.google.com](https://colab.research.google.com)使用笔记本的GitHub
    URL来打开GitHub笔记本。
- en: Using graph machine learning for imbalanced data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用图机器学习处理不平衡数据
- en: In this section, we will see when graphs can be useful tools in machine learning,
    when to use graph ML models in general, and how they can be helpful on certain
    kinds of imbalanced datasets. We’ll also be exploring how graph ML models can
    outperform classical models such as XGBoost on certain imbalanced datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨在机器学习中何时图形是有用的工具，一般何时使用图机器学习模型，以及它们在特定类型的失衡数据集上如何有所帮助。我们还将探讨图机器学习模型如何在某些失衡数据集上优于经典模型，如XGBoost。
- en: Graphs are incredibly versatile data structures that can represent complex relationships
    and structures, from social networks and web pages (think of links as edges) to
    molecules in chemistry (consider atoms as nodes and the bonds between them as
    edges) and various other domains. Graph models allow us to represent the relationships
    in data, which can be helpful to make predictions and gain insights, even for
    problems where the relationships are not explicitly defined.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图形是极其灵活的数据结构，可以表示复杂的关系和结构，从社交网络和网页（将链接视为边）到化学中的分子（将原子视为节点，它们之间的键视为边）以及各种其他领域。图模型使我们能够表示数据中的关系，这对于预测和洞察力是有帮助的，即使对于关系没有明确定义的问题也是如此。
- en: Understanding graphs
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解图形
- en: Graphs are the foundation of graph ML, so it’s important to understand them
    first. In the context of computer science, a graph is a collection of nodes (or
    vertices) and edges. Nodes represent entities, and edges represent relationships
    or interactions between these entities. For example, in a social network, each
    person can be a node, and the friendship between two people can be an edge.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图形是图机器学习的基础，因此首先理解它们是很重要的。在计算机科学领域，图是由节点（或顶点）和边组成的一个集合。节点代表实体，边代表这些实体之间的关系或交互。例如，在一个社交网络中，每个人可以是一个节点，两个人之间的友谊可以是一条边。
- en: Graphs can be either directed (edges have a direction) or undirected (edges
    do not have a direction). They can also be weighted (edges have a value) or unweighted
    (edges do not have a value).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图形可以是定向的（边有方向）或非定向的（边没有方向）。它们也可以是有权重的（边有值）或无权重的（边没有值）。
- en: '*Figure 9**.1* shows a sample tabular dataset on the left and its corresponding
    graph representation on the right:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.1* 展示了一个样本表格数据集在左侧及其对应的图形表示在右侧：'
- en: '![](img/B17259_09_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_09_01.jpg)'
- en: Figure 9.1 – Tabular data (left) contrasted with its visual graph representation
    (right)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 表格数据（左侧）与其视觉图形表示（右侧）进行对比
- en: The graph representation on the right emphasizes the relationships between various
    entities. In the tabular representation on the left, devices and their IP addresses
    are listed with connection details along with network bandwidth. The graphical
    representation on the right visually represents these connections, allowing easier
    comprehension of the network topology. Devices are nodes, connections are edges
    with bandwidth are the weights. Graphs provide a clearer view of interrelationships
    than tables, emphasizing network design insights.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的图形表示强调了各种实体之间的关系。在左侧的表格表示中，设备和它们的IP地址以及网络带宽的连接细节被列出。右侧的图形表示直观地表示了这些连接，使得网络拓扑结构更容易理解。设备是节点，连接是带有带宽的边，权重。图形比表格提供了更清晰的相互关系视图，强调了网络设计洞察。
- en: In the following section, we will get an overview of how ML can be applied to
    graphs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将概述机器学习如何应用于图形。
- en: Graph machine learning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图机器学习
- en: '**Graph Machine Learning** (**GML**) is a set of techniques that use the structure
    of a graph to extract features and make predictions. GML algorithms can leverage
    the rich information contained in the graph structure, such as the connections
    between nodes and the patterns of these connections, to improve model performance,
    especially on imbalanced data.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**图机器学习**（**GML**）是一组使用图的架构来提取特征和进行预测的技术。GML算法可以利用图结构中包含的丰富信息，例如节点之间的连接和这些连接的模式，以提高模型性能，尤其是在失衡数据上。'
- en: Two popular neural network GML algorithms are GCNs and **Graph Attention Networks**
    (**GATs**). Both algorithms use the graph structure to aggregate information from
    a node’s neighborhood. However, they differ in how they weigh the importance of
    a node’s neighbors. GCN gives equal weight to all neighbors, while GAT uses attention
    mechanisms to assign different weights to different neighbors. We will limit our
    discussion to GCNs only in this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 两种流行的神经网络GML算法是GCNs和**图注意力网络**（**GATs**）。这两种算法都使用图结构从节点的邻域中聚合信息。然而，它们在如何权衡节点邻居的重要性方面有所不同。GCN对所有邻居给予相同的权重，而GAT使用注意力机制为不同的邻居分配不同的权重。在本章中，我们将仅讨论GCNs。
- en: Dealing with imbalanced data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理不平衡数据
- en: In ML, when one class significantly outnumbers the others, the model may become
    biased toward the majority class, leading to poor performance on the minority
    class. This is problematic because, often, the minority class is the one of interest.
    For example, in fraud detection, the number of non-fraud cases significantly outnumbers
    the fraud cases, but it’s the fraud cases that we’re interested in detecting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，当一个类别显著多于其他类别时，模型可能会偏向多数类别，导致对少数类别的性能不佳。这是问题所在，因为通常，少数类别才是我们感兴趣的。例如，在欺诈检测中，非欺诈案例的数量显著多于欺诈案例，但我们感兴趣的是检测欺诈案例。
- en: In the context of GML, the structure of the graph can provide additional information
    that can help mitigate the effects of data imbalance. For example, minority class
    nodes might be more closely connected to each other than to majority class nodes.
    GML algorithms can leverage this structure to improve the performance of the minority
    class.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在GML的背景下，图的结构可以提供额外的信息，有助于减轻数据不平衡的影响。例如，少数类别的节点可能比多数类别的节点彼此之间更紧密地连接。GML算法可以利用这种结构来提高少数类别的性能。
- en: GCNs
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GCNs
- en: We will briefly discuss the key ideas behind what GCNs are and how they work.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要讨论GCNs背后的关键思想和它们是如何工作的。
- en: GCNs offer a unique method of processing structured data. Unlike standard neural
    networks that assume independent and identically distributed data, GCNs can operate
    over graph data, capturing dependencies and connections between nodes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GCNs提供了一种独特的处理结构化数据的方法。与假设独立同分布数据的标准神经网络不同，GCNs可以在图数据上操作，捕捉节点之间的依赖和连接。
- en: 'The essence of GCNs is message passing, which can be broken down as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GCNs的本质是消息传递，可以分解如下：
- en: '**Node messaging**: Each node in the graph sends out and receives messages
    through its edges to and from its neighbors'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点消息传递**：图中的每个节点通过其边向其邻居发送和接收消息'
- en: '**Aggregation**: Nodes aggregate these messages to gain a broader understanding
    of these local neighborhoods'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合**：节点将这些消息聚合起来，以获得对这些局部邻域更广泛的理解'
- en: In GCNs, full feature vectors are passed around instead of just the labels of
    the nodes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCNs中，传递的是完整的特征向量，而不是仅仅节点的标签。
- en: 'Think of a GCN layer as a transformation step. The primary operations can be
    viewed as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将GCN层视为一个转换步骤。主要操作可以看作如下：
- en: '**Aggregate neighbors**: Nodes pull features from their neighbors, leading
    to an aggregation'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合邻居**：节点从其邻居那里提取特征，导致聚合'
- en: '**Neural network transformation**: The aggregated feature set from the previous
    step then undergoes transformation via the neural network layer'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络转换**：前一步骤中聚合的特征集随后通过神经网络层进行转换'
- en: Let’s explore GCNs using the example of photos on Facebook. Users upload photos,
    and our objective is to categorize these images as either spam or non-spam. This
    categorization is based on the image content as well as the IP address or user
    ID.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过Facebook上的照片示例来探索GCNs。用户上传照片，我们的目标是将这些图像分类为垃圾邮件或非垃圾邮件。这种分类基于图像内容以及IP地址或用户ID。
- en: Let’s imagine we have a graph where each node is a Facebook photo, and two photos
    are connected if they were uploaded using either the same IP address or the same
    account.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象我们有一个图，其中每个节点是一张Facebook照片，如果两张照片是使用相同的IP地址或相同的账户上传的，那么这两张照片就是相连的。
- en: Let’s say we want to use the actual content of the photo (possibly a feature
    vector from a pre-trained CNN or some metadata) as a node attribute. Let’s assume
    that we have a 5-dimensional vector representing each photo’s features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要使用照片的实际内容（可能是一个从预训练的CNN或某些元数据中得到的特征向量）作为节点属性。假设我们有一个5维向量来表示每张照片的特征。
- en: '![](img/B17259_09_02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_02.jpg)'
- en: Figure 9.2 – An image with a 5-dimensional feature embedding
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – graph creation
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will create a graph where every vertex symbolizes a Facebook image. We will
    establish a link between two images if they were uploaded via an identical IP
    address or user ID.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – A diagram with images connected to each other if they share the
    IP or user ID
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – image representation
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Represent each image using a 5-dimensional vector. This can be accomplished
    by using image metadata, features derived from trained neural networks, or other
    techniques suitable for image data (*Figure 9**.2*).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – a singular-layer GCN for image analysis
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When a specific image is passed through a single-layer GCN, here is what will
    happen:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: We aggregate all the neighboring image’s feature vectors. The neighbors are
    images with matching IP addresses or user IDs.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An average function is used to combine vectors. Let’s call the combined vector
    the neighborhood average vector.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The neighborhood average vector is multiplied using a weight matrix (of, say,
    size 5x1, as shown in *Figure 9**.4*).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, an activation function is applied to the outcome to get a single value,
    which suggests the likelihood of the image being spam.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17259_09_04.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – How a single GCN layer works
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – a dual-layer GCN
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multilayer GCNs, like traditional deep neural networks, can be stacked into
    multiple layers:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Raw node features feed into the first layer
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subsequent layer’s input is the previous layer’s output
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With each added layer, GCNs grasp more extended neighborhood information. For
    instance, in a two-layer GCN, information can ripple two hops away in the graph.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: With a foundational understanding of graph ML and GCNs in place, we’re set to
    explore a case study. We’ll compare the performance of a graph model to other
    models, including a classical ML model, on an imbalanced graph dataset. Our aim
    is to determine whether graph models can outperform other models by leveraging
    the relationships between graph structures.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Case study – the performance of XGBoost, MLP, and a GCN on an imbalanced dataset
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the Facebook Page-Page dataset from the **PyTorch Geometric** (**PyG**)
    library, designed to create and train deep learning models on graphs and other
    irregular structures. This dataset comprises a large collection of social networks
    from Facebook, where nodes represent official Facebook pages and edges signify
    reciprocal likes between them. Each node is labeled with one of four categories:
    Politicians, Governmental Organizations, Television Shows, or Companies. The task
    is to predict these categories based on the node characteristics, which are derived
    from descriptions provided by the page owners.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The dataset serves as a challenging benchmark for graph neural network models
    due to its size and complexity. It was collected via the Facebook Graph API in
    November 2017 and focuses on multi-class node classification within the four aforementioned
    categories. You can find more about the dataset at [https://snap.stanford.edu/data/facebook-large-page-page-network.html](https://snap.stanford.edu/data/facebook-large-page-page-network.html).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集由于其规模和复杂性，成为图神经网络模型的一个具有挑战性的基准。它于2017年11月通过Facebook Graph API收集，并专注于上述四个类别中的多类节点分类。您可以在[https://snap.stanford.edu/data/facebook-large-page-page-network.html](https://snap.stanford.edu/data/facebook-large-page-page-network.html)了解更多关于该数据集的信息。
- en: 'We start by importing some of the common libraries and the Facebook dataset:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入一些常用库和Facebook数据集：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The data object here is of type `torch_geometric.data`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里数据对象的数据类型为`torch_geometric.data`。
- en: 'Here are some statistics about the graph data:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于图数据的统计数据：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s print the features and label in a tabular format:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以表格格式打印特征和标签：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This prints the features and labels contained in the `dfx` DataFrame:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这打印出包含在`dfx` DataFrame中的特征和标签：
- en: '|  | **1** | **2** | **…** | **127** | **label** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | **1** | **2** | **…** | **127** | **标签** |'
- en: '| 0 | -0.262576 | -0.276483 | … | -0.223836 | 0 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 0 | -0.262576 | -0.276483 | … | -0.223836 | 0 |'
- en: '| 1 | -0.262576 | -0.276483 | … | -0.128634 | 2 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 1 | -0.262576 | -0.276483 | … | -0.128634 | 2 |'
- en: '| 2 | -0.262576 | -0.265053 | … | -0.223836 | 1 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 2 | -0.262576 | -0.265053 | … | -0.223836 | 1 |'
- en: '| ... | ... | ... | ... | ... | ... |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... | ... |'
- en: '| 22468 | -0.262576 | -0.276483 | … | -0.218148 | 1 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 22468 | -0.262576 | -0.276483 | … | -0.218148 | 1 |'
- en: '| 22469 | -0.232275 | -0.276483 | … | -0.221275 | 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 22469 | -0.232275 | -0.276483 | … | -0.221275 | 0 |'
- en: Table 9.1 – A dataset with each row showing feature values; the last column
    shows the label for each data point
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 – 每行显示特征值的数据集；最后一列显示每个数据点的标签
- en: 'The overall printed result is as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的整体结果如下：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These 127 features have been generated using the Doc2Vec technique from the
    page description text. These features act like an embedding vector for each Facebook
    page.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这127个特征是通过从页面描述文本中使用Doc2Vec技术生成的。这些特征就像每个Facebook页面的嵌入向量。
- en: 'In *Figure 9**.5*, we visualize the dataset using Gephi, which is a graph visualization
    software:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.5*中，我们使用Gephi（一种图形可视化软件）可视化数据集：
- en: '![](img/B17259_09_05.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_05.jpg)'
- en: Figure 9.5 – The Facebook Page-Page dataset Gephi visualization
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – Facebook页面-页面数据集Gephi可视化
- en: 'The graph contains both inter-category and intra-category connections, but
    the latter is more dominant, highlighting the mutual-like affinity within the
    same category. This leads to distinct clusters, offering a bird’s-eye view of
    the strong intra-category affiliations on Facebook. If we analyze the original
    dataset for various classes, their proportion is not so imbalanced. So, let’s
    add some imbalance by removing some nodes randomly (as shown in *Figure 9**.6*):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该图包含跨类别和类别内的连接，但后者更为突出，突显了同一类别内的相互类似亲和力。这导致了不同的集群，提供了对Facebook上强大的类别内关联的鸟瞰图。如果我们分析原始数据集中各种类别的数据，它们的比例并不那么不平衡。因此，让我们通过随机删除一些节点来添加一些不平衡（如图*9.6*所示）：
- en: '| **Class** | **Number of nodes in the** **original dataset** | **Number of
    nodes after removing** **some nodes** |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **原始数据集中节点的数量** | **删除一些节点后的节点数量** |'
- en: '| 0 | 3,327 | 3,327 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 3,327 | 3,327 |'
- en: '| 1 | 6,495 | 1,410 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6,495 | 1,410 |'
- en: '| 2 | 6,880 | 460 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 6,880 | 460 |'
- en: '| 3 | 5,768 | 256 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 5,768 | 256 |'
- en: Table 9.2 – The distribution of various classes in the dataset
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.2 – 数据集中各种类别的分布
- en: 'Here is what the distribution of data looks like after adding imbalance:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 添加不平衡后数据的分布如下所示：
- en: '![](img/B17259_09_06.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_06.jpg)'
- en: Figure 9.6 – The distribution of various classes after adding imbalance
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 添加不平衡后各种类别的分布
- en: 'Let’s split the data into training and test sets by specifying their ranges
    of indices. In the `Data` object, we can specify this range to denote training
    and test sets, using masks:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过指定索引的范围将数据分为训练集和测试集。在`Data`对象中，我们可以使用掩码指定此范围以表示训练集和测试集：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Training an XGBoost model
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练XGBoost模型
- en: Let’s set up a simple baseline using an XGBoost model on this dataset.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在该数据集上使用XGBoost模型设置一个简单的基线。
- en: 'First, let’s create our train/test dataset using the masks we created earlier:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用我们之前创建的掩码来创建我们的训练/测试数据集：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we train and evaluate on the data:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在数据上训练和评估：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This prints the following accuracy value on the test set:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这在测试集上打印出以下准确度值：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s plot the PR curve:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制PR曲线：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This prints the PR curve (*Figure 9**.7*) and area for various classes, using
    the XGBoost model. The area for the most imbalanced class, 3, is the lowest, as
    expected.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印出使用XGBoost模型的各个类别的PR曲线（*图9**.7*）和面积。最不平衡的类别3的面积最低，正如预期的那样。
- en: '![](img/B17259_09_07.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_07.jpg)'
- en: Figure 9.7 – The PR curve using XGBoost
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 使用XGBoost的PR曲线
- en: Training a MultiLayer Perceptron model
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练多层感知器模型
- en: We can set up another baseline using the simplest of deep learning models, the
    **MultiLayer Perceptron** (**MLP**). *Figure 9**.8* shows the PR curve for each
    class. Overall, the MLP did worse than XGBoost, but its performance on the most
    imbalanced class, 3, is better than that of XGBoost.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用最简单的深度学习模型，即**多层感知器**（**MLP**）来设置另一个基线。*图9**.8*显示了每个类的PR曲线。总体而言，MLP的表现不如XGBoost，但在最不平衡的类别3上的表现优于XGBoost。
- en: '![](img/B17259_09_08.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_08.jpg)'
- en: Figure 9.8 – The PR curve using the MLP model
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 使用MLP模型的PR曲线
- en: Training a GCN model
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练GCN模型
- en: Finally, we switch to using a graph convolutional network, which is a generalization
    of the convolution layers in CNNs. A GCN, as we discussed previously, uses the
    structure of the graph to update the features of each node, based on its neighbors’
    features. In other words, each node gets to learn from its friends!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们切换到使用图卷积网络，这是CNN中卷积层的一种推广。正如我们之前讨论的，GCN使用图的结构根据其邻居的特征更新每个节点的特征。换句话说，每个节点都可以从它的朋友那里学习！
- en: 'The first step involves importing the required libraries. Here, we import `PyTorch`,
    the `GCNConv` module from PyG, and the `functional` module from PyTorch:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步涉及导入所需的库。在这里，我们导入`PyTorch`、PyG中的`GCNConv`模块和PyTorch的`functional`模块：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `GraphConvolutionalNetwork` class is a representation of our model. The
    class inherits from PyTorch’s `nn.Module`. It contains an initializer, a forward
    function for forward propagation, a function to train the model, and a function
    to evaluate the model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`GraphConvolutionalNetwork`类是我们模型的表示。该类继承自PyTorch的`nn.Module`。它包含一个初始化器、一个用于前向传播的前向函数、一个训练模型的函数和一个评估模型的函数：'
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the `__init__()` function, we initialize the layers of the model. Our model
    contains two **Graph Convolutional Network layers** (**GCNConv layers**). The
    dimensions of the input, hidden, and output layers are required as arguments.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在`__init__()`函数中，我们初始化模型的层。我们的模型包含两个**图卷积网络层**（**GCNConv层**）。输入、隐藏和输出层的维度作为参数要求。
- en: 'Then, we define a `forward()` function to perform forward propagation through
    the network. It takes the node features and edge index as input, applies the first
    GCN layer followed by a ReLU activation function, and then applies the second
    GCN layer. The function then applies a `log_softmax` activation function and returns
    the result:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个`forward()`函数来通过网络执行前向传播。它接受节点特征和边索引作为输入，应用第一个GCN层，然后是ReLU激活函数，接着应用第二个GCN层。该函数随后应用`log_softmax`激活函数并返回结果：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `train_model()` function trains the model. It takes in the data and the
    number of epochs as input. It sets the model to training mode and initializes
    the **negative log-likelihood loss** (**NLLLoss**) as the loss function, and Adam
    as the optimizer. It then runs a loop for the specified number of epochs to train
    the model. Within each epoch, it computes the output of the model, calculates
    the loss and accuracy, performs backpropagation, and updates the model parameters.
    It also calculates and prints the training and validation losses and accuracies
    every 20 epochs:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_model()`函数用于训练模型。它接受数据和epoch数作为输入。它将模型设置为训练模式，并将**负对数似然损失**（**NLLLoss**）作为损失函数，Adam作为优化器。然后它运行指定数量的epoch来训练模型。在每个epoch中，它计算模型的输出，计算损失和准确率，执行反向传播并更新模型参数。它还会每20个epoch计算并打印训练和验证的损失和准确率：'
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `evaluate_model` function is used to evaluate the model. It sets the model
    to evaluation mode and calculates the output of the model and the test accuracy.
    It returns the test accuracy and the output for the test data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_model`函数用于评估模型。它将模型设置为评估模式，并计算模型的输出和测试准确率。它返回测试准确率和测试数据的输出。'
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We start the training process and then evaluate the model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始训练过程，然后评估模型：
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produces the following output:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s print the PR curves:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印PR曲线：
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/B17259_09_09.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_09_09.jpg)'
- en: Figure 9.9 – The PR curve using the GCN model
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – 使用GCN模型的PR曲线
- en: 'In *Table 9.3*, we compare the overall accuracy values as well as class-wise
    accuracy values of various models:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在*表9.3*中，我们比较了各种模型的整体准确率以及按类别划分的准确率：
- en: '| **Accuracy %** | **MLP** | **XGBoost** | **GCN** |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| **准确率百分比** | **MLP** | **XGBoost** | **GCN** |'
- en: '| Overall | 76.5 | 83.9 | **90.9** |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | 76.5 | 83.9 | **90.9** |'
- en: '| Class 0 | 84.9 | 95.2 | **96.6** |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 类别 0 | 84.9 | 95.2 | **96.6** |'
- en: '| Class 1 | 72.9 | 78.0 | **88.1** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 类别 1 | 72.9 | 78.0 | **88.1** |'
- en: '| Class 2 | 33.3 | 57.1 | **71.4** |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 类别 2 | 33.3 | 57.1 | **71.4** |'
- en: '| Class 3 | 68.8 | 37.5 | **75.0** |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 类别 3 | 68.8 | 37.5 | **75.0** |'
- en: Table 9.3 – Class-wise accuracy values in % on the Facebook Page-Page network
    dataset
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.3 – 在Facebook页面-页面网络数据集上的类别准确率百分比
- en: 'Here are some insights from *Table 9.3*:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些来自*表9.3*的见解：
- en: '**Overall performance**:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总体性能**：'
- en: GCN outshines both MLP and XGBoost with an overall accuracy of 90.9%. GCN is
    the best for this network data, excelling in all classes.
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCN以90.9%的整体准确率超越了MLP和XGBoost。GCN对于这种网络数据来说是最好的，在所有类别中都表现出色。
- en: '**Class-specific insights**:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别特定见解**：'
- en: '**Class 0**: GCN and XGBoost do well on class 0.'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别 0**：GCN和XGBoost在类别0上表现良好。'
- en: '**Classes 1–3**: GCN leads, while MLP and XGBoost struggle, especially in classes
    2 and 3\. Note in particular that on class 3, which had the fewest number of examples
    in the training data, GCN performed significantly better than others.'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别 1–3**：GCN领先，而MLP和XGBoost在类别2和3中表现不佳。特别是要注意，在训练数据中示例数量最少的类别3上，GCN的表现显著优于其他模型。'
- en: Here, we compared the performance of the traditional ML algorithm of XGBoost
    and a basic MLP deep learning model with GCN, a graph ML model on an imbalanced
    dataset. The results showed that graph ML algorithms can outperform traditional
    algorithms, demonstrating the potential of graph ML to deal with imbalanced data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们比较了XGBoost这种传统机器学习算法、基本的MLP深度学习模型与GCN（图机器学习模型）在不平衡数据集上的性能。结果显示，图机器学习算法可以超越传统算法，展示了图机器学习处理不平衡数据的潜力。
- en: The superior performance of graph ML algorithms can be attributed to their ability
    to leverage the structure of the graph. By aggregating information from a node’s
    neighborhood, graph ML algorithms can capture local and global patterns in data
    that traditional algorithms might miss.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图机器学习算法优越性能的归因于它们能够利用图的结构。通过从节点的邻域聚合信息，图机器学习算法可以捕捉到传统算法可能错过的数据中的局部和全局模式。
- en: 🚀 Graph ML at Uber and Grab
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 优步和Grab的图机器学习
- en: '🎯 **Problem** **being solved**:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 🎯 **解决的问题**：
- en: Both Uber and Grab aimed to tackle the complex issue of fraud across their diverse
    service offerings, ranging from ride-hailing to food delivery and financial services.
    Uber focused on collusion fraud [1], where groups of users work together to commit
    fraud. For instance, users can collaborate to take fake trips using stolen credit
    cards and then request chargebacks from the bank to get refunds for those illegitimate
    purchases. Grab aimed for a general fraud detection framework that could adapt
    to new patterns [2].
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 优步和Grab都旨在解决其多样化的服务中复杂的欺诈问题，从打车到送餐和金融服务。优步专注于共谋欺诈[1]，即用户群体共同实施欺诈。例如，用户可以合作使用被盗信用卡进行虚假行程，然后向银行申请退款以获得那些非法购买的退款。Grab旨在建立一个通用的欺诈检测框架，能够适应新的模式[2]。
- en: '⚖️ **Data** **imbalance issue**:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ⚖️ **数据不平衡问题**：
- en: Fraudulent activities were rare but diverse, creating a class imbalance problem.
    Both companies faced the challenge of adapting to new fraud patterns.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈活动虽然罕见但种类繁多，造成了类别不平衡问题。两家公司都面临着适应新欺诈模式挑战。
- en: '🎨 **Graph** **modeling strategy**:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 🎨 **图建模策略**：
- en: '• **Graph models**: Both companies employed **Relational Graph Convolutional
    Networks** (**RGCNs**) to capture complex relationships indicative of fraud. To
    determine whether an Uber user is fraudulent, Uber wanted to leverage not just
    the features of the target user but also the features of users connected to them
    within a defined network distance.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: • **图模型**：两家公司都采用了**关系图卷积网络**（**RGCNs**）来捕捉欺诈的复杂关系。为了确定优步用户是否欺诈，优步不仅想利用目标用户的特征，还想利用在定义的网络距离内与之相连的用户的特征。
- en: '• **Semi-supervised learning**: Grab’s RGCN model was trained on a graph with
    millions of nodes and edges, where only a small percentage had labels. Tree-based
    models rely heavily on quality labels and feature engineering, while graph-based
    models need minimal feature engineering and excel in detecting unknown fraud,
    using graph structures.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: • **半监督学习**：Grab的RGCN模型在一个包含数百万个节点和边的图上进行了训练，其中只有一小部分有标签。基于树的模型严重依赖于高质量的标签和特征工程，而基于图的模型需要的特征工程最少，在检测未知欺诈方面表现出色，利用图结构。
- en: '📊**Real-world impact**:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 📊**实际影响**：
- en: Graph-based models proved to be effective in detecting both known and unknown
    fraud risks. They require less feature engineering and are less dependent on labels,
    making them a sustainable foundation to combat various types of fraud risks. However,
    Grab does not use RGCN for real-time model prediction due to latency concerns
    [2].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的模型在检测已知和未知欺诈风险方面已被证明是有效的。它们需要的特征工程较少，且对标签的依赖性较低，这使得它们成为对抗各种类型欺诈风险的可持续基础。然而，由于延迟问题，Grab没有使用RGCN进行实时模型预测[2]。
- en: '🛠 **Challenges** **and tips**:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 🛠 **挑战** **和技巧**：
- en: '• **Data pipeline and scalability**: Large graph sizes necessitated distributed
    training and prediction. Future work was needed to enhance real-time capabilities
    at Uber.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: • **数据管道和可扩展性**：大型图大小需要分布式训练和预测。在Uber，未来工作需要增强实时能力。
- en: '• **Batch real-time prediction**: For Grab, real-time graph updates were computationally
    intensive, making batch real-time predictions a viable solution.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: • **批量实时预测**：对于Grab来说，实时图更新计算密集，使得批量实时预测成为可行的解决方案。
- en: In conclusion, graph ML offers a promising approach to deal with imbalanced
    data, when the data either inherently has a graph structure or we think we can
    exploit the interconnectedness in the data. By leveraging the rich information
    contained in the graph structure, graph ML algorithms can improve model performance
    and provide more accurate and reliable predictions. As more data becomes available
    and graphs become larger and more complex, its potential to deal with imbalanced
    data will only continue to grow.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，图机器学习为处理不平衡数据提供了一种有前景的方法，当数据本身具有图结构，或者我们认为可以利用数据中的相互关联性时。通过利用图结构中包含的丰富信息，图机器学习算法可以提高模型性能，并提供更准确、更可靠的预测。随着数据的增多和图变得更大、更复杂，其处理不平衡数据的能力将只会持续增长。
- en: In the following section, we will shift our focus to a different strategy called
    hard example mining, which operates on the principle of prioritizing the most
    challenging examples in our dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把重点转向另一种称为硬例挖掘的不同策略，该策略基于优先处理数据集中最具挑战性的示例的原则。
- en: Hard example mining
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬例挖掘
- en: 'Hard example mining is a technique in deep learning that forces the model to
    pay more attention to these difficult examples, and to prevent overfitting to
    the majority of the samples that are easy to predict. To do this, hard example
    mining identifies and selects the most challenging samples in the dataset and
    then backpropagates the loss incurred only by those challenging samples. Hard
    example mining is often used in computer vision tasks such as object detection.
    Hard examples can be of two kinds:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 硬例挖掘是深度学习中的一种技术，它迫使模型更加关注这些困难示例，并防止模型过度拟合那些容易预测的大多数样本。为此，硬例挖掘识别并选择数据集中最具挑战性的样本，然后仅对这些具有挑战性的样本进行反向传播损失。硬例挖掘常用于计算机视觉任务，如目标检测。硬例可以分为两种：
- en: '**Hard positive examples** are the correctly labeled examples with low prediction
    scores'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬正例**是指那些预测分数低但标签正确的示例'
- en: '**Hard negative examples** are incorrectly labeled examples with high prediction
    scores, which are obvious mistakes made by the model'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬负例**是指那些标签错误但预测分数高的示例，这是模型犯的明显错误'
- en: The term “mining” refers to the process of finding such examples that are “hard.”
    The idea of hard negative mining is not really new and is quite similar to the
    idea of **boosting**, on which the popular algorithms of boosted decision trees
    are based. The boosted decision trees essentially figure out the examples on which
    the model makes mistakes, and then a new model (called a weak learner) is trained
    on such “hard” examples.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: “挖掘”一词指的是寻找这些“困难”示例的过程。硬负例挖掘的想法实际上并不新颖，并且与**提升**（boosting）的想法非常相似，而提升是流行的提升决策树算法的基础。提升决策树本质上确定了模型出错的地方，然后在这些“困难”示例上训练一个新的模型（称为弱学习器）。
- en: When dealing with large datasets, processing all training data to identify difficult
    examples can be time-consuming. This motivates our exploration of the online version
    of hard example mining.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大型数据集时，处理所有训练数据以识别困难例子可能很耗时。这促使我们探索硬样本挖掘的在线版本。
- en: Online Hard Example Mining
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线硬样本挖掘
- en: In **Online Hard Example Mining** (**OHEM**) [3], the “hard” examples are figured
    out for each batch of the training cycle, where we take the *k* examples, which
    have the lowest value of the loss. We then backpropagate the loss for only those
    *k* examples during the training.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在**在线硬样本挖掘**（**OHEM**）[3]中，每个训练周期的批次都会确定“硬”的例子，其中我们选取了*最小的k*个例子，这些例子具有最低的损失值。然后我们只在训练中反向传播这些*最小的k*个例子的损失。
- en: This way, the network focuses on the most difficult samples that have more information
    than the easy samples, and the model improves faster with less training data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，网络专注于比简单样本具有更多信息的最困难样本，并且模型在较少的训练数据下更快地提高。
- en: The OHEM technique, introduced in the paper by Shrivastava et al. [3], has been
    quite popular. It is a technique primarily used in object detection to improve
    model performance by focusing on challenging cases. It aims to efficiently select
    a subset of “hard” negative examples that are most informative to train the model.
    As an example, imagine we’re developing a facial recognition model, and our dataset
    consists of images with faces (positive examples) and images without faces (negative
    examples). In practice, we often encounter a large number of negative examples
    compared to a smaller set of positive ones. To make our training more efficient,
    it’s wise to select a subset of the most challenging negative examples that will
    be most informative for our model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Shrivastava等人[3]在论文中介绍的开源硬例子挖掘（OHEM）技术相当受欢迎。这是一种主要用于目标检测的技术，通过关注具有挑战性的案例来提高模型性能。它的目标是高效地选择一组“硬”的负面例子，这些例子对训练模型最有信息量。例如，想象我们正在开发一个面部识别模型，我们的数据集由带有面部（正例）的图像和没有面部（负例）的图像组成。在实践中，我们经常遇到比正例数量少得多的负例。为了使我们的训练更有效率，选择一组最具挑战性的负面例子，这些例子对我们的模型最有信息量是明智的。
- en: In our experiment, we found that online hard example mining did help the imbalanced
    MNIST dataset and improved our model’s performance on the most imbalanced classes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们发现在线硬样本挖掘确实有助于不平衡的MNIST数据集，并提高了我们模型在最不平衡的类别上的性能。
- en: 'Here is the core implementation of the OHEM function:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是OHEM函数的核心实现：
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the `NLL_OHEM` class, we first computed the regular cross-entropy loss, and
    then we figured out the *k* smallest loss values. These *k* values denote the
    hardest *k* examples that the model had trouble with. We then only propagate those
    *k* loss values during the backpropagation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在`NLL_OHEM`类中，我们首先计算了常规的交叉熵损失，然后确定了*最小的k*个损失值。这些*最小的k*个值表示模型难以处理的*最小的k*个例子。然后我们只在反向传播中传播这些*最小的k*个损失值。
- en: As we did earlier in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235), *Algorithm-Level
    Deep Learning Techniques*, we will continue using the long-tailed version of the
    MNIST dataset (*Figure 9**.10*).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第8章*](B17259_08.xhtml#_idTextAnchor235)中提到的，*算法级深度学习技术*，我们将继续使用MNIST数据集的长尾版本（*图9.10*）。
- en: '![](img/B17259_09_10.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_09_10.jpg)'
- en: Figure 9.10 – An imbalanced MNIST dataset showing the counts of each class
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – 一个不平衡的MNIST数据集，显示了每个类的计数
- en: In *Figure 9**.11*, we show the performance of OHEM loss when compared with
    cross-entropy loss after 20 epochs.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.11*中，我们展示了OHEM损失与交叉熵损失在20个epoch后的性能。
- en: '![](img/B17259_09_11.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_09_11.jpg)'
- en: Figure 9.11 – A performance comparison of online hard example mining when compared
    with cross-entropy loss
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 – 与交叉熵损失相比的在线硬样本挖掘性能比较
- en: It’s evident that the most significant improvements are observed for the classes
    with the highest level of imbalance. Though some research works [4] have tried
    to apply OHEM to general problems without much success, we think this is a good
    technique to be aware of in general.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，对于不平衡程度最高的类别，观察到的改进最为显著。尽管一些研究工作[4]试图将OHEM应用于一般问题而没有取得太大成功，但我们认为这是一个值得注意的好技术。
- en: In the following section, we will introduce our final topic of minority class
    incremental rectification.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍我们关于少数类增量校正的最后一个主题。
- en: Minority class incremental rectification
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 少数类增量校正
- en: Minority class incremental rectification is a deep learning technique that boosts
    the representation of minority classes in imbalanced datasets using a **Class
    Rectification Loss** (**CRL**). This strategy dynamically adjusts to class imbalance,
    enhancing model performance by incorporating hard example mining and other methods.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique is based on the paper by Dong et al. [5][6]. Here are the main
    steps of the technique:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '**Class identification in** **each batch**:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Binary classification**: We consider a class as a minority if it makes up
    less than 50% of the batch. The rest is the majority class.'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class classification**: We define all minority classes as those that
    collectively account for no more than 50% of the batch. The remaining classes
    are treated as majority classes.'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute the class** **rectification loss**:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Locate** **challenging samples**:'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Find hard positives**: We identify samples from the minority class that our
    model incorrectly assesses with low prediction scores.'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Find hard negatives**: We locate samples from other (majority) classes that
    our model mistakenly assigns high prediction scores for the minority class.'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Construct triplets**:'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use minority samples as anchors**: We use each sample from the minority class
    as an anchor for triplet formation.'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create triplets**: We form triplets using an anchor sample, a hard positive,
    and a hard negative.'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculate distances within triplets**: We define the distance (d) between
    matched (anchor and hard positive) and unmatched (anchor and hard negative) pairs
    as follows:'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: d(anchor, hard positive) = ∣ Prediction score of anchor − Prediction score of
    hard positive ∣
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d(anchor, hard negative) = Prediction score of anchor − Prediction score of
    hard negative
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Impose margin ranking**: We ensure the distance from the anchor to the hard
    negative is greater than the distance from the anchor to the hard positive, increased
    by a margin.'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Formulate final** **loss function**:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Class imbalance rectification**: We modify the standard cross-entropy loss
    to address the class imbalance by introducing a **CRL** term.'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom loss calculation**: We use the formed triplets to compute an average
    sum of the defined distances.'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss equation**:'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: L final = α × L CRL + (1 − α) × L CE
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Here, L CRL is the CRL loss, L CE is the cross-entropy loss, and α is a hyperparameter
    dependent upon the amount of class imbalance in the dataset.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_12.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – A comic illustrating the usage of triplet loss in class rectification
    loss
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the hard sample mining technique in minority class incremental rectification
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The minority class incremental rectification technique uses the hard negative
    technique but with two customizations:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: It uses only minority classes for hard mining
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses both hard positives and hard negatives for loss computation (triplet
    margin loss)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key highlight of the minority class incremental rectification technique
    in handling highly imbalanced datasets is that it uses the triplet margin loss
    on the minority class of the batch that it operates upon. This makes sure that
    the model incrementally optimizes the triplet loss for the minority class in every
    batch.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Our results on imbalanced MNIST data by using `ClassRectificationLoss` were
    relatively mediocre compared to the baseline model that employed cross-entropy
    loss. This performance difference could be due to the technique’s suitability
    for very large-scale training data, as opposed to a much smaller dataset such
    as MNIST, which we used here. Please find the complete notebook in the GitHub
    repo.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that the original authors of the paper applied this method
    to the CelebA face attribute dataset, which is extensive and multi-label as well
    as multi-class. *Table 9.4* presents the results from the paper, where they used
    a five-layer CNN as a baseline and compared CRL with oversampling, undersampling,
    and cost-sensitive techniques.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attributes (****imbalance ratio)** | **Baseline (****five-layer CNN)**
    | **Over-sampling** | **Under-sampling** | **Cost-sensitive** | **CRL** |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| Bald (1:43) | 93 | 92 | 79 | 93 | **99** |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| Mustache (1:24) | 88 | 90 | 60 | 88 | **93** |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| Gray hair (1:23) | 90 | 90 | 88 | 90 | **96** |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| Pale skin (1:22) | 81 | 82 | 78 | 80 | **92** |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| Double chin (1:20) | 83 | 84 | 80 | 84 | **89** |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: Table 9.4 – A performance comparison of CRL on facial attribute recognition
    on the CelebA benchmark, using class-balanced accuracy (in %) (adapted from Dong
    et al. [6])
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: As is evident from the table, the CRL technique consistently outperforms other
    methods across various facial attributes, even in high imbalance scenarios. Specifically,
    for the **Bald** attribute, with a 1:43 imbalance ratio, CRL achieved a remarkable
    99% accuracy. Its effectiveness is also evident in attributes such as **Mustache**
    and **Gray hair**, where it surpassed the baseline by 5% and 6%, respectively.
    This demonstrates CRL’s superior ability to address class imbalances.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_09_13.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – A visual representation of CRL regularization in rectifying model
    biases from class-imbalanced training data
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the `ClassRectificationLoss` class provides a custom loss function
    that combines triplet loss and negative log-likelihood loss while also considering
    class imbalance in the dataset. This can be a useful tool to train models on imbalanced
    datasets where the minority class samples are of particular interest.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explored a few modern deep learning strategies to handle imbalanced
    data, including graph ML, hard example mining, and minority class incremental
    rectification. By blending data-level and algorithm-level techniques, and sometimes
    even transitioning a problem paradigm from tabular to graph-based data representation,
    we can effectively leverage challenging examples, improve the representation of
    less common classes, and advance our ability to manage data imbalance.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to graph ML and saw how it can be useful
    for certain imbalanced datasets. We trained and compared the performance of the
    GCN model with baselines of XGBoost and MLP on the Facebook page-page dataset.
    For certain datasets (including tabular ones), where we are able to leverage the
    rich and interconnected structure of the graph data, the graph ML models can beat
    even XGBoost models. As we continue to encounter increasingly complex and interconnected
    data, the importance and relevance of graph ML models will only continue to grow.
    Understanding and utilizing these algorithms can be invaluable in your arsenal.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: We then went over a hard mining technique, where the “hard” examples with the
    lowest loss values are first identified. Then, the loss for only *k* such examples
    is backpropagated in order to force a model to focus on the minority class examples,
    which the model has the most trouble learning about. Finally, we deep-dived into
    another hybrid deep learning technique called minority class incremental rectification.
    This method employs triplet loss on examples that are mined using the online hard
    example mining technique. Because the minority class incremental rectification
    method combines hard sample mining from minority groups with a regularized objective
    function, known as CRL, it is considered a hybrid approach that combines both
    data-level and algorithm-level deep learning techniques.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: We hope this chapter equipped you with the confidence to extract key insights
    from new techniques and understand their main ideas, taken directly from research
    papers.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will talk about model calibration, its importance,
    and some of the popular techniques to calibrate ML models.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apply triplet loss to the imbalanced MNIST dataset, and see whether the model’s
    performance is better than using the cross-entropy loss function.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply minority class incremental rectification technique to the imbalanced datasets
    – CIFAR10-LT and CIFAR100-LT. For a reference implementation of this technique
    on the MNIST-LT dataset, you can refer to the accompanying GitHub notebook.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Fraud Detection: Using Relational Graph Learning to Detect Collusion (**2021)*:
    [https://www.uber.com/blog/fraud-detection/](https://www.uber.com/blog/fraud-detection/).'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Graph for fraud detection (**2022)*: [https://engineering.grab.com/graph-for-fraud-detection](https://engineering.grab.com/graph-for-fraud-detection).'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. Shrivastava, A. Gupta, and R. Girshick, *“Training Region-Based Object Detectors
    with Online Hard Example Mining,”* in 2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), Las Vegas, NV, USA, Jun. 2016, pp. 761–769: doi:
    10.1109/CVPR.2016.89.'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Marius Schmidt-Mengin, Théodore Soulier, Mariem Hamzaoui, Arya Yazdan-Panah,
    Benedetta Bod-ini, et al. *“Online hard example mining vs. fixed oversampling
    strategy for segmentation of new multiple sclerosis lesions from longitudinal
    FLAIR MRI”*. Frontiers in Neuroscience, 2022, 16, pp.100405\. 10.3389/fnins.2022.1004050\.
    hal-03836922.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q. Dong, S. Gong, and X. Zhu, *“Class Rectification Hard Mining for Imbalanced
    Deep Learning,”* in 2017 IEEE International Conference on Computer Vision (ICCV),
    Venice, Oct. 2017, pp. 1869–1878\. doi: 10.1109/ICCV.2017.205.'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q. Dong, S. Gong, and X. Zhu, *“Imbalanced Deep Learning by Minority Class
    Incremental Rectification.”* arXiv, Apr. 28, 2018\. Accessed: Jul. 26, 2022\.
    [Online]. Available: [http://arxiv.org/abs/1804.10851](http://arxiv.org/abs/1804.10851).'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
