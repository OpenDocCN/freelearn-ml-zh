<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Recommending Movies Using Affinity Analysis</h1>
            </header>

            <article>
                
<p><span><span><span>In this chapter, we will look at <strong>affinity analysis</strong> which determines when objects occur frequently together. This is also colloquially called market basket analysis, after one of the common use cases - determining when items are purchased together frequently in a store.</span></span></span></p>
<p><span><span><span>In <span class="packt_screen"><a href="e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml">Chapter 3</a></span><em>, Predicting Sports Winners with Decision Trees</em>, we looked at an object as a focus and used features to describe that object. In this chapter, the data has a different form. We have transactions where the objects of interest (movies, in this chapter) are used within those transactions in some way. The aim is to discover when objects occur simultaneously. In a case where we wish to work out when two movies are recommended by the same reviewers, we can use affinity analysis.</span></span></span></p>
<p><span><span><span>The key concepts of this chapter are as follows:</span></span></span></p>
<ul>
<li><span><span><span>Affinity analysis for product recommendations</span></span></span></li>
<li><span><span><span>Feature association mining using the Apriori algorithm</span></span></span></li>
<li><span><span><span>Recommendation Systems and the inherent challenges </span></span></span></li>
<li><span><span><span>Sparse data formats and how to use them</span></span></span></li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Affinity analysis</h1>
            </header>

            <article>
                
<p><span><span><span>Affinity analysis is the task of determining when objects are used in similar ways. In the previous chapter, we focused on whether the objects themselves are similar - in our case whether the games were similar in nature. The data for affinity analysis is often described in the form of a transaction. Intuitively, this comes from a transaction at a store—determining when objects are purchased together as a way to recommend products to users that they might purchase.</span></span></span></p>
<p><span><span><span>However, affinity analysis can be applied to many processes that do not use transactions in this sense:</span></span></span></p>
<ul>
<li><span><span><span>Fraud detection</span></span></span></li>
<li><span><span><span>Customer segmentation</span></span></span></li>
<li><span><span><span>Software optimization</span></span></span></li>
<li><span><span><span>Product recommendations</span></span></span></li>
</ul>
<p><span><span><span>Affinity analysis is usually much more exploratory than classification. At the very least, we often simply rank the results and choose the top five recommendations (or some other number), rather than expect the algorithm to give us a specific answer.</span></span></span></p>
<p><span><span><span>Furthermore, we often don't have the complete dataset we expect for many classification tasks. For instance, in movie recommendation, we have reviews from different people on different movies. However, it is highly unlikely we have each reviewer review all of the movies in our dataset. This leaves an important and difficult question in affinity analysis. If a reviewer hasn't reviewed a movie, is that an indication that they aren't interested in the movie (and therefore wouldn't recommend it) or simply that they haven't reviewed it yet?</span></span></span></p>
<p><span><span><span>Thinking about gaps in your datasets can lead to questions like this. In turn, that can lead to answers that may help improve the efficacy of your approach. As a budding data miner, knowing where your models and methodologies need improvement is key to creating great results.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Algorithms for affinity analysis</h1>
            </header>

            <article>
                
<p><span><span><span>We introduced a basic method for affinity analysis in</span></span></span> <a href="">Chapter 1</a><em>, Getting Started with Data Mining</em><span><span><span>, which tested all of the possible rule combinations. We computed the confidence and support for each rule, which in turn allowed us to rank them to find the best rules.</span></span></span></p>
<p><span><span><span>However, this approach is not efficient. Our dataset in <a href="">Chapter 1</a><em>, Getting Started with Data Mining</em>, had just five items for sale. We could expect even a small store to have hundreds of items for sale, while many online stores would have thousands (or millions!). With a naive rule creation, such as our previous algorithm from <a href="">Chapter 1</a><em>, Getting Started with Data Mining</em>, the growth in the time needed to compute these rules increases exponentially. As we add more items, the time it takes to compute all rules increases significantly faster. Specifically, the total possible number of rules is</span></span></span> <em>2n - 1</em><span><span><span>. For our five-item dataset, there are 31 possible rules. For 10 items, it is 1023. For just 100 items, the number has 30 digits. Even the drastic increase in computing power couldn't possibly keep up with the increases in the number of items stored online. Therefore, we need algorithms that work smarter, as opposed to computers that work harder.</span></span></span></p>
<p><span><span><span>The classic algorithm for affinity analysis is called the <strong>Apriori algorithm</strong>. It addresses the exponential problem of creating sets of items that occur frequently within a database, called <strong>frequent itemsets</strong>. Once these frequent itemsets are discovered, creating association rules is straightforward, which we will see later in the chapter.</span></span></span></p>
<p><span><span><span>The intuition behind Apriori is both simple and clever. First, we ensure that a rule has sufficient <span><span>support</span></span> within the dataset. Defining a minimum support level is the key parameter for Apriori. To build a frequent itemset we combine smaller frequent itemsets. For itemset <span class="packt_screen">(A, B)</span> to have a support of at least 30, both <span class="packt_screen">A</span> and <span class="packt_screen">B</span> must occur at least 30 times in the database. This property extends to larger sets as well. For an itemset <span class="packt_screen">(A, B, C, D)</span> to be considered frequent, the set <span class="packt_screen">(A, B, C)</span> must also be frequent (as must <span class="packt_screen">D</span>).</span></span></span></p>
<p><span><span><span>These <span><span>frequent itemsets</span></span> can be built and possible itemsets that are not frequent (of which there are many) will never be tested. This saves significant time in testing new rules, as the number of frequent itemsets is expected to be significantly fewer than the total number of possible itemsets.</span></span></span></p>
<p><span><span><span>Other example algorithms for affinity analysis build on this, or similar concepts, including the <strong>Eclat</strong> and <strong>FP-growth</strong> algorithms. There are many improvements to these algorithms in the data mining literature that further improve the efficiency of the method. In this chapter, we will focus on the basic Apriori algorithm.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Overall methodology</h1>
            </header>

            <article>
                
<p><span><span><span>To perform association rule mining for affinity analysis, we first use the Apriori algorithm to generate frequent itemsets. Next, we create association rules (for example, <em>if a person recommended movie X, they would also recommend movie Y</em>) by testing combinations of premises and conclusions within those frequent itemsets.</span></span></span></p>
<ol>
<li><span><span><span>For the first stage, the Apriori algorithm needs a value for the minimum support that an itemset needs to be considered frequent. Any itemsets with less support will not be considered.</span></span></span></li>
</ol>
<div class="packt_infobox"><span><span><span>Setting this minimum support too low will cause Apriori to test a larger number of itemsets, slowing the algorithm down. Setting it too high will result in fewer itemsets being considered frequent.</span></span></span></div>
<ol start="2">
<li>In the second stage, after the frequent itemsets have been discovered, association rules are tested based on their confidence. We could choose a minimum confidence level, a number of rules to return, or simply return all of them and let the user decide what to do with them.</li>
</ol>
<div class="packt_tip"><span><span><span>In this chapter, we will return only rules above a given confidence level. Therefore, we need to set our minimum confidence level. Setting this too low will result in rules that have a high support, but are not very accurate. Setting this higher will result in only more accurate rules being returned, but with fewer rules being discovered overall.</span></span></span></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Dealing with the movie recommendation problem</h1>
            </header>

            <article>
                
<p><span><span><span>Product recommendation is a big business. Online stores use it to up-sell to customers by recommending other products that they could buy. Making better recommendations leads to better sales. When online shopping is selling to millions of customers every year, there is a lot of potential money to be made by selling more items to these customers.</span></span></span></p>
<p><span><span><span>Product recommendations, including movie and books, have been researched for many years; however, the field gained a significant boost when Netflix ran their Netflix Prize between 2007 and 2009. This competition aimed to determine if anyone can predict a user's rating of a film better than Netflix was currently doing. The prize went to a team that was just over 10 percent better than the current solution. While this may not seem like a large improvement, such an improvement would net millions to Netflix in revenue from better movie recommendations over the following years.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Obtaining the dataset</h1>
            </header>

            <article>
                
<p><span><span><span>Since the inception of the Netflix Prize, Grouplens, a research group at the University of Minnesota, has released several datasets that are often used for testing algorithms in this area. They have released several versions of a movie rating dataset, which have different sizes. There is a version with 100,000 reviews, one with 1 million reviews and one with 10 million reviews.</span></span></span></p>
<p><span><span><span>The datasets are available from <span><span><span><span><span><a href="http://grouplens.org/datasets/movielens/">http://grouplens.org/datasets/movielens/</a></span></span></span></span></span> and the dataset we are going to use in this chapter is the <em>MovieLens 100K dataset</em> (with 100,000 reviews). Download this dataset and unzip it in your data folder. Start a new Jupyter Notebook and type the following code:</span></span></span></p>
<pre>
<span><span><span>import os<br/></span></span></span><span><span><span>import pandas as pd<br/></span></span></span><span><span><span>data_folder = os.path.join(os.path.expanduser("~"), "Data", "ml-100k")<br/></span></span></span><span><span><span>ratings_filename = os.path.join(data_folder, "u.data")</span></span></span>
</pre>
<p><span><span><span>Ensure that <kbd><span><span><span><span><span>ratings_filename</span></span></span></span></span></kbd> points to the <span><span><span><span><span><span class="packt_screen">u.data</span></span></span></span></span></span> file in the unzipped folder.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Loading with pandas</h1>
            </header>

            <article>
                
<p><span><span><span>The <kbd>MovieLens</kbd> dataset is in a good shape; however, there are some changes from the default options in <kbd><span><span><span><span><span>pandas.read_csv</span></span></span></span></span></kbd> that we need to make. To start with, the data is separated by tabs, not commas. Next, there is no heading line. This means the first line in the file is actually data and we need to manually set the column names.</span></span></span></p>
<p><span><span><span>When loading the file, we set the delimiter parameter to the tab character, tell pandas not to read the first row as the header (with <kbd><span><span><span><span><span>header=None</span></span></span></span></span></kbd>) and to set the column names with given values. Let's look at the following code:</span></span></span></p>
<pre>
<span><span><span>all_ratings = pd.read_csv(ratings_filename, delimiter="t", header=None, names<br/>            = ["UserID", "MovieID", "Rating", "Datetime"])</span></span></span>
</pre>
<p><span><span><span>While we won't use it in this chapter, you can properly parse the date timestamp using the following line. Dates for reviews can be an important feature in recommendation prediction, as movies that are rated together often have more similar rankings than movies ranked separately. Accounting for this can improve models significantly.</span></span></span></p>
<pre>
<span><span><span>all_ratings["Datetime"] = pd.to_datetime(all_ratings['Datetime'], unit='s')</span></span></span>
</pre>
<p><span><span><span>You can view the first few records by running the following in a new cell:</span></span></span></p>
<pre>
<span><span><span>all_ratings.head()</span></span></span>
</pre>
<p><span><span><span>The result will come out looking something like this:</span></span></span></p>
<table>
<tbody>
<tr>
<td/>
<td><span><span><span><span><span><span>UserID</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>MovieID</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>Rating</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>Datetime</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span><span><span><span>0</span></span></span></span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>196</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>242</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>3</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1997-12-04 15:55:49</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span><span><span><span>1</span></span></span></span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>186</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>302</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>3</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1998-04-04 19:22:22</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span><span><span><span>2</span></span></span></span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>22</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>377</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1997-11-07 07:18:36</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span><span><span><span>3</span></span></span></span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>244</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>51</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>2</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1997-11-27 05:02:03</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span><span><span><span>4</span></span></span></span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>166</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>346</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1998-02-02 05:33:16</span></span></span></span></span></span></td>
</tr>
</tbody>
</table>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Sparse data formats</h1>
            </header>

            <article>
                
<p><span><span><span>This dataset is in a sparse format. Each row can be thought of as a cell in a large feature matrix of the type used in previous chapters, where rows are users and columns are individual movies. The first column would be each user's review of the first movie, the second column would be each user's review of the second movie, and so on.</span></span></span></p>
<p><span><span><span>There are around 1,000 users and 1,700 movies in this dataset, which means that the full matrix would be quite large (nearly 2 million entries). We may run into issues storing the whole matrix in memory and computing on it would be troublesome. However, this matrix has the property that most cells are empty, that is, there is no review for most movies for most users. There is no review of movie number <span><span><span><span><span>675</span></span></span></span></span> for user number <span><span><span><span><span>213</span></span></span></span></span> though, and not for most other combinations of user and movie.</span></span></span></p>
<p><span><span><span>The format given here represents the full matrix, but in a more compact way. The first row indicates that user  number <span><span><span><span><span>196</span></span></span></span></span> reviewed movie number <span><span><span><span><span>242</span></span></span></span></span>, giving it a ranking of 3 (out of five) on December 4, 1997.</span></span></span></p>
<p><span><span><span>Any combination of user and movie that isn't in this database is assumed to not exist. This saves significant space, as opposed to storing a bunch of zeroes in memory. This type of format is called a <span><span><span>sparse matrix</span></span></span> format. As a rule of thumb, if you expect about 60 percent or more of your dataset to be empty or zero, a sparse format will take less space to store.</span></span></span></p>
<div class="packt_infobox"><span><span><span>When computing on sparse matrices, the focus isn't usually on the data we don't have—comparing all of the zeroes. We usually focus on the data we have and compare those.</span></span></span></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Understanding the Apriori algorithm and its implementation</h1>
            </header>

            <article>
                
<p><span><span><span>The goal of this chapter is to produce rules of the following form: <em>if a person recommends this set of movies, they will also recommend this movie</em>. We will also discuss extensions where a person who recommends a set of movies, is likely to recommend another particular movie.</span></span></span></p>
<p><span><span><span>To do this, we first need to determine if a person recommends a movie. We can do this by creating a new feature <span><span><span><span><span><span class="packt_screen">Favorable</span></span></span></span></span></span>, which is <span><span><span><span><span><span class="packt_screen">True</span></span></span></span></span></span> if the person gave a favorable review to a movie:</span></span></span></p>
<pre>
<span><span><span>all_ratings["Favorable"] = all_ratings["Rating"] &gt; 3</span></span></span>
</pre>
<p><span><span><span>We can see the new feature by viewing the dataset:</span></span></span></p>
<pre>
<span><span><span>all_ratings[10:15]</span></span></span>
</pre>
<table>
<tbody>
<tr>
<td/>
<td><span><span><span><span><span><span>UserID</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>MovieID</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>Rating</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>Datetime</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>Favorable</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span><span><span><span>10</span></span></span></span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>62</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>257</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>2</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1997-11-12 22:07:14</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>False</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span><span><span><span>11</span></span></span></span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>286</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1014</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>5</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1997-11-17 15:38:45</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>True</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span><span><span><span>12</span></span></span></span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>200</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>222</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>5</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1997-10-05 09:05:40</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>True</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span><span><span><span>13</span></span></span></span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>210</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>40</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>3</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1998-03-27 21:59:54</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>False</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span><span><span><span>14</span></span></span></span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>224</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>29</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>3</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>1998-02-21 23:40:57</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>False</span></span></span></span></span></span></td>
</tr>
</tbody>
</table>
<p><span><span><span>We will sample our dataset to form training data. This also helps reduce the size of the dataset that will be searched, making the Apriori algorithm run faster. We obtain all reviews from the first 200 users:</span></span></span></p>
<pre>
<span><span><span>ratings = all_ratings[all_ratings['UserID'].isin(range(200))]</span></span></span>
</pre>
<p><span><span><span>Next, we can create a dataset of only the favorable reviews in our sample:</span></span></span></p>
<pre>
<span><span><span>favorable_ratings_mask = ratings["Favorable"]<br/>favorable_ratings = ratings[favorable_ratings_mask]</span></span></span>
</pre>
<p><span><span><span>We will be searching the user's favorable reviews for our itemsets. So, the next thing we need is the movies which each user has given a favorable rating. We can compute this by grouping the dataset by the</span></span></span> <kbd>UserID</kbd> <span><span><span>and iterating over the movies in each group:</span></span></span></p>
<pre>
favorable_reviews_by_users = dict((k, frozenset(v.values)) for k, v in favorable_ratings.groupby("UserID")["MovieID"])
</pre>
<p><span><span><span>In the preceding code, we stored the values as a <kbd><span><span><span><span><span>frozenset</span></span></span></span></span></kbd>, allowing us to quickly check if a movie has been rated by a user.</span></span></span></p>
<p><span><span><span>Sets are much faster than lists for this type of operation, and we will use them in later code.</span></span></span></p>
<p><span><span><span>Finally, we can create a</span></span></span> <kbd>DataFrame</kbd> <span><span><span>that tells us how frequently each movie has been given a favorable review:</span></span></span></p>
<pre>
<span><span><span>num_favorable_by_movie = ratings[["MovieID", "Favorable"]].groupby("MovieID").sum()</span></span></span>
</pre>
<p><span><span><span>We can see the top five movies by running the following code:</span></span></span></p>
<pre>
num_favorable_by_movie.sort_values(by="Favorable", ascending=False).head()
</pre>
<p><span><span><span>Let's see the top five movies list. We only have IDs now, and will get their titles later in the chapter.</span></span></span></p>
<div class="CDPAlignCenter CDPAlign">
<table>
<tbody>
<tr>
<td><span><span><span><span><span><span>Movie ID</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>Favorable</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span>50</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>100</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span>100</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>89</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span>258</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>83</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span>181</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>79</span></span></span></span></span></span></td>
</tr>
<tr>
<td><span><span><span><span><span><span>174</span></span></span></span></span></span></td>
<td><span><span><span><span><span><span>74</span></span></span></span></span></span></td>
</tr>
</tbody>
</table>
</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Looking into the basics of the Apriori algorithm</h1>
            </header>

            <article>
                
<p><span><span><span>The Apriori algorithm is part of our affinity analysis methodology and deals specifically with finding frequent itemsets within the data. The basic procedure of Apriori builds up new candidate itemsets from previously discovered frequent itemsets. These candidates are tested to see if they are frequent, and then the algorithm iterates as explained here:</span></span></span></p>
<ol>
<li><span><span><span>Create initial frequent itemsets by placing each item in its own itemset. Only items with at least the minimum support are used in this step.</span></span></span></li>
<li><span><span><span>New candidate itemsets are created from the most recently discovered frequent itemsets by finding supersets of the existing frequent itemsets.</span></span></span></li>
<li><span><span><span>All candidate itemsets are tested to see if they are frequent. If a candidate is not frequent then it is discarded. If there are no new frequent itemsets from this step, go to the last step.</span></span></span></li>
<li><span><span><span>Store the newly discovered frequent itemsets and go to the second step.</span></span></span></li>
<li><span><span><span>Return all of the discovered frequent itemsets.</span></span></span></li>
</ol>
<p><span><span>This process is outlined in the following workflow:</span></span></p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="345" src="assets/B06162_04_01.jpg" width="251"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Implementing the Apriori algorithm</h1>
            </header>

            <article>
                
<p><span><span><span>On the first iteration of Apriori, the newly discovered itemsets will have a length of 2, as they will be supersets of the initial itemsets created in the first step. On the second iteration (after applying the fourth step and going back to step 2), the newly discovered itemsets will have a length of 3. This allows us to quickly identify the newly discovered itemsets, as needed in the second step.</span></span></span></p>
<p><span><span><span>We can store our discovered frequent itemsets in a dictionary, where the key is the length of the itemsets. This allows us to quickly access the itemsets of a given length, and therefore the most recently discovered frequent itemsets, with the help of the following code:</span></span></span></p>
<pre>
<span><span><span>frequent_itemsets = {}</span></span></span>
</pre>
<p><span><span><span>We also need to define the minimum support needed for an itemset to be considered frequent. This value is chosen based on the dataset but try different values to see how that affects the result. I recommend only changing it by 10 percent at a time though, as the time the algorithm takes to run will be significantly different! Let's set a minimum support value:</span></span></span></p>
<pre>
<span><span><span>min_support = 50</span></span></span>
</pre>
<div class="packt_infobox"><span><span><span>To implement the first step of the Apriori algorithm, we create an itemset with each movie individually and test if the itemset is frequent. We use</span></span></span> <kbd>frozenset</kbd><span><span><span><strong>,</strong> as they allow us to perform faster set-based operations later on, and they can also be used as keys in our counting dictionary (normal sets cannot).</span></span></span></div>
<p><span><span><span>Let's look at the following example of <kbd>frozenset</kbd> code:</span></span></span></p>
<pre>
frequent_itemsets[1] = dict((frozenset((movie_id,)), row["Favorable"])<br/> for movie_id, row in num_favorable_by_movie.iterrows()<br/> if row["Favorable"] &gt; min_support)
</pre>
<p><span><span><span>We implement the second and third steps together for efficiency by creating a function that takes the newly discovered frequent itemsets, creates the supersets, and then tests if they are frequent. First, we set up the function to perform these steps:</span></span></span></p>
<pre>
from collections import defaultdict<br/><br/>def find_frequent_itemsets(favorable_reviews_by_users, k_1_itemsets, min_support):<br/>    counts = defaultdict(int)<br/>    for user, reviews in favorable_reviews_by_users.items():<br/>        for itemset in k_1_itemsets:<br/>            if itemset.issubset(reviews):<br/>                for other_reviewed_movie in reviews - itemset:<br/>                    current_superset = itemset | frozenset((other_reviewed_movie,))<br/>                    counts[current_superset] += 1<br/>    return dict([(itemset, frequency) for itemset, frequency in counts.items() if frequency &gt;= min_support])
</pre>
<p><span><span><span>In keeping with our rule of thumb of reading through the data as little as possible, we iterate over the dataset once per call to this function. While this doesn't matter too much in this implementation (our dataset is relatively small compared to the average computer),</span></span></span> <strong>single-pass</strong> <span><span><span>is a good practice to get into for larger applications. </span></span></span></p>
<p>Let's have a look at the core of this function in detail. W<span><span><span>e iterate through each user, and each of the previously discovered itemsets, and then check if it is a subset of the current set of reviews, which are stored in <kbd>k_1_itemsets</kbd></span></span></span> (note that here, <span class="packt_screen">k_1</span> means <em>k-1</em>)<span><span><span>. If it is, this means that the user has reviewed each movie in the itemset. This is done by the </span></span></span><span><span><span><kbd>itemset.issubset(reviews)</kbd> line.</span></span></span></p>
<p><span><span><span>We can then go through each individual movie that the user has reviewed (that is not already in the itemset), create a superset by combining the itemset with the new movie and record that we saw this superset in our counting dictionary. These are the candidate frequent itemsets for this value of <em>k</em>.</span></span></span></p>
<p><span><span><span>We end our function by testing which of the candidate itemsets have enough support to be considered frequent and return only those that have a support more than our <kbd>min_support</kbd> value.</span></span></span></p>
<p><span><span><span>This function forms the heart of our Apriori implementation and we now create a loop that iterates over the steps of the larger algorithm, storing the new itemsets as we increase <em>k</em> from 1 to a maximum value. In this loop, <span class="packt_screen"><span><span>k</span></span></span> represents the length of the soon-to-be discovered frequent itemsets, allowing us to access the previously most discovered ones by looking in our <span><span><span><span><span>frequent_itemsets</span></span></span></span></span> dictionary using the key</span></span></span> <em>k - 1</em><span><span><span>. We create the frequent itemsets and store them in our dictionary by their length. Let's look at the code:</span></span></span></p>
<pre>
for k in range(2, 20):<br/>    # Generate candidates of length k, using the frequent itemsets of length k-1<br/>    # Only store the frequent itemsets<br/>    cur_frequent_itemsets = find_frequent_itemsets(favorable_reviews_by_users,<br/>                                                   frequent_itemsets[k-1], min_support)<br/>    if len(cur_frequent_itemsets) == 0:<br/>        print("Did not find any frequent itemsets of length {}".format(k))<br/>        sys.stdout.flush()<br/>        break<br/>    else:<br/>        print("I found {} frequent itemsets of length {}".format(len(cur_frequent_itemsets), k))<br/>        sys.stdout.flush()<br/>        frequent_itemsets[k] = cur_frequent_itemsets
</pre>
<p><span><span><span>If we do find frequent itemsets, we print out a message to let us know the loop will be running again. If we don't, we stop iterating, as there cannot be frequent itemsets for</span></span></span> <em>k+1</em> <span><span><span>if there are no frequent itemsets for the current value of</span></span></span> <em>k</em><span><span><span>, therefore we finish the algorithm.</span></span></span></p>
<div class="packt_tip"><span><span><span>We use</span></span></span> <kbd>sys.stdout.flush()</kbd> <span><span><span>to ensure that the printouts happen while the code is still running. Sometimes, in large loops in particular cells, the printouts will not happen until the code has completed. Flushing the output in this way ensures that the printout happens when we want, rather than when the interface decides it can allocate the time to print. Don't flush too frequently though—the flush operation carries a computational cost (as does normal printing) and this will slow down the program.</span></span></span></div>
You can now run the above code.<br/>
The preceding code returns about 2000 frequent itemsets of varying lengths. You'll notice that the number of itemsets grows as the length increases before it shrinks. It grows because of the increasing number of possible rules. After a while, the large number of combinations no longer has the support necessary to be considered frequent. This results in the number shrinking. This shrinking is the benefit of the Apriori algorithm. If we search all possible itemsets (not just the supersets of frequent ones), we would be searching thousands of times more itemsets to see if they are frequent.
<p>Even if this shrinking didn't occur, the algorithm meets an absolute end when rules for a combination of all movies together is discovered. Therefore the Apriori algorithm will always terminate.</p>
<div class="packt_tip">It may take a few minutes for this code to run, more if you have older hardware. If you find you are having trouble running any of the code samples, take a look at using an online cloud provider for additional speed. Details about using the cloud to do the work are given in Appendix, Next Steps.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Extracting association rules</h1>
            </header>

            <article>
                
<p><span><span><span>After the Apriori algorithm has completed, we have a list of frequent itemsets. These aren't exactly association rules, but they can easily be converted into these rules. A frequent itemset is a set of items with a minimum support, while an association rule has a premise and a conclusion. The data is the same for the two.</span></span></span></p>
<div class="packt_infobox"><span><span><span>We can make an <em>association rule from a frequent itemset</em> by taking one of the movies in the itemset and denoting it as the conclusion. The other movies in the itemset will be the premise. This will form rules of the following form: <em>if a reviewer recommends all of the movies in the premise, they will also recommend the conclusion movie</em>.</span></span></span></div>
<p><span><span><span>For each itemset, we can generate a number of association rules by setting each movie to be the conclusion and the remaining movies as the premise. </span></span></span></p>
<p><span><span><span>In code, we first generate a list of all of the rules from each of the frequent itemsets, by iterating over each of the discovered frequent itemsets of each length. We then iterate over every movie in the itemset, as the conclusion.</span></span></span></p>
<pre>
candidate_rules = []<br/>for itemset_length, itemset_counts in frequent_itemsets.items():<br/>    for itemset in itemset_counts.keys():<br/>        for conclusion in itemset:<br/>            premise = itemset - set((conclusion,))<br/>            candidate_rules.append((premise, conclusion))
</pre>
<p><span><span><span>This returns a very large number of candidate rules. We can see some by printing out the first few rules in the list:</span></span></span></p>
<pre>
<span><span><span>print(candidate_rules[:5])</span></span></span>
</pre>
<p><span><span><span>The resulting output shows the rules that were obtained:</span></span></span></p>
<pre>
[(frozenset({79}), 258), (frozenset({258}), 79), (frozenset({50}), 64), (frozenset({64}), 50), (frozenset({127}), 181)]
</pre>
<p><span><span><span>In these <span><span>rules</span></span>, the first part (the <kbd><span><span><span><span><span>frozenset</span></span></span></span></span></kbd><a href="">) is the list of movies in the premise, while the number after it is the conclusion. In the first case, if a reviewer recommends movie 79, they are also likely to recommend movie 258.</a></span></span></span></p>
<p><span><span><span>Next, we compute the confidence of each of these rules. This is performed much like in</span></span></span> <a href="">Chapter 1</a><em>, Getting Started with Data Mining</em><span><span><span>, with the only changes being those necessary for computing using the new data format.</span></span></span></p>
<p><span><span><span>The process of computing confidence starts by creating dictionaries to store how many times we see the premise leading to the conclusion (a <span><span>correct</span></span> example of the rule) and how many times it doesn't (an <span><span>incorrect</span></span> example). We then iterate over all reviews and rules, working out whether the premise of the rule applies and, if it does, whether the conclusion is accurate.</span></span></span></p>
<pre>
correct_counts = defaultdict(int)<br/>incorrect_counts = defaultdict(int)<br/>for user, reviews in favorable_reviews_by_users.items():<br/>    for candidate_rule in candidate_rules:<br/>        premise, conclusion = candidate_rule<br/>        if premise.issubset(reviews):<br/>            if conclusion in reviews:<br/>                correct_counts[candidate_rule] += 1<br/>            else:<br/>                incorrect_counts[candidate_rule] += 1<br/><br/>
</pre>
<p><span><span><span>We then compute the confidence for each rule by dividing the correct count by the total number of times the rule was seen:</span></span></span></p>
<pre>
rule_confidence = {candidate_rule:<br/>                    (correct_counts[candidate_rule] / float(correct_counts[candidate_rule] +  <br/>                      incorrect_counts[candidate_rule]))<br/>                  for candidate_rule in candidate_rules}
</pre>
<p><span><span><span>Now we can print the top five rules by sorting this confidence dictionary and printing the results:</span></span></span></p>
<pre>
<span><span><span>from operator import itemgetter<br/>sorted_confidence = sorted(rule_confidence.items(), key=itemgetter(1), reverse=True)<br/>for index in range(5):<br/>    print("Rule #{0}".format(index + 1))<br/>    premise, conclusion = sorted_confidence[index][0]<br/>    print("Rule: If a person recommends {0} they will also recommend {1}".format(premise, conclusion))<br/>    print(" - Confidence: {0:.3f}".format(rule_confidence[(premise, conclusion)]))<br/>    print("")</span></span></span>
</pre>
<p><span><span><span>The resulting printout shows only the movie IDs, which isn't very helpful without the names of the movies also. The dataset came with a file called <span><span><span><span><span><span class="packt_screen">u.items</span></span></span></span></span></span>, which stores the movie names and their corresponding <span><span><span><span><span><span class="packt_screen">MovieID</span></span></span></span></span></span> (as well as other information, such as the genre).</span></span></span></p>
<p><span><span><span>We can load the titles from this file using pandas. Additional information about the file and categories is available in the <span><span><span><span class="packt_screen">README</span></span></span></span> file that came with the dataset. The data in the files is in CSV format, but with data separated by the <span><span><span><span><span><span class="packt_screen">|</span></span></span></span></span></span> symbol; it has no header<br/>
and the encoding is important to set. The column names were found in the <span><span><span><span><span><span class="packt_screen">README</span></span></span></span></span></span> file.</span></span></span></p>
<pre>
movie_name_filename = os.path.join(data_folder, "u.item")<br/>movie_name_data = pd.read_csv(movie_name_filename, delimiter="|", header=None,<br/>                              encoding = "mac-roman")<br/>movie_name_data.columns = ["MovieID", "Title", "Release Date", "Video Release", "IMDB", "&lt;UNK&gt;",<br/>                           "Action", "Adventure", "Animation", "Children's", "Comedy", "Crime",<br/>                           "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical",   <br/>                           "Mystery", "Romance", "Sci-Fi", "Thriller", "War", "Western"]
</pre>
<p><span><span><span>Getting the movie title is an important and frequently used step, therefore it makes sense to turn it into a function. We will create a function that will return a movie's title from its <span><span><span><span><span><span class="packt_screen">MovieID</span></span></span></span></span></span>, saving us the trouble of looking it up each time. Let's look at the code:</span></span></span></p>
<pre>
def get_movie_name(movie_id):<br/>    title_object = movie_name_data[movie_name_data["MovieID"] == movie_id]["Title"]<br/>    title = title_object.values[0]<br/>    return title
</pre>
<p><span><span><span>In a new Jupyter Notebook cell, we adjust our previous code for printing out the top rules to also include the titles:</span></span></span></p>
<pre>
for index in range(5):<br/>    print("Rule #{0}".format(index + 1))<br/>    premise, conclusion = sorted_confidence[index][0]<br/>    premise_names = ", ".join(get_movie_name(idx) for idx in premise)<br/>    conclusion_name = get_movie_name(conclusion)<br/>    print("Rule: If a person recommends {0} they will also recommend {1}".format(premise_names, conclusion_name))<br/>    print(" - Confidence: {0:.3f}".format(rule_confidence[(premise, conclusion)]))<br/>    print("")
</pre>
<p><span><span><span>The result is much more readable (there are still some issues, but we can ignore them for now):</span></span></span></p>
<pre>
Rule #1<br/>Rule: If a person recommends Shawshank Redemption, The (1994), Silence of the Lambs, The (1991), Pulp Fiction (1994), Star Wars (1977), Twelve Monkeys (1995) they will also recommend Raiders of the Lost Ark (1981)<br/> - Confidence: 1.000<br/><br/>Rule #2<br/>Rule: If a person recommends Silence of the Lambs, The (1991), Fargo (1996), Empire Strikes Back, The (1980), Fugitive, The (1993), Star Wars (1977), Pulp Fiction (1994) they will also recommend Twelve Monkeys (1995)<br/> - Confidence: 1.000<br/><br/>Rule #3<br/>Rule: If a person recommends Silence of the Lambs, The (1991), Empire Strikes Back, The (1980), Return of the Jedi (1983), Raiders of the Lost Ark (1981), Twelve Monkeys (1995) they will also recommend Star Wars (1977)<br/> - Confidence: 1.000<br/><br/>Rule #4<br/>Rule: If a person recommends Shawshank Redemption, The (1994), Silence of the Lambs, The (1991), Fargo (1996), Twelve Monkeys (1995), Empire Strikes Back, The (1980), Star Wars (1977) they will also recommend Raiders of the Lost Ark (1981)<br/> - Confidence: 1.000<br/><br/>Rule #5<br/>Rule: If a person recommends Shawshank Redemption, The (1994), Toy Story (1995), Twelve Monkeys (1995), Empire Strikes Back, The (1980), Fugitive, The (1993), Star Wars (1977) they will also recommend Return of the Jedi (1983)<br/> - Confidence: 1.000
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Evaluating the association rules</h1>
            </header>

            <article>
                
<p><span><span><span>In a broad sense, we can evaluate the association rules using the same concept as for classification. We use a test set of data that was not used for training, and evaluate our discovered rules based on their performance in this test set.</span></span></span></p>
<p><span><span><span>To do this, we will compute the test set confidence, that is, the confidence of each rule on the testing set.</span></span></span> <span><span><span>We won't apply a formal evaluation metric in this case; we simply examine the rules and look for good examples.</span></span></span></p>
<p><span><span><span>Formal evaluation could include a classification accuracy by determining the accuracy of predicting whether a user rates a given movie as favorable. In this case, as described below, we will informally look at the rules to find those that are more reliable:</span></span></span></p>
<ol>
<li><span><span><span>First, we extract the test dataset, which is all of the records that we didn't use in the training set. We used the first 200 users (by ID value) for the training set, and we will use all of the rest for the testing dataset. As with the training set, we will also get the favorable reviews for each of the users in this dataset as well. Let's look at the code:</span></span></span></li>
</ol>
<pre>
test_dataset = all_ratings[~all_ratings['UserID'].isin(range(200))]<br/>test_favorable = test_dataset[test_dataset["Favorable"]]<br/>test_favorable_by_users = dict((k, frozenset(v.values)) for k, v in <br/>                               test_favorable.groupby("UserID")["MovieID"])
</pre>
<ol start="2">
<li>We then count the correct instances where the premise leads to the conclusion, in the same way that we did before. The only change here is the use of the test data instead of the training data. Let's look at the code:</li>
</ol>
<pre>
correct_counts = defaultdict(int)<br/>incorrect_counts = defaultdict(int)<br/>for user, reviews in test_favorable_by_users.items():<br/>    for candidate_rule in candidate_rules:<br/>        premise, conclusion = candidate_rule<br/>        if premise.issubset(reviews):<br/>            if conclusion in reviews:<br/>                correct_counts[candidate_rule] += 1<br/>            else:<br/>                incorrect_counts[candidate_rule] += 1
</pre>
<ol start="3">
<li>Next, we compute the confidence of each rule from the correct counts and sort them. Let's look at the code:</li>
</ol>
<pre>
test_confidence = {candidate_rule:<br/>                             (correct_counts[candidate_rule] / float(correct_counts[candidate_rule] + incorrect_counts[candidate_rule]))<br/>                             for candidate_rule in rule_confidence}<br/>sorted_test_confidence = sorted(test_confidence.items(), key=itemgetter(1), reverse=True)
</pre>
<ol start="4">
<li>Finally, we print out the best association rules with the titles instead of the movie IDs:</li>
</ol>
<pre>
for index in range(10):<br/>    print("Rule #{0}".format(index + 1))<br/>    premise, conclusion = sorted_confidence[index][0]<br/>    premise_names = ", ".join(get_movie_name(idx) for idx in premise)<br/>    conclusion_name = get_movie_name(conclusion)<br/>    print("Rule: If a person recommends {0} they will also recommend {1}".format(premise_names, conclusion_name))<br/>    print(" - Train Confidence: {0:.3f}".format(rule_confidence.get((premise, conclusion), -1)))<br/>    print(" - Test Confidence: {0:.3f}".format(test_confidence.get((premise, conclusion), -1)))<br/>    print("")
</pre>
<p><span><span><span>We can now see which rules are most applicable in new unseen data:</span></span></span></p>
<pre>
Rule #1<br/>Rule: If a person recommends Shawshank Redemption, The (1994), Silence of the Lambs, The (1991), Pulp Fiction (1994), Star Wars (1977), Twelve Monkeys (1995) they will also recommend Raiders of the Lost Ark (1981)<br/> - Train Confidence: 1.000<br/> - Test Confidence: 0.909<br/><br/>Rule #2<br/>Rule: If a person recommends Silence of the Lambs, The (1991), Fargo (1996), Empire Strikes Back, The (1980), Fugitive, The (1993), Star Wars (1977), Pulp Fiction (1994) they will also recommend Twelve Monkeys (1995)<br/> - Train Confidence: 1.000<br/> - Test Confidence: 0.609<br/><br/>Rule #3<br/>Rule: If a person recommends Silence of the Lambs, The (1991), Empire Strikes Back, The (1980), Return of the Jedi (1983), Raiders of the Lost Ark (1981), Twelve Monkeys (1995) they will also recommend Star Wars (1977)<br/> - Train Confidence: 1.000<br/> - Test Confidence: 0.946<br/><br/>Rule #4<br/>Rule: If a person recommends Shawshank Redemption, The (1994), Silence of the Lambs, The (1991), Fargo (1996), Twelve Monkeys (1995), Empire Strikes Back, The (1980), Star Wars (1977) they will also recommend Raiders of the Lost Ark (1981)<br/> - Train Confidence: 1.000<br/> - Test Confidence: 0.971<br/><br/>Rule #5<br/>Rule: If a person recommends Shawshank Redemption, The (1994), Toy Story (1995), Twelve Monkeys (1995), Empire Strikes Back, The (1980), Fugitive, The (1993), Star Wars (1977) they will also recommend Return of the Jedi (1983)<br/> - Train Confidence: 1.000<br/> - Test Confidence: 0.900
</pre>
<p><span><span><span>The second rule, for instance, has a perfect confidence in the training data, but it is only accurate in 60 percent of cases for the test data. Many of the other rules in the top 10 have high confidences in test data, making them good rules for making recommendations.</span></span></span></p>
<p>You may also notice that these movies tend to be very popular and good films. This gives us a baseline algorithm that we could compare against, i.e. instead of trying to do personalized recommendations, just recommend the most liked movies overall. Have a shot at implementing this algorithm - does the Apriori algorithm outperform it and by how much? Another baseline could be to simply recommend movies at random from the same genre.</p>
<div class="packt_tip"><span>If you are looking through the rest of the rules, some will have a test confidence of -1. Confidence values are always between 0 and 1. This value indicates that the particular rule wasn't found in the test dataset at all.</span></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p><span><span><span>In this chapter we performed affinity analysis in order to recommend movies based on a large set of reviewers. We did this in two stages. First, we found frequent itemsets in the data using the Apriori algorithm. Then, we created association rules from those itemsets.</span></span></span></p>
<p><span><span><span>The use of the Apriori algorithm was necessary due to the size of the dataset. In <a href="3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml" target="_blank">Chapter 1</a><em>, Getting Started With Data Mining</em>, we used a brute-force approach, which has exponential growth in the time needed to compute those rules required for a smarter approach. This is a common pattern for data mining: we can solve many problems in a brute force manner for small datasets, but smarter algorithms are required to apply the concepts to larger datasets.</span></span></span></p>
<p><span><span><span>We performed training on a subset of our data in order to find the association rules, and then tested those rules on the rest of the data—a testing set. From what we discussed in the previous chapters, we could extend this concept to use cross-fold validation to better evaluate the rules. This would lead to a more robust evaluation of the quality of each rule.</span></span></span></p>
<p>To take the concepts in this chapter further, investigate which movies obtain high overall scores (i.e. lots of recommendations), but do not have adequate rules to recommend them to new users. How would you alter the algorithm to recommend these movies?</p>
<p><span><span><span>So far, all of our datasets have been described in terms of features. However, not all datasets are <em>pre-defined</em> in this way. In the next chapter, we will look at scikit-learn's transformers (they were introduced in <em>Chapter 3, Predicting Sports Winners with Decision Trees</em>) as a way to extract features from data. We will discuss how to implement our own transformers, extend existing ones, and concepts we can implement using them.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>