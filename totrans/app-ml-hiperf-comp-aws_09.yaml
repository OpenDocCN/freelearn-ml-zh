- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance Optimization for Real-Time Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine Learning** (**ML**) and **Deep Learning** (**DL**) models are used
    in almost every industry, such as e-commerce, manufacturing, life sciences, and
    finance. Due to this, there have been meaningful innovations to improve the performance
    of these models. Since the introduction of transformer-based models in 2018, which
    were initially developed for **Natural Language Processing** (**NLP**) applications,
    the size of the models and the datasets required to train the models has grown
    exponentially. **Transformer-based** models are now used for forecasting as well
    as **computer vision** applications, in addition to NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s travel back in time a little to understand the growth in size of these
    models. **Embeddings from Language Models** (**ELMo**), which was introduced in
    2018, had *93.6 million parameters*, while the **Generative Pretrained Transformer**
    model (also known as **GPT-3**), in 2020, had *175 billion parameters*. Today,
    we have DL models such as **Switch Transformers** ([https://arxiv.org/pdf/2101.03961.pdf](https://arxiv.org/pdf/2101.03961.pdf))
    with more than *1 trillion parameters*. However, the speed of innovation of hardware
    to train and deploy such models is not catching up with the speed of innovation
    of large models. Therefore, we need sophisticated techniques to train and deploy
    these models in a cost-effective yet performant way.
  prefs: []
  type: TYPE_NORMAL
- en: One way to address this is to think in terms of reducing the memory footprint
    of the model. Moreover, many inference workloads must provide flexibility, high
    availability, and the ability to scale as enterprises serve millions or billions
    of users, especially for real-time or near real-time use cases. We need to understand
    which instance type and how many instances to use for deployment. We should also
    understand the key metrics based on which we will optimize the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, to collectively address the preceding scenarios, in this chapter,
    we will dive deep into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the memory footprint of DL models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key metrics for optimizing models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing instance type, load testing, and performance tuning for models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observing results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For details on training large DL models, please refer to [*Chapter 6*](B18493_06.xhtml#_idTextAnchor116),
    *Distributed Training of Machine Learning Models*, where we cover the topic in
    detail along with an example.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should have the following prerequisites before getting started with this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A web browser (for the best experience, it is recommended that you use the Chrome
    or Firefox browser)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the AWS account that you used in [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the SageMaker Studio development environment that we created in [*Chapter
    5*](B18493_05.xhtml#_idTextAnchor095), *Data Analysis*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example Jupyter notebooks for this chapter are provided in the companion GitHub
    repository ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter09](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter09))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the memory footprint of DL models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have trained the model, we need to deploy the model to get predictions,
    which are then used to provide business insights. Sometimes, our model can be
    bigger than the size of the single GPU memory available on the market today. In
    that case, you have two options – either to reduce the memory footprint of the
    model or use distributed deployment techniques. Therefore, in this section, we
    will discuss the following techniques to reduce the memory footprint of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model compilation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s dive deeper into each of these techniques, starting with pruning.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pruning** is the technique of eliminating weights and parameters within a
    DL model that have little or no impact on the performance of the model but a significant
    impact on the inference speed and size of the model. The idea behind pruning methods
    is to make the model’s memory and power efficient, reducing the storage requirement
    and latency of the model. A DL model is basically a neural network with many hidden
    layers connected to each other. As the size of the model increases, the number
    of hidden layers, parameters, and weight connections between the layers also increases.
    Therefore, pruning methods tend to remove unused parameters and weight connections
    without too much bearing on the accuracy of the model, as shown in *Figure 9**.1*
    and *Figure 9**.2*. *Figure 9**.1* shows a neural network before pruning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Simple neural network before pruning](img/B18493_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Simple neural network before pruning
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.2* shows the same neural network after pruning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Simple neural network after pruning](img/B18493_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Simple neural network after pruning
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered pruning, let’s take a look at quantization next.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train a neural network, data is first passed through the network in a forward
    pass, which calculates the activations, and then a backward pass, which uses the
    activations to calculate the gradients. The activations and gradients are usually
    stored in floating point 32, which takes 4 bytes of memory. When you have models
    with billions or trillions of parameters, this number is pretty significant. Therefore,
    **quantization** is a technique that reduces the model size by decreasing the
    precision of weights, biases, and activations of the model, such as floating point
    16 or 8, or even to integer 8, which takes significantly less memory. For example,
    the GPT-J-6B model, which has 6 billion trainable parameters, takes about 23 GB
    of memory, as shown in *Figure 9**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.3* shows the number of parameters and size of the model when loaded
    from the Hugging Face library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – The model size and number of parameters for the GPT-J-6B model](img/B18493_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – The model size and number of parameters for the GPT-J-6B model
  prefs: []
  type: TYPE_NORMAL
- en: 'The same model when loaded with 16-bit floating point precision takes about
    11 GB of memory, which is a memory reduction of about half, and can fit into a
    single GPU memory for inference as shown in *Figure 9**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – The model size and number of parameters for the GPT-J-6B model
    with FP16 precision](img/B18493_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – The model size and number of parameters for the GPT-J-6B model
    with FP16 precision
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 9**.3* and *Figure 9**.4*, quantization can be a useful
    technique for reducing the memory footprint of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at another technique, called model compilation.
  prefs: []
  type: TYPE_NORMAL
- en: Model compilation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we go into model compilation, let’s first understand compilation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compilation** is the process of converting a human-readable program that
    is a set of instructions into a machine-readable program. This core idea is used
    as a backbone for many programming languages, such as C, C++, and Java. This process
    also introduces efficiencies in the runtime environment of the program, such as
    making it platform-independent, reducing the memory size of the program, and so
    on. Most programming languages come with a **compiler**, which is used to compile
    the code into a machine-readable form as writing a compiler is a tedious process
    and requires a deep understanding of the programming language as well as the hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar idea is used for compiling ML models. With ML, compilers place the
    core operations of the neural network on GPUs in a way that minimizes overhead.
    This reduces the memory footprint of the model, improves the performance efficiency,
    and makes it hardware agnostic. The concept is depicted in *Figure 9**.5*, where
    the compiler takes a model developed in PyTorch, TensorFlow, XGBoost, and so on,
    converts it into an intermediate representation that is language agnostic, and
    then converts it into machine-readable code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – High-level model compilation process](img/B18493_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – High-level model compilation process
  prefs: []
  type: TYPE_NORMAL
- en: Model compilation eliminates the effort required to fine-tune the model for
    the specific hardware and software configurations of each platform. There are
    many frameworks available today using which you can compile your models, such
    as **TVM** and **ONNX**, each with its own pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussing compilers in detail is out of the scope of this book. For details
    on TVM, refer to this link: [https://tvm.apache.org/](https://tvm.apache.org/).
    And for details about ONNX, refer to this link: [https://onnx.ai/](https://onnx.ai/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss a feature of Amazon SageMaker called **SageMaker Neo**, which
    is used to optimize ML models for inference on multiple platforms. Neo automatically
    optimizes models written in various frameworks, such as Gluon, Keras, PyTorch,
    TensorFlow, and so on, for inference on different platforms, such as Linux and
    Windows, as well as many different processors. For a complete list of frameworks
    or processors supported by Neo, please refer to this link: [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge.html).'
  prefs: []
  type: TYPE_NORMAL
- en: The way Neo works is that it will first read the model, convert the framework-specific
    operations and functions into an intermediate representation that is framework
    agnostic, and finally, apply a series of optimizations. It will then generate
    binary code for optimized operations, write it to the shared object library and
    save the model definition and parameters in separate files ([https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html)).
  prefs: []
  type: TYPE_NORMAL
- en: It also provides a runtime for each target platform that loads and executes
    the compiled model. Moreover, it can optimize the models with parameters either
    in floating point 32 (`FP32`), quantized into integer 8 (`INT8`), or in floating
    point 16 (`FP16`). It can improve the model’s performance up to 25 times with
    less than one-tenth of the footprint of a DL framework such as TensorFlow or PyTorch.
    To understand this further, let’s take a pretrained image classification model
    from PyTorch and optimize it using SageMaker Neo.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'We touched on SageMaker Neo in [*Chapter 8*](B18493_08.xhtml#_idTextAnchor161),
    *Optimizing and Managing Machine Learning Models for Edge Deployment*. Here, we
    will be using the same example to explain it in detail. For the full code, refer
    to the GitHub link: [https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/sagemaker_notebook/1_compile_resnet_model_egde_manager.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/sagemaker_notebook/1_compile_resnet_model_egde_manager.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to optimize a pretrained model using SageMaker Neo:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will download a pretrained model from the PyTorch library as shown
    in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to save the model in `model.tar.gz` format, which SageMaker requires,
    specify the input data shape, and upload the model to **Amazon S3**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we have the model in SageMaker format, we will prepare the parameters
    required for model compilation. Most importantly, you need to mention the `target_device`
    parameter, as based on it, SageMaker Neo will compile the model for the particular
    hardware on which the model will be deployed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will declare the `PyTorchModel` object provided by SageMaker, which
    will have the necessary configurations, such as the model’s S3 path, the framework
    version, the inference script, the Python version, and so on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will use the `PyTorchModel` object to create the compilation job
    and deploy the compiled model to the `ml.c5.2xlarge` instance, since the model
    was compiled for `ml.c5` as the target device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the model has finished compiling, you can then deploy the compiled model
    to make inference. In this case, we are deploying the model for **real-time inference**
    as an endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more details on different deployment options provided by SageMaker, refer
    to [*Chapter 7*](B18493_07.xhtml#_idTextAnchor128), *Deploying Machine Learning
    Models* *at Scale*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, once the model is deployed, we can invoke the endpoint for inference as
    shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since we have deployed the model as a real-time endpoint, you will be charged
    for the instance on which the model is deployed. Therefore, if you are not using
    the endpoint, make sure to delete it using the following code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following code snippet to delete the endpoint, if you are not using
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we understand how to optimize a model using SageMaker Neo for inference,
    let’s talk about some of the key metrics that you should consider when trying
    to improve the latency of models. The ideas covered in the next section apply
    to DL models, even when you are not using SageMaker Neo, as you might not be able
    to compile all models with Neo. You can see the supported models and frameworks
    for SageMaker Neo here: [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Key metrics for optimizing models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to real-time inference, optimizing a model for performance usually
    includes metrics such as latency, throughput, and model size. To optimize the
    model size, the process typically involves having a trained model, checking the
    size of the model, and if it does not fit into single CPU/GPU memory, you can
    choose any of the techniques discussed in the *Reducing the memory footprint of
    DL models* section to prepare it for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: For deployment, one of the best practices is to standardize the environment.
    This will involve the use of containers to deploy the model, irrespective of whether
    you are deploying it on your own server or using Amazon SageMaker. The process
    is illustrated in *Figure 9**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Model deployed as a real-time endpoint](img/B18493_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Model deployed as a real-time endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, we will first prepare the model for deployment, select or create
    a container to standardize the environment, followed by deploying the container
    on an instance(s). Therefore, to optimize the model’s performance, it is important
    to look for both inference and instance metrics. Inference metrics include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Invocations**: The number of requests sent to the model endpoint. You can
    get the total number of requests by using the sum statistics in **Amazon CloudWatch**,
    which monitors the AWS resources or applications that you run on AWS in real time,
    including SageMaker ([https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invocations per instance**: If your model is deployed on more than one machine,
    then it’s important to understand the number of invocations sent to a model on
    each instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model latency**: This is the time interval taken by a model to respond. It
    includes the local communication time taken to send the request, for the model
    to complete the inference in the container, and to get the response from the container
    of the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*model latency = request time + inference time taken by the model + response
    time from* *the container*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overhead latency**: This is the time taken to respond after the endpoint
    has received the request minus the model latency. It can depend on multiple factors,
    such as request size, request frequency, authentication/authorization of the request,
    and response payload size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maximum Invocations**: This is the maximum number of requests to an endpoint
    per minute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost per hour**: This gives the estimated cost per hour for your endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost per inference**: This provides the estimated cost per inference on your
    endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Cost metrics provide the cost in US dollars.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to inference metrics, you should also consider optimizing for instance
    metrics such as GPU utilization, GPU memory, CPU utilization, and CPU memory based
    on the instance type selected. Amazon SageMaker offers more than 70 instances
    from which you can choose to deploy your model. This brings up an additional question
    on how to determine which instance type and the number of instances to select
    for deploying the model in order to achieve your performance requirements. Let’s
    discuss the approach for selecting the optimal instance for your model in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the instance type, load testing, and performance tuning for models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, based on the model type (ML model or DL model) and model size,
    you can make a heuristic guess to test the model’s performance on a few instances.
    This approach is fast but might not be the best approach. Therefore, in order
    to optimize this process, alternatively, you can use the **Inference Recommender**
    feature of Amazon SageMaker ([https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html)),
    which automates the load testing and model tuning process across the SageMaker
    ML instances. It helps you to deploy the ML models on the optimized hardware,
    based on your performance requirements, at the lowest possible cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example by using a pretrained image classification model to understand
    how Inference Recommender works. The following steps outline the process of using
    Inference Recommender:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determine the ML model details, such as framework and domain. The following
    is a code snippet for this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take a pretrained model and package it in the compressed TAR file (`*.tar.gz`)
    format, which SageMaker understands, and upload the model to Amazon S3\. If you
    have trained the model on SageMaker, then you can skip this step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select the inference container, which can be a prebuilt Docker container provided
    by AWS or your own custom container. In our example, we are fetching a prebuilt
    PyTorch container image provided by AWS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a sample payload. In our example, we have images in `.jpg` format, compress
    them in a TAR file, and upload it to Amazon S3\. In this example, we are only
    using four images, but it’s recommended to add a variety of samples, which is
    reflective of your actual payloads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compress the payload in TAR format as shown in the following code snippet and
    upload it to S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'Once, we have the payload in TAR format, let’s use the following code snippet
    to upload it to Amazon S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'Register the model in the **model registry**, which is used to catalog models
    for production, manage model versions, associate metadata, manage the approval
    status of the model, deploy models to production, and automate the model deployment
    process ([https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html)).
    Registering a model in the model registry is a two-step process, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a model package group, which will have all the versions of the model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Register a model version to the model package group. To get the recommended
    instance type, you have two options – either you can specify a list of instances
    that you want Inference Recommender to use or you can not provide the instance
    list and it will pick the right instance based on the ML domain and ML task. For
    our example, we will use a list of common instance types used for image classification
    algorithms. This involves three steps.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, create an input dictionary with configuration for registering the model,
    as shown in the following code snippet:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE139]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'The second step is to create a model inference specification object, which
    will consist of providing details about the container, framework, model input,
    content type, and the S3 path of the trained model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, after providing the inference specification, we will then create the
    model package. Once the model package is created, you can then get the model package
    ARN, and it will also be visible in the SageMaker Studio UI, under **Model Registry**:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Now, that the model has been registered, we will create an Inference Recommender
    job. There are two options – either you can create a default job to get instance
    recommendations or you can use an advanced job, where you can provide your inference
    requirements, tune environment variables, and perform more extensive load tests.
    An advanced job takes more time than a default job and depends on your traffic
    pattern and the number of instance types on which it will run the load tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example, we will create a default job, which will return a list of instance
    type recommendations including environment variables, cost, throughput, model
    latency, and the maximum number of invocations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how you can create a default job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Code to create a custom load test is provided in the GitHub repository: [Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/Chapter09/1_inference_recommender_custom_load_test.ipynb](http://Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/Chapter09/1_inference_recommender_custom_load_test.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss the results provided by the Inference Recommender
    job.
  prefs: []
  type: TYPE_NORMAL
- en: Observing the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recommendation provided by Inference Recommender includes instance metrics,
    performance metrics, and cost metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Instance metrics include `InstanceType`, `InitialInstanceCount`, and `EnvironmentParameters`,
    which are tuned according to the job for better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Performance metrics include `MaxInvocations` and `ModelLatency`, whereas cost
    metrics include `CostPerHour` and `CostPerInference`.
  prefs: []
  type: TYPE_NORMAL
- en: These metrics enable you to make informed trade-offs between cost and performance.
    For example, if your business requirement is overall price performance with an
    emphasis on throughput, then you should focus on `CostPerInference`. If your requirement
    is a balance between latency and throughput, then you should focus on `ModelLatency`
    and `MaxInvocations` metrics.
  prefs: []
  type: TYPE_NORMAL
- en: You can view the results of the Inference Recommender job either through an
    API call or in the SageMaker Studio UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code snippet for observing the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: 'You can observe the results from the SageMaker Studio UI by logging into SageMaker
    Studio, clicking on the orange triangle icon, and selecting **Model registry**
    from the drop-down menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Inference Recommender results](img/B18493_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Inference Recommender results
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how the Inference Recommender feature of Amazon SageMaker
    can be used to get the right instance type and instance count, let’s take a look
    at the topics covered in this chapter in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed various techniques for optimizing ML and DL models
    for real-time inference. We talked about different ways to reduce the memory footprint
    of DL models, such as pruning and quantization, followed by a deeper dive into
    model compilation. We then discussed key metrics that can help in evaluating the
    performance of models. Finally, we did a deep dive into how you can select the
    right instance, run load tests, and automatically perform model tuning using SageMaker
    Inference Recommender’s capability.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss visualizing and exploring large amounts
    of data on AWS.
  prefs: []
  type: TYPE_NORMAL
