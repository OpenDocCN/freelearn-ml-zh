- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Performance Optimization for Real-Time Inference
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时推理的性能优化
- en: '**Machine Learning** (**ML**) and **Deep Learning** (**DL**) models are used
    in almost every industry, such as e-commerce, manufacturing, life sciences, and
    finance. Due to this, there have been meaningful innovations to improve the performance
    of these models. Since the introduction of transformer-based models in 2018, which
    were initially developed for **Natural Language Processing** (**NLP**) applications,
    the size of the models and the datasets required to train the models has grown
    exponentially. **Transformer-based** models are now used for forecasting as well
    as **computer vision** applications, in addition to NLP.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）和**深度学习**（**DL**）模型几乎在所有行业中都得到应用，例如电子商务、制造业、生命科学和金融。因此，为了提高这些模型的表现，已经产生了有意义的创新。自2018年引入基于转换器的模型以来，这些模型最初是为**自然语言处理**（**NLP**）应用开发的，模型的规模和训练模型所需的数据集规模已经呈指数增长。**基于转换器**的模型现在也用于预测以及**计算机视觉**应用，除了NLP。'
- en: Let’s travel back in time a little to understand the growth in size of these
    models. **Embeddings from Language Models** (**ELMo**), which was introduced in
    2018, had *93.6 million parameters*, while the **Generative Pretrained Transformer**
    model (also known as **GPT-3**), in 2020, had *175 billion parameters*. Today,
    we have DL models such as **Switch Transformers** ([https://arxiv.org/pdf/2101.03961.pdf](https://arxiv.org/pdf/2101.03961.pdf))
    with more than *1 trillion parameters*. However, the speed of innovation of hardware
    to train and deploy such models is not catching up with the speed of innovation
    of large models. Therefore, we need sophisticated techniques to train and deploy
    these models in a cost-effective yet performant way.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这些模型规模的增长。**语言模型嵌入**（**ELMo**），于2018年推出，有*9360万个参数*，而**生成预训练转换器**模型（也称为**GPT-3**），在2020年，有*1750亿个参数*。今天，我们有了具有超过*1000亿个参数*的深度学习模型，如**开关转换器**（[https://arxiv.org/pdf/2101.03961.pdf](https://arxiv.org/pdf/2101.03961.pdf)）。然而，用于训练和部署此类模型的硬件创新速度并没有跟上大型模型创新的速度。因此，我们需要复杂的技巧以成本效益和性能的方式训练和部署这些模型。
- en: One way to address this is to think in terms of reducing the memory footprint
    of the model. Moreover, many inference workloads must provide flexibility, high
    availability, and the ability to scale as enterprises serve millions or billions
    of users, especially for real-time or near real-time use cases. We need to understand
    which instance type and how many instances to use for deployment. We should also
    understand the key metrics based on which we will optimize the models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一问题的方法之一是考虑减少模型的内存占用。此外，许多推理工作负载必须提供灵活性、高可用性以及随着企业服务数百万或数十亿用户的能力，特别是对于实时或近实时用例。我们需要了解用于部署的实例类型以及需要多少实例。我们还应该了解基于哪些关键指标来优化模型。
- en: 'Therefore, to collectively address the preceding scenarios, in this chapter,
    we will dive deep into the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了共同解决上述场景，在本章中，我们将深入探讨以下主题：
- en: Reducing the memory footprint of DL models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少深度学习模型的内存占用
- en: Key metrics for optimizing models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化模型的关键指标
- en: Choosing instance type, load testing, and performance tuning for models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择实例类型、负载测试和模型性能调整
- en: Observing results
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察结果
- en: Important note
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For details on training large DL models, please refer to [*Chapter 6*](B18493_06.xhtml#_idTextAnchor116),
    *Distributed Training of Machine Learning Models*, where we cover the topic in
    detail along with an example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有关训练大型深度学习模型的详细信息，请参阅[*第6章*](B18493_06.xhtml#_idTextAnchor116)，*机器学习模型的分布式训练*，其中我们详细介绍了该主题，并附带了一个示例。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You should have the following prerequisites before getting started with this
    chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本章之前，您应该具备以下先决条件：
- en: A web browser (for the best experience, it is recommended that you use the Chrome
    or Firefox browser)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个网络浏览器（为了获得最佳体验，建议您使用Chrome或Firefox浏览器）
- en: Access to the AWS account that you used in [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis*
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问您在[*第5章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析*中使用的AWS账户
- en: Access to the SageMaker Studio development environment that we created in [*Chapter
    5*](B18493_05.xhtml#_idTextAnchor095), *Data Analysis*
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问我们在[*第5章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析*中创建的SageMaker Studio开发环境
- en: Example Jupyter notebooks for this chapter are provided in the companion GitHub
    repository ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter09](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter09))
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章节的示例 Jupyter 笔记本可在配套的 GitHub 仓库中找到 ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter09](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter09))
- en: Reducing the memory footprint of DL models
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降低深度学习模型的内存占用
- en: 'Once we have trained the model, we need to deploy the model to get predictions,
    which are then used to provide business insights. Sometimes, our model can be
    bigger than the size of the single GPU memory available on the market today. In
    that case, you have two options – either to reduce the memory footprint of the
    model or use distributed deployment techniques. Therefore, in this section, we
    will discuss the following techniques to reduce the memory footprint of the model:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了模型，我们需要部署模型以获取预测结果，这些结果随后被用来提供业务洞察。有时，我们的模型可能比市场上今天可用的单个 GPU 内存大小还要大。在这种情况下，你有两个选择——要么减小模型的内存占用，要么使用分布式部署技术。因此，在本节中，我们将讨论以下技术来减小模型的内存占用：
- en: Pruning
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪枝
- en: Quantization
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化
- en: Model compilation
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型编译
- en: Let’s dive deeper into each of these techniques, starting with pruning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这些技术的每一个，从剪枝开始。
- en: Pruning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝
- en: '**Pruning** is the technique of eliminating weights and parameters within a
    DL model that have little or no impact on the performance of the model but a significant
    impact on the inference speed and size of the model. The idea behind pruning methods
    is to make the model’s memory and power efficient, reducing the storage requirement
    and latency of the model. A DL model is basically a neural network with many hidden
    layers connected to each other. As the size of the model increases, the number
    of hidden layers, parameters, and weight connections between the layers also increases.
    Therefore, pruning methods tend to remove unused parameters and weight connections
    without too much bearing on the accuracy of the model, as shown in *Figure 9**.1*
    and *Figure 9**.2*. *Figure 9**.1* shows a neural network before pruning:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**剪枝**是一种消除深度学习模型中权重和参数的技术，这些权重和参数对模型性能的影响很小或没有，但对推理速度和模型大小有显著影响。剪枝方法背后的理念是使模型在内存和功耗上更高效，减少模型的存储需求和延迟。深度学习模型基本上是一个具有许多相互连接的隐藏层的神经网络。随着模型大小的增加，隐藏层的数量、参数以及层之间的权重连接也会增加。因此，剪枝方法倾向于移除未使用的参数和权重连接，而对模型的准确性影响不大，如图
    *图 9*.1* 和 *图 9*.2* 所示。*图 9*.1 展示了剪枝前的神经网络：'
- en: '![Figure 9.1 – Simple neural network before pruning](img/B18493_09_001.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 剪枝前的简单神经网络](img/B18493_09_001.jpg)'
- en: Figure 9.1 – Simple neural network before pruning
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 剪枝前的简单神经网络
- en: '*Figure 9**.2* shows the same neural network after pruning:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9*.2 展示了剪枝后的相同神经网络：'
- en: '![Figure 9.2 – Simple neural network after pruning](img/B18493_09_002.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 剪枝后的简单神经网络](img/B18493_09_002.jpg)'
- en: Figure 9.2 – Simple neural network after pruning
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 剪枝后的简单神经网络
- en: Now that we’ve covered pruning, let’s take a look at quantization next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了剪枝，接下来让我们看看量化。
- en: Quantization
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: To train a neural network, data is first passed through the network in a forward
    pass, which calculates the activations, and then a backward pass, which uses the
    activations to calculate the gradients. The activations and gradients are usually
    stored in floating point 32, which takes 4 bytes of memory. When you have models
    with billions or trillions of parameters, this number is pretty significant. Therefore,
    **quantization** is a technique that reduces the model size by decreasing the
    precision of weights, biases, and activations of the model, such as floating point
    16 or 8, or even to integer 8, which takes significantly less memory. For example,
    the GPT-J-6B model, which has 6 billion trainable parameters, takes about 23 GB
    of memory, as shown in *Figure 9**.3*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个神经网络，首先需要将数据通过前向传递传递到网络中，这会计算激活值，然后是反向传递，它使用激活值来计算梯度。激活值和梯度通常存储在浮点 32 中，这需要
    4 字节内存。当你有具有数十亿或数万亿参数的模型时，这个数字相当可观。因此，**量化**是一种通过降低模型中权重、偏置和激活的精度来减小模型大小的技术，例如浮点
    16 或 8，甚至到整数 8，这会显著减少内存占用。例如，具有 60 亿个可训练参数的 GPT-J-6B 模型大约需要 23 GB 的内存，如图 *图 9*.3*
    所示。
- en: '*Figure 9**.3* shows the number of parameters and size of the model when loaded
    from the Hugging Face library:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.3* 展示了从 Hugging Face 库加载时的模型参数数量和大小：'
- en: '![Figure 9.3 – The model size and number of parameters for the GPT-J-6B model](img/B18493_09_003.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – GPT-J-6B 模型的模型大小和参数数量](img/B18493_09_003.jpg)'
- en: Figure 9.3 – The model size and number of parameters for the GPT-J-6B model
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – GPT-J-6B 模型的模型大小和参数数量
- en: 'The same model when loaded with 16-bit floating point precision takes about
    11 GB of memory, which is a memory reduction of about half, and can fit into a
    single GPU memory for inference as shown in *Figure 9**.4*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 16 位浮点精度加载相同模型时，大约需要 11 GB 的内存，这大约是内存减少了一半，并且可以像 *图 9.4* 中所示的那样适应单个 GPU
    的内存进行推理：
- en: '![Figure 9.4 – The model size and number of parameters for the GPT-J-6B model
    with FP16 precision](img/B18493_09_004.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – GPT-J-6B 模型在 FP16 精度下的模型大小和参数数量](img/B18493_09_004.jpg)'
- en: Figure 9.4 – The model size and number of parameters for the GPT-J-6B model
    with FP16 precision
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – GPT-J-6B 模型在 FP16 精度下的模型大小和参数数量
- en: As shown in *Figure 9**.3* and *Figure 9**.4*, quantization can be a useful
    technique for reducing the memory footprint of the model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 9.3* 和 *图 9.4* 所示，量化可以是一种减少模型内存占用大小的有用技术。
- en: Now, let’s take a look at another technique, called model compilation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看另一种技术，称为模型编译。
- en: Model compilation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型编译
- en: Before we go into model compilation, let’s first understand compilation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入模型编译之前，让我们首先了解编译。
- en: '**Compilation** is the process of converting a human-readable program that
    is a set of instructions into a machine-readable program. This core idea is used
    as a backbone for many programming languages, such as C, C++, and Java. This process
    also introduces efficiencies in the runtime environment of the program, such as
    making it platform-independent, reducing the memory size of the program, and so
    on. Most programming languages come with a **compiler**, which is used to compile
    the code into a machine-readable form as writing a compiler is a tedious process
    and requires a deep understanding of the programming language as well as the hardware.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**编译**是将人类可读的程序（一组指令）转换为机器可读程序的过程。这一核心思想被用作许多编程语言（如 C、C++ 和 Java）的骨干。这个过程还在程序的运行时环境中引入了效率，例如使其平台无关、减少程序内存大小等。大多数编程语言都自带
    **编译器**，用于将代码编译成机器可读形式，因为编写编译器是一个繁琐的过程，需要深入理解编程语言以及硬件。'
- en: 'A similar idea is used for compiling ML models. With ML, compilers place the
    core operations of the neural network on GPUs in a way that minimizes overhead.
    This reduces the memory footprint of the model, improves the performance efficiency,
    and makes it hardware agnostic. The concept is depicted in *Figure 9**.5*, where
    the compiler takes a model developed in PyTorch, TensorFlow, XGBoost, and so on,
    converts it into an intermediate representation that is language agnostic, and
    then converts it into machine-readable code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在编译机器学习模型时，使用了类似的想法。使用机器学习，编译器将神经网络的核心操作放置在 GPU 上，以最小化开销。这减少了模型占用的内存，提高了性能效率，并使其与硬件无关。这一概念在
    *图 9.5* 中得到展示，其中编译器将使用 PyTorch、TensorFlow、XGBoost 等开发的模型转换为一种语言无关的中间表示，然后将其转换为机器可读的代码：
- en: '![Figure 9.5 – High-level model compilation process](img/B18493_09_005.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – 高级模型编译过程](img/B18493_09_005.jpg)'
- en: Figure 9.5 – High-level model compilation process
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 高级模型编译过程
- en: Model compilation eliminates the effort required to fine-tune the model for
    the specific hardware and software configurations of each platform. There are
    many frameworks available today using which you can compile your models, such
    as **TVM** and **ONNX**, each with its own pros and cons.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 模型编译消除了针对每个平台的特定硬件和软件配置微调模型所需的努力。现在有许多框架可供使用，可以编译您的模型，例如 **TVM** 和 **ONNX**，每个都有自己的优缺点。
- en: Note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Discussing compilers in detail is out of the scope of this book. For details
    on TVM, refer to this link: [https://tvm.apache.org/](https://tvm.apache.org/).
    And for details about ONNX, refer to this link: [https://onnx.ai/](https://onnx.ai/).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 详细讨论编译器超出了本书的范围。有关 TVM 的详细信息，请参阅此链接：[https://tvm.apache.org/](https://tvm.apache.org/)。有关
    ONNX 的详细信息，请参阅此链接：[https://onnx.ai/](https://onnx.ai/)。
- en: 'Let’s discuss a feature of Amazon SageMaker called **SageMaker Neo**, which
    is used to optimize ML models for inference on multiple platforms. Neo automatically
    optimizes models written in various frameworks, such as Gluon, Keras, PyTorch,
    TensorFlow, and so on, for inference on different platforms, such as Linux and
    Windows, as well as many different processors. For a complete list of frameworks
    or processors supported by Neo, please refer to this link: [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge.html).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论Amazon SageMaker的一个功能，称为**SageMaker Neo**，它用于优化适用于多个平台的机器学习模型。Neo自动优化各种框架（如Gluon、Keras、PyTorch、TensorFlow等）编写的模型，以便在不同的平台（如Linux和Windows）以及许多不同的处理器上进行推理。有关Neo支持的框架或处理器的完整列表，请参阅此链接：[https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge.html)。
- en: The way Neo works is that it will first read the model, convert the framework-specific
    operations and functions into an intermediate representation that is framework
    agnostic, and finally, apply a series of optimizations. It will then generate
    binary code for optimized operations, write it to the shared object library and
    save the model definition and parameters in separate files ([https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Neo的工作方式是首先读取模型，将框架特定的操作和函数转换为无框架的中间表示，然后应用一系列优化。接下来，它将为优化操作生成二进制代码，将其写入共享对象库，并将模型定义和参数保存到单独的文件中（[https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html)）。
- en: It also provides a runtime for each target platform that loads and executes
    the compiled model. Moreover, it can optimize the models with parameters either
    in floating point 32 (`FP32`), quantized into integer 8 (`INT8`), or in floating
    point 16 (`FP16`). It can improve the model’s performance up to 25 times with
    less than one-tenth of the footprint of a DL framework such as TensorFlow or PyTorch.
    To understand this further, let’s take a pretrained image classification model
    from PyTorch and optimize it using SageMaker Neo.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 它还为每个目标平台提供了一个运行时，用于加载和执行编译后的模型。此外，它可以将参数优化为浮点32 (`FP32`)、量化为整数8 (`INT8`)或浮点16
    (`FP16`)。它可以将模型性能提高高达25倍，同时占用比TensorFlow或PyTorch等深度学习框架少十分之一的内存。为了进一步了解这一点，让我们从一个PyTorch的预训练图像分类模型开始，并使用SageMaker
    Neo对其进行优化。
- en: Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'We touched on SageMaker Neo in [*Chapter 8*](B18493_08.xhtml#_idTextAnchor161),
    *Optimizing and Managing Machine Learning Models for Edge Deployment*. Here, we
    will be using the same example to explain it in detail. For the full code, refer
    to the GitHub link: [https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/sagemaker_notebook/1_compile_resnet_model_egde_manager.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/sagemaker_notebook/1_compile_resnet_model_egde_manager.ipynb).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第8章*](B18493_08.xhtml#_idTextAnchor161)中提到了SageMaker Neo，*优化和管理边缘部署的机器学习模型*。在这里，我们将使用相同的示例来详细解释它。完整的代码请参考GitHub链接：[https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/sagemaker_notebook/1_compile_resnet_model_egde_manager.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/sagemaker_notebook/1_compile_resnet_model_egde_manager.ipynb)。
- en: 'Follow these steps to optimize a pretrained model using SageMaker Neo:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤使用SageMaker Neo优化预训练模型：
- en: 'First, we will download a pretrained model from the PyTorch library as shown
    in the following code snippet:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将从PyTorch库中下载一个预训练模型，如下面的代码片段所示：
- en: '[PRE0]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we need to save the model in `model.tar.gz` format, which SageMaker requires,
    specify the input data shape, and upload the model to **Amazon S3**:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将模型保存为SageMaker所需的`model.tar.gz`格式，指定输入数据形状，并将模型上传到**Amazon S3**：
- en: '[PRE6]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once we have the model in SageMaker format, we will prepare the parameters
    required for model compilation. Most importantly, you need to mention the `target_device`
    parameter, as based on it, SageMaker Neo will compile the model for the particular
    hardware on which the model will be deployed:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了SageMaker格式的模型，我们将准备模型编译所需的参数。最重要的是，您需要提及`target_device`参数，因为SageMaker
    Neo将根据它为模型部署的特定硬件编译模型：
- en: '[PRE16]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we will declare the `PyTorchModel` object provided by SageMaker, which
    will have the necessary configurations, such as the model’s S3 path, the framework
    version, the inference script, the Python version, and so on:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将声明SageMaker提供的`PyTorchModel`对象，它将包含必要的配置，例如模型的S3路径、框架版本、推理脚本、Python版本等：
- en: '[PRE26]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, we will use the `PyTorchModel` object to create the compilation job
    and deploy the compiled model to the `ml.c5.2xlarge` instance, since the model
    was compiled for `ml.c5` as the target device:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用`PyTorchModel`对象创建编译作业，并将编译好的模型部署到`ml.c5.2xlarge`实例，因为模型是为`ml.c5`作为目标设备编译的：
- en: '[PRE41]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Once the model has finished compiling, you can then deploy the compiled model
    to make inference. In this case, we are deploying the model for **real-time inference**
    as an endpoint:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型完成编译，您就可以部署编译好的模型进行推理。在这种情况下，我们将模型作为端点部署以进行**实时推理**：
- en: '[PRE66]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Note
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more details on different deployment options provided by SageMaker, refer
    to [*Chapter 7*](B18493_07.xhtml#_idTextAnchor128), *Deploying Machine Learning
    Models* *at Scale*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有关SageMaker提供的不同部署选项的更多详细信息，请参阅[*第7章*](B18493_07.xhtml#_idTextAnchor128)，*大规模部署机器学习模型*。
- en: 'Now, once the model is deployed, we can invoke the endpoint for inference as
    shown in the following code snippet:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，一旦模型部署完成，我们可以像以下代码片段所示那样调用端点进行推理：
- en: '[PRE71]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since we have deployed the model as a real-time endpoint, you will be charged
    for the instance on which the model is deployed. Therefore, if you are not using
    the endpoint, make sure to delete it using the following code snippet.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经将模型部署为实时端点，您将根据模型部署的实例付费。因此，如果您不使用端点，请确保使用以下代码片段将其删除。
- en: 'Use the following code snippet to delete the endpoint, if you are not using
    it:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您不使用它，请使用以下代码片段删除端点：
- en: '[PRE86]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Now that we understand how to optimize a model using SageMaker Neo for inference,
    let’s talk about some of the key metrics that you should consider when trying
    to improve the latency of models. The ideas covered in the next section apply
    to DL models, even when you are not using SageMaker Neo, as you might not be able
    to compile all models with Neo. You can see the supported models and frameworks
    for SageMaker Neo here: [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用SageMaker Neo优化模型进行推理，让我们谈谈在尝试提高模型延迟时应该考虑的一些关键指标。下一节中介绍的思想适用于深度学习模型，即使您不使用SageMaker
    Neo，因为您可能无法使用Neo编译所有模型。您可以在以下位置查看SageMaker Neo支持的模型和框架：[https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html)。
- en: Key metrics for optimizing models
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化模型的关键指标
- en: When it comes to real-time inference, optimizing a model for performance usually
    includes metrics such as latency, throughput, and model size. To optimize the
    model size, the process typically involves having a trained model, checking the
    size of the model, and if it does not fit into single CPU/GPU memory, you can
    choose any of the techniques discussed in the *Reducing the memory footprint of
    DL models* section to prepare it for deployment.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到实时推理时，为了提高性能，优化模型通常包括指标，如延迟、吞吐量和模型大小。为了优化模型大小，通常涉及有一个训练好的模型，检查模型的大小，如果它不适合单个CPU/GPU内存，您可以选择在*减少深度学习模型内存占用*部分讨论的任何技术来准备它以供部署。
- en: For deployment, one of the best practices is to standardize the environment.
    This will involve the use of containers to deploy the model, irrespective of whether
    you are deploying it on your own server or using Amazon SageMaker. The process
    is illustrated in *Figure 9**.6*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于部署，最佳实践之一是标准化环境。这涉及到使用容器来部署模型，无论您是在自己的服务器上部署还是使用Amazon SageMaker。该过程在*图9.6*中说明。6。
- en: '![Figure 9.6 – Model deployed as a real-time endpoint](img/B18493_09_006.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – 作为实时端点部署的模型](img/B18493_09_006.jpg)'
- en: Figure 9.6 – Model deployed as a real-time endpoint
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 作为实时端点部署的模型
- en: 'To summarize, we will first prepare the model for deployment, select or create
    a container to standardize the environment, followed by deploying the container
    on an instance(s). Therefore, to optimize the model’s performance, it is important
    to look for both inference and instance metrics. Inference metrics include the
    following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们首先将为部署准备模型，选择或创建一个容器以标准化环境，然后部署容器到实例（们）。因此，为了优化模型性能，寻找推理和实例指标都很重要。推理指标包括以下内容：
- en: '**Invocations**: The number of requests sent to the model endpoint. You can
    get the total number of requests by using the sum statistics in **Amazon CloudWatch**,
    which monitors the AWS resources or applications that you run on AWS in real time,
    including SageMaker ([https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html)).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调用次数**：发送到模型端点的请求数量。您可以通过使用**Amazon CloudWatch**中的总和统计来获取总请求数量，**Amazon CloudWatch**实时监控您在AWS上运行的资源或应用程序，包括SageMaker（[https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html)）。'
- en: '**Invocations per instance**: If your model is deployed on more than one machine,
    then it’s important to understand the number of invocations sent to a model on
    each instance.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每实例调用次数**：如果您的模型部署在多台机器上，那么了解发送到每个实例上的模型调用次数很重要。'
- en: '**Model latency**: This is the time interval taken by a model to respond. It
    includes the local communication time taken to send the request, for the model
    to complete the inference in the container, and to get the response from the container
    of the model:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型延迟**：这是模型响应所需的时间间隔。它包括发送请求的本地通信时间、模型在容器中完成推理以及从模型容器获取响应的时间：'
- en: '*model latency = request time + inference time taken by the model + response
    time from* *the container*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型延迟 = 请求时间 + 模型在容器中完成推理所需的时间 + 从容器获取响应的时间*'
- en: '**Overhead latency**: This is the time taken to respond after the endpoint
    has received the request minus the model latency. It can depend on multiple factors,
    such as request size, request frequency, authentication/authorization of the request,
    and response payload size.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开销延迟**：这是端点收到请求后响应所需的时间减去模型延迟。它可能取决于多个因素，例如请求大小、请求频率、请求的认证/授权以及响应负载大小。'
- en: '**Maximum Invocations**: This is the maximum number of requests to an endpoint
    per minute.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大调用次数**：这是每分钟对端点的最大请求数量。'
- en: '**Cost per hour**: This gives the estimated cost per hour for your endpoint.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每小时成本**：这表示您端点的每小时预估成本。'
- en: '**Cost per inference**: This provides the estimated cost per inference on your
    endpoint.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每推理成本**：这提供了您端点每推理的预估成本。'
- en: Note
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Cost metrics provide the cost in US dollars.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 成本指标以美元为单位提供成本。
- en: In addition to inference metrics, you should also consider optimizing for instance
    metrics such as GPU utilization, GPU memory, CPU utilization, and CPU memory based
    on the instance type selected. Amazon SageMaker offers more than 70 instances
    from which you can choose to deploy your model. This brings up an additional question
    on how to determine which instance type and the number of instances to select
    for deploying the model in order to achieve your performance requirements. Let’s
    discuss the approach for selecting the optimal instance for your model in the
    next section.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 除了推理指标外，您还应该根据所选实例类型优化实例指标，例如GPU利用率、GPU内存、CPU利用率和CPU内存。Amazon SageMaker提供了超过70种实例，您可以从中选择来部署您的模型。这引发了一个额外的问题，即如何确定选择哪种实例类型和实例数量来部署模型以实现您的性能要求。让我们在下一节中讨论为您的模型选择最佳实例的方法。
- en: Choosing the instance type, load testing, and performance tuning for models
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择实例类型、负载测试和模型性能调整
- en: Traditionally, based on the model type (ML model or DL model) and model size,
    you can make a heuristic guess to test the model’s performance on a few instances.
    This approach is fast but might not be the best approach. Therefore, in order
    to optimize this process, alternatively, you can use the **Inference Recommender**
    feature of Amazon SageMaker ([https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html)),
    which automates the load testing and model tuning process across the SageMaker
    ML instances. It helps you to deploy the ML models on the optimized hardware,
    based on your performance requirements, at the lowest possible cost.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，根据模型类型（ML模型或DL模型）和模型大小，你可以根据经验猜测来测试模型在几个实例上的性能。这种方法速度快，但可能不是最佳方法。因此，为了优化此过程，你可以使用Amazon
    SageMaker的**推理推荐器**功能（[https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html)），该功能自动化了SageMaker
    ML实例上的负载测试和模型调整过程。它帮助你以最低的成本，根据你的性能要求，在优化的硬件上部署ML模型。
- en: 'Let’s take an example by using a pretrained image classification model to understand
    how Inference Recommender works. The following steps outline the process of using
    Inference Recommender:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用预训练的图像分类模型来举例说明如何理解推理推荐器的工作原理。以下步骤概述了使用推理推荐器的过程：
- en: 'Determine the ML model details, such as framework and domain. The following
    is a code snippet for this:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定ML模型细节，例如框架和领域。以下是一个代码片段示例：
- en: '[PRE96]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Take a pretrained model and package it in the compressed TAR file (`*.tar.gz`)
    format, which SageMaker understands, and upload the model to Amazon S3\. If you
    have trained the model on SageMaker, then you can skip this step:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取一个预训练模型，将其打包成SageMaker理解的压缩TAR文件（`*.tar.gz`）格式，并将模型上传到Amazon S3。如果你已经在SageMaker上训练了模型，那么你可以跳过此步骤：
- en: '[PRE101]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Select the inference container, which can be a prebuilt Docker container provided
    by AWS or your own custom container. In our example, we are fetching a prebuilt
    PyTorch container image provided by AWS:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择推理容器，它可以是AWS提供的预构建Docker容器或你自己的自定义容器。在我们的例子中，我们正在获取AWS提供的预构建PyTorch容器镜像：
- en: '[PRE113]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Create a sample payload. In our example, we have images in `.jpg` format, compress
    them in a TAR file, and upload it to Amazon S3\. In this example, we are only
    using four images, but it’s recommended to add a variety of samples, which is
    reflective of your actual payloads:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个样本有效载荷。在我们的例子中，我们有`.jpg`格式的图像，将它们压缩成TAR文件，并上传到Amazon S3。在这个例子中，我们只使用了四张图像，但建议添加各种样本，以反映你的实际有效载荷：
- en: '[PRE125]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Compress the payload in TAR format as shown in the following code snippet and
    upload it to S3:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下代码片段压缩有效载荷，并将其上传到S3：
- en: '[PRE129]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Once, we have the payload in TAR format, let’s use the following code snippet
    to upload it to Amazon S3:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了TAR格式的有效载荷，让我们使用以下代码片段将其上传到Amazon S3：
- en: '[PRE130]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'Register the model in the **model registry**, which is used to catalog models
    for production, manage model versions, associate metadata, manage the approval
    status of the model, deploy models to production, and automate the model deployment
    process ([https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html)).
    Registering a model in the model registry is a two-step process, as shown here:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**模型注册表**中注册模型，该注册表用于对生产中的模型进行编目、管理模型版本、关联元数据、管理模型的批准状态、将模型部署到生产环境，以及自动化模型部署过程（[https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html)）。在模型注册表中注册模型是一个两步过程，如下所示：
- en: 'Create a model package group, which will have all the versions of the model:'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个模型包组，它将包含模型的全部版本：
- en: '[PRE131]'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: Register a model version to the model package group. To get the recommended
    instance type, you have two options – either you can specify a list of instances
    that you want Inference Recommender to use or you can not provide the instance
    list and it will pick the right instance based on the ML domain and ML task. For
    our example, we will use a list of common instance types used for image classification
    algorithms. This involves three steps.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型版本注册到模型包组中。要获取推荐的实例类型，你有两种选择——你既可以指定Inference Recommender要使用的实例列表，也可以不提供实例列表，它将根据ML领域和ML任务选择正确的实例。在我们的例子中，我们将使用用于图像分类算法的常见实例类型列表。这涉及三个步骤。
- en: 'First, create an input dictionary with configuration for registering the model,
    as shown in the following code snippet:'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个输入字典，用于注册模型，如下面的代码片段所示：
- en: '[PRE138]'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'The second step is to create a model inference specification object, which
    will consist of providing details about the container, framework, model input,
    content type, and the S3 path of the trained model:'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步是创建模型推理规范对象，这将包括提供有关容器、框架、模型输入、内容类型以及训练模型S3路径的详细信息：
- en: '[PRE151]'
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'Finally, after providing the inference specification, we will then create the
    model package. Once the model package is created, you can then get the model package
    ARN, and it will also be visible in the SageMaker Studio UI, under **Model Registry**:'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在提供推理规范后，我们将创建模型包。一旦模型包创建完成，你就可以获取模型包的ARN，它也会在SageMaker Studio UI的**模型注册**下可见：
- en: '[PRE170]'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: Now, that the model has been registered, we will create an Inference Recommender
    job. There are two options – either you can create a default job to get instance
    recommendations or you can use an advanced job, where you can provide your inference
    requirements, tune environment variables, and perform more extensive load tests.
    An advanced job takes more time than a default job and depends on your traffic
    pattern and the number of instance types on which it will run the load tests.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，模型已经注册，我们将创建一个推理推荐器作业。有两种选择——你可以创建一个默认作业以获取实例推荐，或者你可以使用一个高级作业，其中你可以提供你的推理要求、调整环境变量并执行更广泛的负载测试。高级作业比默认作业花费更多时间，并且取决于你的流量模式和它将在哪些实例类型上运行负载测试的数量。
- en: In this example, we will create a default job, which will return a list of instance
    type recommendations including environment variables, cost, throughput, model
    latency, and the maximum number of invocations.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将创建一个默认作业，这将返回一个包括环境变量、成本、吞吐量、模型延迟和最大调用次数的实例类型推荐列表。
- en: 'The following code snippet shows how you can create a default job:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何创建默认作业：
- en: '[PRE173]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: Note
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Code to create a custom load test is provided in the GitHub repository: [Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/Chapter09/1_inference_recommender_custom_load_test.ipynb](http://Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/Chapter09/1_inference_recommender_custom_load_test.ipynb).'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub仓库中提供了创建自定义负载测试的代码：[Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/Chapter09/1_inference_recommender_custom_load_test.ipynb](http://Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/Chapter09/1_inference_recommender_custom_load_test.ipynb)。
- en: In the next section, we will discuss the results provided by the Inference Recommender
    job.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论推理推荐器作业提供的结果。
- en: Observing the results
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察结果
- en: The recommendation provided by Inference Recommender includes instance metrics,
    performance metrics, and cost metrics.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 推理推荐器提供的推荐包括实例指标、性能指标和成本指标。
- en: Instance metrics include `InstanceType`, `InitialInstanceCount`, and `EnvironmentParameters`,
    which are tuned according to the job for better performance.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 实例指标包括`InstanceType`、`InitialInstanceCount`和`EnvironmentParameters`，这些指标根据作业进行了调整以获得更好的性能。
- en: Performance metrics include `MaxInvocations` and `ModelLatency`, whereas cost
    metrics include `CostPerHour` and `CostPerInference`.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标包括`MaxInvocations`和`ModelLatency`，而成本指标包括`CostPerHour`和`CostPerInference`。
- en: These metrics enable you to make informed trade-offs between cost and performance.
    For example, if your business requirement is overall price performance with an
    emphasis on throughput, then you should focus on `CostPerInference`. If your requirement
    is a balance between latency and throughput, then you should focus on `ModelLatency`
    and `MaxInvocations` metrics.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标使你能够在成本和性能之间做出明智的权衡。例如，如果你的业务需求是整体价格性能，且注重吞吐量，那么你应该关注`CostPerInference`。如果你的需求是在延迟和吞吐量之间取得平衡，那么你应该关注`ModelLatency`和`MaxInvocations`指标。
- en: You can view the results of the Inference Recommender job either through an
    API call or in the SageMaker Studio UI.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过API调用或在SageMaker Studio UI中查看推理推荐器作业的结果。
- en: 'The following is the code snippet for observing the results:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段用于观察结果：
- en: '[PRE174]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: 'You can observe the results from the SageMaker Studio UI by logging into SageMaker
    Studio, clicking on the orange triangle icon, and selecting **Model registry**
    from the drop-down menu:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过登录SageMaker Studio，点击橙色三角形图标，并在下拉菜单中选择**模型注册**来观察SageMaker Studio UI中的结果：
- en: '![Figure 9.7 – Inference Recommender results](img/B18493_09_007.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 推理推荐器结果](img/B18493_09_007.jpg)'
- en: Figure 9.7 – Inference Recommender results
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 推理推荐器结果
- en: Now that we understand how the Inference Recommender feature of Amazon SageMaker
    can be used to get the right instance type and instance count, let’s take a look
    at the topics covered in this chapter in the next section.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用Amazon SageMaker的推理推荐器功能来获取正确的实例类型和实例数量，让我们在下一节中查看本章涵盖的主题。
- en: Summary
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed various techniques for optimizing ML and DL models
    for real-time inference. We talked about different ways to reduce the memory footprint
    of DL models, such as pruning and quantization, followed by a deeper dive into
    model compilation. We then discussed key metrics that can help in evaluating the
    performance of models. Finally, we did a deep dive into how you can select the
    right instance, run load tests, and automatically perform model tuning using SageMaker
    Inference Recommender’s capability.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了各种优化机器学习和深度学习模型以实现实时推理的技术。我们讨论了不同的方法来减少深度学习模型的内存占用，例如剪枝和量化，随后深入探讨了模型编译。然后，我们讨论了可以帮助评估模型性能的关键指标。最后，我们深入探讨了如何使用SageMaker推理推荐器的功能来选择合适的实例、运行负载测试以及自动进行模型调优。
- en: In the next chapter, we will discuss visualizing and exploring large amounts
    of data on AWS.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论在AWS上可视化探索大量数据的方法。
