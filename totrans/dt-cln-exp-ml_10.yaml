- en: '*Chapter 7*: Linear Regression Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第七章*: 线性回归模型'
- en: Linear regression is perhaps the most well-known machine learning algorithm,
    having origins in statistical learning at least 200 years ago. If you took a statistics,
    econometrics, or psychometrics course in college, you were likely introduced to
    linear regression, even if you took that course long before machine learning was
    taught in undergraduate courses. As it turns out, many social and physical phenomena
    can be successfully modeled as a function of a linear combination of predictor
    variables. This is as useful for machine learning as it has been for statistical
    learning all these years, though, with machine learning, we care much less about
    the parameter values than we do about predictions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归可能是最著名的机器学习算法，其起源至少可以追溯到200年前的统计学习。如果你在大学里学习了统计学、计量经济学或心理测量学课程，你很可能已经接触到了线性回归，即使你在机器学习在本科课程中教授之前就已经上了这门课。实际上，许多社会和物理现象都可以成功地被建模为预测变量线性组合的函数。尽管如此，线性回归对机器学习来说仍然非常有用，就像这些年来对统计学习一样，不过，在机器学习中，我们更关心预测而不是参数值。
- en: Linear regression is a very good choice for modeling a continuous target, assuming
    that our features and target have certain qualities. In this chapter, we will
    go over the assumptions of linear regression models and construct a model using
    data that is largely consistent with these assumptions. However, we will also
    explore alternative approaches, such as nonlinear regression, which we use when
    these assumptions do not hold. We will conclude this chapter by looking at techniques
    that address the possibility of overfitting, such as lasso regression.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的特征和目标具有某些特性，线性回归是建模连续目标的一个非常好的选择。在本章中，我们将讨论线性回归模型的假设，并使用与这些假设大部分一致的数据构建模型。然而，我们还将探索替代方法，例如非线性回归，当这些假设不成立时我们会使用它。我们将通过查看解决过拟合可能性的技术来结束本章，例如lasso回归。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Key concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键概念
- en: Linear regression and gradient descent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归与梯度下降
- en: Using classical linear regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用经典线性回归
- en: Using lasso regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用lasso回归
- en: Using non-linear regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用非线性回归
- en: Regression with gradient descent
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度下降进行回归
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will stick to the libraries that are available with most
    scientific distributions of Python – NumPy, pandas, and scikit-learn. The code
    for this chapter can be found in this book’s GitHub repository at [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将坚持使用大多数Python科学发行版中可用的库——NumPy、pandas和scikit-learn。本章的代码可以在本书的GitHub仓库中找到，网址为[https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning)。
- en: Key concepts
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键概念
- en: The typical analyst who has been doing predictive modeling for a while has constructed
    tens, perhaps hundreds, of linear regression models over the years. If you worked
    for a large accounting firm in the late 1980s, as I did, and you were doing forecasting,
    you may have spent your whole day, every day, specifying linear models. You would
    have run all conceivable permutations of independent variables and transformations
    of dependent variables, and diligently looked for evidence of heteroscedasticity
    (non-constant variance in residuals) or multicollinearity (highly correlated features).
    But most of all, you worked hard to identify key predictor variables and address
    any bias in your parameter estimates (your coefficients or weights).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 经常从事预测建模的分析师通常会构建数十个，甚至数百个线性回归模型。如果你像我一样，在20世纪80年代末为一家大型会计师事务所工作，并且从事预测工作，你可能每天都会花整天时间指定线性模型。你会运行所有可能的独立变量排列和因变量变换，并勤奋地寻找异方差性（残差中的非恒定方差）或多重共线性（高度相关的特征）的证据。但最重要的是，你努力识别关键预测变量，并解决任何参数估计（你的系数或权重）中的偏差。
- en: Key assumptions of linear regression models
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归模型的关键假设
- en: 'Much of that effort still applies today, though there is now much more emphasis
    on the accuracy of predictions than on parameter estimates. We worry about overfitting
    now, in a way that we did not 30 years ago. We are also more likely to seek alternatives
    when the assumptions of linear regression models are violated. These assumptions
    are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那些努力的大部分至今仍然适用，尽管现在对预测准确性的重视程度超过了参数估计。我们现在担心过度拟合，而30年前我们并没有这样做。当线性回归模型的假设被违反时，我们也更有可能寻求替代方案。这些假设如下：
- en: There there is a linear relationship between features (independent variables)
    and the target (dependent variable)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征（自变量）与目标（因变量）之间存在线性关系
- en: That the residuals (the difference between actual and predicted values) are
    normally distributed
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偶然误差（实际值与预测值之间的差异）是正态分布的
- en: That the residuals are independent across observations
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偶然误差在观测之间是独立的
- en: That the variance of residuals is constant
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偶然误差的方差是恒定的
- en: It is not unusual for one or more of these assumptions to be violated with real-world
    data. The relationship between a feature and target is often not linear. The influence
    of the feature may vary across the range of that feature. Anyone familiar with
    the expression “*too many cooks in the kitchen*” likely appreciates that the marginal
    increase in productivity with the fifth cook may not be as great as with the second
    or third.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，这些假设中有一个或多个被违反并不罕见。特征与目标之间的关系通常是线性的。特征的影响可能在该特征的范围内变化。任何熟悉“*厨房里的人太多*”这个表达的人可能都会欣赏到，第五个厨师带来的边际生产力的增加可能不如第二个或第三个厨师那么大。
- en: Our residuals are sometimes not normally distributed. This can indicate that
    our model is less accurate along certain ranges of our target. For example, it
    is not unusual to have smaller residuals along the middle of the target’s range,
    say the 25th to 75th percentile, and higher residuals at the extremes. This can
    happen when the relationship with the target is nonlinear.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的偶然误差有时不是正态分布的。这表明我们的模型在目标的一些范围内可能不够准确。例如，在目标范围的中间部分（比如第25到第75百分位数）可能会有较小的偶然误差，而在两端可能会有较大的偶然误差。这可能是由于与目标的关系是非线性的。
- en: There are several reasons why residuals may not be independent. This is often
    the case with time series data. For a model of daily stock price, the residuals
    may be correlated for adjacent days. This is referred to as autocorrelation. This
    can also be a problem with longitudinal or repeated measures data. For example,
    we may have test scores for 600 students in 20 different classrooms or annual
    wage income for 100 people. Our residuals would not be independent if our model
    failed to account for there being no variation in some features across a group
    – the classroom-determined and person-determined features in these examples.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 偶然误差可能不独立的原因有几个。这在时间序列数据中通常是这种情况。对于一个日股价模型，相邻天的偶然误差可能是相关的。这被称为自相关。这也可能是纵向或重复测量数据的问题。例如，我们可能有20个不同教室中600名学生的考试成绩，或者100人的年度工资收入。如果我们的模型未能考虑到某些特征在群体中不存在变化——在这些例子中是教室决定和个人决定的特征——那么我们的偶然误差就不会是独立的。
- en: Finally, it is not uncommon for our residuals to have greater variability along
    different ranges of a feature. If we are predicting temperatures at weather stations
    around the world, and latitude is one of the features we are using, there is a
    chance that there will be greater residuals at higher latitude values. This is
    known as heteroscedasticity. This may also be an indicator that our model has
    omitted important predictors.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的偶然误差在不同特征的不同范围内可能会有更大的变异性。如果我们正在预测全球气象站的温度，并且纬度是我们使用的特征之一，那么在较高的纬度值上可能会有更大的偶然误差。这被称为异方差性。这也可能表明我们的模型遗漏了重要的预测因子。
- en: Beyond these four key assumptions, another common challenge with linear regression
    is the high correlation among features. This is known as multicollinearity. As
    we discussed in [*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature
    Selection*, we likely increase the risk of overfitting when our model struggles
    to isolate the independent effect of a particular feature because it moves so
    much with another feature. This will be familiar to any of you who have spent
    weeks building a model where the coefficients shift dramatically with each new
    specification.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这四个关键假设之外，线性回归的另一个常见挑战是特征之间的高度相关性。这被称为多重共线性。正如我们在[*第五章*](B17978_05_ePub.xhtml#_idTextAnchor058)中讨论的[*特征选择*]，当我们的模型难以隔离特定特征的独立影响，因为它与另一个特征变化很大时，我们可能会增加过拟合的风险。对于那些花费数周时间构建模型的人来说，这将是熟悉的，其中系数随着每个新规格的提出而大幅变动。
- en: When one or more of these assumptions is violated, we may still be able to use
    a traditional regression model. However, we may need to transform the data in
    some way. We will discuss techniques for identifying violations of these assumptions,
    the implications of those violations for model performance, and possible ways
    to address these issues throughout this chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当违反这些假设中的一个或多个时，我们仍然可以使用传统的回归模型。然而，我们可能需要以某种方式转换数据。我们将在本章中讨论识别这些假设违反的技术、这些违反对模型性能的影响以及解决这些问题的可能方法。
- en: Linear regression and ordinary least squares
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归和普通最小二乘法
- en: 'The most common estimation technique for linear regression is **ordinary least
    squares** (**OLS**). OLS selects coefficients that minimize the sum of the squared
    distance between the actual target values and the predicted values. More precisely,
    OLS minimizes the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归中最常见的估计技术是**普通最小二乘法**（**OLS**）。OLS选择系数，使得实际目标值与预测值之间平方距离之和最小。更精确地说，OLS最小化以下：
- en: '![](img/B17978_07_001.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_07_001.jpg)'
- en: Here, ![](img/B17978_07_002.png) is the actual value at the ith observation
    and ![](img/B17978_07_003.png) is the predicted value. As we have discussed, the
    difference between the actual target value and the predicted target value, ![](img/B17978_07_004.png),
    is known as the residual.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![图片](img/B17978_07_002.png)是第i个观测的实际值，![图片](img/B17978_07_003.png)是预测值。正如我们讨论过的，实际目标值与预测目标值之间的差异，![图片](img/B17978_07_004.png)，被称为残差。
- en: 'Graphically, OLS fits a line through our data that minimizes the vertical distance
    of data points from that line. The following plot illustrates a model with one
    feature, known as simple linear regression, with made-up data points. The vertical
    distance between each data point and the regression line is the residual, which
    can be positive or negative:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从图形上看，普通最小二乘法（OLS）通过我们的数据拟合一条线，使得数据点到这条线的垂直距离最小。以下图表展示了一个具有一个特征（称为简单线性回归）的模型，使用的是虚构的数据点。每个数据点到回归线的垂直距离是残差，可以是正数或负数：
- en: '![Figure 7.1 – Ordinary least squares regression line ](img/B17978_07_0011.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 普通最小二乘法回归线](img/B17978_07_0011.jpg)'
- en: Figure 7.1 – Ordinary least squares regression line
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 普通最小二乘法回归线
- en: 'The line,![](img/B17978_07_005.png), gives us the predicted value of *y* for
    each value of *x*. It is equal to the estimated intercept, ![](img/B17978_07_006.png),
    plus the estimated coefficient for the feature times the feature value, ![](img/B17978_07_007.png).
    This is the OLS line. Any other straight line through the data would result in
    a higher sum of squared residuals. This can be extended to multiple linear regression
    models – that is, those with more than one feature:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这条线，![图片](img/B17978_07_005.png)，给出了每个*x*值的*y*预测值。它等于估计的截距，![图片](img/B17978_07_006.png)，加上特征估计系数乘以特征值，![图片](img/B17978_07_007.png)。这就是OLS线。任何其他穿过数据的直线都会导致平方残差之和更高。这可以扩展到多线性回归模型
    – 即具有一个以上特征的模型：
- en: '![](img/B17978_07_008.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_07_008.jpg)'
- en: Here, *y* is the target, each *x* is a feature, each ![](img/B17978_07_009.png)
    is a coefficient (or the intercept), *n* is the number of features, and *ɛ* is
    an error term. Each coefficient is the estimated change in the target from a 1-unit
    change in the associated feature. This is a good place to notice that the coefficient
    is constant across the whole range of each feature; that is, an increase in the
    feature from 0 to 1 is assumed to have the same impact on the target as from 999
    to 1000\. However, this does not always make sense. Later in this chapter, we
    will discuss how to use transformations when the relationship between a feature
    and the target is not linear.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y* 是目标，每个 *x* 是一个特征，每个 ![](img/B17978_07_009.png) 是一个系数（或截距），*n* 是特征的数量，*ɛ*
    是一个误差项。每个系数是从关联特征1单位变化引起的目标估计变化。这是一个很好的地方来注意到系数在整个特征的范围上是恒定的；也就是说，特征从0到1的增加被认为对目标的影响与从999到1000的影响相同。然而，这并不总是有道理。在本章的后面，我们将讨论当特征与目标之间的关系不是线性的情况下如何使用变换。
- en: An important advantage of linear regression is that it is not as computationally
    expensive as other supervised regression algorithms. When linear regression performs
    well, based on metrics such as those we discussed in the previous chapter, it
    is a good choice. This is particularly true when you have large amounts of data
    to train or your business process does not permit large blocks of time for model
    training. The efficiency of the algorithm can also make it feasible to use more
    resource-intensive feature selection techniques, such as wrapper methods, which
    we discussed in [*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature
    Selection*. As we saw there, you may not want to use exhaustive feature selection
    with a decision tree regressor. However, it may be perfectly fine with a linear
    regression model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的一个重要优点是它不像其他监督回归算法那样计算量大。当线性回归表现良好，基于我们之前章节讨论的指标时，它是一个好的选择。这尤其适用于你有大量数据需要训练或你的业务流程不允许大量时间用于模型训练的情况。算法的效率还可以使其使用更资源密集的特征选择技术成为可能，例如我们讨论过的包装方法，这在[*第五章*](B17978_05_ePub.xhtml#_idTextAnchor058)
    *特征选择*中提到。正如我们看到的，你可能不想使用决策树回归器的穷举特征选择。然而，对于线性回归模型来说，这可能是完全可行的。
- en: Linear regression and gradient descent
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归和梯度下降
- en: We can use gradient descent, rather than ordinary least squares, to estimate
    our linear regression parameters. Gradient descent iterates over possible coefficient
    values to find those that minimize the residual sum of squares. It starts with
    random coefficient values and calculates the sum of squared errors for that iteration.
    Then, it generates new values for coefficients that yield smaller residuals than
    those from the previous step. We specify a learning rate when using gradient descent.
    The learning rate determines the amount of improvement in residuals at each step.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用梯度下降而不是普通最小二乘法来估计我们的线性回归参数。梯度下降通过迭代可能的系数值来找到那些使残差平方和最小的值。它从随机的系数值开始，并计算该迭代的平方误差总和。然后，它为系数生成新的值，这些值比上一步的残差更小。当我们使用梯度下降时，我们指定一个学习率。学习率决定了每一步残差改进的量。
- en: Gradient descent can often be a good choice when working with very large datasets.
    It may be the only choice if the full dataset does not fit into your machine’s
    memory. We will use both OLS and gradient descent to estimate our parameters in
    the next section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理非常大的数据集时，梯度下降通常是一个不错的选择。如果整个数据集无法装入你的机器内存，它可能就是唯一的选择。在下一节中，我们将使用OLS和梯度下降来估计我们的参数。
- en: Using classical linear regression
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用经典线性回归
- en: In this section, we will specify a fairly straightforward linear model. We will
    use it to predict the implied gasoline tax of a country based on several national
    economic and political measures. But before we specify our model, we need to do
    the pre-processing tasks we discussed in the first few chapters of this book.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将指定一个相当直接的线性模型。我们将使用它来根据几个国家的经济和政治指标预测隐含的汽油税。但在我们指定模型之前，我们需要完成本书前几章讨论的预处理任务。
- en: Pre-processing the data for our regression model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对我们的回归模型进行数据预处理
- en: We will use pipelines to pre-process our data in this chapter, and throughout
    the rest of this book. We need to impute values where they are missing, identify
    and handle outliers, and encode and scale our data. We also need to do this in
    a way that avoids data leakage and cleans the training data without peeking ahead
    to the testing data. As we saw in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078),
    *Preparing for Model Evaluation*, scikit-learn’s pipelines can help with these
    tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用管道来预处理我们的数据，并在本书的其余部分也是如此。我们需要在数据缺失的地方填充值，识别和处理异常值，以及编码和缩放我们的数据。我们还需要以避免数据泄露并清理训练数据而不提前查看测试数据的方式来做这些。正如我们在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)中看到的，*准备模型评估*，scikit-learn的管道可以帮助我们完成这些任务。
- en: The dataset we will use contains the implied gasoline tax for each country and
    some possible predictors, including national income per capita, government debt,
    fuel income dependency, extent of car use, and measures of democratic processes
    and government effectiveness.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集包含每个国家的隐含汽油税和一些可能的预测因子，包括人均国民收入、政府债务、燃料收入依赖性、汽车使用范围以及民主过程和政府有效性的衡量指标。
- en: Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset on implied gasoline tax by country is available for public use
    on the Harvard Dataverse at [https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RX4JGK](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RX4JGK).
    It was compiled by *Paasha Mahdavi*, *Cesar B. Martinez-Alvarez*, and *Michael
    L. Ross*. The implied gasoline tax is calculated based on the difference between
    the world benchmark price and the local price for a liter of gas. A local price
    above the benchmark price represents a tax. When the benchmark price is higher,
    it can be considered a subsidy. We will use 2014 data for each country for this
    analysis.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关于各国隐含汽油税的数据集可以在哈佛数据共享平台[https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RX4JGK](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RX4JGK)上供公众使用。它由*Paasha
    Mahdavi*、*Cesar B. Martinez-Alvarez*和*Michael L. Ross*编制。隐含汽油税是根据每升汽油的世界基准价格和当地价格之间的差异来计算的。当地价格高于基准价格表示征税。当基准价格更高时，它可以被视为补贴。我们将使用每个国家的2014年数据进行分析。
- en: 'Let’s start by pre-processing the data:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先对数据进行预处理：
- en: 'First, we load many of the libraries we worked with in the last few chapters.
    However, we also need two new libraries to build the pipeline for our data – `ColumnTransformer`
    and `TransformedTargetRegressor`. These libraries allow us to build a pipeline
    that does different pre-processing on numerical and categorical features, and
    that also transforms our target:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加载了许多我们在上一章中使用的库。然而，我们还需要两个新的库来构建我们的数据管道——`ColumnTransformer`和`TransformedTargetRegressor`。这些库允许我们构建一个对数值和分类特征进行不同预处理的管道，并且还可以转换我们的目标：
- en: '[PRE0]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can extend the functionality of a scikit-learn pipeline by adding our own
    classes. Let’s add a class to handle extreme values called `OutlierTrans`.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过添加自己的类来扩展scikit-learn管道的功能。让我们添加一个名为`OutlierTrans`的类来处理极端值。
- en: To include this class in a pipeline, it must inherit from the `BaseEstimator`
    class. We must also inherit from `TransformerMixin`, though there are other possibilities.
    Our class needs the `fit` and `transform` methods. We can put code for assigning
    extreme values as missing in the `transform` method.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这个类包含在管道中，它必须继承自`BaseEstimator`类。我们还必须继承自`TransformerMixin`，尽管还有其他可能性。我们的类需要`fit`和`transform`方法。我们可以在`transform`方法中放置将极端值赋为缺失值的代码。
- en: 'But before we can use our class, we need to import it. To import it, we need
    to append the `helperfunctions` subfolder, since that is where we have placed
    the `preprocfunc` module that contains our class:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们能够使用我们的类之前，我们需要导入它。为了导入它，我们需要追加`helperfunctions`子文件夹，因为那里是我们放置包含我们的类的`preprocfunc`模块的地方：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This imports the `OutlierTrans` class, which we can add to the pipelines we
    create:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这导入了`OutlierTrans`类，我们可以将其添加到我们创建的管道中：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `OutlierTrans` class uses a fairly standard univariate approach for identifying
    an outlier. It calculates the **interquartile range** (**IQR**) for each feature,
    then sets any value that is more than 1.5 times the IQR above the third quartile
    or below the first quartile to missing. We can change the threshold to something
    other than 1.5, such as 2.0, if we want to be more conservative. (We discussed
    this technique for identifying outliers in [*Chapter 1*](B17978_01_ePub.xhtml#_idTextAnchor014),
    *Examining the Distribution of Features and Targets*.)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`OutlierTrans`类使用一个相当标准的单变量方法来识别异常值。它计算每个特征的**四分位数范围**（**IQR**），然后将任何高于第三四分位数1.5倍或低于第一四分位数1.5倍以上的值设置为缺失。如果我们想更保守一些，可以将阈值改为其他值，例如2.0。（我们在[*第1章*](B17978_01_ePub.xhtml#_idTextAnchor014)，*检查特征和目标的分布*中讨论了识别异常值的技术。）'
- en: 'Next, we load the gasoline tax data for 2014\. There are 154 rows – one for
    each country in the DataFrame. A few features have some missing values, but only
    one, `motorization_rate`, has double-digit missings. `motorization_rate` is the
    number of cars per person:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载2014年的汽油税数据。有154行——数据框中每个国家一行。一些特征有一些缺失值，但只有`motorization_rate`有两位数的缺失。`motorization_rate`是每人的汽车数量：
- en: '[PRE3]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s separate the features into numerical and binary features. We will put
    `motorization_rate` into a special category because we anticipate having to do
    a little more with it than with the other features:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将特征分为数值特征和二进制特征。我们将`motorization_rate`放入一个特殊类别，因为我们预计需要比其他特征做更多的工作：
- en: '[PRE4]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We should look at some descriptives for the numeric features and the target.
    Our target, `gas_tax_imp`, has a median value of 0.52\. Notice that some of the
    features have a very different range. More than half of the countries have a `polity`
    score of 7 or higher; 10 is the highest possible `polity` score, meaning most
    democratic. Most countries have a negative value for government effectiveness.
    `democracy_index` is a very similar measure to `polity`, though there is more
    variation:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该查看一些数值特征和目标的描述性统计。我们的目标`gas_tax_imp`的中位数为0.52。请注意，一些特征的范围非常不同。超过一半的国家`polity`得分为7或更高；10是可能的最高`polity`得分，意味着最民主。大多数国家的政府有效性值为负。`democracy_index`与`polity`是非常相似的一个度量，尽管有更多的变化：
- en: '[PRE5]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s also look at the distribution of the binary features. We must set `normalize`
    to `True` to generate ratios rather than counts. The `democracy_polity` and `autocracy_polity`
    features are just binarized versions of the `polity` feature; very high `polity`
    scores get `democracy_polity` values of 1, while very low `polity` scores get
    `autocracy_polity` values of 1\. Similarly, `democracy` is a dummy feature for
    those countries with high `democracy_index` values. Interestingly, nearly half
    of the countries (0.46) have a national oil company, and almost a quarter (0.23)
    have a state-owned national oil company:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再看看二进制特征的分布。我们必须将`normalize`设置为`True`以生成比率而不是计数。`democracy_polity`和`autocracy_polity`特征是`polity`特征的二进制版本；非常高的`polity`得分得到`democracy_polity`值为1，而非常低的`polity`得分得到`autocracy_polity`值为1。同样，`democracy`是一个虚拟特征，用于那些`democracy_index`值较高的国家。有趣的是，近一半的国家（0.46）拥有国家石油公司，而几乎四分之一（0.23）拥有国有国家石油公司：
- en: '[PRE6]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This all looks to be in fairly good shape. However, we will need to do some
    work on the missing values for several features. We also need to do some scaling,
    but there is no need to do any encoding because we can use the binary features
    as they are. Some features are correlated, so we need to do some feature elimination.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都看起来相当不错。然而，我们需要对几个特征的缺失值做一些工作。我们还需要做一些缩放，但不需要进行任何编码，因为我们可以直接使用二进制特征。一些特征是相关的，因此我们需要进行一些特征消除。
- en: 'We begin our pre-processing by creating training and testing DataFrames. We
    will only reserve 20% for testing:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过创建训练和测试数据框开始预处理。我们将只为测试保留20%：
- en: '[PRE7]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We need to build a pipeline with column transformations so that we can do different
    pre-processing on numeric and categorical data. We will construct a pipeline,
    `standtrans`, for all of the numeric columns in `num_cols`. First, we want to
    set outliers to missing. We will define an outlier value as one that is more than
    two times the interquartile range above the third quartile, or below the first
    quartile, for that feature. We will use `SimpleImputer` to set missing values
    to the median.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要构建一个包含列转换的管道，以便我们可以对数值数据和分类数据进行不同的预处理。我们将为`num_cols`中的所有数值列构建一个管道，命名为`standtrans`。首先，我们想要将异常值设置为缺失值。我们将异常值定义为超过第三四分位数两倍以上的值，或者低于第一四分位数的值。我们将使用`SimpleImputer`将缺失值设置为该特征的中间值。
- en: We do not want to scale the binary features in `dummy_cols`, but we do want
    to use `SimpleImputer` to set missing values to the most frequent value for each
    categorical feature.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望对`dummy_cols`中的二元特征进行缩放，但我们确实想要使用`SimpleImputer`将每个分类特征的缺失值设置为最频繁的值。
- en: 'We won’t use `SimpleImputer` for `motorization_rate`. Remember that `motorization_rate`
    is not in the `num_cols` list – it is in the `spec_cols` list. We set up a special
    pipeline, `spectrans`, for features in `spec_cols`. We will use `motorization_rate`
    values:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会为`motorization_rate`使用`SimpleImputer`。记住，`motorization_rate`不在`num_cols`列表中——它在`spec_cols`列表中。我们为`spec_cols`中的特征设置了一个特殊的管道，`spectrans`。我们将使用`motorization_rate`值：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This sets up all of the pre-processing we want to do on the gasoline tax data.
    To do the transformations, all we need to do is call the `fit` method of the column
    transformer. However, we will not do that yet because we also want to add feature
    selection to the pipeline and get it to run a linear regression. We will do that
    in the next few steps.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这设置了我们在汽油税数据上想要进行的所有预处理。要进行转换，我们只需要调用列转换器的`fit`方法。然而，我们目前不会这样做，因为我们还想要将特征选择添加到管道中，并使其运行线性回归。我们将在接下来的几个步骤中完成这些操作。
- en: Running and evaluating our linear model
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行和评估我们的线性模型
- en: We will use **recursive feature elimination** (**RFE**) to select features for
    our model. RFE has the advantages of wrapper feature selection methods – it evaluates
    features based on a selected algorithm, and it considers multivariate relationships
    in that assessment. However, it can also be computationally expensive. Since we
    do not have many features or observations, that is not much of a problem in this
    case.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**递归特征消除**（**RFE**）来选择模型的特征。RFE具有包装特征选择方法的优点——它根据选定的算法评估特征，并在评估中考虑多元关系。然而，它也可能在计算上很昂贵。由于我们没有很多特征或观测值，在这种情况下这并不是一个大问题。
- en: 'After selecting the features, we run a linear regression model and take a look
    at our predictions. Let’s get started:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择特征后，我们运行线性回归模型并查看我们的预测。让我们开始吧：
- en: First, we create linear regression and recursive feature elimination instances
    and add them to the pipeline. We also create a `TransformedTargetRegressor` object
    since we still need to transform the target. We pass our pipeline to the regressor
    parameter of `TransformedTargetRegressor`.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建线性回归和递归特征消除实例，并将它们添加到管道中。我们还创建了一个`TransformedTargetRegressor`对象，因为我们仍然需要转换目标。我们将我们的管道传递给`TransformedTargetRegressor`的回归器参数。
- en: 'Now, we can call the target regressor’s `fit` method. After that, the `support_`
    attribute of the pipeline’s `rfe` step will give us the selected features. Similarly,
    we can get the coefficients by getting the `coef_` value of the `linearregression`
    step. The key here is that referencing `ttr.regressor` gets us to the pipeline:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以调用目标回归器的`fit`方法。之后，管道的`rfe`步骤的`support_`属性将给我们提供选定的特征。同样，我们可以通过获取`linearregression`步骤的`coef_`值来获取系数。关键在于通过引用`ttr.regressor`我们能够访问到管道：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our feature selection identified the VAT rate, government debt, a measure of
    government effectiveness (`goveffect`), whether the country is in the autocracy
    category, whether there is a national oil company and one that is state-owned,
    and the motorization rate as the top seven features. The number of features indicated
    is an example of a hyperparameter, and our choice of seven here is fairly arbitrary.
    We will discuss techniques for hyperparameter tuning in the next section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特征选择确定了增值税率、政府债务、政府有效性指标（`goveffect`）、国家是否属于威权主义类别、是否有国家石油公司以及是否为国有企业，以及机动化率作为前七个特征。特征的数目是一个超参数的例子，我们在这里选择七个是相当随意的。我们将在下一节讨论超参数调整的技术。
- en: Notice that of the several autocracy/democracy measures in the dataset, the
    one that seems to matter most is the autocracy dummy, which has a value of 1 for
    countries with very low `polity` scores. It is estimated to have a negative effect
    gasoline taxes; that is, to reduce them.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在数据集中的几个威权/民主措施中，似乎最重要的是威权虚拟变量，对于`polity`得分非常低的国家的值为1。它被估计对汽油税有负面影响；也就是说，会减少它们。
- en: 'Let’s take a look at the predictions and the residuals. We can pass the features
    from the testing data to the transformer’s/pipeline’s `predict` method to generate
    the predictions. There is a little positive skew and some overall bias; the residuals
    are negative overall:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看预测值和残差。我们可以将测试数据中的特征传递给transformer的/pipeline的`predict`方法来生成预测值。存在一点正偏斜和整体偏差；残差总体上是负的：
- en: '[PRE10]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s also generate some overall model evaluation statistics. We get a mean
    absolute error of `0.23`. That’s not a great average error, given that the median
    value for the gas tax price is `0.52`. The r-squared is decent, however:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也生成一些整体模型评估统计信息。我们得到平均绝对误差为`0.23`。考虑到汽油税价格的中间值为`0.52`，这不是一个很好的平均误差。然而，r-squared值还不错：
- en: '[PRE11]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'It is usually helpful to look at a plot of the residuals. Let’s also draw a
    red dashed line at the average value of the residuals:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常查看残差的图表是有帮助的。让我们也画一条代表残差平均值的红色虚线：
- en: '[PRE12]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This produces the following plot:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 7.2 – Gas tax model residuals ](img/B17978_07_002.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 汽油税模型残差](img/B17978_07_002.jpg)'
- en: Figure 7.2 – Gas tax model residuals
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 汽油税模型残差
- en: This plot shows the positive skew. Moreover, our model is somewhat more likely
    to over-predict the gas tax than under-predict it. (The residual is negative when
    the prediction is greater than the actual target value.)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表显示了正偏斜。此外，我们的模型更有可能高估汽油税而不是低估它。（当预测值大于实际目标值时，残差为负。）
- en: 'Let’s also look at a scatterplot of the predictions against the residuals.
    Let’s draw a red dashed line at 0 on the *Y*-axis:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也看看预测值与残差的散点图。让我们在*Y*轴上画一条代表0的红色虚线：
- en: '[PRE13]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This produces the following plot:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 7.3 – Scatter plot of predictions against residuals ](img/B17978_07_003.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 预测值与残差的散点图](img/B17978_07_003.jpg)'
- en: Figure 7.3 – Scatter plot of predictions against residuals
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 预测值与残差的散点图
- en: Here, overprediction occurs throughout the range of predicted values, but there
    are no underpredictions (positive residuals) with predictions below 0 or above
    1\. This should give us some doubts about our assumption of linearity.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，高估发生在预测值的整个范围内，但没有低估（正残差）出现在预测值低于0或高于1的情况下。这应该让我们对我们的线性假设产生一些怀疑。
- en: Improving our model evaluation
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高我们的模型评估
- en: One problem with how we have evaluated our model so far is that we are not making
    great use of the data. We are only training on about 80% of the data. Our metrics
    are also quite dependent on the testing data being representative of the real
    world we want to predict. However, it might not be. We can improve our odds with
    k-fold cross-validation, as we discussed in the previous chapter.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止评估模型的一个问题是，我们没有充分利用数据。我们只在大约80%的数据上进行了训练。我们的指标也相当依赖于测试数据是否代表我们想要预测的真实世界。然而，可能并不总是如此。正如前一章所讨论的，我们可以通过k折交叉验证来提高我们的机会。
- en: Since we have been using pipelines for our analysis, we have already done much
    of the work we need for k-fold cross-validation. Recall from the previous chapter
    that the k-fold model evaluation divides our data into k equal parts. One of the
    folds is designated for testing and the rest for training. This is repeated k
    times, with a different fold being used for testing each time.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们一直在使用pipelines进行我们的分析，我们已经为k折交叉验证做了很多工作。回想一下前一章，k折模型评估将我们的数据分成k个相等的部分。其中一部分被指定为测试，其余部分用于训练。这重复k次，每次使用不同的折进行测试。
- en: 'Let’s try k-fold cross-validation with our linear regression model:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用我们的线性回归模型进行k折交叉验证：
- en: 'We will start by creating new training and testing DataFrames, leaving just
    10% for later validation. We do not need to retain as much data for validation,
    though it is a good idea to always hold a little back:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先创建新的训练和测试DataFrames，留下10%用于后续验证。虽然保留数据用于验证不是必须的，但保留一小部分数据总是一个好主意：
- en: '[PRE14]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We also need to instantiate `KFold` and `LinearRegression` objects:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要实例化`KFold`和`LinearRegression`对象：
- en: '[PRE15]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we are ready to run our k-fold cross validation. We indicate that we want
    both r-squared and mean absolute error for each split. `cross_validate` automatically
    gives us fit and score times for each fold:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好运行我们的k折交叉验证。我们表示我们想要每个分割的r-squared和平均绝对误差。"cross_validate"自动为我们提供每个折叠的拟合和评分时间：
- en: '[PRE16]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: These scores are not very impressive. We do not end up explaining as much of
    the variance as we would like. R-squared scores average about 0.62 across the
    three folds. This is partly because the testing DataFrames of each fold are quite
    small, with about 40 observations in each. Nonetheless, we should explore modifications
    of the classical linear regression approach, such as regularization and non-linear
    regression.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分数并不十分令人印象深刻。我们没有解释掉我们希望解释的那么多方差。R-squared分数在三个折叠中平均约为0.62。这部分的理由是每个折叠的测试DataFrame相当小，每个大约有40个观测值。尽管如此，我们应该探索对经典线性回归方法的修改，例如正则化和非线性回归。
- en: One advantage of regularization is that we may be able to get similar results
    without going through a computationally expensive feature selection process. Regularization
    can also help us avoid overfitting. We will explore lasso regression with the
    same data in the next section. We will also look into non-linear regression strategies.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的一个优点是，我们可能在不需要经过计算成本高昂的特征选择过程的情况下获得类似的结果。正则化还可以帮助我们避免过拟合。在下一节中，我们将使用相同的数据探索lasso回归。我们还将研究非线性回归策略。
- en: Using lasso regression
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用lasso回归
- en: A key characteristic of OLS is that it produces the parameter estimates with
    the least bias. However, OLS estimates may have a higher variance than we want.
    We need to be careful about overfitting when we use a classical linear regression
    model. One strategy to reduce the likelihood of overfitting is to use regularization.
    Regularization may also allow us to combine feature selection and model training.
    This may matter for datasets with a large number of features or observations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: OLS的关键特征是它以最小的偏差产生参数估计。然而，OLS估计可能比我们想要的方差更高。当我们使用经典线性回归模型时，我们需要小心过拟合。减少过拟合可能性的一个策略是使用正则化。正则化还可能允许我们将特征选择和模型训练结合起来。这对于具有大量特征或观测值的数据集可能很重要。
- en: Whereas OLS minimizes mean squared error, regularization techniques seek both
    minimal error and a reduced number of features. Lasso regression, which we explore
    in this section, uses L1 regularization, which penalizes the absolute value of
    the coefficients. Ridge regression is similar. It uses L2 regularization, which
    penalizes the squared values of the coefficients. Elastic net regression uses
    both L1 and L2 regularization.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与OLS最小化均方误差不同，正则化技术寻求最小误差和减少特征数量。我们在本节中探讨的lasso回归使用L1正则化，它惩罚系数的绝对值。岭回归类似。它使用L2正则化，惩罚系数的平方值。弹性网络回归使用L1和L2正则化。
- en: 'Once again, we will work with the gasoline tax data from the previous section:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们将使用上一节中的汽油税数据：
- en: 'We will start by importing the same libraries as in the previous section, except
    we will import the `Lasso` module rather than the `linearregression` module:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入与上一节相同的库，除了我们将导入`Lasso`模块而不是`linearregression`模块：
- en: '[PRE17]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will also need the `OutlierTrans` class that we created:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要我们创建的`OutlierTrans`类：
- en: '[PRE18]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let’s load the gasoline tax data and create testing and training DataFrames:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们加载汽油税数据并创建测试和训练DataFrame：
- en: '[PRE19]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We also need to set up the column transformations:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要设置列转换：
- en: '[PRE20]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we are ready to fit our model. We will start with a fairly conservative
    alpha of 0.1\. The higher the alpha, the greater the penalties for our coefficients.
    At 0, we get the same results as with linear regression. In addition to column
    transformation and lasso regression, our pipeline uses KNN imputation for missing
    values. We will also use the target transformer to scale the gasoline tax target.
    We will pass the pipeline we just created to the regressor parameter of the target
    transformer before we fit it:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好拟合我们的模型。我们将从一个相当保守的alpha值0.1开始。alpha值越高，我们的系数惩罚就越大。在0时，我们得到与线性回归相同的结果。除了列转换和lasso回归之外，我们的管道还使用KNN插补缺失值。我们还将使用目标转换器来缩放汽油税目标。在我们拟合之前，我们将刚刚创建的管道传递给目标转换器的回归器参数：
- en: '[PRE21]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s take a look at the coefficients from lasso regression. If we compare
    them to the coefficients from linear regression in the previous section, we notice
    that we end up selecting the same features. Those features that were eliminated
    with recursive feature selection are largely the same ones that get near zero
    values with lasso regression:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看 lasso 回归的系数。如果我们将它们与上一节中线性回归的系数进行比较，我们会注意到我们最终选择了相同的特征。那些在递归特征选择中被消除的特征，在很大程度上与
    lasso 回归中得到接近零值的特征相同：
- en: '[PRE22]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s look at the predictions and residuals for this model. The residuals look
    decent, with little bias and not much skew:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看这个模型的预测值和残差。残差看起来相当不错，几乎没有偏差，也没有很大的偏斜：
- en: '[PRE23]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s also generate the mean absolute error and r-squared. These are not impressive
    scores. The r-squared is lower than with linear regression, but the mean absolute
    error is about the same:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也生成平均绝对误差和 r 平方。这些分数并不令人印象深刻。r 平方低于线性回归，但平均绝对误差大致相同：
- en: '[PRE24]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We should look at a histogram of the residuals. The distribution of the residuals
    is quite similar to the linear regression model:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该查看残差直方图。残差的分布与线性回归模型相当相似：
- en: '[PRE25]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This produces the following plot:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 7.4 – Gas tax model residuals ](img/B17978_07_004.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 汽油税模型残差](img/B17978_07_004.jpg)'
- en: Figure 7.4 – Gas tax model residuals
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 汽油税模型残差
- en: 'Let’s also look at a scatter plot of the predicted values on the residuals.
    Our model is likely to over-predict at the lower ranges and under-predict at the
    upper ranges. This is a change from the linear model, where we consistently over
    predicted at both extremes:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也看看预测值与残差的散点图。我们的模型可能在较低范围内高估，在较高范围内低估。这与线性模型不同，线性模型在两个极端都持续高估：
- en: '[PRE26]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This produces the following plot:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 7.5 – Scatter plot of predictions against residuals ](img/B17978_07_005.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 预测值与残差的散点图](img/B17978_07_005.jpg)'
- en: Figure 7.5 – Scatter plot of predictions against residuals
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 预测值与残差的散点图
- en: 'We will conclude by performing k-fold cross-validation on the model. The scores
    are lower than but close to those of the linear regression model:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过在模型上执行 k 折交叉验证来结束。这些分数低于线性回归模型的分数，但接近：
- en: '[PRE27]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This gives us a model that is not any better than our original model, but it
    at least handles the feature selection process more efficiently. It is also possible
    that we could get better results if we tried different values for the alpha hyperparameter.
    Why not 0.05 or 1.0 instead? We will try to answer that in the next two steps.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了一个模型，它并不比我们的原始模型更好，但它至少更有效地处理了特征选择过程。也有可能如果我们尝试不同的 alpha 超参数值，我们可能会得到更好的结果。为什么不试试
    0.05 或 1.0 呢？我们将在接下来的两个步骤中尝试回答这个问题。
- en: Tuning hyperparameters with grid searches
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用网格搜索调整超参数
- en: Figuring out the best value for a hyperparameter, such as the alpha value in
    the previous example, is known as hyperparameter tuning. One tool in scikit-learn
    for hyperparameter tuning is `GridSearchCV`. The CV suffix is for cross-validate.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 确定超参数的最佳值，例如前一个例子中的 alpha 值，被称为超参数调整。scikit-learn 中用于超参数调整的一个工具是 `GridSearchCV`。CV
    后缀表示交叉验证。
- en: 'Using `GridSearchCV` is very straightforward. If we already have a pipeline,
    as we do in this case, we pass it to a `GridSearchCV` object, along with a dictionary
    of parameters. `GridSearchCV` will try all combinations of parameters and return
    the best one. Let’s try it on our lasso regression model:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `GridSearchCV` 非常简单。如果我们已经有了管道，就像我们在这个例子中做的那样，我们将它传递给一个 `GridSearchCV` 对象，以及一个参数字典。`GridSearchCV`
    将尝试所有参数组合，并返回最佳组合。让我们在我们的 lasso 回归模型上试一试：
- en: 'First, we will instantiate a `lasso` object and create a dictionary with the
    hyperparameters to be tuned. The dictionary, `lasso_params`, indicates that we
    want to try all the alpha values between 0.05 and 0.9 at 0.5 intervals. We cannot
    choose any name we want for the dictionary key. `regressor__lasso__alpha` is based
    on the names of the steps in the pipeline. Also, notice that we are using double
    underscores. Single underscores will return an error:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将实例化一个 `lasso` 对象，并创建一个包含要调整的超参数的字典。这个字典 `lasso_params` 表示我们想要尝试 0.05 和
    0.9 之间的所有 alpha 值，以 0.5 的间隔。我们无法为字典键选择任何想要的名称。`regressor__lasso__alpha` 是基于管道中步骤的名称。注意，我们正在使用双下划线。单下划线将返回错误：
- en: '[PRE28]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we can run the grid search. We will pass the pipeline, which is a `TransformedTargetRegressor`
    in this case, and the dictionary to `GridSearchCV`. The `best_params_` attribute
    indicates that the best alpha is `0.05`. When we use that value, we get an r-squared
    of `0.60`:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以运行网格搜索。我们将传递管道，在这个案例中是一个 `TransformedTargetRegressor`，以及字典到 `GridSearchCV`。`best_params_`
    属性表明最佳 alpha 值为 `0.05`。当我们使用该值时，我们得到一个 r-squared 值为 `0.60`：
- en: '[PRE29]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The lasso regression model comes close to but does not do quite as well as the
    linear model in terms of mean absolute error and r-squared. One benefit of lasso
    regression is that we do not need to do a separate feature selection step before
    training our model. (Recall that for wrapper feature selection methods, the model
    needs to be trained during feature selection as well as after, as we discussed
    in [*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature Selection*.)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso 回归模型在平均绝对误差和 r-squared 方面接近线性模型，但并不完全一样。Lasso 回归的一个优点是，在训练我们的模型之前，我们不需要进行单独的特征选择步骤。（回想一下，对于包装特征选择方法，模型需要在特征选择期间以及之后进行训练，正如我们在
    [*第 5 章*](B17978_05_ePub.xhtml#_idTextAnchor058) 中讨论的，*特征选择*。）
- en: Using non-linear regression
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用非线性回归
- en: 'Linear regression assumes that the relationship of a feature to the target
    is constant across the range of the feature. You may recall that the simple linear
    regression equation that we discussed at the beginning of this chapter had one
    slope estimate for each feature:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归假设特征与目标之间的关系在特征的范围内是恒定的。你可能还记得，我们在本章开头讨论的简单线性回归方程为每个特征有一个斜率估计：
- en: '![](img/B17978_07_010.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B17978_07_010.jpg]'
- en: Here, *y* is the target, each *x* is a feature, and each β is a coefficient
    (or the intercept). If the true relationships between features and targets are
    nonlinear, our model will likely perform poorly.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y* 是目标，每个 *x* 是一个特征，每个 β 是一个系数（或截距）。如果特征与目标之间的真实关系是非线性的，我们的模型可能表现不佳。
- en: Fortunately, we can still make good use of OLS when we cannot assume a linear
    relationship between the features and the target. We can use the same linear regression
    algorithm that we used in the previous section, but with a polynomial transformation
    of the features. This is referred to as polynomial regression.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，当我们无法假设特征与目标之间存在线性关系时，我们仍然可以很好地利用 OLS。我们可以使用与上一节相同的线性回归算法，但使用特征的多项式变换。这被称为多项式回归。
- en: 'We add a power to the feature to run a polynomial regression. This gives us
    the following equation:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给特征添加一个幂来运行多项式回归。这给我们以下方程：
- en: '![](img/B17978_07_011.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B17978_07_011.jpg]'
- en: 'The following plot compares predicted values for linear versus polynomial regression.
    The polynomial curve seems to fit the fictional data points better than the linear
    regression line:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图比较了线性回归与多项式回归的预测值。多项式曲线似乎比线性回归线更好地拟合了虚构的数据点：
- en: '![Figure 7.6 – Illustration of the polynomial equation curve and linear equation
    line ](img/B17978_07_006.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 多项式方程曲线和线性方程线的示意图](img/B17978_07_006.jpg)'
- en: Figure 7.6 – Illustration of the polynomial equation curve and linear equation
    line
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 多项式方程曲线和线性方程线的示意图
- en: 'In this section, we will experiment with both a linear model and a non-linear
    model of average annual temperatures at weather stations across the world. We
    will use latitude and elevation as features. First, we will predict temperature
    using multiple linear regression, and then try a model with polynomial transformations.
    Follow these steps:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对全球气象站平均年温度的线性模型和非线性模型进行实验。我们将使用纬度和海拔作为特征。首先，我们将使用多元线性回归来预测温度，然后尝试使用多项式变换的模型。按照以下步骤进行：
- en: 'We will start by importing the necessary libraries. These libraries will be
    familiar if you have been working through this chapter:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入必要的库。如果你一直在本章中工作，这些库将很熟悉：
- en: '[PRE30]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We also need to import the module that contains our class for identifying outlier
    values:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要导入包含我们用于识别异常值类别的模块：
- en: '[PRE31]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We load the land temperature data, identify the features we want, and generate
    some descriptive statistics. There are a few missing values for elevation and
    some extreme negative values for average annual temperature. The range of values
    for the target and features is very different, so we will probably want to scale
    our data:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载陆地温度数据，识别我们想要的特征，并生成一些描述性统计。对于海拔高度有一些缺失值，对于平均年温度有一些极端的负值。目标和特征值的范围差异很大，所以我们可能需要缩放我们的数据：
- en: '[PRE32]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we create the training and testing DataFrames:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建训练和测试数据框：
- en: '[PRE33]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we build a pipeline to handle our pre-processing – set outlier values
    to missing, do KNN imputation for all missing values, and scale the features –
    and then run a linear model. We do k-fold cross-validation with 10 folds and get
    an average r-squared of 0.79 and a mean absolute error of -2.8:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们构建一个管道来处理我们的预处理 – 将异常值设置为缺失，对所有缺失值进行KNN插补，并对特征进行缩放 – 然后运行线性模型。我们进行10折交叉验证，得到平均r-squared为0.79，平均绝对误差为-2.8：
- en: '[PRE34]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Notice that we are quite conservative with our identification of outliers. We
    pass a threshold value of 3, meaning that a value needs to be more than three
    times the interquartile range above or below that range. Obviously, we would typically
    give much more thought to the identification of outliers. We demonstrate here
    only how to handle outliers in a pipeline once we have decided that that makes
    sense.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在识别异常值方面非常保守。我们传递了一个阈值为3，这意味着一个值需要比四分位数范围高或低三倍以上。显然，我们通常会更多地考虑异常值的识别。在这里，我们只演示了在决定这样做是有意义之后，如何在管道中处理异常值。
- en: 'Let’s see the predictions and the residuals. There is almost no bias overall
    (the average of the residuals is 0), but there is some negative skew:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看预测值和残差。总体上几乎没有偏差（残差的平均值是0），但有一些负偏斜：
- en: '[PRE35]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It is easy to see this skew if we create a histogram of the residuals. There
    are some extreme negative residuals – that is, times when we over-predict the
    average temperature by a lot:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们创建残差的直方图，很容易看到这种偏斜。有一些极端的负残差 – 即我们过度预测平均温度的次数：
- en: '[PRE36]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This produces the following plot:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 7.7 – Temperature model residuals ](img/B17978_07_007.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 温度模型残差](img/B17978_07_007.jpg)'
- en: Figure 7.7 – Temperature model residuals
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 温度模型残差
- en: 'It can also be helpful to plot the predicted values against the residuals:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 也可以通过绘制预测值与残差的关系图来有所帮助：
- en: '[PRE37]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This produces the following plot:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 7.8 – Scatter plot of predictions against residuals ](img/B17978_07_0081.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 预测值与残差的散点图](img/B17978_07_0081.jpg)'
- en: Figure 7.8 – Scatter plot of predictions against residuals
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 预测值与残差的散点图
- en: Our model over-predicts all predictions above approximately 28 degrees Celsius.
    It is also likely to underpredict when it predicts values between 18 and 28.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在预测大约28摄氏度以上的所有预测值时都会过度预测。它也可能会在预测18到28之间的值时低估。
- en: 'Let’s see if we get any better results with polynomial regression:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看多项式回归是否能得到更好的结果：
- en: 'First, we will create a `PolynomialFeatures` object with a `degree` of `4`
    and fit it. We can pass the original feature names to the `get_feature_names`
    method to get the names of the columns that will be returned after the transformation.
    Second, third, and fourth power values of each feature are created, as well as
    interaction effects between variables (such as `latabs` * `elevation`). We do
    not need to run the fit here since that will happen in the pipeline. We are just
    doing it here to get a feel for how it works:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个`PolynomialFeatures`对象，其`degree`为`4`，并进行拟合。我们可以通过传递原始特征名称给`get_feature_names`方法来获取变换后返回的列名。每个特征的二次、三次和四次幂值被创建，以及变量之间的交互效应（例如`latabs`
    * `elevation`）。在这里我们不需要运行拟合，因为那将在管道中发生。我们只是在这里做这个来了解它是如何工作的：
- en: '[PRE38]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we will create a pipeline for our polynomial regression. The pipeline
    is pretty much the same as with linear regression, except we insert the polynomial
    transformation step after the KNN imputation:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将为多项式回归创建一个管道。这个管道基本上与线性回归相同，只是在KNN插补之后插入多项式变换步骤：
- en: '[PRE39]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, let’s create predictions and residuals based on the polynomial model.
    There is a little less skew in the residuals than with the linear model:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们基于多项式模型创建预测值和残差。与线性模型相比，残差中的偏斜度要小一些：
- en: '[PRE40]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We should take a look at a histogram of the residuals:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该看一下残差的直方图：
- en: '[PRE41]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This produces the following plot:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 7.9 – Temperature model residuals ](img/B17978_07_009.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 温度模型残差](img/B17978_07_009.jpg)'
- en: Figure 7.9 – Temperature model residuals
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 温度模型残差
- en: 'Let’s also do another scatter plot of predicted values against residuals. These
    look a little bit better than the residuals with the linear model, particularly
    at the upper ranges of the predictions:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也做一个预测值与残差的散点图。这些看起来比线性模型的残差要好一些，尤其是在预测的上限范围内：
- en: '[PRE42]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This produces the following plot:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 7.10 – Scatter plot of predictions against residuals ](img/B17978_07_0101.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 预测值与残差的散点图](img/B17978_07_0101.jpg)'
- en: Figure 7.10 – Scatter plot of predictions against residuals
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 预测值与残差的散点图
- en: 'Let’s do k-fold cross-validation once again and take the average r-squared
    value across the folds. There are improvements in both the r-squared and mean
    absolute error compared to the linear model:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次进行k折交叉验证，并取各折的平均r-squared值。与线性模型相比，r-squared和平均绝对误差都有所提高：
- en: '[PRE43]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The polynomial transformation improved our overall results, particularly within
    certain ranges of our predictors. The residuals at high temperatures were noticeably
    lower. It is often a good idea to try a polynomial transformation when our residuals
    suggest that there might be a nonlinear relationship between our features and
    our target.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式变换提高了我们的整体结果，尤其是在预测变量的某些范围内。高温下的残差明显较低。当我们的残差表明特征和目标之间可能存在非线性关系时，尝试多项式变换通常是一个好主意。
- en: Regression with gradient descent
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降回归
- en: 'Gradient descent can be a good alternative to ordinary least squares for optimizing
    the loss function of a linear model. This is particularly true when working with
    very large datasets. In this section, we will use gradient descent with the land
    temperatures dataset, mainly to demonstrate how to use it and to give us another
    opportunity to explore exhaustive grid searches. Let’s get started:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降可以是有序最小二乘法优化线性模型损失函数的一个很好的替代方案。这在处理非常大的数据集时尤其正确。在本节中，我们将使用梯度下降和陆地温度数据集，主要为了演示如何使用它，并给我们另一个机会探索穷举网格搜索。让我们开始吧：
- en: 'First, we will load the same libraries we have been working with so far, plus
    the stochastic gradient descent regressor from scikit-learn:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将加载我们迄今为止一直在使用的相同库，以及来自scikit-learn的随机梯度下降回归器：
- en: '[PRE44]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we will load the land temperatures dataset again and create training
    and testing DataFrames:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将再次加载陆地温度数据集并创建训练和测试数据框：
- en: '[PRE45]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we will set up a pipeline to deal with outliers, impute values for missings,
    and scale the data before running the gradient descent:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设置一个管道来处理异常值，在运行梯度下降之前填充缺失值并缩放数据：
- en: '[PRE46]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Now, we need to create a dictionary to indicate the hyperparameters we want
    to tune and the values to try. We want to try values for alpha, the loss function,
    epsilon, and the penalty. We will give each key in the dictionary a prefix of
    `regressor__sgdregressor__` because that is where the stochastic gradient descent
    regressor can be found in the pipeline.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个字典来指示我们想要调整的超参数和要尝试的值。我们想要尝试alpha、损失函数、epsilon和惩罚的值。我们将为字典中的每个键添加前缀`regressor__sgdregressor__`，因为随机梯度下降回归器可以在管道中找到。
- en: The `alpha` parameter determines the size of the penalty. The default is `0.0001`.
    We can choose L1, L2, or elastic net regularization. We will select `huber` and
    `epsilon_insensitive` as loss functions to include in the search. The default
    loss function is `squared_error`, but that would just give us ordinary least squares
    again.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`alpha`参数决定了惩罚的大小。默认值是`0.0001`。我们可以选择L1、L2或弹性网络正则化。我们将选择`huber`和`epsilon_insensitive`作为要包含在搜索中的损失函数。默认损失函数是`squared_error`，但这只会给我们普通的有序最小二乘法。'
- en: 'The `huber` loss function is less sensitive to outliers than is OLS. How sensitive
    it is, is based on the value of epsilon we specify. With the `epsilon_insensitive`
    loss function, errors within a given range (epsilon) are not penalized (we will
    construct models with epsilon-insensitive tubes in the next chapter, where we’ll
    examine support vector regression):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`huber`损失函数对异常值不如OLS敏感。它的敏感度取决于我们指定的epsilon的值。使用`epsilon_insensitive`损失函数，给定范围（epsilon）内的错误不会受到惩罚（我们将在下一章构建具有epsilon-insensitive管的模型，我们将检查支持向量回归）：'
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, we are ready to run an exhaustive grid search. The best parameters are
    `0.001` for alpha, `1.3` for epsilon, `huber` for the loss function, and elastic
    net regularization:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好运行一个全面的网格搜索。最佳参数是alpha为`0.001`，epsilon为`1.3`，损失函数为`huber`，正则化为弹性网络：
- en: '[PRE48]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'I usually find it helpful to look at the hyperparameters for some of the other
    grid search iterations. Huber loss models, with either elastic net or L2 regularization,
    perform best:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我通常发现查看一些其他网格搜索迭代的超参数很有帮助。具有弹性网络或L2正则化的Huber损失模型表现最佳：
- en: '[PRE49]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Stochastic gradient descent is a generalized approach to optimization and can
    be applied to many machine learning problems. It is often quite efficient, as
    we can see here. We were able to run an exhaustive grid search on penalty, penalty
    size, epsilon, and loss function fairly quickly.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降是一种通用的优化方法，可以应用于许多机器学习问题。它通常非常高效，正如我们在这里所看到的。我们能够相当快地运行一个关于惩罚、惩罚大小、epsilon和损失函数的全面网格搜索。
- en: Summary
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter allowed us to explore a very well-known machine learning algorithm:
    linear regression. We examined the qualities of a feature space that makes it
    a good candidate for a linear model. We also explored how to improve a linear
    model, when necessary, with regularization and with transformations. Then, we
    looked at stochastic gradient descent as an alternative to OLS optimization. We
    also learned how to add our own classes to a pipeline and how to do hyperparameter
    tuning.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使我们能够探索一个非常著名的机器学习算法：线性回归。我们考察了使特征空间成为线性模型良好候选者的特性。我们还探讨了在必要时如何通过正则化和变换来改进线性模型。然后，我们研究了随机梯度下降作为OLS优化的替代方案。我们还学习了如何将我们自己的类添加到管道中以及如何进行超参数调整。
- en: In the next chapter, we will explore support vector regression.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索支持向量回归。
