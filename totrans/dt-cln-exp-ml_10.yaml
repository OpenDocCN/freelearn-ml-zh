- en: '*Chapter 7*: Linear Regression Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is perhaps the most well-known machine learning algorithm,
    having origins in statistical learning at least 200 years ago. If you took a statistics,
    econometrics, or psychometrics course in college, you were likely introduced to
    linear regression, even if you took that course long before machine learning was
    taught in undergraduate courses. As it turns out, many social and physical phenomena
    can be successfully modeled as a function of a linear combination of predictor
    variables. This is as useful for machine learning as it has been for statistical
    learning all these years, though, with machine learning, we care much less about
    the parameter values than we do about predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is a very good choice for modeling a continuous target, assuming
    that our features and target have certain qualities. In this chapter, we will
    go over the assumptions of linear regression models and construct a model using
    data that is largely consistent with these assumptions. However, we will also
    explore alternative approaches, such as nonlinear regression, which we use when
    these assumptions do not hold. We will conclude this chapter by looking at techniques
    that address the possibility of overfitting, such as lasso regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression and gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using classical linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using lasso regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using non-linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression with gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will stick to the libraries that are available with most
    scientific distributions of Python – NumPy, pandas, and scikit-learn. The code
    for this chapter can be found in this book’s GitHub repository at [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning).
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The typical analyst who has been doing predictive modeling for a while has constructed
    tens, perhaps hundreds, of linear regression models over the years. If you worked
    for a large accounting firm in the late 1980s, as I did, and you were doing forecasting,
    you may have spent your whole day, every day, specifying linear models. You would
    have run all conceivable permutations of independent variables and transformations
    of dependent variables, and diligently looked for evidence of heteroscedasticity
    (non-constant variance in residuals) or multicollinearity (highly correlated features).
    But most of all, you worked hard to identify key predictor variables and address
    any bias in your parameter estimates (your coefficients or weights).
  prefs: []
  type: TYPE_NORMAL
- en: Key assumptions of linear regression models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Much of that effort still applies today, though there is now much more emphasis
    on the accuracy of predictions than on parameter estimates. We worry about overfitting
    now, in a way that we did not 30 years ago. We are also more likely to seek alternatives
    when the assumptions of linear regression models are violated. These assumptions
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: There there is a linear relationship between features (independent variables)
    and the target (dependent variable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That the residuals (the difference between actual and predicted values) are
    normally distributed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That the residuals are independent across observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That the variance of residuals is constant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not unusual for one or more of these assumptions to be violated with real-world
    data. The relationship between a feature and target is often not linear. The influence
    of the feature may vary across the range of that feature. Anyone familiar with
    the expression “*too many cooks in the kitchen*” likely appreciates that the marginal
    increase in productivity with the fifth cook may not be as great as with the second
    or third.
  prefs: []
  type: TYPE_NORMAL
- en: Our residuals are sometimes not normally distributed. This can indicate that
    our model is less accurate along certain ranges of our target. For example, it
    is not unusual to have smaller residuals along the middle of the target’s range,
    say the 25th to 75th percentile, and higher residuals at the extremes. This can
    happen when the relationship with the target is nonlinear.
  prefs: []
  type: TYPE_NORMAL
- en: There are several reasons why residuals may not be independent. This is often
    the case with time series data. For a model of daily stock price, the residuals
    may be correlated for adjacent days. This is referred to as autocorrelation. This
    can also be a problem with longitudinal or repeated measures data. For example,
    we may have test scores for 600 students in 20 different classrooms or annual
    wage income for 100 people. Our residuals would not be independent if our model
    failed to account for there being no variation in some features across a group
    – the classroom-determined and person-determined features in these examples.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is not uncommon for our residuals to have greater variability along
    different ranges of a feature. If we are predicting temperatures at weather stations
    around the world, and latitude is one of the features we are using, there is a
    chance that there will be greater residuals at higher latitude values. This is
    known as heteroscedasticity. This may also be an indicator that our model has
    omitted important predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond these four key assumptions, another common challenge with linear regression
    is the high correlation among features. This is known as multicollinearity. As
    we discussed in [*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature
    Selection*, we likely increase the risk of overfitting when our model struggles
    to isolate the independent effect of a particular feature because it moves so
    much with another feature. This will be familiar to any of you who have spent
    weeks building a model where the coefficients shift dramatically with each new
    specification.
  prefs: []
  type: TYPE_NORMAL
- en: When one or more of these assumptions is violated, we may still be able to use
    a traditional regression model. However, we may need to transform the data in
    some way. We will discuss techniques for identifying violations of these assumptions,
    the implications of those violations for model performance, and possible ways
    to address these issues throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression and ordinary least squares
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most common estimation technique for linear regression is **ordinary least
    squares** (**OLS**). OLS selects coefficients that minimize the sum of the squared
    distance between the actual target values and the predicted values. More precisely,
    OLS minimizes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17978_07_002.png) is the actual value at the ith observation
    and ![](img/B17978_07_003.png) is the predicted value. As we have discussed, the
    difference between the actual target value and the predicted target value, ![](img/B17978_07_004.png),
    is known as the residual.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graphically, OLS fits a line through our data that minimizes the vertical distance
    of data points from that line. The following plot illustrates a model with one
    feature, known as simple linear regression, with made-up data points. The vertical
    distance between each data point and the regression line is the residual, which
    can be positive or negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Ordinary least squares regression line ](img/B17978_07_0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Ordinary least squares regression line
  prefs: []
  type: TYPE_NORMAL
- en: 'The line,![](img/B17978_07_005.png), gives us the predicted value of *y* for
    each value of *x*. It is equal to the estimated intercept, ![](img/B17978_07_006.png),
    plus the estimated coefficient for the feature times the feature value, ![](img/B17978_07_007.png).
    This is the OLS line. Any other straight line through the data would result in
    a higher sum of squared residuals. This can be extended to multiple linear regression
    models – that is, those with more than one feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* is the target, each *x* is a feature, each ![](img/B17978_07_009.png)
    is a coefficient (or the intercept), *n* is the number of features, and *ɛ* is
    an error term. Each coefficient is the estimated change in the target from a 1-unit
    change in the associated feature. This is a good place to notice that the coefficient
    is constant across the whole range of each feature; that is, an increase in the
    feature from 0 to 1 is assumed to have the same impact on the target as from 999
    to 1000\. However, this does not always make sense. Later in this chapter, we
    will discuss how to use transformations when the relationship between a feature
    and the target is not linear.
  prefs: []
  type: TYPE_NORMAL
- en: An important advantage of linear regression is that it is not as computationally
    expensive as other supervised regression algorithms. When linear regression performs
    well, based on metrics such as those we discussed in the previous chapter, it
    is a good choice. This is particularly true when you have large amounts of data
    to train or your business process does not permit large blocks of time for model
    training. The efficiency of the algorithm can also make it feasible to use more
    resource-intensive feature selection techniques, such as wrapper methods, which
    we discussed in [*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature
    Selection*. As we saw there, you may not want to use exhaustive feature selection
    with a decision tree regressor. However, it may be perfectly fine with a linear
    regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression and gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use gradient descent, rather than ordinary least squares, to estimate
    our linear regression parameters. Gradient descent iterates over possible coefficient
    values to find those that minimize the residual sum of squares. It starts with
    random coefficient values and calculates the sum of squared errors for that iteration.
    Then, it generates new values for coefficients that yield smaller residuals than
    those from the previous step. We specify a learning rate when using gradient descent.
    The learning rate determines the amount of improvement in residuals at each step.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent can often be a good choice when working with very large datasets.
    It may be the only choice if the full dataset does not fit into your machine’s
    memory. We will use both OLS and gradient descent to estimate our parameters in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using classical linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will specify a fairly straightforward linear model. We will
    use it to predict the implied gasoline tax of a country based on several national
    economic and political measures. But before we specify our model, we need to do
    the pre-processing tasks we discussed in the first few chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing the data for our regression model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use pipelines to pre-process our data in this chapter, and throughout
    the rest of this book. We need to impute values where they are missing, identify
    and handle outliers, and encode and scale our data. We also need to do this in
    a way that avoids data leakage and cleans the training data without peeking ahead
    to the testing data. As we saw in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078),
    *Preparing for Model Evaluation*, scikit-learn’s pipelines can help with these
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we will use contains the implied gasoline tax for each country and
    some possible predictors, including national income per capita, government debt,
    fuel income dependency, extent of car use, and measures of democratic processes
    and government effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This dataset on implied gasoline tax by country is available for public use
    on the Harvard Dataverse at [https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RX4JGK](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RX4JGK).
    It was compiled by *Paasha Mahdavi*, *Cesar B. Martinez-Alvarez*, and *Michael
    L. Ross*. The implied gasoline tax is calculated based on the difference between
    the world benchmark price and the local price for a liter of gas. A local price
    above the benchmark price represents a tax. When the benchmark price is higher,
    it can be considered a subsidy. We will use 2014 data for each country for this
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by pre-processing the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load many of the libraries we worked with in the last few chapters.
    However, we also need two new libraries to build the pipeline for our data – `ColumnTransformer`
    and `TransformedTargetRegressor`. These libraries allow us to build a pipeline
    that does different pre-processing on numerical and categorical features, and
    that also transforms our target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can extend the functionality of a scikit-learn pipeline by adding our own
    classes. Let’s add a class to handle extreme values called `OutlierTrans`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To include this class in a pipeline, it must inherit from the `BaseEstimator`
    class. We must also inherit from `TransformerMixin`, though there are other possibilities.
    Our class needs the `fit` and `transform` methods. We can put code for assigning
    extreme values as missing in the `transform` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before we can use our class, we need to import it. To import it, we need
    to append the `helperfunctions` subfolder, since that is where we have placed
    the `preprocfunc` module that contains our class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This imports the `OutlierTrans` class, which we can add to the pipelines we
    create:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `OutlierTrans` class uses a fairly standard univariate approach for identifying
    an outlier. It calculates the **interquartile range** (**IQR**) for each feature,
    then sets any value that is more than 1.5 times the IQR above the third quartile
    or below the first quartile to missing. We can change the threshold to something
    other than 1.5, such as 2.0, if we want to be more conservative. (We discussed
    this technique for identifying outliers in [*Chapter 1*](B17978_01_ePub.xhtml#_idTextAnchor014),
    *Examining the Distribution of Features and Targets*.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we load the gasoline tax data for 2014\. There are 154 rows – one for
    each country in the DataFrame. A few features have some missing values, but only
    one, `motorization_rate`, has double-digit missings. `motorization_rate` is the
    number of cars per person:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s separate the features into numerical and binary features. We will put
    `motorization_rate` into a special category because we anticipate having to do
    a little more with it than with the other features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should look at some descriptives for the numeric features and the target.
    Our target, `gas_tax_imp`, has a median value of 0.52\. Notice that some of the
    features have a very different range. More than half of the countries have a `polity`
    score of 7 or higher; 10 is the highest possible `polity` score, meaning most
    democratic. Most countries have a negative value for government effectiveness.
    `democracy_index` is a very similar measure to `polity`, though there is more
    variation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s also look at the distribution of the binary features. We must set `normalize`
    to `True` to generate ratios rather than counts. The `democracy_polity` and `autocracy_polity`
    features are just binarized versions of the `polity` feature; very high `polity`
    scores get `democracy_polity` values of 1, while very low `polity` scores get
    `autocracy_polity` values of 1\. Similarly, `democracy` is a dummy feature for
    those countries with high `democracy_index` values. Interestingly, nearly half
    of the countries (0.46) have a national oil company, and almost a quarter (0.23)
    have a state-owned national oil company:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This all looks to be in fairly good shape. However, we will need to do some
    work on the missing values for several features. We also need to do some scaling,
    but there is no need to do any encoding because we can use the binary features
    as they are. Some features are correlated, so we need to do some feature elimination.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin our pre-processing by creating training and testing DataFrames. We
    will only reserve 20% for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We need to build a pipeline with column transformations so that we can do different
    pre-processing on numeric and categorical data. We will construct a pipeline,
    `standtrans`, for all of the numeric columns in `num_cols`. First, we want to
    set outliers to missing. We will define an outlier value as one that is more than
    two times the interquartile range above the third quartile, or below the first
    quartile, for that feature. We will use `SimpleImputer` to set missing values
    to the median.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We do not want to scale the binary features in `dummy_cols`, but we do want
    to use `SimpleImputer` to set missing values to the most frequent value for each
    categorical feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'We won’t use `SimpleImputer` for `motorization_rate`. Remember that `motorization_rate`
    is not in the `num_cols` list – it is in the `spec_cols` list. We set up a special
    pipeline, `spectrans`, for features in `spec_cols`. We will use `motorization_rate`
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This sets up all of the pre-processing we want to do on the gasoline tax data.
    To do the transformations, all we need to do is call the `fit` method of the column
    transformer. However, we will not do that yet because we also want to add feature
    selection to the pipeline and get it to run a linear regression. We will do that
    in the next few steps.
  prefs: []
  type: TYPE_NORMAL
- en: Running and evaluating our linear model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use **recursive feature elimination** (**RFE**) to select features for
    our model. RFE has the advantages of wrapper feature selection methods – it evaluates
    features based on a selected algorithm, and it considers multivariate relationships
    in that assessment. However, it can also be computationally expensive. Since we
    do not have many features or observations, that is not much of a problem in this
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'After selecting the features, we run a linear regression model and take a look
    at our predictions. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we create linear regression and recursive feature elimination instances
    and add them to the pipeline. We also create a `TransformedTargetRegressor` object
    since we still need to transform the target. We pass our pipeline to the regressor
    parameter of `TransformedTargetRegressor`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can call the target regressor’s `fit` method. After that, the `support_`
    attribute of the pipeline’s `rfe` step will give us the selected features. Similarly,
    we can get the coefficients by getting the `coef_` value of the `linearregression`
    step. The key here is that referencing `ttr.regressor` gets us to the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Our feature selection identified the VAT rate, government debt, a measure of
    government effectiveness (`goveffect`), whether the country is in the autocracy
    category, whether there is a national oil company and one that is state-owned,
    and the motorization rate as the top seven features. The number of features indicated
    is an example of a hyperparameter, and our choice of seven here is fairly arbitrary.
    We will discuss techniques for hyperparameter tuning in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that of the several autocracy/democracy measures in the dataset, the
    one that seems to matter most is the autocracy dummy, which has a value of 1 for
    countries with very low `polity` scores. It is estimated to have a negative effect
    gasoline taxes; that is, to reduce them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the predictions and the residuals. We can pass the features
    from the testing data to the transformer’s/pipeline’s `predict` method to generate
    the predictions. There is a little positive skew and some overall bias; the residuals
    are negative overall:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s also generate some overall model evaluation statistics. We get a mean
    absolute error of `0.23`. That’s not a great average error, given that the median
    value for the gas tax price is `0.52`. The r-squared is decent, however:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is usually helpful to look at a plot of the residuals. Let’s also draw a
    red dashed line at the average value of the residuals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Gas tax model residuals ](img/B17978_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Gas tax model residuals
  prefs: []
  type: TYPE_NORMAL
- en: This plot shows the positive skew. Moreover, our model is somewhat more likely
    to over-predict the gas tax than under-predict it. (The residual is negative when
    the prediction is greater than the actual target value.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also look at a scatterplot of the predictions against the residuals.
    Let’s draw a red dashed line at 0 on the *Y*-axis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Scatter plot of predictions against residuals ](img/B17978_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Scatter plot of predictions against residuals
  prefs: []
  type: TYPE_NORMAL
- en: Here, overprediction occurs throughout the range of predicted values, but there
    are no underpredictions (positive residuals) with predictions below 0 or above
    1\. This should give us some doubts about our assumption of linearity.
  prefs: []
  type: TYPE_NORMAL
- en: Improving our model evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One problem with how we have evaluated our model so far is that we are not making
    great use of the data. We are only training on about 80% of the data. Our metrics
    are also quite dependent on the testing data being representative of the real
    world we want to predict. However, it might not be. We can improve our odds with
    k-fold cross-validation, as we discussed in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have been using pipelines for our analysis, we have already done much
    of the work we need for k-fold cross-validation. Recall from the previous chapter
    that the k-fold model evaluation divides our data into k equal parts. One of the
    folds is designated for testing and the rest for training. This is repeated k
    times, with a different fold being used for testing each time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try k-fold cross-validation with our linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by creating new training and testing DataFrames, leaving just
    10% for later validation. We do not need to retain as much data for validation,
    though it is a good idea to always hold a little back:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to instantiate `KFold` and `LinearRegression` objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to run our k-fold cross validation. We indicate that we want
    both r-squared and mean absolute error for each split. `cross_validate` automatically
    gives us fit and score times for each fold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These scores are not very impressive. We do not end up explaining as much of
    the variance as we would like. R-squared scores average about 0.62 across the
    three folds. This is partly because the testing DataFrames of each fold are quite
    small, with about 40 observations in each. Nonetheless, we should explore modifications
    of the classical linear regression approach, such as regularization and non-linear
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of regularization is that we may be able to get similar results
    without going through a computationally expensive feature selection process. Regularization
    can also help us avoid overfitting. We will explore lasso regression with the
    same data in the next section. We will also look into non-linear regression strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Using lasso regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key characteristic of OLS is that it produces the parameter estimates with
    the least bias. However, OLS estimates may have a higher variance than we want.
    We need to be careful about overfitting when we use a classical linear regression
    model. One strategy to reduce the likelihood of overfitting is to use regularization.
    Regularization may also allow us to combine feature selection and model training.
    This may matter for datasets with a large number of features or observations.
  prefs: []
  type: TYPE_NORMAL
- en: Whereas OLS minimizes mean squared error, regularization techniques seek both
    minimal error and a reduced number of features. Lasso regression, which we explore
    in this section, uses L1 regularization, which penalizes the absolute value of
    the coefficients. Ridge regression is similar. It uses L2 regularization, which
    penalizes the squared values of the coefficients. Elastic net regression uses
    both L1 and L2 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, we will work with the gasoline tax data from the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the same libraries as in the previous section, except
    we will import the `Lasso` module rather than the `linearregression` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will also need the `OutlierTrans` class that we created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s load the gasoline tax data and create testing and training DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to set up the column transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to fit our model. We will start with a fairly conservative
    alpha of 0.1\. The higher the alpha, the greater the penalties for our coefficients.
    At 0, we get the same results as with linear regression. In addition to column
    transformation and lasso regression, our pipeline uses KNN imputation for missing
    values. We will also use the target transformer to scale the gasoline tax target.
    We will pass the pipeline we just created to the regressor parameter of the target
    transformer before we fit it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s take a look at the coefficients from lasso regression. If we compare
    them to the coefficients from linear regression in the previous section, we notice
    that we end up selecting the same features. Those features that were eliminated
    with recursive feature selection are largely the same ones that get near zero
    values with lasso regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at the predictions and residuals for this model. The residuals look
    decent, with little bias and not much skew:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s also generate the mean absolute error and r-squared. These are not impressive
    scores. The r-squared is lower than with linear regression, but the mean absolute
    error is about the same:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should look at a histogram of the residuals. The distribution of the residuals
    is quite similar to the linear regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Gas tax model residuals ](img/B17978_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Gas tax model residuals
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also look at a scatter plot of the predicted values on the residuals.
    Our model is likely to over-predict at the lower ranges and under-predict at the
    upper ranges. This is a change from the linear model, where we consistently over
    predicted at both extremes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Scatter plot of predictions against residuals ](img/B17978_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Scatter plot of predictions against residuals
  prefs: []
  type: TYPE_NORMAL
- en: 'We will conclude by performing k-fold cross-validation on the model. The scores
    are lower than but close to those of the linear regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This gives us a model that is not any better than our original model, but it
    at least handles the feature selection process more efficiently. It is also possible
    that we could get better results if we tried different values for the alpha hyperparameter.
    Why not 0.05 or 1.0 instead? We will try to answer that in the next two steps.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters with grid searches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Figuring out the best value for a hyperparameter, such as the alpha value in
    the previous example, is known as hyperparameter tuning. One tool in scikit-learn
    for hyperparameter tuning is `GridSearchCV`. The CV suffix is for cross-validate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `GridSearchCV` is very straightforward. If we already have a pipeline,
    as we do in this case, we pass it to a `GridSearchCV` object, along with a dictionary
    of parameters. `GridSearchCV` will try all combinations of parameters and return
    the best one. Let’s try it on our lasso regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will instantiate a `lasso` object and create a dictionary with the
    hyperparameters to be tuned. The dictionary, `lasso_params`, indicates that we
    want to try all the alpha values between 0.05 and 0.9 at 0.5 intervals. We cannot
    choose any name we want for the dictionary key. `regressor__lasso__alpha` is based
    on the names of the steps in the pipeline. Also, notice that we are using double
    underscores. Single underscores will return an error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can run the grid search. We will pass the pipeline, which is a `TransformedTargetRegressor`
    in this case, and the dictionary to `GridSearchCV`. The `best_params_` attribute
    indicates that the best alpha is `0.05`. When we use that value, we get an r-squared
    of `0.60`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The lasso regression model comes close to but does not do quite as well as the
    linear model in terms of mean absolute error and r-squared. One benefit of lasso
    regression is that we do not need to do a separate feature selection step before
    training our model. (Recall that for wrapper feature selection methods, the model
    needs to be trained during feature selection as well as after, as we discussed
    in [*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature Selection*.)
  prefs: []
  type: TYPE_NORMAL
- en: Using non-linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression assumes that the relationship of a feature to the target
    is constant across the range of the feature. You may recall that the simple linear
    regression equation that we discussed at the beginning of this chapter had one
    slope estimate for each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* is the target, each *x* is a feature, and each β is a coefficient
    (or the intercept). If the true relationships between features and targets are
    nonlinear, our model will likely perform poorly.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we can still make good use of OLS when we cannot assume a linear
    relationship between the features and the target. We can use the same linear regression
    algorithm that we used in the previous section, but with a polynomial transformation
    of the features. This is referred to as polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'We add a power to the feature to run a polynomial regression. This gives us
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following plot compares predicted values for linear versus polynomial regression.
    The polynomial curve seems to fit the fictional data points better than the linear
    regression line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Illustration of the polynomial equation curve and linear equation
    line ](img/B17978_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Illustration of the polynomial equation curve and linear equation
    line
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will experiment with both a linear model and a non-linear
    model of average annual temperatures at weather stations across the world. We
    will use latitude and elevation as features. First, we will predict temperature
    using multiple linear regression, and then try a model with polynomial transformations.
    Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the necessary libraries. These libraries will be
    familiar if you have been working through this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to import the module that contains our class for identifying outlier
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the land temperature data, identify the features we want, and generate
    some descriptive statistics. There are a few missing values for elevation and
    some extreme negative values for average annual temperature. The range of values
    for the target and features is very different, so we will probably want to scale
    our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create the training and testing DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we build a pipeline to handle our pre-processing – set outlier values
    to missing, do KNN imputation for all missing values, and scale the features –
    and then run a linear model. We do k-fold cross-validation with 10 folds and get
    an average r-squared of 0.79 and a mean absolute error of -2.8:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we are quite conservative with our identification of outliers. We
    pass a threshold value of 3, meaning that a value needs to be more than three
    times the interquartile range above or below that range. Obviously, we would typically
    give much more thought to the identification of outliers. We demonstrate here
    only how to handle outliers in a pipeline once we have decided that that makes
    sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the predictions and the residuals. There is almost no bias overall
    (the average of the residuals is 0), but there is some negative skew:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is easy to see this skew if we create a histogram of the residuals. There
    are some extreme negative residuals – that is, times when we over-predict the
    average temperature by a lot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Temperature model residuals ](img/B17978_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Temperature model residuals
  prefs: []
  type: TYPE_NORMAL
- en: 'It can also be helpful to plot the predicted values against the residuals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Scatter plot of predictions against residuals ](img/B17978_07_0081.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Scatter plot of predictions against residuals
  prefs: []
  type: TYPE_NORMAL
- en: Our model over-predicts all predictions above approximately 28 degrees Celsius.
    It is also likely to underpredict when it predicts values between 18 and 28.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if we get any better results with polynomial regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create a `PolynomialFeatures` object with a `degree` of `4`
    and fit it. We can pass the original feature names to the `get_feature_names`
    method to get the names of the columns that will be returned after the transformation.
    Second, third, and fourth power values of each feature are created, as well as
    interaction effects between variables (such as `latabs` * `elevation`). We do
    not need to run the fit here since that will happen in the pipeline. We are just
    doing it here to get a feel for how it works:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create a pipeline for our polynomial regression. The pipeline
    is pretty much the same as with linear regression, except we insert the polynomial
    transformation step after the KNN imputation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create predictions and residuals based on the polynomial model.
    There is a little less skew in the residuals than with the linear model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should take a look at a histogram of the residuals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Temperature model residuals ](img/B17978_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Temperature model residuals
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also do another scatter plot of predicted values against residuals. These
    look a little bit better than the residuals with the linear model, particularly
    at the upper ranges of the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Scatter plot of predictions against residuals ](img/B17978_07_0101.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Scatter plot of predictions against residuals
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do k-fold cross-validation once again and take the average r-squared
    value across the folds. There are improvements in both the r-squared and mean
    absolute error compared to the linear model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The polynomial transformation improved our overall results, particularly within
    certain ranges of our predictors. The residuals at high temperatures were noticeably
    lower. It is often a good idea to try a polynomial transformation when our residuals
    suggest that there might be a nonlinear relationship between our features and
    our target.
  prefs: []
  type: TYPE_NORMAL
- en: Regression with gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gradient descent can be a good alternative to ordinary least squares for optimizing
    the loss function of a linear model. This is particularly true when working with
    very large datasets. In this section, we will use gradient descent with the land
    temperatures dataset, mainly to demonstrate how to use it and to give us another
    opportunity to explore exhaustive grid searches. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will load the same libraries we have been working with so far, plus
    the stochastic gradient descent regressor from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will load the land temperatures dataset again and create training
    and testing DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will set up a pipeline to deal with outliers, impute values for missings,
    and scale the data before running the gradient descent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we need to create a dictionary to indicate the hyperparameters we want
    to tune and the values to try. We want to try values for alpha, the loss function,
    epsilon, and the penalty. We will give each key in the dictionary a prefix of
    `regressor__sgdregressor__` because that is where the stochastic gradient descent
    regressor can be found in the pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `alpha` parameter determines the size of the penalty. The default is `0.0001`.
    We can choose L1, L2, or elastic net regularization. We will select `huber` and
    `epsilon_insensitive` as loss functions to include in the search. The default
    loss function is `squared_error`, but that would just give us ordinary least squares
    again.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `huber` loss function is less sensitive to outliers than is OLS. How sensitive
    it is, is based on the value of epsilon we specify. With the `epsilon_insensitive`
    loss function, errors within a given range (epsilon) are not penalized (we will
    construct models with epsilon-insensitive tubes in the next chapter, where we’ll
    examine support vector regression):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to run an exhaustive grid search. The best parameters are
    `0.001` for alpha, `1.3` for epsilon, `huber` for the loss function, and elastic
    net regularization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'I usually find it helpful to look at the hyperparameters for some of the other
    grid search iterations. Huber loss models, with either elastic net or L2 regularization,
    perform best:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Stochastic gradient descent is a generalized approach to optimization and can
    be applied to many machine learning problems. It is often quite efficient, as
    we can see here. We were able to run an exhaustive grid search on penalty, penalty
    size, epsilon, and loss function fairly quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter allowed us to explore a very well-known machine learning algorithm:
    linear regression. We examined the qualities of a feature space that makes it
    a good candidate for a linear model. We also explored how to improve a linear
    model, when necessary, with regularization and with transformations. Then, we
    looked at stochastic gradient descent as an alternative to OLS optimization. We
    also learned how to add our own classes to a pipeline and how to do hyperparameter
    tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore support vector regression.
  prefs: []
  type: TYPE_NORMAL
