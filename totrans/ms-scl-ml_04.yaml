- en: Chapter 4. Supervised and Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 监督学习和无监督学习
- en: I covered the basics of the MLlib library in the previous chapter, but MLlib,
    at least at the time of writing this book, is more like a fast-moving target that
    is gaining the lead rather than a well-structured implementation that everyone
    uses in production or even has a consistent and tested documentation. In this
    situation, as people say, rather than giving you the fish, I will try to focus
    on well-established concepts behind the libraries and teach the process of fishing
    in this book in order to avoid the need to drastically modify the chapters with
    each new MLlib release. For better or worse, this increasingly seems to be a skill
    that a data scientist needs to possess.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上一章中介绍了MLlib库的基础知识，但至少在撰写本书时，MLlib更像是一个快速发展的目标，它正在取得领先地位，而不是一个结构良好的实现，每个人都用于生产或甚至有一个一致且经过测试的文档。在这种情况下，正如人们所说，与其给你鱼，我更愿意专注于库背后的既定概念，并在本书中教授捕鱼的过程，以避免每次MLlib发布时都需要大幅修改章节。不管好坏，这似乎越来越成为一个数据科学家需要掌握的技能。
- en: Statistics and machine learning inherently deal with uncertainty, due to one
    or another reason we covered in [Chapter 2](ch02.xhtml "Chapter 2. Data Pipelines
    and Modeling"), *Data Pipelines and Modeling*. While some datasets might be completely
    random, the goal here is to find trends, structure, and patterns beyond what a
    random number generator will provide you. The fundamental value of ML is that
    we can generalize these patterns and improve on at least some metrics. Let's see
    what basic tools are available within Scala/Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学和机器学习本质上处理不确定性，由于我们在第二章中提到的各种原因，[第二章](ch02.xhtml "第二章 数据管道和建模")，*数据管道和建模*。虽然一些数据集可能是完全随机的，但在这里的目标是找到随机数生成器无法提供的趋势、结构和模式。机器学习的基本价值在于我们可以将这些模式进行泛化，并在至少某些指标上取得改进。让我们看看Scala/Spark中可用的基本工具。
- en: In this chapter, I am covering supervised and unsupervised leaning, the two
    historically different approaches. Supervised learning is traditionally used when
    we have a specific goal to predict a label, or a specific attribute of a dataset.
    Unsupervised learning can be used to understand internal structure and dependencies
    between any attributes of a dataset, and is often used to group the records or
    attributes in meaningful clusters. In practice, both methods can be used to complement
    and aid each other.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将介绍监督学习和无监督学习，这两种历史上不同的方法。监督学习传统上用于当我们有一个特定的目标来预测标签或数据集的特定属性时。无监督学习可用于理解数据集中任何属性的内部结构和依赖关系，并且通常用于将记录或属性分组到有意义的聚类中。在实践中，这两种方法都可以用来补充和辅助对方。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Learning standard models for supervised learning – decision trees and logistic
    regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习监督学习的标准模型——决策树和逻辑回归
- en: Discussing the staple of unsupervised learning – k-means clustering and its
    derivatives
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论无监督学习的基石——k-均值聚类及其衍生方法
- en: Understanding metrics and methods to evaluate the effectiveness of the above
    algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解评估上述算法有效性的指标和方法
- en: Having a glimpse of extending the above methods on special cases of streaming
    data, sparse data, and non-structured data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 略窥一斑，将上述方法扩展到流数据、稀疏数据和非结构化数据的特殊情况
- en: Records and supervised learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记录和监督学习
- en: 'For the purpose of this chapter, a record is an observation or measurement
    of one or several attributes. We assume that the observations might contain noise
    ![Records and supervised learning](img/B04935_04_01F.jpg) (or be inaccurate for
    one or other reason):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章的目的，一个记录是对一个或多个属性的观察或测量。我们假设这些观察可能包含噪声 ![记录和监督学习](img/B04935_04_01F.jpg)（或者由于某种原因不准确）：
- en: '![Records and supervised learning](img/B04935_04_02F.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![记录和监督学习](img/B04935_04_02F.jpg)'
- en: While we believe that there is some pattern or correlation between the attributes,
    the one that we are after and want to uncover, the noise is uncorrelated across
    either the attributes or the records. In statistical terms, we say that the values
    for each record are drawn from the same distribution and are independent (or *i.i.d*.
    in statistical terms). The order of records does not matter. One of the attributes,
    usually the first, might be designated to be the label.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们相信属性之间存在某种模式或相关性，但我们追求并希望揭示的那个属性与记录中的噪声是不相关的。在统计学上，我们说每个记录的值都来自相同的分布，并且是独立的（或用统计学术语表示为*i.i.d*）。记录的顺序并不重要。其中一个属性，通常是第一个，可能被指定为标签。
- en: 'Supervised learning is when the goal is to predict the label *yi*:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是指目标是要预测标签*yi*：
- en: '![Records and supervised learning](img/B04935_04_03F.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![记录和监督学习](img/B04935_04_03F.jpg)'
- en: Here, *N* is the number of remaining attributes. In other words, the goal is
    to generalize the patterns so that we can predict the label by just knowing the
    other attributes, whether because we cannot physically get the measurement or
    just want to explore the structure of the dataset without having the immediate
    goal to predict the label.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*N*是剩余属性的数目。换句话说，目标是泛化模式，以便我们只需知道其他属性就能预测标签，无论是由于我们无法物理获取测量值，还是仅仅想探索数据集的结构而不具有预测标签的即时目标。
- en: The unsupervised learning is when we don't use the label—we just try to explore
    the structure and correlations to understand the dataset to, potentially, predict
    the label better. The number of problems in this latter category has increased
    recently with the emergence of learning for unstructured data and streams, each
    of which, I'll be covering later in the book in separate chapters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是指我们不使用标签——我们只是尝试探索结构和相关性，以了解数据集，从而可能更好地预测标签。随着无结构数据学习和流的学习的出现，这一类问题最近有所增加，我将在本书的单独章节中分别介绍。
- en: Iris dataset
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鸢尾花数据库
- en: 'I will demonstrate the concept of records and labels based on one of the most
    famous datasets in machine learning, the Iris dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)).
    The Iris dataset contains 50 records for each of the three types of Iris flower,
    150 lines of total five fields. Each line is a measurement of the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过机器学习中最著名的数据库之一，即鸢尾花数据库（[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)），来演示记录和标签的概念。鸢尾花数据库包含三种鸢尾花类型的50条记录，总共有150行，五个字段。每一行是以下测量值：
- en: Sepal length in cm
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花萼长度（厘米）
- en: Sepal width in cm
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花萼宽度（厘米）
- en: Petal length in cm
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度（厘米）
- en: Petal width in cm
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度（厘米）
- en: 'With the final field being the type of the flower (*setosa*, *versicolor*,
    or *virginica*). The classic problem is to predict the label, which, in this case,
    is a categorical attribute with three possible values as a function of the first
    four attributes:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个字段是花的类型（*setosa*、*versicolor*或*virginica*）。经典问题是要预测标签，在这种情况下，这是一个具有三个可能值的分类属性，这些值是前四个属性的功能：
- en: '![Iris dataset](img/B04935_04_04F.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![鸢尾花数据库](img/B04935_04_04F.jpg)'
- en: 'One option would be to draw a plane in the four-dimensional space that separates
    all four labels. Unfortunately, as one can find out, while one of the classes
    is clearly separable, the remaining two are not, as shown in the following multidimensional
    scatterplot (we have used Data Desk software to create it):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是在四维空间中绘制一个平面，将所有四个标签分开。不幸的是，正如人们可以发现的，虽然其中一个类别明显可分，但剩下的两个类别不可分，如下面的多维散点图所示（我们使用了Data
    Desk软件创建它）：
- en: '![Iris dataset](img/B04935_04_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![鸢尾花数据库](img/B04935_04_01.jpg)'
- en: Figure 04-1\. The Iris dataset as a three-dimensional plot. The Iris setosa
    records, shown by crosses, can be separated from the other two types based on
    petal length and width.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 04-1\. 鸢尾花数据库作为三维图。鸢尾花*setosa*记录，用交叉表示，可以根据花瓣长度和宽度与其他两种类型分开。
- en: 'The colors and shapes are assigned according to the following table:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色和形状根据以下表格分配：
- en: '| Label | Color | Shape |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 颜色 | 形状 |'
- en: '| --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *Iris setosa* | Blue | x |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| *Iris setosa* | 蓝色 | x |'
- en: '| *Iris versicolor* | Green | Vertical bar |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| *Iris versicolor* | 绿色 | 垂直条 |'
- en: '| *Iris virginica* | Purple | Horizontal bar |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| *Iris virginica* | 紫色 | 水平条 |'
- en: The *Iris setosa* is separable because it happens to have a very short petal
    length and width compared to the two other types.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*Iris setosa*是可分离的，因为它碰巧具有非常短的花瓣长度和宽度，与其他两种类型相比。'
- en: Let's see how we can use MLlib to find that separating multidimensional plane.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用MLlib找到那个分隔多维平面的方法。
- en: Labeled point
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记点
- en: 'The labeled datasets used to have a very special place in ML—we will discuss
    unsupervised learning later in the chapter, where we do not need a label, so MLlib
    has a special data type to represent a record with a `org.apache.spark.mllib.regression.LabeledPoint`
    label (refer to [https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point](https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point)).
    To read the Iris dataset from a text file, we need to transform the original UCI
    repository file into the so-called LIBSVM text format. While there are plenty
    of converters from CSV to LIBSVM format, I''d like to use a simple AWK script
    to do the job:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 之前用于标记的数据集在机器学习中占有特殊地位——我们将在本章后面讨论无监督学习，在那里我们不需要标签，因此MLlib有一个特殊的数据类型来表示带有`org.apache.spark.mllib.regression.LabeledPoint`标签的记录（参考[https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point](https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point)）。要从文本文件中读取Iris数据集，我们需要将原始UCI仓库文件转换为所谓的LIBSVM文本格式。虽然有很多从CSV到LIBSVM格式的转换器，但我希望使用一个简单的AWK脚本来完成这项工作：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Why do we need LIBSVM format?**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么我们需要LIBSVM格式？**'
- en: LIBSVM is the format that many libraries use. First, LIBSVM takes only continuous
    attributes. While a lot of datasets in the real world contain discrete or categorical
    attributes, internally they are always converted to a numerical representation
    for efficiency reasons, even if the L1 or L2 metrics on the resulting numerical
    attribute does not make much sense in the unordered discrete values. Second, the
    LIBSVM format allows for efficient sparse data representation. While the Iris
    dataset is not sparse, almost all of the modern big data sources are sparse, and
    the format allows for efficient storage by only storing the provided values. Many
    modern big data key-value and traditional RDBMS databases actually do the same
    for efficiency reasons.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LIBSVM是许多库使用的格式。首先，LIBSVM只接受连续属性。虽然现实世界中的许多数据集包含离散或分类属性，但出于效率考虑，它们在内部始终转换为数值表示，即使结果数值属性上的L1或L2度量在无序的离散值上没有太多意义。其次，LIBSVM格式允许高效地表示稀疏数据。虽然Iris数据集不是稀疏的，但几乎所有现代大数据源都是稀疏的，该格式通过仅存储提供的值来实现高效存储。许多现代大数据键值和传统的关系型数据库管理系统实际上出于效率考虑也这样做。
- en: The code might be more complex for missing values, but we know that the Iris
    dataset is not sparse—otherwise we'd complement our code with a bunch of if statements.
    We mapped the last two labels to 1 for our purpose now.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于缺失值，代码可能更复杂，但我们知道Iris数据集不是稀疏的——否则我们会在代码中添加一堆if语句。现在，我们将最后两个标签映射为1。
- en: SVMWithSGD
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVMWithSGD
- en: 'Now, let''s run the **Linear Support Vector Machine** (**SVM**) SVMWithSGD
    code from MLlib:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行MLlib中的**线性支持向量机**（SVM）SVMWithSGD代码：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So, you just run one of the most complex algorithms in the machine learning
    toolbox: SVM. The result is a separating plane that distinguishes *Iris setosa*
    flowers from the other two types. The model in this case is exactly the intercept
    and the coefficients of the plane that best separates the labels:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你只需运行机器学习工具箱中最复杂的算法之一：SVM。结果是区分*伊丽莎白*花与其他两种类型的分离平面。在这种情况下，模型正是最佳分离标签的截距和平面系数：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If one looks under the hood, the model is stored in a `parquet` file, which
    can be dumped using `parquet-tool`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果深入探究，模型存储在一个`parquet`文件中，可以使用`parquet-tool`进行导出：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The **Receiver Operating Characteristic** (**ROC**) is a common measure of the
    classifier to be able to correctly rank the records according to their numeric
    label. We will consider precision metrics in more detail in [Chapter 9](ch09.xhtml
    "Chapter 9. NLP in Scala"), *NLP in Scala*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（ROC）是衡量分类器能否根据其数值标签正确排序记录的常用指标。我们将在[第9章](ch09.xhtml "第9章. Scala中的NLP")中更详细地考虑精度指标，*Scala中的NLP*。'
- en: Tip
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**What is ROC?**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是ROC？**'
- en: 'ROC has emerged in signal processing with the first application to measure
    the accuracy of analog radars. The common measure of accuracy is area under ROC,
    which, shortly, is the probability of two randomly chosen points to be ranked
    correctly according to their labels (the *0* label should always have a lower
    rank than the *1* label). AUROC has a number of attractive characteristics:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ROC（接收者操作特征）在信号处理中崭露头角，其首次应用是测量模拟雷达的准确性。准确性的常用指标是ROC曲线下的面积，简而言之，就是随机选择两个点按其标签正确排序的概率（标签*0*应始终具有比标签*1*低的排名）。AUROC具有许多吸引人的特性：
- en: The value, at least theoretically, does not depend on the oversampling rate,
    that is, the rate at which we see *0* labels as opposed to *1* labels.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该值，至少从理论上讲，不依赖于过采样率，即我们看到*0*标签与*1*标签的比例。
- en: The value does not depend on the sample size, excluding the expected variance
    due to the limited sample size.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该值不依赖于样本大小，排除了由于样本量有限而产生的预期方差。
- en: Adding a constant to the final score does not change the ROC, thus the intercept
    can always be set to *0*. Computing the ROC requires a sort with respect to the
    generated score.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将常数添加到最终得分不会改变ROC，因此截距可以始终设置为*0*。计算ROC需要根据生成的得分进行排序。
- en: 'Of course, separating the remaining two labels is a harder problem since the
    plane that separated *Iris versicolor* from *Iris virginica* does not exist: the
    AUROC score will be less than *1.0*. However, the SVM method will find the plane
    that best differentiates between the latter two classes.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，分离剩余的两个标签是一个更难的问题，因为将*Iris versicolor*与*Iris virginica*分开的平面不存在：AUROC分数将小于*1.0*。然而，SVM方法将找到最佳区分后两个类别的平面。
- en: Logistic regression
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Logistic regression is one of the oldest classification methods. The outcome
    of the logistic regression is also a set of weights, which define the hyperplane,
    but the loss function is logistic loss instead of *L2*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是最古老的分类方法之一。逻辑回归的结果也是一组权重，这些权重定义了超平面，但损失函数是逻辑损失而不是*L2*：
- en: '![Logistic regression](img/B04935_04_05F.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/B04935_04_05F.jpg)'
- en: 'Logit function is a frequent choice when the label is binary (as *y = +/- 1*
    in the above equation):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当标签是二进制时（如上式中的*y = +/- 1*），对数几率函数是常见的选择：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The labels in this case can be any integer in the range *[0, k)*, where *k*
    is the total number of classes (the correct class will be determined by building
    multiple binary logistic regression models against the pivot class, which in this
    case, is the class with the *0* label) (*The Elements of Statistical Learning*
    by *Trevor Hastie*, *Robert Tibshirani*, *Jerome Friedman*, *Springer Series in
    Statistics*).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，标签可以是范围*[0, k)*内的任何整数，其中*k*是类的总数（正确的类别将通过针对枢轴类（在这种情况下，是带有*0*标签的类别）构建多个二元逻辑回归模型来确定）(*《统计学习的要素》由*Trevor
    Hastie*，*Robert Tibshirani*，*Jerome Friedman*，*Springer Series in Statistics*）。
- en: The accuracy metric is precision, or the percentage of records predicted correctly
    (which is 95% in our case).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度指标是精确度，即预测正确的记录百分比（在我们的案例中为95%）。
- en: Decision tree
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: 'The preceding two methods describe linear models. Unfortunately, the linear
    approach does not always work for complex interactions between attributes. Assume
    that the label looks like an exclusive *OR: 0* if *X ≠ Y* and *1* if *X = Y*:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '前两种方法描述了线性模型。不幸的是，线性方法并不总是适用于属性之间的复杂交互。假设标签看起来像这样的排他性*OR: 0*，如果*X ≠ Y*，则为*1*，如果*X
    = Y*：'
- en: '| X | Y | Label |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| X | Y | 标签 |'
- en: '| --- | --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 0 | 0 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 |'
- en: '| 0 | 1 | 0 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 0 |'
- en: '| 1 | 1 | 1 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 |'
- en: '| 0 | 0 | 1 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 1 |'
- en: 'There is no hyperplane that can differentiate between the two labels in the
    *XY* space. Recursive split solution, where the split on each level is made on
    only one variable or a linear combination thereof might work a bit better in these
    case. Decision trees are also known to work well with sparse and interaction-rich
    datasets:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在*XY*空间中，没有超平面可以区分这两个标签。递归分割解决方案，其中每个级别的分割仅基于一个变量或其线性组合，在这种情况下可能效果会更好。决策树也已知与稀疏和交互丰富的数据集配合得很好：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the error (misprediction) rate on hold-out 30% sample is only
    2.6%. The 30% sample of 150 is only 45 records, which means we missed only 1 record
    from the whole test set. Certainly, the result might and will change with a different
    seed, and we need a more rigorous cross-validation technique to prove the accuracy
    of the model, but this is enough for a rough estimate of model performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在保留30%样本的误差（误判）率仅为2.6%。150个样本中的30%只有45个记录，这意味着我们只从整个测试集中遗漏了一个记录。当然，结果可能会改变，也可能会有所不同，我们需要更严格的交叉验证技术来证明模型的准确性，但这足以对模型性能进行粗略估计。
- en: Decision tree generalizes on regression case, that is, when the label is continuous
    in nature. In this case, the splitting criterion is minimization of weighted variance,
    as opposed to entropy gain or gini in the case of classification. I will talk
    more about the differences in [Chapter 5](ch05.xhtml "Chapter 5. Regression and
    Classification"), *Regression and Classification*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在回归案例中泛化，即当标签本质上是连续的时候。在这种情况下，分割标准是最小化加权方差，而不是分类案例中的熵增益或基尼。我将在[第5章](ch05.xhtml
    "第5章。回归和分类")*回归和分类*中更多地讨论这些差异。
- en: 'There are a number of parameters, which can be tuned to improve the performance:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多参数可以调整以改善性能：
- en: '| Parameter | Description | Recommended value |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 | 推荐值 |'
- en: '| --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `maxDepth` | This is the maximum depth of the tree. Deep trees are costly
    and usually are more likely to overfit. Shallow trees are more efficient and better
    for bagging/boosting algorithms such as AdaBoost. | This depends on the size of
    the original dataset. It is worth experimenting and plotting the accuracy of the
    resulting tree versus the parameter to find out the optimum. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `maxDepth` | 这是树的最大深度。深度树成本较高，通常更容易过拟合。浅层树更高效，更适合AdaBoost等bagging/boosting算法。
    | 这取决于原始数据集的大小。值得尝试并绘制结果树的准确性与参数之间的关系图，以找出最佳值。 |'
- en: '| `minInstancesPerNode` | This also limits the size of the tree: once the number
    of instances falls under this threshold, no further splitting occurs. | The value
    is usually 10-100, depending on the complexity of the original dataset and the
    number of potential labels. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `minInstancesPerNode` | 这也限制了树的大小：一旦实例数量低于此阈值，就不会再进行分割。 | 该值通常是10-100，具体取决于原始数据集的复杂性和潜在标签的数量。
    |'
- en: '| `maxBins` | This is used only for continuous attributes: the number of bins
    to split the original range. | Large number of bins increase computation and communication
    cost. One can also consider the option of pre-discretizing the attribute based
    on domain knowledge. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `maxBins` | 这仅用于连续属性：分割原始范围的箱数。 | 大量的箱会增加计算和通信成本。也可以考虑根据领域知识对属性进行预离散化的选项。
    |'
- en: '| `minInfoGain` | This is the amount of information gain (entropy), impurity
    (gini), or variance (regression) gain to split a node. | The default is *0*, but
    you can increase the default to limit the tree size and reduce the risk of overfitting.
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `minInfoGain` | 这是分割节点所需的信息增益（熵）、不纯度（基尼）或方差（回归）增益。 | 默认值为*0*，但您可以增加默认值以限制树的大小并降低过拟合的风险。
    |'
- en: '| `maxMemoryInMB` | This is the amount of memory to be used for collecting
    sufficient statistics. | The default value is conservatively chosen to be 256
    MB to allow the decision algorithm to work in most scenarios. Increasing `maxMemoryInMB`
    can lead to faster training (if the memory is available) by allowing fewer passes
    over the data. However, there may be decreasing returns as `maxMemoryInMB` grows,
    as the amount of communication on each iteration can be proportional to `maxMemoryInMB`.
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `maxMemoryInMB` | 这是用于收集足够统计信息的内存量。 | 默认值保守地选择为256 MB，以允许决策算法在大多数场景下工作。增加`maxMemoryInMB`可以通过允许对数据进行更少的遍历来加快训练速度（如果内存可用）。然而，随着`maxMemoryInMB`的增加，每轮迭代中的通信量可能成比例增加，因此可能存在递减的回报。
    |'
- en: '| `subsamplingRate` | This is the fraction of the training data used for learning
    the decision tree. | This parameter is most relevant for training ensembles of
    trees (using `RandomForest` and `GradientBoostedTrees`), where it can be useful
    to subsample the original data. For training a single decision tree, this parameter
    is less useful since the number of training instances is generally not the main
    constraint. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `subsamplingRate` | 这是用于学习决策树的训练数据比例。 | 此参数对于训练树集成（使用`RandomForest`和`GradientBoostedTrees`）最为相关，在这种情况下，对原始数据进行子采样可能很有用。对于训练单个决策树，此参数不太有用，因为训练实例的数量通常不是主要约束。
    |'
- en: '| `useNodeIdCache` | If this is set to true, the algorithm will avoid passing
    the current model (tree or trees) to executors on each iteration. | This can be
    useful with deep trees (speeding up computation on workers) and for large random
    forests (reducing communication on each iteration). |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| `useNodeIdCache` | 如果设置为true，算法将避免在每次迭代中将当前模型（树或树集）传递给执行者。 | 这对于深度树（加快工作者的计算速度）和大型随机森林（减少每轮迭代中的通信）可能很有用。
    |'
- en: '| `checkpointDir:` | This is the directory for checkpointing the node ID cache
    RDDs. | This is an optimization to save intermediate results to avoid recomputation
    in case of node failure. Set it in large clusters or with unreliable nodes. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `checkpointDir:` | 这是节点ID缓存RDDs进行checkpoint的目录。| 这是一个优化，用于将中间结果保存下来，以避免节点失败时的重新计算。在大集群或不稳定的节点上设置。|'
- en: '| `checkpointInterval` | This is the frequency for checkpointing the node ID
    cache RDDs. | Setting this too low will cause extra overhead from writing to HDFS
    and setting this too high can cause problems if executors fail and the RDD needs
    to be recomputed. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `checkpointInterval` | 这是节点ID缓存RDDs进行checkpoint的频率。| 设置得太低会导致写入HDFS的额外开销，设置得太高则可能在执行器失败且需要重新计算RDD时出现问题。|'
- en: Bagging and boosting – ensemble learning methods
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging和boosting – 集成学习方法
- en: As a portfolio of stocks has better characteristics compared to individual equities,
    models can be combined to produce better classifiers. Usually, these methods work
    really well with decision trees as the training technique can be modified to produce
    models with large variations. One way is to train the model on random subsets
    of the original data or random subsets of attributes, which is called random forest.
    Another way is to generate a sequence of models, where misclassified instances
    are reweighted to get a larger weight in each subsequent iteration. It has been
    shown that this method has a relation to gradient descent methods in the model
    parameter space. While these are valid and interesting techniques, they usually
    require much more space in terms of model storage and are less interpretable compared
    to bare decision tree models. For Spark, the ensemble models are currently under
    development—the umbrella issue is SPARK-3703 ([https://issues.apache.org/jira/browse/SPARK-3703](https://issues.apache.org/jira/browse/SPARK-3703)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与单个股票相比，股票组合具有更好的特性，因此可以将模型结合起来以产生更好的分类器。通常，这些方法与决策树作为训练技术结合得非常好，因为训练技术可以被修改以产生具有大量变化的模型。一种方法是在原始数据的随机子集或属性的随机子集上训练模型，这被称为随机森林。另一种方法是通过生成一系列模型，将错误分类的实例重新加权，以便在后续的每次迭代中获得更大的权重。已经证明，这种方法与模型参数空间中的梯度下降方法有关。虽然这些是有效且有趣的技术，但它们通常需要更多的空间来存储模型，并且与裸决策树模型相比，可解释性较差。对于Spark，集成模型目前正在开发中——主要问题为SPARK-3703
    ([https://issues.apache.org/jira/browse/SPARK-3703](https://issues.apache.org/jira/browse/SPARK-3703))。
- en: Unsupervised learning
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: If we get rid of the label in the Iris dataset, it would be nice if some algorithm
    could recover the original grouping, maybe without the exact label names—*setosa*,
    *versicolor*, and *virginica*. Unsupervised learning has multiple applications
    in compression and encoding, CRM, recommendation engines, and security to uncover
    internal structure without actually having the exact labels. The labels sometimes
    can be given base on the singularity in attribute value distributions. For example,
    *Iris setosa* can be described as a *Flower with Small Leaves*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从Iris数据集中移除标签，如果某些算法能够恢复原始分组，那就很好了，也许不需要确切的标签名称——*setosa*，*versicolor*，和*virginica*。无监督学习在压缩和编码、CRM、推荐引擎和安全领域有多个应用，可以揭示内部结构而不需要确切的标签。有时，可以根据属性值分布的奇异性给出标签。例如，*Iris
    setosa*可以描述为*小叶花*。
- en: While a supervised learning problem can always be cast as unsupervised by disregarding
    the label, the reverse is also true. A clustering algorithm can be cast as a density-estimation
    problem by assigning label *1* to all vectors and generating random vectors with
    label *0* (*The Elements of Statistical Learning* by *Trevor Hastie*, *Robert
    Tibshirani*, *Jerome Friedman*, *Springer Series in Statistics*). The difference
    between the two is formal and it's even fuzzier with non-structured and nested
    data. Often, running unsupervised algorithms in labeled datasets leads to a better
    understanding of the dependencies and thus a better selection and performance
    of the supervised algorithm.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以通过忽略标签将监督学习问题表示为无监督学习，但反之亦然。可以将聚类算法表示为密度估计问题，通过将所有向量分配标签*1*并生成带有标签*0*的随机向量（*《统计学习的要素》*，作者：*特里夫·哈斯蒂*，*罗伯特·蒂布斯哈里*，*杰罗姆·弗里德曼*，*斯普林格统计系列*）。两者之间的区别是正式的，对于非结构化和嵌套数据来说甚至更加模糊。通常，在标记数据集上运行无监督算法可以更好地理解依赖关系，从而更好地选择和表现监督算法。
- en: 'One of the most popular algorithms for clustering and unsupervised learning
    in k-means (and its variants, k-median and k-center, will be described later):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是k-means（及其变体，如k-median和k-center，将在后面描述）中用于聚类和无监督学习最受欢迎的算法之一：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: One can see that the first center, the one with index `0`, has petal length
    and width of `1.464` and `0.244`, which is much shorter than the other two—`5.715`
    and `2.054`, `4.389` and `1.434`). The prediction completely matches the first
    cluster, corresponding to *Iris setosa*, but has a few mispredictions for the
    other two.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到第一个中心，即索引为`0`的中心，花瓣长度和宽度为`1.464`和`0.244`，这比其他两个——`5.715`和`2.054`，`4.389`和`1.434`——要短得多。预测完全匹配第一个聚类，对应于*Iris
    setosa*，但对其他两个有一些误判。
- en: 'The measure of cluster quality might depend on the (desired) labels if we want
    to achieve a desired classification result, but since the algorithm has no information
    about the labeling, a more common measure is the sum of distances from centroids
    to the points in each of the clusters. Here is a graph of `WSSSE`, depending on
    the number of clusters:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要达到预期的分类结果，聚类质量的度量可能取决于（期望的）标签，但由于算法没有关于标签的信息，一个更常见的度量是每个聚类中从质心到点的距离之和。以下是`WSSSE`随聚类数量变化的图表：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As expected, the average distance is decreasing as more clusters are configured.
    A common method to determine the optimal number of clusters—in our example, we
    know that there are three types of flowers—is to add a penalty function. A common
    penalty is the log of the number of clusters as we expect a convex function. What
    would be the coefficient in front of log? If each vector is associated with its
    own cluster, the sum of all distances will be zero, so if we would like a metric
    that achieves approximately the same value at both ends of the set of possible
    values, `1` to `150`, the coefficient should be `680.8244/log(150)`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，随着配置的聚类数量增加，平均距离在减小。确定最佳聚类数量的一个常见方法是添加一个惩罚函数。一个常见的惩罚是聚类数量的对数，因为我们期望一个凸函数。对数前面的系数是多少？如果每个向量都与自己的聚类相关联，所有距离的总和将为零，因此如果我们希望一个度量在可能的值集的两端都达到大约相同的值，即`1`到`150`，那么系数应该是`680.8244/log(150)`：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here is how the sum of the squared distances with penalty looks as a graph:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是平方距离之和（含惩罚项）的图表表示：
- en: '![Unsupervised learning](img/B04935_04_02.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/B04935_04_02.jpg)'
- en: Figure 04-2\. The measure of the clustering quality as a function of the number
    of clusters
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图04-2. 聚类质量度量作为聚类数量的函数
- en: 'Besides k-means clustering, MLlib also has implementations of the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了k-means聚类之外，MLlib还实现了以下算法：
- en: Gaussian mixture
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合
- en: '**Power Iteration Clustering** (**PIC**)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幂迭代聚类**（**PIC**）'
- en: '**Latent Dirichlet Allocation** (**LDA**)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）'
- en: Streaming k-means
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式k-means
- en: 'The Gaussian mixture is another classical mechanism, particularly known for
    spectral analysis. Gaussian mixture decomposition is appropriate, where the attributes
    are continuous and we know that they are likely to come from a set of Gaussian
    distributions. For example, while the potential groups of points corresponding
    to clusters may have the average for all attributes, say **Var1** and **Var2**,
    the points might be centered around two intersecting hyperplanes, as shown in
    the following diagram:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合是另一种经典机制，特别以其频谱分析而闻名。高斯混合分解适用于属性连续且我们知道它们可能来自一组高斯分布的情况。例如，虽然对应于聚类的潜在点集可能具有所有属性的均值，比如**Var1**和**Var2**，但点可能围绕两个相交的超平面中心，如下面的图所示：
- en: '![Unsupervised learning](img/B04935_04_03.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/B04935_04_03.jpg)'
- en: Figure 04-3\. A mixture of two Gaussians that cannot be properly described by
    k-means clustering
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图04-3. 不能由k-means聚类正确描述的两个高斯混合
- en: This renders the k-means algorithm ineffective as it will not be able to distinguish
    between the two (of course a simple non-linear transformation such as a distance
    to one of the hyperplanes will solve the problem, but this is where domain knowledge
    and expertise as a data scientist are handy).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得k-means算法变得无效，因为它无法区分这两个（当然，一个简单的非线性变换，如到其中一个超平面的距离，将解决这个问题，但这就是领域知识和数据科学家专业知识派上用场的地方）。
- en: PIC is using clustering vertices of a graph provided pairwise similarity measures
    given as edge properties. It computes a pseudo-eigenvector of the normalized affinity
    matrix of the graph via power iteration and uses it to cluster vertices. MLlib
    includes an implementation of PIC using GraphX as its backend. It takes an RDD
    of (`srcId`, `dstId`, similarity) tuples and outputs a model with the clustering
    assignments. The similarities must be non-negative. PIC assumes that the similarity
    measure is symmetric. A pair (`srcId`, `dstId`) regardless of the ordering should
    appear at most once in the input data. If a pair is missing from the input, their
    similarity is treated as zero.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: PIC使用图聚类顶点，并提供了作为边属性的成对相似度度量。它通过幂迭代计算图归一化亲和矩阵的伪特征向量，并使用它来聚类顶点。MLlib包括了一个使用GraphX作为其后端的PIC实现。它接受一个包含(`srcId`,
    `dstId`, 相似度)元组的RDD，并输出一个具有聚类分配的模型。相似度必须为非负。PIC假设相似度度量是对称的。无论顺序如何，一对(`srcId`,
    `dstId`)在输入数据中最多只能出现一次。如果一对缺失于输入，则它们的相似度被视为零。
- en: LDA can be used for clustering documents based on keyword frequencies. Rather
    than estimating a clustering using a traditional distance, LDA uses a function
    based on a statistical model of how text documents are generated.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: LDA可以用于基于关键词频率对文档进行聚类。LDA不是使用传统距离来估计聚类，而是使用基于文本文档生成统计模型的函数。
- en: 'Finally, streaming k-means is a modification of the k-means algorithm, where
    the clusters can be adjusted with new batches of data. For each batch of data,
    we assign all points to their nearest cluster, compute new cluster centers based
    on the assignment, and then update each cluster parameters using the equations:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，流式k-means是k-means算法的一种改进，其中聚类可以通过新批次的数据进行调整。对于每个数据批次，我们将所有点分配到最近的聚类，根据分配计算新的聚类中心，然后使用以下方程更新每个聚类参数：
- en: '![Unsupervised learning](img/B04935_04_06F.jpg)![Unsupervised learning](img/B04935_04_07F.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/B04935_04_06F.jpg)![无监督学习](img/B04935_04_07F.jpg)'
- en: Here, *c* *t* and *c'* *t* are the centers of from the old model and the ones
    computed for the new batch and *n* *t* and *n'* *t* are the number of vectors
    from the old model and for the new batch. By changing the *α* parameter, we can
    control how much information from the old runs can influence the clustering—*0*
    means the new cluster centers are totally based on the points in the new batch,
    while *1* means that we accommodate for all points that we have seen so far.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*c* *t* 和 *c'* *t* 是旧模型和为新批次计算的中心，而 *n* *t* 和 *n'* *t* 是旧模型和新批次中的向量数量。通过改变
    *α* 参数，我们可以控制旧运行的信息对聚类的影响程度——*0* 表示新聚类中心完全基于新批次中的点，而 *1* 表示我们为迄今为止看到的所有点做出调整。
- en: k-means clustering has many modifications. For example, k-medians computes the
    cluster centers as medians of the attribute values, not mean, which works much
    better for some distributions and with *L1* target distance metric (absolute value
    of the difference) as opposed to *L2* (the sum of squares). K-medians centers
    are not necessarily present as a specific point in the dataset. K-medoids is another
    algorithm from the same family, where the resulting cluster center has to be an
    actual instance in the input set and we actually do not need to have the global
    sort, only the pairwise distances between the points. Many variations of the techniques
    exist on how to choose the original seed cluster centers and converge on the optimal
    number of clusters (besides the simple log trick I have shown).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类有许多改进。例如，k-medians计算聚类中心为属性值的中位数，而不是均值，这对于某些分布和与*L1*目标距离度量（差值的绝对值）相比*L2*（平方和）来说效果更好。K-medians中心不一定是数据集中具体的一个点。K-medoids是同一家族中的另一个算法，其中结果聚类中心必须是输入集中的实际实例，并且我们实际上不需要全局排序，只需要点之间的成对距离。关于如何选择原始种子聚类中心和收敛到最佳聚类数量（除了我展示的简单对数技巧之外）的技术有许多变体。
- en: Another big class of clustering algorithms is hierarchical clustering. Hierarchical
    clustering is either done from the top—akin to the decision tree algorithms—or
    from the bottom; we first find the closest neighbors, pair them, and continue
    the pairing process up the hierarchy until all records are merged. The advantage
    of hierarchical clustering is that it can be made deterministic and relatively
    fast, even though the cost of one iteration in k-means is probably going to be
    better. However, as mentioned, the unsupervised problem can actually be converted
    to a density-estimation supervised problem, with all the supervised learning techniques
    available. So have fun understanding the data!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一大类聚类算法是层次聚类。层次聚类可以是自顶向下进行的——类似于决策树算法——或者自底向上；我们首先找到最近的邻居，将它们配对，然后继续向上配对直到所有记录合并。层次聚类的优点是它可以被设计成确定性的并且相对较快，尽管k-means算法单次迭代的成本可能更好。然而，正如提到的，无监督问题实际上可以被转换成一个密度估计的监督问题，可以使用所有可用的监督学习技术。所以，享受理解数据吧！
- en: Problem dimensionality
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题维度
- en: The larger the attribute space or the number of dimensions, the harder it is
    to usually predict the label for a given combination of attribute values. This
    is mostly due to the fact that the total number of possible distinct combinations
    of attributes increases exponentially with the dimensionality of the attribute
    space—at least in the case of discrete variables (in case of continuous variables,
    the situation is more complex and depends on the metrics used), and it is becoming
    harder to generalize.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 属性空间越大或维度越多，通常预测给定属性值组合的标签就越困难。这主要是因为属性空间的可能的不同属性组合的总数随着属性空间维度的增加而指数级增长——至少在离散变量的情况下（对于连续变量，情况更复杂，取决于使用的度量），并且泛化的难度越来越大。
- en: The effective dimensionality of the problem might be different from the dimensionality
    of the input space. For example, if the label depends only on the linear combination
    of the (continuous) input attributes, the problem is called linearly separable
    and its internal dimensionality is one—we still have to find the coefficients
    for this linear combination like in logistic regression though.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的有效维度可能与输入空间的维度不同。例如，如果标签只依赖于（连续）输入属性的线性组合，那么问题被称为线性可分，其内部维度为一个是——我们仍然需要在逻辑回归中找到这个线性组合的系数。
- en: This idea is also sometimes referred to as a **Vapnik–Chervonenkis** (**VC**)
    dimension of a problem, model, or algorithm—the expressive power of the model
    depending on how complex the dependencies that it can solve, or shatter, might
    be. More complex problems require algorithms with higher VC dimensions and larger
    training sets. However, using an algorithm with higher VC dimension on a simple
    problem can lead to overfitting and worse generalization to new data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这种想法有时也被称为问题、模型或算法的**Vapnik–Chervonenkis**（**VC**）维度——模型的表达能力取决于它能够解决或分解的依赖关系的复杂程度。更复杂的问题需要具有更高VC维度的算法和更大的训练集。然而，在简单问题上使用具有更高VC维度的算法可能会导致过拟合，并且对新数据的泛化更差。
- en: 'If the units of input attributes are comparable, say all of them are meters
    or units of time, PCA, or more generally, kernel methods, can be used to reduce
    the dimensionality of the input space:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入属性的单元是可比较的，比如说它们都是米或时间的单位，PCA，或者更普遍地，核方法，可以用来降低输入空间的维度：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we reduced the original four-dimensional problem to two-dimensional. Like
    averaging, computing linear combinations of input attributes and selecting only
    those that describe most of the variance helps to reduce noise.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将原始的四维问题降低到二维。像平均一样，计算输入属性的线性组合并仅选择描述大部分方差的那部分，有助于减少噪声。
- en: Summary
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at supervised and unsupervised learning and a few
    examples of how to run them in Spark/Scala. We considered SVM, logistic regression,
    decision tree, and k-means in the example of UCI Iris dataset. This is in no way
    a complete guide, and many other libraries either exist or are being made as we
    speak, but I would bet that you can solve 99% of the immediate data analysis problems
    just with these tools.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了监督学习和无监督学习，以及如何在Spark/Scala中运行它们的几个示例。我们以UCI Iris数据集为例，考虑了SVM、逻辑回归、决策树和k-means。这绝对不是一份完整的指南，而且现在存在或正在制作许多其他库，但我敢打赌，你只需使用这些工具就可以解决99%的即时数据分析问题。
- en: This will give you a very fast shortcut on how to start being productive with
    a new dataset. There are many other ways to look at the datasets, but before we
    get into more advanced topics, let's discuss regression and classification in
    the next chapter, that is, how to predict continuous and discrete labels.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为您快速提供如何开始使用新数据集提高生产力的捷径。查看数据集的方法有很多，但在我们深入更高级的主题之前，让我们在下一章讨论回归和分类，即如何预测连续和离散标签。
