- en: Chapter 4. Supervised and Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I covered the basics of the MLlib library in the previous chapter, but MLlib,
    at least at the time of writing this book, is more like a fast-moving target that
    is gaining the lead rather than a well-structured implementation that everyone
    uses in production or even has a consistent and tested documentation. In this
    situation, as people say, rather than giving you the fish, I will try to focus
    on well-established concepts behind the libraries and teach the process of fishing
    in this book in order to avoid the need to drastically modify the chapters with
    each new MLlib release. For better or worse, this increasingly seems to be a skill
    that a data scientist needs to possess.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics and machine learning inherently deal with uncertainty, due to one
    or another reason we covered in [Chapter 2](ch02.xhtml "Chapter 2. Data Pipelines
    and Modeling"), *Data Pipelines and Modeling*. While some datasets might be completely
    random, the goal here is to find trends, structure, and patterns beyond what a
    random number generator will provide you. The fundamental value of ML is that
    we can generalize these patterns and improve on at least some metrics. Let's see
    what basic tools are available within Scala/Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I am covering supervised and unsupervised leaning, the two
    historically different approaches. Supervised learning is traditionally used when
    we have a specific goal to predict a label, or a specific attribute of a dataset.
    Unsupervised learning can be used to understand internal structure and dependencies
    between any attributes of a dataset, and is often used to group the records or
    attributes in meaningful clusters. In practice, both methods can be used to complement
    and aid each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning standard models for supervised learning – decision trees and logistic
    regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing the staple of unsupervised learning – k-means clustering and its
    derivatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding metrics and methods to evaluate the effectiveness of the above
    algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having a glimpse of extending the above methods on special cases of streaming
    data, sparse data, and non-structured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Records and supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the purpose of this chapter, a record is an observation or measurement
    of one or several attributes. We assume that the observations might contain noise
    ![Records and supervised learning](img/B04935_04_01F.jpg) (or be inaccurate for
    one or other reason):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Records and supervised learning](img/B04935_04_02F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: While we believe that there is some pattern or correlation between the attributes,
    the one that we are after and want to uncover, the noise is uncorrelated across
    either the attributes or the records. In statistical terms, we say that the values
    for each record are drawn from the same distribution and are independent (or *i.i.d*.
    in statistical terms). The order of records does not matter. One of the attributes,
    usually the first, might be designated to be the label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning is when the goal is to predict the label *yi*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Records and supervised learning](img/B04935_04_03F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *N* is the number of remaining attributes. In other words, the goal is
    to generalize the patterns so that we can predict the label by just knowing the
    other attributes, whether because we cannot physically get the measurement or
    just want to explore the structure of the dataset without having the immediate
    goal to predict the label.
  prefs: []
  type: TYPE_NORMAL
- en: The unsupervised learning is when we don't use the label—we just try to explore
    the structure and correlations to understand the dataset to, potentially, predict
    the label better. The number of problems in this latter category has increased
    recently with the emergence of learning for unstructured data and streams, each
    of which, I'll be covering later in the book in separate chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Iris dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I will demonstrate the concept of records and labels based on one of the most
    famous datasets in machine learning, the Iris dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)).
    The Iris dataset contains 50 records for each of the three types of Iris flower,
    150 lines of total five fields. Each line is a measurement of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the final field being the type of the flower (*setosa*, *versicolor*,
    or *virginica*). The classic problem is to predict the label, which, in this case,
    is a categorical attribute with three possible values as a function of the first
    four attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Iris dataset](img/B04935_04_04F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'One option would be to draw a plane in the four-dimensional space that separates
    all four labels. Unfortunately, as one can find out, while one of the classes
    is clearly separable, the remaining two are not, as shown in the following multidimensional
    scatterplot (we have used Data Desk software to create it):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Iris dataset](img/B04935_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 04-1\. The Iris dataset as a three-dimensional plot. The Iris setosa
    records, shown by crosses, can be separated from the other two types based on
    petal length and width.
  prefs: []
  type: TYPE_NORMAL
- en: 'The colors and shapes are assigned according to the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Label | Color | Shape |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *Iris setosa* | Blue | x |'
  prefs: []
  type: TYPE_TB
- en: '| *Iris versicolor* | Green | Vertical bar |'
  prefs: []
  type: TYPE_TB
- en: '| *Iris virginica* | Purple | Horizontal bar |'
  prefs: []
  type: TYPE_TB
- en: The *Iris setosa* is separable because it happens to have a very short petal
    length and width compared to the two other types.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how we can use MLlib to find that separating multidimensional plane.
  prefs: []
  type: TYPE_NORMAL
- en: Labeled point
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The labeled datasets used to have a very special place in ML—we will discuss
    unsupervised learning later in the chapter, where we do not need a label, so MLlib
    has a special data type to represent a record with a `org.apache.spark.mllib.regression.LabeledPoint`
    label (refer to [https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point](https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point)).
    To read the Iris dataset from a text file, we need to transform the original UCI
    repository file into the so-called LIBSVM text format. While there are plenty
    of converters from CSV to LIBSVM format, I''d like to use a simple AWK script
    to do the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Why do we need LIBSVM format?**'
  prefs: []
  type: TYPE_NORMAL
- en: LIBSVM is the format that many libraries use. First, LIBSVM takes only continuous
    attributes. While a lot of datasets in the real world contain discrete or categorical
    attributes, internally they are always converted to a numerical representation
    for efficiency reasons, even if the L1 or L2 metrics on the resulting numerical
    attribute does not make much sense in the unordered discrete values. Second, the
    LIBSVM format allows for efficient sparse data representation. While the Iris
    dataset is not sparse, almost all of the modern big data sources are sparse, and
    the format allows for efficient storage by only storing the provided values. Many
    modern big data key-value and traditional RDBMS databases actually do the same
    for efficiency reasons.
  prefs: []
  type: TYPE_NORMAL
- en: The code might be more complex for missing values, but we know that the Iris
    dataset is not sparse—otherwise we'd complement our code with a bunch of if statements.
    We mapped the last two labels to 1 for our purpose now.
  prefs: []
  type: TYPE_NORMAL
- en: SVMWithSGD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s run the **Linear Support Vector Machine** (**SVM**) SVMWithSGD
    code from MLlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'So, you just run one of the most complex algorithms in the machine learning
    toolbox: SVM. The result is a separating plane that distinguishes *Iris setosa*
    flowers from the other two types. The model in this case is exactly the intercept
    and the coefficients of the plane that best separates the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If one looks under the hood, the model is stored in a `parquet` file, which
    can be dumped using `parquet-tool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The **Receiver Operating Characteristic** (**ROC**) is a common measure of the
    classifier to be able to correctly rank the records according to their numeric
    label. We will consider precision metrics in more detail in [Chapter 9](ch09.xhtml
    "Chapter 9. NLP in Scala"), *NLP in Scala*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**What is ROC?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'ROC has emerged in signal processing with the first application to measure
    the accuracy of analog radars. The common measure of accuracy is area under ROC,
    which, shortly, is the probability of two randomly chosen points to be ranked
    correctly according to their labels (the *0* label should always have a lower
    rank than the *1* label). AUROC has a number of attractive characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The value, at least theoretically, does not depend on the oversampling rate,
    that is, the rate at which we see *0* labels as opposed to *1* labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value does not depend on the sample size, excluding the expected variance
    due to the limited sample size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a constant to the final score does not change the ROC, thus the intercept
    can always be set to *0*. Computing the ROC requires a sort with respect to the
    generated score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of course, separating the remaining two labels is a harder problem since the
    plane that separated *Iris versicolor* from *Iris virginica* does not exist: the
    AUROC score will be less than *1.0*. However, the SVM method will find the plane
    that best differentiates between the latter two classes.'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is one of the oldest classification methods. The outcome
    of the logistic regression is also a set of weights, which define the hyperplane,
    but the loss function is logistic loss instead of *L2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04935_04_05F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Logit function is a frequent choice when the label is binary (as *y = +/- 1*
    in the above equation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The labels in this case can be any integer in the range *[0, k)*, where *k*
    is the total number of classes (the correct class will be determined by building
    multiple binary logistic regression models against the pivot class, which in this
    case, is the class with the *0* label) (*The Elements of Statistical Learning*
    by *Trevor Hastie*, *Robert Tibshirani*, *Jerome Friedman*, *Springer Series in
    Statistics*).
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy metric is precision, or the percentage of records predicted correctly
    (which is 95% in our case).
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding two methods describe linear models. Unfortunately, the linear
    approach does not always work for complex interactions between attributes. Assume
    that the label looks like an exclusive *OR: 0* if *X ≠ Y* and *1* if *X = Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| X | Y | Label |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'There is no hyperplane that can differentiate between the two labels in the
    *XY* space. Recursive split solution, where the split on each level is made on
    only one variable or a linear combination thereof might work a bit better in these
    case. Decision trees are also known to work well with sparse and interaction-rich
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the error (misprediction) rate on hold-out 30% sample is only
    2.6%. The 30% sample of 150 is only 45 records, which means we missed only 1 record
    from the whole test set. Certainly, the result might and will change with a different
    seed, and we need a more rigorous cross-validation technique to prove the accuracy
    of the model, but this is enough for a rough estimate of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree generalizes on regression case, that is, when the label is continuous
    in nature. In this case, the splitting criterion is minimization of weighted variance,
    as opposed to entropy gain or gini in the case of classification. I will talk
    more about the differences in [Chapter 5](ch05.xhtml "Chapter 5. Regression and
    Classification"), *Regression and Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of parameters, which can be tuned to improve the performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Description | Recommended value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `maxDepth` | This is the maximum depth of the tree. Deep trees are costly
    and usually are more likely to overfit. Shallow trees are more efficient and better
    for bagging/boosting algorithms such as AdaBoost. | This depends on the size of
    the original dataset. It is worth experimenting and plotting the accuracy of the
    resulting tree versus the parameter to find out the optimum. |'
  prefs: []
  type: TYPE_TB
- en: '| `minInstancesPerNode` | This also limits the size of the tree: once the number
    of instances falls under this threshold, no further splitting occurs. | The value
    is usually 10-100, depending on the complexity of the original dataset and the
    number of potential labels. |'
  prefs: []
  type: TYPE_TB
- en: '| `maxBins` | This is used only for continuous attributes: the number of bins
    to split the original range. | Large number of bins increase computation and communication
    cost. One can also consider the option of pre-discretizing the attribute based
    on domain knowledge. |'
  prefs: []
  type: TYPE_TB
- en: '| `minInfoGain` | This is the amount of information gain (entropy), impurity
    (gini), or variance (regression) gain to split a node. | The default is *0*, but
    you can increase the default to limit the tree size and reduce the risk of overfitting.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `maxMemoryInMB` | This is the amount of memory to be used for collecting
    sufficient statistics. | The default value is conservatively chosen to be 256
    MB to allow the decision algorithm to work in most scenarios. Increasing `maxMemoryInMB`
    can lead to faster training (if the memory is available) by allowing fewer passes
    over the data. However, there may be decreasing returns as `maxMemoryInMB` grows,
    as the amount of communication on each iteration can be proportional to `maxMemoryInMB`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `subsamplingRate` | This is the fraction of the training data used for learning
    the decision tree. | This parameter is most relevant for training ensembles of
    trees (using `RandomForest` and `GradientBoostedTrees`), where it can be useful
    to subsample the original data. For training a single decision tree, this parameter
    is less useful since the number of training instances is generally not the main
    constraint. |'
  prefs: []
  type: TYPE_TB
- en: '| `useNodeIdCache` | If this is set to true, the algorithm will avoid passing
    the current model (tree or trees) to executors on each iteration. | This can be
    useful with deep trees (speeding up computation on workers) and for large random
    forests (reducing communication on each iteration). |'
  prefs: []
  type: TYPE_TB
- en: '| `checkpointDir:` | This is the directory for checkpointing the node ID cache
    RDDs. | This is an optimization to save intermediate results to avoid recomputation
    in case of node failure. Set it in large clusters or with unreliable nodes. |'
  prefs: []
  type: TYPE_TB
- en: '| `checkpointInterval` | This is the frequency for checkpointing the node ID
    cache RDDs. | Setting this too low will cause extra overhead from writing to HDFS
    and setting this too high can cause problems if executors fail and the RDD needs
    to be recomputed. |'
  prefs: []
  type: TYPE_TB
- en: Bagging and boosting – ensemble learning methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a portfolio of stocks has better characteristics compared to individual equities,
    models can be combined to produce better classifiers. Usually, these methods work
    really well with decision trees as the training technique can be modified to produce
    models with large variations. One way is to train the model on random subsets
    of the original data or random subsets of attributes, which is called random forest.
    Another way is to generate a sequence of models, where misclassified instances
    are reweighted to get a larger weight in each subsequent iteration. It has been
    shown that this method has a relation to gradient descent methods in the model
    parameter space. While these are valid and interesting techniques, they usually
    require much more space in terms of model storage and are less interpretable compared
    to bare decision tree models. For Spark, the ensemble models are currently under
    development—the umbrella issue is SPARK-3703 ([https://issues.apache.org/jira/browse/SPARK-3703](https://issues.apache.org/jira/browse/SPARK-3703)).
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we get rid of the label in the Iris dataset, it would be nice if some algorithm
    could recover the original grouping, maybe without the exact label names—*setosa*,
    *versicolor*, and *virginica*. Unsupervised learning has multiple applications
    in compression and encoding, CRM, recommendation engines, and security to uncover
    internal structure without actually having the exact labels. The labels sometimes
    can be given base on the singularity in attribute value distributions. For example,
    *Iris setosa* can be described as a *Flower with Small Leaves*.
  prefs: []
  type: TYPE_NORMAL
- en: While a supervised learning problem can always be cast as unsupervised by disregarding
    the label, the reverse is also true. A clustering algorithm can be cast as a density-estimation
    problem by assigning label *1* to all vectors and generating random vectors with
    label *0* (*The Elements of Statistical Learning* by *Trevor Hastie*, *Robert
    Tibshirani*, *Jerome Friedman*, *Springer Series in Statistics*). The difference
    between the two is formal and it's even fuzzier with non-structured and nested
    data. Often, running unsupervised algorithms in labeled datasets leads to a better
    understanding of the dependencies and thus a better selection and performance
    of the supervised algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most popular algorithms for clustering and unsupervised learning
    in k-means (and its variants, k-median and k-center, will be described later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: One can see that the first center, the one with index `0`, has petal length
    and width of `1.464` and `0.244`, which is much shorter than the other two—`5.715`
    and `2.054`, `4.389` and `1.434`). The prediction completely matches the first
    cluster, corresponding to *Iris setosa*, but has a few mispredictions for the
    other two.
  prefs: []
  type: TYPE_NORMAL
- en: 'The measure of cluster quality might depend on the (desired) labels if we want
    to achieve a desired classification result, but since the algorithm has no information
    about the labeling, a more common measure is the sum of distances from centroids
    to the points in each of the clusters. Here is a graph of `WSSSE`, depending on
    the number of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the average distance is decreasing as more clusters are configured.
    A common method to determine the optimal number of clusters—in our example, we
    know that there are three types of flowers—is to add a penalty function. A common
    penalty is the log of the number of clusters as we expect a convex function. What
    would be the coefficient in front of log? If each vector is associated with its
    own cluster, the sum of all distances will be zero, so if we would like a metric
    that achieves approximately the same value at both ends of the set of possible
    values, `1` to `150`, the coefficient should be `680.8244/log(150)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is how the sum of the squared distances with penalty looks as a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised learning](img/B04935_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 04-2\. The measure of the clustering quality as a function of the number
    of clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides k-means clustering, MLlib also has implementations of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian mixture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power Iteration Clustering** (**PIC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Dirichlet Allocation** (**LDA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming k-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Gaussian mixture is another classical mechanism, particularly known for
    spectral analysis. Gaussian mixture decomposition is appropriate, where the attributes
    are continuous and we know that they are likely to come from a set of Gaussian
    distributions. For example, while the potential groups of points corresponding
    to clusters may have the average for all attributes, say **Var1** and **Var2**,
    the points might be centered around two intersecting hyperplanes, as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised learning](img/B04935_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 04-3\. A mixture of two Gaussians that cannot be properly described by
    k-means clustering
  prefs: []
  type: TYPE_NORMAL
- en: This renders the k-means algorithm ineffective as it will not be able to distinguish
    between the two (of course a simple non-linear transformation such as a distance
    to one of the hyperplanes will solve the problem, but this is where domain knowledge
    and expertise as a data scientist are handy).
  prefs: []
  type: TYPE_NORMAL
- en: PIC is using clustering vertices of a graph provided pairwise similarity measures
    given as edge properties. It computes a pseudo-eigenvector of the normalized affinity
    matrix of the graph via power iteration and uses it to cluster vertices. MLlib
    includes an implementation of PIC using GraphX as its backend. It takes an RDD
    of (`srcId`, `dstId`, similarity) tuples and outputs a model with the clustering
    assignments. The similarities must be non-negative. PIC assumes that the similarity
    measure is symmetric. A pair (`srcId`, `dstId`) regardless of the ordering should
    appear at most once in the input data. If a pair is missing from the input, their
    similarity is treated as zero.
  prefs: []
  type: TYPE_NORMAL
- en: LDA can be used for clustering documents based on keyword frequencies. Rather
    than estimating a clustering using a traditional distance, LDA uses a function
    based on a statistical model of how text documents are generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, streaming k-means is a modification of the k-means algorithm, where
    the clusters can be adjusted with new batches of data. For each batch of data,
    we assign all points to their nearest cluster, compute new cluster centers based
    on the assignment, and then update each cluster parameters using the equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised learning](img/B04935_04_06F.jpg)![Unsupervised learning](img/B04935_04_07F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *c* *t* and *c'* *t* are the centers of from the old model and the ones
    computed for the new batch and *n* *t* and *n'* *t* are the number of vectors
    from the old model and for the new batch. By changing the *α* parameter, we can
    control how much information from the old runs can influence the clustering—*0*
    means the new cluster centers are totally based on the points in the new batch,
    while *1* means that we accommodate for all points that we have seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering has many modifications. For example, k-medians computes the
    cluster centers as medians of the attribute values, not mean, which works much
    better for some distributions and with *L1* target distance metric (absolute value
    of the difference) as opposed to *L2* (the sum of squares). K-medians centers
    are not necessarily present as a specific point in the dataset. K-medoids is another
    algorithm from the same family, where the resulting cluster center has to be an
    actual instance in the input set and we actually do not need to have the global
    sort, only the pairwise distances between the points. Many variations of the techniques
    exist on how to choose the original seed cluster centers and converge on the optimal
    number of clusters (besides the simple log trick I have shown).
  prefs: []
  type: TYPE_NORMAL
- en: Another big class of clustering algorithms is hierarchical clustering. Hierarchical
    clustering is either done from the top—akin to the decision tree algorithms—or
    from the bottom; we first find the closest neighbors, pair them, and continue
    the pairing process up the hierarchy until all records are merged. The advantage
    of hierarchical clustering is that it can be made deterministic and relatively
    fast, even though the cost of one iteration in k-means is probably going to be
    better. However, as mentioned, the unsupervised problem can actually be converted
    to a density-estimation supervised problem, with all the supervised learning techniques
    available. So have fun understanding the data!
  prefs: []
  type: TYPE_NORMAL
- en: Problem dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The larger the attribute space or the number of dimensions, the harder it is
    to usually predict the label for a given combination of attribute values. This
    is mostly due to the fact that the total number of possible distinct combinations
    of attributes increases exponentially with the dimensionality of the attribute
    space—at least in the case of discrete variables (in case of continuous variables,
    the situation is more complex and depends on the metrics used), and it is becoming
    harder to generalize.
  prefs: []
  type: TYPE_NORMAL
- en: The effective dimensionality of the problem might be different from the dimensionality
    of the input space. For example, if the label depends only on the linear combination
    of the (continuous) input attributes, the problem is called linearly separable
    and its internal dimensionality is one—we still have to find the coefficients
    for this linear combination like in logistic regression though.
  prefs: []
  type: TYPE_NORMAL
- en: This idea is also sometimes referred to as a **Vapnik–Chervonenkis** (**VC**)
    dimension of a problem, model, or algorithm—the expressive power of the model
    depending on how complex the dependencies that it can solve, or shatter, might
    be. More complex problems require algorithms with higher VC dimensions and larger
    training sets. However, using an algorithm with higher VC dimension on a simple
    problem can lead to overfitting and worse generalization to new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the units of input attributes are comparable, say all of them are meters
    or units of time, PCA, or more generally, kernel methods, can be used to reduce
    the dimensionality of the input space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, we reduced the original four-dimensional problem to two-dimensional. Like
    averaging, computing linear combinations of input attributes and selecting only
    those that describe most of the variance helps to reduce noise.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at supervised and unsupervised learning and a few
    examples of how to run them in Spark/Scala. We considered SVM, logistic regression,
    decision tree, and k-means in the example of UCI Iris dataset. This is in no way
    a complete guide, and many other libraries either exist or are being made as we
    speak, but I would bet that you can solve 99% of the immediate data analysis problems
    just with these tools.
  prefs: []
  type: TYPE_NORMAL
- en: This will give you a very fast shortcut on how to start being productive with
    a new dataset. There are many other ways to look at the datasets, but before we
    get into more advanced topics, let's discuss regression and classification in
    the next chapter, that is, how to predict continuous and discrete labels.
  prefs: []
  type: TYPE_NORMAL
