- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon SageMaker Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, you learned several methods of model optimization
    and evaluation techniques. You also learned various ways of storing data, processing
    data, and applying different statistical approaches to data. So, how can you now
    build a pipeline for this? Well, you can read data, process data, and build **machine
    learning (ML)** models on the processed data. But what if my first ML model does
    not perform well? Can I fine-tune my model? The answer is *yes*; you can do nearly
    everything using Amazon SageMaker. In this chapter, you will walk you through
    the following topics using Amazon SageMaker:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding different instances of Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning and preparing data in Jupyter Notebook in Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training in Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SageMaker’s built-in ML algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing custom training and inference code in SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the data used in this chapter’s examples from GitHub at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: Creating notebooks in Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are working with ML, then you need to perform actions such as storing
    data, processing data, preparing data for model training, model training, and
    deploying the model for inference. They are complex, and each of these stages
    requires a machine to perform the task. With Amazon SageMaker, life becomes much
    easier when carrying out these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: What is Amazon SageMaker?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker provides training instances to train a model using the data and provides
    endpoint instances to infer by using the model. It also provides notebook instances
    running on the Jupyter Notebook to clean and understand the data. If you are happy
    with your cleaning process, then you should store the cleaned data in S3 as part
    of the staging for training. You can launch training instances to consume this
    training data and produce an ML model. The ML model can be stored in S3, and endpoint
    instances can consume the model to produce results for end users.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you draw this in a block diagram, then it will look similar to *Figure 9**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – A pictorial representation of the different layers of the Amazon
    SageMaker instances](img/B21197_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – A pictorial representation of the different layers of the Amazon
    SageMaker instances
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you will take a look at the Amazon SageMaker console and get a better
    feel for it. Once you log in to your AWS account and go to Amazon SageMaker, you
    will see something similar to *Figure 9**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – A quick look at the SageMaker console](img/B21197_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – A quick look at the SageMaker console
  prefs: []
  type: TYPE_NORMAL
- en: There are three different sections in the menu on the left, labeled **Notebook**,
    **Training**, and **Inference**, that have been expanded in *Figure 9**.2* so
    that you can dive in and understand them better.
  prefs: []
  type: TYPE_NORMAL
- en: '**Notebook** has three different options that you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Notebook instances**: This helps you create, open, start, and stop notebook
    instances. These instances are responsible for running Jupyter Notebooks. They
    allow you to choose the instance type based on the workload of the use case. The
    best practice is to use a notebook instance to orchestrate the data pipeline for
    processing a large dataset. For example, making a call from a notebook instance
    to AWS Glue for ETL services or Amazon EMR to run Spark applications. If you are
    asked to create a secure notebook instance outside AWS, then you need to take
    care of endpoint security, network security, launching the machine, managing storage
    on it, and managing Jupyter Notebook applications running on the instance. The
    user does not need to manage any of these with SageMaker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip install` or `conda install`. However, as soon as the notebook instance
    is terminated, the customization will be lost. To avoid such a scenario, you can
    customize your notebook instance through a script provided through `/home/ec2-user/anaconda3/envs/`
    and customize the specific environment as required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Git repositories**: AWS CodeCommit, GitHub, or any other Git server can be
    associated with the notebook instance for the persistence of your notebooks. If
    access is given, then the same notebook can be used by other developers to collaborate
    and save code in a source-control fashion. Git repositories can either be added
    separately using this option or they can be associated with a notebook instance
    during the creation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 9**.2*, **Training** offers **Algorithms**, **Training**
    **jobs**, and **Hyperparameter** **tuning** **jobs**. Let’s understand their usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithms**: This is the first step toward deciding on an algorithm that
    you are going to run on our cleaned data. You can either choose a custom algorithm
    or create a custom algorithm based on the use case. Otherwise, you can run SageMaker
    algorithms on the cleaned data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training jobs**: You can create training jobs from a notebook instance via
    API calls. You can set the number of instances, input the data source details,
    perform checkpoint configuration, and output data configuration. Amazon SageMaker
    manages the training instances and stores the model artifacts as output in the
    specified location. Both incremental training (that is, to train the model from
    time to time for better results) and managed spot training (that is, to reduce
    costs) can also be achieved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning jobs**: Usually, hyperparameters are set for an algorithm
    prior to the training process. During the training process, you let the algorithm
    figure out the best values for these parameters. With hyperparameter tuning, you
    obtain the best model that has the best value of hyperparameters. This can be
    done through a console or via API calls. The same can be orchestrated from a notebook
    instance too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference** has many offerings and is evolving every day:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compilation jobs**: If your model is trained using an ML framework such as
    Keras, MXNet, ONNX, PyTorch, TFLite, TensorFlow, or XGBoost, and your model artifacts
    are available on a S3 bucket, then you can choose either **Target device** or
    **Target platform**. The Target device option is used to specify where you will
    deploy your model, such as an AWS SageMaker ML instance or an AWS IoT Greengrass
    device. The Target platform option is used to decide the operating system, architecture,
    and accelerator on which you want your model to run. You can also store the compiled
    module in your S3 bucket for future use. This essentially helps you in cross-platform
    model deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model packages**: These are used to create deployable SageMaker models. You
    can create your own algorithm, package it using the model package APIs, and publish
    it to AWS Marketplace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models**: Models are created using model artifacts. They are similar to mathematical
    equations with variables; that is, you input the values for the variables and
    get an output. These models are stored in S3 and will be used for inference by
    the endpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VariantWeight` API to make the endpoints serve 80% of the requests with the
    old model and 20% of the requests with the new model. This is the most common
    production scenario where the data changes rapidly and the model needs to be trained
    and tuned periodically. Another possible use case is to test the model results
    with live data, then a certain percentage of the requests can be routed to the
    new model, and the results can be monitored to ascertain the accuracy of the model
    on real-time unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Endpoints**: These are used to create a URL to which the model is exposed
    and can be requested to give the model results as a response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InputFilter`, `JoinSource`, and `OutputFilter` APIs can be used to associate
    input records with output results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have got an overview of Amazon SageMaker. Now, put your knowledge to work
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Amazon SageMaker console keeps changing. There’s a possibility that when
    you are reading this book, the console might look different.
  prefs: []
  type: TYPE_NORMAL
- en: Training Data Location and Formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you embark on the journey of setting up your AWS SageMaker training job,
    understanding the diverse data storage and reading options is crucial. To ensure
    a seamless training experience, delve into the supported options and their benefits.
  prefs: []
  type: TYPE_NORMAL
- en: 'First you will look at the **supported data** **storage options**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Simple Storage Service (****Amazon S3):**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overview**: Amazon SageMaker provides robust support for storing training
    datasets in Amazon S3, offering reliability and scalability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage Example**: You can configure your dataset using an Amazon S3 prefix,
    manifest file, or augmented manifest file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Elastic File System (****Amazon EFS):**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overview**: SageMaker extends its support to Amazon EFS, facilitating file
    system access to the dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage Example**: Data stored in Amazon EFS must be pre-existing before initiating
    the training job.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon FSx** **for Lustre:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overview**: Achieving high throughput and low-latency file retrieval, SageMaker
    mounts the FSx for Lustre file system to the training instance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage Example**: FSx for Lustre can scale seamlessly, providing a performant
    option for your training data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Here are the **input modes for** **data access:**
  prefs: []
  type: TYPE_NORMAL
- en: '**File Mode:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overview**: Default input mode where SageMaker downloads the entire dataset
    to the Docker container before training starts.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage Example**: Compatible with SageMaker local mode and supports sharding
    for distributed training.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast** **File Mode:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overview**: Combining file system access with the efficiency of pipe mode,
    fast file mode identifies data files at the start but delays the download until
    necessary.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage Example**: Streamlines training startup time, particularly beneficial
    when dealing with a large dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipe Mode:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overview**: Streams data directly from an Amazon S3 data source, providing
    faster start times and better throughput.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage Example**: Historically used, but largely replaced by the simpler-to-use
    fast file mode.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And lastly, look at the **specialized** **storage classes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon S3 Express** **One Zone:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overview**: A high-performance, single Availability Zone storage class, optimizing
    compute performance and costs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage Example**: Supports file mode, fast file mode, and pipe mode for SageMaker
    model training.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon EFS and Amazon FSx** **for Lustre:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overview**: SageMaker supports both Amazon EFS and Amazon FSx for Lustre,
    offering flexibility in choosing the right storage solution for your training
    data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage Example**: Mounting the file systems to the training instance ensures
    seamless access during training.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the nuances of data storage and reading options for AWS SageMaker
    training jobs empowers you to tailor your setup to specific requirements. In the
    upcoming sections, you’ll explore more facets of AWS SageMaker to deepen your
    understanding and proficiency in machine learning workflows. Let’s put our knowledge
    to work in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon SageMaker notebook instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The very first step, in this section, is to create a Jupyter Notebook, and
    this requires a notebook instance. You can start by creating a notebook instance,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sign in to your AWS account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to `Services >` `Amazon SageMaker`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the left navigation pane, click on **Notebook instances** and then click
    on the **Create notebook** **instance** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a `notebookinstance` and leave the `ml.t2.medium` setting. In the `Create
    a new role` in **IAM role**. You will be asked to specify the bucket name. For
    the purpose of this example, it’s chosen as any bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Following the successful creation of a role, you should see something similar
    to *Figure 9**.3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Amazon SageMaker role creation](img/B21197_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Amazon SageMaker role creation
  prefs: []
  type: TYPE_NORMAL
- en: Leave everything else on their default settings and click on the **Create notebook**
    **instance** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the instance is in the `InService` state, select the instance. Click on
    the **Actions** drop-down menu and choose **Open Jupyter**. This opens your Jupyter
    Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, you are all set to run our Jupyter Notebook on the newly created instance.
    You will perform **Exploratory Data Analysis (EDA)** and plot different types
    of graphs to visualize the data. Once you are familiar with the Jupyter Notebook,
    you will build some models to predict house prices in Boston. You will apply the
    algorithms that you have learned in previous chapters and compare them to find
    the best model that offers the best prediction according to our data. Let’s dive
    in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the Jupyter Notebook, click on **New** and select **Terminal**. Run the
    following commands in Command Prompt to download the code to the instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the Git repository is cloned to the SageMaker notebook instance, type `exit`
    into Command Prompt to quit. Now, your code is ready to execute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to `Chapter-9` in the Jupyter Notebook’s Files section, as shown in
    *Figure 9**.4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Jupyter Notebook](img/B21197_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Jupyter Notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the first notebook in `1.Boston-House-Price-SageMaker-Notebook-Instance-Example.ipynb`.
    It will prompt you to choose the kernel for the notebook. Please select `conda_python3`,
    as shown in *Figure 9**.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Jupyter Notebook kernel selection](img/B21197_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Jupyter Notebook kernel selection
  prefs: []
  type: TYPE_NORMAL
- en: From the notebook, navigate to `Kernel > Restart & Clear Output`. Click on the
    play icon to run the cells one after another. Please ensure you have run each
    individual cell and inspect the output from each execution/run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can experiment by adding cells and deleting cells to familiarize yourself
    with the Jupyter Notebook operations. In one of the paragraphs, there is a bash
    command that allows you to install the `xgboost` libraries from the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final cell explains how you have compared the different scores of various
    modeling techniques to draw a conclusion mathematically. *Figure 9**.6* clearly
    shows that the best model for predicting house prices in Boston is XGBoost:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Comparing the models](img/B21197_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Comparing the models
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve completed the execution of this notebook, please feel free to shut
    down the kernel and stop your notebook instance from the SageMaker console. This
    is a best practice to reduce costs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next hands-on section, you will familiarize ourselves with Amazon SageMaker’s
    training and inference instances. You will also use the Amazon SageMaker API to
    make this process easier. You will use the same notebook instance as you did in
    the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon SageMaker’s training and inference instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will learn about training a model and hosting the model
    to generate its predicted results. Let’s dive in by using the notebook instance
    from the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: Sign in to your AWS account at [https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on `notebookinstance`. Once the status moves to `InService`, open it
    in a new tab, as shown in *Figure 9**.7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.7 – The InService instance](img/B21197_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – The InService instance
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the tab named **SageMaker Examples** from the Jupyter Notebook home
    page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the `k_nearest_neighbors_covtype.ipynb` notebook. Click on **Use** and
    create a copy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you run the following code block, as shown in *Figure 9**.8*, you can
    also check a training job in `Training > Training jobs` of the SageMaker home
    page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.8 – The SageMaker fit API call](img/B21197_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – The SageMaker fit API call
  prefs: []
  type: TYPE_NORMAL
- en: 'The training job looks similar to *Figure 9**.9*. It launches an ECS container
    in the backend and uses the IAM execution role created in the previous example
    to run the training job for this request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Training jobs](img/B21197_09_09_New.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Training jobs
  prefs: []
  type: TYPE_NORMAL
- en: If you go inside and check the logs in CloudWatch, it gives you more details
    about the containers and the steps they performed. As an ML engineer, it’s worth
    going in and checking the CloudWatch metrics for your algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, if you run the following paragraph, as shown in *Figure 9**.10*, in the
    notebook, then it will create an endpoint configuration and an endpoint where
    the model from the earlier training job is deployed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I have changed the instance type to reduce costs. It is the instance or the
    machine that will host your model. Please choose your instance wisely. You will
    learn about choosing instance types in the next section. I have also changed `endpoint_name`
    so that it can be recognized easily:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Creating the predictor object with endpoint details](img/B21197_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Creating the predictor object with endpoint details
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to `Inference > Endpoints`. This will show you the endpoint that was
    created as a result of the previous paragraph’s execution. This endpoint has a
    configuration and can be navigated and traced through `Inference >` `Endpoint
    Configurations`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you view the **Inference** section in the notebook, you will notice that
    it uses the test data to predict results. It uses the predictor object from the
    SageMaker API to make predictions. The predictor object contains the endpoint
    details, model name, and instance type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The API call to the endpoint occurs in the **Inference** section and is authenticated
    via the IAM role with which the notebook instance is created. The same API calls
    can be traced through CloudWatch invocation metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, running the `delete_endpoint` method in the notebook will delete the
    endpoint. To delete the endpoint configurations, navigate to `Inference > Endpoint
    Configurations` and select the configuration on the screen. Click on `Actions
    > Delete >` `Delete`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, please feel free to shut down the kernel and stop your notebook instance
    from the SageMaker console. This is a best practice to reduce costs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, you learned how to use the notebook instance, training instances,
    inference endpoints, and endpoint configurations to clean our data, train models,
    and generate predicted results from them. In the next section, you will learn
    about model tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Model tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21197_07.xhtml#_idTextAnchor970)*, Evaluating and Optimizing
    Models*, you learned many important concepts about model tuning. Let’s now explore
    this topic from a practical perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to tune a model on SageMaker, you have to call `create_hyper_parameter_tuning_job`
    and pass the following main parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`HyperParameterTuningJobName`: This is the name of the tuning job. It is useful
    to track the training jobs that have been started on behalf of your tuning job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HyperParameterTuningJobConfig`: Here, you can configure your tuning options.
    For example, which parameters you want to tune, the range of values for them,
    the type of optimization (such as random search or Bayesian search), the maximum
    number of training jobs you want to spin up, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TrainingJobDefinition`: Here, you can configure your training job. For example,
    the data channels, the output location, the resource configurations, the evaluation
    metrics, and the stop conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In SageMaker, the main metric that you want to use to evaluate the models to
    select the best one is known as an **objective metric**.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, you are configuring `HyperParameterTuningJobConfig`
    for a decision tree-based algorithm. You want to check the best configuration
    for a `max_depth` hyperparameter, which is responsible for controlling the depth
    of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `IntegerParameterRanges`, you have to specify the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The hyperparameter name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum value that you want to test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum value that you want to test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Each type of hyperparameter must fit in one of the parameter range sections,
    such as categorical, continuous, or integer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In `ResourceLimits`, you are specifying the number of training jobs along with
    the number of parallel jobs that you want to run. Remember that the goal of the
    tuning process is to execute many training jobs with different hyperparameter
    settings. This is so that the best one will be selected for the final model. That’s
    why you have to specify these training job execution rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'You then set up our search strategy in `Strategy` and, finally, set up the
    objective function in `HyperParameterTuningJobObjective`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The second important configuration you need to set is `TrainingJobDefinition`.
    Here, you have to specify all the details regarding the training jobs that will
    be executed. One of the most important settings is the `TrainingImage` setting,
    which refers to the container that will be started to execute the training processes.
    This container, as expected, must have your training algorithm implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you present an example of a built-in algorithm, eXtreme Gradient Boosting,
    so that you can set the training image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can go ahead and set your training definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you have to specify the data input configuration, which is also known
    as the data channels. In the following section of code, you are setting up two
    data channels – train and validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'You also need to specify where the results will be stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you set the resource configurations, roles, static parameters, and
    stopping conditions. In the following section of code, you want to use two instances
    of type `ml.c4.2xlarge` with 10 GB of storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Please note that you are using other variables in this configuration file, `bucket`
    and `prefix`, which should be replaced by your bucket name and prefix key (if
    needed), respectively. You are also referring to `s3_input_train` and `s3_input_validation`,
    which are two variables that point to the train and validation datasets in S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have set your configurations, you can spin up the tuning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s find out how to track the execution of this process.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking your training jobs and selecting the best model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have started the tuning process, there are two additional steps that
    you might want to check: tracking the process of tuning and selecting the winner
    model (that is, the one with the best set of hyperparameters).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to find your training jobs, you should go to the SageMaker console
    and navigate to Hyperparameter training jobs. You will then find a list of executed
    tuning jobs, including yours:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Finding your tuning job](img/B21197_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Finding your tuning job
  prefs: []
  type: TYPE_NORMAL
- en: 'If you access your tuning job, by clicking under its name, you will find a
    summary page, which includes the most relevant information regarding the tuning
    process. On the **Training jobs** tab, you will see all the training jobs that
    have been executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Summary of the training jobs in the tuning process](img/B21197_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Summary of the training jobs in the tuning process
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if you click on the **Best training job** tab, you will find the best
    set of hyperparameters for your model, including a handy button for creating a
    new model based on those best hyperparameters that have just been found:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Finding the best set of hyperparameters](img/B21197_09_13_New.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Finding the best set of hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, SageMaker is very intuitive, and once you know the main concepts
    behind model optimization, playing with SageMaker should be easier. Now, you understand
    how to use SageMaker for our specific needs. In the next section, you will explore
    how to select the instance type for various use cases and the security of our
    notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing instance types in Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SageMaker uses a pay-for-usage model. There is no minimum fee for it.
  prefs: []
  type: TYPE_NORMAL
- en: When you think about instances on SageMaker, it all starts with an EC2 instance.
    This instance is responsible for all your processing. It’s a managed EC2 instance.
    These instances won’t show up in the EC2 console and cannot be SSHed either. The
    names of this instance type start with `ml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker offers instances of the following families:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **t** family: This is the burstable CPU family. With this family, you get
    a balanced ratio of CPU and memory. This means that if you have a long-running
    training job, then you lose performance over time as you spend the CPU credits.
    If you have very small jobs, then they are cost-effective. For example, if you
    want a notebook instance to launch training jobs, then this family is the most
    appropriate and cost-effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **m** family: In the previous family, you saw that CPU credits are consumed
    faster due to their burstable nature. If you have a long-running ML job that requires
    constant throughput, then this is the right family. It comes with a similar CPU
    and memory ratio as the **t** family.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **r** family: This is a memory-optimized family. *When do you need this?*
    Well, imagine a use case where you have to load the data in memory and do some
    data engineering on the data. In this scenario, you will require more memory and
    your job will be memory-optimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **c** family: c-family instances are compute-optimized. This is a requirement
    for jobs that need higher compute power and less memory to store the data. If
    you refer to the following table, c5.2x large has 8 vCPU and 16 GiB memory, which
    makes it compute-optimized with less memory. For example, if a use case needs
    to be tested on fewer records and it is compute savvy, then this instance family
    is the to-go option to get some sample records from a huge **DataFrame** and test
    your algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **p** family: This is a GPU family that supports accelerated computing
    jobs such as training and inference. Notably, **p**-family instances are ideal
    for handling large, distributed training jobs that result in less time required
    for training and are thus much more cost-effective. The p3/p3dn GPU compute instance
    can go up to 1 petaFLOP per second compute with up to 256 GB of GPU memory and
    100 Gbps (gigabits) of networking with 8x NVIDIA v100 GPUs. They are highly optimized
    for training and are not fully utilized for inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **g** family: For cost-effective, small-scale training jobs, **g**-family
    GPU instances are ideal. G4 has the lowest cost per inference for GPU instances.
    It uses T4 NVIDIA GPUs. The G4 GPU compute instance goes up to 520 TeraFLOPs of
    compute time with 8x NVIDIA T4 GPUs. This instance family is the best for simple
    networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following table, you have a visual comparison between the CPU and memory
    ratio of 2x large instance types from each family:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **t3.2x large** | **m5.2x large** | **r5.2x large** | **c5.2x large** | **p3.2x
    large** | **g4dn.2x large** |'
  prefs: []
  type: TYPE_TB
- en: '| 8 vCPU, 32 GiB | 8 vCPU, 32 GiB | 8 vCPU, 64 GiB | 8 vCPU, 16 GiB | 8 vCPU,
    61 GiB | 8 vCPU, 32 GiB |'
  prefs: []
  type: TYPE_TB
- en: Table 9.1 – A table showing the CPU and memory ratio of different instance types
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: To remember this easily, you can think of t for Tiny, m for Medium, c for Compute,
    and p and g for GPU. The CPU-related family instance types are t, m, r, and c.
    The GPU-related family instance types are p and g.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right instance type for a training job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is no rule of thumb to determine the instance type that you require. It
    changes based on the size of the data, the complexity of the network, the ML algorithm
    in question, and several other factors such as time and cost. Asking the right
    questions will allow you to save money and make your project cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: If the deciding factor is *instance size*, then classifying the problem as one
    for CPUs or GPUs is the right step. Once that is done, then it is good to consider
    whether it could be multi-GPU or multi-CPU, answering the question about distributed
    training. This also solves your *instance count* factor. If it’s compute intensive,
    then it would be wise to check the memory requirements too.
  prefs: []
  type: TYPE_NORMAL
- en: The next deciding factor is the *instance family*. The right question here is,
    *is the chosen instance optimized for time and cost?* In the previous step, you
    figured out whether the problem can be solved best by either a CPU or GPU, and
    this narrows down the selection process. Now, let’s learn about inference jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right instance type for an inference job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The majority of the cost and complexity of ML in production is inference. Usually,
    inference runs on a single input in real time. Inference jobs are usually less
    compute/memory-intensive. They have to be highly available as they run all the
    time and serve end-user requests or are integrated into a wider application.
  prefs: []
  type: TYPE_NORMAL
- en: You can choose any of the instance types that you learned about so far based
    on the given workload. Other than that, AWS has **Inf1** and **Elastic Inference**
    type instances for inference. Elastic inference allows you to attach a fraction
    of a GPU instance to any CPU instance.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example where an application is integrated with inference jobs.
    In this case, the CPU and memory requirements for the application are different
    from the inference jobs’ CPU and memory requirements. For use cases such as this,
    you need to choose the right instance type and size. In such scenarios, it is
    good to have a separation between your application fleets and inference fleets.
    This might require some management. If such management is a problem for your requirement,
    then choose Elastic Inference, where the application and inference jobs can be
    colocated. This means that you can host multiple models on the same fleet, and
    you can load all of these different models on different accelerators in memory,
    and concurrent requests can be served.
  prefs: []
  type: TYPE_NORMAL
- en: It’s always recommended that you run some examples in a lower environment before
    deciding on your instance types and family in the production environment. For
    Production environments, you need to manage your scalability configurations for
    your Amazon SageMaker hosted models. You will understand this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Taking care of Scalability Configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To kickstart auto scaling for your model, you can take advantage of the SageMaker
    console, **AWS Command Line Interface (AWS CLI)**, or an **AWS SDK** through the
    **Application Auto Scaling API**. For those inclined towards the CLI or API, the
    process involves registering the model as a scalable target, defining the scaling
    policy, and then applying it. If you opt for the SageMaker console, simply navigate
    to **Endpoints** under **Inference** in the navigation pane, locate your model’s
    endpoint name, and choose it along with the variant name to activate auto scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now dive into the intricacies of scaling policies.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Policy Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Auto scaling is driven by scaling policies, which determine how instances are
    added or removed in response to varying workloads. Two options are at your disposal:
    target tracking and step scaling policies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Target Tracking Scaling Policies: Our recommendation is to leverage target
    tracking scaling policies. Here, you select a CloudWatch metric and set a target
    value. Auto scaling takes care of creating and managing CloudWatch alarms, adjusting
    the number of instances to maintain the metric close to the specified target value.
    For instance, a scaling policy targeting the InvocationsPerInstance metric with
    a target value of 70 ensures the metric hovers around that value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step Scaling Policies: Step scaling is for advanced configurations, allowing
    you to specify instance deployment under specific conditions. However, for simplicity
    and full automation, target tracking scaling is preferred. Note that step scaling
    is managed exclusively through the AWS CLI or Application Auto Scaling API.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a target tracking scaling policy involves specifying the metric, such
    as the average number of invocations per instance, and the target value, for example,
    70 invocations per instance per minute. You have the flexibility to create target
    tracking scaling policies based on predefined or custom metrics. Cooldown periods,
    which prevent rapid capacity fluctuations, can also be configured optionally.
  prefs: []
  type: TYPE_NORMAL
- en: Scale Based on a Schedule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scheduled actions enable scaling activities at specific times, either as a one-time
    event or on a recurring schedule. These actions can work in tandem with your scaling
    policy, allowing dynamic decisions based on changing workloads. Scheduled scaling
    is managed exclusively through the AWS CLI or Application Auto Scaling API.
  prefs: []
  type: TYPE_NORMAL
- en: Minimum and Maximum Scaling Limits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before crafting a scaling policy, it’s essential to set minimum and maximum
    scaling limits. The minimum value, set to at least 1, represents the minimum number
    of instances, while the maximum value signifies the upper cap. SageMaker auto
    scaling adheres to these limits and automatically scales in to the minimum specified
    instances when traffic becomes zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have three options to specify these limits:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the console to update the Minimum instance count and Maximum instance count
    settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the AWS CLI, including the --min-capacity and --max-capacity options with
    the register-scalable-target command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call the RegisterScalableTarget API, specifying the MinCapacity and MaxCapacity
    parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cooldown Period
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The cooldown period is pivotal for preventing over-scaling during scale-in or
    scale-out activities. It slows down subsequent scaling actions until the period
    expires, safeguarding against rapid capacity fluctuations. You can configure the
    cooldown period within your scaling policy.
  prefs: []
  type: TYPE_NORMAL
- en: If not specified, the default cooldown period is 300 seconds for both scale-in
    and scale-out. Adjust this value based on your model’s traffic characteristics;
    consider increasing for frequent spikes or multiple scaling policies, and decrease
    if instances need to be added swiftly.
  prefs: []
  type: TYPE_NORMAL
- en: As you embark on optimizing your model’s scalability, keep these configurations
    in mind to ensure a seamless and cost-effective experience. In the next section,
    you will dive into and understand the different ways of securing our Amazon SageMaker
    notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Securing SageMaker notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are reading this section of the chapter, then you have already learned
    how to use notebook instances, which type of training instances should be chosen,
    and how to configure and use endpoints. Now, let’s learn about securing those
    instances. The following aspects will help to secure the instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encryption**: When you talk about securing something via encryption, you
    are talking about safeguarding data. But what does this mean? It means protecting
    data at rest using encryption, protecting data in transit with encryption, and
    using KMS for better role separation and internet traffic privacy through TLS
    1.2 encryption. SageMaker instances can be launched with encrypted volumes by
    using an AWS-managed KMS key. This helps you to secure the Jupyter Notebook server
    by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RootAccess` field to `Disabled` when you call `CreateNotebookInstance` or
    `UpdateNotebookInstance`. The data scientist will have access to their user space
    and can install Python packages. However, they cannot sudo into the root user
    and make changes to the operating system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IAM role**: During the launch of a notebook instance, it is necessary to
    create an IAM role for execution or to use an existing role for execution. This
    is used to launch the service-managed EC2 instance with an instance profile associated
    with the role. This role will restrict the API calls based on the policies attached
    to this role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VPC connection**: When you launch a SageMaker notebook instance, by default,
    it gets created within the SageMaker service account, which has a service-managed
    VPC, and it will, by default, have access to the internet via an internet gateway,
    and that gateway is managed by the service. If you are only dealing with AWS-related
    services, then it is recommended that you launch a SageMaker notebook instance
    in your VPC within a private subnet and with a well-customized security group.
    The AWS services can be invoked or used from this notebook instance via VPC endpoints
    attached to that VPC. The best practice is to control them via endpoint policies
    for better API controls. This enforces the restriction on data egress outside
    your VPC and secured environment. In order to capture all network traffic, you
    can turn on the VPC flow logs, which can be monitored and tracked via CloudWatch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EnableNetworkIsolation` parameter to `True` when you call `CreateTrainingJob`,
    `CreateHyperParameterTuningJob`, or `CreateModel`. Network isolation can be used
    along with the VPC, which ensures that containers cannot make any outbound network
    calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connecting a private network to your VPC**: You can launch your SageMaker
    notebook instance inside the private subnet of your VPC. This can access data
    from your private network by communicating with the private network, which can
    be done by connecting your private network to your VPC by using Amazon VPN or
    AWS Direct Connect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you learned several ways in which you can secure our SageMaker
    notebooks. In the next section, you will learn about SageMaker Debugger.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Debugger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn about Amazon SageMaker Debugger, unraveling
    the intricacies of monitoring, profiling, and debugging ML model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitoring and profiling**: SageMaker Debugger captures model metrics and
    keeps a real-time eye on system resources during training, eliminating the need
    for additional code. It not only provides a window into the training process but
    empowers instant issue correction, expediting training and elevating model quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic detection and analysis**: A true time-saver, Debugger automatically
    spots and notifies you of common training errors, such as oversized or undersized
    gradient values. Say goodbye to days of troubleshooting; Debugger reduces it to
    mere hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Profiling capabilities**: Venture into the realm of profiling with Debugger,
    which meticulously monitors system resource utilization metrics and allows you
    to profile training jobs. This involves collecting detailed metrics from your
    ML framework, identifying anomalies in resource usage, and swiftly pinpointing
    bottlenecks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Built-in analysis and actions**: Debugger introduces built-in analysis rules
    that tirelessly examine the training data emitted, encompassing input, output,
    and transformations (tensors). But that’s not all—users have the freedom to craft
    custom rules, analyze specific conditions, and even dictate actions triggered
    by rule events, such as stopping training or sending notifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with SageMaker Studio**: It is possible to visualize Debugger
    results seamlessly within SageMaker Studio, treating yourself to charts depicting
    CPU utilization, GPU activity, network usage, and more. There is also a heat map,
    offering a visual timeline of system resource utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Profiler output**: Peek into profiling results, an exhaustive dossier on
    system resource usage covering GPU, CPU, network, memory, and I/O. It’s your one-stop
    shop for understanding the inner workings of your training job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Debugger insights and optimization**: Beyond detection, Debugger evolves
    into an advisor, identifying issues in your training jobs, providing insights,
    and suggesting optimizations. Whether it’s tweaking the batch size or altering
    the distributed training strategy, Debugger guides you towards optimal performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CloudWatch integration**: Stay in the loop with Debugger’s integration with
    CloudWatch. Configure alerts for specific conditions and ensure you are always
    ahead of potential hiccups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Downloadable reports**: Don’t miss a beat—download HTML reports summarizing
    Debugger’s insights and profiling results for thorough offline analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a nutshell, Amazon SageMaker Debugger emerges as a holistic toolkit, empowering
    you to monitor, profile, and debug your ML models with finesse. It’s not just
    a tool; it’s your ally in the journey to model optimization. In the next section,
    you will understand the usage of SageMaker AutoPilot/AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Autopilot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML model development has historically been a daunting task, demanding considerable
    expertise and time. Amazon SageMaker Autopilot emerges as a game-changer, simplifying
    this intricate process and transforming it into a streamlined experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon SageMaker Autopilot presents a rich array of features to facilitate
    the development of ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automatic model building**: SageMaker Autopilot removes the complexities
    of constructing ML models by taking charge and automating the entire process with
    a simple mandate from the user: provide a tabular dataset and designate the target
    column for prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data processing and enhancement**: Autopilot seamlessly handles data preprocessing
    tasks, filling in missing data, offering statistical insights into dataset columns,
    and extracting valuable information from non-numeric columns. This guarantees
    that input data is finely tuned for model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Problem type detection**: Autopilot showcases intelligence by automatically
    detecting the problem type—whether it’s classification or regression—based on
    the characteristics of the provided data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm exploration and optimization**: Users can explore a myriad of high-performing
    algorithms, with Autopilot efficiently training and optimizing hundreds of models
    to pinpoint the one that aligns best with the user’s requirements. The entire
    process is automated, lifting the burden off the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-world examples**: Picture a retail company aiming to predict customer
    purchasing behavior. With SageMaker Autopilot, the company inputs historical purchase
    data, designates the target variable (e.g., whether a customer makes a purchase
    or not), and Autopilot takes the reins, autonomously exploring and optimizing
    various ML models. This facilitates deploying a predictive model without the need
    for profound ML expertise. In another scenario, a financial institution assessing
    credit risk can leverage SageMaker Autopilot. By providing a dataset with customer
    information and credit history, and specifying the target variable (creditworthiness),
    the institution can harness Autopilot to automatically build, train, and optimize
    models for precise credit risk prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model understanding and deployment**: SageMaker Autopilot not only automates
    model creation but places a premium on interpretability. Users gain insights into
    how the generated models make predictions. The Amazon SageMaker Studio Notebook
    serves as a platform for accessing, refining, and recreating models, ensuring
    continuous model enhancement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon SageMaker Autopilot heralds a shift in the landscape of ML, making it
    accessible to a wider audience. By automating the heavy lifting of model development,
    Autopilot empowers users to focus on the strategic aspects of their business problems,
    liberating them from the intricacies of ML. As organizations embrace ML for decision-making,
    SageMaker Autopilot emerges as a revolutionary tool, unlocking the power of AI
    without the need for extensive data science expertise. In the next section, you
    will dive deeper into model monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Model Monitor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the ever-evolving realm of ML, ensuring the reliability and robustness of
    models in real-world production settings is paramount. In this section, you will
    delve into the profound significance, practical applications, and potent features
    of Amazon SageMaker Model Monitor—an instrumental component tailored to tackle
    the challenge of model drift in live production environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The essence of model monitoring**: As ML models venture into real-world deployment,
    the ongoing degradation of their effectiveness—attributed to shifts in data distributions
    or alterations in user behavior—poses a substantial threat known as model drift.
    Continuous monitoring becomes the linchpin for proactively identifying and rectifying
    these deviations, safeguarding the accuracy and reliability of ML predictions
    and, consequently, business outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An automated guardian**: Amazon SageMaker Model Monitor emerges as a guiding
    light in the ML landscape, delivering an automated solution for the continual
    vigilance of ML models in production. From detecting data drift to ensuring model
    quality, it presents a comprehensive suite to meet the challenges posed by the
    ever-evolving nature of real-world data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated analysis**: Model Monitor takes the reins of model analysis, automating
    the inspection of deployed models based on predefined or user-provided rules at
    regular intervals. This relieves users from the burden of constructing custom
    tooling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical rules**: With built-in statistical rules, Model Monitor spans
    a spectrum of potential issues, covering outliers, completeness, and drift in
    data distributions. These rules empower the system to pinpoint anomalies and deviations
    from the anticipated model behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CloudWatch integration**: Seamlessly integrating with Amazon CloudWatch,
    Model Monitor emits metrics when rule violations occur. Users can set up alarms
    based on these metrics, ensuring prompt notification and allowing timely intervention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data drift monitoring**: Excelling in identifying changes in data distributions,
    Model Monitor provides insights into how input data evolves over time. Whether
    it’s a shift in units or a sudden influx of null values, Model Monitor remains
    vigilant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model quality monitoring**: Beyond data drift, the system monitors the performance
    of the model itself. Degradation in model accuracy triggers alerts, notifying
    users of potential issues that might impact the model’s predictive capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon SageMaker Model Monitor orchestrates a seamless end-to-end flow for deploying
    and monitoring models. From model deployment and data capture to baselining and
    continuous monitoring, the process ensures a comprehensive approach to maintaining
    model stability over time.
  prefs: []
  type: TYPE_NORMAL
- en: In the expansive landscape of ML, Amazon SageMaker Model Monitor stands as a
    guiding force, addressing the critical need for the continuous monitoring of models
    in production. Its automated analysis, integration with CloudWatch, and focus
    on both data and model quality drift make it an indispensable tool for organizations
    relying on ML for pivotal decision-making. As businesses increasingly depend on
    the stability and accuracy of ML models, SageMaker Model Monitor stands tall,
    offering a robust solution to the ever-evolving challenges of the ML landscape.
    In the next section, you will learn about making our SageMaker training process
    faster with Training Compiler.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Training Compiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you’ve reached this section, you are about to delve into the world of **SageMaker
    Training Compiler (SMTC)**, a game-changing tool designed to supercharge the training
    of your ML models on SageMaker by optimizing intricate training scripts. Picture
    this: faster training, swifter model development, and an open door to experimentation.
    That’s the primary goal of SMTC—improving training speed to bring agility to your
    model development journey. The following are the major advantages of using SMTC:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling challenges**: Embarking on the journey of training large-scale models,
    especially those with billions of parameters, often feels like navigating uncharted
    engineering territory. SMTC, however, rises to the occasion by optimizing the
    entire training process, conquering the challenges that come with scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency at its core**: SMTC takes the reins of GPU memory usage, ushering
    in a realm where larger batch sizes become not just a possibility but a reality.
    This optimization translates into accelerated training times, a boon for any data
    scientist seeking efficiency gains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost savings**: Time is money, and in the realm of ML, it’s no different.
    By accelerating training jobs, SMTC isn’t just speeding up your models; it’s potentially
    reducing your costs. How? Well, you pay based on training time, and faster training
    means less time on the clock.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Throughput improvement**: The tool has demonstrated throughput improvements,
    leading to faster training without sacrificing model accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A couple of examples of efficiency, cost savings, autoscaling through SMTC
    in the scenarios/use cases of LLMs, and batch size optimization for NLP problems
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Large Language Models (LLMs)**: SMTC is particularly beneficial for training
    LLMs, including BERT, DistilBERT, RoBERTa, and GPT-2\. These models involve massive
    parameter sizes, making the scaling of training a non-trivial task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size optimization**: SMTC allows users to experiment with larger batch
    sizes, which is especially useful for tasks where efficiency gains can be achieved,
    such as in **Natural Language Processing (NLP)** or computer vision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker Training Compiler is not just a black box; it’s a meticulous craftsman
    at work. It takes your deep learning models from their high-level language representation
    and transforms them into hardware-optimized instructions. This involves graph-level
    optimizations, dataflow-level optimizations, and backend optimizations, culminating
    in an optimized model that dances gracefully with hardware resources. The result?
    Faster training, thanks to the magic of compilation. In the next section, you
    will learn about Amazon SageMaker Data Wrangler—an integral component within SageMaker
    Studio Classic.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Data Wrangler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you’ll unravel the significance and benefits of Data Wrangler,
    dissecting its role as an end-to-end solution for importing, preparing, transforming,
    featurizing, and analyzing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing data with ease**: Data Wrangler simplifies the process of importing
    data from various sources, such as Amazon **Simple Storage Service (S3)**, Amazon
    Athena, Amazon Redshift, Snowflake, and Databricks. Whether your data resides
    in the cloud or within specific databases, Data Wrangler seamlessly connects to
    the source and imports it, setting the stage for comprehensive data handling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constructing data flows**: Picture a scenario where you can effortlessly
    design a data flow, mapping out a sequence of ML data preparation steps. This
    is where Data Wrangler shines. By combining datasets from diverse sources and
    specifying the transformations needed, you sculpt a data prep workflow ready to
    integrate into your ML pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transforming data with precision**: Cleanse and transform your dataset with
    finesse using Data Wrangler. Standard transforms, such as those for string, vector,
    and numeric data formatting, are at your disposal. Dive deeper into feature engineering
    with specialized transforms like text and date/time embedding, along with categorical
    encoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaining insights and ensuring data quality**: Data integrity is paramount,
    and Data Wrangler acknowledges this with its **Data Insights and Quality Report**
    feature. This allows you to automatically verify data quality, identify abnormalities,
    and ensure your dataset meets the highest standards before it becomes the backbone
    of your ML endeavors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**In-depth analysis made simple**: Delve into the intricacies of your dataset
    at any juncture with Data Wrangler’s built-in visualization tools. From scatter
    plots to histograms, you can analyze features with ease. Data analysis tools like
    target leakage analysis and quick modeling can also be leveraged to comprehend
    feature correlation and make informed decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seamless export for further experiments**: Data preparation doesn’t end with
    Data Wrangler—it extends to the next phases of your workflow. Export your meticulously
    crafted data prep workflow to various destinations. Whether it’s an Amazon S3
    bucket, SageMaker Model Building Pipelines for automated deployment, the SageMaker
    Feature Store for centralized storage, or a custom Python script for tailored
    workflows—Data Wrangler ensures your data is where you need it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon SageMaker Data Wrangler isn’t just a tool; it’s a powerhouse for simplifying
    and enhancing your data handling processes. The ability to seamlessly integrate
    with your ML workflows, the precision in transforming data, and the flexibility
    in exporting for further utilization make Data Wrangler a cornerstone of the SageMaker
    ecosystem. In the next section, you will learn about SageMaker Feature Store –
    an organized repository for storing, retrieving, and seamlessly sharing ML features.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Feature Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you are building a recommendation system. In the absence of Feature
    Store, you’d navigate a landscape of manual feature engineering, scattered feature
    storage, and constant vigilance for consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Feature management in an ML pipeline is challenging due to the dispersed nature
    of feature engineering, involving various teams and tools. Collaboration issues
    arise when different teams handle different aspects of feature storage, leading
    to inconsistencies and versioning problems. The dynamic nature of features evolving
    over time complicates change tracking and ensuring reproducibility. SageMaker
    Feature Store addresses these challenges by providing a centralized repository
    for features, enabling seamless sharing, versioning, and consistent access across
    the ML pipeline, thus simplifying collaboration, enhancing reproducibility, and
    promoting data consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Now, user data, including age, location, browsing history, and item data such
    as category and price, have a unified home with Feature Store. Training and inference
    become a joyride, with easy access and sharing of these features, promoting efficiency
    and unwavering consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'To navigate the terrain of SageMaker Feature Store, let’s familiarize ourselves
    with some key terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature store**: At its core, a feature store is the storage and data management
    layer for ML features. It stands as the single source of truth, handling storage,
    retrieval, removal, tracking, sharing, discovery, and access control for features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Online** **store**: This is the realm of low latency and high availability,
    allowing real-time lookup of records. The online store ensures quick access to
    the latest record via the GetRecord API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offline** **store**: When sub-second latency reads are not a priority, the
    offline store stores historical data in your Amazon S3 bucket. It’s your go-to
    for storing and serving features for exploration, model training, and batch inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature** **group**: The cornerstone of Feature Store, a feature group contains
    the data and metadata crucial for ML model training or prediction. It logically
    groups features used to describe records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature**: A property serving as an input for ML model training or prediction.
    In the Feature Store API, a feature is an attribute of a record.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature** **definition**: Comprising a name and data type (integral, string,
    or fractional), a feature definition is an integral part of a feature group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Record**: A collection of values for features tied to a single record identifier.
    The record identifier and event time values uniquely identify a record within
    a feature group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Record** **identifier** **name**: Each record within a feature group is defined
    and identified with a record identifier name. It must refer to one of the names
    of a feature defined in the feature group’s feature definitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event** **time**: The time when record events occur is marked with timestamps,
    which are vital for differentiating records. The online store contains the record
    corresponding to the latest event time, while the offline store contains all historic
    records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingestion**: The process of adding new records to a feature group, usually
    achieved through the PutRecord API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s combine the tools covered in this chapter so far to navigate through
    an example of fraud detection in financial transactions. *Table 9.2* shows a synthetic
    dataset for financial transactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **TransactionID** | **Amount** | **Merchant** | **CardType** | **IsFraud**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 500.25 | Amazon | Visa | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 120.50 | Walmart | Mastercard | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 89.99 | Apple | Amex | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 300.75 | Amazon | Visa | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 45.00 | Netflix | Mastercard | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 9.2 – Example dataset for financial transactions
  prefs: []
  type: TYPE_NORMAL
- en: You will now see the applications of SageMaker Feature Store, SageMaker Training
    Compiler, SageMaker Debugger, and SageMaker Model Monitor on the above dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '`Amount`, `Merchant`, `CardType`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ingest data into Feature Store: Use the SageMaker Feature Store API to ingest
    the dataset into Feature Store:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`# Example code for defining a training job with SageMaker Training Compiler`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`# Example code for integrating SageMaker Debugger in the training script`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Model deployment and inference**: You can deploy your trained model with
    SageMaker, tapping into the rich repository of features stored in the Feature
    Store. Real-time monitoring with SageMaker Model Monitor ensures the model’s health
    in the dynamic world of inference.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`# Example code for capturing baseline statistics with SageMaker Model Monitor`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the next section, you will learn about Amazon SageMaker Edge Manager, a service
    provided by AWS to facilitate the deployment and management of ML models on edge
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Edge Manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SageMaker Edge Manager is designed to address the challenges faced by ML developers
    when operating models on fleets of edge devices. Some of the key functions that
    SageMaker Edge Manager can perform are highlighted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model compilation**: Utilizes Amazon SageMaker Neo to compile models for
    various target devices and operating environments, including Linux, Windows, Android,
    iOS, and macOS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment**: Signs each model with an AWS key, packages it with its
    runtime, and includes all necessary credentials for deployment on specific devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model server concept**: Introduces a model server concept to efficiently
    run multiple models on edge devices, optimizing hardware resource utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous monitoring**: Provides tools for continuous monitoring of model
    health, allowing developers to collect metrics, sample input/output data, and
    send this data securely to the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model drift detection**: Allows the detection of model quality decay over
    time due to real-world data drift, enabling developers to take corrective action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with SageMaker Ground Truth**: Integrates with SageMaker Ground
    Truth for data labeling and retraining, ensuring that models stay accurate and
    effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let’s understand a few real-world challenges and their solutions using
    SageMaker Edge Manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High** **resource requirements:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Challenge**: ML models, especially deep learning models, can have high resource
    requirements.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solution**: SageMaker Edge Manager uses SageMaker Neo to compile models,
    making them more efficient and allowing them to run up to 25 times faster on certain
    target hardware.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running** **multiple models:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Challenge**: Many ML applications require running multiple models simultaneously.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solution**: Introduces a model server concept, enabling the efficient execution
    of multiple models in series or in parallel on edge devices.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model quality decay** **in production:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Challenge**: Real-world data drifts over time, causing model quality decay.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solution**: SageMaker Edge Manager supports continuous monitoring, allowing
    developers to detect and address model quality decay using metrics and drift detection
    tools.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some examples showcasing different applications of SageMaker
    Edge Manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time predictions in autonomous vehicles**: Edge devices in autonomous
    vehicles need to provide real-time predictions for navigation and obstacle avoidance.
    SageMaker Edge Manager optimizes models for these devices, ensuring low-latency
    predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy-preserving personal devices**: Personal devices such as smartphones
    and smart cameras can keep data on the device using SageMaker Edge Manager, preserving
    user privacy and reducing the need for extensive data transfer to the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous monitoring in Industrial IoT**: Industrial IoT deployments with
    sensors on machines can benefit from the continuous monitoring provided by SageMaker
    Edge Manager. This helps identify and address model quality decay in a dynamic
    environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, Amazon SageMaker Edge Manager streamlines the deployment and management
    of ML models on edge devices, addressing resource constraints, enabling efficient
    model execution, and ensuring continuous monitoring for sustained model accuracy.
    In the next section, you will learn about a no-code solution offered by Amazon
    SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Canvas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn the core of SageMaker Canvas, elucidating its
    features and the significance it holds for organizations keen on infusing ML into
    their decision-making processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon SageMaker Canvas is a cloud-based service offered by AWS that streamlines
    the ML process through a visual interface for constructing, training, and deploying
    ML models—all without the need for coding. Nestled within the Amazon SageMaker
    suite, it caters to a diverse audience by democratizing ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code-free model building**: SageMaker Canvas obliterates the traditional
    barriers encountered when adopting ML, enabling users to forge models without
    the need for code. This feature proves pivotal for business professionals seeking
    to harness the potency of ML for predictive analytics, despite lacking coding
    expertise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study: A marketing professional without any ML knowledge can utilize SageMaker
    Canvas to predict customer churn. The intuitive interface guides them through
    the process, making predictive analytics accessible to a wider audience.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Versatile user interface**: The user-friendly interface of SageMaker Canvas
    accommodates users with varying levels of expertise. It empowers users to create
    predictions across diverse use cases, from inventory planning to sentiment analysis,
    rendering it a versatile tool for businesses spanning different industries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study: A supply chain manager can leverage SageMaker Canvas to predict
    optimal inventory levels based on historical data, seasonality, and market trends,
    streamlining the planning process and minimizing stockouts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Built-in data preparation functions**: SageMaker Canvas comes equipped with
    built-in data preparation functions and operators, facilitating the import and
    analysis of disparate cloud and on-premises data sources. This feature streamlines
    the exploration and visualization of relationships between features, enabling
    the seamless creation of new features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study: A data analyst can import and analyze customer data from various
    sources using SageMaker Canvas. This allows them to identify key factors influencing
    purchasing decisions and create predictive models to enhance targeted marketing
    strategies.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Collaboration and model sharing**: SageMaker Canvas fosters collaboration
    by enabling users to share, review, and update ML models across different tools
    and teams. This collaborative aspect ensures that knowledge and insights derived
    from ML are disseminated effectively within the organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study: A data science team collaborates with business analysts using SageMaker
    Canvas to develop a fraud detection model. The model can be shared seamlessly,
    allowing real-time updates and improvements based on evolving data patterns.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Amazon SageMaker Canvas acts as a catalyst for transforming ML from a specialized
    skill to a tool accessible to a broader audience. Its features, including code-free
    model building, a versatile user interface, and collaborative capabilities, underscore
    its importance in simplifying the ML lifecycle. As organizations strive to harness
    the power of data-driven insights, SageMaker Canvas stands at the forefront, enabling
    them to innovate, make informed decisions, and thrive in an increasingly competitive
    landscape.
  prefs: []
  type: TYPE_NORMAL
- en: You have now reached the end of this section and the end of this chapter. Next,
    let’s summarize what you have learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the usage of SageMaker for creating notebook
    instances and training instances. As you went through, you learned how to use
    SageMaker for hyperparameter tuning jobs. As the security of your assets in AWS
    is an essential part of your work, you also learned the various ways to secure
    SageMaker instances.
  prefs: []
  type: TYPE_NORMAL
- en: AWS products are evolving every day to help you solve IT problems. It’s not
    easy to remember all the product names. The only way to learn is through practice.
    When you are solving a problem or building a product, focus on different technological
    areas of your product. Those areas can be scheduling jobs, logging, tracing, monitoring
    metrics, autoscaling, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Compute time, storage, and networking are the baselines. It is recommended that
    you practice some examples for each of these services. Referring to the AWS documentation
    to resolve any doubts is also a useful option. It is always important to design
    your solutions in a cost-effective way and exploring cost optimizations when using
    these services is as important as building the solution itself. I wish you all
    the best!
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH09](https://packt.link/MLSC01E2_CH09).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 9**.14*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.14 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – QR code that opens Chapter Review Questions for logged-in users
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 9**.15*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Chapter Review Questions for Chapter 9](img/B21197_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – Chapter Review Questions for Chapter 9
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  prefs: []
  type: TYPE_TB
- en: Table 9.3 – Sample timing practice drills on the online platform
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  prefs: []
  type: TYPE_NORMAL
