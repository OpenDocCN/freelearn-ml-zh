- en: Chapter 13. Scaling Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have reviewed a steady stream of pertinent topics concerning
    *statistics* and specifically, *predictive analytics*. In this chapter, we look
    to provide a tutorial dedicated to applying those concepts and practices to very
    large datasets. First, we'll begin by defining the phrase very large – at least
    as it is used to describe data defined (that we want to train our predictive models
    on or run our statistical algorithms against). Next, we will review the list of
    the challenges imposed by using bigger data sources, and finally, we will offer
    some ideas for meeting these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our chapter is broken down into the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The phases of an analytics project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experience and data of scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The characteristics of big data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training models at scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specific challenges (of big data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A path forward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting the project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The phases of a general purpose predictive analytics project may be straightforward
    and perhaps easy (it's the practice of carrying out each of these phases effectively
    that is challenging).
  prefs: []
  type: TYPE_NORMAL
- en: '![Starting the project](img/00212.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The Phases of a predictive analytics project
  prefs: []
  type: TYPE_NORMAL
- en: 'These phases are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define** (the data).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Profile & Prepare** (the data).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Determine the Question** (what to predict).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Choose** the algorithm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Apply** the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An interesting thought:'
  prefs: []
  type: TYPE_NORMAL
- en: '"…Once you have enough data, you start to see patterns," he said. "You can
    build a model of how these data work. Once you build a model, you can predict…"'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Bertolucci, 2013
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'At the beginning of any (and every) analytics project, data is defined – reviewed
    and analyzed: source, format, state, interval, and so on (some refer to this as
    the process of investigating the breadth and depth of available data).'
  prefs: []
  type: TYPE_NORMAL
- en: One exercise demanded is to perform what is referred to as profiling the data
    source, or to establishing your data's profile by determining its characteristics,
    relationships, and patterns (and context). This process will, hopefully, produce
    a clearer view of the content and quality the data to be used in the project –
    that is, the data profile.
  prefs: []
  type: TYPE_NORMAL
- en: Then, after the exercise of profiling is completed, one would most likely proceed
    with performing some form of data scrubbing (this is also sometimes referred to
    as cleansing or in some cases preparing) in an effort to improve its level of
    quality. During the process of cleansing or scrubbing your data, you would most
    likely perform tasks such as aggregation, appending, merging, reformatting fields,
    changing variable types or adding missing values, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data profiling techniques can include specific analysis types such as a *univariate
    analysis* which involves frequency analysis for categorical variables and understanding
    distribution and summary statistics for continuous variables. This aids in missing
    value treatment, understanding distribution, and outlier treatment.
  prefs: []
  type: TYPE_NORMAL
- en: Experience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When soliciting advice from a **subject matter expert** (**SME**), one would
    probably likely agree that an individual with more experience most likely will
    be able to provide a better service. With predictive analytics projects, the objective
    is not what the data can tell us, but what the data can tell us about an objective
    or problem, therefore, the size or amount of the data source (the amount of experience)
    available for the project becomes much more important. Typically, the more the
    data, the better.
  prefs: []
  type: TYPE_NORMAL
- en: So, at what point is it acceptable to say you have enough data for your predictive
    project? The politically correct answer to this question is that it depends. Some
    types of data science and predictive analysis projects require more specific data
    requirements than others will, effectively setting what the minimal data volumes
    might be.
  prefs: []
  type: TYPE_NORMAL
- en: In an extreme case, predicting may require data spanning many years or even
    many decades – as larger amounts of data can yield a breadth of patterns surrounding
    behaviors and decisions, and so on. Why? Because typically, analyzing (or training
    a model with) more data develops a more comprehensive understanding or a **better
    prediction**.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, perhaps a general rule of thumb is to collect as much data
    as possible (depending upon the objective or type of application). Some experts
    might suggest collecting at least three years ', and preferably five years', worth
    of data before beginning any predictive analysis project. Of course, years may
    not be the appropriate measure depending upon the type of application. For example,
    cases might be more appropriate or lines of text, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In practice, if an application was built around hospital visits, the more patient
    cases (typically millions) the better; a word predictor application would want
    to have as many text sentences or word phrases (tens of millions) as it could
    (to be effective).
  prefs: []
  type: TYPE_NORMAL
- en: Another predictive analytics data controversy might be understanding the idea
    of *sufficient* versus *enough*.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, given a shortage of volume or *quantity*, the wise data scientist
    would always focus on the *quality* or suitability of the data. This means that
    even though the volume of data is less than hoped for, the quality of the data,
    based upon the objective of the project, is deemed sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Given an understanding of all of the preceding points, it is important to gauge
    your data to determine if the volume of your data has reached the tipping point
    – that point where typical analytical activities begin to become onerous to perform.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover how to establish that data volume tipping
    point as it is always better to understand and expect challenges before you begin
    your heavy model training rather than finding out the hard way, after you've already
    begun.
  prefs: []
  type: TYPE_NORMAL
- en: Data of scale – big data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we use the phrase, data of scale we are not referring to the statistical
    measurement scales of interval, ordinal, nominal, and dichotomous. We are using
    the phrase loosely to convey the size, volume, or complexity of the data source
    to be used in your analytics project.
  prefs: []
  type: TYPE_NORMAL
- en: The, by now, well-known buzz word, **big data** might (loosely) fit here, so
    let us take pause here to define how we are using the term big data.
  prefs: []
  type: TYPE_NORMAL
- en: A large assemblage of data, datasets that are so large or complex that traditional
    data processing applications are inadequate, and data about every aspect of our
    lives have all been used to define or refer to big data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates big data''s three v''s:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data of scale – big data](img/00213.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In 2001, then Gartner analyst *Doug Laney* introduced the 3Vs concept to describe
    the occurrence of big data. The 3Vs, according to *Laney*, are volume, variety,
    and velocity. The Vs make up the dimensionality of big data: volume (or the measurable
    amount of data), variety (meaning the number of types of data), and velocity (referring
    to the speed of processing or dealing with that data).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Laney''s explanation can be reviewed here: [http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf](http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: Using the *volume*, *variety*, and *velocity* concept, it is easier to foresee
    how a *big data* source can be or quickly become increasingly challenging to work
    with, and as these dimensions' increase or expand they will only encumber the
    ability to effectively train predictive models on the data further.
  prefs: []
  type: TYPE_NORMAL
- en: Using Excel to gauge your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microsoft Excel is not a tool to be used to determine if your data qualifies
    as big data.
  prefs: []
  type: TYPE_NORMAL
- en: '**If your data is too big for Microsoft Excel it still really doesn''t necessarily
    qualify as big data.** In fact, gigabytes of data, still manageable with various
    techniques, enterprise, and even open source tools, especially with the lower
    cost of storage today.'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to be able to realistically size or scale the data technology
    (keeping in mind expected data growth rates) you will be using in your predictive
    project before selecting an approach or even beginning any profiling or preparing
    work effort. This time is well spent as it will save time later that may be lost
    due to performance bottlenecks or rewriting scripts to use a different approach
    (one that can handle bigger data sources).
  prefs: []
  type: TYPE_NORMAL
- en: So, the question becomes, how do you gauge your data – is it really big data?
    Is it manageable? Or does it fall into that category that will require special
    handling or pre-processing before it can be effectively used for your predictive
    analytics objective?
  prefs: []
  type: TYPE_NORMAL
- en: Characteristics of big data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For you to determine if your data source qualifies as big data or as needing
    special handling, you can start by examining your data source in the following
    areas:'
  prefs: []
  type: TYPE_NORMAL
- en: The volume (amount) of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The variety of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of different sources and spans of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's examine each of these areas.
  prefs: []
  type: TYPE_NORMAL
- en: Volume
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are talking about the number of rows or records, then most likely your
    data source is not a big data source since big data is typically measured in gigabytes,
    terabytes, and petabytes. However, space doesn't always mean big, as these size
    measurements can vary greatly in terms of both volume and functionality. Additionally,
    data sources of several million records may qualify as big data, given their structure
    (or lack of structure).
  prefs: []
  type: TYPE_NORMAL
- en: Varieties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data used in predictive models may be structured or unstructured (or both) and
    include transactions from databases, survey results, website logs, application
    messages, and so on (by using a data source consisting of a higher variety of
    data, you are usually able to cover a broader context for the analytics you derive
    from it). Variety, much like volume, is considered a normal qualifier for big
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Sources and spans
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the data source for your predictive analytics project is the result of integrating
    several sources, you most likely hit on both criteria of volume and variety and
    your data qualifies as big data. If your project uses data that is affected by
    governmental mandates, consumer requests is a historical analysis, you are almost
    certainty using big data. Government regulations usually require that certain
    types of data need to be stored for several years. Products can be consumer driven
    over the lifetime of the product and with today's trends, historical analysis
    data is usually available for more than five years. Again, all examples of big
    data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will often find that data sources typically fall into one of the following
    three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Sources with little or no structure in the data (such as simple text files).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sources containing both structured and unstructured data (like data that is
    sourced from document management systems or various websites, and so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sources containing highly structured data (like transactional data stored in
    a relational database example).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: how your data source is categorized will determine how you prepare and work
    with your data in each phase of your predictive analytics project.
  prefs: []
  type: TYPE_NORMAL
- en: Although data sources with structure can obviously still fall into the category
    of big data, it's data containing both structured and unstructured data (and of
    course totally unstructured data) that fit as big data and will require special
    handling and or pre-processing.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical noise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we should take a note here that other factors (other than those discussed
    already in the chapter) can qualify your project data source as being unwieldy,
    overly complex, or a big data source.
  prefs: []
  type: TYPE_NORMAL
- en: 'These include (but are not limited to):'
  prefs: []
  type: TYPE_NORMAL
- en: Statistical noise (a term for recognized amounts of unexplained variations within
    the data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data suffering from mismatched understandings (the differences in interpretations
    of the data by communities, cultures, practices, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have determined that the data source that you will be using in your
    predictive analytics project seems to qualify as big (again as we are using the
    term here) then you can proceed with the process of deciding how to manage and
    manipulate that data source, based upon the known challenges this type of data
    demands, so as to be most effective.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will review some of these common problems, before we
    go on to offer useable solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Training models at scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an earlier section of this chapter, we listed and studied what the industry
    experts agree on as the most common phases of any predictive analytics project.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recall, they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the data source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling and preparation of the data source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the question(s) that you want to ask your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing an algorithm to train on the data source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application of a predictive model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a predictive analytics project using big data, those same phases are present,
    but may be slightly varied and require some supplementary efforts.
  prefs: []
  type: TYPE_NORMAL
- en: Pain by phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the initial phase of a project, once you've chosen a source for your data
    (determined the data source), the data must be attained. Some industry experts
    describe this as the acquisition and recording of data. In a predictive project
    that involves a more common data source, access to the data might be as straightforward
    as opening a file on your local disk; with a big data source, it's a bit more
    difficult. For example, suppose your project sources data from a combination of
    devices (multiple servers and many mobile devices, that is, the Internet of Things
    data).
  prefs: []
  type: TYPE_NORMAL
- en: This activity-generated data might include a combination of website tracking
    information, application logs, sensor data – among other machine-generated content
    – perfect for your analysis. You can see how the effort to access this information
    as a single data source for your project would take some effort (and expertise!).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the profiling and preparation phase, data is extracted, cleaned, and annotated.
    Typically, any analytics project will require this pre-processing of the data:
    setting context, identifying operational definitions and statistical types, and
    so on. This step is critical as this is the phase where we establish an understanding
    of the data challenges so that later surprises can be minimized. This phase usually
    involves time spent querying and re-querying the data, creating visualizations
    to validate findings, and then performing updates to the data to address areas
    of concern. Big data inhibits these activities since it includes either more data
    to process, is perhaps inconsistent in format, and could be changing rapidly.'
  prefs: []
  type: TYPE_NORMAL
- en: In the phase where question determination takes place, data integration, aggregation,
    and the representation of the data must be considered so that the proper questions
    to ask the data can be identified. This phase may be divided into three steps;
    preparation, integration, and determination (of questions). The prep step involves
    assembling the data, identification of unique keys, aggregation/duplication, scrubbing
    as required, format manipulation, and perhaps mapping of values. The integration
    step involves merging data, testing, and reconciliation. Finally, project questions
    are established. Once again, big data's volumes, varieties, and velocities can
    slow this phase down considerably.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an algorithm and application of a predictive model are the phases where
    there is analysis, modeling, and interpretation of the data. Considering the volumes,
    varieties, and velocities of a big data source, selecting the appropriate algorithm
    to be used to train data can be much more involved. An example would be the idea
    that predictive modeling works best given the lowest level of granularity possible
    and, in the previous phase, perhaps the sheer volume of a big data source required
    that extensive aggregation be done, thus potentially burying anomalies and variations
    that exist within the data.
  prefs: []
  type: TYPE_NORMAL
- en: Specific challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s take a few moments to address some very specific challenges brought
    on by big data. Among these top topics are:'
  prefs: []
  type: TYPE_NORMAL
- en: Heterogeneity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timeliness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heterogeneity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By variety, we usually need to consider the heterogeneity of data types, representation,
    and semantic interpretation. Efforts to correctly review and understand these
    variations in big data sources can be time consuming and complex. Interestingly,
    an element may be homogeneous (more uniform) on a larger scale, compared to being
    heterogeneous (less uniform) on a smaller scale. This means that your approach
    to addressing a big data source may cause very different results!
  prefs: []
  type: TYPE_NORMAL
- en: Scale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We've already touched on the idea of scale – typically scale refers to the sheer
    size of the data source, but could also refer to its complexities.
  prefs: []
  type: TYPE_NORMAL
- en: Location
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Typically, you'll see that when you decide to use a big data source, it is not
    located all in one place, but spread throughout electronic space. This means that
    any process (manual or automated) will have to consolidate the data – physically
    or virtually before it can be properly used in a project.
  prefs: []
  type: TYPE_NORMAL
- en: Timeliness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The bigger the data, the more time it will take to analyze. However, it is not
    just this time that is meant when one speaks of velocity in the context of big
    data. Rather, there is the challenge of the acquisition rate of the data. In other
    words, with data piling up or being updated continuously within the data source,
    when (or how often) is the correct snapshot established? In addition, scanning
    the entire data source to find a suitable sample pertinent to a particular predictive
    analytics objective is obviously impractical.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data privacy should be a consideration when using any data source, and one that
    increases in complexity in the context of big data. The most well-known example
    of this is with electronic health records – which have strict laws governing them.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose, for example having the requirement to pre-process a big data source
    that is over a terabyte in size to hide both a user's identify and location information?
  prefs: []
  type: TYPE_NORMAL
- en: Collaborations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One might think that in this day and age, analytics and predictive models are
    entirely computational (especially when you hear the term machine learning), however,
    no matter how advanced a predicative algorithm or model proclaims to be, there
    remain many patterns in data that humans can simply detect, but computer algorithms
    no matter how complex in their logic, have a hard time finding.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a new following in analytics that may be considered a sub-field of
    *visual* analytics that utilizes SME input, at least with respect to the modelling
    and analysis phase of a predictive project.
  prefs: []
  type: TYPE_NORMAL
- en: To include a subject matter expert in a predictive analytics project might not
    be a huge issue but, with a big data source, it often takes multiple experts from
    different domains to really understand what is going on with the data and to share
    their respective exploration of results and advice.
  prefs: []
  type: TYPE_NORMAL
- en: These multiple experts may be separated in space and time and be difficult to
    assemble in one location at one time. Again, causing additional time and effort
    to be spent.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Believe it or not, most predictive analytics projects are repeated for a variety
    of reasons. For example, if the results are in question for any reason or if the
    data is suspect, all of the phases of the project may be repeated. Reproduction
    of a big data analytics project is seldom reasonable. In most cases, all that
    can be done is that the bad data in a big data resource will be found and flagged
    as such.
  prefs: []
  type: TYPE_NORMAL
- en: A path forward
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, the inkling of having more than enough data for training a model seems very
    appealing.
  prefs: []
  type: TYPE_NORMAL
- en: Big data sources would appear to answer this desire, however in practice, a
    big data source is not often (if ever) analyzed in its entirety. You can pretty
    much count on performing a sweeping filtering process aimed to reduce the big
    data into small(er) data (more on this in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will review various approaches to addressing the
    various challenges of using big data as a source for your predictive analytics
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Opportunities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we offer a few recommendations for handling big data sources
    in predictive analytic projects using R. Also, we'll offer some practical use
    case examples.
  prefs: []
  type: TYPE_NORMAL
- en: Bigger data, bigger hardware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are starting with the most obvious option first.
  prefs: []
  type: TYPE_NORMAL
- en: To be clear, R keeps all of its objects in memory, which is a limitation if
    the data source gets too large. One of the easiest ways to deal with big data
    in R is simply to increase the machine's memory.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, R can use 8 TB of RAM if it runs on a 64-bit machine
    (compared to only 2 GB addressable RAM on 32-bit machines). Most machines used
    for predictive analytics projects are (at least should be) 64-bit already, so
    you just need to add RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are both 32-bit and 64-bit versions of R. Do yourself a favor and use
    the 64-bit version!
  prefs: []
  type: TYPE_NORMAL
- en: If you know your data source well and have added appropriate amounts of memory
    to your machine, then you'll most likely be OK to work with a big data source
    efficiently, especially if you use one of the approaches outlined in the following
    sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most straightforward and proven approaches to taming a big data source
    with R (or any language for that matter) is to create workable subsets of data
    prepared from the big data resource.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we have patient health records making up a current big
    data source. There are literally trillions of patient case records in the data
    with more added almost every minute. These cases record both the basics (sex,
    age, height, weight, and so on) as well as specifics around the patient's background
    (such as if the patient is a smoker, drinker, currently on medications, has ever
    been operated on, and so on). Luckily, our file does not contain any information
    that can be used to identify the patient (such as name or social security number)
    so we won't be in violation of any laws.
  prefs: []
  type: TYPE_NORMAL
- en: The data source is fed by hospitals and doctors' offices all over the country.
    Our predictive project is one that is looking to determine relationships between
    a patient's health and the state that they live in. Rather than attempting to
    train on all of the data (a mostly impractical effort), we can use some logic
    to prepare our series of smaller, more workable subsets. For example, we could
    simply separate out our overall data source into 50 smaller files – one for each
    state. This would help, but the smaller files may still be massive, so with a
    little profiling of the data, we may be able to identify other measurements that
    we can use to divide our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of data discovery and separation might look pretty close to the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are dealing with a big data source and are not sure of the number
    of cases or records within the file, we can start by creating an R data object
    from our comma separated file and restricting the number of records to be read:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`x` now contains 150 records that we can review looking for interesting measures
    we may be able to use to logically split our data on. You can also utilize the
    summary function to evaluate variables within the data source. For example, we
    see that column 9 is the patients'' home state, column 5 is the patients'' current
    body weight, and column 79 indicates the patients'' weight 1 year ago:![Breaking
    up](img/00214.jpeg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we can perhaps create a series of smaller subsets where there are 50 state
    files, but each containing only cases that have patients who have gained more
    than five pounds in the past year:![Breaking up](img/00215.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We do end up with 50 files, but each file should be much smaller and easier
    to work with then a single, large big data source. This is also a simple example
    and in practice, you may (and probably will) end up rerunning the split code and
    stitching together multiple state files.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding is one example of how big data research typically works—by constructing
    smaller datasets that can be efficiently analyzed!
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another method for dealing with the volume of a big data source is with population
    sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling is a selection of or a subset of cases from within a statistical population
    intended to estimate or represent characteristics of the whole population. The
    net-net is the size of the data to be trained on, is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: There is some concern that sampling may decrease the performance (not as in
    processing time, but in the accuracy of results generated) of a model. This may
    be somewhat true as typically the more data the model is trained on, the better
    the result, but depending upon the objective, the decrease in performance can
    be negligible.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, it is safe to say that if sampling can be avoided, it is recommendable
    to use another big data strategy. But if you find that sampling is necessary,
    it still can lead to satisfying models.
  prefs: []
  type: TYPE_NORMAL
- en: When you use sampling as a big data predictive strategy, you should try to keep
    the sample as big as you can, consider carefully the size of the sample in proportion
    to the full population and ensure as best you can that the sample is not biased.
  prefs: []
  type: TYPE_NORMAL
- en: One of the easiest methods for creating a sample is with the R function sample.
    Sample takes a sample of the specified size from the elements of `x` using either
    with or without replacement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines of R code are a simple example of creating a random sample
    of 500 cases from our original data. Notice the row counts (indicated by using
    the R function `nrow`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling](img/00216.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Aggregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another method for reducing the size of a big data source (again depending on
    your projects' objectives) is by statistical aggregation of the data. In other
    words, you simply may not require the level of granularity in the data that is
    available.
  prefs: []
  type: TYPE_NORMAL
- en: In statistical data aggregation, the data can be combined from several measurements.
    This means that groups of observations are replaced with summary statistics based
    on those observations. Aggregation is used a lot in descriptive analytics, but
    can also be used to prepare data for a predictive project.
  prefs: []
  type: TYPE_NORMAL
- en: For larger and especially disparately located big data sources one might use
    a Hadoop and Hive (or similar technology) solution to aggregate the data. If the
    data is in a transactional database, you may even be able to use native SQL. In
    an all-natural R solution, you have more work to do.
  prefs: []
  type: TYPE_NORMAL
- en: R provides a convenient function named `aggregate` that can be used for big
    data aggregation, once you have determined how you want to (or need to) use the
    data in your project.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code shows applying the function to the original
    data (stored in the data object named `x`) being aggregated on the 3 variables
    (patient `sex`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Going back to an earlier section, to our example of splitting data into 50
    state files, we could potentially instead use the R code shown as follows to aggregate
    and generate summary statistics by state. Notice that the original case count
    was 5,994 and after aggregating the data, we have a case count of 50 (one summary
    record for each state):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Aggregation](img/00217.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Dimensional reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 8](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 8. Dimensionality Reduction"), *Dimensionality Reduction*, we introduced
    the process of dimensional reduction, which (as we pointed out then) allows the
    data scientist to minimize the data's dimensionality, but can also reduce the
    overall volume of a big data source, thereby reducing the amount of time and memory
    required for processing the data, allowing it to be more easily visualized, and
    eliminating features irrelevant to the model's purpose, reduce model noise, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Like breaking up the data into smaller more manageable files, using dimensional
    reduction will help, but takes a good understanding of the data as well as perhaps
    plenty of processing steps to eventually produce a workable data population.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since R is an in-memory language, it sometimes has a reputation of not being
    able to handle big data. However, using some creativity and strategic thinking,
    you can use big data in your predictive analytics projects quite successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the preceding approaches, there are currently a number of alternative
    approaches you may wish to research, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are packages available that avoid storing data in memory. Instead, objects
    are stored on hard disk and analyzed in chunks. As a side effect, the chunking
    also leads naturally to parallelization, if the algorithms allow parallel analysis
    of the chunks in principle. You can search: Revolution R Enterprise for some background
    on the topic.'
  prefs: []
  type: TYPE_NORMAL
- en: Alternative language integrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Integrating higher performing programming languages is becoming a popular alternative
    to dealing with big data sources in R. This concept takes portions of R code and
    moves them to another language that may be better suited to carry out the logic
    or work than R is. This blends the best of R while avoiding performance bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: This outsourcing of code chunks from R to another language can easily be hidden
    in functions. In this case, proficiency in other programming languages is mandatory
    for the developers, but not for the users of these functions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we broke out a typical predict analytics project into phases
    and explained that the first phase is where you define what data is to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the more the data, there is better the performance (or results) of
    a predictive model, but at some point (as in the case of a big data source) there
    may be too much data, at least to effectively deal with.
  prefs: []
  type: TYPE_NORMAL
- en: After reviewing the reasons why big data is so challenging, we instructed on
    how to gauge your data source, to qualify it as a big source, and then offered
    various proven techniques for addressing the common challenges of using big data.
  prefs: []
  type: TYPE_NORMAL
