<html><head></head><body>
		<div id="_idContainer195">
			<h1 id="_idParaDest-149" class="chapter-number"><a id="_idTextAnchor192"/>11</h1>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor193"/>Bringing Your Own Models for Database Inference</h1>
			<p><a id="_idTextAnchor194"/>In this book, we’ve <a id="_idIndexMarker502"/>covered the process of training models natively using <strong class="bold">Redshift Machine Learning</strong> (<strong class="bold">Redshift ML</strong>). However, there may be instances where you need to utilize models built outside of Redshift. To address this, Redshift ML offers the <strong class="bold">Bring Your Own Model</strong> (<strong class="bold">BYOM</strong>) feature, allowing <a id="_idIndexMarker503"/>users to integrate their Amazon SageMaker machine learning models with Amazon Redshift. This feature facilitates making predictions and performing other machine learning tasks on data stored in the warehouse, without requiring <span class="No-Break">data movement.</span></p>
			<p>BYOM offers two approaches: <strong class="bold">local inference</strong> and <strong class="bold">remote inference</strong>. In this chapter, we’ll delve into the workings of <a id="_idIndexMarker504"/>BYOM and explore the various options available for creating and integrating BYOM. You’ll be <a id="_idIndexMarker505"/>guided through the process of building a machine learning model in Amazon SageMaker, and subsequently, employing Redshift ML’s BYOM feature to bring that model to Redshift. Moreover, you’ll learn how to apply these models to the data stored in Redshift’s data warehouse to <span class="No-Break">make predictions.</span></p>
			<p>By the end of this chapter, you’ll be proficient in bringing Amazon SageMaker-created models and executing predictions within Amazon Redshift. Utilizing BYOM, you can deploy models such <a id="_idIndexMarker506"/>as <strong class="bold">XGBoost</strong> and a <strong class="bold">multilayer perceptron</strong> (<strong class="bold">MLP</strong>) to <a id="_idIndexMarker507"/>Redshift ML. Once a pre-trained model is deployed on Redshift ML, you can run inferences locally on Redshift without relying on a SageMaker endpoint or SageMaker Studio. This simplicity empowers data analysts to conduct inference on new data using models created externally to Redshift, eliminating concerns about accessing <span class="No-Break">SageMaker’s services.</span></p>
			<p>This method significantly speeds up the delivery of machine learning models created outside of Redshift to the data team. Furthermore, since Redshift ML interacts with native Redshift SQL, the user experience for the data team remains consistent with other data analysis work performed on the <span class="No-Break">data warehouse.</span></p>
			<p>In this chapter, we will go through the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Benefits <span class="No-Break">of BYOM</span></li>
				<li>Supported <span class="No-Break">model types</span></li>
				<li>BYOM for <span class="No-Break">local inference</span></li>
				<li>BYOM for <span class="No-Break">remote infe<a id="_idTextAnchor195"/>rence</span></li>
			</ul>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor196"/>Technical requirements</h1>
			<p>This chapter requires a web browser and access to <span class="No-Break">the following:</span></p>
			<ul>
				<li>An <span class="No-Break">AWS account</span></li>
				<li>An Amazon Redshift <span class="No-Break">Serverless endpoint</span></li>
				<li>An Amazon <span class="No-Break">SageMaker notebook</span></li>
				<li>Amazon Redshift Query <span class="No-Break">Editor v2</span></li>
				<li>Completing the <em class="italic">Getting started with Amazon Redshift Serverless</em> section in <a href="B19071_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a></li>
			</ul>
			<p>You can find the code used in this <span class="No-Break">chapter here:</span></p>
			<p><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Red<span id="_idTextAnchor197"/>shift</span></a></p>
			<p>The data files required for this chapter are located in a public S3 <span class="No-Break">bucket: </span><span class="No-Break"><strong class="source-inline">s3://packt-serverless-ml-redsh<a id="_idTextAnchor198"/>ift/</strong></span><span class="No-Break">.</span></p>
			<p><span class="No-Break">Let’s b<a id="_idTextAnchor199"/>egin!</span></p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor200"/>Benefits of BYOM</h1>
			<p>With Amazon Redshift ML, you<a id="_idIndexMarker508"/> can use an existing ML model built in Amazon SageMaker and use it in Redshift without having to retrain it. To use BYOM, you need to provide model artifacts or a SageMaker endpoint, which takes a batch of data and returns predictions. BYOM is useful in cases where a machine learning model is not yet available in Redshift ML, for example, at the time of writing this book, a Random Cut Forest model is not yet available in Redshift ML, so you can build this model in SageMaker and <a id="_idIndexMarker509"/>easily bring it to Redshift and then use it against the data stored <span class="No-Break">in Redshift.</span></p>
			<p>Here are some specific <a id="_idIndexMarker510"/>benefits of using Redshift ML with your own <span class="No-Break">ML model:</span></p>
			<ul>
				<li><strong class="bold">Improved efficiency</strong>: By using an existing ML model, you can save time and resources that would otherwise be spent on training a <span class="No-Break">new model</span></li>
				<li><strong class="bold">Easy integration</strong>: Redshift ML makes it easy to integrate your ML model into your data pipeline, allowing you to use it for real-time predictions or <span class="No-Break">batch predictions</span></li>
				<li><strong class="bold">Scalability</strong>: Redshift ML is built on top of the highly scalable and performant Amazon Redshift data warehouse, so you can use your ML model to make predictions<a id="_idIndexMarker511"/> on large datasets without worrying about <span class="No-Break">performan<a id="_idTextAnchor201"/>ce issues</span></li>
			</ul>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor202"/>Supported model types</h2>
			<p>Amazon <a id="_idIndexMarker512"/>Redshift ML supports a wide range of machine learning models through the BYOM feature. Some common types of models that can be used with BYOM include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Linear regression models</strong>: These models are like number predictors. They take into account <a id="_idIndexMarker513"/>several factors or features and use them to guess a specific numerical outcome. For example, if you want to predict the price of a house, a linear regression model would consider factors such as the size of the house, the number of rooms, and the location to estimate the <span class="No-Break">house’s price.</span></li>
				<li><strong class="bold">Logistic regression models</strong>: These models are binary outcome predictors. Instead of guessing<a id="_idIndexMarker514"/> numbers, they answer <em class="italic">yes</em> or <em class="italic">no</em> questions or make <em class="italic">0</em>/<em class="italic">1</em> predictions. For instance, if you want to predict whether a student will pass or fail an exam, a logistic regression model would consider factors such as the student’s study hours, previous test scores, and attendance to determine the likelihood of passing <span class="No-Break">the exam.</span></li>
				<li><strong class="bold">Decision tree models</strong>: These are used to make predictions based on a tree-like structure. Think of it like a <a id="_idIndexMarker515"/>decision-making tree for predictions. You start at the top and follow branches based on known features. At each branch, you make a decision based on a feature and keep going until you reach a final prediction at the leaves. It’s a step-by-step process to find the most <span class="No-Break">likely outcome.</span></li>
				<li><strong class="bold">Random forest models</strong>: These <a id="_idIndexMarker516"/>are ensembles of decision trees. Groups of decision trees work together. Each tree is trained on a different part of the data. To make a prediction, all the trees give their answers, and their predictions are averaged to get the final result. It’s like taking the opinions of multiple trees to make a more <span class="No-Break">accurate guess.</span></li>
				<li><strong class="bold">Gradient boosting models</strong>: These are also ensembles of decision trees, These are groups of decision <a id="_idIndexMarker517"/>trees that work together, but here, unlike in a random forest model, the trees are trained one after the other, and each tree tries to fix the mistakes of the previous one. They learn from each other’s errors and become better as a team. It’s like a learning process where they keep improving until they make good <span class="No-Break">predictions together.</span></li>
				<li><strong class="bold">Neural network models</strong>: These<a id="_idIndexMarker518"/> are complex, multi-layered models that are able to learn complex patterns in data. These models are capable of learning intricate patterns in data. They operate using a process of information analysis, discovering underlying correlations similar to the functioning of interconnected neurons in the human brain. Through extensive training and exposure to diverse datasets, the model refines its ability to decipher complex patterns, making it proficient in uncovering intricate relationships within <span class="No-Break">new data.</span></li>
				<li><strong class="bold">Support vector machines</strong> (<strong class="bold">SVMs</strong>): SVMs are <a id="_idIndexMarker519"/>powerful classifiers, acting like incredibly intelligent dividers. Imagine a 3D space with points representing different things. SVMs determine the most optimal way to draw a line or plane, called a hyperplane, that perfectly separates two distinct groups of points. It’s as if they possess an extraordinary ability to find the perfect boundary, ensuring the two groups are kept as far apart as possible, such as drawing an invisible but flawless line that keeps <a id="_idIndexMarker520"/>everything perfectly organized on <span class="No-Break">each side.</span></li>
			</ul>
			<p>These are just a few examples of the types of models that can be used with BYOM in Amazon Redshift. In general, any model that can be represented as a set of model artifacts and a prediction function can be used with BYOM <span class="No-Break">in Redshift.</span></p>
			<p>We have learned what Redshift ML BYOM is and its benefits. In the next section, you will create a BYOM local<a id="_idTextAnchor203"/> <span class="No-Break">inference model.</span></p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor204"/>Creating the BYOM local inference model</h1>
			<p>With BYOM <a id="_idIndexMarker521"/>local inference, the machine learning model and its dependencies are packaged into a group of files and deployed to Amazon Redshift where the data is stored, allowing users to make predictions on the stored data. Model artifacts and their dependencies are created when a model is trained and created on the Amazon SageMaker platform. By deploying the model directly onto the Redshift service, you are not moving the data over the network to another service. Local inference can be useful for scenarios where the data is sensitive or requires low <span class="No-Break">latency predictions.</span></p>
			<p>Let’s start working on creating the BYOM loc<a id="_idTextAnchor205"/>al <span class="No-Break">inference model.</span></p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor206"/>Creating a local inference model</h2>
			<p>To create the<a id="_idIndexMarker522"/> BYOM local inference model, the first step involves training and validating an Amazon SageMaker model. For this purpose, we will train and validate an XGBoost linear regression machine learning model on Amazon SageMaker. Follow the instructions found here to create the Amazon <span class="No-Break">SageMaker model:</span></p>
			<p><a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb"><span class="No-Break">https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb</span></a></p>
			<p>After you have followed the instructions given at the preceding URL, validate the model by running prediction functions. Now, let’s move on to the next steps. After successfully generating the predictions, we will create the Redshift ML model. Using the same <a id="_idIndexMarker523"/>notebook, let’s run a few commands to <a id="_idTextAnchor207"/>set <span class="No-Break">some parameters.</span></p>
			<h3>Creating the model and running predictions on Redshift</h3>
			<p>Now, validate<a id="_idIndexMarker524"/> the model by running <span class="No-Break">prediction functions.</span></p>
			<p>With the model trained and validated in SageMaker, it’s time to import it into Redshift. In the next section, using the same SageMaker notebook, we will set up the required parameters to build the Redshift <strong class="source-inline">CREATE MODEL</strong> statement. You will use this statement in Query Editor v2 to create your model in Redshift ML, enabling you to perform local inference on the data stored in the Redshift cluster with the integrated <span class="No-Break">SageMaker model.</span></p>
			<h3>Setting up the parameters</h3>
			<p>Before setting up the <a id="_idIndexMarker525"/>parameters, run the following command in Query Editor v2 to create the schema for <span class="No-Break">this chapter:</span></p>
			<pre class="source-code">
Create schema chapter11_byom;</pre>
			<p>The first step of this process is setting up the following <span class="No-Break">parameter values:</span></p>
			<ul>
				<li><strong class="source-inline">S3_BUCKET</strong> is used to store Redshift <span class="No-Break">ML artifacts.</span></li>
				<li><strong class="source-inline">MODEL_PATH</strong> is the S3 location of the model artifact of the Amazon SageMaker model. Optionally, you can print <strong class="source-inline">model_data</strong> using the <strong class="source-inline">print</strong> function in Python and look at the <span class="No-Break">artifact location.</span></li>
				<li><strong class="source-inline">REDSHIFT_IAM_ROLE</strong> is the <span class="No-Break">cluster role:</span></li>
			</ul>
			<pre class="source-code">
#provide your s3 bucket here
S3_BUCKET='Redshift ML s3 bucket name'
#provide the model path, this is coming from the model_data parameter
MODEL_PATH=model_data
#Provide Redshift cluster attached role ARN
REDSHIFT_IAM_ROLE = 'Redshift Cluster IAM Role'</pre>
			<p>Next, we will <a id="_idIndexMarker526"/>generate the <strong class="source-inline">CREATE MODEL</strong> statement that you are going to run <span class="No-Break">on Redshift.</span></p>
			<h3>Generating the CREATE MODEL statement</h3>
			<p>Execute the code provided here in a <a id="_idIndexMarker527"/>Jupyter notebook to automatically generate the <strong class="source-inline">CREATE </strong><span class="No-Break"><strong class="source-inline">MODEL</strong></span><span class="No-Break"> statement:</span></p>
			<pre class="source-code">
sql_text=("drop model if exists predict_abalone_age; \
 CREATE MODEL chapter11_byom.predict_abalone_age \
FROM '{}' \
FUNCTION predict_abalone_age ( int, int, float, float,float,float,float,float,float) \
RETURNS int \
IAM_ROLE '{}' \
settings( S3_BUCKET '{}') \
")
print (sql_text.format(model_data,REDSHIFT_IAM_ROLE, S3_BUCKET))</pre>
			<p>The output of the preceding statement is the <strong class="source-inline">CREATE MODEL</strong> statement that you are going to run in Query Editor v2. Please copy the statement and head over to Query Editor v2 to perform the <span class="No-Break">remaining steps.</span></p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor208"/>Running local inference on Redshift</h2>
			<p>The <a id="_idIndexMarker528"/>following is the <strong class="source-inline">CREATE MODEL</strong> statement. You should have a similar one generated, where <strong class="source-inline">FROM</strong>, <strong class="source-inline">IAM_ROLE</strong>, and <strong class="source-inline">S3_BUCKET</strong> have <span class="No-Break">different values:</span></p>
			<pre class="source-code">
CREATE MODEL chapter11_byom.predict_abalone_age
FROM 's3://redshift-ml-22-redshiftmlbucket-1cckvqgktpfe0/sagemaker/DEMO-xgboost-abalone-default/single-xgboost/DEMO-xgboost-regression-2022-12-31-01-45-30/output/model.tar.gz'
FUNCTION predict_abalone_age ( int, int, float, float,float,float,float,float,float) RETURNS int IAM_ROLE 'arn:aws:iam::215830312345:role/spectrumrs'
settings( S3_BUCKET 'redshift-ml-22-redshiftmlbucket-1cckvqgktpfe0') ;</pre>
			<p>In the preceding command, the <strong class="source-inline">FROM</strong> clause takes <strong class="source-inline">model_data</strong> as input, which contains the SageMaker model artifacts. When this command is run, Amazon Redshift ML compiles the model,  deploys it to Redshift, and creates a <strong class="source-inline">predict_abalone_age</strong> prediction function, which is used in an SQL command to generate predictions natively <span class="No-Break">in Redshift.</span></p>
			<p>Once the <strong class="source-inline">CREATE MODEL</strong> statement is completed, you can use the <strong class="source-inline">show model</strong> command to see the <span class="No-Break">model’s status:</span></p>
			<pre class="source-code">
show model chapter11_byom.predict_abalone_age;</pre>
			<p>Here is <span class="No-Break">the output:</span></p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/B19071_11_01.jpg" alt="Figure 11.1 – Local inference model metadata"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Local inference model metadata</p>
			<p>Notice that <strong class="bold">Model State</strong> is <strong class="bold">READY</strong> and <strong class="bold">S3 Model Path</strong> is the one we gave when creating the model. <strong class="bold">Inference Type</strong> is <strong class="bold">Local</strong>, which means the model type is <span class="No-Break">local inference.</span></p>
			<p>We have<a id="_idIndexMarker529"/> successfully created the local inference model; now, let’s prepare a test dataset to test whether the local inference is wo<a id="_idTextAnchor209"/>rking without <span class="No-Break">any issues.</span></p>
			<h3>Data preparation</h3>
			<p>Load the <a id="_idIndexMarker530"/>test data from the S3 bucket to a Redshift table to test our local <span class="No-Break">inference model.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Please update <strong class="source-inline">IAM_ROLE</strong>. Do not change the S3 <span class="No-Break">bucket location.</span></p>
			<p>Run the <a id="_idIndexMarker531"/>following command to create the table and load <span class="No-Break">the data:</span></p>
			<pre class="source-code">
drop table if exists chapter11_byom.abalone_test;
create table chapter11_byom.abalone_test
(Rings int, sex int,Length_ float, Diameter float, Height float, WholeWeight float, ShuckedWeight float,VisceraWeight float, ShellWeight float );
copy chapter11_byom.abalone_test
from 's3://jumpstart-cache-prod-us-east-1/1p-notebooks-datasets/abalone/text-csv/test/'
IAM_ROLE 'arn:aws:iam::212330312345:role/spectrumrs'
csv ;</pre>
			<p>Sample the test table to make sure the data <span class="No-Break">is loaded:</span></p>
			<pre class="source-code">
select * from chapter11_byom.abalone_test limit 10;</pre>
			<p>Here is the <span class="No-Break">sample dataset:</span></p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/B19071_11_02.jpg" alt="Figure 11.2 – Showing sample records from the test dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Showing sample records from the test dataset</p>
			<p>Now that we have loaded the test data, let’s run the <strong class="source-inline">SELECT</strong> command, which invokes the <span class="No-Break"><strong class="source-inline">pre<a id="_idTextAnchor210"/>dict_abalone_age</strong></span><span class="No-Break"> function.</span></p>
			<h3>Inference</h3>
			<p>Now, call the <a id="_idIndexMarker532"/>prediction function that was created as part of the <strong class="source-inline">CREATE </strong><span class="No-Break"><strong class="source-inline">MODEL</strong></span><span class="No-Break"> statement:</span></p>
			<pre class="source-code">
Select original_age, predicted_age, original_age-predicted_age as Error
From(
select predict_abalone_age(Rings,sex,
Length_ ,
Diameter ,
Height ,
WholeWeight ,
ShuckedWeight ,
VisceraWeight ,
ShellWeight ) predicted_age, rings as original_age
from chapter11_byom.abalone_test ) a;</pre>
			<p>Here’s the output of the predictions generated using <span class="No-Break">local inference:</span></p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/B19071_11_03.jpg" alt="Figure 11.3 – Showing actual versus predicted values"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Showing actual versus predicted values</p>
			<p>We have successfully trained and validated a SageMaker model and then deployed it to Redshift ML. We also generated predictions using the local inference function. This demonstrates Redshift’s BYOM local <span class="No-Break">inference feature.</span></p>
			<p>In the next section, you are going to learn about the BYOM<a id="_idTextAnchor211"/> remote <span class="No-Break">inference feature.</span></p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor212"/>BYOM using a SageMaker endpoint for remote inference</h1>
			<p>In this section, we will explore <a id="_idIndexMarker533"/>how to create a BYOM remote inference for an Amazon SageMaker Random Cut <a id="_idIndexMarker534"/>Forest model. This means you are bringing your own machine learning model, which is trained on data outside of Redshift, and using it to make predictions on data stored in a Redshift cluster using an endpoint. In this method, to use BYOM for remote inference, a machine learning model is trained, an endpoint is created in Amazon SageMaker, and then the endpoint is accessed from within a Redshift query using SQL functions provided by the Amazon Redshift <span class="No-Break">ML extension.</span></p>
			<p>This <a id="_idIndexMarker535"/>method is useful when Redshift ML does not natively support models, for example, a Random<a id="_idIndexMarker536"/> Cut Forest model. You can read more about Random Cut Forest <span class="No-Break">here<a id="_idTextAnchor213"/>: </span><a href="https://tinyurl.com/348v8nnw"><span class="No-Break">https://tinyurl.com/348v8nnw</span></a><span class="No-Break">.</span></p>
			<p>To demonstrate this feature, you will first need to follow the instructions found in this notebook (<a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb">https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb</a>) to create a Random Cut Forest machine learning model using Amazon SageMaker to detect anomalies. Please complete the Amazon SageMaker model training and validate the model to make sure the endpoint is working and then proceed to the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor214"/>Creating BYOM remote inference</h2>
			<p>Once you have<a id="_idIndexMarker537"/> validated that the SageMaker endpoint is deployed and working properly, let’s define a <strong class="source-inline">CREATE MODEL</strong> reference point inside Redshift by specifying the SageMaker endpoint. Using the same notebook, let’s build the <strong class="source-inline">CREATE MODEL</strong> statement in Jupyter and run it in Query <span class="No-Break">Editor v2.</span></p>
			<h3>Setting up the parameters</h3>
			<p>Let’s start by <a id="_idIndexMarker538"/>setting up <span class="No-Break">the parameters:</span></p>
			<ul>
				<li><strong class="source-inline">S3_Bucket</strong> is used to store Redshift <span class="No-Break">ML artifacts</span></li>
				<li><strong class="source-inline">SAGEMAKER_ENDPOINT</strong> is the model endpoint on the SageMaker side to run <span class="No-Break">inferences against</span></li>
				<li><strong class="source-inline">REDSHIFT_IAM_ROLE</strong> is the <span class="No-Break">cluster role:</span></li>
			</ul>
			<pre class="source-code">
REDSHIFT_IAM_ROLE = 'arn:aws:iam::215830312345:role/spectrumrs'
SAGEMAKER_ENDPOINT = rcf_inference.endpoint</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Please <a id="_idIndexMarker539"/>update <strong class="source-inline">REDSHIFT_IAM_ROLE</strong> with your Redshift <span class="No-Break">cluster role.</span></p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor215"/>Generating the BYOM remote inference command</h2>
			<p>Let’s <a id="_idIndexMarker540"/>generate the <strong class="source-inline">CREATE MODEL</strong> statement by running the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
sql_text=("drop model if exists chapter11_byom.remote_random_cut_forest;\
CREATE MODEL chapter11_byom.remote_random_cut_forest\
 FUNCTION remote_fn_rcf (int)\
 RETURNS decimal(10,6)\
 SAGEMAKER'{}'\
 IAM_ROLE'{}'\
")
print(sql_text.format(SAGEMAKER_ENDPOINT,REDSHIFT_IAM_ROLE))</pre>
			<p>You have finished the work with the Jupyter notebook. Now you have a pre-trained model in Amazon SageMaker and the next step is to bring it into Redshift ML. To do so, access Query Editor v2, connect to the Serverless endpoint, and run the commands <span class="No-Break">outlined next.</span></p>
			<p>In Query Editor v2, run the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
CREATE MODEL chapter11_byom.remote_random_cut_forest
FUNCTION remote_fn_rcf (int) RETURNS decimal(10,6)
SAGEMAKER'randomcutforest-2022-12-31-03-48-13-259'
IAM_ROLE'arn:aws:iam:: 215830312345:role/spectrumrs'
;</pre>
			<p>Retrieve<a id="_idIndexMarker541"/> the model metadata by running the <strong class="source-inline">show </strong><span class="No-Break"><strong class="source-inline">model</strong></span><span class="No-Break"> command:</span></p>
			<pre class="source-code">
show model chapter11_byom.remote_random_cut_forest;</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/B19071_11_04.jpg" alt="Figure 11.4 – Remote inference model metadata"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Remote inference model metadata</p>
			<p>Notice that in the model metadata, the <strong class="bold">Model State</strong> parameter is set to <strong class="bold">READY</strong>, indicating that the model is deployed. The <strong class="bold">Endpoint</strong> name is <strong class="bold">randomcutforest-2022-12-31-03-48-13-259</strong>. <strong class="bold">Inference Type</strong> is set to <strong class="bold">Remote</strong> inference. When this model is run, Redshift ML sends data stored in Redshift in batches to SageMaker, where <a id="_idIndexMarker542"/>inferences are generated. Generated predicted values are then sent back to Redshift, which are eventually presented to <span class="No-Break">the user.</span></p>
			<p>We have successfully deployed the model. In the next section, let’s <span class="No-Break">run predictions.</span></p>
			<h3>The data preparation script</h3>
			<p>The<a id="_idIndexMarker543"/> following code snippet shows the data preparation script that you will need to run on Redshift. We will create the table that will be used to run <span class="No-Break">inference on:</span></p>
			<pre class="source-code">
COPY chapter11_byom.rcf_taxi_data
FROM 's3://sagemaker-sample-files/datasets/tabular/anomaly_benchmark_taxi/NAB_nyc_taxi.csv'
IAM_ROLE 'arn:aws:iam::215830312345:role/spectrumrs' ignoreheader 1 csv delimiter ',';</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Please update the <strong class="source-inline">IAM_ROLE</strong> parameter with your Redshift cluster <span class="No-Break">attached role.</span></p>
			<p>Sample the data to make sure data <span class="No-Break">is loaded:</span></p>
			<pre class="source-code">
select * from chapter11_byom.rcf_taxi_data limit 10;</pre>
			<p>Here’s <span class="No-Break">the output:</span></p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/B19071_11_05.jpg" alt="Figure 11.5 – Showing sample records from the test dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Showing sample records from the test dataset</p>
			<p>Now that <a id="_idIndexMarker544"/>we have the remote inference endpoint and test dataset, let’s invoke the <span class="No-Break">prediction function.</span></p>
			<h3>Computing anomaly scores</h3>
			<p>Now, let’s <a id="_idIndexMarker545"/>compute the anomaly scores from the entire <span class="No-Break">taxi dataset:</span></p>
			<pre class="source-code">
select ride_timestamp, nbr_passengers, chapter11_byom.remote_fn_rcf(nbr_passengers) as score
from chapter11_byom.rcf_taxi_data;</pre>
			<p>The following is the output of the remote <span class="No-Break">inference predictions:</span></p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/B19071_11_06.jpg" alt="Figure 11.6 – Showing remote function prediction values"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Showing remote function prediction values</p>
			<p>The <a id="_idIndexMarker546"/>preceding output shows the anomalous score for different days and the number <span class="No-Break">of passengers.</span></p>
			<p>In the following code snippet, we will print any data points with scores greater than 3 and standard deviations (approximately the 99.9th percentile) from the <span class="No-Break">mean score:</span></p>
			<pre class="source-code">
with score_cutoff as
(select stddev(chapter11_byom.remote_fn_rcf(nbr_passengers)) as std, avg(chapter11_byom.remote_fn_rcf(nbr_passengers)) as mean, ( mean + 3 * std ) as score_cutoff_value
From chapter11_byom.rcf_taxi_data)
select ride_timestamp, nbr_passengers, chapter11_byom.remote_fn_rcf(nbr_passengers) as score
from chapter11_byom.3rcf_taxi_data
where score &gt; (select score_cutoff_value from score_cutoff)
;</pre>
			<p>The<a id="_idIndexMarker547"/> output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/B19071_11_07.jpg" alt="Figure 11.7 – Showing unacceptable anomaly scores"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Showing unacceptable anomaly scores</p>
			<p>In the preceding results, we see that some days’ ridership is way higher and our remote inference function is flagging them as anomalous. This concludes the section on bringing remote inference models <span class="No-Break">into Redshift.</span></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor216"/>Summary</h1>
			<p>In this chapter, we discussed the benefits and use cases of Amazon Redshift ML BYOM for local and remote inference. We created two SageMaker models and then imported them into Redshift ML as local inference and remote inference model types. We loaded test datasets in Redshift and then we ran the prediction functions and validated both types. This demonstrates how Redshift simplifies and empowers the business community to perform inference on new data using models created outside. This method speeds up the delivery of machine learning models created outside of Redshift to the data <span class="No-Break">warehouse team.</span></p>
			<p>In the next chapter, you are going to learn about Amazon Forecast, which enables you to perform forecasting using <span class="No-Break">Redshift ML.</span></p>
		</div>
	</body></html>