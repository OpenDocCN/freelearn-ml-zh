<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11" class="calibre1"/>Chapter 11. Processing Video Sequences</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Reading video sequences</li><li class="listitem">Processing the video frames</li><li class="listitem">Writing video sequences</li><li class="listitem">Tracking feature points in a video</li><li class="listitem">Extracting the foreground objects in a video</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch11lvl1sec66" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre8">Video signals constitute a rich source of visual information. They are made of a sequence of images, called <a id="id850" class="calibre1"/>
<strong class="calibre2">frames</strong>, that are taken at regular time intervals (specified as the <a id="id851" class="calibre1"/>
<strong class="calibre2">frame rate</strong>, generally expressed in frames per second) and show a scene in motion. With the advent of powerful computers, it is now possible to perform advanced visual analysis on video sequences—sometimes at rates close to, or even faster than, the actual video frame rate. This chapter will show you how to read, process, and store video sequences.</p><p class="calibre8">We will see that once the individual frames of a video sequence have been extracted, the different image processing functions presented in this book can be applied to each of them. In addition, we will also look at a few algorithms that perform a temporal analysis of the video sequence, compare adjacent frames to track objects, or cumulate image statistics over time in order to extract foreground objects.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec67" class="calibre1"/>Reading video sequences</h1></div></div></div><p class="calibre8">In <a id="id852" class="calibre1"/>order to process a video sequence, we need to be able to read each of its frames. OpenCV has put in place an easy-to-use framework that can help us perform frame extraction from video files or even from USB or IP cameras. This recipe shows you how to use it.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec196" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">Basically, all you need to do in order to read the frames of a video sequence is create an instance of<a id="id853" class="calibre1"/> the <code class="email">cv::VideoCapture</code> class. You then create a loop that will extract and read each video frame. Here is a basic <code class="email">main</code> function that displays the frames of a video sequence:</p><div><pre class="programlisting">int main()
{
  // Open the video file
  cv::VideoCapture capture("bike.avi");
  // check if video successfully opened
  if (!capture.isOpened())
    return 1;

  // Get the frame rate
  double rate= capture.get(CV_CAP_PROP_FPS);

  bool stop(false);
  cv::Mat frame; // current video frame
  cv::namedWindow("Extracted Frame");

  // Delay between each frame in ms
  // corresponds to video frame rate
  int delay= 1000/rate;

  // for all frames in video
  while (!stop) {

    // read next frame if any
    if (!capture.read(frame))
      break;

    cv::imshow("Extracted Frame",frame);

    // introduce a delay
    // or press key to stop
    if (cv::waitKey(delay)&gt;=0)
      stop= true;
  }

  // Close the video file.
  // Not required since called by destructor
  capture.release();
  return 0;
}</pre></div><p class="calibre8">A window will appear on which the video will play as shown in the following screenshot:</p><div><img src="img/00186.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec197" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">To open a video, you <a id="id854" class="calibre1"/>simply need to specify the video filename. This can be done by providing the name of the file in <a id="id855" class="calibre1"/>the constructor of the <code class="email">cv::VideoCapture</code> object. It is also possible to use the open method if the <code class="email">cv::VideoCapture object</code> has already been created. Once the video is successfully opened (this can be verified through the <code class="email">isOpened</code> method), it is possible to start the frame extraction. It is also possible to query the <code class="email">cv::VideoCapture</code> object for information associated with the video file by using its get method with the appropriate flag. In the preceding example, we obtained the frame rate using the <code class="email">CV_CAP_PROP_FPS</code> flag. Since it is a generic function, it always returns a double even if another type would be expected in some cases. For example, the total number of frames in the video file would be obtained (as an integer) as follows:</p><div><pre class="programlisting">long t= static_cast&lt;long&gt;(
              capture.get(CV_CAP_PROP_FRAME_COUNT));</pre></div><p class="calibre8">Have a look at the different flags that are available in the OpenCV documentation in order to find out what information can be obtained from the video.</p><p class="calibre8">There is also a <code class="email">set</code> method that allows you to input parameters into the <code class="email">cv::VideoCapture</code> instance. For example, you can request to move to a specific frame using the <code class="email">CV_CAP_PROP_POS_FRAMES</code> flag:</p><div><pre class="programlisting">// goto frame 100
double position= 100.0; 
capture.set(CV_CAP_PROP_POS_FRAMES, position);</pre></div><p class="calibre8">You can <a id="id856" class="calibre1"/>also specify the position in milliseconds using <code class="email">CV_CAP_PROP_POS_MSEC</code>, or you can specify the relative position inside the video using <code class="email">CV_CAP_PROP_POS_AVI_RATIO</code> (with 0.0 corresponding to the beginning of the video and 1.0 to the end). The method returns <code class="email">true</code> if the requested parameter setting is successful. Note that the possibility to get or set a particular video parameter largely depends on the codec that is used to compress and store the video sequence. If you are unsuccessful with some parameters, that could be simply due to the specific codec you are using.</p><p class="calibre8">Once the captured video is successfully opened, the frames can be sequentially obtained by repetitively calling the <code class="email">read</code> method as we did in the example of the previous section. One can equivalently call the overloaded reading operator:</p><div><pre class="programlisting">capture &gt;&gt; frame;</pre></div><p class="calibre8">It is also possible to call the two basic methods:</p><div><pre class="programlisting">capture.grab();
capture.retrieve(frame);</pre></div><p class="calibre8">Also note how, in our example, we introduced a delay in displaying each frame. This is done using the <code class="email">cv::waitKey</code> function. Here, we set the delay at a value that corresponds to the input video frame rate (if <code class="email">fps</code> is the number of frames per second, then <code class="email">1000/fps</code> is the delay between two frames in milliseconds). You can obviously change this value to display the video at a slower or faster speed. However, if you are going to display the video frames, it is important that you insert such a delay if you want to make sure that the window has sufficient time to refresh (since it is a process of low priority, it will never refresh if the CPU is too busy). The <code class="email">cv::waitKey</code> function <a id="id857" class="calibre1"/>also allows us to interrupt the reading process by pressing any key. In such a case, the function returns the ASCII code of the key that is pressed. Note that if the delay specified to the <code class="email">cv::waitKey</code> function is <code class="email">0</code>, then it will wait indefinitely for the user to press a key. This is very useful if someone wants to trace a process by examining the results frame by frame.</p><p class="calibre8">The final statement calls the <code class="email">release</code> method, which will close the video file. However, this call is not required since <code class="email">release</code> is also called by the <code class="email">cv::VideoCapture</code> destructor.</p><p class="calibre8">It is important to note that in order to open the specified video file, your computer must have the <a id="id858" class="calibre1"/>corresponding codec installed; otherwise, <code class="email">cv::VideoCapture</code> will not be able to decode the input file. Normally, if you are able to open your video file with a video player on your machine (such as Windows Media Player), then OpenCV should also be able to read this file.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec198" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">You can also read the video stream capture of a camera that is connected to your computer (a USB camera, for example). In this case, you simply specify an ID number (an integer) instead of a filename to the <code class="email">open</code> function. Specifying <code class="email">0</code> for the ID will open the default installed camera. In this case, the role of the <a id="id859" class="calibre1"/>
<code class="email">cv::waitKey</code> function that stops the processing becomes essential, since the video stream from the camera will be infinitely read.</p><p class="calibre8">Finally, it is also possible to load a video from the Web. In this case, all you have to do is provide the correct address, for example:</p><div><pre class="programlisting">  cv::VideoCapture capture("http://www.laganiere.name/bike.avi");</pre></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec199" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre9">Writing video sequences</em> recipe in this chapter has more information on video codecs.</li><li class="listitem">The <a class="calibre1" href="http://ffmpeg.org/">http://ffmpeg.org/</a> website presents a complete open source and cross-platform solution for audio/video reading, recording, converting, and streaming. The OpenCV classes that manipulate video files are built on top of this library.</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec68" class="calibre1"/>Processing the video frames</h1></div></div></div><p class="calibre8">In this<a id="id860" class="calibre1"/> recipe, our objective is to apply some processing function to each of the frames of a video sequence. We will do this by encapsulating the OpenCV video capture framework into our own class. Among other things, this class will allow us to specify a function that will be called each time a new frame is extracted.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec200" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">What we want is to be able to specify a processing function (a callback function) that will be called for each frame of a video sequence. This function can be defined as receiving a <code class="email">cv::Mat</code> instance and outputting a processed frame. Therefore, in our framework, the processing function must have the following signature to be a valid callback:</p><div><pre class="programlisting">void processFrame(cv::Mat&amp; img, cv::Mat&amp; out);</pre></div><p class="calibre8">As an example of such a processing function, consider the following simple function that computes the Canny edges of an input image:</p><div><pre class="programlisting">void canny(cv::Mat&amp; img, cv::Mat&amp; out) {
  // Convert to gray
  if (img.channels()==3)
    cv::cvtColor(img,out,CV_BGR2GRAY);
  // Compute Canny edges
  cv::Canny(out,out,100,200);
  // Invert the image
  cv::threshold(out,out,128,255,cv::THRESH_BINARY_INV);
}</pre></div><p class="calibre8">Our <code class="email">VideoProcessor</code> class<a id="id861" class="calibre1"/> encapsulates all aspects of a video-processing task. Using this class, the procedure will be to create a class instance, specify an input video file, attach the callback function to it, and then start the process. Programmatically, these steps are accomplished using our proposed class, as follows:</p><div><pre class="programlisting">  // Create instance
  VideoProcessor processor;
  // Open video file
  processor.setInput("bike.avi");
  // Declare a window to display the video
  processor.displayInput("Current Frame");
  processor.displayOutput("Output Frame");
  // Play the video at the original frame rate
  processor.setDelay(1000./processor.getFrameRate());
  // Set the frame processor callback function
  processor.setFrameProcessor(canny);
  // Start the process
  processor.run();</pre></div><p class="calibre8">If this code is run, then two windows will play the input video and the output result at the original frame rate (a consequence of the delay introduced by the <code class="email">setDelay</code> method). For example, considering the input video for which a frame is shown in the previous recipe, the output window will look as follows:</p><div><img src="img/00187.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec201" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">As we did in <a id="id862" class="calibre1"/>other recipes, our objective was to create a class that encapsulates the common functionalities of a video-processing algorithm. As one might expect, the class includes several member variables that control the different aspects of the video frame processing:</p><div><pre class="programlisting">class VideoProcessor {

  private:

  // the OpenCV video capture object
  cv::VideoCapture capture;
  // the callback function to be called 
  // for the processing of each frame
  void (*process)(cv::Mat&amp;, cv::Mat&amp;);
  // a bool to determine if the 
  // process callback will be called
  bool callIt;
  // Input display window name
  std::string windowNameInput;
  // Output display window name
  std::string windowNameOutput;
  // delay between each frame processing
  int delay;
  // number of processed frames 
  long fnumber;
  // stop at this frame number
  long frameToStop;
  // to stop the processing
  bool stop;</pre></div><p class="calibre8">The first <a id="id863" class="calibre1"/>member variable is the <a id="id864" class="calibre1"/>
<code class="email">cv::VideoCapture</code> object. The second attribute is the <code class="email">process</code> function pointer that will point to the callback function. This function can be specified using the corresponding setter method:</p><div><pre class="programlisting">    // set the callback function that 
    // will be called for each frame
    void setFrameProcessor(
      void (*frameProcessingCallback)
        cv::Mat&amp;, cv::Mat&amp;)) {

          process= frameProcessingCallback;
    }</pre></div><p class="calibre8">The following method opens the video file:</p><div><pre class="programlisting">    // set the name of the video file
    bool setInput(std::string filename) {

      fnumber= 0;
      // In case a resource was already 
      // associated with the VideoCapture instance
      capture.release();
      // Open the video file
      return capture.open(filename);
   }</pre></div><p class="calibre8">It is generally interesting to display the frames as they are processed. Therefore, two methods are used to create the display windows:</p><div><pre class="programlisting">    // to display the input frames
    void displayInput(std::string wn) {

      windowNameInput= wn;
      cv::namedWindow(windowNameInput);
    }

    // to display the processed frames
    void displayOutput(std::string wn) {
      windowNameOutput= wn;
      cv::namedWindow(windowNameOutput);
    }</pre></div><p class="calibre8">The main <a id="id865" class="calibre1"/>method, called <code class="email">run</code>, is the one that contains the frame extraction loop:</p><div><pre class="programlisting">    // to grab (and process) the frames of the sequence
    void run() {

      // current frame
      cv::Mat frame;
      // output frame
      cv::Mat output;

      // if no capture device has been set
      if (!isOpened())
        return;

      stop= false;

      while (!isStopped()) {

        // read next frame if any
        if (!readNextFrame(frame))
          break;

        // display input frame
        if (windowNameInput.length()!=0) 
          cv::imshow(windowNameInput,frame);

        // calling the process function
        if (callIt) {

          // process the frame
          process(frame, output);
          // increment frame number
          fnumber++;

          } else { // no processing
            output= frame;
          }

          // display output frame
          if (windowNameOutput.length()!=0) 
            cv::imshow(windowNameOutput,output);
          // introduce a delay
          if (delay&gt;=0 &amp;&amp; cv::waitKey(delay)&gt;=0)
            stopIt();

          // check if we should stop
          if (frameToStop&gt;=0 &amp;&amp; 
            getFrameNumber()==frameToStop)
              stopIt();
        }
      }

      // Stop the processing
      void stopIt() {

        stop= true;
      }

      // Is the process stopped?
      bool isStopped() {

        return stop;
      }

      // Is a capture device opened?
      bool isOpened() {

        capture.isOpened();
      }

      // set a delay between each frame
      // 0 means wait at each frame
      // negative means no delay
      void setDelay(int d) {

        delay= d;
      }</pre></div><p class="calibre8">This <a id="id866" class="calibre1"/>method uses a <code class="email">private</code> method that reads the frames:</p><div><pre class="programlisting">    // to get the next frame 
    // could be: video file or camera
    bool readNextFrame(cv::Mat&amp; frame) {

      return capture.read(frame);
    }</pre></div><p class="calibre8">The <code class="email">run</code> method proceeds by first calling the <code class="email">read</code> method of the <code class="email">cv::VideoCapture</code> OpenCV class. There <a id="id867" class="calibre1"/>is then a series of operations that are executed, but before each of them is invoked, a check is made to determine whether it has been requested. The input window is displayed only if an input window name has been specified (using the <code class="email">displayInput</code> method); the callback function is called only if one has been specified (using <code class="email">setFrameProcessor</code>). The output window is displayed only if an output window name has been defined (using <code class="email">displayOutput</code>); a delay is introduced only if one has been specified (using <code class="email">setDelay</code> method). Finally, the current frame number is checked if a stop frame has been defined (using <code class="email">stopAtFrameNo</code>).</p><p class="calibre8">One might also wish to simply open and play the video file (without calling the callback function). Therefore, we have two methods that specify whether or not we want the callback function to be called:</p><div><pre class="programlisting">    // process callback to be called
    void callProcess() {

      callIt= true;
    }

    // do not call process callback
    void dontCallProcess() {

      callIt= false;
    }</pre></div><p class="calibre8">Finally, the class also offers us the possibility to stop at a certain frame number:</p><div><pre class="programlisting">    void stopAtFrameNo(long frame) {

      frameToStop= frame;
    }

    // return the frame number of the next frame
    long getFrameNumber() {

      // get info of from the capture device
      long fnumber= static_cast&lt;long&gt;(
        capture.get(CV_CAP_PROP_POS_FRAMES));
      return fnumber; 
    }</pre></div><p class="calibre8">The class also contains a number of getter and setter methods that are basically just a wrapper over the general <code class="email">set</code> and <code class="email">get</code> methods of the <code class="email">cv::VideoCapture</code> framework.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec202" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">Our <code class="email">VideoProcessor</code> class is there to facilitate the deployment of a video-processing module. Few additional refinements can be made to it.</p><div><div><div><div><h3 class="title2"><a id="ch11lvl3sec52" class="calibre1"/>Processing a sequence of images</h3></div></div></div><p class="calibre8">Sometimes, the <a id="id868" class="calibre1"/>input sequence is made of a series of images that are individually stored in distinct files. Our class can be easily modified to accommodate such input. You just need to add a member variable that will hold a vector of image filenames and its corresponding iterator:</p><div><pre class="programlisting">    // vector of image filename to be used as input
    std::vector&lt;std::string&gt; images; 
    // image vector iterator
    std::vector&lt;std::string&gt;::const_iterator itImg;</pre></div><p class="calibre8">A new <code class="email">setInput</code> method is used to specify the filenames to be read:</p><div><pre class="programlisting">    // set the vector of input images
    bool setInput(const std::vector&lt;std::string&gt;&amp; imgs) {

      fnumber= 0;
      // In case a resource was already 
      // associated with the VideoCapture instance
      capture.release();

      // the input will be this vector of images
      images= imgs;
      itImg= images.begin();

      return true;
    }</pre></div><p class="calibre8">The <code class="email">isOpened</code> method becomes as follows:</p><div><pre class="programlisting">    // Is a capture device opened?
    bool isOpened() {

      return capture.isOpened() || !images.empty();
    }</pre></div><p class="calibre8">The last method that needs to be modified is the private <code class="email">readNextFrame</code> method that will read from the video or from the vector of filenames, depending on the input that has been specified. The test is that if the vector of image filenames is not empty, then that is because the input is an image sequence. The call to <code class="email">setInput</code> with a video filename clears this <a id="id869" class="calibre1"/>vector:</p><div><pre class="programlisting">    // to get the next frame 
    // could be: video file; camera; vector of images
    bool readNextFrame(cv::Mat&amp; frame) {

      if (images.size()==0)
        return capture.read(frame);

      else {

          if (itImg != images.end()) {

            frame= cv::imread(*itImg);
            itImg++;
            return frame.data != 0;

          } else

            return false;
      }
    }</pre></div></div><div><div><div><div><h3 class="title2"><a id="ch11lvl3sec53" class="calibre1"/>Using a frame processor class</h3></div></div></div><p class="calibre8">In an <a id="id870" class="calibre1"/>object-oriented context, it might make more sense to use a frame processing class instead of a frame processing function. Indeed, a class would give the programmer much more flexibility in the definition of a video-processing algorithm. We can, therefore, define an interface that any class that wishes to be used inside the <code class="email">VideoProcessor</code> will need to implement:</p><div><pre class="programlisting">// The frame processor interface
class FrameProcessor {

  public:
  // processing method
  virtual void process(cv:: Mat &amp;input, cv:: Mat &amp;output)= 0;
};</pre></div><p class="calibre8">A setter method allows you to input a <code class="email">FrameProcessor</code> instance to the <code class="email">VideoProcessor</code> framework and assign it to the added member variable <code class="email">frameProcessor</code> that is defined as a pointer to a <code class="email">FrameProcessor</code> object:</p><div><pre class="programlisting">    // set the instance of the class that 
    // implements the FrameProcessor interface
    void setFrameProcessor(FrameProcessor* frameProcessorPtr)
    {

      // invalidate callback function
      process= 0;
      // this is the frame processor instance 
      // that will be called
      frameProcessor= frameProcessorPtr;
      callProcess();
    }</pre></div><p class="calibre8">When a <a id="id871" class="calibre1"/>frame processor class instance is specified, it invalidates any frame processing function that could have been set previously. The same obviously applies if a frame processing function is specified instead. The <code class="email">while</code> loop of the <code class="email">run</code> method is modified to take into account this modification:</p><div><pre class="programlisting">        while (!isStopped()) {

          // read next frame if any
          if (!readNextFrame(frame))
            break;

          // display input frame
          if (windowNameInput.length()!=0) 
            cv::imshow(windowNameInput,frame);

          // ** calling the process function or method **
          if (callIt) {

            // process the frame
            if (process) // if call back function
              process(frame, output);
            else if (frameProcessor) 
              // if class interface instance
              frameProcessor-&gt;process(frame,output);
            // increment frame number
            fnumber++;
          } else {

            output= frame;
          }
           // display output frame
           if (windowNameOutput.length()!=0)
             cv::imshow(windowNameOutput,output);
           // introduce a delay
           if (delay&gt;=0 &amp;&amp; cv::waitKey(delay)&gt;=0)
             stopIt();
           // check if we should stop
           if (frameToStop&gt;=0 &amp;&amp; getFrameNumber()==frameToStop)
             stopIt();
        }</pre></div></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec203" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre9">Tracking feature points in a video</em> recipe in this chapter gives you an example of how to use the <code class="email">FrameProcessor</code> class interface.</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec69" class="calibre1"/>Writing video sequences</h1></div></div></div><p class="calibre8">In the<a id="id872" class="calibre1"/> previous recipes, we learned how to read a video file and extract its frames. This recipe will show you how to write frames and, therefore, create a video file. This will allow us to complete the typical video-processing chain: reading an input video stream, processing its frames, and then storing the results in a new video file.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec204" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">Writing video files in OpenCV is done using the <a id="id873" class="calibre1"/>
<code class="email">cv::VideoWriter</code> class. An instance is constructed by specifying the filename, the frame rate at which the generated video should play, the size of each frame, and whether or not the video will be created in color:</p><div><pre class="programlisting">writer.open(outputFile, // filename
    codec,          // codec to be used 
    framerate,      // frame rate of the video
    frameSize,      // frame size
    isColor);       // color video?</pre></div><p class="calibre8">In addition, you must specify the way you want the video data to be saved. This is the <code class="email">codec</code> argument; this will be discussed at the end of this recipe.</p><p class="calibre8">Once the video file is opened, frames can be added to it by repetitively calling the <code class="email">write</code> method:</p><div><pre class="programlisting">writer.write(frame); // add the frame to the video file</pre></div><p class="calibre8">Using the <code class="email">cv::VideoWriter</code> class, our <code class="email">VideoProcessor</code> class introduced in the previous recipe can easily be expanded in order to give it the ability to write video files. A simple program that will read a video, process it, and write the result to a video file would then be written as follows:</p><div><pre class="programlisting">    // Create instance
    VideoProcessor processor;

    // Open video file
    processor.setInput("bike.avi");
    processor.setFrameProcessor(canny);
    processor.setOutput("bikeOut.avi");
    // Start the process
    processor.run();</pre></div><p class="calibre8">Proceeding as we did in the preceding recipe, we also want to give the user the possibility to write the <a id="id874" class="calibre1"/>frames as individual images. In our framework, we adopt a naming convention that consists of a prefix name followed by a number made of a given number of digits. This number is automatically incremented as frames are saved. Then, to save the output result as a series of images, you would change the preceding statement with this one:</p><div><pre class="programlisting">    processor.setOutput("bikeOut",  //prefix
    ".jpg",     // extension
    3,          // number of digits
    0)// starting index</pre></div><p class="calibre8">Using the specified number of digits, this call will create the <code class="email">bikeOut000.jpg</code>, <code class="email">bikeOut001.jpg</code>, and <code class="email">bikeOut002.jpg</code> files, and so on.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec205" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">Let's now describe how to modify our <code class="email">VideoProcessor</code> class in order to give it the ability to write video files. First, a <code class="email">cv::VideoWriter</code> variable member must be added to our class (plus a few other attributes):</p><div><pre class="programlisting">class VideoProcessor {

  private:

  ...
  // the OpenCV video writer object
  cv::VideoWriter writer;
  // output filename
  std::string outputFile;
  // current index for output images
  int currentIndex;
  // number of digits in output image filename
  int digits;
  // extension of output images
  std::string extension;</pre></div><p class="calibre8">An extra method is used to specify (and open) the output video file:</p><div><pre class="programlisting">    // set the output video file
    // by default the same parameters than 
    // input video will be used
    bool setOutput(const std::string &amp;filename, int codec=0, double framerate=0.0, bool isColor=true) {

      outputFile= filename;
      extension.clear();

      if (framerate==0.0) 
        framerate= getFrameRate(); // same as input

      char c[4];
      // use same codec as input
      if (codec==0) { 
        codec= getCodec(c);
      }

      // Open output video
      return writer.open(outputFile, // filename
      codec,          // codec to be used 
      framerate,      // frame rate of the video
      getFrameSize(), // frame size
      isColor);       // color video?
    }</pre></div><p class="calibre8">A private<a id="id875" class="calibre1"/> method, called the <code class="email">writeNextFrame</code> method, handles the frame writing procedure (in a video file or as a series of images):</p><div><pre class="programlisting">    // to write the output frame 
    // could be: video file or images
    void writeNextFrame(cv::Mat&amp; frame) {
      if (extension.length()) { // then we write images

        std::stringstream ss;
        // compose the output filename
        ss &lt;&lt; outputFile &lt;&lt; std::setfill('0') &lt;&lt; std::setw(digits) &lt;&lt; currentIndex++ &lt;&lt; extension;
        cv::imwrite(ss.str(),frame);

      } else { // then write to video file 
        writer.write(frame);
      }
    }</pre></div><p class="calibre8">For the case where the output is made of individual image files, we need an additional setter method:</p><div><pre class="programlisting">    // set the output as a series of image files
    // extension must be ".jpg", ".bmp" ...
    bool setOutput(const std::string &amp;filename, // prefix
      const std::string &amp;ext, // image file extension 
      int numberOfDigits=3,   // number of digits
      int startIndex=0) {     // start index

      // number of digits must be positive
      if (numberOfDigits&lt;0)
        return false;

      // filenames and their common extension
      outputFile= filename;
      extension= ext;

      // number of digits in the file numbering scheme
      digits= numberOfDigits;
      // start numbering at this index
      currentIndex= startIndex;

      return true;
    }</pre></div><p class="calibre8">Finally, a new<a id="id876" class="calibre1"/> step is then added to the video capture loop of the <code class="email">run</code> method:</p><div><pre class="programlisting">        while (!isStopped()) {

          // read next frame if any
          if (!readNextFrame(frame))
            break;

          // display input frame
          if (windowNameInput.length()!=0) 
            cv::imshow(windowNameInput,frame);

          // calling the process function or method
          if (callIt) {

            // process the frame
            if (process)
              process(frame, output);
            else if (frameProcessor) 
              frameProcessor-&gt;process(frame,output);
            // increment frame number
            fnumber++;

          } else {

            output= frame;
          }

          // ** write output sequence **
          if (outputFile.length()!=0)
            writeNextFrame(output);

          // display output frame
          if (windowNameOutput.length()!=0) 
            cv::imshow(windowNameOutput,output);
         
          // introduce a delay
          if (delay&gt;=0 &amp;&amp; cv::waitKey(delay)&gt;=0)
            stopIt();

          // check if we should stop
          if (frameToStop&gt;=0 &amp;&amp; getFrameNumber()==frameToStop)
            stopIt();
        }
      }</pre></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec206" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">When a video<a id="id877" class="calibre1"/> is written to a file, it is saved using a codec. A <strong class="calibre2">codec</strong><a id="id878" class="calibre1"/> is a software module that is capable of encoding and decoding video streams. The codec defines both the format of the file and the compression scheme that is used to store the information. Obviously, a video that has been encoded using a given codec must be decoded with the same codec. For this reason, four-character codes have been introduced to uniquely identified codecs. This way, when a software tool needs to write a video file, it determines the codec to be used by reading the specified four-character code.</p><div><div><div><div><h3 class="title2"><a id="ch11lvl3sec54" class="calibre1"/>The codec four-character code</h3></div></div></div><p class="calibre8">As the <a id="id879" class="calibre1"/>name suggests, the four-character code is made up of four ASCII characters that can also be converted into an integer by appending them together. Using the <code class="email">CV_CAP_PROP_FOURCC </code>flag of the get method of an opened <code class="email">cv::VideoCapture</code> instance, you can obtain this code of an opened video file. We can define a method in our <code class="email">VideoProcessor</code> class to return the four-character code of an input video:</p><div><pre class="programlisting">    // get the codec of input video
    int getCodec(char codec[4]) {

      // undefined for vector of images
      if (images.size()!=0) return -1;

      union { // data structure for the 4-char code
        nt value;
        char code[4]; } returned;

      // get the code
      returned.value= static_cast&lt;int&gt;(capture.get(CV_CAP_PROP_FOURCC));

      // get the 4 characters
      codec[0]= returned.code[0];
      codec[1]= returned.code[1];
      codec[2]= returned.code[2];
      codec[3]= returned.code[3];

      // return the int value corresponding to the code
      return returned.value;
    }</pre></div><p class="calibre8">The <code class="email">get</code> method<a id="id880" class="calibre1"/> always returns a <code class="email">double</code> value that is then casted into an integer. This integer represents the code from which the four characters can be extracted using a <code class="email">union</code> data structure. If we open our test video sequence, then we have the following statements:</p><div><pre class="programlisting">  char codec[4];
  processor.getCodec(codec);
  std::cout &lt;&lt; "Codec: " &lt;&lt; codec[0] &lt;&lt; codec[1] &lt;&lt; codec[2] &lt;&lt; codec[3] &lt;&lt; std::endl;</pre></div><p class="calibre8">From the preceding statements, we obtain the following:</p><div><pre class="programlisting">Codec : XVID</pre></div><p class="calibre8">When a video file is written, the codec must be specified using its four-character code. This is the second parameter in the <code class="email">open</code> method of the <code class="email">cv::VideoWriter</code> class. You can use, for example, the same one as the input video (this is the default option in our <code class="email">setOutput</code> method). You can also pass the value <code class="email">-1</code> and the method will pop up a window that will ask you to select one codec from the list of available codecs, as shown here:</p><div><img src="img/00188.jpeg" alt="The codec four-character code" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The list you will<a id="id881" class="calibre1"/> see on this window corresponds to the list of installed codecs on your machine. The code of the selected codec is then automatically sent to the <code class="email">open</code> method.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec207" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <a class="calibre1" href="https://www.xvid.com/">https://www.xvid.com/</a> website offers you an open source video codec library based on the MPEG-4 standard for video compression. Xvid also has a competitor called DivX, which offers proprietary but free codec and software tools.</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec70" class="calibre1"/>Tracking feature points in a video</h1></div></div></div><p class="calibre8">This chapter <a id="id882" class="calibre1"/>is about reading, writing, and processing video <a id="id883" class="calibre1"/>sequences. The objective is to be able to analyze a complete video sequence. As an example, in this recipe, you will learn how to perform temporal analysis of the sequence in order to track feature points as they move from frame to frame.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec208" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">To start the tracking process, the first thing to do is to detect the feature points in an initial frame. You then try to track these points in the next frame. Obviously, since we are dealing with a video sequence, there is a good chance that the object on which the feature points are found has moved (this motion can also be due to camera movement). Therefore, you must search around a point's previous location in order to find its new location in the next frame. This is what accomplishes the <code class="email">cv::calcOpticalFlowPyrLK</code> function. You input two consecutive frames and a vector of feature points in the first image; the function returns a vector of new point locations. To track points over a complete sequence, you repeat this process from frame to frame. Note that as you follow the points across<a id="id884" class="calibre1"/> the sequence, you will unavoidably lose track of some of them such that the number of tracked feature points will gradually reduce. Therefore, it could be a good idea to detect new features from time to time.</p><p class="calibre8">We will now<a id="id885" class="calibre1"/> take benefit of the framework we defined in the previous recipes and we will define a class that implements the <code class="email">FrameProcessor</code> interface introduced in the <em class="calibre9">Processing the video frames</em> recipe of this chapter. The data attributes of this class include the variables that are required to perform both the detection of feature points and their tracking:</p><div><pre class="programlisting">class FeatureTracker : public FrameProcessor {

  cv::Mat gray;         // current gray-level image
  cv::Mat gray_prev;      // previous gray-level image
  // tracked features from 0-&gt;1
  std::vector&lt;cv::Point2f&gt; points[2]; 
  // initial position of tracked points
  std::vector&lt;cv::Point2f&gt; initial;   
  std::vector&lt;cv::Point2f&gt; features;  // detected features
  int max_count;     // maximum number of features to detect
  double qlevel;    // quality level for feature detection
  double minDist;   // min distance between two points
  std::vector&lt;uchar&gt; status; // status of tracked features
  std::vector&lt;float&gt; err;    // error in tracking

  public:

  FeatureTracker() : max_count(500), qlevel(0.01), minDist(10.) {}</pre></div><p class="calibre8">Next, we define the <code class="email">process</code> method that will be called for each frame of the sequence. Basically, we need to proceed as follows. First, feature points are detected if necessary. Next, these points are tracked. You reject points that you cannot track or you no longer want to track. You are now ready to handle the successfully tracked points. Finally, the current frame and its points become the previous frame and points for the next iteration. Here is how to do this:</p><div><pre class="programlisting">  void process(cv:: Mat &amp;frame, cv:: Mat &amp;output) {

    // convert to gray-level image
    cv::cvtColor(frame, gray, CV_BGR2GRAY); 
    frame.copyTo(output);

    // 1. if new feature points must be added
    if(addNewPoints())
    {
      // detect feature points
      detectFeaturePoints();
      // add the detected features to 
      // the currently tracked features
      points[0].insert(points[0].end(),features.begin(),features.end());
      initial.insert(initial.end(),features.begin(),features.end());
    }

    // for first image of the sequence
    if(gray_prev.empty())
      gray.copyTo(gray_prev);

    // 2. track features
    cv::calcOpticalFlowPyrLK(gray_prev, gray, // 2 consecutive images
    points[0], // input point positions in first image
    points[1], // output point positions in the 2nd image
    status,    // tracking success
    err);      // tracking error

    // 3. loop over the tracked points to reject some
    int k=0;
    for( int i= 0; i &lt; points[1].size(); i++ ) {

      // do we keep this point?
      if (acceptTrackedPoint(i)) {
        // keep this point in vector
        initial[k]= initial[i];
        points[1][k++] = points[1][i];
      }
    }

    // eliminate unsuccesful points
    points[1].resize(k);
    initial.resize(k);

    // 4. handle the accepted tracked points
    handleTrackedPoints(frame, output);

    // 5. current points and image become previous ones
    std::swap(points[1], points[0]);
    cv::swap(gray_prev, gray);
  }</pre></div><p class="calibre8">This method <a id="id886" class="calibre1"/>makes use of four utility methods. It should be easy for you to change any of these methods in<a id="id887" class="calibre1"/> order to define a new behavior for your own tracker. The first of these methods detects the feature points. Note that we already discussed the <code class="email">cv::goodFeatureToTrack</code> function in the first recipe of <a class="calibre1" title="Chapter 8. Detecting Interest Points" href="part0058_split_000.html#page">Chapter 8</a>, <em class="calibre9">Detecting Interest Points</em>:</p><div><pre class="programlisting">  // feature point detection
  void detectFeaturePoints() {

    // detect the features
    cv::goodFeaturesToTrack(gray, // the image 
      features,   // the output detected features
      max_count,  // the maximum number of features 
      qlevel,     // quality level
      minDist);   // min distance between two features
  }</pre></div><p class="calibre8">The second method determines whether new feature points should be detected:</p><div><pre class="programlisting">  // determine if new points should be added
  bool addNewPoints() {

    // if too few points
      return points[0].size()&lt;=10;
  }</pre></div><p class="calibre8">The third method rejects some of the tracked points based on a criteria defined by the application. Here, we decided to reject points that do not move (in addition to those that cannot be tracked by the <code class="email">cv::calcOpticalFlowPyrLK</code> function):</p><div><pre class="programlisting">  // determine which tracked point should be accepted
  bool acceptTrackedPoint(int i) {

    return status[i] &amp;&amp;
    // if point has moved
    (abs(points[0][i].x-points[1][i].x)+(abs(points[0][i].y-points[1][i].y))&gt;2);
  }</pre></div><p class="calibre8">Finally, the fourth method handles the tracked feature points by drawing all of the tracked points with a line that joins them to their initial position (that is, the position where they were detected the first time) on the current frame:</p><div><pre class="programlisting">  // handle the currently tracked points
  void handleTrackedPoints(cv:: Mat &amp;frame, cv:: Mat &amp;output) {

    // for all tracked points
    for(int i= 0; i &lt; points[1].size(); i++ ) {

      // draw line and circle
      cv::line(output, 
        initial[i],  // initial position 
        points[1][i],// new position 
        cv::Scalar(255,255,255));
        cv::circle(output, points[1][i], 3, cv::Scalar(255,255,255),-1);
    }
  }</pre></div><p class="calibre8">A simple <a id="id888" class="calibre1"/>main<a id="id889" class="calibre1"/> function to track feature points in a video sequence would then be written as follows:</p><div><pre class="programlisting">int main()
{
  // Create video procesor instance
  VideoProcessor processor;

  // Create feature tracker instance
  FeatureTracker tracker;
   // Open video file
   processor.setInput("../bike.avi");

  // set frame processor
  processor.setFrameProcessor(&amp;tracker);

  // Declare a window to display the video
  processor.displayOutput("Tracked Features");

  // Play the video at the original frame rate
  processor.etDelayetDelay(1000./processor.getFrameRate());

  // Start the process
  processor.run();
}</pre></div><p class="calibre8">The resulting program will show you the evolution of the moving tracked features over time. Here are, for example, two such frames at two different instants. In this video, the camera is fixed. The young cyclist is, therefore, the only moving object. Here is the result that is obtained after a few frames have been processed:</p><div><img src="img/00189.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">A few <a id="id890" class="calibre1"/>seconds<a id="id891" class="calibre1"/> later, we obtain the following frame:</p><div><img src="img/00190.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec209" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">To track <a id="id892" class="calibre1"/>feature points from frame to frame, we must locate the <a id="id893" class="calibre1"/>new position of a feature point in the subsequent frame. If we assume that the intensity of the feature point does not change from one frame to the next one, we are looking for a displacement <em class="calibre9">(u,v)</em> as follows:</p><div><img src="img/00191.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, <em class="calibre9">I</em>
<sub class="calibre20"><em class="calibre21">t</em>
</sub> and <em class="calibre9">I</em>
<sub class="calibre20"><em class="calibre21">t+1</em>
</sub> are the current frame and the one at the next instant, respectively. This constant intensity assumption generally holds for small displacement in images that are taken at two nearby instants. We can then use the Taylor expansion in order to approximate this equation by an equation that involves the image derivatives:</p><div><img src="img/00192.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This latter equation leads us to another equation (as a consequence of the constant intensity assumption that cancels the two intensity terms):</p><div><img src="img/00193.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This well-known constraint is the <a id="id894" class="calibre1"/>fundamental <strong class="calibre2">optical flow</strong> constraint equation. This constraint is exploited by the so-called Lukas-Kanade feature-tracking algorithm that also makes an additional assumption that the displacement of all <a id="id895" class="calibre1"/>points in the neighborhood of the feature point is the same. We can, therefore, impose the optical flow constraint for all of these points with a unique <em class="calibre9">(u,v)</em> unknown displacement. This gives us more equations than the number<a id="id896" class="calibre1"/> of unknowns (2), and therefore, we can solve this system of equations in a mean-square sense. In practice, it is solved iteratively and the OpenCV implementation also offers us the possibility to perform this estimation at a different resolution in order to make the search more efficient and more tolerant to larger displacement. By default, the number of image levels is <code class="email">3</code> and the window size is <code class="email">15</code>. These parameters can obviously be changed. You can also specify the termination criteria, which define the conditions that stop the iterative search. The sixth parameter of <code class="email">cv::calcOpticalFlowPyrLK</code> contains the residual mean-square error that can be used to assess the quality of the tracking. The fifth parameter contains binary flags that tell us whether tracking the corresponding point was considered successful or not.</p><p class="calibre8">The preceding description represents the basic principles behind the Lukas-Kanade tracker. The current implementation contains other optimizations and improvements that make the algorithm more efficient in the computation of the displacement of a large number of feature points.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec210" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><a class="calibre1" title="Chapter 8. Detecting Interest Points" href="part0058_split_000.html#page">Chapter 8</a>, <em class="calibre9">Detecting Interest Points</em>, has a discussion on feature point detection.</li><li class="listitem">The classic article by B. Lucas and T. Kanade, <em class="calibre9">An Iterative Image Registration Technique with an Application to Stereo Vision</em> in <em class="calibre9">Int. Joint Conference in Artificial Intelligence, pp. 674-679, 1981</em>, describes the original feature point tracking algorithm.</li><li class="listitem">The article by J. Shi and C. Tomasi, <em class="calibre9">Good Features to Track in IEEE Conference on Computer Vision and Pattern Recognition</em>, pp. 593-600, 1994, describes an improved version of the original feature point tracking algorithm.</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec71" class="calibre1"/>Extracting the foreground objects in a video</h1></div></div></div><p class="calibre8">When a<a id="id897" class="calibre1"/> fixed camera observes a scene, the background remains mostly unchanged. In this case, the interesting elements are the moving objects<a id="id898" class="calibre1"/> that evolve inside this scene. In order to extract these foreground objects, we need to build a model of the background, and then compare this model with a current frame in order to detect any foreground objects. This is what we will do in this recipe. Foreground extraction is a fundamental step in intelligent surveillance applications.</p><p class="calibre8">If we had an image of the background of the scene (that is, a frame that contains no foreground objects) at our disposal, then it would be easy to extract the foreground of a current frame through a simple image difference:</p><div><pre class="programlisting">  // compute difference between current image and background
  cv::absdiff(backgroundImage,currentImage,foreground);</pre></div><p class="calibre8">Each pixel for which this difference is high enough would then be declared as a foreground pixel. However, most of the time, this background image is not readily available. Indeed, it could be difficult to guarantee that no foreground objects are present in a given image, and in busy scenes, such situations might rarely occur. Moreover, the background scene often evolves over time because, for instance, the lighting condition changes (for example, from sunrise to sunset) or because new objects can be added or removed from the background.</p><p class="calibre8">Therefore, it is necessary to dynamically build a model of the background scene. This can be done by observing the scene for a period of time. If we assume that most often, the background is visible at each pixel location, then it could be a good strategy to simply compute the average of all of the observations. However, this is not feasible for a number of reasons. First, this would require a large number of images to be stored before computing the background. Second, while we are accumulating images to compute our average image, no foreground extraction will be done. This solution also raises the problem of when and how many images should be accumulated to compute an acceptable background model. In addition, the images where a given pixel is observing a foreground object would have an impact on the computation of the average background.</p><p class="calibre8">A better strategy is to dynamically build the background model by regularly updating it. This can be accomplished by computing what is called a<a id="id899" class="calibre1"/> <strong class="calibre2">running average</strong> (also called <strong class="calibre2">moving average</strong>). This is a way to compute the average value of a temporal signal that takes into account the latest received values. If pt is the pixel value at a given time <em class="calibre9">t</em> and <em class="calibre9">μ</em>
<sub class="calibre20"><em class="calibre21">t-1</em></sub> is the current average value, then this average is updated using the following formula:</p><div><img src="img/00194.jpeg" alt="Extracting the foreground objects in a video" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The <em class="calibre9">α</em> parameter is called the<a id="id900" class="calibre1"/> <strong class="calibre2">learning rate</strong>, and it defines the influence of the current value over the currently estimated average. The larger this value is, the faster the <a id="id901" class="calibre1"/>running average will <a id="id902" class="calibre1"/>adapt to changes in the observed values. To build a background model, one just has to compute a running average for every pixel of the incoming frames. The decision to declare a foreground pixel is then simply based on the difference between the current image and the background model.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec211" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">Let's build a class that will learn about a background model using moving averages and that will extract foreground objects by subtraction. The required attributes are the following:</p><div><pre class="programlisting">class BGFGSegmentor : public FrameProcessor {

  cv::Mat gray;         // current gray-level image
  cv::Mat background;   // accumulated background
  cv::Mat backImage;    // current background image
  cv::Mat foreground;   // foreground image
  // learning rate in background accumulation
  double learningRate;
  int threshold;        // threshold for foreground extraction</pre></div><p class="calibre8">The main process consists of comparing the current frame with the background model and then updating this model:</p><div><pre class="programlisting">  // processing method
  void process(cv:: Mat &amp;frame, cv:: Mat &amp;output) {

    // convert to gray-level image
    cv::cvtColor(frame, gray, CV_BGR2GRAY); 

    // initialize background to 1st frame
    if (background.empty())
      gray.convertTo(background, CV_32F);

    // convert background to 8U
    background.convertTo(backImage,CV_8U);

    // compute difference between image and background
    cv::absdiff(backImage,gray,foreground);
    // apply threshold to foreground image        
    cv::threshold(foreground,output,threshold,255,cv::THRESH_BINARY_INV);

    // accumulate background
    cv::accumulateWeighted(gray, background,
      // alpha*gray + (1-alpha)*background
      learningRate,  // alpha 
      output);       // mask

    }</pre></div><p class="calibre8">Using<a id="id903" class="calibre1"/> our<a id="id904" class="calibre1"/> video-processing framework, the foreground extraction program will be built as follows:</p><div><pre class="programlisting">int main()
{
  // Create video procesor instance
  VideoProcessor processor;

  // Create background/foreground segmentor 
  BGFGSegmentor segmentor;
  segmentor.setThreshold(25);

   // Open video file
   processor.setInput("bike.avi");

  // set frame processor
  processor.setFrameProcessor(&amp;segmentor);

  // Declare a window to display the video
  processor.displayOutput("Extracted Foreground");

  // Play the video at the original frame rate
  processor.setDelay(1000./processor.getFrameRate());

  // Start the process
  processor.run();
}</pre></div><p class="calibre8">One of the resulting binary foreground images that will be displayed is as follows:</p><div><img src="img/00195.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec212" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">Computing<a id="id905" class="calibre1"/> the running average of an image is easily accomplished through the <code class="email">cv::accumulateWeighted</code> function that applies the running average <a id="id906" class="calibre1"/>formula to each pixel of the image. Note that the resulting image must be a floating point image. This is why we had to convert the background model into a background image before comparing it with the current frame. A simple thresholded absolute difference (computed by <code class="email">cv::absdiff</code> followed by <code class="email">cv::threshold</code>) extracts the foreground image. Note that we then used the foreground image as a mask to cv<code class="email">::accumulateWeighte</code>d in order to avoid the updating of pixels declared as foreground. This works because our foreground image is defined as being false (that is, <code class="email">0</code>) at foreground pixels (which also explains why the foreground objects are displayed as black pixels in the resulting image).</p><p class="calibre8">Finally, it should be noted that, for simplicity, the background model that is built by our program is based on the gray-level version of the extracted frames. Maintaining a color background would require the computation of a running average in some color space. However, the main difficulty in the presented approach is to determine the appropriate value for the threshold that would give good results for a given video.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec213" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">The preceding simple method to extract foreground objects in a scene works well for simple scenes that show a relatively stable background. However, in many situations, the background scene might fluctuate in certain areas between different values, thus causing frequent false foreground detections. These might be due to, for example, a moving background object (for example, tree leaves) or a glaring effect (for example, on the surface of water). Casted shadows also pose a problem since they are often detected as part of a moving object. In order to cope with these problems, more sophisticated background modeling methods have been introduced.</p><div><div><div><div><h3 class="title2"><a id="ch11lvl3sec55" class="calibre1"/>The Mixture of Gaussian method</h3></div></div></div><p class="calibre8">One of these <a id="id907" class="calibre1"/>algorithms is the <strong class="calibre2">Mixture of Gaussian</strong> method. It proceeds in a way that is similar to the method presented in this recipe but adds a number of improvements.</p><p class="calibre8">First, the method maintains more than one model per pixel (that is, more than one running average). This way, if a background pixel fluctuates between, let's say, two values, two running averages are then stored. A new pixel value will be declared as the foreground only if it does not belong to any of the most frequently observed models. The number of models used is a parameter of the method and a typical value is <code class="email">5</code>.</p><p class="calibre8">Second, not only is the running average maintained for each model, but also for the running variance. This is computed as follows:</p><div><img src="img/00196.jpeg" alt="The Mixture of Gaussian method" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">These computed averages and variances are used to build a Gaussian model from which the probability of a given pixel value to belong to the background can be estimated. This makes it easier to determine an appropriate threshold since it is now expressed as a probability rather than an absolute difference. Consequently, in areas where the background values have larger fluctuations, a greater difference will be required to declare a foreground object.</p><p class="calibre8">Finally, when a given Gaussian model is not hit sufficiently often, it is excluded as being part of the background model. Reciprocally, when a pixel value is found to be outside the currently maintained background models (that is, it is a foreground pixel), a new Gaussian model is created. If in the future this new model becomes a hit, then it becomes associated with the background.</p><p class="calibre8">This more sophisticated algorithm is obviously more complex to implement than our simple background/foreground segmentor. Fortunately, an OpenCV implementation exists, called <code class="email">cv::BackgroundSubtractorMOG</code>, and is defined as a subclass of the more general <code class="email">cv::BackgroundSubtractor</code> class. When used with its default parameter, this class is very easy to use:</p><div><pre class="programlisting">int main()
{
  // Open the video file
  cv::VideoCapture capture("bike.avi");
  // check if video successfully opened
  if (!capture.isOpened())
    return 0;
  // current video frame
  cv::Mat frame; 
  // foreground binary image
  cv::Mat foreground;
  cv::namedWindow("Extracted Foreground");
  // The Mixture of Gaussian object
  // used with all default parameters
  cv::BackgroundSubtractorMOG mog;
  bool stop(false);
  // for all frames in video
  while (!stop) {
    // read next frame if any
    if (!capture.read(frame))
      break;
    // update the background
    // and return the foreground
    mog(frame,foreground,0.01)
    // learning rate
    // Complement the image        
    cv::threshold(foreground,foreground,128,255,cv::THRESH_BINARY_INV);
    // show foreground
    cv::imshow("Extracted Foreground",foreground);

    // introduce a delay
    // or press key to stop
    if (cv::waitKey(10)&gt;=0)
      stop= true;
  }
}</pre></div><p class="calibre8">As it can be seen, it <a id="id908" class="calibre1"/>is just a matter of creating the class instance and calling the method that simultaneously updates the background and returns the foreground image (the extra parameter being the learning rate). Also note that the background model is computed in color here. The method implemented in OpenCV also includes a mechanism to reject shadows by checking whether the observed pixel variation is simply caused by a local change in brightness (if so, then it is probably due to a shadow) or whether it also includes some change in chromaticity.</p><p class="calibre8">A second <a id="id909" class="calibre1"/>implementation is also available and is simply called <code class="email">cv::BackgroundSubtractorMOG2</code>. One of the improvements is that the number of appropriate Gaussian models per pixel to be used is now determined dynamically. You can use this in place of the previous one in the preceding example. You should run these different methods on a number of videos in order to appreciate their respective performances. In general, you will observe that <code class="email">cv::BackgroundSubtractorMOG2</code> is much faster.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec214" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The article by C. Stauffer and W.E.L. Grimson, <em class="calibre9">Adaptive Background Mixture Models for Real-Time Tracking</em>, in <em class="calibre9">Conf. on Computer Vision and Pattern Recognition, 1999</em>, gives you a more complete description of the Mixture of Gaussian algorithm.</li></ul></div></div></div></body></html>