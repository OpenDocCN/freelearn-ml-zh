- en: Advanced Feature Selection in Linear Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"I found that math got to be too abstract for my liking and computer science
    seemed concerned with little details--trying to save a microsecond or a kilobyte
    in a computation. In statistics I found a subject that combined the beauty of
    both math and computer science, using them to solve real-world problems."'
  prefs: []
  type: TYPE_NORMAL
- en: 'This was quoted by *Rob Tibshirani*, *Professor*, *Stanford University* at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://statweb.stanford.edu/~tibs/research_page.html](http://statweb.stanford.edu/~tibs/research_page.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we''ve examined the usage of linear models for both quantitative and
    qualitative outcomes with an emphasis on the techniques of feature selection,
    that is, the methods and techniques to exclude useless or unwanted predictor variables.
    We saw that the linear models can be quite effective in machine learning problems.
    However, newer techniques that have been developed and refined in the last couple
    of decades or so can improve predictive ability and interpretability above and
    beyond the linear models that we discussed in the preceding chapters. In this
    day and age, many datasets have numerous features in relation to the number of
    observations or, as it is called, high-dimensionality. If you''ve ever worked
    on a genomics problem, this will quickly become self-evident. Additionally, with
    the size of the data that we are being asked to work with, a technique like best
    subsets or stepwise feature selection can take inordinate amounts of time to converge
    even on high-speed computers. I''m not talking about minutes: in many cases, hours
    of system time are required to get a best subsets solution.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a better way in these cases. In this chapter, we will look at the concept
    of regularization where the coefficients are constrained or shrunk towards zero.
    There are a number of methods and permutations to these methods of regularization
    but we will focus on Ridge regression, **Least Absolute Shrinkage and Selection
    Operator** (**LASSO**), and finally, elastic net, which combines the benefit of
    both techniques into one.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may recall that our linear model follows the form, *Y = B0 + B[1]x[1] +...B[n]x[n]
    + e*, and also that the best fit tries to minimize the RSS, which is the sum of
    the squared errors of the actual minus the estimate, or *e[1]² + e[2]² + ... e[n]²*.
  prefs: []
  type: TYPE_NORMAL
- en: With regularization, we will apply what is known as **shrinkage penalty** in
    conjunction with the minimization RSS. This penalty consists of a lambda (symbol
    *λ*), along with the normalization of the beta coefficients and weights. How these
    weights are normalized differs in the techniques, and we will discuss them accordingly.
    Quite simply, in our model, we are minimizing *(RSS + λ(normalized coefficients))*.
    We will select *λ*, which is known as the tuning parameter, in our model building
    process. Please note that if lambda is equal to 0, then our model is equivalent
    to OLS, as it cancels out the normalization term.
  prefs: []
  type: TYPE_NORMAL
- en: So what does this do for us and why does it work? First of all, regularization
    methods are very computationally efficient. In best subsets, we are searching
    **2^p models**, and in large datasets, it may not be feasible to attempt. In R,
    we are only fitting one model to each value of lambda and this is far more efficient.
    Another reason goes back to our bias-variance trade-off, which was discussed in
    the preface. In the linear model, where the relationship between the response
    and the predictors is close to linear, the least squares estimates will have low
    bias but may have high variance. This means that a small change in the training
    data can cause a large change in the least squares coefficient estimates (James,
    2013). Regularization through the proper selection of lambda and normalization
    may help you improve the model fit by optimizing the bias-variance trade-off.
    Finally, regularization of coefficients works to solve multi collinearity problems.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's begin by exploring what ridge regression is and what it can and cannot
    do for you. With ridge regression, the normalization term is the sum of the squared
    weights, referred to as an **L2-norm**. Our model is trying to minimize *RSS +
    λ(sum Bj²)*. As lambda increases, the coefficients shrink toward zero but never
    become zero. The benefit may be an improved predictive accuracy, but as it does
    not zero out the weights for any of your features, it could lead to issues in
    the model's interpretation and communication. To help with this problem, we will
    turn to LASSO.
  prefs: []
  type: TYPE_NORMAL
- en: LASSO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LASSO applies the **L1-norm** instead of the L2-norm as in ridge regression,
    which is the sum of the absolute value of the feature weights and thus minimizes
    *RSS + λ(sum |Bj|)*. This shrinkage penalty will indeed force a feature weight
    to zero. This is a clear advantage over ridge regression, as it may greatly improve
    the model interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: The mathematics behind the reason that the L1-norm allows the weights/coefficients
    to become zero, is out of the scope of this book (refer to *Tibsharini*, *1996*
    for further details).
  prefs: []
  type: TYPE_NORMAL
- en: 'If LASSO is so great, then ridge regression must be clearly obsolete. Not so
    fast! In a situation of high collinearity or high pairwise correlations, LASSO
    may force a predictive feature to zero and thus you can lose the predictive ability;
    that is, say if both feature A and B should be in your model, LASSO may shrink
    one of their coefficients to zero. The following quote sums up this issue nicely:'
  prefs: []
  type: TYPE_NORMAL
- en: '"One might expect the lasso to perform better in a setting where a relatively
    small number of predictors have substantial coefficients, and the remaining predictors
    have coefficients that are very small or that equal zero. Ridge regression will
    perform better when the response is a function of many predictors, all with coefficients
    of roughly equal size."'
  prefs: []
  type: TYPE_NORMAL
- en: -(James, 2013)
  prefs: []
  type: TYPE_NORMAL
- en: There is the possibility of achieving the best of both the worlds and that leads
    us to the next topic, elastic net.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The power of elastic net is that, it performs the feature extraction that ridge
    regression does not and it will group the features that LASSO fails to do. Again,
    LASSO will tend to select one feature from a group of correlated ones and ignore
    the rest. Elastic net does this by including a mixing parameter, alpha, in conjunction
    with lambda. Alpha will be between `0` and `1` and as before, lambda will regulate
    the size of the penalty. Please note that an alpha of zero is equal to ridge regression
    and an alpha of one is equivalent to LASSO. Essentially, we are blending the L1
    and L2 penalties by including a second tuning parameter with a quadratic (squared)
    term of the beta coefficients. We will end up with the goal of minimizing *(RSS
    + λ[(1-alpha) (sum|Bj|²)/2 + alpha (sum |Bj|)])/N)*.
  prefs: []
  type: TYPE_NORMAL
- en: Let's put these techniques to test. We will primarily utilize the `leaps`, `glmnet`,
    and `caret` packages to select the appropriate features and thus the appropriate
    model in our business case.
  prefs: []
  type: TYPE_NORMAL
- en: Business case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, we will stick to cancer--prostate cancer in this case. It
    is a small dataset of 97 observations and nine variables but allows you to fully
    grasp what is going on with regularization techniques by allowing a comparison
    with traditional techniques. We will start by performing best subsets regression
    to identify the features and use this as a baseline for our comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*The Stanford University Medical Center* has provided preoperative **Prostate
    Specific Antigen** (**PSA**) data on 97 patients who are about to undergo radical
    prostatectomy (complete prostate removal) for the treatment of prostate cancer.
    The **American Cancer Society** (**ACS**) estimates that nearly 30,000 American
    men died of prostate cancer in 2014 ([http://www.cancer.org/](http://www.cancer.org/)).
    PSA is a protein that is produced by the prostate gland and is found in the bloodstream.
    The goal is to develop a predictive model of PSA among the provided set of clinical
    measures. PSA can be an effective prognostic indicator, among others, of how well
    a patient can and should do after surgery. The patient''s PSA levels are measured
    at various intervals after the surgery and used in various formulas to determine
    if a patient is cancer-free. A preoperative predictive model in conjunction with
    the postoperative data (not provided here) can possibly improve cancer care for
    thousands of men each year.'
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data set for the 97 men is in a data frame with 10 variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lcavol`: This is the log of the cancer volume'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lweight`: This is the log of the prostate weight'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age`: This is the age of the patient in years'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lbph`: This is the log of the amount of **Benign Prostatic Hyperplasia** (**BPH**),
    which is the non-cancerous enlargement of the prostate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`svi`: This is the seminal vesicle invasion and an indicator variable of whether
    or not the cancer cells have invaded the seminal vesicles outside the prostate
    wall (`1` = yes, `0` = no)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lcp`: This is the log of capsular penetration and a measure of how much the
    cancer cells have extended in the covering of the prostate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gleason`: This is the patient''s Gleason score; a score (2-10) provided by
    a pathologist after a biopsy about how abnormal the cancer cells appear--the higher
    the score, the more aggressive the cancer is assumed to be'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pgg4`: This is the percent of Gleason patterns-four or five (high-grade cancer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lpsa`: This is the log of the PSA; it is the response/outcome'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train`: This is a logical vector (true or false) that signifies the training
    or test set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The dataset is contained in the R package `ElemStatLearn`. After loading the
    required packages and data frame, we can begin to explore the variables and any
    possible relationships, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With the packages loaded, bring up the `prostate` dataset and explore its structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The examination of the structure should raise a couple of issues that we will
    need to double-check. If you look at the features, `svi`, `lcp`, `gleason`, and
    `pgg45` have the same number in the first 10 observations, with the exception
    of one--the seventh observation in `gleason`. In order to make sure that these
    are viable as input features, we can use plots and tables so as to understand
    them. To begin with, use the following `plot()` command and input the entire data
    frame, which will create a scatterplot matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With these many variables on one plot, it can get a bit difficult to understand
    what is going on, so we will drill down further. It does look like there is a
    clear linear relationship between our outcomes, `lpsa`, and `lcavol`. It also
    appears that the features mentioned previously have an adequate dispersion and
    are well-balanced across what will become our `train` and `test` sets with the
    possible exception of the `gleason` score. Note that the `gleason` scores captured
    in this dataset are of four values only. If you look at the plot where `train`
    and `gleason` intersect, one of these values is not in either `test` or `train`.
    This could lead to potential problems in our analysis and may require transformation.
    So, let''s create a plot specifically for that feature, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have a problem here. Each dot represents an observation and the *x* axis
    is the observation number in the data frame. There is only one Gleason Score of
    *8.0* and only five of score *9.0*. You can look at the exact counts by producing
    a table of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'What are our options? We could do any of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Exclude the feature altogether
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove only the scores of **8.0** and **9.0**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recode this feature, creating an indicator variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I think it may help if we create a `boxplot` of `Gleason Score` versus `Log
    of PSA`. We used the `ggplot2` package to create boxplots in a prior chapter,
    but one can also create it with base R, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the preceding plot, I think the best option will be to turn this
    into an indicator variable with **0** being a **6** score and **1** being a **7**
    or a higher score. Removing the feature may cause a loss of predictive ability.
    The missing values will also not work with the `glmnet` package that we will use.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can code an indicator variable with one simple line of code using the `ifelse()`
    command by specifying the column in the data frame that you want to change. Then
    follow the logic that, if the observation is number *x*, then code it *y*, or
    else code it *z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As always, let''s verify that the transformation worked as intended by creating
    a table in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'That worked to perfection! As the scatterplot matrix was hard to read, let''s
    move on to a correlation plot, which indicates if a relationship/dependency exists
    between the features. We will create a correlation object using the `cor()` function
    and then take advantage of the `corrplot` library with `corrplot.mixed()`, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: A couple of things jump out here. First, PSA is highly correlated with the log
    of cancer volume (`lcavol`); you may recall that in the scatterplot matrix, it
    appeared to have a highly linear relationship. Second, multicollinearity may become
    an issue; for example, cancer volume is also correlated with capsular penetration
    and this is correlated with the seminal vesicle invasion. This should be an interesting
    learning exercise!
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the learning can begin, the training and testing sets must be created.
    As the observations are already coded as being in the `train` set or not, we can
    use the `subset()` command and set the observations where `train` is coded to
    `TRUE` as our training set and `FALSE` for our testing set. It is also important
    to drop `train` as we do not want that as a feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the data prepared, we will begin the modeling process. For comparison purposes,
    we will create a model using best subsets regression like the previous two chapters
    and then utilize the regularization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Best subsets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following code is, for the most part, a rehash of what we developed in [Chapter
    2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml), *Linear Regression - The Blocking
    and Tackling of Machine Learning*. We will create the best subset object using
    the `regsubsets()` command and specify the `train` portion of `data`. The variables
    that are selected will then be used in a model on the `test` set, which we will
    evaluate with a mean squared error calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model that we are building is written out as `lpsa ~ .` with the tilde
    and period stating that we want to use all the remaining variables in our data
    frame, with the exception of the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'With the model built, you can produce the best subset with two lines of code.
    The first one turns the `summary` model into an object where we can extract the
    various subsets and determine the best one with the `which.min()` command. In
    this instance, I will use BIC, which was discussed in [Chapter 2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml),
    *Linear Regression - The Blocking and Tackling of Machine Learning*, which is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is telling us that the model with the `3` features has the lowest
    `bic` value. A plot can be produced to examine the performance across the subset
    combinations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A more detailed examination is possible by plotting the actual model object,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the previous plot shows us that the three features included in the lowest
    `BIC` are `lcavol`, `lweight`, and `gleason`. It is noteworthy that `lcavol` is
    included in every combination of the models. This is consistent with our earlier
    exploration of the data. We are now ready to try this model on the `test` portion
    of the data, but first, we will produce a plot of the fitted values versus the
    actual values looking for linearity in the solution, and as a check on the constancy
    of the variance. A linear model will need to be created with just the three features
    of interest. Let''s put this in an object called `ols` for the OLS. Then the fits
    from `ols` will be compared to the actual in the training set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An inspection of the plot shows that a linear fit should perform well on this
    data and that the non-constant variance is not a problem. With that, we can see
    how this performs on the test set data by utilizing the `predict()` function and
    specifying `newdata=test`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The values in the object can then be used to create a plot of the `Predicted
    vs Actual` values, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot doesn''t seem to be too terrible. For the most part, it is a linear
    fit with the exception of what looks to be two outliers on the high end of the
    PSA score. Before concluding this section, we will need to calculate **Mean Squared
    Error** (**MSE**) to facilitate comparison across the various modeling techniques.
    This is easy enough where we will just create the residuals and then take the
    mean of their squared values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: So, MSE of `0.508` is our benchmark for going forward.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With ridge regression, we will have all the eight features in the model, so
    this will be an intriguing comparison with the best subsets model. The package
    that we will use and is in fact already loaded, is `glmnet`. The package requires
    that the input features are in a matrix instead of a data frame and for ridge
    regression, we can follow the command sequence of `glmnet(x = our input matrix,
    y = our response, family = the distribution, alpha=0)`. The syntax for alpha relates
    to `0` for ridge regression and `1` for doing LASSO.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the `train` set ready for use in `glmnet` is actually quite easy by
    using `as.matrix()` for the inputs and creating a vector for the response, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the ridge regression by placing it in an object called, appropriately
    I might add, `ridge`. It is important to note here that the `glmnet` package will
    first standardize the inputs before computing the lambda values and then will
    unstandardize the coefficients. You will need to specify the distribution of the
    response variable as `gaussian` as it is continuous and `alpha=0` for ridge regression,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The object has all the information that we need in order to evaluate the technique.
    The first thing to try is the `print()` command, which will show us the number
    of nonzero coefficients, percent deviance explained, and correspondent value of
    `Lambda`. The default number in the package of steps in the algorithm is `100`.
    However, the algorithm will stop prior to `100` steps if the percent deviation
    does not dramatically improve from one lambda to another; that is, the algorithm
    converges to an optimal solution. For the purpose of saving space, I will present
    only the following first five and last ten lambda results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at row `100` for example. It shows us that the number of nonzero coefficients
    or-said another way-the number of features included, is eight; please recall that
    it will always be the same for ridge regression. We also see that the percent
    of deviance explained is `.6971` and the `Lambda` tuning parameter for this row
    is `0.08789`. Here is where we can decide on which lambda to select for the `test`
    set. The lambda of `0.08789` can be used, but let''s make it a little simpler,
    and for the `test` set, try `0.10`. A couple of plots might help here so let''s
    start with the package''s default, adding annotations to the curve by adding `label=TRUE`
    in the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the default plot, the *y* axis is the value of Coefficients and the *x*
    axis is L1 Norm. The plot tells us the coefficient values versus the L1 Norm.
    The top of the plot contains a second *x* axis, which equates to the number of
    features in the model. Perhaps a better way to view this is by looking at the
    coefficient values changing as `lambda` changes. We just need to tweak the code
    in the following `plot()` command by adding `xvar="lambda"`. The other option
    is the percent of deviance explained by substituting `lambda` with `dev`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a worthwhile plot as it shows that as `lambda` decreases, the shrinkage
    parameter decreases and the absolute values of the coefficients increase. To see
    the coefficients at a specific `lambda` value, use the `coef()` command. Here,
    we will specify the `lambda` value that we want to use by specifying s=0.1\. We
    will also state that we want `exact=TRUE`, which tells `glmnet` to fit a model
    with that specific `lambda` value versus interpolating from the values on either
    side of our `lambda`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to note that `age`, `lcp`, and `pgg45` are close to, but not
    quite, zero. Let''s not forget to plot deviance versus coefficients as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing the two previous plots, we can see that as `lambda` decreases, the
    coefficients increase and the percent/fraction of the deviance explained increases.
    If we were to set `lambda` equal to zero, we would have no shrinkage penalty and
    our model would equate the `OLS`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prove this on the `test` set, we will have to transform the features as
    we did for the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use the `predict` function to create an object that we will call `ridge.y`
    with `type = "response"` and our `lambda` equal to `0.10` and plot the `Predicted`
    values versus the `Actual` values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the following command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot of `Predicted` versus `Actual` of `Ridge Regression` seems to be quite
    similar to best subsets, complete with two interesting outliers at the high end
    of the PSA measurements. In the real world, it would be advisable to explore these
    outliers further so as to understand whether they are truly unusual or we are
    missing something. This is where domain expertise would be invaluable. The MSE
    comparison to the benchmark may tell a different story. We first calculate the
    residuals, and then take the mean of those residuals squared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Ridge regression has given us a slightly better MSE. It is now time to put LASSO
    to the test to see if we can decrease our errors even further.
  prefs: []
  type: TYPE_NORMAL
- en: LASSO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run LASSO next is quite simple and we only have to change one number from
    our ridge regression model: that is, change `alpha=0` to `alpha=1` in the `glmnet()`
    syntax. Let''s run this code and also see the output of the model, looking at
    the first five and last 10 results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the model building process stopped at step `69` as the deviance explained
    no longer improved as `lambda` decreased. Also, note that the `Df` column now
    changes along with `lambda`. At first glance, here it seems that all the eight
    features should be in the model with a `lambda` of `0.001572`. However, let''s
    try and find and test a model with fewer features, around seven, for argument''s
    sake. Looking at the rows, we see that around a `lambda` of `0.045`, we end up
    with `7` features versus `8`. Thus, we will plug this `lambda` in for our `test`
    set evaluation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as with ridge regression, we can plot the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is an interesting plot and really shows how LASSO works. Notice how the
    lines labeled **8**, **3**, and **6** behave, which corresponds to the `pgg45`,
    `age`, and `lcp` features respectively. It looks as if `lcp` is at or near zero
    until it is the last feature that is added. We can see the coefficient values
    of the seven feature model just as we did with ridge regression by plugging it
    into `coef()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The LASSO algorithm zeroed out the coefficient for `lcp` at a `lambda` of `0.045`.
    Here is how it performs on the `test` data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We calculate MSE as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: It looks like we have similar plots as before, with only the slightest improvement
    in MSE. Our last best hope for dramatic improvement is with elastic net. To this
    end, we will still use the `glmnet` package. The twist will be that, we will solve
    for `lambda` and for the elastic net parameter known as `alpha`. Recall that `alpha
    = 0` is the ridge regression penalty and `alpha = 1` is the LASSO penalty. The
    elastic net parameter will be `0 ≤ alpha ≤ 1`. Solving for two different parameters
    simultaneously can be complicated and frustrating, but we can use our friend in
    R, the `caret` package, for assistance.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `caret` package stands for classification and regression training. It has
    an excellent companion website to help in understanding all of its capabilities:
    [http://topepo.github.io/caret/index.html.](http://topepo.github.io/caret/index.html.)
    The package has many different functions that you can use and we will revisit
    some of them in the later chapters. For our purpose here, we want to focus on
    finding the optimal mix of lambda and our elastic net mixing parameter, `alpha`.
    This is done using the following simple three-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `expand.grid()` function in base R to create a vector of all the possible
    combinations of `alpha` and `lambda` that we want to investigate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `trainControl()` function from the `caret` package to determine the
    resampling method; we will use LOOCV as we did in [Chapter 2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml),
    *Linear Regression - The Blocking and Tackling of Machine Learning*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a model to select our `alpha` and `lambda` parameters using `glmnet()`
    in caret's `train()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we've selected our parameters, we will apply them to the `test` data in
    the same way as we did with ridge regression and LASSO.
  prefs: []
  type: TYPE_NORMAL
- en: Our grid of combinations should be large enough to capture the best model but
    not too large that it becomes computationally unfeasible. That won't be a problem
    with this size dataset, but keep this in mind for future references.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the values of hyperparameters we can try:'
  prefs: []
  type: TYPE_NORMAL
- en: Alpha from `0` to `1` by `0.2` increments; remember that this is bound by `0`
    and `1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambda from `0.00` to `0.2` in steps of `0.02`; the `0.2` lambda should provide
    a cushion from what we found in ridge regression (lambda=`0.1`) and LASSO (lambda=`0.045`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can create this vector by using the `expand.grid()` function and building
    a sequence of numbers for what the `caret` package will automatically use. The
    `caret` package will take the values for `alpha` and `lambda` with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `table()` function will show us the complete set of 66 combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We can confirm that this is what we wanted--`alpha` from `0` to `1` and `lambda`
    from `0` to `0.2`.
  prefs: []
  type: TYPE_NORMAL
- en: For the resampling method, we will put in the code for `LOOCV` for the method.
    There are also other resampling alternatives such as bootstrapping or k-fold cross-validation
    and numerous options that you can use with `trainControl()`, but we will explore
    these options in future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can tell the model selection criteria with `selectionFunction()` in `trainControl()`.
    For quantitative responses, the algorithm will select based on its default of
    **Root Mean Square Error** (**RMSE**), which is perfect for our purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now time to use `train()` to determine the optimal elastic net parameters.
    The function is similar to `lm()`. We will just add the syntax: `method="glmnet"`,
    `trControl=control` and `tuneGrid=grid`. Let''s put this in an object called `enet.train`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling the object will tell us the parameters that lead to the lowest `RMSE`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`RMSE` was used to select the optimal model using the smallest value. The final
    values used for the model were `alpha = 0` and `lambda = 0.08`.'
  prefs: []
  type: TYPE_NORMAL
- en: This experimental design has led to the optimal tuning parameters of `alpha
    = 0` and `lambda = 0.08`, which is a ridge regression with `s = 0.08` in `glmnet`,
    recall that we used `0.10`. The `R-squared` is 61 percent, which is nothing to
    write home about.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process for the `test` set validation is just as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate MSE as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This model error is similar to the ridge penalty. On the `test` set, our LASSO
    model did the best in terms of errors. We may be over-fitting! Our best subset
    model with three features is the easiest to explain, and in terms of errors, is
    acceptable to the other techniques. We can use cross-validation in the `glmnet`
    package to possibly identify a better solution.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation with glmnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have used `LOOCV` with the `caret` package; now we will try k-fold cross-validation.
    The `glmnet` package defaults to ten folds when estimating lambda in `cv.glmnet()`.
    In k-fold CV, the data is partitioned into an equal number of subsets (folds)
    and a separate model is built on each k-1 set and then tested on the corresponding
    holdout set with the results combined (averaged) to determine the final parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this method, each fold is used as a `test` set only once. The `glmnet` package
    makes it very easy to try this and will provide you with an output of the lambda
    values and the corresponding MSE. It defaults to `alpha = 1`, so if you want to
    try ridge regression or an elastic net mix, you will need to specify it. As we
    will be trying for as few input features as possible, we will stick to the defaults,
    but given the size of the training data, use only three folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot for CV is quite different than the other `glmnet` plots, showing log(Lambda)
    versus Mean-Squared Error along with the number of features. The two dotted vertical
    lines signify the minimum of MSE (left line) and one standard error from the minimum
    (right line). One standard error away from the minimum is a good place to start
    if you have an over-fitting problem. You can also call the exact values of these
    two lambdas, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `lambda.1se`, we can go through the following process of viewing the
    coefficients and validating the model on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This model achieves an error of `0.45` with just five features, zeroing out
    `age`, `lcp`, and `pgg45`.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We looked at five different models in examining this dataset. The following
    points were the `test` set error of these models:'
  prefs: []
  type: TYPE_NORMAL
- en: Best subsets is 0.51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridge regression is 0.48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LASSO is 0.44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic net is 0.48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LASSO with CV is 0.45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On a pure error, LASSO with seven features performed the best. However, does
    this best address the question that we are trying to answer? Perhaps the more
    parsimonious model that we found using CV with a lambda of `~0.125` is more appropriate.
    My inclination is to put forth the latter as it is more interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: Having said all this, there is clearly a need for domain-specific knowledge
    from oncologists, urologists, and pathologists in order to understand what would
    make the most sense. There is that, but there is also the need for more data.
    With this sample size, the results can vary greatly just by changing the randomization
    seeds or creating different `train` and `test` sets (try it and see for yourself.)
    At the end of the day, these results may likely raise more questions than provide
    you with answers. But is this bad? I would say no, unless you made the critical
    mistake of over-promising at the start of the project about what you will be able
    to provide. This is a fair warning to prudently apply the tools put forth in [Chapter
    1](b971f400-c64b-4e11-b9e0-9039038f4536.xhtml), *A Process for Success*.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization and classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The regularization techniques applied above will also work for classification
    problems, both binomial and multinomial.  Therefore, let's not conclude this chapter
    until we apply some sample code on a logistic regression problem, specifically
    the breast cancer data from the prior chapter.  As in regression with a quantitative
    response, this can be an important technique to utilize data sets with high dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that, in the breast cancer data we analyzed, the probability of a tumor
    being malignant can be denoted as follows in a logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(malignant) = 1 / 1 + e^(-(B0 + B1X1 + BnXn))*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have a linear component in the function, L1 and L2 regularization
    can be applied. To demonstrate this, let''s load and prepare the breast cancer
    data like we did in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform the data to an input matrix and the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In the function `cv.glmnet`, we will change the family to binomial and the
    measure to area under the curve, along with five folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting `fitCV` gives us the `AUC` by lambda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the plot command is as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Interesting! Notice the immediate improvement to `AUC` by adding just one feature.
    Let''s just have a look at the coefficients for one standard error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we see that the four features selected are `thickness`, `u.size`, `u.shape`,
    and `nucl`.  Like we did in the prior chapter, let''s look at how it performs
    on the test set in terms of error and `auc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Output from the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The results show that it performed comparable to the logistic regression done
    previously.  It doesn''t look like using `lambda.1se` was optimal and we should
    see if we can improve the output of the sample prediction using `lambda.min`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: There you have it!  An error rate as good as what we did in [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml), *Logistic
    Regression and Discriminant Analysis.*
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the goal was to use a small dataset to provide an introduction
    to practically apply an advanced feature selection for linear models. The outcome
    for our data was quantitative, but the `glmnet` package we used also supports
    qualitative outcomes (binomial and multinomial classifications). An introduction
    to regularization and the three techniques that incorporate it were provided and
    utilized to build and compare models. Regularization is a powerful technique to
    improve computational efficiency and to possibly extract more meaningful features
    when compared to the other modeling techniques. Additionally, we started to use
    the `caret` package to optimize multiple parameters when training a model. Up
    to this point, we've been purely talking about linear models. In the next couple
    of chapters, we will begin to use nonlinear models for both classification and
    regression problems.
  prefs: []
  type: TYPE_NORMAL
