- en: Advanced Feature Selection in Linear Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型中的高级特征选择
- en: '"I found that math got to be too abstract for my liking and computer science
    seemed concerned with little details--trying to save a microsecond or a kilobyte
    in a computation. In statistics I found a subject that combined the beauty of
    both math and computer science, using them to solve real-world problems."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “我发现数学对我来说变得过于抽象，而计算机科学似乎关注细节——试图在计算中节省一微秒或一千字节。在统计学中，我发现了一个结合了数学和计算机科学之美的学科，它们被用来解决现实世界的问题。”
- en: 'This was quoted by *Rob Tibshirani*, *Professor*, *Stanford University* at:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由斯坦福大学的**教授**、**Rob Tibshirani**引用的：
- en: '[https://statweb.stanford.edu/~tibs/research_page.html](http://statweb.stanford.edu/~tibs/research_page.html).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://statweb.stanford.edu/~tibs/research_page.html](http://statweb.stanford.edu/~tibs/research_page.html)。'
- en: 'So far, we''ve examined the usage of linear models for both quantitative and
    qualitative outcomes with an emphasis on the techniques of feature selection,
    that is, the methods and techniques to exclude useless or unwanted predictor variables.
    We saw that the linear models can be quite effective in machine learning problems.
    However, newer techniques that have been developed and refined in the last couple
    of decades or so can improve predictive ability and interpretability above and
    beyond the linear models that we discussed in the preceding chapters. In this
    day and age, many datasets have numerous features in relation to the number of
    observations or, as it is called, high-dimensionality. If you''ve ever worked
    on a genomics problem, this will quickly become self-evident. Additionally, with
    the size of the data that we are being asked to work with, a technique like best
    subsets or stepwise feature selection can take inordinate amounts of time to converge
    even on high-speed computers. I''m not talking about minutes: in many cases, hours
    of system time are required to get a best subsets solution.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了线性模型在定量和定性结果中的应用，重点介绍了特征选择的技术，即排除无用或不需要的预测变量的方法和技巧。我们看到线性模型在机器学习问题中非常有效。然而，在过去几十年中开发并完善的新的技术可以进一步提高预测能力和可解释性，超越我们在前几章中讨论的线性模型。在这个时代，许多数据集与观测数相比具有许多特征，这被称为高维性。如果你曾经从事过基因组学问题，这会很快变得显而易见。此外，随着我们被要求处理的数据量的大小，像最佳子集或逐步特征选择这样的技术可能需要不寻常的时间才能收敛，即使在高速计算机上也是如此。我说的不是分钟：在许多情况下，需要数小时的系统时间才能得到最佳子集解。
- en: There is a better way in these cases. In this chapter, we will look at the concept
    of regularization where the coefficients are constrained or shrunk towards zero.
    There are a number of methods and permutations to these methods of regularization
    but we will focus on Ridge regression, **Least Absolute Shrinkage and Selection
    Operator** (**LASSO**), and finally, elastic net, which combines the benefit of
    both techniques into one.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，有更好的方法。在本章中，我们将探讨正则化的概念，其中系数受到约束或缩小到零。正则化方法及其变体有很多，但我们将重点关注岭回归、**最小绝对收缩和选择算子**（**LASSO**），以及最终结合两种技术优势的弹性网络。
- en: Regularization in a nutshell
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化概述
- en: You may recall that our linear model follows the form, *Y = B0 + B[1]x[1] +...B[n]x[n]
    + e*, and also that the best fit tries to minimize the RSS, which is the sum of
    the squared errors of the actual minus the estimate, or *e[1]² + e[2]² + ... e[n]²*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，我们的线性模型遵循以下形式，*Y = B0 + B[1]x[1] +...B[n]x[n] + e*，以及最佳拟合尝试最小化RSS，即实际值与估计值之差的平方和，或*e[1]²
    + e[2]² + ... e[n]²*。
- en: With regularization, we will apply what is known as **shrinkage penalty** in
    conjunction with the minimization RSS. This penalty consists of a lambda (symbol
    *λ*), along with the normalization of the beta coefficients and weights. How these
    weights are normalized differs in the techniques, and we will discuss them accordingly.
    Quite simply, in our model, we are minimizing *(RSS + λ(normalized coefficients))*.
    We will select *λ*, which is known as the tuning parameter, in our model building
    process. Please note that if lambda is equal to 0, then our model is equivalent
    to OLS, as it cancels out the normalization term.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在正则化的过程中，我们将应用所谓的**收缩惩罚**，与最小化均方误差（RSS）相结合。这个惩罚包括一个lambda（符号 *λ*），以及beta系数和权重的归一化。这些权重如何归一化在不同的技术中有所不同，我们将相应地讨论它们。简单来说，在我们的模型中，我们是在最小化
    *(RSS + λ(归一化系数))*. 我们将在模型构建过程中选择*λ*，这被称为调整参数。请注意，如果lambda等于0，那么我们的模型就等同于OLS，因为归一化项被抵消了。
- en: So what does this do for us and why does it work? First of all, regularization
    methods are very computationally efficient. In best subsets, we are searching
    **2^p models**, and in large datasets, it may not be feasible to attempt. In R,
    we are only fitting one model to each value of lambda and this is far more efficient.
    Another reason goes back to our bias-variance trade-off, which was discussed in
    the preface. In the linear model, where the relationship between the response
    and the predictors is close to linear, the least squares estimates will have low
    bias but may have high variance. This means that a small change in the training
    data can cause a large change in the least squares coefficient estimates (James,
    2013). Regularization through the proper selection of lambda and normalization
    may help you improve the model fit by optimizing the bias-variance trade-off.
    Finally, regularization of coefficients works to solve multi collinearity problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们有什么好处，为什么它有效？首先，正则化方法在计算上非常高效。在最佳子集法中，我们正在搜索**2^p个模型**，在大数据集中，这可能不可行。在R中，我们只针对每个lambda值拟合一个模型，这要高效得多。另一个原因追溯到我们的偏差-方差权衡，这在前言中已经讨论过。在线性模型中，响应和预测变量之间的关系接近线性时，最小二乘估计将具有低偏差但可能具有高方差。这意味着训练数据中的微小变化可能导致最小二乘系数估计发生大的变化（James，2013）。通过适当选择lambda和规范化进行正则化可以帮助你通过优化偏差-方差权衡来提高模型拟合度。最后，系数的正则化有助于解决多重共线性问题。
- en: Ridge regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 岭回归
- en: Let's begin by exploring what ridge regression is and what it can and cannot
    do for you. With ridge regression, the normalization term is the sum of the squared
    weights, referred to as an **L2-norm**. Our model is trying to minimize *RSS +
    λ(sum Bj²)*. As lambda increases, the coefficients shrink toward zero but never
    become zero. The benefit may be an improved predictive accuracy, but as it does
    not zero out the weights for any of your features, it could lead to issues in
    the model's interpretation and communication. To help with this problem, we will
    turn to LASSO.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从探索岭回归是什么以及它能为你们做什么开始。使用岭回归，规范化项是权重平方的和，被称为**L2范数**。我们的模型试图最小化*RSS + λ(sum
    Bj²)*。随着lambda的增加，系数会缩小到零，但永远不会变成零。好处可能是一个改进的预测精度，但它不会为零化任何特征权重，这可能导致模型解释和沟通的问题。为了帮助解决这个问题，我们将转向LASSO。
- en: LASSO
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LASSO
- en: LASSO applies the **L1-norm** instead of the L2-norm as in ridge regression,
    which is the sum of the absolute value of the feature weights and thus minimizes
    *RSS + λ(sum |Bj|)*. This shrinkage penalty will indeed force a feature weight
    to zero. This is a clear advantage over ridge regression, as it may greatly improve
    the model interpretability.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO应用**L1范数**而不是岭回归中的L2范数，即特征权重的绝对值之和，从而最小化*RSS + λ(sum |Bj|)*。这种收缩惩罚确实会迫使特征权重为零。这相对于岭回归是一个明显的优势，因为它可能会大大提高模型的可解释性。
- en: The mathematics behind the reason that the L1-norm allows the weights/coefficients
    to become zero, is out of the scope of this book (refer to *Tibsharini*, *1996*
    for further details).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: L1范数允许权重/系数变为零的原因背后的数学原理超出了本书的范围（有关更详细的信息，请参阅Tibsharini，1996）。
- en: 'If LASSO is so great, then ridge regression must be clearly obsolete. Not so
    fast! In a situation of high collinearity or high pairwise correlations, LASSO
    may force a predictive feature to zero and thus you can lose the predictive ability;
    that is, say if both feature A and B should be in your model, LASSO may shrink
    one of their coefficients to zero. The following quote sums up this issue nicely:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果LASSO如此出色，那么岭回归显然已经过时了。但并非如此快！在高共线性或高成对相关性的情况下，LASSO可能会将预测特征强制设为零，从而你可能会失去预测能力；也就是说，如果特征A和B都应该在你的模型中，LASSO可能会将它们中的一个系数缩小到零。以下引用很好地总结了这个问题：
- en: '"One might expect the lasso to perform better in a setting where a relatively
    small number of predictors have substantial coefficients, and the remaining predictors
    have coefficients that are very small or that equal zero. Ridge regression will
    perform better when the response is a function of many predictors, all with coefficients
    of roughly equal size."'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “人们可能会期望在相对少数的预测变量具有较大系数，而其余预测变量的系数非常小或等于零的环境中，lasso的表现更好。当响应是许多具有大致相等系数的预测变量的函数时，岭回归将表现得更好。”
- en: -(James, 2013)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: -(James, 2013)
- en: There is the possibility of achieving the best of both the worlds and that leads
    us to the next topic, elastic net.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能实现两者的最佳结合，这引出了下一个主题，弹性网络。
- en: Elastic net
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性网络
- en: The power of elastic net is that, it performs the feature extraction that ridge
    regression does not and it will group the features that LASSO fails to do. Again,
    LASSO will tend to select one feature from a group of correlated ones and ignore
    the rest. Elastic net does this by including a mixing parameter, alpha, in conjunction
    with lambda. Alpha will be between `0` and `1` and as before, lambda will regulate
    the size of the penalty. Please note that an alpha of zero is equal to ridge regression
    and an alpha of one is equivalent to LASSO. Essentially, we are blending the L1
    and L2 penalties by including a second tuning parameter with a quadratic (squared)
    term of the beta coefficients. We will end up with the goal of minimizing *(RSS
    + λ[(1-alpha) (sum|Bj|²)/2 + alpha (sum |Bj|)])/N)*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网络的力量在于，它执行了岭回归没有进行的特征提取，并且它将LASSO无法做到的特征分组。再次强调，LASSO倾向于从一组相关特征中选择一个特征，而忽略其余的。弹性网络通过包括一个混合参数alpha，与lambda结合来实现这一点。alpha将在`0`和`1`之间，并且像之前一样，lambda将调节惩罚的大小。请注意，alpha为零等于岭回归，alpha为一等于LASSO。本质上，我们通过包括一个具有二次（平方）项的beta系数的第二个调整参数，将L1和L2惩罚混合在一起。我们的目标将是最小化*(RSS
    + λ[(1-alpha) (sum|Bj|²)/2 + alpha (sum |Bj|)])/N)*。
- en: Let's put these techniques to test. We will primarily utilize the `leaps`, `glmnet`,
    and `caret` packages to select the appropriate features and thus the appropriate
    model in our business case.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些技术付诸实践。我们将主要利用`leaps`、`glmnet`和`caret`包来选择适当特征，从而在我们的商业案例中选择适当的模型。
- en: Business case
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业案例
- en: For this chapter, we will stick to cancer--prostate cancer in this case. It
    is a small dataset of 97 observations and nine variables but allows you to fully
    grasp what is going on with regularization techniques by allowing a comparison
    with traditional techniques. We will start by performing best subsets regression
    to identify the features and use this as a baseline for our comparison.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将坚持使用癌症——在这种情况下是前列腺癌。这是一个包含97个观测值和九个变量的小型数据集，但通过允许与传统技术的比较，它使你能够完全理解正则化技术的工作原理。我们将首先执行最佳子集回归来识别特征，并以此作为我们比较的基线。
- en: Business understanding
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业理解
- en: '*The Stanford University Medical Center* has provided preoperative **Prostate
    Specific Antigen** (**PSA**) data on 97 patients who are about to undergo radical
    prostatectomy (complete prostate removal) for the treatment of prostate cancer.
    The **American Cancer Society** (**ACS**) estimates that nearly 30,000 American
    men died of prostate cancer in 2014 ([http://www.cancer.org/](http://www.cancer.org/)).
    PSA is a protein that is produced by the prostate gland and is found in the bloodstream.
    The goal is to develop a predictive model of PSA among the provided set of clinical
    measures. PSA can be an effective prognostic indicator, among others, of how well
    a patient can and should do after surgery. The patient''s PSA levels are measured
    at various intervals after the surgery and used in various formulas to determine
    if a patient is cancer-free. A preoperative predictive model in conjunction with
    the postoperative data (not provided here) can possibly improve cancer care for
    thousands of men each year.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*斯坦福大学医学中心*为即将进行根治性前列腺切除术（完全切除前列腺）以治疗前列腺癌的97名患者提供了术前**前列腺特异性抗原**（**PSA**）数据。**美国癌症协会**（**ACS**）估计，2014年近30,000名美国男性因前列腺癌去世([http://www.cancer.org/](http://www.cancer.org/))。PSA是由前列腺腺体产生并在血液中发现的蛋白质。目标是开发一个基于提供的临床指标的PSA预测模型。PSA可以作为有效的预后指标之一，表明患者手术后可以并且应该做得如何。患者的PSA水平在手术后的不同时间间隔内进行测量，并用于各种公式，以确定患者是否无癌。术前预测模型与术后数据（此处未提供）相结合，可能每年能改善数千名男性的癌症护理。'
- en: Data understanding and preparation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据理解和准备
- en: 'The data set for the 97 men is in a data frame with 10 variables, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 97名男性的数据集在一个包含10个变量的数据框中，如下所示：
- en: '`lcavol`: This is the log of the cancer volume'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lcavol`: 这是癌症体积的对数'
- en: '`lweight`: This is the log of the prostate weight'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lweight`: 这是前列腺重量的对数'
- en: '`age`: This is the age of the patient in years'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`age`: 这是患者的年龄（以年为单位）'
- en: '`lbph`: This is the log of the amount of **Benign Prostatic Hyperplasia** (**BPH**),
    which is the non-cancerous enlargement of the prostate'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lbph`: 这是良性前列腺增生（**BPH**）数量的对数，良性前列腺增生是前列腺的非癌性增大'
- en: '`svi`: This is the seminal vesicle invasion and an indicator variable of whether
    or not the cancer cells have invaded the seminal vesicles outside the prostate
    wall (`1` = yes, `0` = no)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`svi`：这是精囊侵犯，是一个指示变量，表示癌细胞是否已侵犯前列腺壁外的精囊（`1` = 是，`0` = 否）'
- en: '`lcp`: This is the log of capsular penetration and a measure of how much the
    cancer cells have extended in the covering of the prostate'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lcp`：这是囊性渗透的对数，表示癌细胞在前列腺覆盖层中扩展的程度'
- en: '`gleason`: This is the patient''s Gleason score; a score (2-10) provided by
    a pathologist after a biopsy about how abnormal the cancer cells appear--the higher
    the score, the more aggressive the cancer is assumed to be'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gleason`：这是患者的 Gleason 评分；病理学家在活检后提供的评分（2-10），表示癌细胞外观的异常程度——评分越高，癌症的假设侵略性越强'
- en: '`pgg4`: This is the percent of Gleason patterns-four or five (high-grade cancer)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pgg4`：这是 Gleason 模式的百分比——四或五（高级癌症）'
- en: '`lpsa`: This is the log of the PSA; it is the response/outcome'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lpsa`：这是 PSA 的对数；它是响应/结果'
- en: '`train`: This is a logical vector (true or false) that signifies the training
    or test set'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train`：这是一个逻辑向量（真或假），表示训练集或测试集'
- en: 'The dataset is contained in the R package `ElemStatLearn`. After loading the
    required packages and data frame, we can begin to explore the variables and any
    possible relationships, as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含在 R 包 `ElemStatLearn` 中。在加载所需的包和数据框后，我们可以开始探索变量和任何可能的关系，如下所示：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With the packages loaded, bring up the `prostate` dataset and explore its structure:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 加载了包之后，调用 `prostate` 数据集并探索其结构：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The examination of the structure should raise a couple of issues that we will
    need to double-check. If you look at the features, `svi`, `lcp`, `gleason`, and
    `pgg45` have the same number in the first 10 observations, with the exception
    of one--the seventh observation in `gleason`. In order to make sure that these
    are viable as input features, we can use plots and tables so as to understand
    them. To begin with, use the following `plot()` command and input the entire data
    frame, which will create a scatterplot matrix:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结构的检查应该会引发一些我们需要再次检查的问题。如果你查看特征，`svi`、`lcp`、`gleason` 和 `pgg45` 在前 10 个观察值中具有相同的数量，除了一个——`gleason`
    中的第七个观察值。为了确保这些可以作为输入特征，我们可以使用图表和表格来理解它们。首先，使用以下 `plot()` 命令并输入整个数据框，这将创建一个散点图矩阵：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_04_01.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_01.png)'
- en: 'With these many variables on one plot, it can get a bit difficult to understand
    what is going on, so we will drill down further. It does look like there is a
    clear linear relationship between our outcomes, `lpsa`, and `lcavol`. It also
    appears that the features mentioned previously have an adequate dispersion and
    are well-balanced across what will become our `train` and `test` sets with the
    possible exception of the `gleason` score. Note that the `gleason` scores captured
    in this dataset are of four values only. If you look at the plot where `train`
    and `gleason` intersect, one of these values is not in either `test` or `train`.
    This could lead to potential problems in our analysis and may require transformation.
    So, let''s create a plot specifically for that feature, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个图表上展示这么多变量，理解起来可能会有些困难，所以我们将进一步深入分析。看起来我们的结果 `lpsa` 和 `lcavol` 之间存在明显的线性关系。还看起来，之前提到的特征在将成为我们的
    `train` 和 `test` 集合的分布中具有足够的分散性，并且平衡良好，可能的例外是 `gleason` 评分。注意，这个数据集中捕获的 `gleason`
    评分只有四个值。如果你查看 `train` 和 `gleason` 相交的图表，其中一个值既不在 `test` 也不在 `train` 中。这可能会在我们的分析中引起潜在问题，可能需要转换。因此，让我们为这个特征创建一个特定的图表，如下所示：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following is the output of the preceding command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为前一个命令的输出：
- en: '![](img/image_04_02.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_02.png)'
- en: 'We have a problem here. Each dot represents an observation and the *x* axis
    is the observation number in the data frame. There is only one Gleason Score of
    *8.0* and only five of score *9.0*. You can look at the exact counts by producing
    a table of the features:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们遇到了一个问题。每个点代表一个观察值，*x* 轴是数据框中的观察值编号。只有一个 Gleason 评分为 *8.0*，只有五个评分为 *9.0*。你可以通过生成特征表来查看确切的计数：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'What are our options? We could do any of the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有哪些选择？我们可以做以下任何一项：
- en: Exclude the feature altogether
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全排除这个特征
- en: Remove only the scores of **8.0** and **9.0**
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只移除 **8.0** 和 **9.0** 的评分
- en: Recode this feature, creating an indicator variable
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新编码这个特征，创建一个指示变量
- en: 'I think it may help if we create a `boxplot` of `Gleason Score` versus `Log
    of PSA`. We used the `ggplot2` package to create boxplots in a prior chapter,
    but one can also create it with base R, as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我想如果我们创建一个`boxplot`来比较`Gleason评分`与`PSA对数`可能会有所帮助。我们在前一章中使用了`ggplot2`包来创建箱线图，但也可以使用基础R来实现，如下所示：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_04_03.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_03.png)'
- en: Looking at the preceding plot, I think the best option will be to turn this
    into an indicator variable with **0** being a **6** score and **1** being a **7**
    or a higher score. Removing the feature may cause a loss of predictive ability.
    The missing values will also not work with the `glmnet` package that we will use.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的图表，我认为最好的选择是将它转换为一个指标变量，其中**0**代表**6**分，**1**代表**7**分或更高分。移除这个特征可能会造成预测能力的损失。缺失值也不会与我们将使用的`glmnet`包兼容。
- en: 'You can code an indicator variable with one simple line of code using the `ifelse()`
    command by specifying the column in the data frame that you want to change. Then
    follow the logic that, if the observation is number *x*, then code it *y*, or
    else code it *z*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`ifelse()`命令通过指定要更改的数据框中的列来用一行代码编码一个指标变量。然后遵循以下逻辑：如果观测值是数字*x*，则编码为*y*，否则编码为*z*：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As always, let''s verify that the transformation worked as intended by creating
    a table in the following way:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，让我们通过以下方式创建一个表格来验证转换是否按预期进行：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'That worked to perfection! As the scatterplot matrix was hard to read, let''s
    move on to a correlation plot, which indicates if a relationship/dependency exists
    between the features. We will create a correlation object using the `cor()` function
    and then take advantage of the `corrplot` library with `corrplot.mixed()`, as
    follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切完美无缺！由于散点图矩阵难以阅读，让我们继续到一个相关性图，它表明特征之间是否存在关系/依赖。我们将使用`cor()`函数创建一个相关性对象，然后利用`corrplot`库的`corrplot.mixed()`函数，如下所示：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output of the preceding command is:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_04_04.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_04.png)'
- en: A couple of things jump out here. First, PSA is highly correlated with the log
    of cancer volume (`lcavol`); you may recall that in the scatterplot matrix, it
    appeared to have a highly linear relationship. Second, multicollinearity may become
    an issue; for example, cancer volume is also correlated with capsular penetration
    and this is correlated with the seminal vesicle invasion. This should be an interesting
    learning exercise!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里有几个亮点。首先，PSA（前列腺特异性抗原）与癌症体积的对数（`lcavol`）高度相关；你可能还记得，在散点图矩阵中，它似乎有一个高度线性的关系。其次，多重共线性可能成为一个问题；例如，癌症体积也与囊性浸润相关，而这与精囊浸润相关。这应该是一个有趣的练习！
- en: 'Before the learning can begin, the training and testing sets must be created.
    As the observations are already coded as being in the `train` set or not, we can
    use the `subset()` command and set the observations where `train` is coded to
    `TRUE` as our training set and `FALSE` for our testing set. It is also important
    to drop `train` as we do not want that as a feature:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习开始之前，必须创建训练集和测试集。由于观测值已经编码为是否在`train`集中，我们可以使用`subset()`命令，将`train`编码为`TRUE`的观测值作为我们的训练集，`FALSE`作为我们的测试集。同时，删除`train`也很重要，因为我们不希望它作为一个特征：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Modeling and evaluation
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模与评估
- en: With the data prepared, we will begin the modeling process. For comparison purposes,
    we will create a model using best subsets regression like the previous two chapters
    and then utilize the regularization techniques.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备就绪后，我们将开始建模过程。为了比较，我们将创建一个与前面两章类似的最佳子集回归模型，然后利用正则化技术。
- en: Best subsets
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳子集
- en: The following code is, for the most part, a rehash of what we developed in [Chapter
    2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml), *Linear Regression - The Blocking
    and Tackling of Machine Learning*. We will create the best subset object using
    the `regsubsets()` command and specify the `train` portion of `data`. The variables
    that are selected will then be used in a model on the `test` set, which we will
    evaluate with a mean squared error calculation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码基本上是我们[第2章](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml)中开发的，*线性回归 - 机器学习的基石*的重复。我们将使用`regsubsets()`命令创建最佳子集对象，并指定`data`的`train`部分。然后，选定的变量将用于`test`集上的模型，我们将通过均方误差计算来评估它。
- en: 'The model that we are building is written out as `lpsa ~ .` with the tilde
    and period stating that we want to use all the remaining variables in our data
    frame, with the exception of the response:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建的模型以`lpsa ~ .`的形式写出，波浪线和句点表示我们想要使用数据框中除响应变量之外的所有剩余变量：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With the model built, you can produce the best subset with two lines of code.
    The first one turns the `summary` model into an object where we can extract the
    various subsets and determine the best one with the `which.min()` command. In
    this instance, I will use BIC, which was discussed in [Chapter 2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml),
    *Linear Regression - The Blocking and Tackling of Machine Learning*, which is
    as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建完成后，你可以用两行代码生成最佳子集。第一行将`summary`模型转换为对象，我们可以从中提取各种子集，并使用`which.min()`命令确定最佳子集。在这种情况下，我将使用BIC，这在[第2章](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml)中讨论过，*线性回归
    - 机器学习的技巧和策略*，如下所示：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is telling us that the model with the `3` features has the lowest
    `bic` value. A plot can be produced to examine the performance across the subset
    combinations, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 输出告诉我们，具有`3`个特征的模型具有最低的`bic`值。可以生成一个图表来检查子集组合的性能，如下所示：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following is the output of the preceding command:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为前一个命令的输出：
- en: '![](img/image_04_05.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_05.png)'
- en: 'A more detailed examination is possible by plotting the actual model object,
    as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制实际模型对象，可以进行更详细的分析，如下所示：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the preceding command is:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_04_06.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_06.png)'
- en: 'So, the previous plot shows us that the three features included in the lowest
    `BIC` are `lcavol`, `lweight`, and `gleason`. It is noteworthy that `lcavol` is
    included in every combination of the models. This is consistent with our earlier
    exploration of the data. We are now ready to try this model on the `test` portion
    of the data, but first, we will produce a plot of the fitted values versus the
    actual values looking for linearity in the solution, and as a check on the constancy
    of the variance. A linear model will need to be created with just the three features
    of interest. Let''s put this in an object called `ols` for the OLS. Then the fits
    from `ols` will be compared to the actual in the training set, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前面的图表显示，包含在最低`BIC`值中的三个特征是`lcavol`、`lweight`和`gleason`。值得注意的是，`lcavol`包含在所有模型的组合中。这与我们之前对数据的探索一致。我们现在可以尝试在数据的`test`部分上尝试这个模型，但首先，我们将绘制拟合值与实际值之间的图表，以寻找解的线性关系，并检查方差的一致性。需要一个仅包含三个感兴趣特征的线性模型。让我们将其放入名为`ols`的对象中，然后比较`ols`的拟合值与训练集中的实际值，如下所示：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the output of the preceding command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为前一个命令的输出：
- en: '![](img/image_04_07.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_07.png)'
- en: 'An inspection of the plot shows that a linear fit should perform well on this
    data and that the non-constant variance is not a problem. With that, we can see
    how this performs on the test set data by utilizing the `predict()` function and
    specifying `newdata=test`, as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 检查图表显示，线性拟合应该在这组数据上表现良好，并且非恒定方差不是问题。因此，我们可以通过使用`predict()`函数并指定`newdata=test`来查看测试集数据上的表现，如下所示：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The values in the object can then be used to create a plot of the `Predicted
    vs Actual` values, as shown in the following image:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以使用对象中的值来创建`预测值 vs 实际值`的图表，如下所示：
- en: '![](img/image_04_08.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_08.png)'
- en: 'The plot doesn''t seem to be too terrible. For the most part, it is a linear
    fit with the exception of what looks to be two outliers on the high end of the
    PSA score. Before concluding this section, we will need to calculate **Mean Squared
    Error** (**MSE**) to facilitate comparison across the various modeling techniques.
    This is easy enough where we will just create the residuals and then take the
    mean of their squared values, as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图表看起来并不太糟糕。大部分是线性拟合，除了看起来像是PSA分数高端的两个异常值。在结束本节之前，我们需要计算**均方误差**（**MSE**）以促进各种建模技术的比较。这很容易，我们只需创建残差，然后取其平方值的平均值，如下所示：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: So, MSE of `0.508` is our benchmark for going forward.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MSE为`0.508`是我们的基准。
- en: Ridge regression
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ridge回归
- en: With ridge regression, we will have all the eight features in the model, so
    this will be an intriguing comparison with the best subsets model. The package
    that we will use and is in fact already loaded, is `glmnet`. The package requires
    that the input features are in a matrix instead of a data frame and for ridge
    regression, we can follow the command sequence of `glmnet(x = our input matrix,
    y = our response, family = the distribution, alpha=0)`. The syntax for alpha relates
    to `0` for ridge regression and `1` for doing LASSO.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用岭回归，我们将拥有模型中的所有八个特征，所以这将与最优子集模型进行比较，这是一个很有趣的比较。我们将使用的包是`glmnet`，实际上它已经加载了。该包要求输入特征以矩阵形式而不是数据框形式存在，对于岭回归，我们可以遵循`glmnet(x
    = our input matrix, y = our response, family = the distribution, alpha=0)`的命令序列。`alpha`的语法与岭回归的`0`相关，与LASSO的`1`相关。
- en: 'To get the `train` set ready for use in `glmnet` is actually quite easy by
    using `as.matrix()` for the inputs and creating a vector for the response, as
    follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要将`train`集准备好用于在`glmnet`中使用，实际上通过为输入使用`as.matrix()`并创建一个响应向量，是非常简单的，如下所示：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, run the ridge regression by placing it in an object called, appropriately
    I might add, `ridge`. It is important to note here that the `glmnet` package will
    first standardize the inputs before computing the lambda values and then will
    unstandardize the coefficients. You will need to specify the distribution of the
    response variable as `gaussian` as it is continuous and `alpha=0` for ridge regression,
    as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过将其放置在一个名为`ridge`的对象中运行岭回归。在这里需要注意的是，`glmnet`包将首先对输入进行标准化，然后计算`Lambda`值，然后将对系数进行反标准化。您需要指定响应变量的分布为`gaussian`，因为它连续，并且对于岭回归，`alpha=0`，如下所示：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The object has all the information that we need in order to evaluate the technique.
    The first thing to try is the `print()` command, which will show us the number
    of nonzero coefficients, percent deviance explained, and correspondent value of
    `Lambda`. The default number in the package of steps in the algorithm is `100`.
    However, the algorithm will stop prior to `100` steps if the percent deviation
    does not dramatically improve from one lambda to another; that is, the algorithm
    converges to an optimal solution. For the purpose of saving space, I will present
    only the following first five and last ten lambda results:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 该对象包含了我们评估技术所需的所有信息。首先尝试的是`print()`命令，它将显示非零系数的数量、解释的偏差百分比以及相应的`Lambda`值。包中算法步骤的默认数量是`100`。然而，如果从一个`Lambda`到另一个`Lambda`的百分比偏差没有显著改善，算法将在`100`步之前停止；也就是说，算法收敛到最优解。为了节省空间，我将只展示以下前五个和最后十个`Lambda`的结果：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Look at row `100` for example. It shows us that the number of nonzero coefficients
    or-said another way-the number of features included, is eight; please recall that
    it will always be the same for ridge regression. We also see that the percent
    of deviance explained is `.6971` and the `Lambda` tuning parameter for this row
    is `0.08789`. Here is where we can decide on which lambda to select for the `test`
    set. The lambda of `0.08789` can be used, but let''s make it a little simpler,
    and for the `test` set, try `0.10`. A couple of plots might help here so let''s
    start with the package''s default, adding annotations to the curve by adding `label=TRUE`
    in the following syntax:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以行`100`为例。它显示给我们，非零系数的数量，或者说换一种说法，包含的特征数量，是八个；请记住，对于岭回归来说，这始终是相同的。我们还看到，解释的偏差百分比是`.6971`，这一行的`Lambda`调整参数是`0.08789`。在这里，我们可以决定为`test`集选择哪个`Lambda`。可以使用`0.08789`的`Lambda`，但让我们让它更简单一些，对于`test`集，尝试`0.10`。几个图表可能会有所帮助，所以让我们从包的默认设置开始，通过在以下语法中添加`label=TRUE`来向曲线添加注释：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following is the output of the preceding command:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是上述命令的结果：
- en: '![](img/image_04_09.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_09.png)'
- en: 'In the default plot, the *y* axis is the value of Coefficients and the *x*
    axis is L1 Norm. The plot tells us the coefficient values versus the L1 Norm.
    The top of the plot contains a second *x* axis, which equates to the number of
    features in the model. Perhaps a better way to view this is by looking at the
    coefficient values changing as `lambda` changes. We just need to tweak the code
    in the following `plot()` command by adding `xvar="lambda"`. The other option
    is the percent of deviance explained by substituting `lambda` with `dev`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在默认图中，*y*轴是系数的值，*x*轴是L1范数。这个图告诉我们系数值与L1范数的关系。图的顶部包含第二个*x*轴，它等于模型中的特征数量。也许更好的方式是通过观察`lambda`变化时系数值的变化来查看。我们只需要调整以下`plot()`命令中的代码，添加`xvar="lambda"`。另一个选项是将`lambda`替换为`dev`来表示解释的偏差的百分比：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_04_10.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_10.png)'
- en: 'This is a worthwhile plot as it shows that as `lambda` decreases, the shrinkage
    parameter decreases and the absolute values of the coefficients increase. To see
    the coefficients at a specific `lambda` value, use the `coef()` command. Here,
    we will specify the `lambda` value that we want to use by specifying s=0.1\. We
    will also state that we want `exact=TRUE`, which tells `glmnet` to fit a model
    with that specific `lambda` value versus interpolating from the values on either
    side of our `lambda`, as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有价值的图示，因为它表明当`lambda`值减小时，收缩参数减小，系数的绝对值增加。要查看特定`lambda`值的系数，请使用`coef()`命令。在这里，我们将通过指定s=0.1来指定我们想要使用的`lambda`值。我们还将声明我们想要`exact=TRUE`，这将告诉`glmnet`使用该特定的`lambda`值来拟合模型，而不是从我们的`lambda`值两侧的值进行插值，如下所示：
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It is important to note that `age`, `lcp`, and `pgg45` are close to, but not
    quite, zero. Let''s not forget to plot deviance versus coefficients as well:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意`age`、`lcp`和`pgg45`接近于零，但并不完全为零。我们不要忘记同时绘制偏差与系数的关系图：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_04_11.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_11.png)'
- en: Comparing the two previous plots, we can see that as `lambda` decreases, the
    coefficients increase and the percent/fraction of the deviance explained increases.
    If we were to set `lambda` equal to zero, we would have no shrinkage penalty and
    our model would equate the `OLS`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 比较前两个图，我们可以看到当`lambda`值减小时，系数增加，解释的偏差的百分比/分数也增加。如果我们把`lambda`设为零，我们将没有收缩惩罚，我们的模型将与`OLS`等价。
- en: 'To prove this on the `test` set, we will have to transform the features as
    we did for the training data:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在`test`集上证明这一点，我们必须像对训练数据那样转换特征：
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, we use the `predict` function to create an object that we will call `ridge.y`
    with `type = "response"` and our `lambda` equal to `0.10` and plot the `Predicted`
    values versus the `Actual` values, as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`predict`函数创建一个名为`ridge.y`的对象，类型为`response`，`lambda`值为`0.10`，并绘制`预测值`与`实际值`的关系图，如下所示：
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the following command is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令的输出如下：
- en: '![](img/image_04_12.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_12.png)'
- en: 'The plot of `Predicted` versus `Actual` of `Ridge Regression` seems to be quite
    similar to best subsets, complete with two interesting outliers at the high end
    of the PSA measurements. In the real world, it would be advisable to explore these
    outliers further so as to understand whether they are truly unusual or we are
    missing something. This is where domain expertise would be invaluable. The MSE
    comparison to the benchmark may tell a different story. We first calculate the
    residuals, and then take the mean of those residuals squared:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`岭回归`的`预测值`与`实际值`的图似乎与最佳子集非常相似，包括两个在PSA测量值高端的有趣异常值。在现实世界中，建议进一步探索这些异常值，以了解它们是否确实不寻常，或者我们是否遗漏了某些东西。这正是领域专业知识非常有价值的地方。与基准的MSE比较可能会讲述不同的故事。我们首先计算残差，然后取这些残差的平方的平均值：'
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Ridge regression has given us a slightly better MSE. It is now time to put LASSO
    to the test to see if we can decrease our errors even further.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归给了我们一个稍微更好的MSE。现在是时候测试LASSO，看看我们是否可以进一步减少错误。
- en: LASSO
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LASSO
- en: 'To run LASSO next is quite simple and we only have to change one number from
    our ridge regression model: that is, change `alpha=0` to `alpha=1` in the `glmnet()`
    syntax. Let''s run this code and also see the output of the model, looking at
    the first five and last 10 results:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 运行LASSO的下一步非常简单，我们只需要从岭回归模型中更改一个数字：即，在`glmnet()`语法中将`alpha=0`更改为`alpha=1`。让我们运行这段代码，并查看模型的输出，查看前五个和最后十个结果：
- en: '[PRE27]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Note that the model building process stopped at step `69` as the deviance explained
    no longer improved as `lambda` decreased. Also, note that the `Df` column now
    changes along with `lambda`. At first glance, here it seems that all the eight
    features should be in the model with a `lambda` of `0.001572`. However, let''s
    try and find and test a model with fewer features, around seven, for argument''s
    sake. Looking at the rows, we see that around a `lambda` of `0.045`, we end up
    with `7` features versus `8`. Thus, we will plug this `lambda` in for our `test`
    set evaluation, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型构建过程在`69`步停止，因为随着`lambda`的减小，解释的偏差不再提高。此外，注意现在`Df`列会随着`lambda`的变化而变化。乍一看，这里似乎所有八个特征都应该包含在具有`0.001572`的`lambda`的模型中。然而，为了论证，让我们尝试找到一个具有大约七个特征的模型，并对其进行测试。查看行，我们看到在约`0.045`的`lambda`下，我们最终得到`7`个特征而不是`8`。因此，我们将这个`lambda`值用于我们的`test`集评估，如下所示：
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Just as with ridge regression, we can plot the results:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如与岭回归一样，我们可以绘制结果：
- en: '[PRE29]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following is the output of the preceding command:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面命令的输出：
- en: '![](img/image_04_13.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_13.png)'
- en: 'This is an interesting plot and really shows how LASSO works. Notice how the
    lines labeled **8**, **3**, and **6** behave, which corresponds to the `pgg45`,
    `age`, and `lcp` features respectively. It looks as if `lcp` is at or near zero
    until it is the last feature that is added. We can see the coefficient values
    of the seven feature model just as we did with ridge regression by plugging it
    into `coef()`, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的图表，真正展示了LASSO是如何工作的。注意标签为**8**、**3**和**6**的线条是如何表现的，分别对应`pgg45`、`age`和`lcp`特征。看起来`lcp`在或接近零，直到它是最后一个添加的特征。我们可以像使用岭回归一样通过将其插入`coef()`来看到七个特征模型的系数值，如下所示：
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The LASSO algorithm zeroed out the coefficient for `lcp` at a `lambda` of `0.045`.
    Here is how it performs on the `test` data:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO算法在`0.045`的`lambda`下将`lcp`的系数置零。以下是它在`test`数据上的表现：
- en: '[PRE31]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 前面命令的输出如下：
- en: '![](img/image_04_14.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_14.png)'
- en: 'We calculate MSE as we did before:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像之前一样计算MSE：
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: It looks like we have similar plots as before, with only the slightest improvement
    in MSE. Our last best hope for dramatic improvement is with elastic net. To this
    end, we will still use the `glmnet` package. The twist will be that, we will solve
    for `lambda` and for the elastic net parameter known as `alpha`. Recall that `alpha
    = 0` is the ridge regression penalty and `alpha = 1` is the LASSO penalty. The
    elastic net parameter will be `0 ≤ alpha ≤ 1`. Solving for two different parameters
    simultaneously can be complicated and frustrating, but we can use our friend in
    R, the `caret` package, for assistance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们有了与之前相似的图表，只是MSE略有提高。我们最后的希望是使用弹性网络来实现显著的改进。为此，我们仍然会使用`glmnet`包。转折点是，我们将求解`lambda`和弹性网络参数，称为`alpha`。回想一下，`alpha
    = 0`是岭回归惩罚，`alpha = 1`是LASSO惩罚。弹性网络参数将是`0 ≤ alpha ≤ 1`。同时求解两个不同的参数可能会很复杂和令人沮丧，但我们可以使用R的朋友，即`caret`包，来提供帮助。
- en: Elastic net
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性网络
- en: 'The `caret` package stands for classification and regression training. It has
    an excellent companion website to help in understanding all of its capabilities:
    [http://topepo.github.io/caret/index.html.](http://topepo.github.io/caret/index.html.)
    The package has many different functions that you can use and we will revisit
    some of them in the later chapters. For our purpose here, we want to focus on
    finding the optimal mix of lambda and our elastic net mixing parameter, `alpha`.
    This is done using the following simple three-step process:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`caret`包代表分类和回归训练。它有一个优秀的配套网站，可以帮助理解其所有功能：[http://topepo.github.io/caret/index.html.](http://topepo.github.io/caret/index.html.)
    该包有许多不同的函数可供使用，我们将在后面的章节中回顾其中的一些。就我们在这里的目的而言，我们想要专注于找到lambda和我们的弹性网络混合参数`alpha`的最佳组合。这是通过以下简单的三步过程完成的：'
- en: Use the `expand.grid()` function in base R to create a vector of all the possible
    combinations of `alpha` and `lambda` that we want to investigate.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基础R中的`expand.grid()`函数创建一个包含我们想要调查的所有可能的`alpha`和`lambda`组合的向量。
- en: Use the `trainControl()` function from the `caret` package to determine the
    resampling method; we will use LOOCV as we did in [Chapter 2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml),
    *Linear Regression - The Blocking and Tackling of Machine Learning*.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`caret`包中的`trainControl()`函数确定重采样方法；我们将使用与[第2章](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml)中相同的方法，即线性回归——机器学习的阻力和技巧。
- en: Train a model to select our `alpha` and `lambda` parameters using `glmnet()`
    in caret's `train()` function.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`caret`包的`train()`函数中的`glmnet()`函数训练一个模型来选择我们的`alpha`和`lambda`参数。
- en: Once we've selected our parameters, we will apply them to the `test` data in
    the same way as we did with ridge regression and LASSO.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们选定了参数，我们将以与之前进行岭回归和LASSO相同的方式将它们应用于`test`数据。
- en: Our grid of combinations should be large enough to capture the best model but
    not too large that it becomes computationally unfeasible. That won't be a problem
    with this size dataset, but keep this in mind for future references.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的组合网格应该足够大，以捕捉最佳模型，但又不至于太大，以至于变得计算上不可行。对于这个大小的数据集来说，这不会是问题，但请记住这一点以备将来参考。
- en: 'Here are the values of hyperparameters we can try:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们可以尝试的超参数值：
- en: Alpha from `0` to `1` by `0.2` increments; remember that this is bound by `0`
    and `1`
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Alpha`从`0`到`1`以`0.2`的增量；记住这被限制在`0`和`1`之间'
- en: Lambda from `0.00` to `0.2` in steps of `0.02`; the `0.2` lambda should provide
    a cushion from what we found in ridge regression (lambda=`0.1`) and LASSO (lambda=`0.045`)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lambda`从`0.00`到`0.2`以`0.02`的步长；`0.2`的`lambda`应该为我们从岭回归（lambda=`0.1`）和LASSO（lambda=`0.045`）中找到的提供缓冲。'
- en: 'You can create this vector by using the `expand.grid()` function and building
    a sequence of numbers for what the `caret` package will automatically use. The
    `caret` package will take the values for `alpha` and `lambda` with the following
    code:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`expand.grid()`函数和构建一个序列，为`caret`包将自动使用的`alpha`和`lambda`值创建这个向量。`caret`包将使用以下代码来获取`alpha`和`lambda`的值：
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `table()` function will show us the complete set of 66 combinations:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`table()`函数将显示完整的66种组合集：'
- en: '[PRE34]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can confirm that this is what we wanted--`alpha` from `0` to `1` and `lambda`
    from `0` to `0.2`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以确认这正是我们想要的--`alpha`从`0`到`1`，`lambda`从`0`到`0.2`。
- en: For the resampling method, we will put in the code for `LOOCV` for the method.
    There are also other resampling alternatives such as bootstrapping or k-fold cross-validation
    and numerous options that you can use with `trainControl()`, but we will explore
    these options in future chapters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于重采样方法，我们将为`LOOCV`方法输入代码。还有其他重采样替代方案，如自助法或k折交叉验证，以及您可以使用`trainControl()`的许多选项，但我们将在未来的章节中探讨这些选项。
- en: 'You can tell the model selection criteria with `selectionFunction()` in `trainControl()`.
    For quantitative responses, the algorithm will select based on its default of
    **Root Mean Square Error** (**RMSE**), which is perfect for our purposes:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`trainControl()`中的`selectionFunction()`来告诉模型选择标准。对于定量响应，算法将根据其默认的**均方根误差**（**RMSE**）进行选择，这对于我们的目的来说非常合适：
- en: '[PRE35]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It is now time to use `train()` to determine the optimal elastic net parameters.
    The function is similar to `lm()`. We will just add the syntax: `method="glmnet"`,
    `trControl=control` and `tuneGrid=grid`. Let''s put this in an object called `enet.train`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用`train()`来确定最优的弹性网络参数了。该函数与`lm()`类似。我们只需添加语法：`method="glmnet"`，`trControl=control`和`tuneGrid=grid`。让我们将这个放入一个名为`enet.train`的对象中：
- en: '[PRE36]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Calling the object will tell us the parameters that lead to the lowest `RMSE`,
    as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 调用对象将告诉我们导致最低`RMSE`的参数，如下所示：
- en: '[PRE37]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`RMSE` was used to select the optimal model using the smallest value. The final
    values used for the model were `alpha = 0` and `lambda = 0.08`.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RMSE`通过选择最小值来选择最佳模型。用于模型的最终值是`alpha = 0`和`lambda = 0.08`。
- en: This experimental design has led to the optimal tuning parameters of `alpha
    = 0` and `lambda = 0.08`, which is a ridge regression with `s = 0.08` in `glmnet`,
    recall that we used `0.10`. The `R-squared` is 61 percent, which is nothing to
    write home about.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实验设计导致了最优的调整参数`alpha = 0`和`lambda = 0.08`，这在`glmnet`中对应于`s = 0.08`的岭回归，记住我们使用了`0.10`。`R-squared`为61%，这并不值得大书特书。
- en: 'The process for the `test` set validation is just as before:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`test`集验证的过程与之前相同：'
- en: '[PRE38]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出如下：
- en: '![](img/image_04_15.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_15.png)'
- en: 'Calculate MSE as we did before:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 按照之前的方法计算MSE：
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This model error is similar to the ridge penalty. On the `test` set, our LASSO
    model did the best in terms of errors. We may be over-fitting! Our best subset
    model with three features is the easiest to explain, and in terms of errors, is
    acceptable to the other techniques. We can use cross-validation in the `glmnet`
    package to possibly identify a better solution.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型误差类似于岭回归惩罚。在`test`集上，我们的LASSO模型在误差方面表现最佳。我们可能存在过拟合！我们具有三个特征的最好子集模型最容易解释，并且在误差方面，对其他技术来说是可接受的。我们可以使用`glmnet`包中的交叉验证来可能识别更好的解决方案。
- en: Cross-validation with glmnet
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用glmnet进行交叉验证
- en: We have used `LOOCV` with the `caret` package; now we will try k-fold cross-validation.
    The `glmnet` package defaults to ten folds when estimating lambda in `cv.glmnet()`.
    In k-fold CV, the data is partitioned into an equal number of subsets (folds)
    and a separate model is built on each k-1 set and then tested on the corresponding
    holdout set with the results combined (averaged) to determine the final parameters.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用`LOOCV`和`caret`包；现在我们将尝试k折交叉验证。`glmnet`包在`cv.glmnet()`中估计lambda时默认为十折。在k折CV中，数据被分成相等数量的子集（折），然后在每个k-1集上构建一个单独的模型，然后在相应的保留集上进行测试，将结果（平均）合并以确定最终参数。
- en: 'In this method, each fold is used as a `test` set only once. The `glmnet` package
    makes it very easy to try this and will provide you with an output of the lambda
    values and the corresponding MSE. It defaults to `alpha = 1`, so if you want to
    try ridge regression or an elastic net mix, you will need to specify it. As we
    will be trying for as few input features as possible, we will stick to the defaults,
    but given the size of the training data, use only three folds:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在此方法中，每个折只作为`test`集使用一次。`glmnet`包使得尝试此方法非常容易，并将为您提供lambda值和相应的MSE的输出。默认值为`alpha
    = 1`，因此如果您想尝试岭回归或弹性网络混合，您需要指定它。由于我们将尝试尽可能少的输入特征，我们将坚持默认设置，但鉴于训练数据的大小，只使用三个折：
- en: '[PRE40]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/image_04_16.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_16.png)'
- en: 'The plot for CV is quite different than the other `glmnet` plots, showing log(Lambda)
    versus Mean-Squared Error along with the number of features. The two dotted vertical
    lines signify the minimum of MSE (left line) and one standard error from the minimum
    (right line). One standard error away from the minimum is a good place to start
    if you have an over-fitting problem. You can also call the exact values of these
    two lambdas, as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: CV的图表与其他`glmnet`图表大不相同，显示了log(Lambda)与均方误差的关系，以及特征的数量。两条虚线垂直线表示MSE的最小值（左侧线）和从最小值起的一个标准误差（右侧线）。如果存在过拟合问题，从最小值起的一个标准误差是一个好的起点。您还可以调用这两个lambda的确切值，如下所示：
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Using `lambda.1se`, we can go through the following process of viewing the
    coefficients and validating the model on the test data:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`lambda.1se`，我们可以通过以下过程查看系数并在测试数据上验证模型：
- en: '[PRE42]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This model achieves an error of `0.45` with just five features, zeroing out
    `age`, `lcp`, and `pgg45`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型仅使用五个特征就实现了`0.45`的误差，将`age`、`lcp`和`pgg45`归零。
- en: Model selection
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型选择
- en: 'We looked at five different models in examining this dataset. The following
    points were the `test` set error of these models:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查此数据集时，我们考虑了五个不同的模型。以下点是这些模型的`test`集误差：
- en: Best subsets is 0.51
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳子集的值为0.51
- en: Ridge regression is 0.48
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ridge回归的值为0.48
- en: LASSO is 0.44
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LASSO的值为0.44
- en: Elastic net is 0.48
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性网络为0.48
- en: LASSO with CV is 0.45
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LASSO与CV的值为0.45
- en: On a pure error, LASSO with seven features performed the best. However, does
    this best address the question that we are trying to answer? Perhaps the more
    parsimonious model that we found using CV with a lambda of `~0.125` is more appropriate.
    My inclination is to put forth the latter as it is more interpretable.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在纯误差方面，具有七个特征的LASSO表现最佳。然而，这最好地解决了我们试图回答的问题吗？也许我们使用CV和lambda值为`~0.125`找到的更简约的模型更合适。我的倾向是提出后者，因为它更易于解释。
- en: Having said all this, there is clearly a need for domain-specific knowledge
    from oncologists, urologists, and pathologists in order to understand what would
    make the most sense. There is that, but there is also the need for more data.
    With this sample size, the results can vary greatly just by changing the randomization
    seeds or creating different `train` and `test` sets (try it and see for yourself.)
    At the end of the day, these results may likely raise more questions than provide
    you with answers. But is this bad? I would say no, unless you made the critical
    mistake of over-promising at the start of the project about what you will be able
    to provide. This is a fair warning to prudently apply the tools put forth in [Chapter
    1](b971f400-c64b-4e11-b9e0-9039038f4536.xhtml), *A Process for Success*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多，很明显，我们需要来自肿瘤学家、泌尿科医生和病理学家的领域特定知识，以便理解什么最有意义。这是肯定的，但也有更多数据的需求。在这个样本大小下，结果可能会因为改变随机化种子或创建不同的`train`和`test`集而有很大差异（试试看你自己。）最终，这些结果可能提出的问题比提供的答案还多。但这不好吗？我会说不是，除非你在项目开始时犯了关键的错误，过度承诺了你将能提供什么。这是对谨慎应用[第1章](b971f400-c64b-4e11-b9e0-9039038f4536.xhtml)，*成功流程*中提出的工具的一个公平警告。
- en: Regularization and classification
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化和分类
- en: The regularization techniques applied above will also work for classification
    problems, both binomial and multinomial.  Therefore, let's not conclude this chapter
    until we apply some sample code on a logistic regression problem, specifically
    the breast cancer data from the prior chapter.  As in regression with a quantitative
    response, this can be an important technique to utilize data sets with high dimensionality.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 上述应用的正则化技术也适用于分类问题，包括二项式和多项式。因此，在我们对前一章中的逻辑回归问题，特别是乳腺癌数据应用一些示例代码之前，不要结束本章。正如在具有定量响应的回归中，这可以是一个利用高维数据集的重要技术。
- en: Logistic regression example
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归示例
- en: 'Recall that, in the breast cancer data we analyzed, the probability of a tumor
    being malignant can be denoted as follows in a logistic function:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在我们分析的乳腺癌数据中，肿瘤恶性的概率可以用以下逻辑函数表示：
- en: '*P(malignant) = 1 / 1 + e^(-(B0 + B1X1 + BnXn))*'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(恶性) = 1 / (1 + e^(-(B0 + B1X1 + BnXn)))*'
- en: 'Since we have a linear component in the function, L1 and L2 regularization
    can be applied. To demonstrate this, let''s load and prepare the breast cancer
    data like we did in the previous chapter:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由于函数中有线性成分，我们可以应用L1和L2正则化。为了演示这一点，让我们像上一章那样加载和准备乳腺癌数据：
- en: '[PRE43]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Transform the data to an input matrix and the labels:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据转换为输入矩阵和标签：
- en: '[PRE44]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In the function `cv.glmnet`, we will change the family to binomial and the
    measure to area under the curve, along with five folds:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在`cv.glmnet`函数中，我们将家族改为二项式，度量改为曲线下面积，并使用五折：
- en: '[PRE45]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Plotting `fitCV` gives us the `AUC` by lambda:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制`fitCV`，我们可以得到由lambda决定的`AUC`：
- en: '[PRE46]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output from the plot command is as shown:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图命令的输出如下所示：
- en: '![](img/image_04_17.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_17.png)'
- en: 'Interesting! Notice the immediate improvement to `AUC` by adding just one feature.
    Let''s just have a look at the coefficients for one standard error:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣！注意仅添加一个特征就立即提高了`AUC`。让我们看看一个标准误差的系数：
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Here, we see that the four features selected are `thickness`, `u.size`, `u.shape`,
    and `nucl`.  Like we did in the prior chapter, let''s look at how it performs
    on the test set in terms of error and `auc`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到选出的四个特征是`厚度`、`u.size`、`u.shape`和`nucl`。像上一章一样，让我们看看它在测试集上的表现，从错误和`auc`的角度来看：
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Output from the previous code:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码的输出：
- en: '![](img/image_04_18.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_04_18.png)'
- en: 'The results show that it performed comparable to the logistic regression done
    previously.  It doesn''t look like using `lambda.1se` was optimal and we should
    see if we can improve the output of the sample prediction using `lambda.min`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，它的表现与之前进行的逻辑回归相当。看起来使用`lambda.1se`并不最优，我们应该看看是否可以使用`lambda.min`来改善样本预测的输出：
- en: '[PRE49]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: There you have it!  An error rate as good as what we did in [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml), *Logistic
    Regression and Discriminant Analysis.*
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是了！错误率与我们在[第3章](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml)，*逻辑回归和判别分析*中所做的一样好。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, the goal was to use a small dataset to provide an introduction
    to practically apply an advanced feature selection for linear models. The outcome
    for our data was quantitative, but the `glmnet` package we used also supports
    qualitative outcomes (binomial and multinomial classifications). An introduction
    to regularization and the three techniques that incorporate it were provided and
    utilized to build and compare models. Regularization is a powerful technique to
    improve computational efficiency and to possibly extract more meaningful features
    when compared to the other modeling techniques. Additionally, we started to use
    the `caret` package to optimize multiple parameters when training a model. Up
    to this point, we've been purely talking about linear models. In the next couple
    of chapters, we will begin to use nonlinear models for both classification and
    regression problems.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，目标是使用一个小数据集来介绍如何在实际中应用高级特征选择方法于线性模型。我们数据的结果是定量的，但我们使用的`glmnet`包也支持定性结果（二项式和多项式分类）。我们提供了正则化的介绍以及包含它的三种技术，并利用这些技术来构建和比较模型。正则化是一种强大的技术，可以提高计算效率，并且与其他建模技术相比，可能提取出更有意义的特点。此外，我们开始使用`caret`包来优化模型训练时的多个参数。到目前为止，我们一直在纯粹地讨论线性模型。在接下来的几章中，我们将开始使用非线性模型来解决分类和回归问题。
