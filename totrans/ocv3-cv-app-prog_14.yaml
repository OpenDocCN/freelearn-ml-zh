- en: Chapter 14. Learning from Examples
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章. 从示例中学习
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下食谱：
- en: Recognizing faces using nearest neighbors of local binary patterns
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用局部二值模式的最近邻识别面部
- en: Finding objects and faces with a cascade of Haar features
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Haar特征级联查找对象和面部
- en: Detecting objects and people with Support Vector Machines and histograms of
    oriented gradients
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用支持向量机和方向梯度直方图检测对象和人物
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Machine learning is nowadays, very often used to solve difficult machine vision
    problems. In fact, it is a rich field of research encompassing many important
    concepts that would deserve a complete cookbook by itself. This chapter surveys
    some of the main machine learning techniques and explains how these can be deployed
    in computer vision systems using OpenCV.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，机器学习经常被用来解决困难的机器视觉问题。实际上，它是一个包含许多重要概念的丰富研究领域，本身就值得一本完整的食谱。本章概述了一些主要的机器学习技术，并解释了如何使用OpenCV将这些技术部署在计算机视觉系统中。
- en: At the core of machine learning is the development of computer systems that
    can learn how to react to data inputs by themselves. Instead of being explicitly
    programmed, machine learning systems automatically adapt and evolve when examples
    of desired behaviors are presented to them. Once a successful training phase is
    completed, it is expected that the trained system will output the correct response
    to new unseen queries.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的核心是开发能够自行学习如何对数据输入做出反应的计算机系统。机器学习系统不是被明确编程的，而是在展示期望行为的示例时自动适应和进化。一旦完成成功的训练阶段，预期训练后的系统将对新的未见查询输出正确的响应。
- en: Machine learning can solve many types of problems; our focus here will be on
    classification problems. Formally, in order to build a classifier that can recognize
    instances of a specific class of concepts, this one must be trained with a large
    set of annotated samples. In a 2-class problem, this set will be made of **positive
    samples** representing instances of the class to be learned, and of **negative
    samples** made of counter-examples of instances not belonging to the class of
    interest. From these observations, a **decision function** predicting the correct
    class of any input instances has to be learned.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以解决许多类型的问题；我们在这里的关注将是分类问题。形式上，为了构建一个能够识别特定概念类实例的分类器，必须使用大量标注样本对其进行训练。在二分类问题中，这个集合将包括**正样本**，代表要学习的类的实例，以及**负样本**，由不属于感兴趣类的不属于该类的实例的逆例组成。从这些观察结果中，必须学习一个**决策函数**，该函数可以预测任何输入实例的正确类别。
- en: In computer vision, those samples are images (or video segments). The first
    thing to do is therefore find a representation that will ideally describe the
    content of each image in a compact and distinctive way. One simplistic representation
    could be to use a fixed-size thumbnail image. The row-by-row succession of the
    pixels of this thumbnail image forms a vector that can then be used as a training
    sample presented to a machine learning algorithm. Other alternative and probably
    more effective representations can also be used. The recipes of this chapter describe
    different image representations and introduce some well-known machine learning
    algorithms. We should emphasize that we will not be able to cover in detail, all
    the theoretical aspects of the different machine learning techniques discussed
    in the recipes; our objective is rather to present the main principles governing
    their functioning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，这些样本是图像（或视频片段）。因此，首先要做的是找到一个理想的表达方式，以紧凑和独特的方式描述每张图像的内容。一个简单的表达方式可能是使用固定大小的缩略图图像。这个缩略图图像的像素按行排列形成一个向量，然后可以作为一个训练样本提交给机器学习算法。也可以使用其他替代方案，可能更有效的表示。本章的食谱描述了不同的图像表示，并介绍了一些著名的机器学习算法。我们应该强调，我们无法在食谱中详细涵盖所有讨论的不同机器学习技术的理论方面；我们的目标更多的是展示控制它们功能的主要原则。
- en: Recognizing faces using nearest neighbors of local binary patterns
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用局部二值模式的最近邻识别面部
- en: Our first exploration of machine learning techniques will start with what is
    probably the simplest approach, namely **nearest neighbor classification**. We
    will also present the local binary pattern feature, a popular representation encoding
    the textural patterns and contours of an image in a contrast independent way.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对机器学习技术的第一次探索将从可能是最简单的方法开始，即**最近邻分类**。我们还将介绍局部二值模式特征，这是一种流行的表示，以对比度无关的方式编码图像的纹理模式和轮廓。
- en: Our illustrative example will concern the face recognition problem. This is
    a very challenging problem that has been the object of numerous researches over
    the past 20 years. The basic solution we present here is one of the face recognition
    methods implemented in OpenCV. You will quickly realize that this solution is
    not very robust and works only under very favorable conditions. Nevertheless,
    this approach constitutes an excellent introduction to machine learning and to
    the face recognition problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例将涉及人脸识别问题。这是一个极具挑战性的问题，在过去20年中一直是众多研究的目标。我们在这里提出的基本解决方案是OpenCV中实现的人脸识别方法之一。你很快就会意识到这个解决方案并不非常稳健，只在非常有利的情况下才能工作。尽管如此，这种方法构成了对机器学习和人脸识别问题的极好介绍。
- en: How to do it...
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: The OpenCV library proposes a number of face recognition methods implemented
    as a subclass of the generic `cv::face::FaceRecognizer`. In this recipe, we will
    have a look at the `cv::face::LBPHFaceRecognizer` class, which is interesting
    to us because it is based on a simple but often very effective classification
    approach, the nearest neighbor classifier. Moreover, the image representation
    it uses is built from the **local binary pattern** feature (**LBP**) which is
    a very popular way of describing image patterns.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV库提出了一系列作为通用`cv::face::FaceRecognizer`子类的实现的人脸识别方法。在本例中，我们将探讨`cv::face::LBPHFaceRecognizer`类，它对我们来说很有趣，因为它基于一种简单但通常非常有效的分类方法，即最近邻分类器。此外，它所使用的图像表示是由**局部二值模式**特征（**LBP**）构建的，这是一种描述图像模式非常流行的方法。
- en: 'In order to create an instance of the `cv::face::LBPHFaceRecognizer`, its static
    `create` method is called:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建`cv::face::LBPHFaceRecognizer`的一个实例，需要调用其静态`create`方法：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As will be explained in the next section, the first two arguments provided
    serve to describe the characteristic of the LBP feature to be used. The next step
    is to feed the recognizer with a number of reference face images. This is done
    by providing two vectors, one containing the face images and the other one containing
    the associated labels. Each label is an arbitrary integer value identifying a
    particular individual. The idea is to train the recognizer by showing it different
    images of each of the people to be recognized. As you may imagine, the more representative
    images you provide, the better the chances that the correct person will be identified.
    In our very simplistic example, we simply provide two images of two reference
    persons. The `train` method is the one to call:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如下一节所述，提供的第一个参数用于描述要使用的LBP特征的特性。下一步是将一系列参考人脸图像输入识别器。这是通过提供两个向量来完成的，一个包含人脸图像，另一个包含相关的标签。每个标签是一个任意整数，用于标识特定的个人。想法是通过向识别器展示每个人的不同图像来训练它。正如你可能想象的那样，你提供的代表性图像越多，正确识别某人的机会就越大。在我们的非常简单的例子中，我们只提供了两个参考人物的图像。要调用的是`train`方法：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The images used are below, with the top row being images of person `0` and
    the second row images of person `1`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的图像如下，第一行是人物`0`的图像，第二行是人物`1`的图像：
- en: '![How to do it...](img/image_14_001.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/image_14_001.jpg)'
- en: 'The quality of these reference images is also very important. In addition,
    it would be a good idea to have them normalized such as to have the main facial
    features at standardized locations. For example, having the tip of the nose located
    in the middle of the image, and the two eyes horizontally aligned at a specific
    image row. Facial feature detection methods exist that can be used to automatically
    normalize face images this way. This was not done in our example, and the robustness
    of the recognizer will suffer from this. Nevertheless, this one is ready to be
    used, an input image can be provided, and it will try to predict the label to
    which this face image corresponds:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参考图像的质量也非常重要。此外，将它们归一化以使主要面部特征位于标准位置是一个好主意。例如，将鼻尖定位在图像中间，并将两只眼睛水平对齐在特定的图像行上。存在面部特征检测方法，可以用来自动以这种方式归一化面部图像。在我们的示例中，我们没有这样做，这会导致识别器的鲁棒性受到影响。尽管如此，这个识别器已经准备好使用，可以提供输入图像，并且它将尝试预测与该面部图像对应的标签：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our input image is the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入图像如下：
- en: '![How to do it...](img/image_14_002.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/image_14_002.jpg)'
- en: Not only does the recognizer return the predicted label, but it also returns
    a confidence score. In the case of the `cv::face::LBPHFaceRecognizer`, the lower
    this confidence value is, the more confident is the recognizer of its prediction.
    Here, we obtain a correct label prediction (`1`) with a confidence value of `90.3`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 识别器不仅返回预测的标签，还返回一个置信度分数。在`cv::face::LBPHFaceRecognizer`的情况下，这个置信度值越低，识别器对其预测的信心就越大。在这里，我们获得了一个正确的标签预测（`1`），置信度值为`90.3`。
- en: How it works...
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In order to understand the functioning of the face recognition approach presented
    in this recipe, we need to explain its two main components: the image representation
    used and the classification method that is applied.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解本食谱中展示的人脸识别方法的运作原理，我们需要解释其两个主要组件：所使用的图像表示和应用的分类方法。
- en: 'As its name indicates, the `cv::face::LBPHFaceRecognizer` algorithm makes use
    of the LBP feature. This is a contrast independent way of describing image patterns
    present in an image. It is a local representation that transforms every pixel
    into a binary representation encoding the pattern of image intensities found in
    a neighborhood. To achieve this goal, a simple rule is applied; a local pixel
    is compared to each of its selected neighbors; if its value is greater than that
    of its neighbor, then a `0` is assigned to the corresponding bit position, if
    not, then a `1` is assigned. In its simplest and most common form, each pixel
    is compared to its `8` immediate neighbors, which generates an 8-bit pattern.
    For example, let''s consider the following local pattern:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，`cv::face::LBPHFaceRecognizer`算法使用LBP特征。这是一种不依赖于对比度的描述图像中图像模式的方法。它是一种局部表示，将每个像素转换为一个二进制表示，该表示编码了在邻域中找到的图像强度模式。为了实现这一目标，应用了一个简单的规则；将局部像素与其选定的每个邻居进行比较；如果其值大于其邻居的值，则将`0`分配给相应的位位置，如果不是，则将`1`分配。在其最简单和最常见的形式中，每个像素与其`8`个直接邻居进行比较，这生成一个8位模式。例如，让我们考虑以下局部模式：
- en: '![How it works...](img/B05388_14_19.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/B05388_14_19.jpg)'
- en: 'Applying the described rule generates the following binary values:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 应用所描述的规则会生成以下二进制值：
- en: '![How it works...](img/B05388_14_20.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/B05388_14_20.jpg)'
- en: 'Taking as initial position, the top left pixel and moving clockwise, the central
    pixel will be replaced by the binary sequence `11011000`. Generating a complete
    8-bit LBP image is then easily achieved by looping over all pixels of an image
    to produce all corresponding LBP bytes. This is accomplished by the following
    function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以初始位置为左上角的像素，按顺时针方向移动，中心像素将被替换为二进制序列`11011000`。然后通过遍历图像的所有像素来生成所有相应的LBP字节，从而轻松地生成完整的8位LBP图像。这是通过以下函数实现的：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The body of the loop compares each pixel with its `8` neighbors and the bit
    values are assigned through simple bit shifts. With the following image:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 循环体将每个像素与其`8`个邻居进行比较，并通过简单的位移动来分配位值。以下是一个图像示例：
- en: '![How it works...](img/image_14_003.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/image_14_003.jpg)'
- en: 'An LBP image is obtained and can be displayed as a gray-level image:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 获得一个LBP图像，可以显示为灰度图像：
- en: '![How it works...](img/image_14_004.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/image_14_004.jpg)'
- en: This gray-level representation is not really interpretable, but it simply illustrates
    the encoding process that occurred.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灰度级表示实际上并不容易理解，但它只是简单地说明了发生的编码过程。
- en: Returning to our `cv::face::LBPHFaceRecognizer` class, it can be seen that the
    first two parameters of its `create` method specify the size (radius in pixels)
    and dimension (number of pixels along the circle, possibly applying interpolation)
    of the neighborhood to be considered. Once the LBP image is generated, the image
    is divided into a grid. The size of this grid is specified as the third parameter
    of the `create` method. For each block of this grid, a histogram of LBP values
    is constructed. A global image representation is finally obtained by concatenating
    the bin counts of all these histograms into one large vector. With an `8×8` grid,
    the set of computed 256-bin histograms then forms a 16384-dimensional vector.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的`cv::face::LBPHFaceRecognizer`类，可以看到其`create`方法的前两个参数指定了要考虑的邻域的大小（像素半径）和维度（圆周上的像素数，可能应用插值）。一旦生成了LBP图像，该图像被划分为一个网格。这个网格的大小被指定为`create`方法的第三个参数。对于这个网格的每一个块，构建一个LBP值的直方图。通过将这些直方图的bin计数连接成一个大的向量，最终获得一个全局图像表示。使用`8×8`网格，计算出的256-bin直方图集形成一个16384维向量。
- en: The `train` method of the `cv::face::LBPHFaceRecognizer` class therefore generates
    this long vector for each of the provided reference images. Each face image can
    then be seen as a point in a very high dimensional space. When a new image is
    submitted to the recognizer through its `predict` method, the closest reference
    point to this image is found. The label associated with this point is therefore
    the predicted label and the confidence value will be the computed distance. This
    is the principle that defines a nearest neighbor classifier. One more ingredient
    is generally added. If the nearest neighbor of the input point is too far from
    it, then this could mean that this point in fact does not belong to any of the
    reference classes. How far away must this point be to be considered as an outlier?
    This is specified by the fourth parameter of the `create` method of the `cv::face::LBPHFaceRecognizer`
    class.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`cv::face::LBPHFaceRecognizer`类的`train`方法为提供的每个参考图像生成这个长向量。每个面部图像可以看作是在一个非常高维空间中的一个点。当通过其`predict`方法提交新图像时，找到与该图像最近的参考点。因此，与该点关联的标签是预测标签，置信度值将是计算出的距离。这是定义最近邻分类器的原则。通常还会添加另一个成分。如果输入点的最近邻点离它太远，这可能意味着这个点实际上不属于任何参考类别。这个点必须离多远才能被认为是异常值？这由`cv::face::LBPHFaceRecognizer`类的`create`方法的第四个参数指定。
- en: As you can see, this is a very simple idea and it turns out to be very effective
    when the different classes generate distinct clouds of points in the representational
    space. Another benefit of this approach is that the method implicitly handles
    multiple classes, as it simply reads the predicted class from its nearest neighbors.
    The main drawback is its computational cost. Finding the nearest neighbor in such
    a large space, possibly composed of many reference points, can take time. Storing
    all these reference points is also costly in memory.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这是一个非常简单的想法，当不同的类别在表示空间中生成不同的点云时，它非常有效。这种方法的好处之一是它隐式地处理多个类别，因为它简单地从其最近邻读取预测类别。主要缺点是它的计算成本。在这样一个可能由许多参考点组成的大空间中找到最近邻可能需要时间。存储所有这些参考点在内存中也很昂贵。
- en: See also
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: 'The article by *T. Ahonen*, *A. Hadid* and *M. Pietikainen*, *Face description
    with Local Binary Patterns: Application to Face Recognition* in IEEE transaction
    on *Pattern Analysis and Machine Intelligence*, 2006 describes the use of LBP
    for face recognition'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由*T. Ahonen*、*A. Hadid*和*M. Pietikainen*撰写的文章《使用局部二值模式进行人脸描述：应用于人脸识别》发表在2006年的IEEE《模式分析与机器智能》交易上，描述了LBP在人脸识别中的应用。
- en: The article by *B. Froba* and *A. Ernst*, *Face detection with the modified
    census transform* in IEEE conference on*Automatic Face and Gesture Recognition*,
    2004 proposes a variant of the LBP features
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由*B. Froba*和*A. Ernst*撰写的文章《使用改进的计数变换进行人脸检测》发表在2004年的IEEE会议《自动人脸和手势识别》上，提出了一种LBP特征的变体。
- en: The article by *M. Uricar*, *V. Franc* and *V. Hlavac*, *Detector of Facial
    Landmarks Learned by the Structured Output SVM* in International Conference on
    *Computer Vision Theory and Applications*, 2012 describes a facial feature detector
    based on the SVMs discussed in the last recipe of this chapter
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由*M. Uricar*、*V. Franc*和*V. Hlavac*撰写的文章《基于结构化输出SVM学习的人脸特征检测器》发表在2012年的《计算机视觉理论与应用国际会议》上，描述了一种基于本章最后一种SVM讨论的人脸特征检测器。
- en: Finding objects and faces with a cascade of Haar features
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Haar特征的级联查找对象和面部
- en: We learned in the previous recipe, some of the basic concepts of machine learning.
    We showed how a classifier can be built by collecting samples of the different
    classes of interest. However, for the approach that was considered in this previous
    recipe, training a classifier simply consists of storing all the samples' representations.
    From there, the label of any new instance can be predicted by looking at the closest
    (nearest neighbor) labeled point. For most machine learning methods, training
    is rather an iterative process during which machinery is built by looping over
    the samples. Performance of the classifier thus produced gradually improves as
    more samples are presented. Learning eventually stops when a certain performance
    criterion is reached or when no more improvements can be obtained by considering
    the current training dataset. This recipe will present a machine learning algorithm
    that follows this procedure, the **cascade of boosted classifiers**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个菜谱中，我们学习了机器学习的一些基本概念。我们展示了如何通过收集不同类别的样本来构建分类器。然而，对于前一个菜谱中考虑的方法，训练分类器简单地就是存储所有样本的表示。从那里，任何新实例的标签可以通过查看最近的（最近邻）标记点来预测。对于大多数机器学习方法，训练是一个迭代过程，其中通过遍历样本来构建机器。因此，产生的分类器的性能随着更多样本的呈现而逐渐提高。当达到某个性能标准或考虑当前训练数据集时不再能获得更多改进时，学习最终停止。这个菜谱将介绍一个遵循此程序的机器学习算法，即**提升分类器的级联**。
- en: But before we look at this classifier, we will first turn our attention to the
    Haar feature image representation. We indeed learned that a good representation
    is an essential ingredient in the production of a robust classifier. LBPs, as
    described in the previous recipe, *Recognizing faces using nearest neighbors of
    local binary patterns*, constitute one possible choice; the next section describes
    another popular representation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看这个分类器之前，我们首先将注意力转向Haar特征图像表示。我们确实了解到，一个好的表示是生产鲁棒分类器的一个基本要素。正如前一个菜谱中描述的，LBPs（局部二值模式）构成了一种可能的选择；下一节将描述另一种流行的表示。
- en: Getting ready
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备中
- en: The first step in the generation of a classifier is to assemble a (preferably)
    large collection of image samples showing different instances of the classes of
    objects to be identified. The way these samples are represented has been shown
    to have an important impact on the performance of the classifier that is to be
    built from them. Pixel-level representations are generally considered to be too
    low-level to robustly describe the intrinsic characteristics of each class of
    objects. Representations that can describe, at various scales, the distinctive
    patterns present in an image are preferable. This is the objective of the **Haar
    features** also sometimes called Haar-like features because they derive from the
    Haar transform basis functions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生成分类器的第一步是收集一个（最好是）大型的图像样本集合，展示要识别的对象类别的不同实例。这些样本的表示方式已被证明对从它们构建的分类器的性能有重要影响。像素级表示通常被认为太低级，无法鲁棒地描述每个对象类别的内在特征。能够描述图像中存在的独特模式的各种尺度的表示更受欢迎。这就是**Haar特征**的目标，有时也称为Haar-like特征，因为它们来自Haar变换基函数。
- en: The Haar features define small rectangular areas of pixels, these later being
    compared through simple subtractions. Three different configurations are generally
    considered, namely the 2-rectangle, 3-rectangle, and 4-rectangle features
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Haar特征定义了像素的小矩形区域，这些区域随后通过简单的减法进行比较。通常考虑三种不同的配置，即2矩形、3矩形和4矩形特征
- en: '![Getting ready](img/image_14_005.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![准备中](img/image_14_005.jpg)'
- en: 'These features can be of any size and applied on any area of the image to be
    represented. For example, here are two Haar features applied on a face image:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征可以是任何大小，并且可以应用于要表示的图像的任何区域。例如，这里有两个应用于人脸图像的Haar特征：
- en: '![Getting ready](img/image_14_006.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![准备中](img/image_14_006.jpg)'
- en: Building a Haar representation consists of selecting a number of Haar features
    of given types, sizes, and locations and applying them on images. The specific
    set of values obtained from the chosen set of Haar features constitutes the image
    representation. The challenge is then to determine which set of features to select.
    Indeed, to distinguish one class of objects from another, some Haar features must
    be more relevant than others. For example, in the case of the class of face images,
    applying a 3-rectangle Haar feature between the eyes (as shown in the figure above)
    could be a good idea as we expect all face images to consistently produce a high
    value in this case. Obviously, since there exist hundreds of thousands of possible
    Haar features, it would certainly be difficult to manually make a good selection.
    We are then looking for a machine learning method that would select the most relevant
    features for a given class of objects.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 构建Haar表示包括选择一定数量的Haar特征，这些特征具有给定的类型、大小和位置，并将它们应用于图像。从所选的Haar特征集中获得的具体值集构成了图像表示。挑战在于确定要选择哪些特征集。确实，为了区分一类对象和另一类对象，某些Haar特征必须比其他特征更相关。例如，在面部图像类的情况下，在眼睛之间应用一个3矩形Haar特征（如图所示）可能是一个好主意，因为我们期望所有面部图像在这种情况下都能产生一个高值。显然，由于存在数十万个可能的Haar特征，手动做出好的选择肯定很困难。因此，我们正在寻找一种机器学习方法，该方法将选择给定类别对象的最相关特征。
- en: How to do it...
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: In this recipe, we will learn how we can build, using OpenCV, a **boosted cascade
    of features** to produce a 2-class classifier. But before we do, let's explain
    the terminology that is used here. A 2-class classifier is one that can identify
    the instances of one class (for example, face images) from the rest (for example,
    images that do not contain faces). We therefore have in this case the **positive
    samples** (that is, face images) and the **negative samples** (that is, non-face
    images), these latter are also called the background images. The classifier of
    this recipe will be made of a cascade of simple classifiers that will be sequentially
    applied. Each stage of the cascade will make a quick decision about rejecting
    or not rejecting the object shown based on the values obtained for a small subset
    of features. This cascade is boosted in the sense that each stage improves (boosts)
    the performance of the previous ones by making more accurate decisions. The main
    advantage of this approach is that the early stages of the cascade are composed
    of simple tests that can then quickly reject instances that certainly do not belong
    to the class of interest. These early rejections make the cascade classifier quick,
    because when searching for a class of objects by scanning an image, most sub-windows
    to be tested will not belong to the class of interest. This way, only few windows
    will have to pass through all stages before being accepted or rejected.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将学习如何使用OpenCV构建一个**特征级联提升**，以生成一个二分类器。但在我们这样做之前，让我们解释一下这里使用的术语。一个二分类器是指能够从其余部分（例如，不包含面部图像的图像）中识别出一个类别的实例（例如，面部图像）。因此，在这种情况下，我们有**正样本**（即面部图像）和**负样本**（即非面部图像），这些后者也被称为背景图像。本菜谱中的分类器将由一系列简单分类器组成，这些分类器将依次应用。级联的每个阶段都将基于一小部分特征值快速做出拒绝或不拒绝显示对象的决策。这种级联是提升的，因为每个阶段通过做出更准确的决策来提高（提升）之前阶段的性能。这种方法的主要优势在于，级联的早期阶段由简单的测试组成，这些测试可以快速拒绝肯定不属于感兴趣类别的实例。这些早期拒绝使得级联分类器快速，因为在通过扫描图像查找对象类别时，大多数要测试的子窗口都不会属于感兴趣类别。这样，只有少数窗口需要在被接受或拒绝之前通过所有阶段。
- en: In order to train a boosted classifier cascade for a specific class, OpenCV
    offers a software tool that will perform all the required operations. When you
    install the library, you should have two executable modules created and located
    in the appropriate `bin` directory, these are `opencv_createsamples.exe` and `opencv_traincascade.exe`.
    Make sure your system `PATH` points to this directory so that you can execute
    these tools from anywhere.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为特定类别训练一个提升分类器级联，OpenCV提供了一个将执行所有必需操作的软件工具。当你安装库时，你应该在适当的`bin`目录中创建了两个可执行模块，这些是`opencv_createsamples.exe`和`opencv_traincascade.exe`。确保你的系统`PATH`指向此目录，这样你就可以在任何地方执行这些工具。
- en: 'When training a classifier, the first thing to do is to collect the samples.
    The positive ones are made of images showing instances of the target class. In
    our simple example, we decided to train a classifier to recognize stop signs.
    Here are the few positive samples we have collected:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练分类器时，首先要做的事情是收集样本。正样本由显示目标类别实例的图像组成。在我们的简单例子中，我们决定训练一个分类器来识别停车标志。以下是我们已经收集的几个正样本：
- en: '![How to do it...](img/image_14_007.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/image_14_007.jpg)'
- en: 'The list of the positive samples to be used must be specified in a text file
    that we have, here, named `stop.txt`. It contains image filenames and bounding
    box coordinates:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拥有的文本文件中必须指定要使用的正样本列表，这里命名为`stop.txt`。它包含图像文件名和边界框坐标：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first number after the filename is the number of positive samples visible
    in the image. Next is the upper left coordinate of the bounding box containing
    this positive sample and finally its width and height. In our case, the positive
    samples have already been extracted from their original images, this is why we
    have always one sample per file and upper-left coordinates at `(0,0)`. Once this
    file is available, you can then create the positive sample file by running the
    extractor tool.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 文件名后的第一个数字是图像中可见的正样本数量。接下来是包含此正样本的边界框的左上角坐标，最后是其宽度和高度。在我们的例子中，正样本已经从原始图像中提取出来，这就是为什么我们每个文件只有一个样本，左上角坐标在`(0,0)`。一旦这个文件可用，您就可以通过运行提取工具来创建正样本文件。
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will create an output file `stop.vec` that will contain all the positive
    samples specified in the input text file. Note that we made the sample size smaller
    (`24×24`) than the original size (`64×64`). The extractor tool resizes all samples
    to the specified size. Usually, Haar features work better with smaller templates,
    but this is something that has to be validated on a case-by-case basis.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个输出文件`stop.vec`，其中将包含输入文本文件中指定的所有正样本。请注意，我们使样本大小小于原始大小（`24×24`），而原始大小是`64×64`。提取工具将所有样本调整到指定的大小。通常，Haar特征与较小的模板配合得更好，但这需要在每个具体案例中验证。
- en: The negative samples are simply background images containing no instances of
    the class of interest (no stop signs in our case). But these images should show
    a good variety of what the classifier is expected to see. These negative images
    could be of any size, the training tool will extract random negative samples from
    them. Here is one example of a background image we wish to use.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 负样本仅仅是包含感兴趣类别（在我们的例子中即没有停车标志）实例的背景图像。但这些图像应该展示出分类器预期看到的各种情况。这些负图像可以是任何大小，训练工具将从它们中提取随机的负样本。以下是我们希望使用的背景图像的一个示例。
- en: '![How to do it...](img/image_14_008.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/image_14_008.jpg)'
- en: 'Once the positive and negative sample sets are in place, the classifier cascade
    is ready to be trained. Calling the tool is done as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦正负样本集已经就绪，分类器级联就准备好进行训练。调用工具的方式如下：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The parameters used here will be explained in the next section. Note that this
    training process can take a very long time; in some complex cases with thousands
    of samples, it can even take days to execute. As it runs, the cascade trainer
    will print out performance reports each time the training of a stage is completed.
    In particular, the classifier will tell you what the current **hit rate** (**HR**)
    is; this is the percentage of positive samples that are currently accepted by
    the cascade (that is, correctly recognized as positive instances, they are also
    called the **true positives**). You want this number to be as close as possible
    to `1.0`. It will also give you the current **false alarm rate** (**FA**) which
    is the number of tested negative samples that are wrongly classified as positive
    instances (also called the **false positives**). You want this number to be as
    close as possible to `0.0`. These numbers are reported for each of the features
    introduced in each stage.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的参数将在下一节中解释。请注意，这个过程可能需要非常长的时间；在某些具有数千个样本的复杂案例中，执行甚至可能需要几天。在运行过程中，级联训练器将在每个阶段的训练完成后打印出性能报告。特别是，分类器会告诉您当前的**命中率**（**HR**）；这是当前被级联接受（即正确识别为正实例）的正样本的百分比。您希望这个数字尽可能接近`1.0`。它还会给出当前的**误报率**（**FA**），即被错误分类为正实例的测试负样本数量（也称为**假阳性**）。您希望这个数字尽可能接近`0.0`。这些数字会为每个阶段中引入的每个特征报告。
- en: Our simple example took only few seconds. The structure of the classifier produced
    is described in an XML file that results from the training phase. The classifier
    is then ready to be used! You can submit any sample to it and it will tell you
    if it thinks that it is a positive or a negative one.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的简单示例只用了几秒钟。产生的分类器结构在训练阶段的 XML 文件中描述。分类器随后就准备好使用了！你可以提交任何样本给它，它将告诉你它认为这是一个正样本还是一个负样本。
- en: 'In our example, we trained our classifier with `24×24` images but in general,
    what you want is to find out if there are any instances of your class of objects
    somewhere in an image (of any size). To achieve this objective, you simply have
    to scan the input image and extract all possible windows of the sample size. If
    your classifier is accurate enough, only the windows that contain the seek objects
    will return a positive detection. But this works as long as the visible positive
    samples have the appropriate size. To detect instances at multiple scales, you
    then have to build a pyramid of images by reducing the size of the original image
    by a certain factor at each level of the pyramid. This way, bigger objects will
    eventually fit the trained sample size as we go down the pyramid. This is a long
    process, but the good news is that OpenCV provides a class that implements this
    process. Its use is pretty straightforward. First you construct the classifier
    by loading the appropriate XML file:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们使用 `24×24` 的图像来训练我们的分类器，但通常，你想要找出图像（任何大小）中是否有任何你的一类对象的实例。为了达到这个目标，你只需扫描输入图像并提取所有可能的样本大小的窗口。如果你的分类器足够准确，只有包含所寻对象的窗口才会返回正检测。但是，只要可见的正样本具有适当的大小，这种方法才会有效。为了在多个尺度上检测实例，你必须通过在每个金字塔级别上以一定比例减小原始图像的大小来构建图像金字塔。这样，较大的对象最终会适应我们在金字塔下降过程中训练的样本大小。这是一个漫长的过程，但好消息是
    OpenCV 提供了一个实现此过程的类。它的使用相当简单。首先，通过加载适当的 XML 文件来构建分类器：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, you call the `detection` method with an input image:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你调用带有输入图像的 `detection` 方法：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The result is provided as a vector of `cv::Rect` instances. To visualize the
    detection results, you just have to draw these rectangles on your input image:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 结果以 `cv::Rect` 实例的向量形式提供。为了可视化检测结果，你只需在输入图像上绘制这些矩形：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When our classifier is tested on an image, here is the result we obtained:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的分类器在图像上进行测试时，这是我们获得的结果：
- en: '![How to do it...](img/image_14_009.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/image_14_009.jpg)'
- en: How it works...
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In the previous section we explained how it is possible to build an OpenCV cascade
    of classifiers using positive and negative samples of a class of objects. We will
    now overview the basic steps of the learning algorithm used to train this cascade.
    Our cascade has been trained using the Haar features that were described in the
    introductory section of this recipe but, as we will see, any other simple feature
    can be used to build a boosted cascade. As the theory and principles of boosted
    learning are pretty complex, we will not cover all aspects in this recipe; interested
    readers should refer to the articles listed in the last section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们解释了如何使用一类对象的正负样本构建 OpenCV 级联分类器。现在，我们将概述用于训练此级联的学习算法的基本步骤。我们的级联是通过使用本食谱介绍部分中描述的
    Haar 特征进行训练的，但正如我们将看到的，任何其他简单特征都可以用来构建一个提升级联。由于提升学习的理论和原理相当复杂，我们不会在本食谱中涵盖所有方面；感兴趣的读者应参考最后部分列出的文章。
- en: Let's first restate that there are two core ideas behind the cascade of boosted
    classifiers. The first one is that a strong classifier can be built by combining
    together several weak classifiers (that is, those based on simple features). Secondly,
    because in machine vision, negative instances are found much more frequently than
    the positive ones, effective classification can be performed in stages. The early
    stages make quick rejection of obvious negative instances, and more refined decisions
    can be made at later stages for more difficult samples. Based on these two ideas,
    we now describe the boosted cascade learning algorithm. Our explanations are based
    on the variant of boosting called **AdaBoost** **,** which is the one most often
    used. Our description will also allow us to explain some of the parameters used
    in the `opencv_traincascade` tool.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们重申，在级联提升分类器的背后有两个核心思想。第一个思想是，可以通过组合多个弱分类器（即基于简单特征的分类器）来构建一个强分类器。其次，因为在机器视觉中，负实例比正实例出现得更为频繁，因此可以分阶段进行有效的分类。早期阶段快速拒绝明显的负实例，而在后期阶段对更难处理的样本做出更精细的决策。基于这两个思想，我们现在描述提升级联学习算法。我们的解释基于称为**AdaBoost**的增强方法，这是最常使用的一种。我们的描述还将使我们能够解释`opencv_traincascade`工具中使用的某些参数。
- en: In this recipe, we use the Haar features in order to build our weak classifier.
    When one Haar feature is applied (of given type, size, and location), a value
    is obtained. A simple classifier is then obtained by finding the threshold value
    that would best classify the negative and positive class instances based on this
    feature value. To find this optimal threshold, we have at our disposal, a number
    of positive and negative samples (the number of positive and negative samples
    to be used at this step by `opencv_traincascade` is given by the `-numPos` and
    `-numNeg` parameters). Since we have a large number of possible Haar features,
    we examine all of them and select the one that best classifies our sample set.
    Obviously, this very basic classifier will make errors (that is, misclassify several
    samples); this is why we need to build several of these classifiers. These classifiers
    are added iteratively, each time searching for the new Haar feature giving the
    best classification. But since, at each iteration, we want to focus on the samples
    that are currently misclassified, the classification performance is measured by
    giving a higher weight to the misclassified samples. A set of simple classifiers
    is thus obtained and a strong classifier is then built from a weighted sum of
    these weak classifiers (classifiers with better performance being given a higher
    weight). Following this approach, a strong classifier with good performance can
    be obtained by combining a few hundred simple features.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，我们使用Haar特征来构建我们的弱分类器。当应用一个Haar特征（给定类型、大小和位置）时，会得到一个值。然后通过找到最佳阈值值来获得一个简单的分类器，这个阈值值将根据这个特征值将负类和正类实例区分开来。为了找到这个最佳阈值，我们有大量的正样本和负样本可供使用（在`opencv_traincascade`中，这一步要使用的正样本和负样本的数量由`-numPos`和`-numNeg`参数给出）。由于我们有许多可能的Haar特征，我们检查所有这些特征，并选择那些最能分类我们的样本集的特征。显然，这个非常基础的分类器会犯错误（即错误地分类几个样本）；这就是为什么我们需要构建多个这样的分类器。这些分类器是逐个添加的，每次都寻找提供最佳分类的新Haar特征。但是，由于在每次迭代中，我们希望专注于当前被错误分类的样本，因此分类性能是通过给予错误分类样本更高的权重来衡量的。因此，我们获得了一系列简单的分类器，然后通过这些弱分类器的加权求和（性能更好的分类器给予更高的权重）构建一个强分类器。按照这种方法，通过结合几百个简单特征，可以获得性能良好的强分类器。
- en: 'But in order to build a cascade of classifiers in which early rejection is
    a central mechanism, we do not want a strong classifier made of a large number
    of weak classifiers. Instead, we need to find very small classifiers that will
    use only a handful of Haar features in order to quickly reject the obvious negative
    samples while keeping all positive ones. In its classical form, AdaBoost aims
    at minimizing the total classification error by counting the number of false negatives
    (a positive sample classified as a negative one) and false positives (a negative
    sample classified as a positive one). In the present case, we need to have most,
    if not all, the positive samples correctly classified while minimizing the false
    positive rate. Fortunately, it is possible to modify AdaBoost such that true positives
    are rewarded more strongly. Consequently, when training each stage of a cascade,
    two criteria must be set: the minimum hit rate and the maximum false alarm rate;
    in `opencv_traincascade` these are specified using the `-minHitRate` (`0.995`
    default value) and `-maxFalseAlarmRate` (`0.5` default value) parameters. Haar
    features are added to the stage until the two performance criteria are met. The
    minimum hit rate must be set pretty high to make sure the positive instances will
    go through the next stage; remember that if a positive instance is rejected by
    a stage, then this error cannot be recovered. Therefore, to facilitate the generation
    of a classifier of low complexity, you should set the maximum false alarm rate
    relatively high. Otherwise, your stage will need many Haar features in order to
    meet the performance criteria, which contradicts the idea of early rejection by
    simple and quick to compute classifier stages.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为了构建一个早期拒绝作为核心机制的分类器级联，我们不想使用由大量弱分类器组成的一个强分类器。相反，我们需要找到非常小的分类器，这些分类器将仅使用少量Haar特征，以便快速拒绝明显的负样本，同时保留所有正样本。在其经典形式中，AdaBoost旨在通过计算错误否定（将正样本分类为负样本）和错误肯定（将负样本分类为正样本）的数量来最小化总的分类错误。在当前情况下，我们需要尽可能多，如果不是所有，的正样本被正确分类，同时最小化错误肯定率。幸运的是，可以通过修改AdaBoost来使真正的正样本得到更强的奖励。因此，在训练级联的每个阶段时，必须设置两个标准：最小命中率和最大误报率；在`opencv_traincascade`中，这些参数使用`-minHitRate`（默认值为`0.995`）和`-maxFalseAlarmRate`（默认值为`0.5`）参数指定。只有当两个性能标准得到满足时，Haar特征才会被添加到阶段。最小命中率必须设置得相当高，以确保正实例将进入下一阶段；记住，如果一个正实例被某个阶段拒绝，那么这个错误是无法恢复的。因此，为了便于生成低复杂度的分类器，你应该将最大误报率相对设置得较高。否则，你的阶段将需要许多Haar特征才能满足性能标准，这与简单快速计算分类器阶段的早期拒绝理念相矛盾。
- en: A good cascade will therefore be made of early stages with few features, the
    number of features per stage growing as you go up the cascade. In `opencv_traincascade`,
    the maximum number of features per stage is set using the `-maxWeakCount` (default
    is `100`) parameter and the number of stages is set using `-numStages` (default
    is `20`).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个好的级联将包含具有少量特征的前期阶段，每个阶段的特征数量随着级联的上升而增加。在`opencv_traincascade`中，每个阶段的最大特征数量使用`-maxWeakCount`（默认为`100`）参数设置，阶段数量使用`-numStages`（默认为`20`）参数设置。
- en: When the training of a new stage starts, then new negative samples must be collected.
    These are extracted from the provided background images. The difficulty here is
    to find negative samples that pass through all previous stages (that is, that
    are wrongly classified as positives). The more stages you have trained, the more
    difficult it will be to collect these negative samples. This is why it is important
    to provide the classifier with a large variety of background images. It will then
    be able to extract patches from these that are difficult to classify (because
    they resemble the positive samples). Note also that if at a given stage, the two
    performance criteria are met without adding any new features, then the cascade
    training is stopped at this point (you can use it as is, or re-train it by providing
    more difficult samples). Reciprocally, if the stage is unable to meet the performance
    criteria, the training will also be stopped; in this case you should retry a new
    training with easier performance criteria.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当新阶段的训练开始时，就必须收集新的负样本。这些样本是从提供的背景图像中提取的。这里的困难在于找到能够通过所有先前阶段（即被错误地分类为正样本）的负样本。你训练的阶段越多，收集这些负样本就越困难。这就是为什么提供大量背景图像给分类器很重要。它将能够从中提取出难以分类的片段（因为它们与正样本相似）。请注意，如果在某个阶段，两个性能标准都达到了，而没有添加任何新的特征，那么级联训练将在这一点停止（你可以直接使用它，或者通过提供更困难的样本重新训练它）。相反，如果阶段无法满足性能标准，训练也将停止；在这种情况下，你应该尝试使用更容易的性能标准进行新的训练。
- en: With a cascade made of `n` stages, it can easily be shown that the global performance
    of the classifier will be at least better than `minHitRate^n` and `maxFalseAlarmRate^n`.
    This is the result of each stage being built on top of the results of the previous
    cascade of stages. For example, if we consider the default values of `opencv_traincascade`,
    we expect our classifier to have an accuracy (hit rate) of `0.995^(20)` and a
    false alarm rate of `0.5^(20)`. This means that 90% of the positive instances
    will be correctly identified and 0.001% of negative samples will be wrongly classified
    as positive. Note that an important consequence of the fact that a fraction of
    the positive samples will be lost as we go up the cascade is that you always have
    to provide more positive samples than the specified number of samples to use in
    each stage. In the numerical example we just gave, we need `numPos` to be set
    at 90% of the number of available positive samples.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于由`n`个阶段组成的级联，可以很容易地证明分类器的全局性能至少会优于`minHitRate^n`和`maxFalseAlarmRate^n`。这是由于每个阶段都是建立在先前级联阶段的结果之上的结果。例如，如果我们考虑`opencv_traincascade`的默认值，我们预计我们的分类器将有一个准确率（命中率）为`0.995^(20)`和误报率为`0.5^(20)`。这意味着90%的正样本将被正确识别，0.001%的负样本将被错误地分类为正样本。请注意，由于我们在级联过程中会丢失一部分正样本，因此你总是需要提供比每个阶段指定的样本数量更多的正样本。在我们刚才给出的数值示例中，我们需要将`numPos`设置为可用正样本数量的90%。
- en: One important question is how many samples should be used for training? This
    is difficult to answer but, obviously, your positive sample set must be large
    enough to cover a wide range of possible appearances of your class instances.
    Your background images should also be relevant. In the case of our stop sign detector,
    we included urban images as stop sign are expected to be seen in that context.
    A usual rule of thumb is to have `numNeg= 2*numPos`, but this has be validated
    on your own dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的问题是应该使用多少个样本进行训练？这很难回答，但显然，你的正样本集必须足够大，以涵盖你类别实例可能出现的广泛范围。你的背景图像也应该是相关的。在我们的停车标志检测器的情况下，我们包括了城市图像，因为停车标志预计将在这种背景下被看到。一个常见的经验法则是`numNeg=
    2*numPos`，但这一点需要在你的数据集上进行验证。
- en: Finally, we explained in this recipe how to build a cascade of classifiers using
    Haar features. Such features can also be built using other features such as the
    Local Binary Patterns discussed in the previous recipes or the histograms of oriented
    gradient that will be presented in the next recipe. The `opencv_traincascade`
    has a `-featureType` parameter allowing selection of different feature types.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在这份食谱中解释了如何使用Haar特征构建分类器的级联。这样的特征也可以使用其他特征构建，例如在之前的食谱中讨论的局部二进制模式或将在下一食谱中介绍的梯度直方图。`opencv_traincascade`有一个`-featureType`参数，允许选择不同的特征类型。
- en: There's more...
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The OpenCV library proposes a number of pre-trained cascades that you can use
    to detect faces, facial features, people, and other things. You will find these
    cascades in the form of XML files in the data directory of the library source
    directory.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV库提供了一些预训练的级联，您可以使用这些级联来检测人脸、面部特征、人物和其他物体。您将在库源目录的数据目录中以XML文件的形式找到这些级联。
- en: Face detection with a Haar cascade
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Haar级联进行人脸检测
- en: 'The pre-trained models are ready to be used. All you have to do is to create
    an instance of the `cv::CascadeClassifier` class using the appropriate XML file:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的模型已准备好使用。您只需使用适当的XML文件创建`cv::CascadeClassifier`类的实例即可：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then to detect faces with Haar features, you proceed this way:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后要使用Haar特征检测人脸，您按以下步骤进行：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The same process can be repeated for an eye detector, and the following image
    is obtained:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的过程可以用于眼检测器，以下图像是得到的：
- en: '![Face detection with a Haar cascade](img/image_14_010.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![使用Haar级联进行人脸检测](img/image_14_010.jpg)'
- en: See also
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Describing and matching local intensity patterns* recipe in [Chapter 9](ch09.html
    "Chapter 9. Describing and Matching Interest Points") , *Describing and Matching
    Interest Points*, described the SURF descriptor which also uses Haar-like features
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.html "第9章. 描述和匹配兴趣点")的*《描述和匹配兴趣点》*中，“描述和匹配局部强度模式”食谱描述了SURF描述符，它也使用了Haar-like特征
- en: The article *Rapid object detection using a boosted cascade of simple features*
    by *P. Viola* and *M. Jones* in *Computer Vision and Pattern Recognition* conference,
    2001, is the classical paper that describes the cascade of boosted classifiers
    and the Haar features
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2001年，P. Viola和M. Jones在《计算机视觉和模式识别》会议上发表的*《使用简单特征快速检测物体》*一文是描述提升分类器级联和Haar特征的经典论文
- en: The article *A short introduction to boosting* by *Y. Freund* and *R.E. Schapire*
    in *Journal of Japanese Society for Artificial Intelligence*, 1999 describes the
    theoretical foundations of boosting
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1999年，Y. Freund和R.E. Schapire在《日本人工智能学会杂志》上发表的*《提升的简要介绍》*一文描述了提升的理论基础
- en: The article *Filtered Channel Features for Pedestrian Detection* by *S. Zhang*,
    *R. Benenson* and *B. Schiele* in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2015 presents features similar to Haar and that can produce highly
    accurate detections
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Zhang、R. Benenson和B. Schiele在2015年《IEEE计算机视觉和模式识别会议》上发表的*《用于行人检测的滤波通道特征》*一文提出了类似于Haar的特征，可以产生高度准确的检测
- en: Detecting objects and people with Support Vector Machines and histograms of
    oriented gradients
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机和方向梯度直方图检测物体和人物
- en: This recipe presents another machine learning method, the **Support Vector Machines**
    (**SVM**), which can produce accurate 2-class classifiers from training data.
    They have been largely used to solve many computer vision problems. This time,
    classification is solved by using a mathematical formulation that looks at the
    geometry of the problem in high-dimension spaces.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱介绍另一种机器学习方法，即**支持向量机**（**SVM**），它可以从训练数据中生成准确的二分类器。它们已被广泛用于解决许多计算机视觉问题。这次，分类是通过使用一个数学公式来解决的，该公式考虑了高维空间中问题的几何形状。
- en: In addition, we will also present a new image representation that is often used
    in conjunction with SVMs to produce robust object detectors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将介绍一种新的图像表示方法，该方法通常与SVMs一起使用，以生成鲁棒的物体检测器。
- en: Getting ready
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Images of objects are mainly characterized by their shape and textural content.
    This is the aspect that is captured by the **Histogram of Oriented Gradients**
    (**HOG**) representation. As its name indicates, this representation is based
    on building histograms from image gradients. In particular, because we are more
    interested by shapes and textures, it is the distribution of the gradient orientations
    that is analyzed. In addition, in order to take into consideration the spatial
    distribution of these gradients, multiple histograms are computed over a grid
    that divides the image into regions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 物体的图像主要以其形状和纹理内容为特征。这是由**方向梯度直方图**（**HOG**）表示所捕捉的方面。正如其名称所示，这种表示是基于从图像梯度构建直方图。特别是，因为我们更感兴趣的是形状和纹理，所以分析的是梯度方向分布。此外，为了考虑这些梯度的空间分布，在将图像划分为区域的网格上计算多个直方图。
- en: The first step in building a HOG representation is therefore to compute the
    gradient of an image. The image is then subdivided into small cells (for example,
    `8×8` pixels) and histograms of gradient orientations are built for each of these
    cells. The range of possible orientations must therefore be divided into bins.
    Most often, only the gradient orientations are considered but not their directions
    (these are called unsigned gradients). In this case, the range of possible orientations
    is from `0` to `180` degrees. A 9-bin histogram in this case would divide the
    possible orientations into intervals of `20` degrees. Each gradient vector in
    a cell contributes to a bin with a weight corresponding to the magnitude of this
    gradient.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，构建 HOG 表示的第一步是计算图像的梯度。然后，图像被细分为小的单元（例如，`8×8` 像素），并为这些单元中的每一个构建梯度方向的直方图。因此，必须将可能的取向范围划分为区间。通常，只考虑梯度方向，而不考虑它们的方向（这些被称为无符号梯度）。在这种情况下，可能的取向范围从
    `0` 到 `180` 度。在这种情况下，9 个区间的直方图会将可能的取向划分为 `20` 度的区间。每个单元中的梯度向量都会对与该梯度大小相对应的权重的一个区间做出贡献。
- en: The cells are then grouped into blocks. A block is then made of a certain number
    of cells. These blocks that cover the image can overlap each other (that is, they
    can share cells). For example, in the case where blocks are made of `2×2` cells,
    a new block can be defined every one cell; this would represent a block stride
    of `1` cell and each cell (except the last one in a row) would then contribute
    to `2` blocks. Conversely, with a block stride of `2` cells, the blocks would
    not overlap at all. A block contains a certain number of cell histograms (for
    example, `4` in the case of a block made of `2×2` cells). These histograms are
    simply concatenated together to form a long vector (for example, `4` histograms
    of `9` bins each then produce a vector of length `36`). To make the representation
    invariant to changes in contrast, this vector is then normalized (for example,
    each element is divided by the magnitude of the vector). Finally you also concatenate
    together all the vectors associated with all blocks of the image (row order) into
    a very large one (for example, in a `64×64` image, you will have a total of seven
    `16×16` blocks when a stride of `1` is applied on cells of size `8×8`; this represents
    a final vector of `49x36 = 1764` dimensions). This long vector is the HOG representation
    of the image.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些单元被分组到块中。一个块由一定数量的单元组成。这些覆盖图像的块可以相互重叠（即，它们可以共享单元）。例如，在块由 `2×2` 单元组成的情况下，每块单元可以定义一个新的块；这将代表
    `1` 单元的块步长，每个单元（除了行中的最后一个单元）将贡献 `2` 个块。相反，如果块步长为 `2` 单元，则块将完全不重叠。一个块包含一定数量的单元直方图（例如，一个由
    `2×2` 单元组成的块中有 `4` 个）。这些直方图被简单地连接起来形成一个长向量（例如，`4` 个每个有 `9` 个区间的直方图将产生一个长度为 `36`
    的向量）。为了使表示对对比度变化不变，这个向量随后被归一化（例如，每个元素被除以向量的幅度）。最后，你还将与图像中所有块的关联的所有向量（按行顺序）连接成一个非常大的向量（例如，在一个
    `64×64` 的图像中，当在 `8×8` 大小的单元上应用 `1` 的步长时，你将总共拥有七个 `16×16` 的块；这代表一个最终向量为 `49x36
    = 1764` 维）。这个长向量是图像的 HOG 表示。
- en: As you can see, the HOG of an image leads to a vector of very high dimension
    (see the *There's more...* section of this recipe that proposes a way to visualize
    a HOG representation). This vector characterizes the image and can then be used
    to classify images of different classes of objects. To achieve this goal, we therefore
    need a machine learning method that can handle vectors of very high dimension.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，图像的 HOG 导致一个非常高维的向量（请参阅本配方中 *还有更多...* 部分提出的可视化 HOG 表示的方法）。这个向量表征了图像，然后可以用来对属于不同类别对象的图像进行分类。为了实现这个目标，因此我们需要一个可以处理非常高维向量的机器学习方法。
- en: How to do it...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this recipe, we will build another stop sign classifier. This is obviously
    just a toy example that serves to illustrate the learning procedure. As we explained
    in the previous recipe, the first step is to collect samples for training. In
    our example, the set of positive samples that we will be using is the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将构建另一个停车标志分类器。这显然只是一个玩具示例，用于说明学习过程。正如我们在前面的配方中解释的那样，第一步是收集训练样本。在我们的例子中，我们将使用的正样本集如下：
- en: '![How to do it...](img/image_14_011.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/image_14_011.jpg)'
- en: 'And our (very small) set of negative samples is as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们（非常小）的负样本集如下：
- en: '![How to do it...](img/image_14_012.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/image_14_012.jpg)'
- en: 'We will now learn how to differentiate these two classes using SVM as implemented
    in the `cv::svm` class. To build a robust classifier, we will represent our class
    instances using HOG as described in the introductory section of this recipe. More
    precisely, we will use `8×8` blocks made of `2×2` cells with a block stride of
    `1` cell:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将学习如何使用`cv::svm`类中实现的SVM来区分这两个类别。为了构建一个健壮的分类器，我们将使用HOG来表示我们的类实例，正如在本食谱的介绍部分所描述的。更确切地说，我们将使用由`2×2`单元格组成的`8×8`块，块步长为`1`个单元格：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'With 9-bin histograms and `64×64` samples, this configuration produces HOG
    vectors (made of `225` blocks) of size `8100`. We compute this descriptor for
    each of our samples and transfer them into a single matrix (one HOG per row):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用9-箱直方图和`64×64`样本，这种配置产生大小为`8100`的HOG向量（由`225`个块组成）。我们为我们的每个样本计算这个描述符，并将它们转换成一个单独的矩阵（每行一个HOG）：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note how we computed the first HOG in order to obtain the size of the descriptor
    and then created the matrix of descriptors. A second matrix is then created to
    contain the labels associated to each sample. In our case, the first rows are
    the positive samples (and must be assigned a label of `1`), the reminder rows
    are the negative  samples (labeled `-1`):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是如何计算第一个HOG以获得描述符的大小，然后创建描述符矩阵的。然后创建第二个矩阵来包含与每个样本关联的标签。在我们的例子中，前几行是正样本（必须分配标签`1`），其余行是负样本（标签为`-1`）：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The next step is to build the SVM classifier that will be used for training;
    we also select the type of SVM and the kernel to be used (these parameters will
    be discussed in the next section):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建用于训练的SVM分类器；我们还选择了要使用的SVM类型和核函数（这些参数将在下一节中讨论）：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We are now ready for training. The labeled samples are first provided to the
    classifier and the `train` method is called:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备开始训练。首先将标记的样本提供给分类器，并调用`train`方法：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once the training phase completes, any sample of unknown class can be submitted
    to the classifier, which will try to predict the class to which it belongs (here
    we test four samples):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练阶段完成，任何未知类别的样本都可以提交给分类器，分类器将尝试预测它所属的类别（在这里我们测试了四个样本）：
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If the classifier has been trained with representative samples, then it should
    be able to correctly predict the label of a new instance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分类器已经用代表性样本进行了训练，那么它应该能够正确预测新实例的标签。
- en: How it works...
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In our stop sign recognition example, each instance of our class is represented
    by a point in an 8100-dimensional HOG space. It is obviously impossible to visualize
    such a large space but the idea behind support vector machines is to trace a boundary
    in that space that will segregate points that belongs to one class from points
    belonging to the other class. More specifically, this boundary will in fact be
    just a simple hyperplane. This idea is better explained considering a 2D space
    where each instance is represented as a 2D point. The hyperplane is, in this case,
    a simple line.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的停车标志识别示例中，我们类中的每个实例都由一个在8100维HOG空间中的点来表示。显然，无法可视化这样一个大的空间，但支持向量机背后的想法是在该空间中绘制一个边界，将属于一个类别的点与属于另一个类别的点分开。更具体地说，这个边界实际上将只是一个简单的超平面。这个想法最好通过考虑一个二维空间来解释，其中每个实例都表示为一个二维点。在这种情况下，超平面是一个简单的直线。
- en: '![How it works...](img/image_14_013.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/image_14_013.jpg)'
- en: This is obviously a trivial example but, conceptually, working in a two-dimensional
    space or in a 8100-dimensional space is the same thing. The preceding figure shows
    then, how a simple line can separate the points of the two classes well. In the
    case illustrated here, it can also be seen that many other lines could also achieve
    this perfect class separation. One question is therefore; which exact line one
    should choose. To answer this question, you must first realize that the samples
    we used to build our classifier constitute just a small snapshot of all possible
    instances that will need to be classified when the classifier is used in a target
    application. This means that we would like our classifier, not only to be able
    to correctly separate the provided sample sets but we also would like this one
    to make the best decision on the future instances shown to it. This concept is
    often referred to as the **generalization** power of a classifier. Intuitively,
    it would be reasonable to believe that our separating hyperplane should be located
    in between the two classes, not closer to one class than the other. More formally,
    SVMs propose setting the hyperplane at a position that maximizes the margin around
    the defined boundary. This **margin** is defined as the minimum distance between
    the separating hyperplane and the closest point in the positive sample set plus
    the distance between the hyperplane and the closest negative sample. The closest
    points (the ones that define the margin) are called the **support vectors**. The
    mathematics behind SVM defines an optimization function aiming at identifying
    these support vectors.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是一个简单的例子，但从概念上讲，在二维空间或8100维空间中工作是同一件事。前面的图显示了如何用一条简单的线很好地分离两个类的点。在本例中，还可以看到许多其他线也能实现这种完美的类分离。因此，一个问题就是；应该选择哪条确切的线。为了回答这个问题，你必须首先意识到，我们用来构建分类器的样本只是所有可能实例的一个小快照，当分类器在目标应用中使用时，需要对这些实例进行分类。这意味着我们希望我们的分类器不仅能够正确地分离提供的样本集，而且我们还希望这个分类器对它展示的未来实例做出最佳决策。这个概念通常被称为分类器的**泛化能力**。直观地，我们相信我们的分离超平面应该位于两个类之间，而不是比另一个类更接近。更正式地说，SVMs提出将超平面设置在最大化定义边界周围边界的位置。这个**边界**被定义为分离超平面与正样本集中最近点的最小距离加上超平面与最近负样本的距离。最近的点（定义边界的点）被称为**支持向量**。SVM背后的数学定义了一个优化函数，旨在识别这些支持向量。
- en: But the proposed solution to the classification problem cannot be that simple.
    What happens if the distribution of the sample points is as follows?
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，针对分类问题的提出的解决方案不可能那么简单。如果样本点的分布如下所示，会发生什么？
- en: '![How it works...](img/image_14_014.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/image_14_014.jpg)'
- en: 'In this case, a simple hyperplane (a line here) cannot achieve a proper separation.
    SVM solves this problem by introducing artificial variables that bring the problem
    into a higher dimensional space through some non-linear transformations. For example,
    in the example above, one might propose to add the distance to the origin as an
    additional variable, that is to compute *r= sqrt(x²+y²)* for each point. We now
    have a three-dimensional space; for simplicity, let''s just draw the points on
    the *(r,x)* plane:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个简单的超平面（这里是一条线）无法实现适当的分离。SVM通过引入人工变量，通过某些非线性变换将问题引入更高维的空间来解决此问题。例如，在上面的例子中，有人可能会提出添加到原点的距离作为一个额外的变量，即计算每个点的*r=
    sqrt(x²+y²)*。我们现在有一个三维空间；为了简单起见，我们只需在*(r,x)*平面上绘制这些点：
- en: '![How it works...](img/image_14_015.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/image_14_015.jpg)'
- en: As you can see, our set of sample points can now be separated by a simple hyperplane.
    This implies that you now have to find the support vectors in this new space.
    In fact, in the SVM formulation, you do not have to bring all the points into
    that new space, you just have to define a way to measure point-to-hyperplane distance
    in that augmented space. SVM therefore defines **kernel functions** that allow
    you to measure this distance in higher space without having to explicitly compute
    the point coordinates in that space. This is just a mathematical trick that explains
    why support vectors producing the maximal margin can be efficiently computed in
    very high (artificial) dimensional space. This also explains why, when you want
    to use support vector machines, you need to specify which kernel you want to use.
    This is by applying these kernels that you will make non-linearly separable to
    become separable in the kernel space.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的样本点现在可以通过一个简单的超平面分离。这意味着您现在必须在这个新空间中找到支持向量。实际上，在SVM公式中，您不需要将所有点都带入这个新空间，您只需要定义一种方法来测量这个增强空间中点到超平面的距离。因此，SVM定义了**核函数**，允许您在不显式计算该空间中点坐标的情况下测量这个距离。这只是一个数学技巧，解释了为什么产生最大间隔的支持向量可以在非常高的（人工）维空间中有效地计算。这也解释了为什么当您想使用支持向量机时，您需要指定要使用哪种核。正是通过应用这些核，您将使非线性不可分的数据在核空间中变得可分。
- en: One important remark here however. Since with Support Vector Machines we often
    work with features of very high dimension (for example, `8100` dimension in our
    HOG example), then it may very well happen that our samples will be separable
    with a simple hyperplane. This is why it still make sense to not use non-linear
    kernels (or more precisely to use a linear kernel, that is, `cv::ml::SVM::LINEAR`)
    and work in the original feature space. The resulting classifier will then be
    computationally simpler. But for more challenging classification problems, kernels
    remain a very effective tool. OpenCV offers you a number of standard kernels (for
    example, radial basis functions, sigmoid functions, and so on); the objective
    of these is to send the samples into a larger non-linear space that will make
    the classes separable by a hyperplane. SVM has a number of variants; the most
    common is the C-SVM, which adds a penalty for each outlier sample that does not
    lie on the right side of the hyperplane.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个重要的说明。由于在使用支持向量机时，我们经常处理非常高维的特征（例如，在我们的HOG示例中为`8100`维），因此我们的样本可能非常容易被一个简单的超平面分离。这就是为什么仍然有意义不使用非线性核（或者更精确地说，使用线性核，即`cv::ml::SVM::LINEAR`）并在原始特征空间中工作。这样得到的分类器在计算上会更简单。但对于更具挑战性的分类问题，核仍然是一个非常有效的工具。OpenCV为你提供了一系列标准核（例如，径向基函数、Sigmoid函数等）；这些核的目标是将样本发送到一个更大的非线性空间，使得类别可以通过超平面分离。支持向量机有多个变体；最常见的是C-SVM，它为每个不在超平面右侧的异常样本添加惩罚。
- en: Finally, we insist on the fact that, because of their strong mathematical foundations,
    SVMs work very well with features of very high dimension. In fact, they have been
    shown to operate best when the number of dimensions of the feature space is larger
    than the number of samples. They are also memory efficient, as they just have
    to store the support vectors (in contrast to a method like nearest-neighbor that
    requires keeping in memory all sample points).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们强调，由于它们强大的数学基础，支持向量机非常适合处理非常高维的特征。事实上，它们已经被证明在特征空间的维数大于样本数量时表现最佳。它们在内存效率上也很有优势，因为它们只需要存储支持向量（与需要保留所有样本点的最近邻方法相比）。
- en: There's more...
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Histograms of oriented gradients and SVM form a good combination for the construction
    of good classifiers. One of the reasons for this success is the fact that HOG
    can be viewed as a robust high-dimensional descriptor that captures the essential
    aspects of an object class. HOG-SVM classifiers have been used successfully in
    many applications; pedestrian detection is one of them.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 方向梯度直方图和SVM对于构建良好的分类器是一个很好的组合。这种成功的一个原因是HOG可以被看作是一个鲁棒的高维描述符，它捕捉了对象类的基本方面。HOG-SVM分类器已经在许多应用中成功使用；行人检测就是其中之一。
- en: Finally, since this is the last recipe of this book, we will therefore end it
    with a perspective on a recent trend in machine learning that is revolutionizing
    computer vision and artificial intelligence.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于这是本书的最后一个配方，因此我们将以机器学习领域最近趋势的视角来结束，这个趋势正在改变计算机视觉和人工智能。
- en: HOG visualization
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HOG可视化
- en: 'HOGs are built from cells combined in overlapping blocks. It is therefore difficult
    to visualize this descriptor. Nevertheless, they are often represented by displaying
    the histograms associated to each cell. In this case, instead of aligning the
    orientation bins in a regular bar graph, a histogram of orientation can be more
    intuitively drawn in a star-shape where each line has the orientation associated
    to the bin it represents and the length of the line is proportional to that bin
    count. These HOGs representations can then be displayed over an image:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: HOG是从重叠的块中组合的单元格构建的。因此，可视化这个描述符很困难。尽管如此，它们通常通过显示与每个单元格关联的直方图来表示。在这种情况下，与在常规条形图中对齐方向箱不同，方向直方图可以更直观地绘制成星形，其中每条线都与它所代表的箱子的方向相关联，线的长度与该箱子的计数成比例。这些HOG表示可以随后显示在图像上：
- en: '![HOG visualization](img/image_14_016.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![HOG可视化](img/image_14_016.jpg)'
- en: 'Each cell HOG representation can be produced by a simple function that accepts
    an iterator pointing to a histogram. Lines of proper orientation and length are
    then drawn for each `bin`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单元格的HOG表示可以通过一个简单的函数生成，该函数接受一个指向直方图的迭代器。然后为每个`bin`绘制适当方向和长度的线条：
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A HOG visualization function will then call this preceding function for each
    cell:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: HOG可视化函数将为每个单元格调用此先前的函数：
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This function computes a HOG descriptor having the specified cell size but made
    of only one large block (that is, a block having the size of the image). This
    representation therefore ignores the effect of normalization that occurs at each
    block level.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数计算具有指定单元格大小的HOG描述符，但由一个大型块（即具有图像大小的块）组成。因此，这种表示忽略了在每个块级别发生的归一化效果。
- en: People detection
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人体检测
- en: 'OpenCV offers a pre-trained people detector based on HOG and SVM. As for the
    classifier cascades of the previous recipe, this SVM classifier can be used to
    detect instances in a full image by scanning a window across the image, at multiple
    scales. You then just have to construct the classifier and perform the detection
    on an image:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV提供了一个基于HOG和SVM的预训练人体检测器。至于之前菜谱中的分类器级联，这个SVM分类器可以通过在图像上扫描多个尺度的窗口来检测整个图像中的实例。然后你只需构建分类器并在图像上执行检测：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The window stride defines how the `128×64` template is moved over the image
    (every `4` pixels horizontally and vertically in our example). Longer strides
    make the detection faster (because less windows are evaluated) but you may then
    miss some people falling in between tested windows. The image padding parameter
    simply adds pixels on the border of the image such that people at the edge of
    the image can be detected. The standard threshold for an SVM classifier is `0`
    (since `1` is the value assigned to positive instances and `-1` to the negative
    ones). But if you really want to be certain that what you detect is a person,
    then you can raise this threshold value (this means that you want **high precision**
    at the price of missing some people in the image). Reciprocally, if you want to
    be certain of detecting all people (that is you want a **high recall** rate),
    then you can lower the threshold; more false detections will occur in that case.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口步长定义了`128×64`模板在图像上移动的方式（在我们的例子中，水平垂直方向上每`4`个像素）。较长的步长会使检测更快（因为评估的窗口更少），但你可能会错过一些落在测试窗口之间的人。图像填充参数简单地在图像的边缘添加像素，以便检测图像边缘的人。SVM分类器的标准阈值是`0`（因为`1`是分配给正实例的值，`-1`是分配给负实例的值）。但如果你真的想确保你检测到的是人，那么你可以提高这个阈值值（这意味着你希望以牺牲图像中一些人为代价来获得**高精度**）。相反，如果你想确保检测到所有人（即你希望有一个**高召回率**），那么你可以降低阈值；在这种情况下，将会发生更多的误检。
- en: 'Here is an example of the detection results obtained:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个检测结果的示例：
- en: '![People detection](img/image_14_017.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![人体检测](img/image_14_017.jpg)'
- en: 'It is important to note that when a classifier is applied to a full image,
    the multiple windows applied at successive locations will often lead to multiple
    detections around a positive sample. The best thing to do when two or more bounding
    boxes overlap at about the same location is to retain only one of them. There
    is a function called `cv::groupRectangles` that simply combines rectangles of
    similar size at similar locations (this function is automatically called by `detectMultiScale`).
    In fact, obtaining a group of detections at a particular location can even be
    seen as an indicator confirming that we indeed have a positive instance at this
    location. This is why the `cv::groupRectangles` function allows us to specify
    the minimum size for a detection cluster to be accepted as a positive detection
    (that is, isolated detection should be discarded). This is the last parameter
    of the `detectMultiScale` method. Setting this one at `0` will keep all detections
    (no grouping done) which, in our example, leads to the following result:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，当分类器应用于整个图像时，在连续位置应用的多窗口通常会围绕正样本产生多个检测。当两个或更多边界框在大约相同的位置重叠时，最好的做法是只保留其中一个。有一个名为
    `cv::groupRectangles` 的函数，它简单地合并相似位置和相似大小的矩形（此函数由 `detectMultiScale` 自动调用）。实际上，在特定位置获得一组检测甚至可以被视为一个指标，确认我们确实在这个位置有一个正实例。这就是为什么
    `cv::groupRectangles` 函数允许我们指定一个检测簇的最小尺寸，以便将其接受为正检测（即孤立检测应被丢弃）。这是 `detectMultiScale`
    方法的最后一个参数。将其设置为 `0` 将保留所有检测（不进行分组），在我们的例子中，这导致了以下结果：
- en: '![People detection](img/image_14_018.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![人物检测](img/image_14_018.jpg)'
- en: Deep learning and Convolutional Neural Networks
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习和卷积神经网络
- en: We cannot conclude this chapter on machine learning without mentioning deep
    convolutional neural networks. The application of these to computer vision classification
    problems has led to impressive results. In fact, their outstanding performance
    when applied to real-world problems is such that they now open the door to a new
    family of applications that could not be envisioned before.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在介绍机器学习这一章时，不能不提及深度卷积神经网络。这些网络在计算机视觉分类问题中的应用已经取得了令人印象深刻的成果。实际上，它们在解决现实世界问题时表现出的卓越性能，以至于现在为之前无法想象的新一代应用打开了大门。
- en: 'Deep learning is based on the theory of neural networks that was introduced
    in the late 1950s. So why are they generating such great interest today? Basically
    for two reasons: first, the computational power that is available nowadays allows
    deploying neural networks of a size that makes them able to solve challenging
    problems. While the first neural network (the perceptron) has only one layer and
    few weight parameters to tune, today''s networks can have hundreds of layers and
    millions of parameters to be optimized (hence the name deep networks). Second,
    the large amount of data available today makes their training possible. In order
    to perform well, deep networks, indeed, required thousands, if not millions, of
    annotated samples (this is required because of the very large number of parameters
    that need to be optimized).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习基于20世纪50年代末引入的神经网络理论。那么，为什么它们今天会如此受到关注呢？基本上有两个原因：首先，现在的计算能力允许部署能够解决挑战性问题的神经网络。虽然第一个神经网络（感知器）只有一个层和很少的权重参数需要调整，但今天的网络可以有数百层和数百万个参数需要优化（因此得名深度网络）。其次，今天可用的海量数据使得它们的训练成为可能。为了表现良好，深度网络确实需要数千甚至数百万个标注样本（这是由于需要优化的参数数量非常庞大）。
- en: The most popular deep networks are the **Convolutional Neural Networks** (**CNN**).
    As the name suggests, they are based on convolution operations (see [Chapter 6](ch06.html
    "Chapter 6. Filtering the Images") , *Filtering the Images*). The parameters to
    learn, in this case, are therefore the values inside the kernel of all filters
    that compose the network. These filters are organized into layers, in which the
    early layers extract the fundamental shapes such as lines and corners while the
    higher layers progressively detect more complex patterns (such as, for example,
    the presence of eyes, mouth, hair, in a human detector).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的深度网络是**卷积神经网络**（**CNN**）。正如其名所示，它们基于卷积操作（参见[第6章](ch06.html "第6章. 过滤图像")，*过滤图像*）。在这种情况下，要学习的参数是所有组成网络的滤波器内核中的值。这些滤波器被组织成层，其中早期层提取基本形状，如线条和角，而高层则逐步检测更复杂的模式（例如，在人类检测器中，例如眼睛、嘴巴、头发）。
- en: OpenCV3 has a **Deep Neural Network** module, but this one is mainly for importing
    deep networks trained using other tools such as TensorFlow, Caffe, or Torch. When
    building your future computer vision applications, you will certainly have to
    have a look at the deep learning theory and its related tools.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV3 拥有一个 **深度神经网络** 模块，但这个模块主要用于导入使用 TensorFlow、Caffe 或 Torch 等其他工具训练的深度网络。当构建你未来的计算机视觉应用时，你肯定会需要查看深度学习理论及其相关工具。
- en: See also
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Describing and matching local intensity patterns* recipe in [Chapter 9](ch09.html
    "Chapter 9. Describing and Matching Interest Points") , *Describing and Matching
    Interest Points*, described the SIFT descriptor which is similar to the HOG descriptor
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *Describing and Matching Interest Points* 的第 9 章 [“描述和匹配兴趣点”](ch09.html "Chapter 9. Describing
    and Matching Interest Points") 中，*Describing and matching local intensity patterns*
    菜单描述了与 HOG 描述符相似的 SIFT 描述符
- en: The article *Histograms of Oriented Gradients for Human Detection* by N. Dalal
    and B. Triggs in *Computer Vision and Pattern Recognition* conference, 2005 is
    the classical paper that introduces histograms of oriented gradients for people
    detection
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N. Dalal 和 B. Triggs 在 2005 年 *Computer Vision and Pattern Recognition* 会议上的文章
    *Histograms of Oriented Gradients for Human Detection* 是介绍用于人体检测的梯度直方图的经典论文
- en: The article *Deep Learning* by Y. LeCun, Y. Bengio and G. Hinton in *Nature,
    no 521*, 2015, is a good starting point for exploring the world of deep learning
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. LeCun、Y. Bengio 和 G. Hinton 在 *Nature, no 521*，2015 年发表的 *Deep Learning*
    文章是探索深度学习世界的良好起点
