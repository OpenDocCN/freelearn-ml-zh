- en: Chapter 14. Learning from Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing faces using nearest neighbors of local binary patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding objects and faces with a cascade of Haar features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting objects and people with Support Vector Machines and histograms of
    oriented gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is nowadays, very often used to solve difficult machine vision
    problems. In fact, it is a rich field of research encompassing many important
    concepts that would deserve a complete cookbook by itself. This chapter surveys
    some of the main machine learning techniques and explains how these can be deployed
    in computer vision systems using OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of machine learning is the development of computer systems that
    can learn how to react to data inputs by themselves. Instead of being explicitly
    programmed, machine learning systems automatically adapt and evolve when examples
    of desired behaviors are presented to them. Once a successful training phase is
    completed, it is expected that the trained system will output the correct response
    to new unseen queries.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning can solve many types of problems; our focus here will be on
    classification problems. Formally, in order to build a classifier that can recognize
    instances of a specific class of concepts, this one must be trained with a large
    set of annotated samples. In a 2-class problem, this set will be made of **positive
    samples** representing instances of the class to be learned, and of **negative
    samples** made of counter-examples of instances not belonging to the class of
    interest. From these observations, a **decision function** predicting the correct
    class of any input instances has to be learned.
  prefs: []
  type: TYPE_NORMAL
- en: In computer vision, those samples are images (or video segments). The first
    thing to do is therefore find a representation that will ideally describe the
    content of each image in a compact and distinctive way. One simplistic representation
    could be to use a fixed-size thumbnail image. The row-by-row succession of the
    pixels of this thumbnail image forms a vector that can then be used as a training
    sample presented to a machine learning algorithm. Other alternative and probably
    more effective representations can also be used. The recipes of this chapter describe
    different image representations and introduce some well-known machine learning
    algorithms. We should emphasize that we will not be able to cover in detail, all
    the theoretical aspects of the different machine learning techniques discussed
    in the recipes; our objective is rather to present the main principles governing
    their functioning.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing faces using nearest neighbors of local binary patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first exploration of machine learning techniques will start with what is
    probably the simplest approach, namely **nearest neighbor classification**. We
    will also present the local binary pattern feature, a popular representation encoding
    the textural patterns and contours of an image in a contrast independent way.
  prefs: []
  type: TYPE_NORMAL
- en: Our illustrative example will concern the face recognition problem. This is
    a very challenging problem that has been the object of numerous researches over
    the past 20 years. The basic solution we present here is one of the face recognition
    methods implemented in OpenCV. You will quickly realize that this solution is
    not very robust and works only under very favorable conditions. Nevertheless,
    this approach constitutes an excellent introduction to machine learning and to
    the face recognition problem.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The OpenCV library proposes a number of face recognition methods implemented
    as a subclass of the generic `cv::face::FaceRecognizer`. In this recipe, we will
    have a look at the `cv::face::LBPHFaceRecognizer` class, which is interesting
    to us because it is based on a simple but often very effective classification
    approach, the nearest neighbor classifier. Moreover, the image representation
    it uses is built from the **local binary pattern** feature (**LBP**) which is
    a very popular way of describing image patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create an instance of the `cv::face::LBPHFaceRecognizer`, its static
    `create` method is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As will be explained in the next section, the first two arguments provided
    serve to describe the characteristic of the LBP feature to be used. The next step
    is to feed the recognizer with a number of reference face images. This is done
    by providing two vectors, one containing the face images and the other one containing
    the associated labels. Each label is an arbitrary integer value identifying a
    particular individual. The idea is to train the recognizer by showing it different
    images of each of the people to be recognized. As you may imagine, the more representative
    images you provide, the better the chances that the correct person will be identified.
    In our very simplistic example, we simply provide two images of two reference
    persons. The `train` method is the one to call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The images used are below, with the top row being images of person `0` and
    the second row images of person `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_14_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The quality of these reference images is also very important. In addition,
    it would be a good idea to have them normalized such as to have the main facial
    features at standardized locations. For example, having the tip of the nose located
    in the middle of the image, and the two eyes horizontally aligned at a specific
    image row. Facial feature detection methods exist that can be used to automatically
    normalize face images this way. This was not done in our example, and the robustness
    of the recognizer will suffer from this. Nevertheless, this one is ready to be
    used, an input image can be provided, and it will try to predict the label to
    which this face image corresponds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Our input image is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_14_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Not only does the recognizer return the predicted label, but it also returns
    a confidence score. In the case of the `cv::face::LBPHFaceRecognizer`, the lower
    this confidence value is, the more confident is the recognizer of its prediction.
    Here, we obtain a correct label prediction (`1`) with a confidence value of `90.3`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to understand the functioning of the face recognition approach presented
    in this recipe, we need to explain its two main components: the image representation
    used and the classification method that is applied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As its name indicates, the `cv::face::LBPHFaceRecognizer` algorithm makes use
    of the LBP feature. This is a contrast independent way of describing image patterns
    present in an image. It is a local representation that transforms every pixel
    into a binary representation encoding the pattern of image intensities found in
    a neighborhood. To achieve this goal, a simple rule is applied; a local pixel
    is compared to each of its selected neighbors; if its value is greater than that
    of its neighbor, then a `0` is assigned to the corresponding bit position, if
    not, then a `1` is assigned. In its simplest and most common form, each pixel
    is compared to its `8` immediate neighbors, which generates an 8-bit pattern.
    For example, let''s consider the following local pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/B05388_14_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Applying the described rule generates the following binary values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/B05388_14_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Taking as initial position, the top left pixel and moving clockwise, the central
    pixel will be replaced by the binary sequence `11011000`. Generating a complete
    8-bit LBP image is then easily achieved by looping over all pixels of an image
    to produce all corresponding LBP bytes. This is accomplished by the following
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The body of the loop compares each pixel with its `8` neighbors and the bit
    values are assigned through simple bit shifts. With the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_14_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'An LBP image is obtained and can be displayed as a gray-level image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_14_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This gray-level representation is not really interpretable, but it simply illustrates
    the encoding process that occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to our `cv::face::LBPHFaceRecognizer` class, it can be seen that the
    first two parameters of its `create` method specify the size (radius in pixels)
    and dimension (number of pixels along the circle, possibly applying interpolation)
    of the neighborhood to be considered. Once the LBP image is generated, the image
    is divided into a grid. The size of this grid is specified as the third parameter
    of the `create` method. For each block of this grid, a histogram of LBP values
    is constructed. A global image representation is finally obtained by concatenating
    the bin counts of all these histograms into one large vector. With an `8×8` grid,
    the set of computed 256-bin histograms then forms a 16384-dimensional vector.
  prefs: []
  type: TYPE_NORMAL
- en: The `train` method of the `cv::face::LBPHFaceRecognizer` class therefore generates
    this long vector for each of the provided reference images. Each face image can
    then be seen as a point in a very high dimensional space. When a new image is
    submitted to the recognizer through its `predict` method, the closest reference
    point to this image is found. The label associated with this point is therefore
    the predicted label and the confidence value will be the computed distance. This
    is the principle that defines a nearest neighbor classifier. One more ingredient
    is generally added. If the nearest neighbor of the input point is too far from
    it, then this could mean that this point in fact does not belong to any of the
    reference classes. How far away must this point be to be considered as an outlier?
    This is specified by the fourth parameter of the `create` method of the `cv::face::LBPHFaceRecognizer`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this is a very simple idea and it turns out to be very effective
    when the different classes generate distinct clouds of points in the representational
    space. Another benefit of this approach is that the method implicitly handles
    multiple classes, as it simply reads the predicted class from its nearest neighbors.
    The main drawback is its computational cost. Finding the nearest neighbor in such
    a large space, possibly composed of many reference points, can take time. Storing
    all these reference points is also costly in memory.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The article by *T. Ahonen*, *A. Hadid* and *M. Pietikainen*, *Face description
    with Local Binary Patterns: Application to Face Recognition* in IEEE transaction
    on *Pattern Analysis and Machine Intelligence*, 2006 describes the use of LBP
    for face recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article by *B. Froba* and *A. Ernst*, *Face detection with the modified
    census transform* in IEEE conference on*Automatic Face and Gesture Recognition*,
    2004 proposes a variant of the LBP features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article by *M. Uricar*, *V. Franc* and *V. Hlavac*, *Detector of Facial
    Landmarks Learned by the Structured Output SVM* in International Conference on
    *Computer Vision Theory and Applications*, 2012 describes a facial feature detector
    based on the SVMs discussed in the last recipe of this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding objects and faces with a cascade of Haar features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned in the previous recipe, some of the basic concepts of machine learning.
    We showed how a classifier can be built by collecting samples of the different
    classes of interest. However, for the approach that was considered in this previous
    recipe, training a classifier simply consists of storing all the samples' representations.
    From there, the label of any new instance can be predicted by looking at the closest
    (nearest neighbor) labeled point. For most machine learning methods, training
    is rather an iterative process during which machinery is built by looping over
    the samples. Performance of the classifier thus produced gradually improves as
    more samples are presented. Learning eventually stops when a certain performance
    criterion is reached or when no more improvements can be obtained by considering
    the current training dataset. This recipe will present a machine learning algorithm
    that follows this procedure, the **cascade of boosted classifiers**.
  prefs: []
  type: TYPE_NORMAL
- en: But before we look at this classifier, we will first turn our attention to the
    Haar feature image representation. We indeed learned that a good representation
    is an essential ingredient in the production of a robust classifier. LBPs, as
    described in the previous recipe, *Recognizing faces using nearest neighbors of
    local binary patterns*, constitute one possible choice; the next section describes
    another popular representation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in the generation of a classifier is to assemble a (preferably)
    large collection of image samples showing different instances of the classes of
    objects to be identified. The way these samples are represented has been shown
    to have an important impact on the performance of the classifier that is to be
    built from them. Pixel-level representations are generally considered to be too
    low-level to robustly describe the intrinsic characteristics of each class of
    objects. Representations that can describe, at various scales, the distinctive
    patterns present in an image are preferable. This is the objective of the **Haar
    features** also sometimes called Haar-like features because they derive from the
    Haar transform basis functions.
  prefs: []
  type: TYPE_NORMAL
- en: The Haar features define small rectangular areas of pixels, these later being
    compared through simple subtractions. Three different configurations are generally
    considered, namely the 2-rectangle, 3-rectangle, and 4-rectangle features
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/image_14_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These features can be of any size and applied on any area of the image to be
    represented. For example, here are two Haar features applied on a face image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/image_14_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Building a Haar representation consists of selecting a number of Haar features
    of given types, sizes, and locations and applying them on images. The specific
    set of values obtained from the chosen set of Haar features constitutes the image
    representation. The challenge is then to determine which set of features to select.
    Indeed, to distinguish one class of objects from another, some Haar features must
    be more relevant than others. For example, in the case of the class of face images,
    applying a 3-rectangle Haar feature between the eyes (as shown in the figure above)
    could be a good idea as we expect all face images to consistently produce a high
    value in this case. Obviously, since there exist hundreds of thousands of possible
    Haar features, it would certainly be difficult to manually make a good selection.
    We are then looking for a machine learning method that would select the most relevant
    features for a given class of objects.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will learn how we can build, using OpenCV, a **boosted cascade
    of features** to produce a 2-class classifier. But before we do, let's explain
    the terminology that is used here. A 2-class classifier is one that can identify
    the instances of one class (for example, face images) from the rest (for example,
    images that do not contain faces). We therefore have in this case the **positive
    samples** (that is, face images) and the **negative samples** (that is, non-face
    images), these latter are also called the background images. The classifier of
    this recipe will be made of a cascade of simple classifiers that will be sequentially
    applied. Each stage of the cascade will make a quick decision about rejecting
    or not rejecting the object shown based on the values obtained for a small subset
    of features. This cascade is boosted in the sense that each stage improves (boosts)
    the performance of the previous ones by making more accurate decisions. The main
    advantage of this approach is that the early stages of the cascade are composed
    of simple tests that can then quickly reject instances that certainly do not belong
    to the class of interest. These early rejections make the cascade classifier quick,
    because when searching for a class of objects by scanning an image, most sub-windows
    to be tested will not belong to the class of interest. This way, only few windows
    will have to pass through all stages before being accepted or rejected.
  prefs: []
  type: TYPE_NORMAL
- en: In order to train a boosted classifier cascade for a specific class, OpenCV
    offers a software tool that will perform all the required operations. When you
    install the library, you should have two executable modules created and located
    in the appropriate `bin` directory, these are `opencv_createsamples.exe` and `opencv_traincascade.exe`.
    Make sure your system `PATH` points to this directory so that you can execute
    these tools from anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'When training a classifier, the first thing to do is to collect the samples.
    The positive ones are made of images showing instances of the target class. In
    our simple example, we decided to train a classifier to recognize stop signs.
    Here are the few positive samples we have collected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_14_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The list of the positive samples to be used must be specified in a text file
    that we have, here, named `stop.txt`. It contains image filenames and bounding
    box coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The first number after the filename is the number of positive samples visible
    in the image. Next is the upper left coordinate of the bounding box containing
    this positive sample and finally its width and height. In our case, the positive
    samples have already been extracted from their original images, this is why we
    have always one sample per file and upper-left coordinates at `(0,0)`. Once this
    file is available, you can then create the positive sample file by running the
    extractor tool.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will create an output file `stop.vec` that will contain all the positive
    samples specified in the input text file. Note that we made the sample size smaller
    (`24×24`) than the original size (`64×64`). The extractor tool resizes all samples
    to the specified size. Usually, Haar features work better with smaller templates,
    but this is something that has to be validated on a case-by-case basis.
  prefs: []
  type: TYPE_NORMAL
- en: The negative samples are simply background images containing no instances of
    the class of interest (no stop signs in our case). But these images should show
    a good variety of what the classifier is expected to see. These negative images
    could be of any size, the training tool will extract random negative samples from
    them. Here is one example of a background image we wish to use.
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_14_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the positive and negative sample sets are in place, the classifier cascade
    is ready to be trained. Calling the tool is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The parameters used here will be explained in the next section. Note that this
    training process can take a very long time; in some complex cases with thousands
    of samples, it can even take days to execute. As it runs, the cascade trainer
    will print out performance reports each time the training of a stage is completed.
    In particular, the classifier will tell you what the current **hit rate** (**HR**)
    is; this is the percentage of positive samples that are currently accepted by
    the cascade (that is, correctly recognized as positive instances, they are also
    called the **true positives**). You want this number to be as close as possible
    to `1.0`. It will also give you the current **false alarm rate** (**FA**) which
    is the number of tested negative samples that are wrongly classified as positive
    instances (also called the **false positives**). You want this number to be as
    close as possible to `0.0`. These numbers are reported for each of the features
    introduced in each stage.
  prefs: []
  type: TYPE_NORMAL
- en: Our simple example took only few seconds. The structure of the classifier produced
    is described in an XML file that results from the training phase. The classifier
    is then ready to be used! You can submit any sample to it and it will tell you
    if it thinks that it is a positive or a negative one.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we trained our classifier with `24×24` images but in general,
    what you want is to find out if there are any instances of your class of objects
    somewhere in an image (of any size). To achieve this objective, you simply have
    to scan the input image and extract all possible windows of the sample size. If
    your classifier is accurate enough, only the windows that contain the seek objects
    will return a positive detection. But this works as long as the visible positive
    samples have the appropriate size. To detect instances at multiple scales, you
    then have to build a pyramid of images by reducing the size of the original image
    by a certain factor at each level of the pyramid. This way, bigger objects will
    eventually fit the trained sample size as we go down the pyramid. This is a long
    process, but the good news is that OpenCV provides a class that implements this
    process. Its use is pretty straightforward. First you construct the classifier
    by loading the appropriate XML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you call the `detection` method with an input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is provided as a vector of `cv::Rect` instances. To visualize the
    detection results, you just have to draw these rectangles on your input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When our classifier is tested on an image, here is the result we obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_14_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section we explained how it is possible to build an OpenCV cascade
    of classifiers using positive and negative samples of a class of objects. We will
    now overview the basic steps of the learning algorithm used to train this cascade.
    Our cascade has been trained using the Haar features that were described in the
    introductory section of this recipe but, as we will see, any other simple feature
    can be used to build a boosted cascade. As the theory and principles of boosted
    learning are pretty complex, we will not cover all aspects in this recipe; interested
    readers should refer to the articles listed in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first restate that there are two core ideas behind the cascade of boosted
    classifiers. The first one is that a strong classifier can be built by combining
    together several weak classifiers (that is, those based on simple features). Secondly,
    because in machine vision, negative instances are found much more frequently than
    the positive ones, effective classification can be performed in stages. The early
    stages make quick rejection of obvious negative instances, and more refined decisions
    can be made at later stages for more difficult samples. Based on these two ideas,
    we now describe the boosted cascade learning algorithm. Our explanations are based
    on the variant of boosting called **AdaBoost** **,** which is the one most often
    used. Our description will also allow us to explain some of the parameters used
    in the `opencv_traincascade` tool.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we use the Haar features in order to build our weak classifier.
    When one Haar feature is applied (of given type, size, and location), a value
    is obtained. A simple classifier is then obtained by finding the threshold value
    that would best classify the negative and positive class instances based on this
    feature value. To find this optimal threshold, we have at our disposal, a number
    of positive and negative samples (the number of positive and negative samples
    to be used at this step by `opencv_traincascade` is given by the `-numPos` and
    `-numNeg` parameters). Since we have a large number of possible Haar features,
    we examine all of them and select the one that best classifies our sample set.
    Obviously, this very basic classifier will make errors (that is, misclassify several
    samples); this is why we need to build several of these classifiers. These classifiers
    are added iteratively, each time searching for the new Haar feature giving the
    best classification. But since, at each iteration, we want to focus on the samples
    that are currently misclassified, the classification performance is measured by
    giving a higher weight to the misclassified samples. A set of simple classifiers
    is thus obtained and a strong classifier is then built from a weighted sum of
    these weak classifiers (classifiers with better performance being given a higher
    weight). Following this approach, a strong classifier with good performance can
    be obtained by combining a few hundred simple features.
  prefs: []
  type: TYPE_NORMAL
- en: 'But in order to build a cascade of classifiers in which early rejection is
    a central mechanism, we do not want a strong classifier made of a large number
    of weak classifiers. Instead, we need to find very small classifiers that will
    use only a handful of Haar features in order to quickly reject the obvious negative
    samples while keeping all positive ones. In its classical form, AdaBoost aims
    at minimizing the total classification error by counting the number of false negatives
    (a positive sample classified as a negative one) and false positives (a negative
    sample classified as a positive one). In the present case, we need to have most,
    if not all, the positive samples correctly classified while minimizing the false
    positive rate. Fortunately, it is possible to modify AdaBoost such that true positives
    are rewarded more strongly. Consequently, when training each stage of a cascade,
    two criteria must be set: the minimum hit rate and the maximum false alarm rate;
    in `opencv_traincascade` these are specified using the `-minHitRate` (`0.995`
    default value) and `-maxFalseAlarmRate` (`0.5` default value) parameters. Haar
    features are added to the stage until the two performance criteria are met. The
    minimum hit rate must be set pretty high to make sure the positive instances will
    go through the next stage; remember that if a positive instance is rejected by
    a stage, then this error cannot be recovered. Therefore, to facilitate the generation
    of a classifier of low complexity, you should set the maximum false alarm rate
    relatively high. Otherwise, your stage will need many Haar features in order to
    meet the performance criteria, which contradicts the idea of early rejection by
    simple and quick to compute classifier stages.'
  prefs: []
  type: TYPE_NORMAL
- en: A good cascade will therefore be made of early stages with few features, the
    number of features per stage growing as you go up the cascade. In `opencv_traincascade`,
    the maximum number of features per stage is set using the `-maxWeakCount` (default
    is `100`) parameter and the number of stages is set using `-numStages` (default
    is `20`).
  prefs: []
  type: TYPE_NORMAL
- en: When the training of a new stage starts, then new negative samples must be collected.
    These are extracted from the provided background images. The difficulty here is
    to find negative samples that pass through all previous stages (that is, that
    are wrongly classified as positives). The more stages you have trained, the more
    difficult it will be to collect these negative samples. This is why it is important
    to provide the classifier with a large variety of background images. It will then
    be able to extract patches from these that are difficult to classify (because
    they resemble the positive samples). Note also that if at a given stage, the two
    performance criteria are met without adding any new features, then the cascade
    training is stopped at this point (you can use it as is, or re-train it by providing
    more difficult samples). Reciprocally, if the stage is unable to meet the performance
    criteria, the training will also be stopped; in this case you should retry a new
    training with easier performance criteria.
  prefs: []
  type: TYPE_NORMAL
- en: With a cascade made of `n` stages, it can easily be shown that the global performance
    of the classifier will be at least better than `minHitRate^n` and `maxFalseAlarmRate^n`.
    This is the result of each stage being built on top of the results of the previous
    cascade of stages. For example, if we consider the default values of `opencv_traincascade`,
    we expect our classifier to have an accuracy (hit rate) of `0.995^(20)` and a
    false alarm rate of `0.5^(20)`. This means that 90% of the positive instances
    will be correctly identified and 0.001% of negative samples will be wrongly classified
    as positive. Note that an important consequence of the fact that a fraction of
    the positive samples will be lost as we go up the cascade is that you always have
    to provide more positive samples than the specified number of samples to use in
    each stage. In the numerical example we just gave, we need `numPos` to be set
    at 90% of the number of available positive samples.
  prefs: []
  type: TYPE_NORMAL
- en: One important question is how many samples should be used for training? This
    is difficult to answer but, obviously, your positive sample set must be large
    enough to cover a wide range of possible appearances of your class instances.
    Your background images should also be relevant. In the case of our stop sign detector,
    we included urban images as stop sign are expected to be seen in that context.
    A usual rule of thumb is to have `numNeg= 2*numPos`, but this has be validated
    on your own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we explained in this recipe how to build a cascade of classifiers using
    Haar features. Such features can also be built using other features such as the
    Local Binary Patterns discussed in the previous recipes or the histograms of oriented
    gradient that will be presented in the next recipe. The `opencv_traincascade`
    has a `-featureType` parameter allowing selection of different feature types.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The OpenCV library proposes a number of pre-trained cascades that you can use
    to detect faces, facial features, people, and other things. You will find these
    cascades in the form of XML files in the data directory of the library source
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection with a Haar cascade
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The pre-trained models are ready to be used. All you have to do is to create
    an instance of the `cv::CascadeClassifier` class using the appropriate XML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then to detect faces with Haar features, you proceed this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The same process can be repeated for an eye detector, and the following image
    is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Face detection with a Haar cascade](img/image_14_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Describing and matching local intensity patterns* recipe in [Chapter 9](ch09.html
    "Chapter 9. Describing and Matching Interest Points") , *Describing and Matching
    Interest Points*, described the SURF descriptor which also uses Haar-like features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article *Rapid object detection using a boosted cascade of simple features*
    by *P. Viola* and *M. Jones* in *Computer Vision and Pattern Recognition* conference,
    2001, is the classical paper that describes the cascade of boosted classifiers
    and the Haar features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article *A short introduction to boosting* by *Y. Freund* and *R.E. Schapire*
    in *Journal of Japanese Society for Artificial Intelligence*, 1999 describes the
    theoretical foundations of boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article *Filtered Channel Features for Pedestrian Detection* by *S. Zhang*,
    *R. Benenson* and *B. Schiele* in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2015 presents features similar to Haar and that can produce highly
    accurate detections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting objects and people with Support Vector Machines and histograms of
    oriented gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe presents another machine learning method, the **Support Vector Machines**
    (**SVM**), which can produce accurate 2-class classifiers from training data.
    They have been largely used to solve many computer vision problems. This time,
    classification is solved by using a mathematical formulation that looks at the
    geometry of the problem in high-dimension spaces.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we will also present a new image representation that is often used
    in conjunction with SVMs to produce robust object detectors.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Images of objects are mainly characterized by their shape and textural content.
    This is the aspect that is captured by the **Histogram of Oriented Gradients**
    (**HOG**) representation. As its name indicates, this representation is based
    on building histograms from image gradients. In particular, because we are more
    interested by shapes and textures, it is the distribution of the gradient orientations
    that is analyzed. In addition, in order to take into consideration the spatial
    distribution of these gradients, multiple histograms are computed over a grid
    that divides the image into regions.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in building a HOG representation is therefore to compute the
    gradient of an image. The image is then subdivided into small cells (for example,
    `8×8` pixels) and histograms of gradient orientations are built for each of these
    cells. The range of possible orientations must therefore be divided into bins.
    Most often, only the gradient orientations are considered but not their directions
    (these are called unsigned gradients). In this case, the range of possible orientations
    is from `0` to `180` degrees. A 9-bin histogram in this case would divide the
    possible orientations into intervals of `20` degrees. Each gradient vector in
    a cell contributes to a bin with a weight corresponding to the magnitude of this
    gradient.
  prefs: []
  type: TYPE_NORMAL
- en: The cells are then grouped into blocks. A block is then made of a certain number
    of cells. These blocks that cover the image can overlap each other (that is, they
    can share cells). For example, in the case where blocks are made of `2×2` cells,
    a new block can be defined every one cell; this would represent a block stride
    of `1` cell and each cell (except the last one in a row) would then contribute
    to `2` blocks. Conversely, with a block stride of `2` cells, the blocks would
    not overlap at all. A block contains a certain number of cell histograms (for
    example, `4` in the case of a block made of `2×2` cells). These histograms are
    simply concatenated together to form a long vector (for example, `4` histograms
    of `9` bins each then produce a vector of length `36`). To make the representation
    invariant to changes in contrast, this vector is then normalized (for example,
    each element is divided by the magnitude of the vector). Finally you also concatenate
    together all the vectors associated with all blocks of the image (row order) into
    a very large one (for example, in a `64×64` image, you will have a total of seven
    `16×16` blocks when a stride of `1` is applied on cells of size `8×8`; this represents
    a final vector of `49x36 = 1764` dimensions). This long vector is the HOG representation
    of the image.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the HOG of an image leads to a vector of very high dimension
    (see the *There's more...* section of this recipe that proposes a way to visualize
    a HOG representation). This vector characterizes the image and can then be used
    to classify images of different classes of objects. To achieve this goal, we therefore
    need a machine learning method that can handle vectors of very high dimension.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will build another stop sign classifier. This is obviously
    just a toy example that serves to illustrate the learning procedure. As we explained
    in the previous recipe, the first step is to collect samples for training. In
    our example, the set of positive samples that we will be using is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_14_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And our (very small) set of negative samples is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_14_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now learn how to differentiate these two classes using SVM as implemented
    in the `cv::svm` class. To build a robust classifier, we will represent our class
    instances using HOG as described in the introductory section of this recipe. More
    precisely, we will use `8×8` blocks made of `2×2` cells with a block stride of
    `1` cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With 9-bin histograms and `64×64` samples, this configuration produces HOG
    vectors (made of `225` blocks) of size `8100`. We compute this descriptor for
    each of our samples and transfer them into a single matrix (one HOG per row):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how we computed the first HOG in order to obtain the size of the descriptor
    and then created the matrix of descriptors. A second matrix is then created to
    contain the labels associated to each sample. In our case, the first rows are
    the positive samples (and must be assigned a label of `1`), the reminder rows
    are the negative  samples (labeled `-1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build the SVM classifier that will be used for training;
    we also select the type of SVM and the kernel to be used (these parameters will
    be discussed in the next section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready for training. The labeled samples are first provided to the
    classifier and the `train` method is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training phase completes, any sample of unknown class can be submitted
    to the classifier, which will try to predict the class to which it belongs (here
    we test four samples):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If the classifier has been trained with representative samples, then it should
    be able to correctly predict the label of a new instance.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our stop sign recognition example, each instance of our class is represented
    by a point in an 8100-dimensional HOG space. It is obviously impossible to visualize
    such a large space but the idea behind support vector machines is to trace a boundary
    in that space that will segregate points that belongs to one class from points
    belonging to the other class. More specifically, this boundary will in fact be
    just a simple hyperplane. This idea is better explained considering a 2D space
    where each instance is represented as a 2D point. The hyperplane is, in this case,
    a simple line.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_14_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is obviously a trivial example but, conceptually, working in a two-dimensional
    space or in a 8100-dimensional space is the same thing. The preceding figure shows
    then, how a simple line can separate the points of the two classes well. In the
    case illustrated here, it can also be seen that many other lines could also achieve
    this perfect class separation. One question is therefore; which exact line one
    should choose. To answer this question, you must first realize that the samples
    we used to build our classifier constitute just a small snapshot of all possible
    instances that will need to be classified when the classifier is used in a target
    application. This means that we would like our classifier, not only to be able
    to correctly separate the provided sample sets but we also would like this one
    to make the best decision on the future instances shown to it. This concept is
    often referred to as the **generalization** power of a classifier. Intuitively,
    it would be reasonable to believe that our separating hyperplane should be located
    in between the two classes, not closer to one class than the other. More formally,
    SVMs propose setting the hyperplane at a position that maximizes the margin around
    the defined boundary. This **margin** is defined as the minimum distance between
    the separating hyperplane and the closest point in the positive sample set plus
    the distance between the hyperplane and the closest negative sample. The closest
    points (the ones that define the margin) are called the **support vectors**. The
    mathematics behind SVM defines an optimization function aiming at identifying
    these support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: But the proposed solution to the classification problem cannot be that simple.
    What happens if the distribution of the sample points is as follows?
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_14_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, a simple hyperplane (a line here) cannot achieve a proper separation.
    SVM solves this problem by introducing artificial variables that bring the problem
    into a higher dimensional space through some non-linear transformations. For example,
    in the example above, one might propose to add the distance to the origin as an
    additional variable, that is to compute *r= sqrt(x²+y²)* for each point. We now
    have a three-dimensional space; for simplicity, let''s just draw the points on
    the *(r,x)* plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_14_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, our set of sample points can now be separated by a simple hyperplane.
    This implies that you now have to find the support vectors in this new space.
    In fact, in the SVM formulation, you do not have to bring all the points into
    that new space, you just have to define a way to measure point-to-hyperplane distance
    in that augmented space. SVM therefore defines **kernel functions** that allow
    you to measure this distance in higher space without having to explicitly compute
    the point coordinates in that space. This is just a mathematical trick that explains
    why support vectors producing the maximal margin can be efficiently computed in
    very high (artificial) dimensional space. This also explains why, when you want
    to use support vector machines, you need to specify which kernel you want to use.
    This is by applying these kernels that you will make non-linearly separable to
    become separable in the kernel space.
  prefs: []
  type: TYPE_NORMAL
- en: One important remark here however. Since with Support Vector Machines we often
    work with features of very high dimension (for example, `8100` dimension in our
    HOG example), then it may very well happen that our samples will be separable
    with a simple hyperplane. This is why it still make sense to not use non-linear
    kernels (or more precisely to use a linear kernel, that is, `cv::ml::SVM::LINEAR`)
    and work in the original feature space. The resulting classifier will then be
    computationally simpler. But for more challenging classification problems, kernels
    remain a very effective tool. OpenCV offers you a number of standard kernels (for
    example, radial basis functions, sigmoid functions, and so on); the objective
    of these is to send the samples into a larger non-linear space that will make
    the classes separable by a hyperplane. SVM has a number of variants; the most
    common is the C-SVM, which adds a penalty for each outlier sample that does not
    lie on the right side of the hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we insist on the fact that, because of their strong mathematical foundations,
    SVMs work very well with features of very high dimension. In fact, they have been
    shown to operate best when the number of dimensions of the feature space is larger
    than the number of samples. They are also memory efficient, as they just have
    to store the support vectors (in contrast to a method like nearest-neighbor that
    requires keeping in memory all sample points).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Histograms of oriented gradients and SVM form a good combination for the construction
    of good classifiers. One of the reasons for this success is the fact that HOG
    can be viewed as a robust high-dimensional descriptor that captures the essential
    aspects of an object class. HOG-SVM classifiers have been used successfully in
    many applications; pedestrian detection is one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, since this is the last recipe of this book, we will therefore end it
    with a perspective on a recent trend in machine learning that is revolutionizing
    computer vision and artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: HOG visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'HOGs are built from cells combined in overlapping blocks. It is therefore difficult
    to visualize this descriptor. Nevertheless, they are often represented by displaying
    the histograms associated to each cell. In this case, instead of aligning the
    orientation bins in a regular bar graph, a histogram of orientation can be more
    intuitively drawn in a star-shape where each line has the orientation associated
    to the bin it represents and the length of the line is proportional to that bin
    count. These HOGs representations can then be displayed over an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![HOG visualization](img/image_14_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Each cell HOG representation can be produced by a simple function that accepts
    an iterator pointing to a histogram. Lines of proper orientation and length are
    then drawn for each `bin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'A HOG visualization function will then call this preceding function for each
    cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This function computes a HOG descriptor having the specified cell size but made
    of only one large block (that is, a block having the size of the image). This
    representation therefore ignores the effect of normalization that occurs at each
    block level.
  prefs: []
  type: TYPE_NORMAL
- en: People detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenCV offers a pre-trained people detector based on HOG and SVM. As for the
    classifier cascades of the previous recipe, this SVM classifier can be used to
    detect instances in a full image by scanning a window across the image, at multiple
    scales. You then just have to construct the classifier and perform the detection
    on an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The window stride defines how the `128×64` template is moved over the image
    (every `4` pixels horizontally and vertically in our example). Longer strides
    make the detection faster (because less windows are evaluated) but you may then
    miss some people falling in between tested windows. The image padding parameter
    simply adds pixels on the border of the image such that people at the edge of
    the image can be detected. The standard threshold for an SVM classifier is `0`
    (since `1` is the value assigned to positive instances and `-1` to the negative
    ones). But if you really want to be certain that what you detect is a person,
    then you can raise this threshold value (this means that you want **high precision**
    at the price of missing some people in the image). Reciprocally, if you want to
    be certain of detecting all people (that is you want a **high recall** rate),
    then you can lower the threshold; more false detections will occur in that case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the detection results obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![People detection](img/image_14_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is important to note that when a classifier is applied to a full image,
    the multiple windows applied at successive locations will often lead to multiple
    detections around a positive sample. The best thing to do when two or more bounding
    boxes overlap at about the same location is to retain only one of them. There
    is a function called `cv::groupRectangles` that simply combines rectangles of
    similar size at similar locations (this function is automatically called by `detectMultiScale`).
    In fact, obtaining a group of detections at a particular location can even be
    seen as an indicator confirming that we indeed have a positive instance at this
    location. This is why the `cv::groupRectangles` function allows us to specify
    the minimum size for a detection cluster to be accepted as a positive detection
    (that is, isolated detection should be discarded). This is the last parameter
    of the `detectMultiScale` method. Setting this one at `0` will keep all detections
    (no grouping done) which, in our example, leads to the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![People detection](img/image_14_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Deep learning and Convolutional Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We cannot conclude this chapter on machine learning without mentioning deep
    convolutional neural networks. The application of these to computer vision classification
    problems has led to impressive results. In fact, their outstanding performance
    when applied to real-world problems is such that they now open the door to a new
    family of applications that could not be envisioned before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning is based on the theory of neural networks that was introduced
    in the late 1950s. So why are they generating such great interest today? Basically
    for two reasons: first, the computational power that is available nowadays allows
    deploying neural networks of a size that makes them able to solve challenging
    problems. While the first neural network (the perceptron) has only one layer and
    few weight parameters to tune, today''s networks can have hundreds of layers and
    millions of parameters to be optimized (hence the name deep networks). Second,
    the large amount of data available today makes their training possible. In order
    to perform well, deep networks, indeed, required thousands, if not millions, of
    annotated samples (this is required because of the very large number of parameters
    that need to be optimized).'
  prefs: []
  type: TYPE_NORMAL
- en: The most popular deep networks are the **Convolutional Neural Networks** (**CNN**).
    As the name suggests, they are based on convolution operations (see [Chapter 6](ch06.html
    "Chapter 6. Filtering the Images") , *Filtering the Images*). The parameters to
    learn, in this case, are therefore the values inside the kernel of all filters
    that compose the network. These filters are organized into layers, in which the
    early layers extract the fundamental shapes such as lines and corners while the
    higher layers progressively detect more complex patterns (such as, for example,
    the presence of eyes, mouth, hair, in a human detector).
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV3 has a **Deep Neural Network** module, but this one is mainly for importing
    deep networks trained using other tools such as TensorFlow, Caffe, or Torch. When
    building your future computer vision applications, you will certainly have to
    have a look at the deep learning theory and its related tools.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Describing and matching local intensity patterns* recipe in [Chapter 9](ch09.html
    "Chapter 9. Describing and Matching Interest Points") , *Describing and Matching
    Interest Points*, described the SIFT descriptor which is similar to the HOG descriptor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article *Histograms of Oriented Gradients for Human Detection* by N. Dalal
    and B. Triggs in *Computer Vision and Pattern Recognition* conference, 2005 is
    the classical paper that introduces histograms of oriented gradients for people
    detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article *Deep Learning* by Y. LeCun, Y. Bengio and G. Hinton in *Nature,
    no 521*, 2015, is a good starting point for exploring the world of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
