<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Understanding Linear Regression</h1></div></div></div><p>In this chapter, we begin our exploration of machine learning models and techniques. The ultimate objective of machine learning is to <em>generalize</em> the facts from some empirical sample data. This is called <strong>generalization</strong>,<a id="id114" class="indexterm"/> and is essentially the ability to use these inferred facts to accurately perform at an accurate rate on new, unseen data. The two broad categories of machine learning are <strong>supervised</strong> learning and <strong>unsupervised</strong> learning. The term <strong>supervised learning</strong><a id="id115" class="indexterm"/> is used to describe the task of machine learning in which an understanding or a model is formulated from some labeled data. By labeled, we mean that the sample data is associated with some observed value. In a basic sense, the model is a statistical description of the data and how the data varies over different parameters. The initial data used by supervised machine learning techniques to create the model is called the <strong>training data</strong><a id="id116" class="indexterm"/> of the model. On the other hand, unsupervised learning<a id="id117" class="indexterm"/> techniques estimate models by finding patterns in unlabeled data. As the data used by unsupervised learning techniques is unlabeled, there is often no definite yes-or-no-based reward system to determine if an estimated model is accurate and correct.</p><p>We will now examine <em>linear regression</em>, which is an interesting model that can be used for prediction. As a type of supervised learning, regression models are created from some data in which a number of parameters are somehow combined to produce several target values. The model actually describes the relation between the target value and the model's parameters, and can be used to predict a target value when supplied with the values for the parameters of the model.</p><p>We will first study linear regression with single as well as multiple variables, and then describe the algorithms that can be used to formulate machine learning models from some given data. We will study the reasoning behind these models and simultaneously demonstrate how we can implement the algorithms to create these models in Clojure.</p><div><div><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Understanding single-variable linear regression</h1></div></div></div><p>We often come across situations where we would need to create an approximate model from some sample data. This model can then be used to predict more such data when its required parameters are supplied. For example, we might want to study the frequency of rainfall on a given day in a particular <a id="id118" class="indexterm"/>city, which we will assume varies depending on the humidity on that day. A formulated model could be useful in predicting the possibility of rainfall on a given day if we know the humidity on that day. We start formulating a model from some data by first fitting a straight line (that is, an equation) with some parameters and coefficients over this data. This type of model is called a <strong>linear regression</strong> model<a id="id119" class="indexterm"/>. We can think of linear regression as a way of fitting a straight line, <img src="img/4351OS_02_01.jpg" alt="Understanding single-variable linear regression"/>, over the sample data, if we assume that the sample data has only a single dimension.</p><p>The linear regression model is simply described as a linear equation that represents the <strong>regressand</strong> or <strong>dependent variable</strong> of the model. The formulated regression model can have one to several parameters depending on the available data, and these parameters of the model are also termed as <strong>regressors</strong>, <strong>features</strong>, or <strong>independent variables</strong> of the model. We will first explore linear regression models with a single independent variable.</p><p>An example problem<a id="id120" class="indexterm"/> for using linear regression with a single variable<a id="id121" class="indexterm"/> would be to predict the probability of rainfall on a particular day, which depends on the humidity on that day. This training data can be represented in the following tabular form:</p><div><img src="img/4351OS_02_85.jpg" alt="Understanding single-variable linear regression"/></div><p>For a single-variable linear model, the dependent variable must vary with respect to a single parameter. Thus, our sample data essentially consists of two vectors, that is, one for the values of the dependent parameter <em>Y</em> and the other for the values of the independent variable <em>X</em>. Both<a id="id122" class="indexterm"/> vectors have the same length. This data can be formally represented as two vectors, or single column matrices, as follows:</p><div><img src="img/4351OS_02_03.jpg" alt="Understanding single-variable linear regression"/></div><p>Let's quickly define the<a id="id123" class="indexterm"/> following two matrices in Clojure, <em>X</em> and <em>Y</em>, to represent some sample data:</p><div><pre class="programlisting">(def X (cl/matrix [8.401 14.475 13.396 12.127 5.044
                      8.339 15.692 17.108 9.253 12.029]))

(def Y (cl/matrix [-1.57 2.32  0.424  0.814 -2.3
           0.01 1.954 2.296 -0.635 0.328]))</pre></div><p>Here, we define 10 points of data; these points can be easily plotted on a scatter graph using the following <a id="id124" class="indexterm"/>Incanter <code class="literal">scatter-plot</code> function:</p><div><pre class="programlisting">(def linear-samp-scatter
  (scatter-plot X Y))

(defn plot-scatter []
  (view linear-samp-scatter))

(plot-scatter)</pre></div><p>The preceding code displays the following scatter plot<a id="id125" class="indexterm"/> of our data:</p><div><img src="img/4351OS_02_04.jpg" alt="Understanding single-variable linear regression"/></div><p>The previous scatter plot is a<a id="id126" class="indexterm"/> simple representation of the 10 data points that we defined in <code class="literal">X</code> and <code class="literal">Y</code>.</p><div><div><h3 class="title"><a id="note11"/>Note</h3><p>The <code class="literal">scatter-plot</code> function can be found in the <code class="literal">charts</code> namespace of the Incanter library. The namespace declaration of a file using this function should look similar to the following declaration:</p><div><pre class="programlisting">(ns my-namespace
  (:use [incanter.charts :only [scatter-plot]]))</pre></div></div></div><p>Now that we have a visualization of our data, let's estimate a linear model over the given data points. We can generate<a id="id127" class="indexterm"/> a linear model of any data using the <code class="literal">linear-model</code> function<a id="id128" class="indexterm"/> from the Incanter library. This function returns a map that describes the formulated model and also a lot of useful data about this model. For starters, we can plot the linear model over our previous scatter plot by using the <code class="literal">:fitted</code> key-value pair from this map. We first get the value of the <code class="literal">:fitted</code> key from the returned map and add it to the scatter plot using the <code class="literal">add-lines</code> function<a id="id129" class="indexterm"/>; this is shown in the following code:</p><div><pre class="programlisting">(def samp-linear-model
  (linear-model Y X))
(defn plot-model []
  (view (add-lines samp-scatter-plot 
          X (:fitted linear-samp-scatter))))

(plot-model)</pre></div><p>This code produces the following self-explanatory plot of the linear model over the scatter plot we defined previously:</p><div><img src="img/4351OS_02_06.jpg" alt="Understanding single-variable linear regression"/></div><p>The previous plot<a id="id130" class="indexterm"/> depicts the linear model <code class="literal">samp-linear-model</code> as a straight line drawn over the 10 data points that we defined in <code class="literal">X</code> and <code class="literal">Y</code>.</p><div><div><h3 class="title"><a id="note12"/>Note</h3><p>The <code class="literal">linear-model</code> function can be found in the <code class="literal">stats</code> namespace of the Incanter library. The namespace declaration of a file using <code class="literal">linear-model</code> should look similar to the following declaration:</p><div><pre class="programlisting">(ns my-namespace
  (:use [incanter.stats :only [linear-model]]))</pre></div></div></div><p>Well, it looks<a id="id131" class="indexterm"/> like Incanter's <code class="literal">linear-model</code> function<a id="id132" class="indexterm"/> did most of the work for us. Essentially, this function creates a linear model of our data by using the <strong>ordinary-least squares</strong> (<strong>OLS</strong>) curve-fitting algorithm<a id="id133" class="indexterm"/>. We will soon dive into the details of this algorithm, but let's first understand how exactly a curve is fit onto some given data.</p><p>Let's first define how a straight line can be represented. In coordinate geometry, a line is simply a function of an independent variable, <em>x</em>, which has a given slope, <em>m</em>, and an intercept, <em>c</em>. The function of the line <em>y</em> can be formally written as <img src="img/4351OS_02_01.jpg" alt="Understanding single-variable linear regression"/>. The slope of the line represents how much the value of <em>y</em> changes when the value of <em>x</em> varies. The intercept of this equation is just where the line meets the <em>y</em> axis of the plot. Note that the equation <em>y</em> is not the same as <em>Y</em>, which actually represents the values of the equation that we have been provided with.</p><p>Analogous to this definition of a straight line from coordinate geometry, we formally define the linear regression model <a id="id134" class="indexterm"/>with a single variable using our definition of the matrices <em>X</em> and <em>Y</em>, as follows:</p><div><img src="img/4351OS_02_09.jpg" alt="Understanding single-variable linear regression"/></div><p>This definition of the linear model with a single variable is actually quite versatile since we can use the same equation to define a linear model with multiple variables; we will see this later in the chapter. In the preceding definition, the term <img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/> is a coefficient that represents the linear <a id="id135" class="indexterm"/>scale of <em>y</em> with respect to <em>x</em>. In terms of geometry, it's simply the slope of a line that fits the given data in matrices <em>X</em> and <em>Y</em>. Since <em>X</em> is a matrix or vector, <img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/> can also be thought of as a scaling factor for the matrix <em>X</em>.</p><p>Also, the term <img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/> is another coefficient that explains the value of <em>y</em> when <em>x</em> is zero. In other words, it's the <em>y</em> intercept of the equation. The coefficient <img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/> of the formulated model is termed as the <strong>regression coefficient</strong> or <strong>effect</strong> of the linear model, and the coefficient <img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/> is termed as the <strong>error term</strong> or <strong>bias</strong> of the model. A model may even have several regression coefficients, as we will see later in this chapter. It turns out that the error <img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/> is actually just another regression coefficient and can be conventionally mentioned along with the other effects of the model. Interestingly, this error determines the scatter or variance of the data in general.</p><p>Using the map returned by the <code class="literal">linear-model</code> function from our earlier example, we can easily inspect the coefficients of the generated model. The returned map has a <code class="literal">:coefs</code> key that maps to a vector containing the coefficients of the model. By convention, the error term is also included in this vector, simply as another coefficient:</p><div><pre class="programlisting">user&gt; (:coefs samp-linear-model)
[-4.1707801647266045 0.39139682427040384]</pre></div><p>Now we've defined a linear model over our data. It's obvious that not all the points will be on a line that is plotted to represent the formulated model. Each data point has some deviation from the linear model's plot over the <em>y</em> axis, and this deviation can be either positive or negative. To represent the overall deviation of the model from the given data, we use the <em>residual sum of squares</em>, <em>mean-squared error</em>, and <em>root mean-squared error</em> functions. The values of these three functions represent a scalar measure of the amount of error in the formulated model.</p><p>The difference between the terms <em>error</em> and <em>residual</em> is that an error is a measure of the amount by which <a id="id136" class="indexterm"/>an observed value differs from its expected value, while a residual is an estimate of the unobservable statistical error, which is simply not modeled or understood by the statistical model that we are using. We can say that, in a set of observed values, the difference between an observed <a id="id137" class="indexterm"/>value and the mean of all values is a residual. The number of residuals in a formulated model must be equal to the number of observed values of the dependent variable in the sample data.</p><p>We can use the <code class="literal">:residuals</code> keyword to fetch the residuals from the linear model generated by the <code class="literal">linear-model</code> function, as shown in the following code:</p><div><pre class="programlisting">user&gt; (:residuals samp-linear-model)
[-0.6873445559690581 0.8253111334125092 -0.6483716931997257 0.2383108767994172 -0.10342541689331242 0.9169220471357067 -0.01701880172457293 -0.22923670489146497 -0.08581465024744239 -0.20933223442208365]</pre></div><p>The <strong>sum of squared errors of prediction</strong> (<strong>SSE</strong>) <a id="id138" class="indexterm"/>is simply the sum of errors in a formulated model. <a id="id139" class="indexterm"/>Note that in the following equation, the sign of the error term <img src="img/4351OS_02_12.jpg" alt="Understanding single-variable linear regression"/> isn't significant since we square this difference value; thus, it will always produce a positive value. The SSE<a id="id140" class="indexterm"/> is also termed as the <strong>residual sum of squares</strong> (<strong>RSS</strong>).</p><div><img src="img/4351OS_02_13.jpg" alt="Understanding single-variable linear regression"/></div><p>The <code class="literal">linear-model</code> function also calculates the SSE of the formulated model, and this value can be retrieved using the <code class="literal">:sse</code> keyword; this is illustrated in the following lines of code:</p><div><pre class="programlisting">user&gt; (:sse samp-linear-model)
2.5862250345284887</pre></div><p>The <strong>mean-squared error</strong> (<strong>MSE</strong>) measures the average magnitude of errors in a formulated model without considering<a id="id141" class="indexterm"/> the direction of the errors. We can calculate this value by squaring the differences of all the given values of the dependent variable and their corresponding predicted values on the formulated linear model, and calculating the mean of these squared errors. The MSE is also termed as the <strong>mean-squared prediction error</strong> of a model. If the MSE of a formulated model is zero, then we can say that the model fits the given data perfectly. Of course, this is practically impossible for real data, although we could find a set of values that produce an MSE of zero in theory.</p><p>For a given set of <em>N</em> values of the dependent variable <img src="img/4351OS_02_14.jpg" alt="Understanding single-variable linear regression"/> and an estimated set of values <img src="img/4351OS_02_15.jpg" alt="Understanding single-variable linear regression"/> calculated from a formulated model, we can<a id="id142" class="indexterm"/> formally represent the MSE function of the formulated model <img src="img/4351OS_02_16.jpg" alt="Understanding single-variable linear regression"/> as follows:</p><div><img src="img/4351OS_02_17.jpg" alt="Understanding single-variable linear regression"/></div><p>The <strong>root mean-squared error</strong> (<strong>RMSE</strong>) <a id="id143" class="indexterm"/>or <strong>root-mean squared deviation</strong> is simply the square root of the MSE and<a id="id144" class="indexterm"/> is often used to measure the deviation of a formulated linear model. The RMSE is partial to<a id="id145" class="indexterm"/> larger errors, and is hence scale-dependent. This means that the RMSE is particularly useful when large errors are undesirable.</p><p>We can formally define <a id="id146" class="indexterm"/>the RMSE of a formulated model as follows:</p><div><img src="img/4351OS_02_18.jpg" alt="Understanding single-variable linear regression"/></div><p>Another measure of the accuracy of a formulated linear model is the <strong>coefficient of determination</strong><a id="id147" class="indexterm"/>, which is written as <img src="img/4351OS_02_19.jpg" alt="Understanding single-variable linear regression"/>. The coefficient of determination indicates how well the formulated model fits the given sample data, and is defined as follows. This coefficient is defined in terms of the mean of observed values in the sample data <img src="img/4351OS_02_20.jpg" alt="Understanding single-variable linear regression"/>, the SSE, and the total sum of errors <img src="img/4351OS_02_21.jpg" alt="Understanding single-variable linear regression"/>.</p><div><img src="img/4351OS_02_22.jpg" alt="Understanding single-variable linear regression"/></div><p>We can retrieve the calculated value of <img src="img/4351OS_02_19.jpg" alt="Understanding single-variable linear regression"/> from the model generated by the <code class="literal">linear-model</code> function by using the <code class="literal">:r-square</code> keyword as follows:</p><div><pre class="programlisting">user&gt; (:r-square samp-linear-model)
0.8837893226172282</pre></div><p>In order to formulate a model that best fits the sample data, we should strive to minimize the previously described values. For some given data, we can formulate several models and calculate the total error for each model. This calculated error can then be used to determine which formulated<a id="id148" class="indexterm"/> model is the best fit for the data, thus selecting the optimal linear model for the given data.</p><p>Based on the MSE of a formulated model, the model is said to have a<a id="id149" class="indexterm"/> <strong>cost function</strong><a id="id150" class="indexterm"/>. The problem of fitting a linear model over some data is equivalent to the problem of minimizing the cost function of a formulated linear model. The cost function, which is represented<a id="id151" class="indexterm"/> as <img src="img/4351OS_02_23.jpg" alt="Understanding single-variable linear regression"/>, can be simply thought of as a function of the parameters of a formulated model. Generally, this cost function translates to the MSE of a model. Since the RMSE varies with the formulated parameters of the model, the following cost function of the model is a function of these parameters:</p><div><img src="img/4351OS_02_24.jpg" alt="Understanding single-variable linear regression"/></div><p>This brings us to the following formal definition of the problem of fitting a linear regression model over some data for the estimated effects <img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/> and <img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/> of a linear model:</p><div><img src="img/4351OS_02_25.jpg" alt="Understanding single-variable linear regression"/></div><p>This definition states that we can estimate a linear model, represented by the parameters <img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/> and <img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/>, by determining the values of these parameters, for which the cost function <img src="img/4351OS_02_23.jpg" alt="Understanding single-variable linear regression"/> takes on the least possible value, ideally zero.</p><div><div><h3 class="title"><a id="note13"/>Note</h3><p>In the preceding equation, the <img src="img/4351OS_02_26.jpg" alt="Understanding single-variable linear regression"/> expression represents the standard norm <em>N</em>-dimensional Euclidian space of the cost function. By the term <em>norm</em>, we mean a function that has only positive values in the <em>N</em>-dimensional space.</p></div></div><p>Let's visualize how the<a id="id152" class="indexterm"/> Euclidian space <a id="id153" class="indexterm"/>of the cost function of a formulated<a id="id154" class="indexterm"/> model varies with respect to the parameters of the model. For this, let's assume that the <img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/> parameter that represents the constant error is zero. A plot of the cost function <img src="img/4351OS_02_27.jpg" alt="Understanding single-variable linear regression"/> of the linear model over the parameter <img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/> will ideally appear as a parabolic curve, similar to the following plot:</p><div><img src="img/4351OS_02_28.jpg" alt="Understanding single-variable linear regression"/></div><p>For a single parameter, <img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/>, we can plot the preceding chart, which has two dimensions. Similarly, for two parameters, <img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/> and <img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/>, of the formulated model, a plot of three dimensions is produced. This<a id="id155" class="indexterm"/> plot appears bowl-shaped or having a<a id="id156" class="indexterm"/> convex surface, as illustrated in the following diagram. Also, we can generalize this for <em>N</em> parameters of the<a id="id157" class="indexterm"/> formulated model and produce a plot of <img src="img/4351OS_02_30.jpg" alt="Understanding single-variable linear regression"/> dimensions.</p><div><img src="img/4351OS_02_31.jpg" alt="Understanding single-variable linear regression"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Understanding gradient descent</h1></div></div></div><p>The gradient descent algorithm<a id="id158" class="indexterm"/> is one of the simplest, although not the most efficient techniques to formulate a linear model that has the least possible value for the cost function or error of the model. This algorithm essentially finds the local minimum of the cost function for a formulated linear model.</p><p>As we previously described, a three-dimensional plot of the cost function for a single-variable linear regression model would appear as a convex or bowl-shaped surface with a <em>global minimum</em>. By minimum, we mean that the cost function has the least possible value at this point on the surface of the plot. The gradient descent algorithm essentially starts from any point on the surface and performs a sequence of steps to approach the local minimum of the surface. </p><p>This process can be imagined as dropping a ball into a valley or between two adjacent hills, as a result of which the ball slowly rolls towards the point that has the least elevation above sea level. The algorithm is repeated until the value of the apparent cost function from the current point on the surface converges to zero, which figuratively means that the ball rolling down the hill comes to a stop, as we described earlier.</p><p>Of course, gradient descent may not really work if there are multiple local minimums on the surface of the plot. However, for an appropriately scaled single-variable linear regression model, the surface of the plot always has a single global minimum, as we illustrated earlier. Thus, we can still use the gradient descent algorithm in such situations to find the global minimum of the surface of the plot.</p><p>The gist of this algorithm is that we start from some point on the surface and then take several steps towards the lowest point. We can formally represent<a id="id159" class="indexterm"/> this with the following equality:</p><div><img src="img/4351OS_02_32.jpg" alt="Understanding gradient descent"/></div><p>Here, we start from the point represented by <img src="img/4351OS_02_33.jpg" alt="Understanding gradient descent"/> on the plot of the cost function <em>J</em>, and incrementally subtract the product of the first-order partial derivative of the cost function <img src="img/4351OS_02_34.jpg" alt="Understanding gradient descent"/>, which is derived with respect to the parameters of the formulated model. This means that we slowly step downwards on the surface towards the local minimum, until we cannot find a lower point on the surface. The term <img src="img/4351OS_02_35.jpg" alt="Understanding gradient descent"/> determines how large our steps towards the local minimum are, and is called the<a id="id160" class="indexterm"/> <em>step</em> of <a id="id161" class="indexterm"/>the gradient descent algorithm. We repeat this iteration until the difference between <img src="img/4351OS_02_36.jpg" alt="Understanding gradient descent"/> and <img src="img/4351OS_02_33.jpg" alt="Understanding gradient descent"/> converges to zero, or at least reduces to a threshold value close to zero.</p><p>The process of <a id="id162" class="indexterm"/>stepping down towards the local minimum of the surface of the cost functions plot is illustrated in the following diagram:</p><div><img src="img/4351OS_02_37.jpg" alt="Understanding gradient descent"/></div><p>The preceding illustration<a id="id163" class="indexterm"/> is a contour diagram of the surface of the plot, in which the circular lines connect the points with an equal height. We start from the point <img src="img/4351OS_02_39.jpg" alt="Understanding gradient descent"/> and perform a single iteration of the gradient descent algorithm to step down the surface to point <img src="img/4351OS_02_40.jpg" alt="Understanding gradient descent"/>. We repeat this process until we reach the local minimum of the surface with respect to the initial starting point <img src="img/4351OS_02_39.jpg" alt="Understanding gradient descent"/>. Note that, through each iteration, the size of the step reduces since the slope of a tangent to this surface also tends to zero as we approach the local minimum.</p><p>For a single-variable linear regression model with an error constant <img src="img/4351OS_02_11.jpg" alt="Understanding gradient descent"/> that is equal to zero, we can simplify the <a id="id164" class="indexterm"/>partial derivative component <img src="img/4351OS_02_34.jpg" alt="Understanding gradient descent"/> of the gradient descent algorithm. When there is only one parameter of the model, <img src="img/4351OS_02_10.jpg" alt="Understanding gradient descent"/>, the first order partial derivate is simply the slope of a tangent at that point on the surface of the plot. Thus, we calculate the slope of this tangent and take a step in the direction of this slope such that we arrive at a point of elevation above the <em>y</em> axis. This is shown in the following formula:</p><div><img src="img/4351OS_02_41.jpg" alt="Understanding gradient descent"/></div><p>We can<a id="id165" class="indexterm"/> implement this simplified version<a id="id166" class="indexterm"/> of the gradient descent algorithm as follows:</p><div><pre class="programlisting">(def gradient-descent-precision 0.001)

(defn gradient-descent
  "Find the local minimum of the cost function's plot"
  [F' x-start step]
  (loop [x-old x-start]
    (let [x-new (- x-old
                   (* step (F' x-old)))
          dx (- x-new x-old)]
      (if (&lt; dx gradient-descent-precision)
        x-new
        (recur x-new)))))</pre></div><p>In the preceding function, we begin from the point <code class="literal">x-start</code> and recursively apply the gradient descent algorithm until the value <code class="literal">x-new</code> converges. Note that this process is implemented as a tail recursive function using the <code class="literal">loop</code> form.</p><p>Using partial differentiation, we can formally express how both the parameters <img src="img/4351OS_02_10.jpg" alt="Understanding gradient descent"/> and <img src="img/4351OS_02_11.jpg" alt="Understanding gradient descent"/> can be calculated using the gradient descent algorithm as follows:</p><div><img src="img/4351OS_02_42.jpg" alt="Understanding gradient descent"/></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Understanding multivariable linear regression</h1></div></div></div><p>A<a id="id167" class="indexterm"/> multivariable linear regression model can have multiple variables<a id="id168" class="indexterm"/> or features, as opposed to the linear regression model with a single variable that we previously studied. Interestingly,<a id="id169" class="indexterm"/> the definition of a linear model with a single variable can itself be extended via matrices to be applied to multiple variables.</p><p>We can extend our previous<a id="id170" class="indexterm"/> example for predicting the probability of rainfall on a particular day to a model with multiple variables by including more independent variables, such as the minimum and maximum temperatures, in the sample data. Thus, the training data for a multivariable linear regression model will look similar to the following illustration:</p><div><img src="img/4351OS_02_43.jpg" alt="Understanding multivariable linear regression"/></div><p>For a multivariable linear regression model,<a id="id171" class="indexterm"/> the training data is defined by two matrices, <em>X</em> and <em>Y</em>. Here, <em>X</em> is an <img src="img/4351OS_02_44.jpg" alt="Understanding multivariable linear regression"/> matrix, where <em>P</em> is the number of independent variables in the model. The matrix <em>Y</em> is a vector of length <em>N</em>, just like in a linear model with a single variable. This model is illustrated as follows:</p><div><img src="img/4351OS_02_45.jpg" alt="Understanding multivariable linear regression"/></div><p>For the following example <a id="id172" class="indexterm"/>of multivariable linear regression in Clojure, we will not generate the sample data through code but use the sample data from the Incanter library. We can fetch any dataset using the Incanter library's<a id="id173" class="indexterm"/> <code class="literal">get-dataset</code> function.</p><div><div><h3 class="title"><a id="note14"/>Note</h3><p>For the upcoming example, the <code class="literal">sel</code>, <code class="literal">to-matrix</code>, and <code class="literal">get-dataset</code> functions from the Incanter library can be imported into our namespace as follows:</p><div><pre class="programlisting">(ns my-namespace
  (:use [incanter.datasets :only [get-dataset]]
        [incanter.core :only [sel to-matrix]]))</pre></div></div></div><p>We can fetch the <strong>Iris</strong> dataset<a id="id174" class="indexterm"/> by calling the <code class="literal">get-dataset</code> function<a id="id175" class="indexterm"/> with the <code class="literal">:iris</code> keyword argument; this is shown as follows:</p><div><pre class="programlisting">(def iris
  (to-matrix (get-dataset :iris)))

(def X (sel iris :cols (range 1 5)))
(def Y (sel iris :cols 0))</pre></div><p>We first define the variable <code class="literal">iris</code> as a matrix using the <code class="literal">to-matrix</code> and <code class="literal">get-dataset</code> functions, and then define two matrices <code class="literal">X</code> and <code class="literal">Y</code>. Here, <code class="literal">Y</code> is actually a vector of 150 values, or a matrix of size <img src="img/4351OS_02_46.jpg" alt="Understanding multivariable linear regression"/>, while <code class="literal">X</code> is a matrix of size <img src="img/4351OS_02_47.jpg" alt="Understanding multivariable linear regression"/>. Hence, <code class="literal">X</code> can be used to represent the values of four independent variables, and <code class="literal">Y</code> represents the values of the dependent variable. Note that the <code class="literal">sel</code> function is used to select a set of columns from the <code class="literal">iris</code> matrix. In fact, we could <a id="id176" class="indexterm"/>select many more such columns from the <code class="literal">iris</code> data matrix, but we will use only four in the following example for the sake of simplicity.</p><div><div><h3 class="title"><a id="note15"/>Note</h3><p>The dataset that we used in the previous code example is the <em>Iris</em> dataset, which is available in the Incanter library. This dataset has quite a bit of historical significance, as it was used by Sir Ronald Fisher to first develop the <strong>linear discriminant analysis</strong> (<strong>LDA</strong>) method<a id="id177" class="indexterm"/> for classification (for more information, refer to "The Species Problem in Iris"). This dataset contains 50 samples of three distinct species of the Iris plant, namely <em>Setosa</em>, <em>Versicolor</em>, and <em>Virginica</em>. Four features of the flowers of these species are measured in each sample, namely the petal width, petal length, sepal width, and sepal length. Note that we will encounter this dataset several times over the course of this book.</p></div></div><p>Interestingly, the <code class="literal">linear-model</code> function accepts a matrix with multiple columns, so we can use this function to fit a <a id="id178" class="indexterm"/>linear regression model over both single variable and multivariable data as follows:</p><div><pre class="programlisting">(def iris-linear-model
  (linear-model Y X))
(defn plot-iris-linear-model []
  (let [x (range -100 100)
        y (:fitted iris-linear-model)]
    (view (xy-plot x y :x-label "X" :y-label "Y"))))

(plot-iris-linear-model)</pre></div><p>In the preceding code example, we plot the linear model using the <code class="literal">xy-plot</code> function<a id="id179" class="indexterm"/> while providing optional parameters to specify the labels of the axes in the defined plot. Also, we specify the range of the <em>x</em> axis by generating a vector using the <code class="literal">range</code> function. The <code class="literal">plot-iris-linear-model</code> function<a id="id180" class="indexterm"/> generates the following plot:</p><div><img src="img/4351OS_02_48.jpg" alt="Understanding multivariable linear regression"/></div><p>Although the curve in the plot produced from the previous example doesn't appear to have any definitive shape, we<a id="id181" class="indexterm"/> can still use this generated model to estimate or predict the value of the dependent variable by supplying values for the independent variables to the formulated model. In order<a id="id182" class="indexterm"/> to do this, we must first define the relationship between the dependent and independent variables of a linear regression model with multiple features.</p><p>A linear regression model of <em>P</em> independent variables produces <img src="img/4351OS_02_50.jpg" alt="Understanding multivariable linear regression"/> regression coefficients, since we include the error constant along with the other coefficients of the model and also define an extra variable <img src="img/4351OS_02_39.jpg" alt="Understanding multivariable linear regression"/>, which is always <em>1</em>.</p><p>The <code class="literal">linear-model</code> function agrees with the proposition that the number of coefficients <em>P</em> in the formulated model is always one more than the total number of independent variables in the sample data <em>N</em>; this is shown in the following code:</p><div><pre class="programlisting">user&gt; (= (count (:coefs iris-linear-model)) 
         (+ 1 (column-count X)))
true</pre></div><p>We formally express the relationship between a multivariable regression model's dependent and independent variables as follows:</p><div><img src="img/4351OS_02_52.jpg" alt="Understanding multivariable linear regression"/></div><p>Since the variable <img src="img/4351OS_02_39.jpg" alt="Understanding multivariable linear regression"/> is always <em>1</em> in the preceding equation, the value <img src="img/4351OS_02_53.jpg" alt="Understanding multivariable linear regression"/> is analogous to the error constant <img src="img/4351OS_02_11.jpg" alt="Understanding multivariable linear regression"/> from the definition of a linear model with a single variable.</p><p>We can define a single vector to represent all the coefficients of the previous equation as <img src="img/4351OS_02_10.jpg" alt="Understanding multivariable linear regression"/>. This vector is termed as the <a id="id183" class="indexterm"/>
<strong>parameter vector</strong><a id="id184" class="indexterm"/> of the formulated regression model. Also, the independent <a id="id185" class="indexterm"/>variables of the model can be represented by a vector. Thus, we can define the regression variable <em>Y</em> as the product of the transpose of the parameter vector and the vector of independent variables of the model:</p><div><img src="img/4351OS_02_54.jpg" alt="Understanding multivariable linear regression"/></div><p>Polynomial functions<a id="id186" class="indexterm"/> can also be reduced to the standard form by substituting a single variable for every higher-order variable in the polynomial equation. For example, consider the following polynomial equation:</p><div><img src="img/4351OS_02_55.jpg" alt="Understanding multivariable linear regression"/></div><p>We can substitute the variables <img src="img/4351OS_02_56.jpg" alt="Understanding multivariable linear regression"/> for <img src="img/4351OS_02_57.jpg" alt="Understanding multivariable linear regression"/> to reduce the equation to the standard form of a multivariable linear regression model.</p><p>This brings us to the following formal definition of the cost function for a linear model with multiple variables, which is simply an extension of the definition of the cost function for a linear model with a single variable:</p><div><img src="img/4351OS_02_58.jpg" alt="Understanding multivariable linear regression"/></div><p>Note that in the preceding definition, we can use the individual coefficients of the model interchangeably with the parameter vector <img src="img/4351OS_02_10.jpg" alt="Understanding multivariable linear regression"/>.</p><p>Analogous to our problem <a id="id187" class="indexterm"/>definition of fitting a model with a single variable over some given data, we can define the problem of formulating a<a id="id188" class="indexterm"/> multivariable linear model as the problem of minimizing the preceding cost function:</p><div><img src="img/4351OS_02_59.jpg" alt="Understanding multivariable linear regression"/></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec08"/>Gradient descent with multiple variables</h2></div></div></div><p>We can apply the gradient descent algorithm to find the local minimum of a model with multiple variables. Of course, since <a id="id189" class="indexterm"/>we have multiple coefficients in the model, we have to apply the algorithm for all these coefficients as opposed to just two coefficients in a regression model with a single variable.</p><p>The gradient descent algorithm can thus be used to find the values of all the coefficients in the parameter vector <img src="img/4351OS_02_10.jpg" alt="Gradient descent with multiple variables"/> of a multivariable linear regression model, and is formally defined as follows:</p><div><img src="img/4351OS_02_60.jpg" alt="Gradient descent with multiple variables"/></div><p>In the preceding definition, the term <img src="img/4351OS_02_61.jpg" alt="Gradient descent with multiple variables"/> simply refers to the sample values for the <img src="img/4351OS_02_62.jpg" alt="Gradient descent with multiple variables"/> independent variable in the formulated model. Also, the variable <img src="img/4351OS_02_63.jpg" alt="Gradient descent with multiple variables"/> is always <em>1</em>. Thus, this definition can be applied to just the two coefficients that correspond to our previous definition of the gradient descent algorithm for a linear regression model with a single variable.</p><p>As we've seen earlier,<a id="id190" class="indexterm"/> the gradient descent algorithm can be applied to a linear regression model with both single and multivariables. For some models, however, the gradient descent algorithm can actually take a lot of iterations, or rather time, to converge the estimated values of the model's coefficients. Sometimes, the algorithm can also diverge, and thus we will be unable to calculate the model's coefficients in such circumstances. Let's examine some of the factors that affect the behavior and performance of this algorithm:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">All the features of the sample data must be scaled with respect to each other. By scaling, we mean that all the values for the independent variables in the sample data take on a similar range of values. Ideally, all independent variables must have observed values between <em>-1</em> and <em>1</em>. This can be formally expressed as follows:<div><img src="img/4351OS_02_64.jpg" alt="Gradient descent with multiple variables"/></div></li><li class="listitem" style="list-style-type: disc">
We can normalize the observed values for the independent variables about the mean of these values. We can further normalize this data by using the standard deviation of the observed values. In summary, we substitute the values with those produced by subtracting the mean of these values, <img src="img/4351OS_02_65.jpg" alt="Gradient descent with multiple variables"/>,and dividing the resulting expression by the standard deviation <img src="img/4351OS_02_66.jpg" alt="Gradient descent with multiple variables"/>. This is shown in the following formula:
<div><img src="img/4351OS_02_67.jpg" alt="Gradient descent with multiple variables"/></div></li><li class="listitem" style="list-style-type: disc">
The stepping or learning rate, <img src="img/4351OS_02_35.jpg" alt="Gradient descent with multiple variables"/>, is another important factor that determines how fast the algorithm converges towards the values of the parameters of the formulated model. Ideally, the stepping rate should be selected so that the differences between the old and new iterated values of the parameters of the model have an <a id="id191" class="indexterm"/>optimal amount of change in every iteration. On one hand, if this value is too large, the algorithm could even produce diverging values for the parameters of the model after each iteration. Thus, the algorithm will never find a global minimum in this case. On the other hand, a small value for this rate could result in slowing down the algorithm through an unnecessarily large number of iterations.
</li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Understanding Ordinary Least Squares</h1></div></div></div><p>Another technique to estimate the parameter vector of a linear regression model is the <strong>Ordinary Least Squares</strong> (<strong>OLS</strong>) method. The OLS method<a id="id192" class="indexterm"/> essentially works by minimizing the sum of squared errors in a linear regression model.</p><p>The sum of squared errors of prediction, or SSE, of a linear regression model can be defined in terms of the model's actual and expected values as follows:</p><div><img src="img/4351OS_02_68.jpg" alt="Understanding Ordinary Least Squares"/></div><p>The preceding definition of the SSE can be factorized using matrix products as follows:</p><div><img src="img/4351OS_02_69.jpg" alt="Understanding Ordinary Least Squares"/></div><p>We can solve the preceding equation for the estimated parameter vector <img src="img/4351OS_02_10.jpg" alt="Understanding Ordinary Least Squares"/> by using the definition of a global minimum. Since this equation is a form of quadratic equation and the term <img src="img/4351OS_02_70.jpg" alt="Understanding Ordinary Least Squares"/> is always greater than zero, the global minimum of the surface of the cost function can be defined<a id="id193" class="indexterm"/> as the point at which the rate of change of the slope of a tangent to the surface at that point is zero. Also, the plot is a function of the parameters of the linear model, and so the equation of the surface plot should be differentiated by the estimated parameter vector <img src="img/4351OS_02_10.jpg" alt="Understanding Ordinary Least Squares"/>. We can thus solve this equation for the optimal parameter vector <img src="img/4351OS_02_10.jpg" alt="Understanding Ordinary Least Squares"/> of the formulated model as follows:</p><div><img src="img/4351OS_02_71.jpg" alt="Understanding Ordinary Least Squares"/></div><p>The last equation in the preceding derivation gives us the definition of the optimal parameter vector <img src="img/4351OS_02_10.jpg" alt="Understanding Ordinary Least Squares"/>, which is formally expressed as follows:</p><div><img src="img/4351OS_02_72.jpg" alt="Understanding Ordinary Least Squares"/></div><p>We can implement the preceding definition of the parameter vector through the OLS method using the core.matrix library's <code class="literal">transpose</code> and <code class="literal">inverse</code> functions and the Incanter library's <code class="literal">bind-columns</code> function:</p><div><pre class="programlisting">(defn linear-model-ols
  "Estimates the coefficients of a multi-var linear
  regression model using Ordinary Least Squares (OLS) method"
  [MX MY]
  (let [X (bind-columns (repeat (row-count MX) 1) MX)
        Xt (cl/matrix (transpose X))
        Xt-X (cl/* Xt X)]
    (cl/* (inverse Xt-X) Xt MY)))

(def ols-linear-model
  (linear-model-ols X Y))

(def ols-linear-model-coefs
  (cl/as-vec ols-linear-model))</pre></div><p>Here, we first add a column in which each element is <code class="literal">1</code>, as the first column of the matrix <code class="literal">MX</code> uses the <code class="literal">bind-columns</code> function<a id="id194" class="indexterm"/>. The extra column that we add represents the <a id="id195" class="indexterm"/>independent variable <img src="img/4351OS_02_39.jpg" alt="Understanding Ordinary Least Squares"/>, whose value is always <code class="literal">1</code>. We then use the <code class="literal">transpose</code> and <code class="literal">inverse</code> functions to calculate the estimated coefficients of the linear regression model for the data in matrices <code class="literal">MX</code> and <code class="literal">MY</code>.</p><div><div><h3 class="title"><a id="note16"/>Note</h3><p>For the current example, the <code class="literal">bind-columns</code> function from the Incanter library can be imported into our namespace as follows:</p><div><pre class="programlisting">(ns my-namespace
  (:use [incanter.core :only [bind-columns]]))</pre></div></div></div><p>The previously defined function can be applied to the matrices that we have previously defined (<em>X</em> and <em>Y</em>) as follows:</p><div><pre class="programlisting">(def ols-linear-model
  (linear-model-ols X Y))

(def ols-linear-model-coefs
  (cl/as-vec ols-linear-model))</pre></div><p>In the preceding code, <code class="literal">ols-linear-model-coefs</code> is simply the variable and <code class="literal">ols-linear-model</code> is a matrix with a single column, which is represented as a vector. We perform this conversion using the <code class="literal">as-vec</code> function from the clatrix library.</p><p>We can actually verify that the coefficients estimated by the <code class="literal">ols-linear-model</code> function <a id="id196" class="indexterm"/>are practically equal to the ones generated by the Incanter library's <code class="literal">linear-model</code> function, which is illustrated as follows:</p><div><pre class="programlisting">user&gt; (cl/as-vec (ols-linear-model X Y))
[1.851198344985435 0.6252788163253274 0.7429244752213087 -0.4044785456588674 -0.22635635488532463]
user&gt; (:coefs iris-linear-model)
[1.851198344985515 0.6252788163253129 0.7429244752213329 -0.40447854565877606 -0.22635635488543926]
user&gt; (every? #(&lt; % 0.0001) 
                      (map - 
                         ols-linear-model-coefs 
                         (:coefs iris-linear-model)))
true</pre></div><p>In the last expression in the preceding code example, we find the difference between the coefficients produced by the <code class="literal">ols-linear-model</code> function<a id="id197" class="indexterm"/>, the difference<a id="id198" class="indexterm"/> produced by the <code class="literal">linear-model</code> function, and check whether each of these differences is less than <code class="literal">0.0001</code>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Using linear regression for prediction</h1></div></div></div><p>Once we've determined the coefficients of a linear regression model, we can use these coefficients to predict the value of the dependent variable of the model. The predicted value is defined by the linear regression model as the sum<a id="id199" class="indexterm"/> of the products of each coefficient and the value of its corresponding independent variable.</p><p>We can easily define the following <a id="id200" class="indexterm"/>generic function, which when supplied with the coefficients and values of independent variables, predicts the value of the dependent variable for a given formulated linear regression model:</p><div><pre class="programlisting">(defn predict [coefs X]
  {:pre [(= (count coefs)
            (+ 1 (count X)))]}
  (let [X-with-1 (conj X 1)
        products (map * coefs X-with-1)]
    (reduce + products)))</pre></div><p>In the preceding function, we use a precondition to assert the number of coefficients and the values of independent variables. This function expects that the number of values of the independent variables is one less than the number of coefficients of the model, as we add an extra parameter to represent an independent variable whose value is always <em>1</em>. The function then calculates the product of the corresponding coefficients and the values of the independent variables using the <code class="literal">map</code> function, and then calculates the sum of these product terms using the <code class="literal">reduce</code> function.</p></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Understanding regularization</h1></div></div></div><p>Linear regression estimates some given training data using a linear equation; this solution may not always be the best fit for the given data. Of course, it depends largely on the problem that we are trying to model. <strong>Regularization</strong><a id="id201" class="indexterm"/> is a commonly used technique to provide a better fit for the data. Generally, a given model is regularized by reducing the effect of some of the independent variables of the model. Alternatively, we could model it as a higher-order polynomial. Regularization isn't exclusive to linear regression, and most machine learning algorithms use some form of regularization in order to create a more accurate model from the given training data.</p><p>A model is said to be <strong>underfit</strong> or <strong>high bias</strong> when it doesn't estimate the dependent variable to a value that is close to the observed values of the dependent variable in the training data. On the other hand, a model can also be called <strong>overfit</strong>, or said to have <strong>high variance</strong>, when the estimated model fits the data perfectly, but isn't general enough to be useful for prediction. Overfit models often describe random errors or noise in the training data instead of the underlying relationship between the dependent and independent variables of the model. The best fit regression model generally lies in between the models created by underfitting and overfitting models and can be obtained through the process of regularization.</p><p>A commonly used method<a id="id202" class="indexterm"/> for the regularization of an underfit or overfit model is <strong>Tikhnov regularization</strong><a id="id203" class="indexterm"/>. In statistics, this method is also called <strong>ridge regression</strong>. We can describe the general form of Tikhnov regularization as follows:</p><div><img src="img/4351OS_02_73.jpg" alt="Understanding regularization"/></div><p>Suppose <em>A</em> represents a mapping from<a id="id204" class="indexterm"/> the vector of independent variables <em>x</em> to the dependent variable <em>y</em>. The value <em>A</em> is analogous to the parameter vector of a regression model. The relationship between the vector <em>x</em> and the observed values of the dependent variable, written as <em>b</em>, can be expressed as follows.</p><p>An underfit model has a significant error, or rather deviation, with respect to the actual data. We should strive to minimize this error. This can be formally expressed as follows and is based on the sum of residues of the estimated model:</p><div><img src="img/4351OS_02_74.jpg" alt="Understanding regularization"/></div><p>Tikhnov regularization adds a penalized least squares term to the preceding equation to prevent overfitting and is formally defined as follows:</p><div><img src="img/4351OS_02_75.jpg" alt="Understanding regularization"/></div><p>The term <img src="img/4351OS_02_76.jpg" alt="Understanding regularization"/> in the preceding equation is called the regularization matrix<a id="id205" class="indexterm"/>. In the simplest form of Tikhnov regularization,<a id="id206" class="indexterm"/> this matrix takes the value <img src="img/4351OS_02_77.jpg" alt="Understanding regularization"/>, where <img src="img/4351OS_02_78.jpg" alt="Understanding regularization"/> is a constant. Although applying this equation to a regression model is beyond the scope of this book, we can use Tikhnov regularization to produce a linear regression model <a id="id207" class="indexterm"/>with the following cost function:</p><div><img src="img/4351OS_02_79.jpg" alt="Understanding regularization"/></div><p>In the preceding equation, the term <img src="img/4351OS_02_80.jpg" alt="Understanding regularization"/> is called the regularization parameter of the model. This value must be chosen appropriately as larger values for this parameter could produce an underfit model.</p><p>Using the previously defined cost function, we can apply a gradient descent to determine the parameter vector as follows:</p><div><img src="img/4351OS_02_81.jpg" alt="Understanding regularization"/></div><p>We can also apply regularization to the OLS method of determining the parameter vector as follows:</p><div><img src="img/4351OS_02_82.jpg" alt="Understanding regularization"/></div><p>In the preceding equation, <em>L</em> is called the smoothing matrix<a id="id208" class="indexterm"/>, and can take on the following forms. Note that we've used the latter form of the definition of <em>L</em> in <a class="link" href="ch01.html" title="Chapter 1. Working with Matrices">Chapter 1</a>, <em>Working with Matrices</em>.</p><div><img src="img/4351OS_01_0100.jpg" alt="Understanding regularization"/></div><p>Interestingly, when <a id="id209" class="indexterm"/>the regularization parameter <img src="img/4351OS_02_80.jpg" alt="Understanding regularization"/> in the preceding equation is <em>0</em>, the regularized solution reduces to the original solution using the OLS method.</p></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec22"/>Summary</h1></div></div></div><p>In this chapter, we've studied linear regression and a couple of algorithms that can be used to formulate an optimal linear regression model from some sample data. The following are some of the other points that we covered:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We discussed linear regression with single and multiple variables</li><li class="listitem" style="list-style-type: disc">We implemented the gradient descent algorithm to formulate a linear regression model with one variable</li><li class="listitem" style="list-style-type: disc">We implemented the <strong>Ordinary Least Squares</strong> (<strong>OLS</strong>) method to find the coefficients of an optimal linear regression model</li><li class="listitem" style="list-style-type: disc">We introduced regularization and how it could be applied to linear regression</li></ul></div><p>In the following chapter, we will study a different area of machine learning, that is, classification. Classification is also a form of regression and is used to categorize data into different classes or groups.</p></div></body></html>