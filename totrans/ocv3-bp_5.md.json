["```py\n/opencv_annotation -images <folder location> -annotations <output file>\n\n```", "```py\n    cmakemake\n    ./object_annotation -images <folder location> -annotations <output file>\n\n    ```", "```py\n    ./folder_listing –folder <folder> -images <images.txt>\n\n    ```", "```py\n    ./object_annotation –images <images.txt> -annotations <annotations.txt>\n\n    ```", "```py\n./opencv_createsamples –info annotations.txt –vec images.vec –bg negatives.txt –num amountSamples –w model_width –h model_height\n\n```", "```py\nhttps://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/average_dimensions/.\n```", "```py\naverage haverage] are [60 60].If we would use those [60 60] dimensions, then we would have a model that can only detect apples equal and larger to that size. However, moving away from the tree will result in not a single apple being detected anymore, since the apples will become smaller in size.Therefore, I suggest reducing the dimensions of the model to, for example, [30 30]. This will result in a model that still has enough pixel information to be robust enough and it will be able to detect up to half the apples of the training apples size.Generally speaking, the rule of thumb can be to take half the size of the average dimensions of the annotated data and ensure that your largest dimension is not bigger than 100 pixels. This last guideline is to ensure that training your model will not increase exponentially in time due to the large model size. If your largest dimension is still over 100 pixels, then just keep halving the dimensions until you go below this threshold.\n```", "```py\n-numNeg: This is the amount of negative samples used at each stage. However, this is not the same as the amount of negative images that were supplied by the negative data. The training samples negative windows from these images in a sequential order at the model size dimensions. Choosing the right amount of negatives is highly dependent on your application.\n\n*   If your application has close to no variation, then supplying a small number of windows could simply do the trick because they will contain most of the background variance.\n*   On the other hand, if the background variation is large, a huge number of samples would be needed to ensure that you train as much random background noise as possible into your model.\n*   A good start is taking a ratio between the number of positive and the number of negative samples equaling 0.5, so double the amount of negative versus positive windows.\n*   Keep in mind that each negative window that is classified correctly at an early stage will be discarded for training in the next stage since it cannot add any extra value to the training process. Therefore, you must be sure that enough unique windows can be grabbed from the negative images. For example, if a model uses 500 negatives at each stage and 100% of those negatives get correctly classified at each stage, then training a model of 20 stages will need 10,000 unique negative samples! Considering that the sequential grabbing of samples does not ensure uniqueness, due to the limited pixel wise movement, this amount can grow drastically.\n\n`-numStages`: This is the amount of weak classifier stages, which is highly dependent on the complexity of the application.\n\n*   The more stages, the longer the training process will take since it becomes harder at each stage to find enough training windows and to find features that correctly separate the data. Moreover, the training time increases in an exponential manner when adding stages.\n*   Therefore, I suggest looking at the reported acceptance ratio that is outputted at each training stage. Once this reaches values of 10^(-5), you can conclude that your model will have reached the best descriptive and generalizing power it could get, according to the training data provided.\n*   Avoid training it to levels of 10^(-5) or lower to avoid overtraining your cascade on your training data. Of course, depending on the amount of training data supplied, the amount of stages to reach this level can differ a lot.\n\n`-bg`: This refers to the location of the text file that contains the locations of the negative training images, also called the negative samples description file.`-vec`: This refers to the location of the training data vector that was generated in the previous step using the create_samples application, which is built-in to the OpenCV 3 software.`-precalcValBufSize` and `-precalcIdxBufSize`: These parameters assign the amount of memory used to calculate all features and the corresponding weak classifiers from the training data. If you have enough RAM memory available, increase these values to 2048 MB or 4096 MB, which will speed up the precalculation time for the features drastically.`-featureType`: Here, you can choose which kind of features are used for creating the weak classifiers.\n\n*   HAAR wavelets are reported to give higher accuracy models.\n*   However, consider training test classifiers with the LBP parameter. It decreases training time of an equal sized model drastically due to the integer calculations instead of the floating point calculations.\n\n`-minHitRate`: This is the threshold that defines how much of your positive samples can be misclassified as negatives at each stage. The default value is 0.995, which is already quite high. The training algorithm will select its stage threshold so that this value can be reached.\n\n*   Making it 0.999, as many people do, is simply impossible and will make your training stop probably after the first stage. It means that only 1 out of 1,000 samples can be wrongly classified over a complete stage.\n*   If you have very challenging data, then lowering this, for example, to 0.990 could be a good start to ensure that the training actually ends up with a useful model.\n\n`-maxFalseAlarmRate`: This is the threshold that defines how much of your negative samples need to be classified as negatives before the boosting process should stop adding weak classifiers to the current stage. The default value is 0.5 and ensures that a stage of weak classifier will only do slightly better than random guessing on the negative samples. Increasing this value too much could lead to a single stage that already filters out most of your given windows, resulting in a very slow model at detection time due to the vast amount of features that need to be validated for each window. This will simply remove the large advantage of the concept of early window rejection.\n```", "```py\nPOS:number_pos_samples_grabbed:total_number_pos_samples_needed NEG:number_neg_samples_grabbed:acceptanceRatioAchieved\n\n```", "```py\n<stages>\n    <_>\n        <maxWeakCount></maxWeakCount>\n        <stageThreshold</stageThreshold>\n        <weakClassifiers>\n            <!-- tree 0 -->\n            <_>\n                <internalNodes></internalNodes>\n                <leafValues></leafValues></_>\n            <!-- tree 1 -->\n            <_>\n                <internalNodes></internalNodes>\n                <leafValues></leafValues></_>\n            <!-- tree 2 -->\n            … … …\n    <!-- stage 1 -->\n    … … …\n</stages>\n<features>\n    … … …\n</features>\n```", "```py\n<internalNodes>\n0 -1 445 -1.4772760681807995e-02\n</internalNodes>\n… … …\n<_>\n    <rects>\n        <_>23 10 1 3 -1.</_>\n        <_>23 11 1 1 3.</_>\n    </rects>\n    <tilted>0</tilted>\n</_>\n```", "```py\n<internalNodes>\n0 -1 46 -67130709 -21569 -1426120013 -1275125205 -21585\n-16385 587145899 -24005\n</internalNodes>\n… … …\n<_>\n    <rect>0 0 3 5</rect>\n</_>\n```", "```py\nNoteThe software for visualizing Haar wavelet or LBP models can be found at [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/visualize_models/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/visualize_models/).\n```", "```py\nvoid CascadeClassifier::detectMultiScale(InputArray image, vector<Rect>& objects, double scaleFactor=1.1, int minNeighbors=3, int flags=0, Size minSize=Size(), Size maxSize=Size())\n```", "```py\nvoid CascadeClassifier::detectMultiScale(InputArray image, vector<Rect>& objects, vector<int>& numDetections, double scaleFactor=1.1, int minNeighbors=3, int flags=0, Size minSize=Size(), Size maxSize=Size())\n```", "```py\nvoid CascadeClassifier::detectMultiScale(InputArray image, std::vector<Rect>& objects, std::vector<int>& rejectLevels, std::vector<double>& levelWeights, double scaleFactor=1.1, int minNeighbors=3, int flags=0, Size minSize=Size(), Size maxSize=Size(), bool outputRejectLevels=false )\n```", "```py\nTipSoftware for performing rotation invariant object detection based on the described third approach can be found at [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/rotation_invariant_detection/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/rotation_invariant_detection/).\n```", "```py\n// Create the 3D model matrix of the input image\nMat image = imread(input_image);\nint steps = max_angle / step_angle;\nvector<Mat> rotated_images;\ncvtColor(rotated, rotated, COLOR_BGR2GRAY);\nequalizeHist( rotated, rotated );\nfor (int i = 0; i < steps; i ++){\n   // Rotate the image\n   Mat rotated = image.clone();\n   rotate(image, (i+1)*step_angle, rotated);\n   // Preprocess the images\n\n   // Add to the collection of rotated and processed images\n   rotated_images.push_back(rotated);\n}\n```", "```py\nvoid rotate(Mat& src, double angle, Mat& dst)\n{\n    Point2f pt(src.cols/2., src.rows/2.);\n    Mat r = getRotationMatrix2D(pt, angle, 1.0);\n    warpAffine(src, dst, r, cv::Size(src.cols, src.rows));\n}\n```", "```py\nSize dimensions = image.size();\nif(dimensions.rows > dimensions.cols){\n   Mat temp = Mat::ones(dimensions.rows, dimensions.rows, image.type()) * 255;\n   int extra_rows = dimensions.rows - dimensions.cols;\n   image.copyTo(temp(0, extra_rows/2, image.rows, image.cols));\n   image = temp.clone();\n}\nif(dimensions.cols > dimensions.rows){\n   Mat temp = Mat::ones(dimensions.cols, dimensions.cols, image.type()) * 255;\n   int extra_cols = dimensions.cols - dimensions.rows;\n   image.copyTo(temp(extra_cols/2, 0, image.rows, image.cols));\n   image = temp.clone();\n}\n```"]