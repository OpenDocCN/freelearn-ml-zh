<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer107">
    <h1 class="chapterNumber">3</h1>
    <h1 class="chapterTitle" id="_idParaDest-63">Lazy Learning – Classification Using Nearest Neighbors</h1>
    <p class="normal">A curious type of dining experience has appeared in cities around the world. Patrons are served in a completely darkened restaurant by waiters who move via memorized routes, using only their senses of touch and sound. The allure of these establishments is the belief that depriving oneself of sight will enhance the senses of taste and smell, and foods will be experienced in new ways. Each bite provides a sense of wonder while discovering the flavors the chef has prepared.</p>
    <p class="normal">Can you imagine how a diner experiences the unseen food? Upon first bite, the senses are overwhelmed. What are the dominant flavors? Does the food taste savory or sweet? Does it taste like something they’ve eaten previously? Personally, I imagine this process of discovery in terms of a slightly modified adage—if it smells like a duck and tastes like a duck, then you are probably eating duck.</p>
    <p class="normal">This illustrates an idea that can be used for machine learning—as does another maxim involving poultry—birds of a feather flock together. Stated differently, things that are alike tend to have properties that are alike. Machine learning uses this principle to classify data by placing it in the same category as similar or “nearest” neighbors. This chapter is devoted to classifiers that use this approach. You will learn:</p>
    <ul>
      <li class="bulletList">The key concepts that define nearest neighbor classifiers and why they are considered “lazy” learners</li>
      <li class="bulletList">Methods to measure the similarity of two examples using distance</li>
      <li class="bulletList">How to apply a popular nearest neighbor classifier called k-NN</li>
    </ul>
    <p class="normal">If all of this talk about food is making you hungry, our first task will be to understand the k-NN approach by putting it to use while we settle a long-running culinary debate.</p>
    <h1 class="heading-1" id="_idParaDest-64">Understanding nearest neighbor classification</h1>
    <p class="normal">In a single sentence, <strong class="keyWord">nearest neighbor</strong> classifiers are defined<a id="_idIndexMarker294"/> by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples. This is analogous to the dining experience described in the chapter introduction, in which a person identifies new foods through comparison to those previously encountered. With nearest neighbor classification, computers apply a human-like ability to recall past experiences to make conclusions about current circumstances. Despite the simplicity of this idea, nearest neighbor methods are extremely powerful. They have been used successfully for:</p>
    <ul>
      <li class="bulletList">Computer vision applications, including<a id="_idIndexMarker295"/> optical character recognition and facial recognition in still images and video</li>
      <li class="bulletList">Recommendation systems that predict<a id="_idIndexMarker296"/> whether a person will enjoy a movie or song</li>
      <li class="bulletList">Identifying patterns in genetic data<a id="_idIndexMarker297"/> to detect specific proteins or diseases</li>
    </ul>
    <p class="normal">In general, nearest neighbor classifiers are well suited for classification tasks where relationships among the features and the target classes are numerous, complicated, or otherwise extremely difficult to understand, yet the items of similar class types tend to be fairly homogeneous. Another way of putting it would be to say that if a concept is difficult to define, but you know it when you see it, then nearest neighbors might be appropriate. On the other hand, if the data is noisy and thus no clear distinction exists among the groups, nearest neighbor algorithms may struggle to identify the class boundaries.</p>
    <h2 class="heading-2" id="_idParaDest-65">The k-NN algorithm</h2>
    <p class="normal">The nearest neighbors approach<a id="_idIndexMarker298"/> to classification is exemplified by the <strong class="keyWord">k-nearest neighbors</strong> algorithm (<strong class="keyWord">k-NN</strong>). Although this is perhaps one of the simplest machine learning algorithms, it is still used widely. The strengths and weaknesses<a id="_idIndexMarker299"/> of this algorithm are as follows:</p>
    <table class="table-container" id="table001-1">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Simple and effective</li>
              <li class="bulletList">Makes no assumptions about the underlying data distribution</li>
              <li class="bulletList">Fast training phase</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Does not produce a model, limiting the ability to understand how the features are related to the class</li>
              <li class="bulletList">Requires selection of an appropriate <em class="italic">k</em></li>
              <li class="bulletList">Slow classification phase</li>
              <li class="bulletList">Nominal features and missing data require additional processing</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">The k-NN algorithm<a id="_idIndexMarker300"/> gets its name from the fact that it uses information about an example’s <em class="italic">k</em> nearest neighbors to classify unlabeled examples. The letter <em class="italic">k</em> is a variable implying that any number of nearest neighbors could be used. After choosing <em class="italic">k</em>, the algorithm requires a training dataset made up of examples that have been classified into several categories, as labeled by a nominal variable. Then, for each unlabeled record in the test dataset, k-NN identifies the <em class="italic">k</em> records in the training data that are the “nearest” in similarity. The unlabeled test instance is assigned the class representing the majority of the <em class="italic">k</em> nearest neighbors.</p>
    <p class="normal">To illustrate this process, let’s revisit the blind tasting experience described in the introduction. Suppose that prior to eating the mystery meal, we had created a dataset in which we recorded our impressions of a set of previously tasted ingredients. To keep things simple, we rated only two features of each ingredient. The first is a measure from 1 to 10 of how crunchy the ingredient is, and the second is a score from 1 to 10 measuring how sweet the ingredient tastes. We then labeled each ingredient as one of three types of food: fruits, vegetables, or proteins, ignoring other foods such as grains and fats.</p>
    <p class="normal">The first few rows of such a dataset might be structured as follows:</p>
    <table class="table-container" id="table002-1">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Ingredient</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Sweetness</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Crunchiness</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Food type</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Apple</p>
          </td>
          <td class="table-cell">
            <p class="normal">10</p>
          </td>
          <td class="table-cell">
            <p class="normal">9</p>
          </td>
          <td class="table-cell">
            <p class="normal">Fruit</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Bacon</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">4</p>
          </td>
          <td class="table-cell">
            <p class="normal">Protein</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Banana</p>
          </td>
          <td class="table-cell">
            <p class="normal">10</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">Fruit</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Carrot</p>
          </td>
          <td class="table-cell">
            <p class="normal">7</p>
          </td>
          <td class="table-cell">
            <p class="normal">10</p>
          </td>
          <td class="table-cell">
            <p class="normal">Vegetable</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Celery</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">10</p>
          </td>
          <td class="table-cell">
            <p class="normal">Vegetable</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">The k-NN algorithm treats the features<a id="_idIndexMarker301"/> as coordinates in a multidimensional <strong class="keyWord">feature space</strong>, which is a space comprising all possible<a id="_idIndexMarker302"/> combinations of feature values. Because the ingredient dataset includes only two features, its feature space is two-dimensional. We can plot two-dimensional data on a scatterplot, with the <em class="italic">x</em> dimension indicating the ingredient’s sweetness and the <em class="italic">y</em> dimension indicating the crunchiness. After adding a few more ingredients to the taste dataset, the scatterplot might look like this:</p>
    <figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" src="../Images/B17290_03_01.png"/></figure>
    <p class="packt_figref">Figure 3.1: A scatterplot of selected foods’ crunchiness versus sweetness</p>
    <p class="normal">Do you notice a pattern? Similar types of food tend to be grouped closely together. As illustrated in <em class="italic">Figure 3.2</em>, vegetables tend to be crunchy but not sweet; fruits tend to be sweet and either crunchy or not crunchy; and proteins tend to be neither crunchy nor sweet:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_03_02.png"/></figure>
    <p class="packt_figref">Figure 3.2: Foods that are similarly classified tend to have similar attributes</p>
    <p class="normal">Suppose that after constructing<a id="_idIndexMarker303"/> this dataset, we decide to use it to settle the age-old question: is a tomato a fruit or a vegetable? We can use the nearest neighbor approach to determine which class is a better fit, as shown in <em class="italic">Figure 3.3</em>:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_03_03.png"/></figure>
    <p class="packt_figref">Figure 3.3: The tomato’s nearest neighbors provide insight into whether it is a fruit or vegetable</p>
    <h3 class="heading-3" id="_idParaDest-66">Measuring similarity with distance</h3>
    <p class="normal">Locating the tomato’s nearest<a id="_idIndexMarker304"/> neighbors requires a <strong class="keyWord">distance function</strong>, which is a formula that measures<a id="_idIndexMarker305"/> the similarity between two instances.</p>
    <p class="normal">There are many ways to calculate distance. The choice of distance function may impact the model’s performance substantially, although it is difficult to know which to use except by comparing them directly on the desired learning task. Traditionally, the k-NN algorithm uses <strong class="keyWord">Euclidean distance</strong>, which is the distance one would<a id="_idIndexMarker306"/> measure if it were possible to use a ruler to connect two points. Euclidean distance is measured “as the crow flies,” which implies the shortest direct route. This is illustrated in the previous figure by the dotted lines connecting the tomato to its neighbors.</p>
    <div class="packt_tip">
      <p class="normal">Another common distance measure is <strong class="keyWord">Manhattan distance</strong>, which is based on the paths a pedestrian<a id="_idIndexMarker307"/> would take by walking city blocks. If you are interested in learning more about other distance measures, you can read the documentation for R’s distance function using the <code class="inlineCode">?dist</code> command.</p>
    </div>
    <p class="normal">Euclidean distance is specified by the following formula, where <em class="italic">p</em> and <em class="italic">q</em> are the examples to be compared, each having <em class="italic">n</em> features. The term <em class="italic">p</em><sup class="superscript-italic" style="font-style: italic;">1</sup> refers to the value of the first feature of example <em class="italic">p</em>, while <em class="italic">q</em><sup class="superscript-italic" style="font-style: italic;">1</sup> refers to the value of the first feature of example <em class="italic">q</em>:</p>
    <p class="center"><img alt="" src="../Images/B17290_03_001.png"/></p>
    <p class="normal">The distance formula involves comparing the values of each example’s features. For example, to calculate the distance between the tomato (sweetness = 6, crunchiness = 4), and the green bean (sweetness = 3, crunchiness = 7), we can use the formula as follows:</p>
    <p class="center"><img alt="" src="../Images/B17290_03_002.png"/></p>
    <p class="normal">In a similar vein, we can calculate the distance between the tomato and several of its closest neighbors as follows:</p>
    <table class="table-container" id="table003">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Ingredient</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Sweetness</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Crunchiness</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Food type</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Distance to the tomato</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Grape</p>
          </td>
          <td class="table-cell">
            <p class="normal">8</p>
          </td>
          <td class="table-cell">
            <p class="normal">5</p>
          </td>
          <td class="table-cell">
            <p class="normal">Fruit</p>
          </td>
          <td class="table-cell">
            <p class="normal">sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Green bean</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">7</p>
          </td>
          <td class="table-cell">
            <p class="normal">Vegetable</p>
          </td>
          <td class="table-cell">
            <p class="normal">sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Nuts</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">6</p>
          </td>
          <td class="table-cell">
            <p class="normal">Protein</p>
          </td>
          <td class="table-cell">
            <p class="normal">sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Orange</p>
          </td>
          <td class="table-cell">
            <p class="normal">7</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">Fruit</p>
          </td>
          <td class="table-cell">
            <p class="normal">sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">To classify the tomato as a vegetable, protein, or fruit, we’ll begin by assigning the tomato the food type of its single<a id="_idIndexMarker308"/> nearest neighbor. This is called 1-NN classification because <em class="italic">k = 1</em>. The orange is the single nearest neighbor to the tomato, with a distance of 1.4. Because an orange is a fruit, the 1-NN algorithm would classify a tomato as a fruit.</p>
    <p class="normal">If we use the k-NN algorithm with <em class="italic">k = 3</em> instead, it performs a vote among the three nearest neighbors: orange, grape, and nuts. Now, because the majority<a id="_idIndexMarker309"/> class among these neighbors is fruit (with two of the three votes), the tomato again is classified as a fruit.</p>
    <h3 class="heading-3" id="_idParaDest-67">Choosing an appropriate k</h3>
    <p class="normal">The decision of how many<a id="_idIndexMarker310"/> neighbors to use for k-NN determines how well the model will generalize to future data. The balance between overfitting and underfitting <a id="_idIndexMarker311"/>the training data is a problem known as the <strong class="keyWord">bias-variance tradeoff</strong>. Choosing a large <em class="italic">k</em> reduces the impact of variance caused by noisy data but can bias the learner such that it runs the risk of ignoring small but important patterns.</p>
    <p class="normal">Suppose we took the extreme stance of setting a very large <em class="italic">k</em>, as large as the total number of observations in the training data. With every training instance represented in the final vote, the most common class always has a majority of the voters. The model would consequently always predict the majority class, regardless of the nearest neighbors.</p>
    <p class="normal">On the opposite extreme, using a single nearest neighbor allows noisy data and outliers to unduly influence the classification of examples. For example, suppose some of the training examples were accidentally mislabeled. Any unlabeled example that happens to be nearest to the incorrectly labeled neighbor will be predicted to have the incorrect class, even if nine other nearby neighbors would have voted differently.</p>
    <p class="normal">Obviously, the best <em class="italic">k</em> value is somewhere between these two extremes.</p>
    <p class="normal"><em class="italic">Figure 3.4</em> illustrates, more generally, how the decision boundary (depicted by a dashed line) is affected by larger <a id="_idIndexMarker312"/>or smaller <em class="italic">k</em> values. Smaller values allow more complex decision boundaries that more carefully fit the training data. The problem is that we do not know whether the straight boundary or the curved boundary better represents the true underlying concept to be learned.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_03_04.png"/></figure>
    <p class="packt_figref">Figure 3.4: A larger k has higher bias and lower variance than a smaller k</p>
    <p class="normal">In practice, the choice of <em class="italic">k</em> depends on the difficulty of the concept to be learned and the number of records in the training data. One common approach is to begin with <em class="italic">k</em> equal to the square root of the number of training examples. In the food classifier developed previously, we might set <em class="italic">k = 4</em> because there were 15 example ingredients in the training data and the square root of 15 is 3.87.</p>
    <p class="normal">However, such rules may not always result in the single best <em class="italic">k</em>. An alternative approach is to test several <em class="italic">k</em> values on a variety of test datasets and choose the one that delivers the best classification performance. That said, unless the data is very noisy, a large training dataset can make the choice of <em class="italic">k</em> less important. This is because even subtle concepts will have a sufficiently large pool of examples to vote as nearest neighbors.</p>
    <div class="packt_tip">
      <p class="normal">A less common, but still interesting, solution to this problem is to choose a larger <em class="italic">k</em> and use a weighted voting process in which the vote of closer neighbors is considered more authoritative<a id="_idIndexMarker313"/> than the vote of neighbors that are far away. Some k-NN implementations offer this option.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-68">Preparing data for use with k-NN</h3>
    <p class="normal">Features are typically transformed<a id="_idIndexMarker314"/> to a standard range prior to applying the k-NN algorithm. The rationale for this step is that the distance formula is highly dependent on how features are measured. In particular, if certain features have a much larger range of values than others, the distance measurements will be strongly dominated by the features with larger ranges. This wasn’t a problem for the food tasting example, as both sweetness and crunchiness were measured on a scale from 1 to 10.</p>
    <p class="normal">However, suppose that we added an additional feature to the dataset to represent a food’s spiciness, which was measured using the Scoville scale. If you are unfamiliar with this metric, it is a standardized measure of spice heat, ranging from zero (not at all spicy) to over a million (for the hottest chili peppers). Since the difference between spicy and non-spicy foods can be over a million while the difference between sweet and non-sweet or crunchy and non-crunchy foods is at most 10, the difference in scale allows the spice level to impact the distance function much more than the other two factors. Without adjusting our data, we might find that our distance measures only differentiate foods by their spiciness; the impact of crunchiness and sweetness would be dwarfed by the contribution of spiciness.</p>
    <p class="normal">The solution is to rescale the features by shrinking or expanding their range such that each one contributes relatively equally to the distance formula. For example, if sweetness and crunchiness are both measured on a scale from 1 to 10, we would also like spiciness to be measured on a scale from 1 to 10. There are several common ways to accomplish such scaling.</p>
    <p class="normal">The traditional method of rescaling features for k-NN is <strong class="keyWord">min-max normalization</strong>. This process transforms<a id="_idIndexMarker315"/> a feature such that all values fall in a range between 0 and 1. The formula for normalizing a feature is as follows:</p>
    <p class="center"><img alt="" src="../Images/B17290_03_003.png"/></p>
    <p class="normal">To transform each value of feature <em class="italic">X</em>, the formula subtracts the minimum <em class="italic">X</em> value and divides it by the range of <em class="italic">X</em>. The resulting normalized feature values<a id="_idIndexMarker316"/> can be interpreted as indicating how far, from 0 percent to 100 percent, the original value fell along the range between the original minimum and maximum.</p>
    <p class="normal">Another common transformation is called <strong class="keyWord">z-score standardization</strong>. The following formula subtracts<a id="_idIndexMarker317"/> the mean value of feature <em class="italic">X</em>, and divides the result by the standard deviation of <em class="italic">X</em>:</p>
    <p class="center"><img alt="" src="../Images/B17290_03_004.png"/></p>
    <p class="normal">This formula, which is based on properties of the normal distribution covered in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>, rescales each of a feature’s values in terms of how many standard deviations they fall above or below the mean. The resulting value is called a <strong class="keyWord">z-score</strong>. The z-scores fall in an unbounded<a id="_idIndexMarker318"/> range of negative and positive numbers. Unlike the normalized values, they have no predefined minimum and maximum.</p>
    <div class="packt_tip">
      <p class="normal">The same rescaling method used on the k-NN training dataset must also be applied to the test examples that the algorithm will later classify. This can lead to a tricky situation<a id="_idIndexMarker319"/> for min-max normalization, as the minimum or maximum of future cases might be outside the range of values observed in the training data. If you know the theoretical minimum or maximum value ahead of time, you can use these constants rather than the observed minimum and maximum values. Alternatively, you can use z-score standardization under the assumption that the future examples are taken from a distribution with the same mean and standard deviation as the training examples.</p>
    </div>
    <p class="normal">The Euclidean distance<a id="_idIndexMarker320"/> formula is undefined for nominal data. Therefore, to calculate the distance between nominal features, we need to convert them into a numeric<a id="_idIndexMarker321"/> format. A typical solution utilizes <strong class="keyWord">dummy coding</strong>, where a value of 1 indicates one category, and 0 indicates the other. For instance, dummy coding for a male or non-male sex variable could be constructed as:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_03_05.png"/></figure>
    <p class="normal">Notice how dummy coding<a id="_idIndexMarker322"/> of the two-category (binary) sex variable results in a single new feature named male. There is no need to construct a separate feature for non-male. Since both are mutually exclusive, knowing one or the other is enough.</p>
    <p class="normal">This is true more generally as well. An <em class="italic">n</em>-category nominal feature can be dummy coded by creating binary indicator variables for <em class="italic">n - 1</em> levels of the feature. For example, dummy coding for a three-category temperature variable (for example, hot, medium, or cold) could be set up as <em class="italic">(3 - 1) = 2</em> features, as shown here:</p>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B17290_03_06.png"/></figure>
    <p class="normal">Knowing that hot and medium are both 0 provides<a id="_idIndexMarker323"/> enough information to know that the temperature is cold, and thus, a third binary feature for the cold category is unnecessary. However, a widely<a id="_idIndexMarker324"/> used close sibling of dummy coding known as <strong class="keyWord">one-hot encoding</strong> creates binary features for all <em class="italic">n</em> levels of the feature, rather than <em class="italic">n - 1</em> as with dummy coding. It is known as “one-hot” because only one attribute is coded as 1 and the others are set to 0.</p>
    <p class="normal">In practice, there is virtually no difference between these two methods, and the results of machine learning will be unaffected by the choice of coding. This being said, one-hot encoding can cause problems with linear models, such as those described in <em class="chapterRef">Chapter 6</em>, <em class="italic">Forecasting Numeric Data – Regression Methods</em>, and thus one-hot encoding is often avoided among statisticians or in fields like economics that rely heavily on such models. On the other hand, one-hot encoding has become prevalent in the field of machine learning and is often treated synonymously with dummy coding for the simple reason that the choice makes virtually no difference in the model fit; yet, in one-hot encoding, the model itself may be easier to understand since all levels of the categorical features are specified explicitly. This book uses only dummy coding since it can be used universally, but you may encounter one-hot encoding elsewhere.</p>
    <p class="normal">A convenient aspect of both dummy and one-hot coding is that the distance between dummy-coded features is always one or zero, and thus, the values fall on the same scale as min-max normalized numeric data. No additional<a id="_idIndexMarker325"/> transformation is necessary.</p>
    <div class="packt_tip">
      <p class="normal">If a nominal feature is ordinal (one could make such an argument for temperature), an alternative to dummy coding<a id="_idIndexMarker326"/> is to number the categories and apply normalization. For instance, cold, warm, and hot could be numbered as 1, 2, and 3, which normalizes to 0, 0.5, and 1. A caveat to this approach is that it should only be used if the steps between categories are equivalent. For instance, although income categories for poor, middle class, and wealthy are ordered, the difference between poor and middle class may be different than the difference between middle class and wealthy. Since the steps between groups are not equal, dummy coding is a safer approach.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-69">Why is the k-NN algorithm lazy?</h2>
    <p class="normal">Classification algorithms<a id="_idIndexMarker327"/> based on nearest neighbor methods are considered <strong class="keyWord">lazy learning</strong> algorithms because, technically<a id="_idIndexMarker328"/> speaking, no abstraction occurs. The abstraction and generalization processes are skipped altogether, which undermines the definition of learning proposed in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introducing Machine Learning</em>.</p>
    <p class="normal">Under the strict definition of learning, a lazy learner is not really learning anything. Instead, it merely stores the training data verbatim. This allows the training phase, which is not actually training anything, to occur very rapidly. Of course, the downside is that the process of making predictions tends to be relatively slow by comparison. Due to the heavy reliance on the training instances<a id="_idIndexMarker329"/> rather than an abstracted<a id="_idIndexMarker330"/> model, lazy learning is also known as <strong class="keyWord">instance-based learning</strong> or <strong class="keyWord">rote learning</strong>.</p>
    <p class="normal">As instance-based learners do not build a model, the method<a id="_idIndexMarker331"/> is said to be in a class of <strong class="keyWord">non-parametric</strong> learning methods—no parameters are learned about the data. Without generating theories about the underlying data, non-parametric methods limit our ability to understand how the classifier is using the data, yet it can still make useful predictions. Non-parametric learning<a id="_idIndexMarker332"/> allows the learner to find natural patterns rather than trying to fit the data into a preconceived and potentially biased functional form.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_03_07.png"/></figure>
    <p class="packt_figref">Figure 3.5: Machine learning algorithms have different biases and may come to different conclusions!</p>
    <p class="normal">Although k-NN classifiers<a id="_idIndexMarker333"/> may be considered lazy, they are still quite powerful. As you will soon see, the simple principles of nearest neighbor learning can be used to automate the process of screening for cancer.</p>
    <h1 class="heading-1" id="_idParaDest-70">Example – diagnosing breast cancer with the k-NN algorithm</h1>
    <p class="normal">Routine breast cancer screening<a id="_idIndexMarker334"/> allows the disease<a id="_idIndexMarker335"/> to be diagnosed and treated prior to it causing noticeable symptoms. The process of early detection involves examining the breast tissue for abnormal lumps or masses. If a lump is found, a fine-needle aspiration biopsy is performed, which uses a hollow needle to extract a small sample of cells from the mass. A clinician then examines the cells under a microscope to determine whether the mass is likely to be malignant or benign.</p>
    <p class="normal">If machine learning could automate the identification of cancerous cells, it would provide considerable benefit to the health system. Automated processes are likely to improve the efficiency of the detection process, allowing physicians to spend less time diagnosing and more time treating the disease. An automated screening system might also provide greater detection accuracy by removing the inherently subjective human component from the process.</p>
    <p class="normal">Let’s investigate the utility of machine<a id="_idIndexMarker336"/> learning for detecting cancer by applying the k-NN algorithm to measurements<a id="_idIndexMarker337"/> of biopsied cells from women with abnormal breast masses.</p>
    <h2 class="heading-2" id="_idParaDest-71">Step 1 – collecting data</h2>
    <p class="normal">We will utilize <a id="_idIndexMarker338"/>the Breast Cancer Wisconsin (Diagnostic) dataset from the UCI Machine Learning Repository at <code class="inlineCode">http://archive.ics.uci.edu/ml</code>. This data was donated by researchers at the University of Wisconsin and includes measurements from digitized images of fine-needle aspirations of a breast mass. The values represent characteristics of the cell nuclei present in the digital image.</p>
    <div class="note">
      <p class="normal">To read more about this dataset, refer to <em class="italic">Breast Cancer Diagnosis and Prognosis via Linear Programming, Mangasarian OL, Street WN, Wolberg WH, Operations Research, 1995, Vol. 43, pp. 570-577</em>.</p>
    </div>
    <p class="normal">The breast cancer data includes 569 examples of cancer biopsies, each with 32 features. One feature is an identification number, another is the cancer diagnosis, and 30 are numeric-valued laboratory measurements. The diagnosis is coded as “M” to indicate malignant or “B” to indicate benign.</p>
    <p class="normal">The 30 numeric measurements comprise the mean, standard error, and worst (that is, largest) value for 10 different characteristics of the digitized cell nuclei, such as radius, texture, area, smoothness, and compactness. Based on the feature names, the dataset seems to measure the shape and size of the cell nuclei, but unless you are an oncologist, you are unlikely to know how each of these relates to benign or malignant masses. No such expertise is necessary, as the computer will discover the important patterns during the machine learning process.</p>
    <h2 class="heading-2" id="_idParaDest-72">Step 2 – exploring and preparing the data</h2>
    <p class="normal">By exploring the data, we may<a id="_idIndexMarker339"/> be able to shine some light <a id="_idIndexMarker340"/>on the relationships between the features and the cancer status. In doing so, we will prepare the data for use with the k-NN learning method.</p>
    <div class="packt_tip">
      <p class="normal">If you plan on following along, download the code and <code class="inlineCode">wisc_bc_data.csv</code> files from the GitHub repository and save them to your R working directory. For this book, the dataset was modified very slightly from its original form. In particular, a header line was added, and the rows of data were randomly ordered.</p>
    </div>
    <p class="normal">We’ll begin by importing the CSV data file as we have done in previous chapters, saving the Wisconsin breast cancer data to the <code class="inlineCode">wbcd</code> data frame:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wbcd &lt;- read.csv(<span class="hljs-string">"wisc_bc_data.csv"</span>)
</code></pre>
    <p class="normal">Using the command <code class="inlineCode">str(wbcd)</code>, we can confirm that the data is structured with 569 examples and 32 features, as we expected. The first several lines of output are as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; str(wbcd)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'data.frame':	569 obs. of  32 variables:
 $ id               : int  87139402 8910251 905520 ...
 $ diagnosis        : chr  "B" "B" "B" "B" ...
 $ radius_mean      : num  12.3 10.6 11 11.3 15.2 ...
 $ texture_mean     : num  12.4 18.9 16.8 13.4 13.2 ...
 $ perimeter_mean   : num  78.8 69.3 70.9 73 97.7 ...
 $ area_mean        : num  464 346 373 385 712 ...
</code></pre>
    <p class="normal">The first feature is an integer variable named <code class="inlineCode">id</code>. As this is simply a unique identifier (ID) for each patient in the data, it does not provide useful information, and we will need to exclude it from the model.</p>
    <div class="packt_tip">
      <p class="normal">Regardless of the machine learning method, ID variables should always be excluded. Neglecting to do so can lead to erroneous findings because the ID can be used to correctly predict each example. Therefore, a model that includes an ID column will almost definitely suffer from overfitting and generalize poorly to future data.</p>
    </div>
    <p class="normal">Let’s drop the <code class="inlineCode">id</code> feature from our data frame. As it is in the first column, we can exclude it by making a copy of the <code class="inlineCode">wbcd</code> data frame without column 1:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wbcd &lt;- wbcd[<span class="hljs-number">-1</span>]
</code></pre>
    <p class="normal">The next feature, <code class="inlineCode">diagnosis</code>, is of particular interest as it is the target outcome we hope to predict. This feature indicates whether the example is from a benign or malignant mass. The <code class="inlineCode">table()</code> output indicates that 357 masses are benign, while 212 are malignant:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; table(wbcd$diagnosis)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  B   M 
357 212
</code></pre>
    <p class="normal">Many R machine learning<a id="_idIndexMarker341"/> classifiers require<a id="_idIndexMarker342"/> the target feature to be coded as a factor, so we will need to recode the <code class="inlineCode">diagnosis</code> column. We will also take this opportunity to give the <code class="inlineCode">"B"</code> and <code class="inlineCode">"M"</code> values more informative labels using the <code class="inlineCode">labels</code> parameter:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wbcd$diagnosis &lt;- factor(wbcd$diagnosis, levels = <span class="hljs-built_in">c</span>(<span class="hljs-string">"</span><span class="hljs-string">B"</span>, <span class="hljs-string">"M"</span>),
                           labels = <span class="hljs-built_in">c</span>(<span class="hljs-string">"Benign"</span>, <span class="hljs-string">"Malignant"</span>))
</code></pre>
    <p class="normal">When we look at the <code class="inlineCode">prop.table()</code> output, we now see that the values have been labeled <code class="inlineCode">Benign</code> and <code class="inlineCode">Malignant</code>, with 62.7 percent and 37.3 percent of the masses, respectively:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; <span class="hljs-built_in">round</span>(prop.table(table(wbcd$diagnosis)) * <span class="hljs-number">100</span>, digits = <span class="hljs-number">1</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Benign Malignant 
     62.7      37.3
</code></pre>
    <p class="normal">The remaining 30 features are all numeric and, as expected, consist of three different measurements of 10 characteristics. For illustrative purposes, we will only take a closer look at three of these features:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; summary(wbcd[<span class="hljs-built_in">c</span>(<span class="hljs-string">"radius_mean"</span>, <span class="hljs-string">"area_mean"</span>, <span class="hljs-string">"smoothness_mean"</span>)])
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  radius_mean       area_mean      smoothness_mean  
 Min.   : 6.981   Min.   : 143.5   Min.   :0.05263  
 1st Qu.:11.700   1st Qu.: 420.3   1st Qu.:0.08637  
 Median :13.370   Median : 551.1   Median :0.09587  
 Mean   :14.127   Mean   : 654.9   Mean   :0.09636  
 3rd Qu.:15.780   3rd Qu.: 782.7   3rd Qu.:0.10530  
 Max.   :28.110   Max.   :2501.0   Max.   :0.16340  
</code></pre>
    <p class="normal">Looking at the three side by side, do you notice anything problematic about the values? Recall that the distance calculation for k-NN is heavily dependent upon the measurement scale of the input features. Since smoothness ranges from 0.05 to 0.16, while area ranges from 143.5 to 2501.0, the impact of area is going to be much greater than smoothness in the distance<a id="_idIndexMarker343"/> calculation. This could potentially<a id="_idIndexMarker344"/> cause problems for our classifier, so let’s apply normalization to rescale the features to a standard range of values.</p>
    <h3 class="heading-3" id="_idParaDest-73">Transformation – normalizing numeric data</h3>
    <p class="normal">To normalize<a id="_idIndexMarker345"/> these features, we need to create a <code class="inlineCode">normalize()</code> function in R. This function takes a vector <code class="inlineCode">x</code> of numeric values, and for each value in <code class="inlineCode">x</code>, subtracts the minimum <code class="inlineCode">x</code> value and divides it by the range of <code class="inlineCode">x</code> values. Lastly, the resulting vector is returned. The code for the function is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; normalize &lt;- <span class="hljs-built_in">function</span>(x) {
      <span class="hljs-built_in">return</span> ((x - <span class="hljs-built_in">min</span>(x)) / (<span class="hljs-built_in">max</span>(x) - <span class="hljs-built_in">min</span>(x)))
}
</code></pre>
    <p class="normal">After executing the previous code, the <code class="inlineCode">normalize()</code> function is available for use in R. Let’s test the function on a couple of vectors:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; normalize(<span class="hljs-built_in">c</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.00 0.25 0.50 0.75 1.00
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; normalize(<span class="hljs-built_in">c</span>(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>, <span class="hljs-number">50</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.00 0.25 0.50 0.75 1.00
</code></pre>
    <p class="normal">The function appears to be working correctly. Even though the values in the second vector are 10 times larger than the first vector, after normalization, they are identical.</p>
    <p class="normal">We can now apply the <code class="inlineCode">normalize()</code> function to the numeric features in our data frame. Rather than normalizing each of the 30 numeric variables individually, we will use one of R’s functions to automate the process.</p>
    <p class="normal">The <code class="inlineCode">lapply()</code> function takes a list and applies a specified function to each list element. As a data frame is a list of equal-length vectors, we can use <code class="inlineCode">lapply()</code> to apply <code class="inlineCode">normalize()</code> to each feature in the data frame. The final step is to convert the list returned by <code class="inlineCode">lapply()</code> to a data frame using the <code class="inlineCode">as.data.frame()</code> function. The full process looks like this:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wbcd_n &lt;- as.data.frame(lapply(wbcd[<span class="hljs-number">2:31</span>], normalize))
</code></pre>
    <p class="normal">In plain English, this command applies the <code class="inlineCode">normalize()</code> function to columns 2 to 31 in the <code class="inlineCode">wbcd</code> data frame, converts the resulting list to a data frame, and assigns it the name <code class="inlineCode">wbcd_n</code>. The <code class="inlineCode">_n</code> suffix is used here as a reminder that the values in <code class="inlineCode">wbcd</code> have been normalized.</p>
    <p class="normal">To confirm that the transformation was applied correctly, let’s look at one variable’s summary statistics:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; summary(wbcd_n$area_mean)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Min.    1st Qu. Median  Mean    3rd Qu. Max.
0.0000  0.1174  0.1729  0.2169  0.2711  1.0000
</code></pre>
    <p class="normal">As expected, the <code class="inlineCode">area_mean</code> variable, which originally<a id="_idIndexMarker346"/> ranged from 143.5 to 2501.0, now ranges from 0 to 1.</p>
    <div class="packt_tip">
      <p class="normal">To simplify data preparation for this example, min-max normalization<a id="_idIndexMarker347"/> was applied to the entire dataset—including the rows that will later become the test set. In a way, this violates our simulation of unseen future data since, in practice, one will generally not know the true minimum and maximum values at the time of model training and future values might fall outside the previously observed range. A better approach might be to normalize the test set using only the minimum and maximum values observed in the training data, and potentially even capping any future values at the prior minimum or maximum levels. This being said, whether normalization is applied to training and test sets together or separately is unlikely to notably impact the model’s performance and does not do so here.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-74">Data preparation – creating training and test datasets</h3>
    <p class="normal">Although all 569 biopsies<a id="_idIndexMarker348"/> are labeled with a benign<a id="_idIndexMarker349"/> or malignant status, it is not very interesting to predict what we already know. Additionally, any performance measures we obtain during training may be misleading, as we do not know the extent to which the data has been overfitted or how well the learner will generalize to new cases. For these reasons, a more interesting question is how well our learner performs on a dataset of unseen data. If we had access to a laboratory, we could apply our learner to measurements taken from the next 100 masses of unknown cancer status and see how well the machine learner’s predictions compare to diagnoses obtained using conventional methods.</p>
    <p class="normal">In the absence of such data, we can simulate this scenario by dividing our data into two portions: a training dataset that will be used to build the k-NN model and a test dataset that will be used to estimate the predictive accuracy of the model. We will use the first 469 records for the training dataset and the remaining 100 to simulate new patients.</p>
    <p class="normal">Using the data extraction methods presented in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>, we will split the <code class="inlineCode">wbcd_n</code> data frame into <code class="inlineCode">wbcd_train</code> and <code class="inlineCode">wbcd_test</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wbcd_train &lt;- wbcd_n[<span class="hljs-number">1:469</span>, ]
&gt; wbcd_test &lt;- wbcd_n[<span class="hljs-number">470:569</span>, ]
</code></pre>
    <p class="normal">If the previous commands<a id="_idIndexMarker350"/> are confusing, remember<a id="_idIndexMarker351"/> that data is extracted from data frames using the <code class="inlineCode">[row, column]</code> syntax. A blank value for the row or column value indicates that all rows or columns should be included. Hence, the first line of code requests rows 1 to 469 and all columns, and the second line requests 100 rows from 470 to 569 and all columns.</p>
    <div class="packt_tip">
      <p class="normal">When constructing training and test datasets, it is important that each dataset is a representative subset of the full set of data. The <code class="inlineCode">wbcd</code> records were already randomly ordered, so we could simply extract 100 consecutive records to create a representative test dataset. This would not be appropriate if the data was ordered chronologically or in groups of similar values. In these cases, random sampling methods would be needed. Random sampling will be discussed in <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>.</p>
    </div>
    <p class="normal">When we constructed our normalized training and test datasets, we excluded the target variable, <code class="inlineCode">diagnosis</code>. For training the k-NN model, we will need to store these class labels in factor vectors, split between the training and test datasets:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wbcd_train_labels &lt;- wbcd[<span class="hljs-number">1:469</span>, <span class="hljs-number">1</span>]
&gt; wbcd_test_labels &lt;- wbcd[<span class="hljs-number">470:569</span>, <span class="hljs-number">1</span>]
</code></pre>
    <p class="normal">This code takes the <code class="inlineCode">diagnosis</code> factor in the first column of the <code class="inlineCode">wbcd</code> data frame and creates the vectors <code class="inlineCode">wbcd_train_labels</code> and <code class="inlineCode">wbcd_test_labels</code>. We will use these in the next steps of training<a id="_idIndexMarker352"/> and evaluating<a id="_idIndexMarker353"/> our classifier.</p>
    <h2 class="heading-2" id="_idParaDest-75">Step 3 – training a model on the data</h2>
    <p class="normal">Equipped with our training data<a id="_idIndexMarker354"/> and vector of labels, we are now ready to classify our test records. For the k-NN algorithm, the training phase involves no model building; the process of training a so-called “lazy” learner like k-NN simply involves storing the input data in a structured format.</p>
    <p class="normal">To classify our test instances, we will use a k-NN implementation from the <code class="inlineCode">class</code> package, which provides a set of basic R functions for classification. If this package is not already installed on your system, you can install it by typing:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; install.packages(<span class="hljs-string">"class"</span>)
</code></pre>
    <p class="normal">To load the package during any session in which you wish to use the functions, simply enter the <code class="inlineCode">library(class)</code> command.</p>
    <p class="normal">The <code class="inlineCode">knn()</code> function in the <code class="inlineCode">class</code> package provides a standard, traditional implementation of the k-NN algorithm. For each instance in the test data, the function will identify the <em class="italic">k</em> nearest neighbors, using Euclidean distance, where <em class="italic">k</em> is a user-specified number. The test instance is classified by taking a “vote” among the <em class="italic">k</em> nearest neighbors—specifically, this involves assigning the class of the majority of neighbors. A tie vote is broken at random.</p>
    <div class="packt_tip">
      <p class="normal">There are several other k-NN functions in other R packages that provide more sophisticated or more efficient <a id="_idIndexMarker355"/>implementations. If you run into limitations with <code class="inlineCode">knn()</code>, search for k-NN on the CRAN website: <a href="https://cran.r-project.org"><span class="url">https://cran.r-project.org</span></a>.</p>
    </div>
    <p class="normal">Training and classification using the <code class="inlineCode">knn()</code> function is performed in a single<a id="_idIndexMarker356"/> command that requires four parameters, as shown in the following table:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" src="../Images/B17290_03_08.png"/></figure>
    <p class="packt_figref">Figure 3.6: kNN classification syntax</p>
    <p class="normal">We now have nearly everything we need to apply the k-NN algorithm to this data. We’ve split our data into training and test datasets, each with the same numeric features. The labels for the training data are stored in a separate factor vector. The only remaining parameter is <code class="inlineCode">k</code>, which specifies the number of neighbors to include in the vote.</p>
    <p class="normal">As our training data includes 469 instances, we might try <code class="inlineCode">k = 21</code>, an odd number roughly equal to the square root of 469. With a two-category outcome, using an odd number eliminates the possibility of ending with a tie vote.</p>
    <p class="normal">Now we can use the <code class="inlineCode">knn()</code> function to classify the test data:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test,
                        cl = wbcd_train_labels, k = <span class="hljs-number">21</span>)
</code></pre>
    <p class="normal">The <code class="inlineCode">knn()</code> function<a id="_idIndexMarker357"/> returns a factor vector of predicted labels for each of the examples in the <code class="inlineCode">wbcd_test</code> dataset. We have assigned these predictions to <code class="inlineCode">wbcd_test_pred</code>.</p>
    <h2 class="heading-2" id="_idParaDest-76">Step 4 – evaluating model performance</h2>
    <p class="normal">The next step of the process<a id="_idIndexMarker358"/> is to evaluate how well the predicted classes in the <code class="inlineCode">wbcd_test_pred</code> vector match the actual values in the <code class="inlineCode">wbcd_test_labels</code> vector. To do this, we can use the <code class="inlineCode">CrossTable()</code> function in the <code class="inlineCode">gmodels</code> package, which was introduced in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>. If you haven’t done so already, please install this package using the <code class="inlineCode">install.packages("gmodels")</code> command.</p>
    <p class="normal">After loading the package with the <code class="inlineCode">library(gmodels)</code> command, we can create a cross tabulation indicating the agreement between the predicted and actual label vectors. Specifying <code class="inlineCode">prop.chisq = FALSE</code> excludes the unnecessary chi-square values from the output:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
             prop.chisq = <span class="hljs-literal">FALSE</span>)
</code></pre>
    <p class="normal">The resulting table looks like this:</p>
    <pre class="programlisting con"><code class="hljs-con">                       | wbcd_test_pred                        
      Wbcd_test_labels |  Benign  |  Malignant  |  Row Total  |
-----------------------|----------|-------------|-------------|
                Benign |       61 |           0 |          61 |
                       |    1.000 |       0.000 |       0.610 |
                       |    0.968 |       0.000 |             |
                       |    0.610 |       0.000 |             |
-----------------------|----------|-------------|-------------|
             Malignant |        2 |          37 |          39 |
                       |    0.051 |       0.949 |       0.390 |
                       |    0.032 |       1.000 |             |
                       |    0.020 |       0.370 |             |
-----------------------|----------|-------------|-------------|
          Column Total |       63 |          37 |         100 |
                       |    0.630 |       0.370 |             |
-----------------------|----------|-------------|-------------|
</code></pre>
    <p class="normal">The cell percentages in the table<a id="_idIndexMarker359"/> indicate the proportion of values that fall into four categories. The top-left cell indicates the <strong class="keyWord">true negative</strong> results. These 61 of 100 values are cases where the mass was benign and the k-NN algorithm correctly identified it as such. The bottom-right cell indicates the <strong class="keyWord">true positive</strong> results, where the classifier and the clinically determined label agree that the mass is malignant. A total of 37 of 100 predictions were true positives.</p>
    <p class="normal">The cells falling on the other diagonal contain counts of examples where the k-NN prediction disagreed with the true label. The two examples in the lower-left cell are <strong class="keyWord">false negative</strong> results; in this case, the predicted value was benign, but the tumor was actually malignant. Errors in this direction could be extremely costly, as they might lead a patient to believe that they are cancer-free, but in reality, the disease may continue to spread.</p>
    <p class="normal">The top-right cell would contain the <strong class="keyWord">false positive</strong> results, if there were any. These values occur when the model has classified a mass as malignant when it actually was benign. Although such errors are less dangerous than a false negative result, they should also be avoided, as they could lead to additional financial burden on the health care system or stress for the patient, as unnecessary tests or treatment may be provided.</p>
    <div class="packt_tip">
      <p class="normal">If we desired, we could eliminate all false negatives by classifying every mass as malignant. Obviously, this is not a realistic strategy. Still, it illustrates the fact that prediction involves striking a balance between the false positive rate and the false negative rate. In <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, you will learn methods for evaluating predictive accuracy that can be used to optimize performance to the costs of each type of error.</p>
    </div>
    <p class="normal">A total of 2 out of 100, or 2 percent of masses were incorrectly classified by the k-NN approach. While 98 percent accuracy seems impressive for a few lines of R code, we might try another iteration of the model to see if we can improve the performance and reduce the number of values<a id="_idIndexMarker360"/> that have been incorrectly classified, especially because the errors were dangerous false negatives.</p>
    <h2 class="heading-2" id="_idParaDest-77">Step 5 – improving model performance</h2>
    <p class="normal">We will attempt two simple<a id="_idIndexMarker361"/> variations on our previous classifier. First, we will employ an alternative method for rescaling our numeric features. Second, we will try several different <em class="italic">k</em> values.</p>
    <h3 class="heading-3" id="_idParaDest-78">Transformation – z-score standardization</h3>
    <p class="normal">Although normalization<a id="_idIndexMarker362"/> is commonly used for k-NN classification, z-score standardization<a id="_idIndexMarker363"/> may be a more appropriate way to rescale the features in a cancer dataset. </p>
    <p class="normal">Since z-score standardized values have no predefined minimum and maximum, extreme values are not compressed towards the center. Even without medical training, one might suspect that a malignant tumor might lead to extreme outliers as tumors grow uncontrollably. With this in mind, it might be reasonable to allow the outliers to be weighted more heavily in the distance calculation. Let’s see whether z-score standardization improves our predictive accuracy.</p>
    <p class="normal">To standardize a vector, we can use R’s built-in <code class="inlineCode">scale()</code> function, which by default rescales values using the z-score standardization. The <code class="inlineCode">scale()</code> function can be applied directly to a data frame, so there is no need to use the <code class="inlineCode">lapply()</code> function. To create a z-score standardized version of the <code class="inlineCode">wbcd</code> data, we can use the following command:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wbcd_z &lt;- as.data.frame(scale(wbcd[<span class="hljs-number">-1</span>]))
</code></pre>
    <p class="normal">This rescales all features with the exception of <code class="inlineCode">diagnosis</code> in the first column and stores the result as the <code class="inlineCode">wbcd_z</code> data frame. The <code class="inlineCode">_z</code> suffix is a reminder that the values were z-score transformed.</p>
    <p class="normal">To confirm that the transformation was applied correctly, we can look at the summary statistics:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; summary(wbcd_z$area_mean)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Min.     1st Qu.  Median   Mean    3rd Qu. Max.
-1.4530  -0.6666  -0.2949  0.0000  0.3632  5.2460
</code></pre>
    <p class="normal">The mean of a z-score standardized variable should always be zero, and the range should be fairly compact. A z-score less than -3 or greater than 3 indicates an extremely rare value. Examining the summary statistics with these criteria in mind, the transformation seems to have worked.</p>
    <p class="normal">As we have done before, we need to divide the z-score-transformed data into training and test sets, and classify the test instances using the <code class="inlineCode">knn()</code> function. We’ll then compare the predicted labels to the actual labels using <code class="inlineCode">CrossTable()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wbcd_train &lt;- wbcd_z[<span class="hljs-number">1:469</span>, ]
&gt; wbcd_test &lt;- wbcd_z[<span class="hljs-number">470:569</span>, ]
&gt; wbcd_train_labels &lt;- wbcd[<span class="hljs-number">1:469</span>, <span class="hljs-number">1</span>]
&gt; wbcd_test_labels &lt;- wbcd[<span class="hljs-number">470:569</span>, <span class="hljs-number">1</span>]
&gt; wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test,
                        cl = wbcd_train_labels, k = <span class="hljs-number">21</span>)
&gt; CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
             prop.chisq = <span class="hljs-literal">FALSE</span>)
</code></pre>
    <p class="normal">Unfortunately, in the following<a id="_idIndexMarker364"/> table, the results<a id="_idIndexMarker365"/> of our new transformation show a slight decline in accuracy. Using the same instances in which we had previously classified 98 percent of examples correctly, we now classified only 95 percent correctly. Making matters worse, we did no better at classifying the dangerous false negatives.</p>
    <pre class="programlisting con"><code class="hljs-con">                       | wbcd_test_pred                        
      Wbcd_test_labels |  Benign  |  Malignant  |  Row Total  |
-----------------------|----------|-------------|-------------|
                Benign |       61 |           0 |          61 |
                       |    1.000 |       0.000 |       0.610 |
                       |    0.924 |       0.000 |             |
                       |    0.610 |       0.000 |             |
-----------------------|----------|-------------|-------------|
             Malignant |        5 |          34 |          39 |
                       |    0.128 |       0.872 |       0.390 |
                       |    0.076 |       1.000 |             |
                       |    0.050 |       0.340 |             |
-----------------------|----------|-------------|-------------|
          Column Total |       66 |          34 |         100 |
                       |    0.660 |       0.340 |             |
-----------------------|----------|-------------|-------------|
</code></pre>
    <h3 class="heading-3" id="_idParaDest-79">Testing alternative values of k</h3>
    <p class="normal">We may be able to optimize<a id="_idIndexMarker366"/> the performance of the k-NN model by examining its performance across various <em class="italic">k</em> values. Using the normalized training and test datasets, the same 100 records need to be classified using several different choices of <em class="italic">k</em>. Given that we are testing only six <em class="italic">k</em> values, these iterations can be performed most simply by using copy-and-paste of our previous <code class="inlineCode">knn()</code> and <code class="inlineCode">CrossTable()</code> functions. However, it is also possible to write a <code class="inlineCode">for</code> loop that runs these two functions for each of the values in a vector named <code class="inlineCode">k_values</code>, as demonstrated in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; k_values &lt;- <span class="hljs-built_in">c</span>(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">11</span>, <span class="hljs-number">15</span>, <span class="hljs-number">21</span>, <span class="hljs-number">27</span>)
&gt; <span class="hljs-keyword">for</span> (k_val <span class="hljs-keyword">in</span> k_values) {
    wbcd_test_pred &lt;- knn(train = wbcd_train,
                          test = wbcd_test,
                          cl = wbcd_train_labels,
                          k = k_val)
    CrossTable(x = wbcd_test_labels,
               y = wbcd_test_pred,
               prop.chisq = <span class="hljs-literal">FALSE</span>)
  }
</code></pre>
    <p class="normal">The <code class="inlineCode">for</code> loop can almost be read as a simple sentence: for each value named <code class="inlineCode">k_val</code> in the <code class="inlineCode">k_values</code> vector, run the <code class="inlineCode">knn()</code> function while setting the parameter <code class="inlineCode">k</code> to the current <code class="inlineCode">k_val</code>, and then produce the <code class="inlineCode">CrossTable()</code> for the resulting predictions.</p>
    <div class="packt_tip">
      <p class="normal">A more sophisticated approach to looping using one of R’s <code class="inlineCode">apply()</code> functions is described in <em class="chapterRef">Chapter 7</em>, <em class="italic">Black-Box Methods – Neural Networks and Support Vector Machines</em>, to test various values of a cost parameter and plot the result. </p>
    </div>
    <p class="normal">The false negatives, false positives, and overall error rate are shown for each iteration:</p>
    <table class="table-container" id="table004">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">k value</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">False negatives</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">False positives</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Error rate</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">4 percent</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">5</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">2 percent</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">11</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">3 percent</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">15</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">3 percent</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">21</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">2 percent</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">27</p>
          </td>
          <td class="table-cell">
            <p class="normal">4</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">4 percent</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Although the classifier<a id="_idIndexMarker367"/> was never perfect, the 1-NN approach was able to avoid some of the false negatives at the expense of adding false positives. It is important to keep in mind, however, that it would be unwise to tailor our approach too closely to our test data; after all, a different set of 100 patient records is likely to be somewhat different from those used to measure<a id="_idIndexMarker368"/> our performance.</p>
    <div class="packt_tip">
      <p class="normal">If you need to be certain that a learner will generalize to future data, you might create several sets of 100 patients at random and repeatedly retest the result. Such methods to carefully evaluate the performance of machine learning models will be discussed further in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>.</p>
    </div>
    <h1 class="heading-1" id="_idParaDest-80">Summary</h1>
    <p class="normal">In this chapter, we learned about classification using k-NN. Unlike many classification algorithms, k-nearest neighbors does not do any learning—at least not according to the formal definition of machine learning. Instead, it simply stores the training data verbatim. Unlabeled test examples are then matched to the most similar records in the training set using a distance function, and the unlabeled example is assigned the label of its nearest neighbors.</p>
    <p class="normal">Although k-NN is a very simple algorithm, it can tackle extremely complex tasks, such as the identification of cancerous masses. In a few simple lines of R code, we were able to correctly identify whether a mass was malignant or benign 98 percent of the time in an example using real-world data. Although this teaching dataset was designed to streamline the process of building a model, the exercise demonstrated the ability of learning algorithms to make accurate predictions much like a human can.</p>
    <p class="normal">In the next chapter, we will examine a classification method that uses probability to estimate the likelihood that an observation falls into certain categories. It will be interesting to compare how this approach differs from k-NN. Later, in <em class="chapterRef">Chapter 9</em>, <em class="italic">Finding Groups of Data – Clustering with k-means</em>, we will learn about a close relative to k-NN, which uses distance measures for a completely different learning task.</p>
    <h1 class="heading-1" id="_idParaDest-81">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>