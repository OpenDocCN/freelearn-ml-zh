- en: Object Detection and Tracking Using OpenCV and CUDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenCV和CUDA进行目标检测和跟踪
- en: The last chapter described basic computer vision operations using OpenCV and
    CUDA. In this chapter, we will see how to use these basic operations along with
    OpenCV and CUDA to develop complex computer vision applications. We will use the
    example of object detection and tracking to demonstrate this concept. Object detection
    and tracking is a very active area of research in computer vision. It deals with
    identifying the location of an object in an image and tracking it in a sequence
    of frames. Many algorithms are proposed for this task based on color, shape, and
    the other salient features of an image. In this chapter, these algorithms are
    implemented using OpenCV and CUDA. We start with an explanation of detecting an
    object based on color, then describe the methods to detect an object with a particular
    shape. All objects have salient features that can be used to detect and track
    objects. This chapter describes the implementation of different feature detection
    algorithms and how they can be used to detect objects. The last part of the chapter
    will demonstrate the use of a background subtraction technique that separates
    the foreground from the background for object detection and tracking.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章描述了使用OpenCV和CUDA的基本计算机视觉操作。在这一章中，我们将看到如何使用这些基本操作以及OpenCV和CUDA来开发复杂的计算机视觉应用。我们将使用目标检测和跟踪的例子来展示这个概念。目标检测和跟踪是计算机视觉中一个非常活跃的研究领域。它涉及在图像中识别物体的位置并在一系列帧中跟踪它。基于颜色、形状和图像的其他显著特征，已经提出了许多用于此任务的方法。在这一章中，这些算法使用OpenCV和CUDA实现。我们首先解释基于颜色的物体检测，然后描述检测特定形状物体的方法。所有物体都有显著的特性，可以用来检测和跟踪物体。本章描述了不同特征检测算法的实现以及如何使用它们来检测物体。本章的最后部分将演示使用背景减除技术，该技术将前景与背景分离以进行目标检测和跟踪。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to object detection and tracking
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测和跟踪简介
- en: Object detection and tracking based on color
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于颜色的目标检测和跟踪
- en: Object detection and tracking based on a shape
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于形状的目标检测和跟踪
- en: Feature-based object detection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于特征的物体检测
- en: Object detection using Haar cascade
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Haar级联的目标检测
- en: Background subtraction methods
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 背景减除方法
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires a good understanding of image processing and computer
    vision. It also requires some basic knowledge of algorithms used for object detection
    and tracking. It needs familiarity with the basic C or C++ programming language,
    CUDA, and all the codes explained in previous chapters. All the code used in this
    chapter can be downloaded from the following GitHub link: [https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA).
    The code can be executed on any operating system, though it has only been tested
    on Ubuntu 16.04\.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要具备良好的图像处理和计算机视觉理解。它还需要一些关于用于目标检测和跟踪的算法的基本知识。需要熟悉基本的C或C++编程语言、CUDA以及前几章中解释的所有代码。本章中使用的所有代码都可以从以下GitHub链接下载：[https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA)。代码可以在任何操作系统上执行，尽管它只在Ubuntu
    16.04上进行了测试。
- en: 'Check out the following video to see the code in action:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际应用：
- en: '[http://bit.ly/2PSRqkU](http://bit.ly/2PSRqkU)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2PSRqkU](http://bit.ly/2PSRqkU)'
- en: Introduction to object detection and tracking
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测和跟踪简介
- en: 'Object detection and tracking is an active research topic in the field of computer
    vision that makes efforts to detect, recognize, and track objects through a series
    of frames. It has been found that object detection and tracking in the video sequence
    is a challenging task and a very time-consuming process. Object detection is the
    first step in building a larger computer vision system. A large amount of information
    can be derived from the detected object, as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测和跟踪是计算机视觉领域的一个活跃的研究课题，它通过一系列帧努力检测、识别和跟踪物体。已经发现，视频序列中的目标检测和跟踪是一个具有挑战性的任务，并且是一个非常耗时的过程。目标检测是构建更大计算机视觉系统的第一步。可以从检测到的物体中推导出大量信息，如下所示：
- en: The detected object can be classified into a particular class
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测到的目标可以被分类到特定的类别
- en: It can be tracked in an image sequence
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在图像序列中进行跟踪
- en: More information about the scene or other object inferences can be derived from
    the detected object
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以从检测到的对象中获取更多关于场景或其他对象推断的信息
- en: Object tracking is defined as the task of detecting objects in every frame of
    the video and establishing the correspondence between the detected objects from
    one frame to the other.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 目标跟踪被定义为在视频的每一帧中检测对象，并建立从一帧到另一帧检测到的对象的对应关系。
- en: Applications of object detection and tracking
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测与跟踪的应用
- en: Object detection and tracking can be used to develop video surveillance systems
    to track suspicious activities, events, and persons. It can be used for developing
    an intelligent traffic system to track vehicles and detect traffic rule violations.
    Object detection is essential in autonomous vehicles to give them information
    about the surroundings and planning for their navigation. It is also useful for
    pedestrian detection or vehicle detection in automatic driver assistance systems.
    It can be used in the medical field for applications like breast cancer detection
    or brain tumor detection and so on. It can be used for face and hand gesture recognition.
    It has a wide application in industrial assembly and quality control in production
    lines. It is vital for image retrieval from search engines and for photo management.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测与跟踪可用于开发视频监控系统以跟踪可疑活动、事件和人员。它可以用于开发智能交通系统以跟踪车辆和检测交通违规行为。在自动驾驶汽车中，目标检测对于提供周围环境信息和规划导航至关重要。它对于自动驾驶员辅助系统中的行人检测或车辆检测也非常有用。它可用于医疗领域的应用，如乳腺癌检测或脑瘤检测等。它可用于面部和手势识别。它在工业装配和生产线的质量控制中具有广泛的应用。对于搜索引擎中的图像检索和照片管理也非常重要。
- en: Challenges in object detection
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测的挑战
- en: Object detection is a challenging task because images in real life are affected
    by noise, illumination variation, dynamic backgrounds, shadowing effect, camera
    jitter, and motion blur. Object detection is difficult when an object to be detected
    is rotated, scaled, or under occlusion. Many applications require detecting more
    than one object class. If a large number of classes are being detected then the
    processing speed becomes an important issue along with the kinds of classes that
    the system can handle without accuracy loss.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是一个具有挑战性的任务，因为现实生活中的图像会受到噪声、光照变化、动态背景、阴影效果、相机抖动和运动模糊的影响。当要检测的对象旋转、缩放或被遮挡时，目标检测变得困难。许多应用需要检测多个对象类别。如果检测的类别数量很大，那么处理速度就成为一个重要问题，同时系统在处理这些类别时，如何不损失准确性也是一个关键问题。
- en: There are many algorithms that overcome some of these challenges. They are discussed
    in this chapter. The chapter does not describe the algorithms in detail, but more
    focus is given on how it can be implemented using CUDA and OpenCV.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多算法可以克服这些挑战中的一些。这些算法在本章中进行了讨论。本章没有详细描述这些算法，但更侧重于如何使用CUDA和OpenCV来实现它们。
- en: Object detection and tracking based on color
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于颜色的目标检测与跟踪
- en: An object has many global features like color and shape, which describe the
    object as a whole. These features can be utilized for the detection of an object
    and tracking it in a sequence of frames. In this section, we will use color as
    a feature to detect an object with a particular color. This method is useful when
    an object to be detected is of a specific color and this color is different to
    the color of the background. If the object and background have the same color,
    then this method for detection will fail. In this section, we will try to detect
    any object with a blue color from a webcam stream using OpenCV and CUDA.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个对象有许多全局特征，如颜色和形状，这些特征描述了对象的整体。这些特征可以用于在一系列帧中检测对象并跟踪它。在本节中，我们将使用颜色作为特征来检测具有特定颜色的对象。当要检测的对象是特定颜色且与背景颜色不同时，这种方法很有用。如果对象和背景颜色相同，则这种检测方法将失败。在本节中，我们将尝试使用OpenCV和CUDA从网络摄像头流中检测任何蓝色对象。
- en: Blue object detection and tracking
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝色目标检测与跟踪
- en: 'The first question that should come to your mind is which color space should
    be used for segmenting blue color. A **Red Green Blue** (**RGB**) color space
    does not separate color information from intensity information. The color spaces
    that separate color information from intensity, like **Hue Saturation Value**
    (HSV) and **YCrCb** (where Y′ is the luma component and CB and CR are the blue-difference
    and red-difference chroma components), are ideal for this kind of task. Every
    color has a specific range in the hue channel that can be utilized for detection
    of that color. The boilerplate code for starting the webcam, capturing frames,
    and uploading on-device memory for a GPU operation is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题可能是应该使用哪种颜色空间来分割蓝色。**红绿蓝**（**RGB**）颜色空间没有将颜色信息与强度信息分开。将颜色信息与强度信息分开的颜色空间，如**色调饱和度值**（HSV）和**YCrCb**（其中Y′是亮度分量，CB和CR是蓝差和红差色度分量），对于这类任务非常理想。每种颜色在色调通道中都有一个特定的范围，可以用来检测该颜色。以下是从开始摄像头、捕获帧到上传设备内存以进行GPU操作的样板代码：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To detect the blue color, we need to find a range for blue color in the HSV
    color space. If a range is accurate then the detection will be accurate. The range
    of blue color for three channels, hue, saturation, and value, is as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要检测蓝色，我们需要在HSV颜色空间中找到蓝色颜色的范围。如果范围准确，则检测将准确。蓝色颜色在三个通道（色调、饱和度和值）中的范围如下：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This range will be used to threshold an image in a particular channel to create
    a mask for the blue color. If this mask is again ANDed with the original frame,
    then only a blue object will be there in the resultant image. The code for this
    is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个范围将被用来在特定通道中对图像进行阈值处理，以创建蓝色颜色的掩码。如果这个掩码再次与原始帧进行AND操作，那么结果图像中就只会剩下蓝色对象。以下是这个操作的代码：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The frame from the webcam is converted to an HSV color space. The blue color
    has a different range in three channels, so each channel has to be thresholded
    individually. The channels are split using the `split` method and thresholded
    using the `threshold` function. The minimum and maximum ranges for each channel
    are used as lower and upper thresholds. The channel value inside this range will
    be converted to white and others are converted to black. These three thresholded
    channels are logically ANDed to get a final mask for a blue color. This mask can
    be used to detect and track an object with a blue color from a video.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 摄像头捕获的帧被转换为HSV颜色空间。蓝色在这三个通道中具有不同的范围，因此每个通道都需要单独进行阈值处理。使用`split`方法将通道分割，并使用`threshold`函数进行阈值处理。每个通道的最小和最大范围用作下限和上限阈值。在此范围内的通道值将被转换为白色，其他则转换为黑色。这三个阈值通道通过逻辑AND操作得到一个用于蓝色颜色的最终掩码。这个掩码可以用来从视频中检测和跟踪具有蓝色颜色的对象。
- en: 'The output of two frames, one without the blue object and the other with the
    blue object, is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 两个帧的输出，一个没有蓝色对象，另一个有蓝色对象，如下所示：
- en: '![](img/32bdf79b-65c0-4e98-a877-663ff2d52588.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/32bdf79b-65c0-4e98-a877-663ff2d52588.png)'
- en: As can be seen from the result, when a frame does not contain any blue object,
    the mask is almost black; whereas in the frame below, when the blue object comes
    into frame, that part turns white. This method will only work when the background
    does not contain the color of an object.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果可以看出，当帧中不包含任何蓝色对象时，掩码几乎为黑色；而在下面的帧中，当蓝色对象进入画面时，该部分变为白色。这种方法仅在背景不包含对象颜色时才会有效。
- en: Object detection and tracking based on shape
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于形状的对象检测与跟踪
- en: The shape of an object can also be utilized as a global feature to detect an
    object with a distinct shape. This shape can be a straight line, polygons, circles,
    or any other irregular shapes. Object boundaries, edges, and contours can be utilized
    to detect an object with a particular shape. In this section, we will use the
    Canny edge detection algorithm and Hough transform to detect two regular shapes,
    which are a line and a circle.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对象的形状也可以作为全局特征来检测具有独特形状的对象。这个形状可以是直线、多边形、圆形或任何其他不规则形状。对象边界、边缘和轮廓可以用来检测具有特定形状的对象。在本节中，我们将使用Canny边缘检测算法和Hough变换来检测两个规则形状，即直线和圆形。
- en: Canny edge detection
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Canny边缘检测
- en: In the last chapter, we saw various high pass filters, which can be used as
    edge detectors. In this section, the Canny edge detection algorithm, which combines
    Gaussian filtering, gradient finding, non-maximum suppression, and hysteresis
    thresholding, is implemented using OpenCV and CUDA. High pass filters, as explained
    in the last chapter, are very sensitive to noise. In Canny edge detection, Gaussian
    smoothing is done before detecting edges, which makes it less sensitive to noises.
    It also has a non-maximum suppression stage after detecting edges to remove unnecessary
    edges from the result.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了各种高通滤波器，这些滤波器可以用作边缘检测器。在本节中，我们使用OpenCV和CUDA实现了结合高斯滤波、梯度查找、非极大值抑制和阈值滞后的Canny边缘检测算法。正如上一章所解释的，高通滤波器对噪声非常敏感。在Canny边缘检测中，在检测边缘之前先进行高斯平滑，这使得它对噪声的敏感性降低。它还在检测边缘后有一个非极大值抑制阶段，以从结果中去除不必要的边缘。
- en: 'Canny edge detection is a computationally intensive task, which is hard to
    use in real-time applications. The CUDA version of the algorithm can be used to
    accelerate it. The code for implementing a Canny edge detection algorithm is described
    below:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Canny边缘检测是一个计算密集型任务，难以用于实时应用。算法的CUDA版本可以用来加速它。实现Canny边缘检测算法的代码描述如下：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'OpenCV and CUDA provides the `createCannyEdgeDetector` class for Canny edge
    detection. The object of this class is created, and many arguments can be passed
    while creating it. The first and second arguments are the low and high thresholds
    for hysteresis thresholding. If the intensity gradient at a point is greater then
    the maximum threshold, then it is categorized as an edge point. If the gradient
    is less than the low threshold, then the point is not an edge point. If the gradient
    is in between thresholds, then whether the point is an edge or not is decided
    based on connectivity. The third argument is the aperture size for the edge detector.
    The final argument is the Boolean argument, which indicates whether to use `L2_norm`
    or `L1_norm` for gradient magnitude calculation. `L2_norm` is computationally
    expensive but it is more accurate. The true value indicates the use of `L2_norm`.
    The output of the code is shown below:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV和CUDA提供了`createCannyEdgeDetector`类用于Canny边缘检测。创建此类的对象时，可以传递许多参数。前两个参数是阈值滞后的低阈值和高阈值。如果某一点的强度梯度大于最大阈值，则将其分类为边缘点。如果梯度小于低阈值，则该点不是边缘点。如果梯度在阈值之间，则根据连通性决定该点是否为边缘点。第三个参数是边缘检测器的孔径大小。最后一个参数是布尔参数，表示是否使用`L2_norm`或`L1_norm`进行梯度幅度计算。`L2_norm`计算成本较高，但更准确。真值表示使用`L2_norm`。代码的输出如下所示：
- en: '![](img/9330eca7-5962-4191-8fc5-82c8628a870f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9330eca7-5962-4191-8fc5-82c8628a870f.png)'
- en: You can play around with the values of the lower and upper thresholds to detect
    edges more accurately for a given image. Edge detection is a very important preprocessing
    step for many computer vision applications and Canny edge detection is widely
    used for that.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以调整下限和上限阈值，以更准确地检测给定图像的边缘。边缘检测是许多计算机视觉应用的重要预处理步骤，Canny边缘检测被广泛用于此目的。
- en: Straight line detection using Hough transform
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用霍夫变换进行直线检测
- en: 'The detection of straight lines is important in many computer vision applications,
    like lane detection. It can also be used to detect lines that are part of other
    regular shapes. Hough transform is a popular feature extraction technique used
    in computer vision to detect straight lines. We will not go into detail about
    how Hough transform detects lines, but we will see how it can be implemented in
    OpenCV and CUDA. The code for implementing Hough transform for line detection
    is as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多计算机视觉应用中，如车道检测，检测直线非常重要。它还可以用来检测其他规则形状的一部分直线。霍夫变换是计算机视觉中用于检测直线的流行特征提取技术。我们不会详细介绍霍夫变换如何检测直线，但我们将看到它如何在OpenCV和CUDA中实现。实现霍夫变换进行直线检测的代码如下：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: OpenCV provides the `createHoughSegmentDetector` class for implementing Hough
    transform. It needs an edge map of an image as input. So edges are detected from
    an image using a Canny edge detector. The output of the Canny edge detector is
    uploaded to the device memory for GPU computation. The edges can also be computed
    on GPU as discussed in the last section.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV提供了`createHoughSegmentDetector`类用于实现霍夫变换。它需要一个图像的边缘图作为输入。因此，使用Canny边缘检测器从图像中检测边缘。Canny边缘检测器的输出上传到设备内存以进行GPU计算。正如上一节所讨论的，边缘也可以在GPU上计算。
- en: The object of `createHoughSegmentDetector` is created. It requires many arguments.
    The first argument indicates the resolution of parameter `r` used in Hough transform,
    which is taken as 1 pixel normally. The second argument is the resolution of parameter
    theta in radians, which is taken as 1 radian or pi/180\. The third argument is
    the minimum number of points that are needed to form a line, which is taken as
    50 pixels. The final argument is the maximum gap between two points to be considered
    as the same line, which is taken as 5 pixels.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了`createHoughSegmentDetector`对象。它需要许多参数。第一个参数表示在霍夫变换中使用的参数`r`的分辨率，通常取1像素。第二个参数是参数theta的弧度分辨率，通常取1弧度或π/180。第三个参数是需要形成线条的最小点数，通常取50像素。最后一个参数是考虑为同一直线的两个点之间的最大间隔，通常取5像素。
- en: The detect method of the created object is used to detect straight lines. It
    needs two arguments. The first argument is the image on which the edges are to
    be detected, and the second argument is the array in which detected line points
    will be stored. The array contains the starting and ending (x,y) points of the
    detected lines. This array is iterated using the `for` loop to draw individual
    lines on an image using the line function from OpenCV. The final image is displayed
    using the `imshow` function.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的对象的检测方法用于检测直线。它需要两个参数。第一个参数是要检测边缘的图像，第二个参数是存储检测到的线条点的数组。该数组包含检测到的线条的起始和结束(x,y)点。使用OpenCV的线条函数通过`for`循环迭代该数组，在图像上绘制单个线条。最终图像使用`imshow`函数显示。
- en: 'Hough transform is a mathematically intensive step. Just to show an advantage
    of CUDA, we will implement the same algorithm for CPU and compare the performance
    of it with a CUDA implementation. The CPU code for Hough transform is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 霍夫变换是一个数学密集型步骤。为了展示CUDA的优势，我们将实现CPU和CUDA的相同算法，并比较它们的性能。以下是对CPU霍夫变换的代码：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `HoughLinesP` function is used for detecting lines on a CPU using probabilistic
    Hough transform. The first two arguments are the source image and the array to
    store output line points. The third and fourth arguments are a resolution for
    `r` and theta. The fifth argument is the threshold that indicates the minimum
    number of intersection points for a line. The sixth argument indicates the minimum
    number of points needed to form a line. The last argument indicates the maximum
    gap between points to be considered on the same line.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`HoughLinesP`函数在CPU上通过概率霍夫变换检测线条。前两个参数是源图像和存储输出线条点的数组。第三个和第四个参数是`r`和theta的分辨率。第五个参数是表示线条的最小交点数的阈值。第六个参数表示形成线条所需的最小点数。最后一个参数表示考虑在相同线条上的点的最大间隔。
- en: 'The array returned by the function is iterated using the `for` loop for displaying
    detected lines on the original image. The output for both the GPU and CPU function
    is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 函数返回的数组使用`for`循环迭代，以在原始图像上显示检测到的线条。GPU和CPU函数的输出如下：
- en: '![](img/7942ad86-ce16-4db4-9fcb-31724c0d1985.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7942ad86-ce16-4db4-9fcb-31724c0d1985.png)'
- en: 'The comparison between the performance of the GPU and CPU code for the Hough
    transform is shown in the following screenshot:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了GPU和CPU代码在霍夫变换性能上的比较：
- en: '![](img/62f0224e-327a-4efc-a17f-bb9679864639.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/62f0224e-327a-4efc-a17f-bb9679864639.png)'
- en: It takes around 4 ms for a single image to process on the CPU and 1.5 ms on
    the GPU, which is equivalent to 248 FPS on the CPU, and 632 FPS on the GPU, which
    is almost 2.5 times an improvement on the GPU.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上处理单个图像大约需要4毫秒，在GPU上需要1.5毫秒，这在CPU上相当于248 FPS，在GPU上相当于632 FPS，几乎是GPU上2.5倍的提升。
- en: Circle detection
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 圆检测
- en: 'Hough transform can also be used for circle detection. It can be used in many
    applications, like ball detection and tracking and coin detection, and so on,
    where objects are circular. OpenCV and CUDA provide a class to implement this.
    The code for coin detection using Hough transform is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 霍夫变换也可以用于圆检测。它可以用于许多应用，如球检测和跟踪以及硬币检测等，在这些应用中，对象是圆形的。OpenCV和CUDA提供了一个类来实现这一点。以下是用霍夫变换进行硬币检测的代码：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There is a `createHoughCirclesDetector` class for detecting the circular object.
    The object of that class is created. Many arguments can be provided while creating
    an object of this class. The first argument is `dp` that signifies an inverse
    ratio of the accumulator resolution to the image resolution, which is mostly taken
    as 1\. The second argument is the minimum distance between the centers of the
    detected circle. The third argument is a Canny threshold and the fourth argument
    is the accumulator threshold. The fifth and sixth arguments are the minimum and
    maximum radiuses of the circles to be detected.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个`createHoughCirclesDetector`类用于检测圆形物体。创建了该类的对象。在创建该类的对象时可以提供许多参数。第一个参数是`dp`，表示累加器分辨率与图像分辨率的倒数，通常取为1。第二个参数是检测到的圆心之间的最小距离。第三个参数是Canny阈值，第四个参数是累加器阈值。第五和第六个参数是要检测的圆的最小和最大半径。
- en: 'The minimum distance between the centers of the circle is taken as `100` pixels.
    You can play around with this value. If this is decreased, then many circles are
    detected falsely on the original image, while if it is increased then some true
    circles may be missed. The last two arguments, which are the minimum and maximum
    radiuses, can be taken as `0` if you don''t know the exact dimension. In the preceding
    code, it is taken as `1` and maximum dimension of an image to detect all circles
    in an image. The output of the program is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 圆心之间的最小距离取为`100`像素。您可以尝试调整这个值。如果这个值减小，那么原始图像上会错误地检测到许多圆，而如果这个值增加，那么一些真正的圆可能会被错过。最后两个参数，即最小和最大半径，如果不知道确切的尺寸，可以取为`0`。在前面的代码中，它被设置为`1`和图像的最大尺寸，以检测图像中的所有圆。程序输出如下：
- en: '![](img/b7076793-815c-4a5f-8693-ba885a00feb0.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b7076793-815c-4a5f-8693-ba885a00feb0.png)'
- en: The Hough transform is very sensitive to Gaussian and salt-pepper noise. So,
    sometimes it is better to preprocess the image with Gaussian and median filters
    before applying Hough transform. It will give more accurate results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Hough变换对高斯噪声和椒盐噪声非常敏感。因此，在应用Hough变换之前，有时最好先使用高斯和中值滤波器对图像进行预处理。这将给出更准确的结果。
- en: To summarize, we have used the Hough line and circle transforms to detect objects
    with regular shapes. Contours and convexity can also be used for shape detection.
    The functions for this are available in OpenCV, but they are not available with
    CUDA implementation. You will have to develop your own versions of these functions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们使用了Hough线变换和圆变换来检测具有规则形状的物体。轮廓和凸性也可以用于形状检测。这些功能在OpenCV中可用，但CUDA实现中不可用。您将不得不开发这些函数的自己的版本。
- en: Key-point detectors and descriptors
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键点检测器和描述符
- en: Till this point, we have used global features like color and shape to detect
    an object. These features are easy to compute, are quick, and require a small
    amount of memory, but they can only be used when some information regarding the
    object is already available. If that is not the case then local features are used,
    which require more computation and memory, but they are more accurate. In this
    section, various algorithms that find local features are explained. They are also
    called key point detectors. Key-points are the points that characterize the image
    and can be used to define an object accurately.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用了全局特征，如颜色和形状来检测物体。这些特征易于计算，快速，并且需要的内存量小，但它们只能在已有关于物体的某些信息的情况下使用。如果没有这些信息，则使用局部特征，这些特征需要更多的计算和内存，但它们更准确。在本节中，解释了寻找局部特征的多种算法。它们也被称为关键点检测器。关键点是表征图像的点，可以用来精确地定义一个物体。
- en: Features from Accelerated Segment Test (FAST) feature detector
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自加速区域测试（FAST）特征检测器的特征
- en: The FAST algorithm is used to detect corner points as key-points from an image.
    It detects the corners by applying a segment test to every pixel. It considers
    a circle of 16 pixels around the pixel. If there are *n* continuous points in
    a circle of radius 16, which have the intensity of pixel greater than *Ip +t*
    or less than *Ip- t,* then that pixel is considered a corner. *Ip* is intensity
    at pixel *p,* and *t* is the selected threshold.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: FAST算法用于从图像中检测角点作为关键点。它通过对每个像素应用段测试来检测角点。它考虑像素周围的16像素圆。如果在半径为16的圆中有连续的*n*个点，其强度大于*Ip+t*或小于*Ip-t*，则该像素被认为是角点。*Ip*是像素*p*的强度，*t*是选定的阈值。
- en: Sometimes instead of checking all points in the radius, a few selected points
    are checked for intensity values to determine corner points. It accelerates the
    performance of the FAST algorithm. FAST provides corner points that can be utilized
    as key-points to detect an object. It is rotation-invariant, as corners of an
    object will remain the same even if the object is rotated. FAST is not scale-invariant,
    as the increase in dimension may result in a smooth transition of intensity values
    rather than a sharp transition at corners.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，不是检查半径内的所有点，而是检查几个选定的点以确定强度值来决定角落点。这加速了FAST算法的性能。FAST提供了可以作为关键点利用的角落点来检测对象。它是旋转不变的，因为即使对象旋转，对象的角落也会保持不变。FAST不是尺度不变的，因为尺寸的增加可能会导致强度值的平滑过渡，而不是在角落处的尖锐过渡。
- en: 'OpenCV and CUDA provide an efficient way of implementing the FAST algorithm.
    The program to detect key-points using the FAST algorithm is shown below:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV和CUDA提供了一个高效实现FAST算法的方法。以下是用FAST算法检测关键点的程序：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: OpenCV and CUDA provide a `FastFeatureDetector` class for implementing the FAST
    algorithm. The object of this class is created using the create method of the
    class. It needs three arguments. The first argument is the intensity threshold
    to be used for the FAST algorithm. The second argument specifies whether to use
    non-maximum suppression or not. It is a Boolean value, which can be specified
    as `true` or `false`. The third argument indicates which FAST method is used for
    calculating the neighborhood. Three methods, `cv2.FAST_FEATURE_DETECTOR_TYPE_5_8`,
    `cv2.FAST_FEATURE_DETECTOR_TYPE_7_12`, and `cv2.FAST_FEATURE_DETECTOR_TYPE_9_16`
    , are available, which can be specified as flags `0`, `1`, or `2`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV和CUDA提供了一个`FastFeatureDetector`类来实现FAST算法。这个类的对象是通过类的create方法创建的。它需要三个参数。第一个参数是要用于FAST算法的强度阈值。第二个参数指定是否使用非最大抑制。它是一个布尔值，可以指定为`true`或`false`。第三个参数表示用于计算邻域的FAST方法。有三个方法，`cv2.FAST_FEATURE_DETECTOR_TYPE_5_8`、`cv2.FAST_FEATURE_DETECTOR_TYPE_7_12`和`cv2.FAST_FEATURE_DETECTOR_TYPE_9_16`，可以作为标志`0`、`1`或`2`指定。
- en: The detect method of the created object is used to detect key-points. It needs
    an input image and vector to store key-points as an argument. The calculated key-points
    can be drawn on an original image using the `drawkey-points` function. It requires
    source image, the vector of the key-points and the destination image as an argument.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的对象的detect方法用于检测关键点。它需要一个输入图像和一个存储关键点的向量作为参数。可以使用`drawkey-points`函数在原始图像上绘制计算出的关键点。它需要源图像、关键点的向量和目标图像作为参数。
- en: 'The intensity threshold can be changed to detect the different number of key-points.
    If the threshold is low, then more key-points will pass the segment test and will
    be categorized as key-points. As this threshold is increased, the number of key-points
    detected will gradually decrease. In the same way, if non-maximum suppression
    is false then more than one key point is detected at a single corner point. The
    output of the code is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 可以改变强度阈值以检测不同数量的关键点。如果阈值低，则更多的关键点会通过分割测试并被归类为关键点。随着阈值的增加，检测到的关键点数量将逐渐减少。同样，如果非最大抑制是假的，则在单个角落点可能会检测到多个关键点。以下是代码的输出：
- en: '![](img/2b5da030-d05b-4e4b-aa85-80d08b02719f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b5da030-d05b-4e4b-aa85-80d08b02719f.png)'
- en: As can be seen from the output, as the threshold increases from 10 to 50 and
    100, the number of key-points decreases. These key-points can be used to detect
    an object in a query image.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出结果可以看出，随着阈值从10增加到50和100，关键点的数量减少。这些关键点可以用于检测查询图像中的对象。
- en: Oriented FAST and Rotated BRIEF (ORB) feature detection
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定向FAST和旋转BRIEF (ORB) 特征检测
- en: ORB is a very efficient feature detection and description algorithm. It is a
    combination of the FAST algorithm for feature detection and the **Binary Robust
    Independent Elementary Features** (**BRIEF**) algorithm for feature description.
    It provides an efficient alternative to the SURF and SIFT algorithms, which are
    widely used for object detection. As they are patented, their use should be paid
    for. ORB matches the performance of SIFT and SURF at no cost.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ORB是一个非常高效的特徵检测和描述算法。它是特征检测的FAST算法和特征描述的**二进制鲁棒独立基本特征**（**BRIEF**）算法的组合。它为广泛用于对象检测的SURF和SIFT算法提供了一个高效的替代方案。由于它们是专利的，使用它们需要付费。ORB在无需付费的情况下匹配SIFT和SURF的性能。
- en: 'OpenCV and CUDA provide an easy API for implementing the ORB algorithm. The
    code for implementing the ORB algorithm is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV和CUDA提供了一个易于实现的ORB算法的API。实现ORB算法的代码如下：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The object of the `ORB` class is created using the create method. All the arguments
    to this method are optional so we are using the default values for it. The detect
    method of the created object is used to detect key-points from an image. It requires
    an input image and the vector of key-points in which the output will be stored
    as arguments. The detected key-points are drawn on the image using the `drawkey-points`
    function. The output of the preceding code is as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`ORB`类的对象是通过`create`方法创建的。此方法的所有参数都是可选的，因此我们使用了它的默认值。创建的对象的`detect`方法用于从图像中检测关键点。它需要一个输入图像和关键点向量的向量，输出将存储在这些参数中。使用`drawkey-points`函数在图像上绘制检测到的关键点。前述代码的输出如下：'
- en: '![](img/979882c6-5901-4d4b-b18d-5dca9b669df7.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/979882c6-5901-4d4b-b18d-5dca9b669df7.png)'
- en: The `ORB` class also provides a method to calculate descriptors for all the
    key-points. These descriptors can accurately describe the object and can be used
    to detect objects from an image. These descriptors can also be used to classify
    an object.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`ORB`类还提供了一个方法来计算所有关键点的描述符。这些描述符可以准确地描述对象，并可用于从图像中检测对象。这些描述符也可以用于对对象进行分类。'
- en: Speeded up robust feature detection and matching
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速鲁棒特征检测和匹配
- en: 'SURF approximates Laplacian of Gaussian with computation based on a simple
    two-dimensional box filter as described in the last chapter. The convolution with
    the box filter can be easily calculated with the help of integral images, which
    improves the performance of the algorithm. SURF relies on the determinant of the
    Hessian matrix for both scale and location. The approximated determinant of Hessian
    can be expressed as:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: SURF通过基于简单二维盒滤波器的计算来近似高斯拉普拉斯。使用积分图像可以轻松计算与盒滤波器的卷积，这提高了算法的性能。SURF依赖于Hessian矩阵的行列式来处理尺度和位置。Hessian矩阵的近似行列式可以表示为：
- en: '![](img/894254f9-4c9b-40da-b0d0-9c0636194147.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/894254f9-4c9b-40da-b0d0-9c0636194147.png)'
- en: Where *w* is a relative weight for the filter response and used to balance the
    expression for the determinant. The *Dx*, *Dy* are the result of the Laplacian
    operator in *X*- and *Y*-direction.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*w*是滤波器响应的相对权重，用于平衡行列式的表达式。*Dx*、*Dy*是拉普拉斯算子在*X*和*Y*方向的结果。
- en: SURF uses wavelet responses in horizontal and vertical directions, using an
    integral image approach for orientation assignment. Adequate Gaussian weights
    are also applied to it. The dominant orientation is estimated by calculating the
    sum of all responses within a sliding orientation window of angle 60 degrees.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SURF在水平和垂直方向上使用小波响应，使用积分图像方法进行方向分配。还应用了适当的Gaussian权重。通过计算角度为60度的滑动方向窗口内所有响应的总和来估计主导方向。
- en: For feature description, SURF uses Haar wavelet responses in horizontal and
    vertical directions. This is computed for all subregions in an image, resulting
    in a SURF feature descriptor with a total of 64 dimensions. The lower the dimensions,
    the higher the speed of computation and matching will be. For more precision,
    the SURF feature descriptor has an extended 128-dimension version. SURF is rotation-invariant
    and scale-invariant.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征描述，SURF在水平和垂直方向上使用Haar小波响应。这是对图像中的所有子区域进行计算的结果，从而得到一个具有64个维度的SURF特征描述符。维度越低，计算和匹配的速度越快。为了提高精度，SURF特征描述符还有一个扩展的128维版本。SURF是旋转不变和尺度不变的。
- en: SURF has a higher processing speed than SIFT because it uses a 64-directional
    feature vector compared to SIFT, which uses a 128-dimensional feature vector.
    SURF is good at handling images with blurring and rotation, but not good at handling
    a viewpoint change and illumination change.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用128维特征向量的SIFT相比，SURF具有更高的处理速度，因为它使用64方向的特性向量。SURF擅长处理模糊和旋转的图像，但不擅长处理视点和光照变化。
- en: 'OpenCV and CUDA provide an API to calculate SURF key-points and descriptors.
    We will also see how these can be used to detect an object in the query image.
    The code for SURF feature detection and matching is as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV和CUDA提供了一个API来计算SURF关键点和描述符。我们还将看到如何使用这些API在查询图像中检测对象。SURF特征检测和匹配的代码如下：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Two images are read from the disk. The first image contains the object to be
    detected. The second image is the query image in which the object is to be searched.
    We will calculate SURF features from both the images, and then these features
    will be matched for detecting an object from a query image.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从磁盘读取两张图像。第一张图像包含要检测的对象。第二张图像是要搜索对象的查询图像。我们将从这两张图像中计算 SURF 特征，然后匹配这些特征以从查询图像中检测对象。
- en: OpenCV provides the `SURF_CUDA` class for calculating SURF features. The object
    of this class is created. It requires the Hessian threshold as an argument. It
    is taken as `150`. This threshold determines how large the output from the Hessian
    determinant calculation must be in order for a point to be considered as a key
    point. A larger threshold value will result in fewer but more salient interest
    points, and a smaller value will result in more numerous but less salient points.
    It can be chosen according to the application.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV 提供了用于计算 SURF 特征的 `SURF_CUDA` 类。该类的对象被创建。它需要一个 Hessian 阈值作为参数。这里取值为 `150`。这个阈值决定了
    Hessian 行列式计算输出的点必须有多大，才能被认为是关键点。更大的阈值值将导致更少但更显著的兴趣点，而较小的值将导致更多但不太显著的点。可以根据应用来选择。
- en: This `surf` object is used to calculate key-points and descriptors from both
    the object and query image. The image, data type of the image, vectors to store
    key-points, and descriptors are passed as an argument. To match the object in
    the query image, descriptors from both the images need to be matched. OpenCV provides
    the different type of matching algorithms for this purpose, like the Brute-Force
    matcher and the **Fast Library for Approximate Nearest Neighbors** (**FLANN**)
    matcher.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `surf` 对象用于从对象和查询图像中计算关键点和描述符。图像、图像的数据类型、存储关键点的向量以及描述符作为参数传递。为了匹配查询图像中的对象，需要匹配两张图像中的描述符。OpenCV
    提供了不同的匹配算法来实现这个目的，如 Brute-Force 匹配器和 **快速近似最近邻库**（**FLANN**）匹配器。
- en: The Brute-Force matcher is used in the program; it is a simple method. It takes
    the descriptor of one feature in an object that is matched with all other features
    in the query image, using some distance calculation. It returns the best match
    key point, or best `k` matches using the nearest neighbor algorithm when using
    the `knnMatch` method of the `matcher` class. The `knnMatch` method requires two
    sets of descriptors along with the number of nearest neighbors. It is taken as
    `3` in the code.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 程序中使用的是 Brute-Force 匹配器；这是一个简单的方法。它使用某种距离计算方法，将对象中与查询图像中所有其他特征匹配的特征描述符取出来。它返回最佳匹配关键点，或使用最近邻算法通过
    `matcher` 类的 `knnMatch` 方法返回最佳 `k` 个匹配。`knnMatch` 方法需要两组描述符以及最近邻的数量。在代码中取值为 `3`。
- en: The good, matching key-points are extracted from the matching points returned
    by the `knnMatch` method. These good matches are found out by using the ratio
    test method, described in the original paper. These good matches are used to detect
    an object from a scene.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `knnMatch` 方法返回的匹配点中提取出良好的匹配关键点。这些良好的匹配是通过使用原始论文中描述的比率测试方法找到的。这些良好的匹配用于从场景中检测对象。
- en: The `drawMatches` function is used to draw a line between matched good points
    from both the images. It requires many arguments. The first argument is the source
    image, the second image is the key-points of the source image, the third argument
    is the second image, the fourth argument is the key-points of the second image,
    and the fifth argument is the output image. The sixth argument is the color of
    the lines and key-points. It is taken as `Scalar::all(-1)` , which indicates that
    a color will be taken randomly. The seventh argument is the color of the key-points,
    which do not have any matches. It is also taken as `Scalar::all(-1)` , which indicates
    that a color will be taken randomly. The last two arguments specify a mask to
    draw matches and flag settings. The empty mask is taken so that all matches are
    drawn.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `drawMatches` 函数在两张图像的匹配良好点之间画线。它需要许多参数。第一个参数是源图像，第二个参数是源图像的关键点，第三个参数是第二张图像，第四个参数是第二张图像的关键点，第五个参数是输出图像。第六个参数是线条和关键点的颜色。这里取值为
    `Scalar::all(-1)`，表示将随机选择颜色。第七个参数是关键点的颜色，这些关键点没有匹配。它也取值为 `Scalar::all(-1)`，表示将随机选择颜色。最后两个参数指定了绘制匹配的掩码和标志设置。使用空掩码，以便绘制所有匹配。
- en: 'These matches can be used to draw a bounding box around the detected object,
    which will localize the object from the scene. The code for drawing a bounding
    box is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些匹配可以用来在检测到的物体周围绘制边界框，这将定位场景中的物体。绘制边界框的代码如下：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: OpenCV provides the `findHomography` function to search for the position, orientation,
    and scale of the object in the scene, based on the good matches. The first two
    arguments are good, matching key-points from the object and scene images. The
    **random sample consensus** (**RANSAC**) method is passed as one of the arguments
    that is used to find the best translation matrix.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV提供了`findHomography`函数，用于根据良好的匹配搜索场景中物体的位置、方向和比例。前两个参数是从物体和场景图像中提取的良好匹配的关键点。**随机样本一致性**（**RANSAC**）方法作为参数之一传递，用于找到最佳平移矩阵。
- en: 'After finding this translation matrix, the `perspectiveTransform` function
    is used to find an object. It requires four corner points and a translation matrix
    as an argument. These transformed points are used to draw a bounding box around
    the detected object. The output of the SURF program for finding features and matching
    objects is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 找到这个平移矩阵后，使用`perspectiveTransform`函数来找到物体。它需要一个四角点和平移矩阵作为参数。这些变换点用于在检测到的物体周围绘制边界框。用于查找特征和匹配物体的SURF程序的输出如下：
- en: '![](img/cbe37af6-d0c7-44ba-a414-69bebff1e47b.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cbe37af6-d0c7-44ba-a414-69bebff1e47b.png)'
- en: The figure contains the object image, query image, and detected image. As can
    be seen from the preceding image, SURF can accurately determine the position of
    the object even if the object is rotated. Though sometimes it may detect the wrong
    features. The Hessian threshold and test ratio can be varied to find the optimum
    matches.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该图包含物体图像、查询图像和检测图像。从前面的图像可以看出，SURF可以准确地确定物体的位置，即使物体被旋转。尽管有时它可能会检测到错误特征。可以通过改变Hessian阈值和测试比率来找到最佳匹配。
- en: So to summarize, in this section we have seen FAST, ORB, and SURF key point
    detection algorithms. We have also seen how these points can be used to match
    and localize an object in an image by using SURF features as an example. You can
    try using FAST and ORB features to do the same. In the next section, we discuss
    Haar cascades in detail for detecting faces and eyes from an image.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结一下，在本节中，我们看到了FAST、ORB和SURF关键点检测算法。我们还看到了如何使用这些点通过使用SURF特征作为示例来匹配和定位图像中的物体。您也可以尝试使用FAST和ORB特征来完成相同的工作。在下一节中，我们将详细讨论用于从图像中检测面部和眼睛的Haar级联。
- en: Object detection using Haar cascades
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Haar级联进行物体检测
- en: 'A Haar cascade uses rectangular features to detect an object. It uses rectangles
    of different sizes to calculate different line and edge features. The rectangle
    contains some black and white regions, as shown in the following figure, and they
    are centered at different positions in an image:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Haar级联使用矩形特征来检测物体。它使用不同大小的矩形来计算不同的线和边缘特征。矩形包含一些黑白区域，如图所示，它们在图像中的不同位置居中：
- en: '![](img/21388154-44a4-49a5-b2e8-1a85342c94ce.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/21388154-44a4-49a5-b2e8-1a85342c94ce.png)'
- en: The idea behind the Haar-like feature selection algorithm is to compute the
    difference between the sum of white pixels and the sum of black pixels inside
    the rectangle.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Haar-like特征选择算法背后的思想是计算矩形内部白色像素总和与黑色像素总和之间的差异。
- en: The main advantage of this method is the fast sum computation using the integral
    image. This makes a Haar cascade ideal for real-time object detection. It requires
    less time for processing an image than algorithms like SURF described previously.
    This algorithm can also be implemented on embedded systems, like Raspberry Pi,
    because it is less computationally intensive and has less memory footprint. It
    is called Haar-like because it is based on the same principle as Haar wavelets.
    Haar cascades are widely used in human body detection, along with its parts like
    face and eye detection. It can also be used for expression analysis. The Haar
    cascade can be utilized for detecting vehicles like a car.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要优势是使用积分图像快速进行求和计算。这使得Haar级联非常适合实时物体检测。它处理图像所需的时间比之前描述的SURF等算法少。由于计算量较小且内存占用较少，该算法也可以在嵌入式系统（如Raspberry
    Pi）上实现。它被称为Haar-like，因为它基于与Haar小波相同的原理。Haar级联在人体检测中广泛使用，包括面部和眼部检测等部分。它还可以用于表情分析。Haar级联可用于检测车辆等物体。
- en: In this section, the Haar cascade is described for detecting faces and eyes
    from an image and webcam. Haar cascade is a machine learning algorithm, which
    needs to be trained to do a particular task. It is difficult to train a Haar cascade
    from scratch for a particular application, so OpenCV provides some trained XML
    files, which can be used to detect objects. These XML files are provided in the
    `opencv\data\haarcascades_cuda` folder of an OpenCV or CUDA installation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，描述了使用Haar级联从图像和摄像头中检测人脸和眼睛的方法。Haar级联是一种机器学习算法，需要对其进行训练以执行特定任务。对于特定应用从头开始训练Haar级联是困难的，因此OpenCV提供了一些训练好的XML文件，可用于检测对象。这些XML文件位于OpenCV或CUDA安装的`opencv\data\haarcascades_cuda`文件夹中。
- en: Face detection using Haar cascades
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Haar级联进行人脸检测
- en: 'We will use the Haar cascade for detecting faces from an image and live webcam
    in this section. The code for detecting faces from an image using the Haar cascade
    is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Haar级联从图像和实时摄像头中检测人脸。使用Haar级联从图像中检测人脸的代码如下：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: OpenCV and CUDA provide the `CascadeClassifier` class that can be used for implementing
    the Haar cascade. The create method is used to create an object of that class.
    It requires the filename of the trained XML file to be loaded. There is the `detectMultiScale`
    method of the created object, which detects an object at multiple scales from
    an image. It requires an image file and a `Gpumat` array to store the output results
    as arguments. This `gpumat` vector is converted to a standard rectangle vector
    by using the convert method of the `CascadeClassifier` object. This converted
    vector contains the coordinates for drawing a rectangle on the detected object.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV和CUDA提供了`CascadeClassifier`类，可用于实现Haar级联。使用create方法创建该类的对象。它需要加载训练好的XML文件的文件名。创建的对象有`detectMultiScale`方法，可以从图像中检测到多个尺度的对象。它需要一个图像文件和一个`Gpumat`数组作为参数来存储输出结果。使用`CascadeClassifier`对象的convert方法将此`gpumat`向量转换为标准矩形向量。此转换向量包含绘制检测到的对象矩形坐标。
- en: 'The `detectMultiScale` function has many parameters that can be modified before
    invoking the function. These include `scaleFactor` that is used to specify how
    much the image size will be reduced at each image scale, and `minNeighbors` specifies
    the value of the minimum neighbors each rectangle should have for retention; `minSize`
    specifies the minimum object size and `maxSize` the maximum object size. All these
    parameters have their default values, so in normal scenarios there is no need
    for modification. If we want to change them, then we can use the following code
    before invoking the `detectMultiscale` function:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`detectMultiScale`函数有许多参数可以在调用函数之前修改。这些包括用于指定每次图像缩放时图像大小将减少多少的`scaleFactor`，以及指定每个矩形应保留的最小邻居数`minNeighbors`；`minSize`指定最小对象大小，`maxSize`指定最大对象大小。所有这些参数都有默认值，所以在正常情况下通常不需要修改。如果我们想更改它们，那么在调用`detectMultiscale`函数之前可以使用以下代码：'
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This first function will set minimum neighbors to `0` and the second function
    will reduce the image size by a factor of `1.01` after every scale. The scale
    factor is very important for detecting objects with a different size. If it is
    large then the algorithm will take less time to complete, but some faces might
    not be detected. If it is small then the algorithm will take more time to complete,
    and it will be more accurate. The output of the preceding code is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数将设置最小邻居数为`0`，第二个函数将在每次缩放后通过一个因子`1.01`减小图像大小。缩放因子对于检测不同大小物体的检测非常重要。如果它很大，则算法完成所需时间会更短，但可能有些面部无法检测到。如果它很小，则算法完成所需时间会更长，并且会更准确。前述代码的输出如下：
- en: '![](img/aff00cf8-0f15-4aba-ae6e-620cb2744731.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aff00cf8-0f15-4aba-ae6e-620cb2744731.png)'
- en: From video
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从视频中
- en: 'The same concept of the Haar cascade can be used to detect faces from videos.
    The code for detecting a face is included inside the `while` loop so that the
    face is detected in every frame of a video. The code for face detection from a
    webcam is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Haar级联的相同概念可以用于从视频中检测人脸。检测人脸的代码包含在`while`循环中，以便在视频的每一帧中检测到人脸。从摄像头进行人脸检测的代码如下：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The webcam is initialized and frames from the webcam are captured one by one.
    This frame is uploaded to the device memory for processing on GPU. The object
    of the `CascadeClassifier` class is created by using the `create` method of the
    class. The XML file for face detection is provided as an argument while creating
    an object. Inside the `while` loop, the `detectMultiscale` method is applied to
    every frame so that faces of different sizes can be detected in each frame. The
    detected location is converted to a rectangle vector using the `convert` method.
    Then this vector is iterated using the `for` loop so that the bounding box can
    be drawn using the `rectangle` function on all the detected faces. The output
    of the program is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4605a82b-4693-4f9a-a19a-6226b654cca4.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Eye detection using Haar cascades
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will describe the use of Haar cascades in detecting the eyes of
    humans. The XML file for a trained Haar cascade for eye detection is provided
    in the OpenCV installation directory. This file is used to detect eyes. The code
    for it is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The code is similar to the code for face detection. This is the advantage of
    using Haar cascades. If an XML file for a trained Haar cascade on a given object
    is available then the same code will work on all applications. Just the name of
    the XML file needs to change when creating an object of the `CascadeClassifier`
    class. In the preceding code, `haarcascade_eye.xml` , which is a trained XML file
    for eye detection, is used. The other code is self-explanatory. The scale factor
    is set at `1.02` so that the image size will be reduced by `1.02` at every scale.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the eye detection program is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed98b373-902a-491b-adfb-2cfd7d5aff16.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: The sizes of eyes are different because of the viewpoint taken to capture the
    image, but still the Haar cascade is able to localize both eyes efficiently. The
    performance of the code can also be measured to see how quickly it can work.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, in this section we demonstrated the use of Haar cascades for face
    and eye detection. It is very simple to implement once the trained file is available,
    and it is a very powerful algorithm. It is widely used in embedded or mobile environments
    where memory and processing power are limited.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Object tracking using background subtraction
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Background subtraction is the process of separating out foreground objects
    from the background in a sequence of video frames. It is widely used in object
    detection and tracking applications to remove the background part. Background
    subtraction is performed in four steps:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Image preprocessing
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modeling of background
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detection of foreground
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data validation
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image preprocessing is always performed to remove any kind of noise present
    in the image. The second step is to model the background so that it can be separated
    from the foreground. In some applications, the first frame of the video is taken
    as the background and it is not updated. The absolute difference between each
    frame and the first frame is taken to separate foreground from background.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图像预处理始终执行以去除图像中存在的任何类型的噪声。第二步是建模背景，以便它可以与前景分离。在某些应用中，视频的第一帧被用作背景，并且不进行更新。通过计算每一帧与第一帧之间的绝对差值来分离前景和背景。
- en: In other techniques, the background is modeled by taking an average or median
    of all the frames that have been seen by the algorithm, and that background is
    separated from the foreground. It will be more robust for illumination changes
    and will produce a more dynamic background than the first method. Even more statistically
    intensive models, like Gaussian models and support vector models that use a history
    of frames, can be used for modeling a background.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他技术中，背景是通过取算法看到的所有帧的平均值或中值来建模的，并且该背景与前景分离。这种方法对于光照变化将更加稳健，并且会产生比第一种方法更动态的背景。还可以使用更统计密集的模型，如使用帧历史记录的高斯模型和支持向量模型来建模背景。
- en: The third step is to segregate the foreground from the modeled background by
    taking the absolute difference between the current frame and the background. This
    absolute difference is compared with the set threshold, and if it is greater than
    the threshold then the objects are considered moving, and if it is less than the
    threshold then the objects are stationary.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是通过计算当前帧与背景之间的绝对差值来将前景从建模的背景中分离出来。这个绝对差值与设定的阈值进行比较，如果它大于阈值，则认为物体是移动的；如果它小于阈值，则认为物体是静止的。
- en: Mixture of Gaussian (MoG) method
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合（MoG）方法
- en: MoG is a widely used background subtraction method used for separating foregrounds
    from backgrounds, based on Gaussian mixtures. The background is continuously updated
    from the sequence of frames. A mixture of K Gaussian distribution is used to categorize
    pixels as being foreground or background. The time sequence of the frame is also
    weighted to improve background modeling. The intensities that are continuously
    changing are categorized as foreground, and the intensities that are static are
    categorized as background.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: MoG是一种广泛使用的背景减法方法，用于根据高斯混合将前景从背景中分离出来。背景从帧序列中持续更新。使用K个高斯分布的混合来分类像素为前景或背景。帧的时间序列也被加权以改进背景建模。持续变化的强度被分类为前景，而静态的强度被分类为背景。
- en: 'OpenCV and CUDA provide an easy API to implement MoG for background subtraction.
    The code for this is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV和CUDA提供了一个简单的API来实现MoG背景减法。相应的代码如下：
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `createBackgroundSubtractorMOG` class is used to create an object for MoG
    implementation. It can be provided with some optional arguments while creating
    an object. The parameters include `history`, `nmixtures`, `backgroundRatio`, and
    `noiseSigma`. The `history` parameter signifies the number of previous frames
    used for modeling the background. Its default value is 200\. The `nmixture` parameter
    specifies the number of Gaussian mixtures used to segregate pixels. Its default
    value is 5\. You can play around with these values according to an application.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`createBackgroundSubtractorMOG`类来创建用于MoG实现的对象。在创建对象时可以提供一些可选参数。这些参数包括`history`、`nmixtures`、`backgroundRatio`和`noiseSigma`。`history`参数表示用于建模背景的先前帧的数量。其默认值是200。`nmixture`参数指定用于分离像素的高斯混合的数量。其默认值是5。您可以根据应用程序的需要调整这些值。
- en: The `apply` method of the created object is used to create a foreground mask
    from the first frame. It requires an input image and an image array to store the
    foreground mask and learning rate as the input. This foreground mask and the background
    image are continuously updated after every frame inside the `while` loop. The
    `getBackgroundImage` function is used to fetch the current background model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的对象的`apply`方法用于从第一帧创建前景掩码。它需要一个输入图像和一个图像数组作为输入来存储前景掩码和学习率。在`while`循环的每一帧之后，都会持续更新前景掩码和背景图像。`getBackgroundImage`函数用于获取当前的背景模型。
- en: The foreground mask is used to create a foreground image that indicates which
    objects are currently moving. It is basically logical and operates between the
    original frame and foreground mask. The foreground mask, foreground image, and
    the modeled background are downloaded to host memory after every frame for displaying
    on the screen.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 前景掩码用于创建一个前景图像，指示哪些对象目前正在移动。它基本上是逻辑的，在原始帧和前景掩码之间操作。前景掩码、前景图像和建模的背景在每帧之后下载到主机内存，以便在屏幕上显示。
- en: 'The MoG model is applied to a video from the PETS 2009 dataset, which is widely
    used for pedestrian detection. It has a static background and persons are moving
    around in the video. The output of two different frames from the video is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: MoG模型应用于PETS 2009数据集的视频，该数据集广泛用于行人检测。它具有静态背景，视频中有人员在移动。视频的两个不同帧的输出如下：
- en: '![](img/506e1519-220b-44a8-8b2d-6d17caefa52d.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/506e1519-220b-44a8-8b2d-6d17caefa52d.png)'
- en: 'As can be seen, MoG models the background very efficiently. Only the persons
    who are moving are present in the foreground mask and foreground image. This foreground
    image can be used for further processing of the detected objects. If a human stops
    walking, then he will start being part of the background, as can be seen from
    the result of the second frame. So this algorithm can only be used to detect moving
    objects. It will not consider static objects. The performance of MoG in terms
    of frame rate is as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，MoG非常有效地建模背景。只有移动的人存在于前景掩码和前景图像中。此前景图像可用于检测到的对象的进一步处理。如果一个人停止行走，那么他将成为背景的一部分，如第二帧的结果所示。因此，此算法只能用于检测移动对象。它不会考虑静态对象。MoG在帧率方面的性能如下：
- en: '![](img/0fdcd137-6d13-4bf1-9f43-e96a16a60237.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fdcd137-6d13-4bf1-9f43-e96a16a60237.png)'
- en: The frame rate is updated after every frame. It can be seen that it is around
    330 frames per second, which is very high and easily used in real-time applications.
    OpenCV and CUDA also provide the second version of MoG, which can be invoked by
    the `createBackgroundSubtractorMOG2` class.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 每帧更新帧率。可以看出，它大约是每秒330帧，这非常高，易于用于实时应用。OpenCV和CUDA还提供了MoG的第二版本，可以通过`createBackgroundSubtractorMOG2`类调用。
- en: GMG for background subtraction
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GMG用于背景减法
- en: The name of the algorithm GMG is derived from the initials of the inventors
    who proposed the algorithm. The algorithm is a combination of background estimation
    and Bayesian segmentation per pixel. It uses Bayesian inference to separate the
    background from the foreground. It also uses the history of frames for modeling
    the background. It is again weighted based on the time sequence of the frame.
    The new observation is weighted more than the old observations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: GMG算法的名称GMG来源于提出该算法的发明者的首字母。该算法是背景估计和每像素贝叶斯分割的组合。它使用贝叶斯推理将背景与前景分开。它还使用帧的历史记录来建模背景。它再次根据帧的时间序列进行加权。新的观测值比旧的观测值加权更多。
- en: 'OpenCV and CUDA provide a similar API to MoG for implementation of the GMG
    algorithm. The code for implementing the GMG algorithm for background subtraction
    is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV和CUDA为GMG算法的实现提供了与MoG类似的API。实现背景减法的GMG算法的代码如下：
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `createBackgroundSubtractorGMG` class is used to create an object for GMG
    implementation. It can be provided with two arguments while creating an object.
    The first argument is the number of previous frames used for modeling the background.
    It is taken as `40` in the code above. The second argument is the decision threshold,
    which is used to categorize pixels as foreground. Its default value is 0.8\.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`createBackgroundSubtractorGMG`类创建用于GMG实现的对象。在创建对象时可以提供两个参数。第一个参数是用于建模背景的先前帧数。在上述代码中取为`40`。第二个参数是决策阈值，用于将像素分类为前景。其默认值为0.8。
- en: 'The `apply` method of the created object is used on the first frame to create
    a foreground mask. The foreground mask and foreground image are continuously updated
    inside the `while` loop by using the history of frames. The foreground mask is
    used to create a foreground image in a similar way as shown for MoG. The output
    of the GMG algorithm on the same video and two frames is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的对象的`apply`方法用于第一帧以创建前景掩码。通过使用帧的历史记录，前景掩码和前景图像在`while`循环内部持续更新。前景掩码用于以类似于MoG所示的方式创建前景图像。GMG算法在相同视频和两个帧上的输出如下：
- en: '![](img/7fc9de7e-58f0-450f-9613-13d086fb19fc.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fc9de7e-58f0-450f-9613-13d086fb19fc.png)'
- en: 'The output of GMG is noisy compared to MoG. The morphological opening and closing
    operation can be applied to the result of GMG to remove shadowing noise present
    in the result. The performance of the GMG algorithm in terms of FPS is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 与MoG相比，GMG的输出噪声更大。可以对GMG的结果应用形态学开闭操作，以去除结果中存在的阴影噪声。GMG算法在FPS方面的性能如下：
- en: '![](img/2e29cf7f-4dac-4595-b16b-f2339c01b284.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2e29cf7f-4dac-4595-b16b-f2339c01b284.png)'
- en: As it is more computationally intensive than MoG, the FPS rate is less, but
    still the FPS performance is 120, which is more than 30 FPS for real-time performance.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它比MoG计算量更大，帧率较低，但仍然达到120 FPS，这比实时性能所需的30 FPS要高。
- en: To summarize, in this section we have seen two methods for background modeling
    and background subtraction. MoG is faster and less noisy compared to the GMG algorithm.
    The GMG algorithm requires morphological operations to remove noise present in
    the result.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本节中我们看到了两种背景建模和背景减法的方法。与GMG算法相比，MoG算法更快且噪声更少。GMG算法需要形态学操作来去除结果中存在的噪声。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter described the role of OpenCV and CUDA in real-time object detection
    and tracking applications. It started with the introduction of object detection
    and tracking, along with challenges encountered in that process and the applications
    of it. Different features like color, shape, histograms, and other distinct key-points,
    like corners, can be used to detect and track objects in an image. Color-based
    object detection is easier to implement, but it requires that the object should
    have a distinct color from the background. For shape-based object detection, the
    Canny edge detection technique has been described to detect edges, and Hough transform
    has been described for straight line and circle detection. It has many applications,
    such as land detection, ball tracking, and so on. The color and shape are global
    features, which are easier to compute and require less memory. They are more susceptible
    to noise. Other algorithms like FAST, ORB, and SURF have been described in detail,
    which can be used to detect key-points from the image, and these key-points can
    be used to describe an image accurately, which in turn can be used to detect an
    object in the image. ORB is open source and gives a comparable result to SURF
    at no cost. SURF is patented but it is faster, scale-invariant, and rotation-invariant.
    The Haar cascade has been described, which is a simple algorithm used to detect
    objects like faces, eyes, and the human body from an image. It can be used in
    embedded systems for real-time applications. The last part of the chapter described
    background subtraction algorithms in detail, such as MoG and GMG, which can separate
    foregrounds from backgrounds. The output of these algorithms can be used for object
    detection and tracking. The next chapter will describe how these applications
    can be deployed on embedded development boards.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了OpenCV和CUDA在实时目标检测和跟踪应用中的作用。它从目标检测和跟踪的介绍开始，包括在这个过程中遇到的问题和它的应用。不同的特征，如颜色、形状、直方图和其他独特的关键点，如角点，可以用来检测和跟踪图像中的对象。基于颜色的目标检测更容易实现，但要求对象与背景有明显的颜色差异。对于基于形状的目标检测，已经描述了Canny边缘检测技术来检测边缘，以及Hough变换用于直线和圆的检测。它有许多应用，如土地检测、球跟踪等。颜色和形状是全局特征，更容易计算且需要的内存较少。它们更容易受到噪声的影响。其他算法如FAST、ORB和SURF已经详细描述，这些算法可以用来从图像中检测关键点，这些关键点可以用来准确描述图像，进而可以用来检测图像中的对象。ORB是开源的，并且无需成本就能提供与SURF相当的结果。SURF是专利的，但它更快，具有尺度不变性和旋转不变性。已经描述了Haar级联，这是一个简单的算法，用于从图像中检测对象，如人脸、眼睛和人体。它可以用于嵌入式系统中的实时应用。本章的最后部分详细描述了背景减法算法，如MoG和GMG，这些算法可以将前景与背景分离。这些算法的输出可以用于目标检测和跟踪。下一章将描述如何将这些应用部署在嵌入式开发板上。
- en: Questions
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Write an OpenCV code for detecting objects of yellow color from the video.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个OpenCV代码，用于从视频中检测黄色对象。
- en: In which situations will the object detection using color fail?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在哪些情况下，使用颜色进行的目标检测会失败？
- en: Why is the Canny edge detection algorithm better than the other edge detection
    algorithm seen in the last chapter?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么Canny边缘检测算法比上一章中看到的其他边缘检测算法更好？
- en: What can be done to reduce the noise sensitivity of Hough transform?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以采取什么措施来降低Hough变换的噪声敏感性？
- en: What is the importance of the threshold in the FAST key point detector?
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在FAST关键点检测器中，阈值的重要性是什么？
- en: What is the importance of the Hessian threshold in the SURF detector?
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在SURF检测器中，Hessian阈值的重要性是什么？
- en: If the scale factor in a Haar cascade is changed from 1.01 to 1.05 then what
    effect will it have on output?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果Haar级联中的尺度因子从1.01变为1.05，那么它将对输出产生什么影响？
- en: Compare the MoG and GMG background subtraction methods. What can be done to
    remove noise from GMG outputs?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较MoG和GMG背景减法方法。如何从GMG输出中去除噪声？
