- en: Object Detection and Tracking Using OpenCV and CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last chapter described basic computer vision operations using OpenCV and
    CUDA. In this chapter, we will see how to use these basic operations along with
    OpenCV and CUDA to develop complex computer vision applications. We will use the
    example of object detection and tracking to demonstrate this concept. Object detection
    and tracking is a very active area of research in computer vision. It deals with
    identifying the location of an object in an image and tracking it in a sequence
    of frames. Many algorithms are proposed for this task based on color, shape, and
    the other salient features of an image. In this chapter, these algorithms are
    implemented using OpenCV and CUDA. We start with an explanation of detecting an
    object based on color, then describe the methods to detect an object with a particular
    shape. All objects have salient features that can be used to detect and track
    objects. This chapter describes the implementation of different feature detection
    algorithms and how they can be used to detect objects. The last part of the chapter
    will demonstrate the use of a background subtraction technique that separates
    the foreground from the background for object detection and tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to object detection and tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection and tracking based on color
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection and tracking based on a shape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature-based object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection using Haar cascade
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Background subtraction methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires a good understanding of image processing and computer
    vision. It also requires some basic knowledge of algorithms used for object detection
    and tracking. It needs familiarity with the basic C or C++ programming language,
    CUDA, and all the codes explained in previous chapters. All the code used in this
    chapter can be downloaded from the following GitHub link: [https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA).
    The code can be executed on any operating system, though it has only been tested
    on Ubuntu 16.04\.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2PSRqkU](http://bit.ly/2PSRqkU)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to object detection and tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Object detection and tracking is an active research topic in the field of computer
    vision that makes efforts to detect, recognize, and track objects through a series
    of frames. It has been found that object detection and tracking in the video sequence
    is a challenging task and a very time-consuming process. Object detection is the
    first step in building a larger computer vision system. A large amount of information
    can be derived from the detected object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The detected object can be classified into a particular class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be tracked in an image sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information about the scene or other object inferences can be derived from
    the detected object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object tracking is defined as the task of detecting objects in every frame of
    the video and establishing the correspondence between the detected objects from
    one frame to the other.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of object detection and tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object detection and tracking can be used to develop video surveillance systems
    to track suspicious activities, events, and persons. It can be used for developing
    an intelligent traffic system to track vehicles and detect traffic rule violations.
    Object detection is essential in autonomous vehicles to give them information
    about the surroundings and planning for their navigation. It is also useful for
    pedestrian detection or vehicle detection in automatic driver assistance systems.
    It can be used in the medical field for applications like breast cancer detection
    or brain tumor detection and so on. It can be used for face and hand gesture recognition.
    It has a wide application in industrial assembly and quality control in production
    lines. It is vital for image retrieval from search engines and for photo management.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object detection is a challenging task because images in real life are affected
    by noise, illumination variation, dynamic backgrounds, shadowing effect, camera
    jitter, and motion blur. Object detection is difficult when an object to be detected
    is rotated, scaled, or under occlusion. Many applications require detecting more
    than one object class. If a large number of classes are being detected then the
    processing speed becomes an important issue along with the kinds of classes that
    the system can handle without accuracy loss.
  prefs: []
  type: TYPE_NORMAL
- en: There are many algorithms that overcome some of these challenges. They are discussed
    in this chapter. The chapter does not describe the algorithms in detail, but more
    focus is given on how it can be implemented using CUDA and OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection and tracking based on color
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An object has many global features like color and shape, which describe the
    object as a whole. These features can be utilized for the detection of an object
    and tracking it in a sequence of frames. In this section, we will use color as
    a feature to detect an object with a particular color. This method is useful when
    an object to be detected is of a specific color and this color is different to
    the color of the background. If the object and background have the same color,
    then this method for detection will fail. In this section, we will try to detect
    any object with a blue color from a webcam stream using OpenCV and CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: Blue object detection and tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first question that should come to your mind is which color space should
    be used for segmenting blue color. A **Red Green Blue** (**RGB**) color space
    does not separate color information from intensity information. The color spaces
    that separate color information from intensity, like **Hue Saturation Value**
    (HSV) and **YCrCb** (where Yâ€² is the luma component and CB and CR are the blue-difference
    and red-difference chroma components), are ideal for this kind of task. Every
    color has a specific range in the hue channel that can be utilized for detection
    of that color. The boilerplate code for starting the webcam, capturing frames,
    and uploading on-device memory for a GPU operation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To detect the blue color, we need to find a range for blue color in the HSV
    color space. If a range is accurate then the detection will be accurate. The range
    of blue color for three channels, hue, saturation, and value, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This range will be used to threshold an image in a particular channel to create
    a mask for the blue color. If this mask is again ANDed with the original frame,
    then only a blue object will be there in the resultant image. The code for this
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The frame from the webcam is converted to an HSV color space. The blue color
    has a different range in three channels, so each channel has to be thresholded
    individually. The channels are split using the `split` method and thresholded
    using the `threshold` function. The minimum and maximum ranges for each channel
    are used as lower and upper thresholds. The channel value inside this range will
    be converted to white and others are converted to black. These three thresholded
    channels are logically ANDed to get a final mask for a blue color. This mask can
    be used to detect and track an object with a blue color from a video.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of two frames, one without the blue object and the other with the
    blue object, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32bdf79b-65c0-4e98-a877-663ff2d52588.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen from the result, when a frame does not contain any blue object,
    the mask is almost black; whereas in the frame below, when the blue object comes
    into frame, that part turns white. This method will only work when the background
    does not contain the color of an object.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection and tracking based on shape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The shape of an object can also be utilized as a global feature to detect an
    object with a distinct shape. This shape can be a straight line, polygons, circles,
    or any other irregular shapes. Object boundaries, edges, and contours can be utilized
    to detect an object with a particular shape. In this section, we will use the
    Canny edge detection algorithm and Hough transform to detect two regular shapes,
    which are a line and a circle.
  prefs: []
  type: TYPE_NORMAL
- en: Canny edge detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we saw various high pass filters, which can be used as
    edge detectors. In this section, the Canny edge detection algorithm, which combines
    Gaussian filtering, gradient finding, non-maximum suppression, and hysteresis
    thresholding, is implemented using OpenCV and CUDA. High pass filters, as explained
    in the last chapter, are very sensitive to noise. In Canny edge detection, Gaussian
    smoothing is done before detecting edges, which makes it less sensitive to noises.
    It also has a non-maximum suppression stage after detecting edges to remove unnecessary
    edges from the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Canny edge detection is a computationally intensive task, which is hard to
    use in real-time applications. The CUDA version of the algorithm can be used to
    accelerate it. The code for implementing a Canny edge detection algorithm is described
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenCV and CUDA provides the `createCannyEdgeDetector` class for Canny edge
    detection. The object of this class is created, and many arguments can be passed
    while creating it. The first and second arguments are the low and high thresholds
    for hysteresis thresholding. If the intensity gradient at a point is greater then
    the maximum threshold, then it is categorized as an edge point. If the gradient
    is less than the low threshold, then the point is not an edge point. If the gradient
    is in between thresholds, then whether the point is an edge or not is decided
    based on connectivity. The third argument is the aperture size for the edge detector.
    The final argument is the Boolean argument, which indicates whether to use `L2_norm`
    or `L1_norm` for gradient magnitude calculation. `L2_norm` is computationally
    expensive but it is more accurate. The true value indicates the use of `L2_norm`.
    The output of the code is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9330eca7-5962-4191-8fc5-82c8628a870f.png)'
  prefs: []
  type: TYPE_IMG
- en: You can play around with the values of the lower and upper thresholds to detect
    edges more accurately for a given image. Edge detection is a very important preprocessing
    step for many computer vision applications and Canny edge detection is widely
    used for that.
  prefs: []
  type: TYPE_NORMAL
- en: Straight line detection using Hough transform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The detection of straight lines is important in many computer vision applications,
    like lane detection. It can also be used to detect lines that are part of other
    regular shapes. Hough transform is a popular feature extraction technique used
    in computer vision to detect straight lines. We will not go into detail about
    how Hough transform detects lines, but we will see how it can be implemented in
    OpenCV and CUDA. The code for implementing Hough transform for line detection
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: OpenCV provides the `createHoughSegmentDetector` class for implementing Hough
    transform. It needs an edge map of an image as input. So edges are detected from
    an image using a Canny edge detector. The output of the Canny edge detector is
    uploaded to the device memory for GPU computation. The edges can also be computed
    on GPU as discussed in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: The object of `createHoughSegmentDetector` is created. It requires many arguments.
    The first argument indicates the resolution of parameter `r` used in Hough transform,
    which is taken as 1 pixel normally. The second argument is the resolution of parameter
    theta in radians, which is taken as 1 radian or pi/180\. The third argument is
    the minimum number of points that are needed to form a line, which is taken as
    50 pixels. The final argument is the maximum gap between two points to be considered
    as the same line, which is taken as 5 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: The detect method of the created object is used to detect straight lines. It
    needs two arguments. The first argument is the image on which the edges are to
    be detected, and the second argument is the array in which detected line points
    will be stored. The array contains the starting and ending (x,y) points of the
    detected lines. This array is iterated using the `for` loop to draw individual
    lines on an image using the line function from OpenCV. The final image is displayed
    using the `imshow` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hough transform is a mathematically intensive step. Just to show an advantage
    of CUDA, we will implement the same algorithm for CPU and compare the performance
    of it with a CUDA implementation. The CPU code for Hough transform is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `HoughLinesP` function is used for detecting lines on a CPU using probabilistic
    Hough transform. The first two arguments are the source image and the array to
    store output line points. The third and fourth arguments are a resolution for
    `r` and theta. The fifth argument is the threshold that indicates the minimum
    number of intersection points for a line. The sixth argument indicates the minimum
    number of points needed to form a line. The last argument indicates the maximum
    gap between points to be considered on the same line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The array returned by the function is iterated using the `for` loop for displaying
    detected lines on the original image. The output for both the GPU and CPU function
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7942ad86-ce16-4db4-9fcb-31724c0d1985.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The comparison between the performance of the GPU and CPU code for the Hough
    transform is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62f0224e-327a-4efc-a17f-bb9679864639.png)'
  prefs: []
  type: TYPE_IMG
- en: It takes around 4 ms for a single image to process on the CPU and 1.5 ms on
    the GPU, which is equivalent to 248 FPS on the CPU, and 632 FPS on the GPU, which
    is almost 2.5 times an improvement on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Circle detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hough transform can also be used for circle detection. It can be used in many
    applications, like ball detection and tracking and coin detection, and so on,
    where objects are circular. OpenCV and CUDA provide a class to implement this.
    The code for coin detection using Hough transform is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There is a `createHoughCirclesDetector` class for detecting the circular object.
    The object of that class is created. Many arguments can be provided while creating
    an object of this class. The first argument is `dp` that signifies an inverse
    ratio of the accumulator resolution to the image resolution, which is mostly taken
    as 1\. The second argument is the minimum distance between the centers of the
    detected circle. The third argument is a Canny threshold and the fourth argument
    is the accumulator threshold. The fifth and sixth arguments are the minimum and
    maximum radiuses of the circles to be detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The minimum distance between the centers of the circle is taken as `100` pixels.
    You can play around with this value. If this is decreased, then many circles are
    detected falsely on the original image, while if it is increased then some true
    circles may be missed. The last two arguments, which are the minimum and maximum
    radiuses, can be taken as `0` if you don''t know the exact dimension. In the preceding
    code, it is taken as `1` and maximum dimension of an image to detect all circles
    in an image. The output of the program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7076793-815c-4a5f-8693-ba885a00feb0.png)'
  prefs: []
  type: TYPE_IMG
- en: The Hough transform is very sensitive to Gaussian and salt-pepper noise. So,
    sometimes it is better to preprocess the image with Gaussian and median filters
    before applying Hough transform. It will give more accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we have used the Hough line and circle transforms to detect objects
    with regular shapes. Contours and convexity can also be used for shape detection.
    The functions for this are available in OpenCV, but they are not available with
    CUDA implementation. You will have to develop your own versions of these functions.
  prefs: []
  type: TYPE_NORMAL
- en: Key-point detectors and descriptors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Till this point, we have used global features like color and shape to detect
    an object. These features are easy to compute, are quick, and require a small
    amount of memory, but they can only be used when some information regarding the
    object is already available. If that is not the case then local features are used,
    which require more computation and memory, but they are more accurate. In this
    section, various algorithms that find local features are explained. They are also
    called key point detectors. Key-points are the points that characterize the image
    and can be used to define an object accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Features from Accelerated Segment Test (FAST) feature detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The FAST algorithm is used to detect corner points as key-points from an image.
    It detects the corners by applying a segment test to every pixel. It considers
    a circle of 16 pixels around the pixel. If there are *n* continuous points in
    a circle of radius 16, which have the intensity of pixel greater than *Ip +t*
    or less than *Ip- t,* then that pixel is considered a corner. *Ip* is intensity
    at pixel *p,* and *t* is the selected threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes instead of checking all points in the radius, a few selected points
    are checked for intensity values to determine corner points. It accelerates the
    performance of the FAST algorithm. FAST provides corner points that can be utilized
    as key-points to detect an object. It is rotation-invariant, as corners of an
    object will remain the same even if the object is rotated. FAST is not scale-invariant,
    as the increase in dimension may result in a smooth transition of intensity values
    rather than a sharp transition at corners.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV and CUDA provide an efficient way of implementing the FAST algorithm.
    The program to detect key-points using the FAST algorithm is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: OpenCV and CUDA provide a `FastFeatureDetector` class for implementing the FAST
    algorithm. The object of this class is created using the create method of the
    class. It needs three arguments. The first argument is the intensity threshold
    to be used for the FAST algorithm. The second argument specifies whether to use
    non-maximum suppression or not. It is a Boolean value, which can be specified
    as `true` or `false`. The third argument indicates which FAST method is used for
    calculating the neighborhood. Three methods, `cv2.FAST_FEATURE_DETECTOR_TYPE_5_8`,
    `cv2.FAST_FEATURE_DETECTOR_TYPE_7_12`, and `cv2.FAST_FEATURE_DETECTOR_TYPE_9_16`
    , are available, which can be specified as flags `0`, `1`, or `2`.
  prefs: []
  type: TYPE_NORMAL
- en: The detect method of the created object is used to detect key-points. It needs
    an input image and vector to store key-points as an argument. The calculated key-points
    can be drawn on an original image using the `drawkey-points` function. It requires
    source image, the vector of the key-points and the destination image as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intensity threshold can be changed to detect the different number of key-points.
    If the threshold is low, then more key-points will pass the segment test and will
    be categorized as key-points. As this threshold is increased, the number of key-points
    detected will gradually decrease. In the same way, if non-maximum suppression
    is false then more than one key point is detected at a single corner point. The
    output of the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b5da030-d05b-4e4b-aa85-80d08b02719f.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen from the output, as the threshold increases from 10 to 50 and
    100, the number of key-points decreases. These key-points can be used to detect
    an object in a query image.
  prefs: []
  type: TYPE_NORMAL
- en: Oriented FAST and Rotated BRIEF (ORB) feature detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ORB is a very efficient feature detection and description algorithm. It is a
    combination of the FAST algorithm for feature detection and the **Binary Robust
    Independent Elementary Features** (**BRIEF**) algorithm for feature description.
    It provides an efficient alternative to the SURF and SIFT algorithms, which are
    widely used for object detection. As they are patented, their use should be paid
    for. ORB matches the performance of SIFT and SURF at no cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV and CUDA provide an easy API for implementing the ORB algorithm. The
    code for implementing the ORB algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The object of the `ORB` class is created using the create method. All the arguments
    to this method are optional so we are using the default values for it. The detect
    method of the created object is used to detect key-points from an image. It requires
    an input image and the vector of key-points in which the output will be stored
    as arguments. The detected key-points are drawn on the image using the `drawkey-points`
    function. The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/979882c6-5901-4d4b-b18d-5dca9b669df7.png)'
  prefs: []
  type: TYPE_IMG
- en: The `ORB` class also provides a method to calculate descriptors for all the
    key-points. These descriptors can accurately describe the object and can be used
    to detect objects from an image. These descriptors can also be used to classify
    an object.
  prefs: []
  type: TYPE_NORMAL
- en: Speeded up robust feature detection and matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SURF approximates Laplacian of Gaussian with computation based on a simple
    two-dimensional box filter as described in the last chapter. The convolution with
    the box filter can be easily calculated with the help of integral images, which
    improves the performance of the algorithm. SURF relies on the determinant of the
    Hessian matrix for both scale and location. The approximated determinant of Hessian
    can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/894254f9-4c9b-40da-b0d0-9c0636194147.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *w* is a relative weight for the filter response and used to balance the
    expression for the determinant. The *Dx*, *Dy* are the result of the Laplacian
    operator in *X*- and *Y*-direction.
  prefs: []
  type: TYPE_NORMAL
- en: SURF uses wavelet responses in horizontal and vertical directions, using an
    integral image approach for orientation assignment. Adequate Gaussian weights
    are also applied to it. The dominant orientation is estimated by calculating the
    sum of all responses within a sliding orientation window of angle 60 degrees.
  prefs: []
  type: TYPE_NORMAL
- en: For feature description, SURF uses Haar wavelet responses in horizontal and
    vertical directions. This is computed for all subregions in an image, resulting
    in a SURF feature descriptor with a total of 64 dimensions. The lower the dimensions,
    the higher the speed of computation and matching will be. For more precision,
    the SURF feature descriptor has an extended 128-dimension version. SURF is rotation-invariant
    and scale-invariant.
  prefs: []
  type: TYPE_NORMAL
- en: SURF has a higher processing speed than SIFT because it uses a 64-directional
    feature vector compared to SIFT, which uses a 128-dimensional feature vector.
    SURF is good at handling images with blurring and rotation, but not good at handling
    a viewpoint change and illumination change.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV and CUDA provide an API to calculate SURF key-points and descriptors.
    We will also see how these can be used to detect an object in the query image.
    The code for SURF feature detection and matching is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Two images are read from the disk. The first image contains the object to be
    detected. The second image is the query image in which the object is to be searched.
    We will calculate SURF features from both the images, and then these features
    will be matched for detecting an object from a query image.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV provides the `SURF_CUDA` class for calculating SURF features. The object
    of this class is created. It requires the Hessian threshold as an argument. It
    is taken as `150`. This threshold determines how large the output from the Hessian
    determinant calculation must be in order for a point to be considered as a key
    point. A larger threshold value will result in fewer but more salient interest
    points, and a smaller value will result in more numerous but less salient points.
    It can be chosen according to the application.
  prefs: []
  type: TYPE_NORMAL
- en: This `surf` object is used to calculate key-points and descriptors from both
    the object and query image. The image, data type of the image, vectors to store
    key-points, and descriptors are passed as an argument. To match the object in
    the query image, descriptors from both the images need to be matched. OpenCV provides
    the different type of matching algorithms for this purpose, like the Brute-Force
    matcher and the **Fast Library for Approximate Nearest Neighbors** (**FLANN**)
    matcher.
  prefs: []
  type: TYPE_NORMAL
- en: The Brute-Force matcher is used in the program; it is a simple method. It takes
    the descriptor of one feature in an object that is matched with all other features
    in the query image, using some distance calculation. It returns the best match
    key point, or best `k` matches using the nearest neighbor algorithm when using
    the `knnMatch` method of the `matcher` class. The `knnMatch` method requires two
    sets of descriptors along with the number of nearest neighbors. It is taken as
    `3` in the code.
  prefs: []
  type: TYPE_NORMAL
- en: The good, matching key-points are extracted from the matching points returned
    by the `knnMatch` method. These good matches are found out by using the ratio
    test method, described in the original paper. These good matches are used to detect
    an object from a scene.
  prefs: []
  type: TYPE_NORMAL
- en: The `drawMatches` function is used to draw a line between matched good points
    from both the images. It requires many arguments. The first argument is the source
    image, the second image is the key-points of the source image, the third argument
    is the second image, the fourth argument is the key-points of the second image,
    and the fifth argument is the output image. The sixth argument is the color of
    the lines and key-points. It is taken as `Scalar::all(-1)` , which indicates that
    a color will be taken randomly. The seventh argument is the color of the key-points,
    which do not have any matches. It is also taken as `Scalar::all(-1)` , which indicates
    that a color will be taken randomly. The last two arguments specify a mask to
    draw matches and flag settings. The empty mask is taken so that all matches are
    drawn.
  prefs: []
  type: TYPE_NORMAL
- en: 'These matches can be used to draw a bounding box around the detected object,
    which will localize the object from the scene. The code for drawing a bounding
    box is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: OpenCV provides the `findHomography` function to search for the position, orientation,
    and scale of the object in the scene, based on the good matches. The first two
    arguments are good, matching key-points from the object and scene images. The
    **random sample consensus** (**RANSAC**) method is passed as one of the arguments
    that is used to find the best translation matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'After finding this translation matrix, the `perspectiveTransform` function
    is used to find an object. It requires four corner points and a translation matrix
    as an argument. These transformed points are used to draw a bounding box around
    the detected object. The output of the SURF program for finding features and matching
    objects is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbe37af6-d0c7-44ba-a414-69bebff1e47b.png)'
  prefs: []
  type: TYPE_IMG
- en: The figure contains the object image, query image, and detected image. As can
    be seen from the preceding image, SURF can accurately determine the position of
    the object even if the object is rotated. Though sometimes it may detect the wrong
    features. The Hessian threshold and test ratio can be varied to find the optimum
    matches.
  prefs: []
  type: TYPE_NORMAL
- en: So to summarize, in this section we have seen FAST, ORB, and SURF key point
    detection algorithms. We have also seen how these points can be used to match
    and localize an object in an image by using SURF features as an example. You can
    try using FAST and ORB features to do the same. In the next section, we discuss
    Haar cascades in detail for detecting faces and eyes from an image.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection using Haar cascades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Haar cascade uses rectangular features to detect an object. It uses rectangles
    of different sizes to calculate different line and edge features. The rectangle
    contains some black and white regions, as shown in the following figure, and they
    are centered at different positions in an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21388154-44a4-49a5-b2e8-1a85342c94ce.png)'
  prefs: []
  type: TYPE_IMG
- en: The idea behind the Haar-like feature selection algorithm is to compute the
    difference between the sum of white pixels and the sum of black pixels inside
    the rectangle.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of this method is the fast sum computation using the integral
    image. This makes a Haar cascade ideal for real-time object detection. It requires
    less time for processing an image than algorithms like SURF described previously.
    This algorithm can also be implemented on embedded systems, like Raspberry Pi,
    because it is less computationally intensive and has less memory footprint. It
    is called Haar-like because it is based on the same principle as Haar wavelets.
    Haar cascades are widely used in human body detection, along with its parts like
    face and eye detection. It can also be used for expression analysis. The Haar
    cascade can be utilized for detecting vehicles like a car.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, the Haar cascade is described for detecting faces and eyes
    from an image and webcam. Haar cascade is a machine learning algorithm, which
    needs to be trained to do a particular task. It is difficult to train a Haar cascade
    from scratch for a particular application, so OpenCV provides some trained XML
    files, which can be used to detect objects. These XML files are provided in the
    `opencv\data\haarcascades_cuda` folder of an OpenCV or CUDA installation.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection using Haar cascades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the Haar cascade for detecting faces from an image and live webcam
    in this section. The code for detecting faces from an image using the Haar cascade
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: OpenCV and CUDA provide the `CascadeClassifier` class that can be used for implementing
    the Haar cascade. The create method is used to create an object of that class.
    It requires the filename of the trained XML file to be loaded. There is the `detectMultiScale`
    method of the created object, which detects an object at multiple scales from
    an image. It requires an image file and a `Gpumat` array to store the output results
    as arguments. This `gpumat` vector is converted to a standard rectangle vector
    by using the convert method of the `CascadeClassifier` object. This converted
    vector contains the coordinates for drawing a rectangle on the detected object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `detectMultiScale` function has many parameters that can be modified before
    invoking the function. These include `scaleFactor` that is used to specify how
    much the image size will be reduced at each image scale, and `minNeighbors` specifies
    the value of the minimum neighbors each rectangle should have for retention; `minSize`
    specifies the minimum object size and `maxSize` the maximum object size. All these
    parameters have their default values, so in normal scenarios there is no need
    for modification. If we want to change them, then we can use the following code
    before invoking the `detectMultiscale` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This first function will set minimum neighbors to `0` and the second function
    will reduce the image size by a factor of `1.01` after every scale. The scale
    factor is very important for detecting objects with a different size. If it is
    large then the algorithm will take less time to complete, but some faces might
    not be detected. If it is small then the algorithm will take more time to complete,
    and it will be more accurate. The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aff00cf8-0f15-4aba-ae6e-620cb2744731.png)'
  prefs: []
  type: TYPE_IMG
- en: From video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The same concept of the Haar cascade can be used to detect faces from videos.
    The code for detecting a face is included inside the `while` loop so that the
    face is detected in every frame of a video. The code for face detection from a
    webcam is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The webcam is initialized and frames from the webcam are captured one by one.
    This frame is uploaded to the device memory for processing on GPU. The object
    of the `CascadeClassifier` class is created by using the `create` method of the
    class. The XML file for face detection is provided as an argument while creating
    an object. Inside the `while` loop, the `detectMultiscale` method is applied to
    every frame so that faces of different sizes can be detected in each frame. The
    detected location is converted to a rectangle vector using the `convert` method.
    Then this vector is iterated using the `for` loop so that the bounding box can
    be drawn using the `rectangle` function on all the detected faces. The output
    of the program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4605a82b-4693-4f9a-a19a-6226b654cca4.png)'
  prefs: []
  type: TYPE_IMG
- en: Eye detection using Haar cascades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will describe the use of Haar cascades in detecting the eyes of
    humans. The XML file for a trained Haar cascade for eye detection is provided
    in the OpenCV installation directory. This file is used to detect eyes. The code
    for it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The code is similar to the code for face detection. This is the advantage of
    using Haar cascades. If an XML file for a trained Haar cascade on a given object
    is available then the same code will work on all applications. Just the name of
    the XML file needs to change when creating an object of the `CascadeClassifier`
    class. In the preceding code, `haarcascade_eye.xml` , which is a trained XML file
    for eye detection, is used. The other code is self-explanatory. The scale factor
    is set at `1.02` so that the image size will be reduced by `1.02` at every scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the eye detection program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed98b373-902a-491b-adfb-2cfd7d5aff16.png)'
  prefs: []
  type: TYPE_IMG
- en: The sizes of eyes are different because of the viewpoint taken to capture the
    image, but still the Haar cascade is able to localize both eyes efficiently. The
    performance of the code can also be measured to see how quickly it can work.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, in this section we demonstrated the use of Haar cascades for face
    and eye detection. It is very simple to implement once the trained file is available,
    and it is a very powerful algorithm. It is widely used in embedded or mobile environments
    where memory and processing power are limited.
  prefs: []
  type: TYPE_NORMAL
- en: Object tracking using background subtraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Background subtraction is the process of separating out foreground objects
    from the background in a sequence of video frames. It is widely used in object
    detection and tracking applications to remove the background part. Background
    subtraction is performed in four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Image preprocessing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modeling of background
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detection of foreground
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data validation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image preprocessing is always performed to remove any kind of noise present
    in the image. The second step is to model the background so that it can be separated
    from the foreground. In some applications, the first frame of the video is taken
    as the background and it is not updated. The absolute difference between each
    frame and the first frame is taken to separate foreground from background.
  prefs: []
  type: TYPE_NORMAL
- en: In other techniques, the background is modeled by taking an average or median
    of all the frames that have been seen by the algorithm, and that background is
    separated from the foreground. It will be more robust for illumination changes
    and will produce a more dynamic background than the first method. Even more statistically
    intensive models, like Gaussian models and support vector models that use a history
    of frames, can be used for modeling a background.
  prefs: []
  type: TYPE_NORMAL
- en: The third step is to segregate the foreground from the modeled background by
    taking the absolute difference between the current frame and the background. This
    absolute difference is compared with the set threshold, and if it is greater than
    the threshold then the objects are considered moving, and if it is less than the
    threshold then the objects are stationary.
  prefs: []
  type: TYPE_NORMAL
- en: Mixture of Gaussian (MoG) method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MoG is a widely used background subtraction method used for separating foregrounds
    from backgrounds, based on Gaussian mixtures. The background is continuously updated
    from the sequence of frames. A mixture of K Gaussian distribution is used to categorize
    pixels as being foreground or background. The time sequence of the frame is also
    weighted to improve background modeling. The intensities that are continuously
    changing are categorized as foreground, and the intensities that are static are
    categorized as background.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV and CUDA provide an easy API to implement MoG for background subtraction.
    The code for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `createBackgroundSubtractorMOG` class is used to create an object for MoG
    implementation. It can be provided with some optional arguments while creating
    an object. The parameters include `history`, `nmixtures`, `backgroundRatio`, and
    `noiseSigma`. The `history` parameter signifies the number of previous frames
    used for modeling the background. Its default value is 200\. The `nmixture` parameter
    specifies the number of Gaussian mixtures used to segregate pixels. Its default
    value is 5\. You can play around with these values according to an application.
  prefs: []
  type: TYPE_NORMAL
- en: The `apply` method of the created object is used to create a foreground mask
    from the first frame. It requires an input image and an image array to store the
    foreground mask and learning rate as the input. This foreground mask and the background
    image are continuously updated after every frame inside the `while` loop. The
    `getBackgroundImage` function is used to fetch the current background model.
  prefs: []
  type: TYPE_NORMAL
- en: The foreground mask is used to create a foreground image that indicates which
    objects are currently moving. It is basically logical and operates between the
    original frame and foreground mask. The foreground mask, foreground image, and
    the modeled background are downloaded to host memory after every frame for displaying
    on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MoG model is applied to a video from the PETS 2009 dataset, which is widely
    used for pedestrian detection. It has a static background and persons are moving
    around in the video. The output of two different frames from the video is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/506e1519-220b-44a8-8b2d-6d17caefa52d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As can be seen, MoG models the background very efficiently. Only the persons
    who are moving are present in the foreground mask and foreground image. This foreground
    image can be used for further processing of the detected objects. If a human stops
    walking, then he will start being part of the background, as can be seen from
    the result of the second frame. So this algorithm can only be used to detect moving
    objects. It will not consider static objects. The performance of MoG in terms
    of frame rate is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fdcd137-6d13-4bf1-9f43-e96a16a60237.png)'
  prefs: []
  type: TYPE_IMG
- en: The frame rate is updated after every frame. It can be seen that it is around
    330 frames per second, which is very high and easily used in real-time applications.
    OpenCV and CUDA also provide the second version of MoG, which can be invoked by
    the `createBackgroundSubtractorMOG2` class.
  prefs: []
  type: TYPE_NORMAL
- en: GMG for background subtraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name of the algorithm GMG is derived from the initials of the inventors
    who proposed the algorithm. The algorithm is a combination of background estimation
    and Bayesian segmentation per pixel. It uses Bayesian inference to separate the
    background from the foreground. It also uses the history of frames for modeling
    the background. It is again weighted based on the time sequence of the frame.
    The new observation is weighted more than the old observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV and CUDA provide a similar API to MoG for implementation of the GMG
    algorithm. The code for implementing the GMG algorithm for background subtraction
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `createBackgroundSubtractorGMG` class is used to create an object for GMG
    implementation. It can be provided with two arguments while creating an object.
    The first argument is the number of previous frames used for modeling the background.
    It is taken as `40` in the code above. The second argument is the decision threshold,
    which is used to categorize pixels as foreground. Its default value is 0.8\.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `apply` method of the created object is used on the first frame to create
    a foreground mask. The foreground mask and foreground image are continuously updated
    inside the `while` loop by using the history of frames. The foreground mask is
    used to create a foreground image in a similar way as shown for MoG. The output
    of the GMG algorithm on the same video and two frames is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fc9de7e-58f0-450f-9613-13d086fb19fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of GMG is noisy compared to MoG. The morphological opening and closing
    operation can be applied to the result of GMG to remove shadowing noise present
    in the result. The performance of the GMG algorithm in terms of FPS is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e29cf7f-4dac-4595-b16b-f2339c01b284.png)'
  prefs: []
  type: TYPE_IMG
- en: As it is more computationally intensive than MoG, the FPS rate is less, but
    still the FPS performance is 120, which is more than 30 FPS for real-time performance.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, in this section we have seen two methods for background modeling
    and background subtraction. MoG is faster and less noisy compared to the GMG algorithm.
    The GMG algorithm requires morphological operations to remove noise present in
    the result.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter described the role of OpenCV and CUDA in real-time object detection
    and tracking applications. It started with the introduction of object detection
    and tracking, along with challenges encountered in that process and the applications
    of it. Different features like color, shape, histograms, and other distinct key-points,
    like corners, can be used to detect and track objects in an image. Color-based
    object detection is easier to implement, but it requires that the object should
    have a distinct color from the background. For shape-based object detection, the
    Canny edge detection technique has been described to detect edges, and Hough transform
    has been described for straight line and circle detection. It has many applications,
    such as land detection, ball tracking, and so on. The color and shape are global
    features, which are easier to compute and require less memory. They are more susceptible
    to noise. Other algorithms like FAST, ORB, and SURF have been described in detail,
    which can be used to detect key-points from the image, and these key-points can
    be used to describe an image accurately, which in turn can be used to detect an
    object in the image. ORB is open source and gives a comparable result to SURF
    at no cost. SURF is patented but it is faster, scale-invariant, and rotation-invariant.
    The Haar cascade has been described, which is a simple algorithm used to detect
    objects like faces, eyes, and the human body from an image. It can be used in
    embedded systems for real-time applications. The last part of the chapter described
    background subtraction algorithms in detail, such as MoG and GMG, which can separate
    foregrounds from backgrounds. The output of these algorithms can be used for object
    detection and tracking. The next chapter will describe how these applications
    can be deployed on embedded development boards.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Write an OpenCV code for detecting objects of yellow color from the video.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In which situations will the object detection using color fail?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the Canny edge detection algorithm better than the other edge detection
    algorithm seen in the last chapter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What can be done to reduce the noise sensitivity of Hough transform?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the importance of the threshold in the FAST key point detector?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the importance of the Hessian threshold in the SURF detector?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the scale factor in a Haar cascade is changed from 1.01 to 1.05 then what
    effect will it have on output?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the MoG and GMG background subtraction methods. What can be done to
    remove noise from GMG outputs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
