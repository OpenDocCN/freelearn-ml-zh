<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Unsupervised Learning Using Apache Spark</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will train and evaluate unsupervised machine learning models applied to a variety of real-world use cases, again using Python, Apache Spark, and its machine learning library, <kbd>MLlib</kbd>. Specifically, we will develop and interpret the following types of unsupervised machine learning models and techniques:</p>
<ul>
<li>Hierarchical clustering</li>
<li>K-means clustering</li>
<li>Principal component analysis</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clustering</h1>
                </header>
            
            <article>
                
<p>As described in <a href="a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml" target="_blank">Chapter 3</a>, <em>Artificial Intelligence and Machine Learning</em>, in unsupervised learning, the goal is to uncover hidden relationships, trends, and patterns given only the input data, <em>x<sub>i</sub></em>, with no output, <em>y<sub>i</sub></em>. In other words, our input dataset will be of the following form:</p>
<p style="padding-left: 150px"><img src="Images/3f2330fc-359c-4772-824c-3ba8d26bb77d.png" style="width:16.83em;height:1.67em;" width="2320" height="230"/></p>
<p>Clustering is a well-known example of a class of unsupervised learning algorithms where the goal is to segment data points into groups, where all of the data points in a specific group share similar features or attributes in common. By the nature of clustering, however, it is recommended that clustering models are trained on large datasets to avoid over fitting. The two most commonly used clustering algorithms are <strong>hierarchical clustering</strong> and <strong>k-means clustering</strong>, which are differentiated from each other by the processes by which they construct clusters. We shall study both of these algorithms in this chapter.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Euclidean distance</h1>
                </header>
            
            <article>
                
<p>By definition, in order to cluster data points into groups, we require an understanding of the <em>distance</em> between two given data points. A common measure of distance is the <strong>Euclidean distance</strong>, which is simply the straight-line distance between two given points in <em>k</em>-dimensional space, where <em>k</em> is the number of independent variables or features. Formally, the Euclidean distance between two points, <em>p</em> and <em>q</em>, given <em>k</em> independent variables or dimensions is defined as follows:</p>
<p style="padding-left: 60px"><img src="Images/1555c487-d45f-4a1d-ac70-8be68fef47a6.png" style="width:37.75em;height:4.25em;" width="5590" height="630"/></p>
<p>Other common measures of distance include the <strong>Manhattan distance</strong>, which is the sum of the absolute values instead of squares ( <img class="fm-editor-equation" src="Images/c9a63cea-d2fb-4e72-b7fc-9c7471329d7a.png" style="width:16.25em;height:1.33em;" width="2800" height="220"/>) and the <strong>maximum coordinate distance</strong>, where measurements are only considered for those data points that deviate the most. For the remainder of this chapter, we will measure the Euclidean distance. Now that we have an understanding of distance, we can define the following measures between two clusters, as illustrated in <em>Figure 5.1</em>:</p>
<ul>
<li>The <em>minimum distance</em> between clusters is the distance between the two points that are the closest to each other.</li>
<li>The <em>maximum distance</em> between clusters is the distance between the two points that are furthest away from each other.</li>
<li>The <em>centroid distance</em> between clusters <span>is the distance between the centroids of each cluster, where the centroid is defined as the average of all data points in a given cluster:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-570 image-border" src="Images/f379f7d2-1234-405f-8932-d9bcbccef7bd.png" style="width:33.00em;height:28.00em;" width="965" height="818"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.1: Cluster distance measures</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hierarchical clustering</h1>
                </header>
            
            <article>
                
<p>In hierarchical clustering, each data point starts off in its own self-defined cluster—for example, if you have 10 data points in your dataset, then there will initially be 10 clusters. The two <em>nearest</em> clusters, as defined by the Euclidean centroid distance, for example, are then combined. This process is then repeated for all distinct clusters until eventually all data points belong in the same cluster.</p>
<p>This process can be visualized using a <strong>dendrogram</strong>, as illustrated in <em>Figure 5.2</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-571 image-border" src="Images/66d40dfe-b419-4837-8425-c7650889885f.png" style="width:35.00em;height:29.83em;" width="1014" height="865"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.2: Hierarchical clustering dendrogram</div>
<p>A dendrogram helps us to decide when to stop the hierarchical clustering process. It is generated by plotting the original data points on the <em>x </em>axis and the distance between clusters on the <em>y </em>axis. As new parent clusters are created, by combining the nearest clusters together, a horizontal line is plotted between those child clusters. Eventually, the dendrogram ends when all data points belong in the same cluster. The aim of the dendrogram is to tell us when to stop the hierarchical clustering process. We can deduce this by drawing a dashed horizontal line across the dendrogram, placed at a position that maximizes the vertical distance between this dashed horizontal line and the next horizontal line (up or down). The final number of clusters at which to stop the hierarchical clustering process is then the number of vertical lines the dashed horizontal line intersects. In <em>Figure 5.2</em>, we would end up with two clusters containing the data points {5, 2, 7} and {8, 4, 10, 6, 1, 3, 9} respectively. However, make sure that the final number of clusters makes sense in the context of your use case.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">K-means clustering</h1>
                </header>
            
            <article>
                
<p>In k-means clustering, a different process is followed in order to segment data points into clusters. First, the final number of clusters, <em>k</em>, must be defined upfront based on the context of your use case. Once defined, each data point is randomly assigned to one of these <em>k</em> clusters, after which the following process is employed:</p>
<ul>
<li>The centroid of each cluster is computed</li>
<li>Data points are then reassigned to those clusters that have the closest centroid to them</li>
<li>The centroids of all clusters are then recomputed</li>
<li>Data points are then reassigned once more</li>
</ul>
<p>This process is repeated until no data points can be reassigned—that is, until there are no further improvements to be had and all data points belong to a cluster that has the closest centroid to them. Therefore, since the centroid of a cluster is defined as the mean average of all data points in a given cluster, k-means clustering effectively partitions the data points into <em>k</em> clusters with each data point assigned to a cluster with a mean average that is closest to it.</p>
<p>Note that in both clustering processes (hierarchical and k-means), a measure of distance needs to be computed. However, distance scales differently based on the type and units of the independent variables involved—for example, height and weight. Therefore, it is important to normalize your data first (sometimes called feature scaling) before training a clustering model so that it works properly. To learn more about normalization, please visit <a href="https://en.wikipedia.org/wiki/Feature_scaling">https://en.wikipedia.org/wiki/Feature_scaling</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Case study – detecting brain tumors</h1>
                </header>
            
            <article>
                
<p>Let's apply k-means clustering to a very important real-world use case: detecting brain tumors from <strong>magnetic resonance imaging</strong> (<strong>MRI</strong>) scans. MRI scans are used across the world to generate detailed images of the human body, and can be used for a wide range of medical applications, from detecting cancerous cells to measuring blood flow. In this case study, we will use grayscale MRI scans of a healthy human brain as the input for a k-means clustering model. We will then apply the trained k-means clustering model to an MRI scan of another human brain to see if we can detect suspicious growths and tumors.</p>
<p>Note that the images we will use in this case study are relatively simple, in that any suspicious growths that are present will be visible to the naked eye. The fundamental purpose of this case study is to show how Python may be used to manipulate images, and how <kbd>MLlib</kbd> may be used to natively train k-means clustering models via its k-means estimator.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feature vectors from images</h1>
                </header>
            
            <article>
                
<p>The first challenge for us is to convert images into numerical feature vectors in order to train our k-means clustering model. In our case, we will be using grayscale MRI scans. A grayscale image in general can be thought of as a matrix of pixel-intensity values between 0 (black) and 1 (white), as illustrated in <em>Figure 5.3</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-572 image-border" src="Images/f66f2887-1962-4aad-8f70-9dd144bde421.png" style="width:36.92em;height:18.33em;" width="1125" height="558"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.3: Grayscale image mapped to a matrix of pixel-intensity values</div>
<p>The dimensions of the resulting matrix is equal to the height (<em>m</em>) and width (<em>n</em>) of the original image in pixels. The input into our k-means clustering model will therefore be (<em>m</em> x <em>n</em>) observations across one independent variable—the pixel-intensity value. This can subsequently be represented as a single vector containing (<em>m</em> x <em>n</em>) numerical elements—that is, (<span class="packt_screen">0.0</span>, <span class="packt_screen">0.0</span>, <span class="packt_screen">0.0</span>, <span class="packt_screen">0.2</span>, <span class="packt_screen">0.3</span>, <span class="packt_screen">0.4</span>, <span class="packt_screen">0.3</span>, <span class="packt_screen">0.4</span>, <span class="packt_screen">0.5</span> …).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image segmentation</h1>
                </header>
            
            <article>
                
<p>Now that we have derived feature vectors from our grayscale MRI image, our k-means clustering model will assign each pixel-intensity value to one of the <em>k</em> clusters when we train it on our MRI scan of a healthy human brain. In the context of the real world, these <em>k</em> clusters represent different substances in the brain, such as grey matter, white matter, fatty tissue, and cerebral fluids, which our model will partition based on color, a process called image segmentation. Once we have trained our k-means clustering model on a healthy human brain and identified <em>k</em> distinct clusters, we can then apply those defined clusters to MRI brain scans of other patients in an attempt to identify the presence and volume of suspicious growths.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">K-means cost function</h1>
                </header>
            
            <article>
                
<p>One of the challenges when using the k-means clustering algorithm is how to choose a suitable value for <em>k</em> upfront, especially if it is not obvious from the wider context of the use case in question. One method to help us is to plot a range of possible values of <em>k</em> on the <em>x </em>axis against the output of the k-means cost function on the <em>y </em>axis. The k-means cost function computes the total sum of the squared distance of every point to its corresponding cluster centroid for that value of <em>k</em>. The goal is to choose a suitable value of <em>k</em> that minimizes the cost function, but that is not so large that it increases the computational complexity of generating the clusters with only a small return in the reduction in cost. We will demonstrate how to generate this plot, and hence choose a suitable value of <em>k</em>, when we develop our Spark application for image segmentation in the next subsection.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">K-means clustering in Apache Spark</h1>
                </header>
            
            <article>
                
<div class="packt_infobox">The MRI brain scans that we will use for our k-means clustering model have been downloaded from <strong>The Cancer Imaging Archive</strong> (<strong>TCIA</strong>), a service that anonymizes and hosts a large archive of medical images of cancer for public download, and that may be found at <a href="http://www.cancerimagingarchive.net/">http://www.cancerimagingarchive.net/</a>.</div>
<p>The MRI scan of our healthy human brain may be found in the GitHub repository accompanying this book, and is called <kbd>mri-images-data</kbd>/<kbd>mri-healthy-brain.png</kbd>. The MRI scan of the test human brain is called <kbd>mri-images-data</kbd>/<kbd>mri-test-brain.png</kbd>. We will use both in the following Spark application when training our k-means clustering model and applying it to image segmentation. Let's begin:</p>
<div class="packt_infobox">The following subsections describe each of the pertinent cells in the corresponding Jupyter notebook for this use case, called <kbd>chp05-01-kmeans-clustering.ipynb</kbd>. It can be found in the GitHub repository accompanying this book.</div>
<ol>
<li>Let's open the grayscale MRI scan of the healthy human brain and take a look at it! We can achieve this using the <kbd>scikit-learn</kbd> machine learning library for Python as follows:</li>
</ol>
<pre style="padding-left: 60px">mri_healthy_brain_image = io.imread(<br/>   'chapter05/data/mri-images-data/mri-healthy-brain.png')<br/>mri_healthy_brain_image_plot = plt.imshow(<br/>   mri_healthy_brain_image, cmap='gray')<br/></pre>
<p style="padding-left: 60px"><span>The rendered image is illustrated in <em>Figure 5.4</em>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-573 image-border" src="Images/641d2583-8db8-43a3-a049-0d373649e56e.png" style="width:36.42em;height:19.92em;" width="1037" height="566"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.4: Original MRI scan rendered using scikit-learn and matplotlib</div>
<ol start="2">
<li><span>We now need to turn this image into a matrix of decimal point pixel-intensity values between 0 and 1. Conveniently, this function is provided out of the box by</span> <kbd>scikit-learn</kbd> <span>using the</span> <kbd>img_as_float</kbd> <span>method, as shown in the following code. The dimensions of the resulting matrix are 256 x 256, implying an original image of 256 x 256 pixels:</span></li>
</ol>
<pre style="padding-left: 60px">mri_healthy_brain_matrix = img_as_float(mri_healthy_brain_image)</pre>
<ol start="3">
<li>Next, we need to flatten this matrix into a single vector of 256 x 256 elements, where each element represents a pixel-intensity value. This can be thought of as another matrix of dimensions 1 x (256 x 256) = 1 x 65536. We can achieve this using the <kbd>numpy</kbd> Python library. First, we convert our original 256 x 256 matrix into a 2-dimensional <kbd>numpy</kbd> array. We then use <kbd>numpy</kbd>'s <kbd>ravel()</kbd> method to flatten this 2-dimensional array into a 1-dimensional array. Finally, we represent this 1-dimensional array as a specialized array, or matrix, of dimensions 1 x 65536 using the <kbd>np.matrix</kbd> command, as follows:</li>
</ol>
<pre style="padding-left: 60px">mri_healthy_brain_2d_array = np.array(mri_healthy_brain_matrix)<br/>   .astype(float)<br/>mri_healthy_brain_1d_array = mri_healthy_brain_2d_array.ravel()<br/>mri_healthy_brain_vector = np.matrix(mri_healthy_brain_1d_array)</pre>
<ol start="4">
<li><span>Now that we have our single vector, represented as a matrix of 1 x 65536 in dimension, we need to convert it into a Spark dataframe. To achieve this, we firstly transpose the matrix using numpy's</span> <kbd>reshape()</kbd> <span>method so that it is 65536 x 1. We then use the</span> <kbd>createDataFrame()</kbd> <span>method, exposed by Spark's SQLContext, to create a Spark dataframe containing 65536 observations/rows and 1 column, representing 65536 pixel-intensity values, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px">mri_healthy_brain_vector_transposed = mri_healthy_brain_vector<br/>   .reshape(mri_healthy_brain_vector.shape[1], <br/>   mri_healthy_brain_vector.shape[0])<br/>mri_healthy_brain_df = sqlContext.createDataFrame(<br/>   pd.DataFrame(mri_healthy_brain_vector_transposed,<br/>   columns = ['pixel_intensity']))</pre>
<ol start="5">
<li><span>We are now ready to generate <kbd>MLlib</kbd> feature vectors using </span><kbd>VectorAssembler</kbd>, a method that we <span>have seen before. The <kbd>feature_columns</kbd> for</span> <kbd>VectorAssembler</kbd> <span>will simply be the sole pixel-intensity column from our Spark dataframe. The output of applying</span> <kbd>VectorAssembler</kbd> <span>to our Spark dataframe via the</span> <kbd>transform()</kbd> <span>method will be a new Spark dataframe called</span> <kbd>mri_healthy_brain_features_df</kbd><span>, containing our 65536 <kbd>MLlib</kbd> feature vectors, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">feature_columns = ['pixel_intensity']<br/>vector_assembler = VectorAssembler(inputCols = feature_columns,<br/>   outputCol = 'features')<br/>mri_healthy_brain_features_df = vector_assembler<br/>   .transform(mri_healthy_brain_df).select('features')</pre>
<ol start="6">
<li><span>We can now compute and plot the output of the k-means cost function for a range of</span> <em>k</em> in order to determine the best value of <em>k</em> for this use case<span>. We achieve this by using <kbd>MLlib</kbd>'s</span> <kbd>KMeans()</kbd> <span>estimator in the Spark dataframe containing our feature vectors, iterating over values of</span> <kbd>k</kbd> <span>in the <kbd>range(2, 20)</kbd>. We can then plot this using the</span> <kbd>matplotlib</kbd> <span>Python library, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px">cost = np.zeros(20)<br/>for k in range(2, 20):<br/>    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol("features")<br/>    model = kmeans.fit(mri_healthy_brain_features_df<br/>       .sample(False, 0.1, seed=12345))<br/>    cost[k] = model.computeCost(mri_healthy_brain_features_df)<br/><br/>fig, ax = plt.subplots(1, 1, figsize =(8, 6))<br/>ax.plot(range(2, 20),cost[2:20])<br/>ax.set_title('Optimal Number of Clusters K based on the<br/>   K-Means Cost Function for a range of K')<br/>ax.set_xlabel('Number of Clusters K')<br/>ax.set_ylabel('K-Means Cost')</pre>
<p style="padding-left: 60px"><span>Based on the resulting plot, as illustrated in <em>Figure 5.5</em>, a value of</span><span> </span><em>k</em><span> </span><span>of either 5 or 6 would seem to be ideal. At these values, the k-means cost is minimized with little return gained thereafter, as shown in the following graph:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-574 image-border" src="Images/4ac60c7e-4c77-4411-b9b6-b4910e063ea3.png" style="width:29.17em;height:21.17em;" width="1073" height="780"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.5: K-means cost function</div>
<ol start="7">
<li><span>We are now ready to train our k-means clustering model! Again, we will use</span> <kbd>MLlib</kbd><span>'s</span> <kbd>KMeans()</kbd> <span>estimator, but this time using a defined value for</span> <em>k</em> <span>(5, in our case, as we decided in step 6). We will then apply it, via the</span> <kbd>fit()</kbd> <span>method, to the Spark dataframe containing our feature vectors and study the centroid values for each of our 5 resulting clusters, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">k = 5<br/>kmeans = KMeans().setK(k).setSeed(12345).setFeaturesCol("features")<br/>kmeans_model = kmeans.fit(mri_healthy_brain_features_df)<br/>kmeans_centers = kmeans_model.clusterCenters()<br/>print("Healthy MRI Scan - K-Means Cluster Centers: \n")<br/>for center in kmeans_centers:<br/>    print(center)</pre>
<ol start="8">
<li><span>Next, we will apply our trained k-means model to the Spark dataframe containing our feature vectors so that we may assign each of the 65536 pixel-intensity values to one of the five clusters. The result will be a new Spark dataframe containing our feature vectors mapped to a</span> prediction<span>, where in this case the prediction is simply a value between 0 and 4, representing one of the five clusters. Then, we convert this new dataframe into a 256 x 256 matrix so that we can visualize the segmented image, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">mri_healthy_brain_clusters_df = kmeans_model<br/>   .transform(mri_healthy_brain_features_df)<br/>   .select('features', 'prediction')<br/>mri_healthy_brain_clusters_matrix = mri_healthy_brain_clusters_df<br/>   .select("prediction").toPandas().values<br/>   .reshape(mri_healthy_brain_matrix.shape[0],<br/>      mri_healthy_brain_matrix.shape[1])<br/>plt.imshow(mri_healthy_brain_clusters_matrix)</pre>
<p style="padding-left: 60px">The resulting segmented image, rendered using <kbd>matplotlib</kbd>, is illustrated in <em>Figure 5.6</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-575 image-border" src="Images/6cf03ae5-c26a-4983-84cf-70d8e7ce9315.png" style="width:26.08em;height:19.75em;" width="747" height="565"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.6: Segmented MRI scan</div>
<ol start="9">
<li><span>Now that we have our five defined clusters, we can apply our trained k-means model to a <em>new</em> image in order to segment it, also based on the same five clusters. First, we load the new grayscale MRI brain scan belonging to the test patient using the</span> <kbd>scikit-learn</kbd> <span>library, as we did before using the following code:</span></li>
</ol>
<pre style="padding-left: 60px">mri_test_brain_image = io.imread(<br/>   'chapter05/data/mri-images-data/mri-test-brain.png')</pre>
<ol start="10">
<li>Once we have loaded the new MRI brain scan image, we need to follow the same process to convert it into a Spark dataframe containing feature vectors representing the pixel-intensity values of the new test image. We then apply the trained k-means model, via the <kbd>transform()</kbd> method, to this test Spark dataframe in order to assign its pixels to one of the five clusters. Finally, we convert the Spark dataframe containing the test image predictions in to a matrix so that we can visualize the segmented test image, as follows:</li>
</ol>
<pre style="padding-left: 60px">mri_test_brain_df = sqlContext<br/>   .createDataFrame(pd.DataFrame(mri_test_brain_vector_transposed,<br/>   columns = ['pixel_intensity']))<br/>mri_test_brain_features_df = vector_assembler<br/>   .transform(mri_test_brain_df)<br/>   .select('features')<br/>mri_test_brain_clusters_df = kmeans_model<br/>   .transform(mri_test_brain_features_df)<br/>   .select('features', 'prediction')<br/>mri_test_brain_clusters_matrix = mri_test_brain_clusters_df<br/>   .select("prediction").toPandas().values.reshape(<br/>   mri_test_brain_matrix.shape[0], mri_test_brain_matrix.shape[1])<br/>plt.imshow(mri_test_brain_clusters_matrix)</pre>
<p style="padding-left: 60px"><span>The resulting segmented image belonging to the test patient, again rendered using <kbd>matplotlib</kbd>, is illustrated in <em>Figure 5.7</em>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-576 image-border" src="Images/8354900b-5a2b-47cd-a4b9-9518b991bd69.png" style="width:41.08em;height:16.33em;" width="1431" height="569"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.7: Segmented MRI scan belonging to the test patient</div>
<p><span>If we compare the two segmented images side by side (as illustrated in <em>Figure 5.8</em>), we will see that, as a result of our k-means clustering model, five different colors have been rendered representing the five different clusters. In turn, these five different clusters represent different substances in the human brain, partitioned by color. We will also see that, in the test MRI brain scan, one of the colors takes up a substantially larger area compared to the healthy MRI brain scan, pointing to a suspicious growth that may potentially be a tumor requiring further analysis, as shown in the following image:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-577 image-border" src="Images/f612788f-acd9-49f0-969c-dcc79a3844d8.png" style="width:31.83em;height:19.83em;" width="1044" height="650"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.8: Comparison of segmented MRI scans</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Principal component analysis</h1>
                </header>
            
            <article>
                
<p>There are numerous real-world use cases where the number of features available that may potentially be used to train a model is very large. A common example is economic data, and using its constituent stock price data, employment data, banking data, industrial data, and housing data together to predict the <strong>gross domestic product</strong> (<strong>GDP</strong>). Such types of data are said to have high dimensionality. Though they offer numerous features that can be used to model a given use case, high-dimensional datasets increase the computational complexity of machine learning algorithms, and more importantly may also result in over fitting. Over fitting is one of the results of the <strong>curse of dimensionality</strong>, which formally describes the problem of analyzing data in high-dimensional spaces (which means that the data may contain many attributes, typically hundreds or even thousands of dimensions/features), but where that analysis no longer holds true in a lower-dimensional space.</p>
<p>Informally, it describes the value of additional dimensions at the cost of model performance. <strong>Principal component analysis</strong> (<strong>PCA</strong>)<strong> </strong>is an <em>unsupervised</em> technique used to preprocess and reduce the dimensionality of high-dimensional datasets while preserving the original structure and relationships inherent to the original dataset so that machine learning models can still learn from them and be used to make accurate predictions.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Case study – movie recommendation system</h1>
                </header>
            
            <article>
                
<p>To better understand PCA, let's study a movie recommendation use case. Our aim is to build a system that can make personalized movie recommendations to users based on historic user-community movie ratings (note that user viewing history data could also be used for such a system, but this is beyond the scope of this example).</p>
<div class="packt_infobox">The historic user-community movie ratings data that we will use for our case study has been downloaded from GroupLens, a research laboratory based at the University of Minnesota that collects movie ratings and makes them available for public download at <a href="https://grouplens.org/datasets/movielens/">https://grouplens.org/datasets/movielens/</a>. For the purposes of this case study, we have transformed the individual <em>movies</em> and <em>ratings</em> datasets into a single pivot table where the 300 rows represent 300 different users, and the 3,000 columns represent 3,000 different movies. This transformed, pipe-delimited dataset can be found in the GitHub repository accompanying this book, and is called <kbd>movie-ratings-data/user-movie-ratings.csv</kbd>.</div>
<p>A sample of the historic user-community movie ratings dataset that we will study looks as follows:</p>
<table style="border-color: #000000;border-collapse: collapse" border="1">
<tbody>
<tr>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p> </p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>Movie #1</strong></p>
<p><strong>Toy Story</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>Movie #2</strong></p>
<p><strong>Monsters Inc.</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>Movie #3</strong></p>
<p><strong>Saw</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>Movie #4</strong></p>
<p><strong>Ring</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>Movie #5</strong></p>
<p><strong>Hitch</strong></p>
</td>
</tr>
<tr>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>User #1</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>NULL</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
</tr>
<tr>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>User #2</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>NULL</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>NULL</p>
</td>
</tr>
<tr>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>User #3</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>NULL</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
</tr>
<tr>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>User #4</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>NULL</p>
</td>
</tr>
<tr>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>User #5</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>NULL</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>NULL</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
</tr>
</tbody>
</table>
<p>In this case, each movie is a different feature (or dimension), and each different user is a different instance (or observation). This sample table, therefore, represents a dataset containing 5 features. However, our actual dataset contains 3,000 different movies, and therefore 3,000 features/dimensions. Furthermore, in a real-life representation, not all users would have rated all the movies, and so there will be a significant number of missing values. Such a dataset, and the matrix used to represent it, is described as <em>sparse</em>. These issues would pose a problem for machine learning algorithms, both in terms of computational complexity and the likelihood of over fitting.</p>
<p>To solve this problem, take a closer look at the previous sample table. It seems that users that rated Movie #1 highly (Toy Story) generally also rated Movie #2 highly (Monsters Inc.) as well. We could say, for example, that User #1 is <em>representative</em> of all fans of computer-animated children's films, and so we could recommend to User #2 the other movies that User #1 has historically rated highly (this type of recommendation system where we use data from other users is called <strong>collaborative filtering</strong>). At a high level, this is what PCA does—it identifies <em>typical representations</em>, called <strong>principal components</strong>, within a high-dimensional dataset so that the dimensions of the original dataset can be reduced while preserving its underlying structure and still be representative in <em>lower</em> dimensions! These reduced datasets can then be fed into machine learning models to make predictions as normal, without the fear of any adverse effects from reducing the raw size of the original dataset. Our formal definition of PCA can therefore now be extended so that we can define PCA as the identification of a linear subspace of lower dimensionality where the largest variance in the original dataset is maintained.</p>
<p>Returning to our historic user-community movie ratings dataset, instead of eliminating Movie #2 entirely, we could seek to create a new feature that combines Movie #1 and Movie #2 in some manner. Extending this concept, we can create new features where each new feature is based on all the old features, and thereafter order these new features by how well they help us in predicting user movie ratings. Once ordered, we can drop the least important ones, thereby resulting in a reduction in dimensionality. So how does PCA achieve this? It does so by performing the following steps:</p>
<ol>
<li>First, we standardize the original high-dimensional dataset.</li>
<li>Next, we take the standardized data and compute a covariance matrix that provides a means to measure how all our features relate to each other.</li>
<li>After computing the covariance matrix, we then find its <em>eigenvectors</em> and corresponding <em>eigenvalues</em>. Eigenvectors represent the principal components and provide a means to understand the direction of the data. Corresponding eigenvalues represent how much variance there is in the data in that direction.</li>
</ol>
<ol start="4">
<li>The eigenvectors are then sorted in descending order based on their corresponding eigenvalues, after which the top <em>k</em> eigenvectors are selected representing the most important representations found in the data.</li>
<li>A new matrix is then constructed with these <em>k</em> eigenvectors, thereby reducing the original <em>n</em>-dimensional dataset into reduced <em>k</em> dimensions.</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Covariance matrix</h1>
                </header>
            
            <article>
                
<p class="mce-root">In mathematics, <strong>variance</strong> refers to a measure of how spread out a dataset is, and is calculated by the sum of the squared distances of each data point, <em>x<sub>i</sub></em>, from the mean <em>x-bar</em>, divided by the total number of data points, <em>N</em>. This is represented by the following formula:</p>
<p style="padding-left: 180px"><img src="Images/787eabe6-db53-4369-9e63-43761b34b1d8.png" style="width:11.00em;height:2.75em;" width="1770" height="440"/></p>
<p><strong>Covariance</strong> refers to a measure of how strong the correlation between two or more random variables is (in our case, our independent variables), and is calculated for variables <em>x</em> and <em>y</em> over <em>i</em> dimensions, as follows:</p>
<p style="padding-left: 150px"><img src="Images/21109685-3428-485a-8bfc-2c75ae5a0a1d.png" style="width:16.67em;height:2.83em;" width="2480" height="420"/></p>
<p>If the covariance is positive, this implies that the independent variables are positively correlated. If the covariance is negative, this implies that the independent variables are negatively correlated. Finally, a covariance of zero implies that there is no correlation between the independent variables. You may note that we described correlation in <a href="ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml" target="_blank">Chapter 4</a>, <em>Supervised Learning Using Apache Spark</em>, when discussing multivariate linear regression. At that time, we computed the one-way covariance mapping between the dependent variable to all its independent variables. Now we are computing the covariance between all variables.</p>
<p>A <strong>covariance matrix</strong> is a symmetric square matrix where the general element (<em>i</em>, <em>j</em>) is the covariance, <em>cov(i, j)</em>, between independent variables <em>i</em> and <em>j</em> (which is the same as the symmetric covariance between <em>j</em> and <em>i</em>). Note that the diagonal in a covariance matrix actually represents just the <em>variance</em> between those elements, by definition.</p>
<p>The covariance matrix is shown in the following table:</p>
<table style="width: 276px;border-color: #000000;border-collapse: collapse" border="1">
<tbody>
<tr>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p> </p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>x</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>y</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>z</strong></p>
</td>
</tr>
<tr>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>x</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>var(x)</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>cov(x, y)</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>cov(x, z)</p>
</td>
</tr>
<tr>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>y</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>cov(y, x)</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>var(y)</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>cov(y, z)</p>
</td>
</tr>
<tr>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p><strong>z</strong></p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>cov(z, x)</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>cov(z, y)</p>
</td>
<td style="padding: 5px" class="CDPAlignCenter CDPAlign">
<p>var(z)</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Identity matrix</h1>
                </header>
            
            <article>
                
<p class="mce-root">An identity matrix is a square matrix in which all the elements along the main diagonal are 1 and the remaining elements are 0. Identity matrices are important for when we need to find all of the eigenvectors for a matrix. For example, a 3 x 3 identity matrix looks as follows:</p>
<p style="padding-left: 240px"><img src="Images/15e61c67-15f7-474d-b0f3-f1b47599e4bf.png" style="width:5.83em;height:4.08em;" width="1040" height="730"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Eigenvectors and eigenvalues</h1>
                </header>
            
            <article>
                
<p>In linear algebra, eigenvectors are a special set of vectors whose <em>direction</em> remains unchanged when a linear transformation is applied to it, and only changes by a <em>scalar</em> factor. In the context of dimensionality reduction, eigenvectors represent the principal components and provide a means to understand the direction of the data.</p>
<p>Consider a matrix, <em>A</em>, of dimensions (<em>m</em> x <em>n</em>). We can multiply <em>A</em> by a vector, <em>x</em> (of dimensions <em>n</em> x 1 by definition), which results in a new vector, <em>b</em> (of dimensions <em>m</em> x 1), as follows:</p>
<p><img src="Images/1a360801-1a58-40ec-acce-35f7e62c63c7.png" style="width:43.75em;height:8.17em;" width="6050" height="1130"/></p>
<p>In other words, <img class="fm-editor-equation" src="Images/ec5d7f90-b023-49b0-bf87-81f10dce29d7.png" style="width:3.42em;height:1.00em;" width="570" height="170"/>.</p>
<p>However, in some cases, the resulting vector, <em>b</em>, is actually a scaled version of the original vector, <em>x</em>. We call this scalar factor <em>λ</em>, in which case the formula above can be rewritten as follows:</p>
<p style="padding-left: 270px"><img src="Images/d1492866-60c5-45cf-bd49-c9612a240212.png" style="width:4.50em;height:1.08em;" width="710" height="170"/></p>
<p>We say that <em>λ</em> is an <em>eigenvalue</em> of matrix <em>A</em>, and <em>x</em> is an <em>eigenvector</em> associated with <em>λ</em>. In the context of dimensionality reduction, eigenvalues represent how much variance there is in the data in that direction.</p>
<p>In order to find all the eigenvectors for a matrix, we need to solve the following equation for each eigenvalue, where <em>I</em> is an identity matrix with the same dimensions as matrix <em>A</em>:</p>
<p style="padding-left: 240px"><img src="Images/c433abbc-2457-45b1-8721-857bfe6dd7da.png" style="width:6.75em;height:1.33em;" width="1160" height="220"/></p>
<p>The process by which to solve this equation is beyond the scope of this book. However, to learn more about eigenvectors and eigenvalues, please visit <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors">https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors</a>.</p>
<p>Once all of the eigenvectors for the covariance matrix are found, these are then sorted in descending order by their corresponding eigenvalues. Since eigenvalues represent the amount of variance in the data for that direction, the first eigenvector in the ordered list represents the principal component that captures the most variance in the original variables from the original dataset, and so on. For example, as illustrated in <em>Figure 5.9</em>, if we were to plot a dataset with two dimensions or features, the first eigenvector (which will be the first principal component in order of importance) would represent the direction of most variation between the two features.</p>
<p>The second eigenvector (the second principal component in order of importance) would represent the direction of second-most variation between the two features:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-578 image-border" src="Images/f81a5743-0cc2-4b65-8cc9-ff48c27cf675.png" style="width:25.17em;height:19.33em;" width="963" height="739"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.9: Principal components across two dimensions</div>
<p>To help choose the number of principal components, <em>k</em>, to select from the top of the ordered list of eigenvectors, we can plot the number of principal components on the <em>x </em>axis against the cumulative explained variance on the <em>y </em>axis, as illustrated in <em>Figure 5.10</em>, where the explained variance is the ratio between the variance of that principal component and the total variance (that is, the sum of all eigenvalues):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-579 image-border" src="Images/8097f834-824c-4dfb-9fc0-663de2082ad7.png" style="width:23.58em;height:18.50em;" width="807" height="632"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.10: Cumulative explained variance</div>
<p>Using <em>Figure 5.10</em> as an example, we would select around the first 300 principal components, as these describe the most variation within the data out of the 3,000 in total. Finally, we construct a new matrix by projecting the original dataset into <em>k</em>-dimensional space represented by the eigenvectors selected, thereby reducing the dimensionality of the original dataset from 3,000 dimensions to 300 dimensions. This preprocessed and reduced dataset can then be used to train machine learning models as normal.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">PCA in Apache Spark</h1>
                </header>
            
            <article>
                
<p>Let's now return to our transformed pipe-delimited user-community movie ratings dataset, <kbd>movie-ratings-data/user-movie-ratings.csv</kbd>, which contains ratings by 300 users covering 3,000 movies. We will develop an application in Apache Spark that seeks to reduce the dimensionality of this dataset while preserving its structure using PCA. To do this, we will go through the following steps:</p>
<div class="mce-root packt_infobox">The following subsections describe each of the pertinent cells in the corresponding Jupyter notebook for this use case, called <kbd>chp05-02-principal-component-analysis.ipynb</kbd>. This can be found in the GitHub repository accompanying this book.</div>
<ol>
<li class="mce-root">First, let's load the transformed, pipe-delimited user-community movie ratings dataset into a Spark dataframe using the following code. The resulting Spark dataframe will have 300 rows (representing the 300 different users) and 3,001 columns (representing the 3,000 different movies plus the user ID column):</li>
</ol>
<pre style="padding-left: 60px">user_movie_ratings_df = sqlContext.read<br/>   .format('com.databricks.spark.csv').options(header = 'true', <br/>   inferschema = 'true', delimiter = '|')<br/>   .load('&lt;Path to CSV File&gt;')<br/>print((user_movie_ratings_df.count(),<br/>   len(user_movie_ratings_df.columns)))</pre>
<ol start="2">
<li>We can now generate <kbd>MLlib</kbd> feature vectors containing 3,000 elements (representing the 3,000 features) using <kbd>MLlib</kbd>'s <kbd>VectorAssembler</kbd>, as we have seen before. We can achieve this using the following code:</li>
</ol>
<pre style="padding-left: 60px">feature_columns = user_movie_ratings_df.columns<br/>feature_columns.remove('userId')<br/>vector_assembler = VectorAssembler(inputCols = feature_columns,<br/>   outputCol = 'features')<br/>user_movie_ratings_features_df = vector_assembler<br/>   .transform(user_movie_ratings_df)<br/>   .select(['userId', 'features']) </pre>
<ol start="3">
<li><span>Before we can reduce the dimensionality of the dataset using PCA, we first need to standardize the features that we described previously. This can be achieved using <kbd>MLlib</kbd>'s</span> <kbd>StandardScaler</kbd> <span>estimator and fitting it to the Spark dataframe containing our feature vectors, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">standardizer = StandardScaler(withMean=True, withStd=True,<br/>   inputCol='features', outputCol='std_features')<br/>standardizer_model = standardizer<br/>   .fit(user_movie_ratings_features_df)<br/>user_movie_ratings_standardized_features_df =<br/>   standardizer_model.transform(user_movie_ratings_features_df) </pre>
<ol start="4">
<li>Next, we convert our scaled features into a <kbd>MLlib</kbd> <kbd>RowMatrix</kbd><em> </em>instance. A <kbd>RowMatrix</kbd> is a distributed matrix with no index, where each row is a vector. We achieve this by converting our scaled features data frame into an RDD and mapping each row of the RDD to the corresponding scaled feature vector. We then pass this RDD to <kbd>MLlib</kbd>'s <kbd>RowMatrix()</kbd> (as shown in the following code), resulting in a matrix of standardized feature vectors of dimensions 300 x 3,000:</li>
</ol>
<pre style="padding-left: 60px">scaled_features_rows_rdd = <br/>   user_movie_ratings_standardized_features_df<br/>   .select("std_features").rdd<br/>scaled_features_matrix = RowMatrix(scaled_features_rows_rdd<br/>   .map(lambda x: x[0].tolist()))</pre>
<ol start="5">
<li><span>Now that we have our standardized data in matrix form, we can easily compute the top</span> <em>k</em> <span>principal components by invoking the</span> <kbd>computePrincipalComponents()</kbd> <span>method exposed by <kbd>MLlib</kbd>'s</span> <kbd>RowMatrix</kbd>. <span>We can compute the top 300 principal components as follows:</span></li>
</ol>
<pre style="padding-left: 60px">number_principal_components = 300<br/>principal_components = scaled_features_matrix<br/>   .computePrincipalComponents(number_principal_components)</pre>
<ol start="6">
<li><span>Now that we have identified the top 300 principal components, we can project the standardized user-community movie ratings data from 3,000 dimensions to a linear subspace of only 300 dimensions while preserving the largest variances from the original dataset. This is achieved by using matrix multiplication and multiplying the matrix containing the standardized data by the matrix containing the top 300 principal components, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">projected_matrix = scaled_features_matrix<br/>   .multiply(principal_components)<br/>print((projected_matrix.numRows(), projected_matrix.numCols()))</pre>
<p style="padding-left: 60px"><span>The resulting matrix now has dimensions of 300 x 300, confirming the reduction in dimensionality from the original 3,000 to only 300! We can now use this projected matrix and its PCA feature vectors as the input into subsequent machine learning models as normal.</span></p>
<ol start="7">
<li>Alternatively, we can use <kbd>MLlib</kbd>'s <kbd>PCA()</kbd> estimator directly on the dataframe containing our standardized feature vectors to generate a new dataframe with a new column containing the PCA feature vectors, as follows:</li>
</ol>
<pre style="padding-left: 60px">pca = PCA(k=number_principal_components, inputCol="std_features",<br/>   outputCol="pca_features")<br/>pca_model = pca.fit(user_movie_ratings_standardized_features_df)<br/>user_movie_ratings_pca_df = pca_model<br/>   .transform(user_movie_ratings_standardized_features_df) </pre>
<p style="padding-left: 60px"><span>Again, this new dataframe and its PCA feature vectors can then be used to train subsequent machine learning models as normal.</span></p>
<ol start="8">
<li>Finally, we can extract the explained variance for each principal component from our PCA model by accessing its <kbd>explainedVariance</kbd> attribute as follows:</li>
</ol>
<pre style="padding-left: 60px">pca_model.explainedVariance</pre>
<p style="padding-left: 60px"><span>The resulting vector (of 300 elements) shows that, in our example, the first eigenvector (and therefore the first principal component) in the ordered list of principal components explains 8.2% of the variance, the second explains 4%, and so on.</span></p>
<p>In this case study, we have demonstrated how we can reduce the dimensionality of the user-community movie ratings dataset from 3,000 dimensions to only 300 dimensions while preserving its structure using PCA. The resulting reduced dataset can then be used to train machine learning models as normal, such as a hierarchical clustering model for collaborative filtering.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have trained and evaluated various unsupervised machine learning models and techniques in Apache Spark using a variety of real-world use cases, including partitioning the various substances found in the human brain using image segmentation and helping to develop a movie recommendation system by reducing the dimensionality of a high-dimensional user-community movie ratings dataset.</p>
<p>In the next chapter, we will develop, test, and evaluate some common algorithms that are used in <strong>natural language processing</strong> (<strong>NLP</strong>) in an attempt to train machines to automatically analyze and understand human text and speech!</p>


            </article>

            
        </section>
    </div>



  </body></html>