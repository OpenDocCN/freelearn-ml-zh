<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Overview of an Amazon Machine Learning Workflow</h1>
            </header>

            <article>
                
<p>This chapter offers an overview of the workflow of a simple Amazon Machine Learning (Amazon ML) project, which comprises three main phases:</p>
<ol>
<li>Preparing the data</li>
<li>Training and selecting the model</li>
<li>Making predictions</li>
</ol>
<p>The reader will learn how to get started on the Amazon Machine Learning platform, how to set up an account, and how to secure it. In the second part, we go through a simple numeric prediction problem based on a classic dataset. We describe each of the three steps mentioned above, what happens, what to expect, and how to interpret the final result.</p>
<p>In this chapter, we will study the following:</p>
<ul>
<li>Opening an Amazon Web Services (AWS) account</li>
<li>Setting up the account</li>
<li>Overview of a standard Amazon Machine Learning workflow</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Opening an Amazon Web Services Account</h1>
            </header>

            <article>
                
<p>Signing up for an AWS account is straightforward. Go to <span class="URLPACKT"><a href="http://aws.amazon.com/" target="_blank">http://aws.amazon.com/</a></span>, and choose <span class="packt_screen">Create a AWS Account</span>. If you don't have an AWS account yet, take advantage of the free tier access. New free-tier accounts enjoy free resources up to a certain limit for up to 12 months. These free resources are available for many AWS services, such as EC2, S3, RDS or Redshift, and so forth. Unfortunately, Amazon Machine Learning is not included in the AWS Free Tier. You will be billed for your Amazon ML usage. However, since Amazon ML requires your data to be stored on S3 or another AWS source such as RedShift, which are included in the Free Tier offer, it will still be advantageous to start with a free tier account. Follow the instructions to open a free tier account. You will be asked for your name, e-mail, address, phone number, and payment information.</p>
<div class="packt_infobox packt_tip">If you are already an Amazon retail customer, it will be easier if you separate your AWS account from your personal retail amazon account by using a different e-mail address for your AWS account. This is especially true if you plan to recover the costs of working on Amazon ML for professional purposes. Using the same e-mail for both your personal retail account (Amazon Prime, echo, and so on) and your AWS account could become confusing.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Security</h1>
            </header>

            <article>
                
<p>The e-mail and password you have used to open an AWS account are called your root credentials. They give you root access to every AWS service, which means unlimited access to unlimited resources. Someone obtaining your root credentials without your knowledge could rack up a heavy bill and they could carry out all types of activities through your account in your name. It is highly recommended not to use this root access in your everyday operations with AWS and to set up your account with the highest security level possible.</p>
<p>Fortunately, AWS offers many ways to control and compartmentalize access to your AWS account through users, groups, roles, policies, passwords, and multi-factor authentication to reduce risks of unlawful access and fraud. Configuring and managing access to your account is done via the <strong>AWS Identity and Access Management (IAM)</strong> service. The IAM service is free of charge.</p>
<p>The IAM service allows you to create and configure all the access points to the different AWS services you plan to use. Having this level of granularity is important. You can restrict access by user, by service, by role, or even enable temporary access through tokens, which are limited in time. Enabling multi-factor authentication is another strongly recommended feature you should enable in order to prevent unauthorized access to your AWS account.</p>
<p>In the context of this book, we will create a single user with unlimited access to only two services: Amazon ML and S3. We will extend this user's access to other AWS services as we need them in following chapters.</p>
<div class="packt_infobox">We won't go through all the features offered by IAM here, but it's strongly recommended that you familiarize yourself with the IAM documentation and best practices (<span class="URLPACKT"><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html</a>).</span></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up the account</h1>
            </header>

            <article>
                
<p>When you set up your account for the first time, you are given access to your root public and secret access keys. These keys will be useful as you manage data on S3 and models in Amazon ML via the command line interface (AWS CLI). These two keys will only be available for you to view and copy at the time of creation. Once that page in your browser is closed, you can no longer access them and will need to create new ones. Creating new root keys is done by accessing <span class="packt_screen">My Account</span> | <span class="packt_screen">Security Credentials</span>. It's worth noting that no one can have access to your keys in AWS, not even the administrator of your account.</p>
<p>We won't go through all the possible actions you can take in IAM to configure, manage, and secure access to your account as a full presentation of IAM is beyond the scope of this book. Your access management needs and policies will depend on the size of your organization and security constraints. We assume here that you are a unique individual user of the account and that you do not need to set up password policies for other users, groups, or roles. However, we strongly recommend you familiarize yourself with IAM documentation and implement the IAM best practice (<span class="URLPACKT"><a href="https://aws.amazon.com/documentation/iam" target="_blank">https://aws.amazon.com/documentation/iam</a>).</span></p>
<p>We will create a new user whose username is <kbd>AML@Packt</kbd> and will use this access for the rest of the book. The only time when we use the root access (with the password you used to create the AWS account in the first place) is when we need to add or remove services to and from the <kbd>AML@Packt</kbd> user, for instance, when we want the user to use Amazon Athena for data processing or <strong>Amazon Lambda</strong> for scripting.</p>
<p>The IAM dashboard is available at <a href="https://console.aws.amazon.com/iam/home#/home" target="_blank"><span class="URLPACKT">https://console.aws.amazon.com/iam</span></a><span class="URLPACKT">.</span> It displays how many IAM assets you have created (users, roles, groups, and so on) as well as your security status as shown by this screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="314" src="assets/image_03_001.png" width="592"/></div>
<p>This screenshot shows that we have implemented the following three items:</p>
<ul>
<li><strong>Delete your root access keys</strong>: These keys were given to you when you created your account. Since they provide unlimited access to your account, you should delete them and use only user-based access keys to access your account.</li>
<li><strong>Activate Multi Factor Authentication on your root account:</strong> After you have logged in with your login and password, Multi Factor Authentication (MFA) requires you to input a six digit code. This code can either be sent to you via text or e-mail or made available via an authenticator app installed on your mobile phone. MFA is a easy-to-implement and efficient means to secure access to your account.</li>
<li><strong>Create individual IAM users:</strong> By creating individual users you can restrict, manage their access level, and deactivate their account easily.</li>
</ul>
<p>You could also create groups to assign permissions to your users and define a password policy.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating a user</h1>
            </header>

            <article>
                
<p>Let's start by creating your user. Go to the IAM dashboard at <a href="https://console.aws.amazon.com/iam/" target="_blank"><span class="URLPACKT">https://console.aws.amazon.com/iam/</span></a> and click on <span class="packt_screen">Users</span> on the left sidebar. The user creation process is straightforward:</p>
<ol>
<li>Click <span class="packt_screen">Create New Users</span>.</li>
<li>Enter your user name. Keep the <span class="packt_screen">Generate an access key for each user</span> checkbox selected.</li>
</ol>
<p>At that point, your user is created and you can choose to view and download the user security credentials. One of these two keys, the <strong>Secret Access Key</strong>, will no longer be available once you move away from this page. Be sure to copy or download the two keys now. If you lose the keys, you can always recreate a pair of new keys for that user from the <span class="packt_screen">IAM &gt; Users dashboard</span>. This screenshot is an example of the two user keys for the <kbd>AML@Packt</kbd> user we just created. Your keys will obviously be different than these ones:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="258" src="assets/image_03_002.png" width="714"/></div>
<p>And that's it! Your <kbd>AML@Packt</kbd> user has been created. You can use <kbd>AML@Packt</kbd>'s access keys to access and manage AWS services via APIs and the command line. At this point, the <kbd>AML@Packt</kbd> user has unlimited access to all AWS services. In order to restrict the access scope of that user, you will need to attach policies to the user.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Defining policies</h1>
            </header>

            <article>
                
<p>Policies declare what services a user or a group can access and the level of access (read-only, full access, and so forth). You can define global policies that take care of several services at the same time and attach them to groups of users, or you can attach specific mono-service policies to your user. This is what we'll do now.</p>
<p>Once you have created your user and downloaded its credentials, you end up on the IAM user dashboard with a list of all your created users. Select the <kbd>AML@Packt</kbd> user and the permissions tab. Check all the services you know that the user will need to access. In our case, we select two services each with full access, which will be sufficient to explore the Amazon Machine Learning service:</p>
<ul>
<li>Amazon Machine Learning Full Access</li>
<li>Amazon S3 Full Access</li>
</ul>
<p>We will add other policies to these users to enable other services (Athena, RedShift, RDS, and so on) as needed later on.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating login credentials</h1>
            </header>

            <article>
                
<p>Last but not least if we want to use the <kbd>AML@Packt</kbd> user to log in to the AWS console, we must create login credentials for that user. As shown in the next screenshot, the <span class="packt_screen">Security Credentials</span> tab is where you manage the user access keys, sign in credentials, and SSH keys:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="422" src="assets/image_03_003.png" width="620"/></div>
<div class="packt_infobox">SSH keys are not the same as access keys. SSH keys will let you SSH into certain assets, such as <em>EC2</em> servers. Many services machine learning included have no use for SSH keys. Access keys, on the other hand, are used to programmatically manage AWS services. Access keys are necessary for setting the credentials needed to use the command line interface (AWS CLI).</div>
<p>Click on <span class="packt_screen">Manage Password</span> and set a password for the user. This is what the permission for user <kbd>AML@Packt</kbd> looks like:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="479" src="assets/image_03_004.png" width="646"/></div>
<p>At this point, our IAM dashboard looks like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="355" src="assets/image_03_005.png" width="669"/></div>
<p>This previous screenshot shows the following:</p>
<ul>
<li>We have deleted the root access keys. We can no longer programmatically access all AWS services in an unlimited fashion via the command line or APIs. We can still log in as root to the AWS console to create and manage access for people, but these will depend on the policies and access level we provide them with.</li>
<li>We have activated <strong>Multi Factor Authentication (MFA)</strong>, a simple and very efficient way to secure access to AWS Services from your root access credentials.</li>
<li>We have created the <kbd>AML@Packt</kbd> user, which we will use to log in to AWS and when using the APIs or the command line.</li>
</ul>
<p>We have not created groups or password policies that would further constrain the root access as we intend to only access Amazon ML through the <kbd>AML@Packt</kbd> user.</p>
<p>Here is a summary of the different ways you can access and use AWS services:</p>
<ul>
<li>Log in on the AWS Console with your root password and login using <strong>Multi Factor Authentication (MFA)</strong>.</li>
<li>Log in with the <kbd>AML@Packt</kbd> user with that user's login and password using MFA. The <kbd>AML@Packt</kbd> user can only use S3 and Amazon ML services. This is quite restrictive, but just the right amount of access for that user, nothing more.</li>
<li>Programmatically access S3 and Amazon ML via the S3 and Amazon ML access keys using the command line interface or AWS SDKs.</li>
</ul>
<p>We had started with one user, the root user able to access everything AWS can offer programmatically and via the console. The new setup is much more secure and worth the time it took to set it up. Our newly gained understanding of IAM roles and policies will also be helpful later on when we start using different AWS services in tandem as the services will need to have appropriate access to one another. </p>
<p>Before we dive into the presentation of a standard Amazon ML workflow, we need a brief word on regions. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Choosing a region</h1>
            </header>

            <article>
                
<p>AWS currently operates data centers in 14 regions across the globe. More regions are being frequently opened across the globe (<span class="URLPACKT"><a href="https://aws.amazon.com/about-aws/global-infrastructure/" target="_blank">https://aws.amazon.com/about-aws/global-infrastructure/</a>)</span>. Most AWS services require you to choose a region of operation. The rule of thumb is to choose the region that is closest to you or the end-users accessing your resources.</p>
<p>Choosing a region may depend on other factors, which may vary across regions, including:</p>
<ul>
<li>Latency</li>
<li>Pricing</li>
<li>Security and compliance rules</li>
<li>Availability of the service</li>
<li>SLA–Service Level Agreements</li>
<li>Use of renewable energy. At the time of writing, AWS offers two carbon neutral regions (<span class="URLPACKT"><a href="https://aws.amazon.com/about-aws/sustainability/" target="_blank">https://aws.amazon.com/about-aws/sustainability/</a>) and is actively creating others .</span></li>
</ul>
<p>Amazon ML is only available in Northern Virginia and Ireland. You can check the following page for AWS availability per region: <span class="URLPACKT"><a href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/" target="_blank">https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/</a>.</span></p>
<div class="packt_infobox"><strong>GovCloud:</strong> From the AWS documentation: The AWS GovCloud (US) is an isolated AWS region designed to host sensitive data and regulated workloads in the cloud, helping customers support their US government compliance requirements. IAM is region free, with the following exception: IAM is available in the US East (N. Virginia) region and in the GovCloud region. Users and roles created in the US East region can be used in all other regions except the GovCloud region. And GovCloud IAM users cannot be used outside of the GovCloud region.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Overview of a standard Amazon Machine Learning workflow</h1>
            </header>

            <article>
                
<p>The Amazon Machine Learning service is available at <a href="https://console.aws.amazon.com/machinelearning/" target="_blank">https://console.aws.amazon.com/machinelearning/</a>. The Amazon ML workflow closely follows a standard Data Science workflow with steps: </p>
<ol>
<li>Extract the data and clean it up. Make it available to the algorithm.</li>
<li>Split the data into a training and validation set, typically a 70/30 split with equal distribution of the predictors in each part.</li>
<li>Select the best model by training several models on the training dataset and comparing their performances on the validation dataset.</li>
<li>Use the best model for predictions on new data.</li>
</ol>
<p>As shown in the following Amazon ML menu, the service is built around four objects:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_006.png"/></div>
<ul>
<li><span class="packt_screen">Datasource</span></li>
<li><span class="packt_screen">ML model</span></li>
<li><span class="packt_screen">Evaluation</span></li>
<li><span class="packt_screen">Prediction</span></li>
</ul>
<p>The Datasource and Model can also be configured and set up in the same flow by creating a new <span class="packt_screen">Datasource and ML model</span>. Let us take a closer look at each one of these steps.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The dataset</h1>
            </header>

            <article>
                
<p>For the rest of the chapter, we will use the simple <kbd>Predicting Weight by Height and Age</kbd> dataset (from <em>Lewis Taylor (1967)</em>) with 237 samples of children's age, weight, height, and gender, which is available at <span class="URLPACKT"><a href="https://v8doc.sas.com/sashtml/stat/chap55/sect51.htm" target="_blank">https://v8doc.sas.com/sashtml/stat/chap55/sect51.htm</a>.</span></p>
<p>This dataset is composed of 237 rows. Each row has the following predictors: sex (F, M), age (in <em>months</em>), height (in <em>inches</em>), and we are trying to predict the weight (in <em>lbs</em>) of these children. There are no missing values and no outliers. The variables are close enough in range and normalization is not required. In short, we do not need to carry out any preprocessing or cleaning on the original dataset. Age, height, and weight are numerical variables (real-valued), and sex is a categorical variable.</p>
<p>We will randomly select 20% of the rows as the held-out subset to use for prediction on previously unseen data and keep the other 80% as training and evaluation data. This data split can be done in Excel or any other spreadsheet editor:</p>
<ul>
<li>By creating a new column with randomly generated numbers</li>
<li>Sorting the spreadsheet by that column</li>
<li>Selecting 190 rows for training and 47 rows for prediction (roughly a 80/20 split)</li>
</ul>
<p>Let us name the training set <kbd>LT67_training.csv</kbd> and the held-out set that we will use for prediction <kbd>LT67_heldout.csv</kbd>, where <em>LT67</em> stands for <em>Lewis and Taylor,</em> the creator of this dataset in 1967.</p>
<div class="packt_infobox">As with all datasets, scripts, and resources mentioned in this book, the training and holdout files are available in the GitHub repository at <a href="https://github.com/alexperrier/packt-aml" target="_blank"><span class="URLPACKT">https://github.com/alexperrier/packt-aml</span></a>.</div>
<p>Note that it is important for the distribution in age, sex, height, and weight to be similar in both subsets. We want the data on which we will make predictions to show patterns that are similar to the data on which we will train and optimize our model.</p>
<p>In the rest of this section, we will do the following:</p>
<ol>
<li>Load the data on S3.</li>
<li>Let Amazon ML infer the schema and transform the data.</li>
<li>Create a model.</li>
<li>Evaluate the model's performance.</li>
<li>Make a prediction on the held-out dataset.</li>
</ol>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Loading the data on S3</h1>
            </header>

            <article>
                
<p>Follow these steps<span class="MsoCommentReference">t</span>o load the training and held-out datasets on S3:</p>
<ol>
<li>Go to your s3 console at <a href="https://console.aws.amazon.com/s3" target="_blank"><span class="URLPACKT">https://console.aws.amazon.com/s3</span></a><span class="URLPACKT">.</span></li>
<li>Create a bucket if you haven't done so already. Buckets are basically folders that are uniquely named across all S3. We created a bucket named <kbd>aml.packt</kbd>. Since that name has now been taken, you will have to choose another bucket name if you are following along with this demonstration.</li>
</ol>
<ol start="3">
<li>Click on the bucket name you created and upload both the <kbd>LT67_training.csv</kbd> and <kbd>LT67_heldout.csv</kbd> files by selecting <span class="packt_screen">Upload</span> from the <span class="packt_screen">Actions</span> drop-down menu:</li>
</ol>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="359" src="assets/image_03_007.png" width="477"/></div>
<p>Both files are small, only a few KB, and hosting costs should remain negligible for that exercise.</p>
<p>Note that for each file, by selecting the <span class="packt_screen">Properties</span> tab on the right, you can specify how your files are accessed, what user, role, group or AWS service may download, read, write, and delete the files, and whether or not they should be accessible from the Open Web. When creating the datasource in Amazon ML, you will be prompted to grant Amazon ML access to your input data. You can specify the access rules to these files now in S3 or simply grant access later on.</p>
<p>Our data is now in the cloud in an S3 bucket. We need to tell Amazon ML where to find that input data by creating a datasource. We will first create the datasource for the training file <kbd>ST67_training.csv</kbd>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Declaring a datasource</h1>
            </header>

            <article>
                
<p>Go to the Amazon ML dashboard, and click on <span class="packt_screen">Create new...</span> | <span class="packt_screen">Datasource and ML model</span>. We will use the faster flow available by default:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="237" src="assets/image_03_008.png" width="257"/></div>
<p>As shown in the following screenshot, you are asked to specify the path to the <kbd>LT67_training.csv</kbd> file <kbd>{S3://bucket}{path}{file}</kbd>. Note that the S3 location field automatically populates with the bucket names and file names that are available to your user:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="236" src="assets/image_03_009.png" width="561"/></div>
<p>Specifying a <span class="packt_screen">Datasource name</span> is useful to organize your Amazon ML assets. By clicking on <span class="packt_screen">Verify</span>, Amazon ML will make sure that it has the proper rights to access the file. In case it needs to be granted access to the file, you will be prompted to do so as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="199" src="assets/image_03_010.png" width="496"/></div>
<p>Just click on <span class="packt_screen">Yes</span> to grant access. At this point, Amazon ML will validate the datasource and analyze its contents.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating the datasource</h1>
            </header>

            <article>
                
<p>An Amazon ML datasource is composed of the following:</p>
<ul>
<li>The location of the data file: The data file is not duplicated or cloned in Amazon ML but accessed from S3</li>
<li>The schema that contains information on<strong> </strong>the type of the variables contained in the CSV file:
<ul>
<li>Categorical</li>
<li>Text</li>
<li>Numeric (real-valued)</li>
<li>Binary</li>
</ul>
</li>
</ul>
<p>As we will see in <span class="ChapterrefPACKT"><a href="08d9b49a-a25c-4706-8846-36be9538b087.xhtml" target="_blank">Chapter 4</a></span>, <em>Loading and Preparing the Dataset</em>, it is possible to supply Amazon ML with your own schema or modify the one created by Amazon ML.</p>
<p>At this point, Amazon ML has a pretty good idea of the type of data in your training dataset. It has identified the different types of variables and knows how many rows it has:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="232" src="assets/image_03_011.png" width="523"/></div>
<p>Move on to the next step by clicking on <span class="packt_screen">Continue</span>, and see what schema Amazon ML has inferred from the dataset as shown in the next screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="331" src="assets/image_03_012.png" width="693"/></div>
<p>Amazon ML needs to know at that point which is the variable you are trying to predict. Be sure to tell Amazon ML the following:</p>
<ul>
<li>The first line in the CSV file contains te column name</li>
<li>The target is the <kbd>weight</kbd></li>
</ul>
<p>We see here that Amazon ML has correctly inferred the following:</p>
<ul>
<li><kbd>sex</kbd> is <span class="packt_screen">categorical</span></li>
<li><kbd>age</kbd>, <kbd>height</kbd>, and <kbd>weight</kbd> are <span class="packt_screen">numeric</span> (continuous real values)</li>
</ul>
<p>Since we chose a numeric variable as the target Amazon ML, will use Linear Regression as the predictive model. For binary or categorical values, we would have used Logistic Regression. This means that Amazon ML will try to find the best <em>a</em>, <em>b</em>, and <em>c</em> coefficients so that the weight predicted by the following equation is as close as possible to the observed real weight present in the data:</p>
<p class="CDPAlignCenter CDPAlign"><em>predicted weight = a * age + b * height + c * sex</em></p>
<p>Amazon ML will then ask you if your data contains a row identifier. In our present case, it does not. Row identifiers are useful when you want to understand the prediction obtained for each row or add an extra column to your dataset later on in your project. Row identifiers are for reference purposes only and are not used by the service to build the model.</p>
<p>You will be asked to review the datasource. You can go back to each one of the previous steps and edit the parameters for the schema, the target and the input data. Now that the data is known to Amazon ML, the next step is to set up the parameters of the algorithm that will train the model.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The model</h1>
            </header>

            <article>
                
<p>We select the default parameters for the training and evaluation settings. Amazon ML will do the following:</p>
<ul>
<li>Create a recipe for data transformation based on the statistical properties it has inferred from the dataset</li>
<li>Split the dataset (<kbd>ST67_training.csv</kbd>) into a training part and a validation part, with a 70/30 split. The split strategy assumes the data has already been shuffled and can be split sequentially.</li>
</ul>
<p>In <a href="08d9b49a-a25c-4706-8846-36be9538b087.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Loading and Preparing the Dataset</em>, we will take the longer road and work directly on the recipes, the schemas, and the validation split. On the next page, you are asked to review the model you just created.</p>
<p>The recipe will be used to transform the data in a similar way for the training and the validation datasets. The only transformation suggested by Amazon ML is to transform the categorical variable <kbd>sex</kbd> into a binary variable, where <kbd>m = 0</kbd> and <kbd>f = 1</kbd> for instance. No other transformation is needed.</p>
<p>The default advanced settings for the model are shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="134" src="assets/image_03_013.png" width="250"/></div>
<p>We see that Amazon ML will pass over the data 10 times, shuffle splitting the data each time. It will use an <span class="packt_screen">L2</span> regularization strategy based on the sum of the square of the coefficients of the regression to prevent overfitting. We will evaluate the predictive power of the model using our <kbd>LT67_heldout.csv</kbd> dataset later on.</p>
<p>Regularization comes in 3 levels with a <em>mild</em> (10<sup>^-6</sup>), <em>medium</em> (10<sup>^-4</sup>), or <em>aggressive</em> (10<sup>^-02</sup>) setting, each value stronger than the previous one. The default setting is mild<em>,</em> the lowest, with a regularization constant of <em>0.00001</em> (10<sup>^-6</sup>) implying that Amazon ML does not anticipate much overfitting on this dataset. This makes sense when the number of predictors, three in our case, is much smaller than the number of samples (190 for the training set).</p>
<p>Clicking on the <span class="packt_screen">Create ML model</span> button will launch the model creation. This takes a few minutes to resolve, depending on the size and complexity of your dataset. You can check its status by refreshing the model page. In the meantime, the model status remains pending.</p>
<p>At that point, Amazon ML will split our training dataset into two subsets: a training and a validation set. It will use the training portion of the data to train several settings of the algorithm and select the best one based on its performance on the training data. It will then apply the associated model to the validation set and return an evaluation score for that model. By default, Amazon ML will sequentially take the first 70% of the samples for training and the remaining 30% for validation.</p>
<p>It's worth noting that Amazon ML will not create two extra files and store them on S3, but instead create two new datasources out of the initial datasource we have previously defined. Each new datasource is obtained from the original one via a <kbd>Data rearrangement</kbd> JSON recipe such as the following:</p>
<pre>
{<br/>  "splitting": {<br/>    "percentBegin": 0,<br/>    "percentEnd": 70<br/>  }<br/>}
</pre>
<p>You can see these two new datasources in the Datasource dashboard. Three datasources are now available where there was initially only one, as shown by the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="62" src="assets/image_03_014.png" width="656"/></div>
<p>While the model is being trained, Amazon ML runs the Stochastic Gradient algorithm several times on the training data with different parameters:</p>
<ul>
<li>Varying the learning rate in increments of powers of 10: 0.01, 0.1, 1, 10, and 100.</li>
<li>Making several passes over the training data while shuffling the samples before each path.</li>
<li>At each pass, calculating the prediction error, the <strong>Root Mean Squared Error (RMSE)</strong>, to estimate how much of an improvement over the last pass was obtained. If the decrease in RMSE is not really significant, the algorithm is considered to have converged, and no further pass shall be made.</li>
<li>At the end of the passes, the setting that ends up with the lowest RMSE wins, and the associated model (the weights of the regression) is selected as the best version.</li>
</ul>
<p>Once the model has finished training, Amazon ML evaluates its performance on the validation datasource. Once the evaluation itself is also ready, you have access to the model's evaluation. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The evaluation of the model</h1>
            </header>

            <article>
                
<p>Amazon ML uses the standard metric RMSE for linear regression. RMSE is defined as the sum of the squares of the difference between the real values and the predicted values:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="57" src="assets/image_03_015.png" width="185"/></div>
<p>Here, <em>ŷ</em> is the predicted values, and <em>y</em> the real values we want to predict (the weight of the children in our case). The closer the predictions are to the real values, the lower the RMSE is. A lower RMSE means a better, more accurate prediction.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Comparing with a baseline</h1>
            </header>

            <article>
                
<p>The RMSE is a relative quantity and not an absolute one. An RMSE of 100 may be good for a certain context, while a RMSE of 10 may be a sign of poor predictions in another context. It is, therefore, important to have a baseline that we can compare our model to. Each Amazon ML evaluation provides a baseline, which is calculated differently depending on the nature of the problem (regression, binary or multiclass classification). The baseline is the score we would obtain using the most simple and obvious model:</p>
<ul>
<li>The baseline for regression is given by a model that always predicts the mean of the target</li>
<li>The baseline for binary classification is an AUC of <em>0.5</em>, which is the score for a model that randomly assigns 0 or 1</li>
<li>The baseline for a multiclass classification problem is the macro average <em>F1</em> score for a model that would always predict the most common class</li>
</ul>
<p>In our current case, when predicting the weight of students from the height, age, and sex, our training dataset has a baseline RMSE of <em>18.71</em>, while our model gives an RMSE of <em>14.44</em>. Our model does 22.8% better than simply predicting the average of the weights of all the students.</p>
<p>Another thing to look at in the evaluation is the distribution of the residuals, which are defined as the difference between <em>ŷ</em> and <em>y</em>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="18" src="assets/image_03_020.png" width="153"/></div>
<p>As we've seen in <span class="ChapterrefPACKT"><a href="767f8b14-c3f2-4e45-bfcd-bb45ae2b9e65.xhtml" target="_blank">Chapter 1</a></span>, <em><span class="cdp-organizer-chapter-title">Introduction</span> to Machine Learning and Predictive Analytics</em>, one of the conditions for a linear regression to be considered valid is that the residuals are independent, identically distributed, and their distribution follows a <strong>Gaussian distribution</strong> or <strong>Bell-shaped curve</strong>. Amazon ML shows a histogram of the residuals that can help us visually assess the Gaussian nature of the residuals distribution.</p>
<p>When the shape bends to the right (left), this means that more predictions are greater (lower) than their targets. In both cases, the conclusion is that there is still some signal in the data that has not been captured by the model. When the distribution of residuals is centered on 0, the linear regression is considered valid. The following diagram shows the distributions of residuals for our current regression model:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="380" src="assets/image_03_021.png" width="469"/></div>
<p>We can see that our predictions are often larger than the targets, which indicates that our model could still be improved. There is information in the data that has not been exploited by that model.</p>
<p>Note that it may not be possible to extract the information in these patterns with a linear regression model. Other more complex models may be more adapted to this particular dataset. Transforming the data to create new variables may also be the key to a better model. The histogram of residuals is a good, simple diagnostic with regard to the quality of our model as it shows us that it could be improved in some way.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Making batch predictions</h1>
            </header>

            <article>
                
<p>We now have a model that has been properly trained and selected among other models. We can use it to make predictions on new data.</p>
<p>Remember that, at the beginning of this chapter, under the section <em>Loading the Data on S3</em>, we uploaded two datasets to S3, the training dataset and the held-out dataset. We've used the training dataset to create the best model possible. We will now apply that model on the held-out dataset.</p>
<p>A batch prediction consists in applying a model to a datasource in order to make predictions on that datasource. We need to tell Amazon ML which model we want to apply on which data.</p>
<p>Batch predictions are different from streaming predictions. With batch predictions, all the data is already made available as a datasource, while for streaming predictions, the data will be fed to the model as it becomes available. The dataset is not available beforehand in its entirety.</p>
<p>In the Main Menu select <span class="packt_screen">Batch Predictions</span> to access the dashboard predictions and click on <span class="packt_screen">Create a New</span> Prediction:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="180" src="assets/image_03_022.png" width="276"/></div>
<p>The first step is to select one of the models available in your model dashboard. You should choose the one that has the lowest RMSE:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="245" src="assets/image_03_023.png" width="737"/></div>
<p>The next step is to associate a datasource to the model you just selected. We had uploaded the held-out dataset to S3 at the beginning of this chapter (under the <em>Loading the data on S3</em> section) but had not used it to create a datasource. We will do so now.</p>
<p>When asked for a datasource in the next screen, make sure to check <span class="packt_screen">My data is in S3, and I need to create a datasource,</span> and then select the held-out dataset that should already be present in your S3 bucket:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_024.png"/></div>
<p>Don't forget to tell Amazon ML that the first line of the file contains columns.</p>
<div class="packt_infobox">In our current project, our held-out dataset also contains the true values for the weight of the students. This would not be the case for "real" data in a real-world project where the real values are truly unknown. However, in our case, this will allow us to calculate the RMSE score of our predictions and assess the quality of these predictions.</div>
<p>The final step is to click on the <span class="packt_screen">Verify</span> button and wait for a few minutes:</p>
<ul>
<li>Amazon ML will run the model on the new datasource and will generate predictions in the form of a CSV file.</li>
<li>Contrary to the evaluation and model-building phase, we now have real predictions. We are also no longer given a score associated with these predictions.</li>
<li>After a few minutes, you will notice a new batch-prediction folder in your S3 bucket. This folder contains a <kbd>manifest</kbd> file and a results folder. The manifest file is a JSON file with the path to the initial datasource and the path to the results file. The results folder contains a gzipped CSV file:</li>
</ul>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_025.png"/></div>
<p>Uncompressed, the CSV file contains two columns, <kbd>trueLabel</kbd>, the initial target from the held-out set, and <kbd>score</kbd>, which corresponds to the predicted values. We can easily calculate the RMSE for those results directly in the spreadsheet through the following steps:</p>
<ol>
<li>Creating a new column that holds the square of the difference of the two columns.</li>
<li>Summing all the rows.</li>
<li>Taking the square root of the result.</li>
</ol>
<p>The following illustration shows how we create a third column <kbd>C</kbd>, as the squared difference between the <kbd>trueLabel</kbd> column A and the <span class="packt_screen">score</span> (or predicted value) column B:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="164" src="assets/image_03_026.png" width="278"/></div>
<p>As shown in the following screenshot, averaging column <kbd>C</kbd> and taking the square root gives an RMSE of 11.96, which is even significantly better than the RMSE we obtained during the evaluation phase (<em>RMSE 14.4</em>):</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="88" src="assets/image_03_027.png" width="343"/></div>
<p>The fact that the RMSE on the held-out set is better than the RMSE on the validation set means that our model did not overfit the training data, since it performed even better on new data than expected. Our model is robust.</p>
<p>The left side of the following graph shows the True (<em>Triangle</em>) and Predicted (<em>Circle</em>) <kbd>Weight</kbd> values for all the samples in the held-out set. The right side shows the histogram of the residuals. Similar to the histogram of residuals we had observed on the validation set, we observe that the residuals are not centered on <em>0</em>. Our model has a tendency to overestimate the weight of the students:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="310" src="assets/image_03_028.png" width="619"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In the first part of this chapter, we went through the Amazon account creation and how to properly set up and secure access to your AWS account. Using a combination of Multi Factor Authentication and User creation, we were able to quickly reach a satisfactory level of safety. AWS is a powerful platform with powerful tools and it's important to implement the best access protection possible.</p>
<p>In the second part, we went through the different steps involved in a simple linear regression prediction, from loading the data into S3, making that data accessible to Amazon ML via datasources, creating models, interpreting evaluations, and making predictions on new data.</p>
<p>The Amazon ML flow is smooth and facilitates the inherent data science loop: data, model, evaluation, and prediction.</p>
<p>In the following c<span class="ChapterrefPACKT">hapter,</span> we will dive further into data preparation and data transformation. This time we will use a classic binary classification problem, namely, <em>survival on the Titanic</em>, which is based on a very interesting dataset.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>