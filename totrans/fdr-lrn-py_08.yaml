- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Introducing Existing Federated Learning Frameworks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍现有的联邦学习框架
- en: The objective of this chapter is to introduce existing **federated learning**
    (**FL**) frameworks and platforms, applying each to federated learning scenarios
    involving toy **machine learning** (**ML**) problems. The platforms focused on
    in this chapter are Flower, TensorFlow Federated, OpenFL, IBM FL, and STADLE –
    the idea behind this selection was to help you by covering a breadth of existing
    FL platforms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是介绍现有的**联邦学习**（**FL**）框架和平台，将每个平台应用于涉及玩具**机器学习**（**ML**）问题的联邦学习场景。本章关注的平台是Flower、TensorFlow
    Federated、OpenFL、IBM FL和STADLE——选择这些平台背后的想法是通过涵盖现有的FL平台范围来帮助你。
- en: By the end of this chapter, you should have a basic understanding of how to
    use each platform for FL, and you should be able to choose a platform based on
    its associated strengths and weaknesses for an FL application.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该对如何使用每个平台进行联邦学习有一个基本的了解，并且你应该能够根据其相关的优势和劣势选择一个平台用于联邦学习应用。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to existing FL frameworks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有FL框架的介绍
- en: Implementations of an example NLP FL task on movie review dataset, using existing
    frameworks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现有框架在电影评论数据集上实现示例NLP FL任务
- en: Implementations of example computer vision FL task with non-IID datasets, using
    existing frameworks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现有框架实现示例计算机视觉FL任务，使用非-IID数据集
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find the supplemental code files for this chapter in the book’s GitHub
    repository:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到本章的补充代码文件：
- en: https://github.com/PacktPublishing/Federated-Learning-with-Python
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: https://github.com/PacktPublishing/Federated-Learning-with-Python
- en: Important note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can use the code files for personal or educational purposes. Please note
    that we will not support deployments for commercial use and will not be responsible
    for any errors, issues, or damage caused by using the code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用代码文件进行个人或教育目的。请注意，我们不会支持商业部署，并且不会对使用代码造成的任何错误、问题或损害负责。
- en: Each implementation example in this chapter was run on an x64 machine running
    Ubuntu 20.04.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的每个实现示例都是在运行Ubuntu 20.04的x64机器上运行的。
- en: 'The implementation of the training code for the NLP example requires the following
    libraries to run:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: NLP示例的训练代码实现需要以下库来运行：
- en: Python 3 (version ≥ 3.8)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3 (版本 ≥ 3.8)
- en: NumPy
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: TensorFlow (version ≥ 2.9.1)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow（版本 ≥ 2.9.1）
- en: TensorFlow Hub (`pip` `install tensorflow-hub`)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Hub (`pip` `install tensorflow-hub`)
- en: TensorFlow Datasets (`pip` `install tensorflow-datasets`)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Datasets (`pip` `install tensorflow-datasets`)
- en: TensorFlow Text (`pip` `install tensorflow-text`)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Text (`pip` `install tensorflow-text`)
- en: Using a GPU with the appropriate TensorFlow installation is recommended to save
    training time for the NLP example, due to the size of the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型的大小，建议使用带有适当TensorFlow安装的GPU来节省NLP示例的训练时间。
- en: 'The implementation of the training code for the **non-IID** (**non-independent
    and identical distribution**) computer vision example requires the following libraries
    to run:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练非-IID（非独立同分布）计算机视觉示例的代码实现需要以下库来运行：
- en: Python 3 (version ≥ 3.8)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3（版本 ≥ 3.8）
- en: NumPy
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: PyTorch (version ≥ 1.9)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch（版本 ≥ 1.9）
- en: Torchvision (version ≥ 0.10.0, tied to PyTorch version)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torchvision（版本 ≥ 0.10.0，与PyTorch版本相关联）
- en: The installation instructions for each FL framework are listed in the following
    subsections.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每个FL框架的安装说明列在以下子节中。
- en: TensorFlow Federated
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Federated
- en: 'You can install the following libraries to use TFF:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以安装以下库来使用TFF：
- en: '`tensorflow_federated` (using the `pip install` `tensorflow_federated` command)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow_federated`（使用`pip install tensorflow_federated`命令）'
- en: '`nest_asyncio` (using the `pip install` `nest_asyncio` command)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nest_asyncio`（使用`pip install nest_asyncio`命令）'
- en: OpenFL
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenFL
- en: You can install OpenFL using `pip` `install openfl`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`pip install openfl`命令安装OpenFL。
- en: 'Alternatively, you can build from source with the following commands:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用以下命令从源代码构建：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: IBM FL
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IBM FL
- en: 'Installing the locally hosted version of IBM FL requires the wheel installation
    file located in the code repository. To perform this installation, run the following
    commands:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 安装IBM FL的本地版本需要位于代码仓库中的wheel安装文件。要执行此安装，请运行以下命令：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Flower
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flower
- en: You can install Flower using the `pip install` `flwr` command.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`pip install flwr`命令安装Flower。
- en: STADLE
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: STADLE
- en: You can install the STADLE client-side library using the `pip install` `stadle-client`
    command.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to FL frameworks
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we introduce the FL frameworks and platforms to be used in the subsequent
    implementation-focused sections.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Flower
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flower ([https://flower.dev/](https://flower.dev/)) is an open source and ML
    framework-agnostic FL framework that aims to be accessible to users. Flower follows
    a standard client-server architecture, in which the clients are set up to receive
    the model parameters from the server, train on local data, and send the new local
    model parameters back to the server.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The high-level orchestration of the federated learning process is dictated by
    what Flower calls strategies, used by the server for aspects such as client selection
    and parameter aggregation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Flower uses **Remote Procedure Calls** (**RPCs**) in order to perform said orchestration
    through client-side execution from messages sent by the server. The extensibility
    of the framework allows researchers to experiment with novel approaches such as
    new aggregation algorithms and communication methods (such as model compression).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Federated (TFF)
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TFF ([https://www.tensorflow.org/federated](https://www.tensorflow.org/federated))
    is an open source FL/computation framework built on top of TensorFlow that aims
    to allow researchers to easily simulate federated learning with existing TensorFlow/Keras
    models and training pipelines. It consists of the Federated Core layer, which
    allows for the implementation of general federated computations, and the Federated
    Learning layer, which is built on top and provides interfaces for FL-specific
    processes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: TFF focuses on single-machine local simulations of FL, using wrappers to create
    TFF-specific datasets, models, and federated computations (core client and server
    computation performed during the FL process) from the standard TensorFlow equivalents.
    The focus on building everything from general federated computations allows researchers
    to implement each step as desired, allowing experimentation to be supported.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: OpenFL
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenFL ([https://github.com/intel/openfl](https://github.com/intel/openfl))
    is an open source FL framework developed by Intel, focused on allowing cross-silo
    privacy-preserving ML to be performed. OpenFL allows for two different workflows
    depending on the desired lifespan of the federation (where federation refers to
    the entire FL system).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'In the aggregator-based workflow, a single experiment and associated federated
    learning plan are sent from the aggregator to the participating *collaborators*
    (agents) to be run as the local training step of the FL process—the federation
    is stopped after the experiment is complete. In the director-based workflow, long-lived
    components are instead used to allow for experiments to be run on demand. The
    following diagram depicts the architecture and users for the director-based workflow:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Architecture of director-based workflow (adapted from https://openfl.readthedocs.io/en/latest/source/openfl/components.html)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – 基于总监的工作流程架构（改编自 https://openfl.readthedocs.io/en/latest/source/openfl/components.html）'
- en: '](img/B18369_08_01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18369_08_01.jpg)'
- en: Figure 8.1 – Architecture of director-based workflow (adapted from https://openfl.readthedocs.io/en/latest/source/openfl/components.html)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 基于总监的工作流程架构（改编自 https://openfl.readthedocs.io/en/latest/source/openfl/components.html）
- en: '**Director Manager** oversees the running of experiments, working with long-lived
    **Envoy** components residing on the collaborator nodes to manage the short-lived
    components (collaborators + aggregator) for each experiment. In targeting the
    cross-silo data scenario, OpenFL applies a unique focus on managing data shards,
    including cases where data representations differ across silos.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**总监经理**负责实验的运行，与位于协作节点上的长期**信使**组件合作，管理每个实验的短期组件（协作者+聚合者）。在针对跨数据孤岛的场景时，OpenFL对管理数据分片给予了独特的关注，包括数据表示在不同孤岛中不同的情况。'
- en: IBM FL
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IBM FL
- en: IBM FL is a framework that also focuses on enterprise FL. It follows a straightforward
    aggregator-party design, where some number of parties with local data collaborate
    with other parties by sending incremental model training results to the aggregator
    and working with the produced aggregate models (following standard client-server
    FL architecture). IBM FL has official support for a number of fusion (aggregation)
    algorithms and certain fairness techniques aimed at combatting bias—the details
    of these algorithms can be found at the repository located at https://github.com/IBM/federated-learning-lib.
    One specific goal of IBM FL is to be highly extensible, allowing users to easily
    make necessary modifications if specific features are desired. It also supports
    a Jupyter-Notebook-based dashboard to aid in orchestrating FL experiments.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: IBM FL是一个也专注于企业联邦学习的框架。它遵循简单的聚合者-参与者设计，其中一些拥有本地数据的参与者通过向聚合者发送增量模型训练结果并与生成的聚合模型（遵循标准的客户端-服务器联邦学习架构）合作，与其他参与者协作。IBM
    FL对多种融合（聚合）算法和旨在对抗偏见的某些公平技术提供官方支持——这些算法的详细信息可以在位于https://github.com/IBM/federated-learning-lib的存储库中找到。IBM
    FL的一个具体目标是高度可扩展，使用户能够轻松地进行必要的修改，以满足特定的功能需求。它还支持基于Jupyter-Notebook的仪表板，以帮助协调联邦学习实验。
- en: STADLE
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: STADLE
- en: Unlike the previous frameworks, STADLE ([https://stadle.ai/](https://stadle.ai/))
    is an ML-framework-agnostic FL and distributed learning SaaS platform that aims
    to allow for the seamless integration of FL into production-ready applications
    and ML pipelines. The goal of STADLE is to minimize the amount of FL-specific
    code necessary for integration, making FL accessible to newcomers while still
    providing flexibility to those looking to experiment.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的框架不同，STADLE ([https://stadle.ai/](https://stadle.ai/)) 是一个与机器学习框架无关的联邦学习和分布式学习SaaS平台，旨在允许无缝地将联邦学习集成到生产就绪的应用程序和机器学习管道中。STADLE的目标是最大限度地减少集成所需的特定于联邦学习的代码量，使联邦学习对新手来说易于访问，同时仍然为那些想要进行实验的人提供灵活性。
- en: 'With the STADLE SaaS platform, users of varying technical abilities can collaborate
    on FL projects at all scales. Performance tracking and model management functionalities
    allow users to produce validated federated models with strong performance, while
    an intuitive configuration panel allows for detailed control over the federated
    learning process. STADLE uses a two-level component hierarchy that allows for
    multiple aggregators to operate in parallel, scaling to match demand. The following
    figure depicts the high-level architecture:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用STADLE SaaS平台，不同技术能力的用户可以在所有规模上协作进行联邦学习项目。性能跟踪和模型管理功能使用户能够生成具有强大性能的验证联邦模型，而直观的配置面板允许对联邦学习过程进行详细控制。STADLE使用两级组件层次结构，允许多个聚合器并行操作，以匹配需求。以下图展示了高级架构：
- en: '![Figure 8.2 – STADLE multi-aggregator architecture'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.2 – STADLE多聚合器架构'
- en: '](img/B18369_08_02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18369_08_02.jpg)'
- en: Figure 8.2 – STADLE multi-aggregator architecture
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – STADLE多聚合器架构
- en: Development of STADLE clients is streamlined with `pip` installation and an
    easy-to-understand configuration file, with several examples made publicly available
    for use as a reference on the different ways STADLE can be integrated into existing
    ML code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: STADLE客户端的开发通过`pip`安装和易于理解的配置文件简化，同时公开提供了一些示例，供用户参考STADLE如何集成到现有的机器学习代码中的不同方式。
- en: PySyft
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySyft
- en: While PySyft ([https://github.com/OpenMined/PySyft](https://github.com/OpenMined/PySyft))
    implementations are not included in this chapter due to ongoing changes in the
    codebase, it is still a major player in the privacy-preserving deep learning space.
    The core principle behind PySyft is to allow for the ability to perform computations
    over data stored on a machine without direct access to said data ever being given.
    This is accomplished by adding an intermediate layer between the user and the
    data location that sends computation requests to participating worker machines,
    returning the computed result to the user while maintaining the privacy of the
    data stored and used by each worker to perform the computation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于代码库的持续变化，PySyft ([https://github.com/OpenMined/PySyft](https://github.com/OpenMined/PySyft))
    的实现不包括在本章中，但它仍然是隐私保护深度学习空间中的主要参与者。PySyft背后的核心原则是允许在不对数据进行直接访问的情况下对存储在机器上的数据进行计算。这是通过在用户和数据位置之间添加一个中间层来实现的，该层向参与工作的机器发送计算请求，将计算结果返回给用户，同时保持每个工人存储和使用的用于执行计算的数据的隐私。
- en: This general capability directly extends itself to FL, reworking each step of
    a normal deep learning training flow to be a computation over the model parameters
    and data stored at each worker (agent) participating in FL. To accomplish this,
    PySyft utilizes hooks that encapsulate the standard PyTorch/TensorFlow libraries,
    modifying the requisite internal functions in order to allow model training and
    testing to be supported as PySyft privacy-preserving computations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通用能力直接扩展到FL，重新设计正常深度学习训练流程的每一步，使其成为对每个参与FL的工人（代理）存储的模型参数和数据的计算。为了实现这一点，PySyft利用钩子封装标准的PyTorch/TensorFlow库，修改必要的内部函数，以便支持模型训练和测试作为PySyft隐私保护计算。
- en: Now that the high-level ideas behind the FL frameworks have been explained,
    we move to the implementation-level details for their practical usage in two example
    scenarios. First, we look at how to modify the existing centralized training code
    for an NLP model to use FL.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经解释了FL框架背后的高级思想，我们将转向其实际应用中的实现级细节，以两个示例场景为例。首先，我们来看如何修改现有的用于NLP模型的集中式训练代码，使其能够使用FL。
- en: Example – the federated training of an NLP model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 - NLP模型的联邦训练
- en: The first ML problem that will be converted into an FL scenario through each
    of the aforementioned FL frameworks will be a classification problem within the
    domain of NLP. At a high level, NLP refers to the intersection of computational
    linguistics and ML with an overarching goal of allowing computers to achieve some
    level of *understanding* from human language – the details of this understanding
    vary widely based on the specific problem being targeted.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述每个FL框架将第一个ML问题转换为FL场景的将是NLP领域的分类问题。从高层次来看，NLP是指计算语言学和ML的交集，其总体目标是使计算机能够从人类语言中达到某种程度的*理解*
    - 这种理解的细节根据要解决的具体问题而大相径庭。
- en: For this example, we will be performing sentiment analysis on movie reviews,
    classifying them as positive or negative. The dataset we will be using is the
    SST-2 dataset (https://nlp.stanford.edu/sentiment/), containing movie reviews
    in a string format and the associated binary labels 0/1 representing negative
    and positive sentiment, respectively.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将对电影评论进行情感分析，将它们分类为正面或负面。我们将使用的数据集是SST-2数据集 (https://nlp.stanford.edu/sentiment/)，包含以字符串格式表示的电影评论和相关的二进制标签0/1，分别代表负面和正面情感。
- en: The model we will use to perform binary classification is a pretrained BERT
    model with a custom classification head. The BERT model allows us to encode a
    sentence into a high-dimensional numerical vector, which can then be passed to
    the classification head to output the binary label prediction; more information
    on the BERT model can be found at https://huggingface.co/blog/bert-101\. We choose
    to use a pretrained model that has already learned how to produce general encodings
    for sentences after a significant amount of training, as opposed to performing
    said training from scratch. This allows us to focus training on the classification
    head to fine-tune the model on the SST-2 dataset, saving time while maintaining
    performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: We will now go through the local (centralized) training code that will be used
    as a base when showing how to use each of the FL frameworks, starting with the
    Keras model definition and dataset loader.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Defining the sentiment analysis model
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `SSTModel` object defined in the `sst_model.py` file is the Keras model
    we will be using for this example.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the requisite libraries:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: TensorFlow Hub is used to easily download the pretrained BERT weights into a
    Keras layer. TensorFlow Text is used when loading in the BERT weights from TensorFlow
    Hub. TensorFlow Datasets will allow us to download and cache the SST-2 dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the model and initialize the model layer objects:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `preprocessor` object takes the raw sentence input batches and converts
    them into the format used by the BERT model. We load the preprocessor and BERT
    layers from TensorFlow Hub, then initialize the dense layers that make up the
    classification head. We use the sigmoid activation function at the end to squash
    the output into the interval (0,1), allowing for comparison with the true labels.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then define the forward pass of the model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We apply leaky ReLU to the BERT output to add non-linearity before passing the
    output to the classification head layers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Creating the data loader
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We also implement a function to load in the SST-2 dataset using the TensorFlow
    Datasets library. First, the training data is loaded and converted into a NumPy
    array for use during training:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We load the test data in a similar manner:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If `client_idx` and `num_clients` are specified, we return the respective partition
    of the training dataset – this will be used for performing FL:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we examine the code to perform local training, located in `local_training.py`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first import the requisite libraries:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can then use the previously defined dataset loader (without splitting) to
    load in the train and test splits:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can now compile the model and begin training:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we evaluate the model on the test split:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The model should reach around 82% test accuracy after three epochs of training.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have gone through the local training code, we can examine how the
    code can be modified to use FL with each of the aforementioned FL frameworks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Adopting an FL training approach
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用FL训练方法
- en: To demonstrate how FL can be applied to the SST model training scenario, we
    have to first split the original SST-2 dataset into disjoint subsets representing
    the local datasets in an FL application. To keep things simple, we will examine
    the case of three agents each training on separate thirds of the dataset.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示如何将FL应用于SST模型训练场景，我们首先需要将原始SST-2数据集拆分成不相交的子集，这些子集代表FL应用中的本地数据集。为了简化问题，我们将研究三个代理各自在数据集的不同三分之一上训练的情况。
- en: For now, these subsets are randomly sampled without replacement from the dataset
    – in the next section, *Federated training of an image classification model on
    non-IID data*, we examine the case where the local datasets are created from a
    biased sampling of the original dataset. Instead of locally training for three
    epochs, we will perform three rounds of FL with each local training phase training
    for one epoch on the local data. FedAvg will be used to aggregate the locally
    trained models at the end of each round. After these three rounds, the aforementioned
    validation metrics will be computed using the final aggregate model, allowing
    for comparisons to be drawn between the local training cases and the FL case.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这些子集是从数据集中随机采样而不重复的 – 在下一节“在非-IID数据上对图像分类模型进行联邦训练”中，我们将研究本地数据集是从原始数据集的有偏采样中创建的情况。我们不会在本地训练三个epoch，而是将进行三轮FL，每轮本地训练阶段在本地数据上训练一个epoch。FedAvg将在每一轮结束时用于聚合本地训练的模型。在这三轮之后，将使用最终的聚合模型计算上述验证指标，从而允许比较本地训练案例和FL案例。
- en: Integrating TensorFlow Federated for SST-2
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成TensorFlow Federated用于SST-2
- en: As previously mentioned, the **TensorFlow Federated** (**TFF**) framework was
    built on top of the TensorFlow and Keras deep learning libraries. The model implementation
    was done using Keras; as a result, the integration of TFF into the local training
    code is relatively straightforward.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，**TensorFlow Federated**（**TFF**）框架是在TensorFlow和Keras深度学习库之上构建的。模型实现是使用Keras完成的；因此，将TFF集成到本地训练代码中相对简单。
- en: 'The first step is to add the TFF-specific imports and FL-specific parameters
    prior to loading the dataset:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是在加载数据集之前添加TFF特定的导入和FL特定的参数：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'TFF allows us to simulate some number of agents by passing the appropriate
    number of datasets (local datasets) to the FL process. To split the SST-2 dataset
    into thirds after preprocessing, we can use the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: TFF允许我们通过向FL过程传递适当数量的数据集（本地数据集）来模拟一定数量的代理。为了在预处理后将SST-2数据集分成三份，我们可以使用以下代码：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we have to wrap the Keras model using a TFF API function to easily create
    the respective `tff.learning.Model` object. We create a function that initializes
    the SST model and passes it along with the input spec (information on the size
    of each data element) to this API function, returning the result – TFF will use
    this function internally to create the model during the FL process:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须使用TFF API函数包装Keras模型，以便轻松创建相应的`tff.learning.Model`对象。我们创建一个函数，初始化SST模型，并将其与输入规范（关于每个数据元素大小的信息）一起传递给这个API函数，返回结果
    – TFF将在FL过程中内部使用此函数来创建模型：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The TFF FedAvg process can then be created, using the `sst_model_fn` function
    along with the optimizers used to update the local models and the aggregate model.
    Using a learning rate of 1.0 for the server optimizer function allows for the
    new aggregate model to replace the old one at the end of each round (as opposed
    to computing a weighted average of the old and new models):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sst_model_fn`函数以及用于更新本地模型和聚合模型的优化器，可以创建TFF FedAvg过程。对于服务器优化器函数使用1.0的学习率，允许在每一轮结束时用新的聚合模型替换旧的模型（而不是计算旧模型和新模型的加权平均值）：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we initialize and run the federated learning process for 10 rounds.
    Each `fed_avg_process.next()` call simulates one round by performing local training
    with three models on the client datasets followed by aggregation using FedAvg.
    The resulting state after the first round is passed to the next call as the starting
    FL state for the round:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们初始化并运行联邦学习过程10轮。每次`fed_avg_process.next()`调用通过在客户端数据集上使用三个模型进行本地训练，然后使用FedAvg进行聚合来模拟一轮。第一轮后的状态被传递到下一次调用，作为该轮的起始FL状态：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After the FL process is completed, we convert the final aggregate `tff.learning.Model`
    object back into the original Keras model format in order to compute the validation
    metrics:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The final accuracy of the aggregate model should be around 82%.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: From this, it should be clear that the TFF FedAvg results are nearly identical
    to those of the local training scenario.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Integrating OpenFL for SST-2
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that OpenFL supports two different workflows: the aggregator-based workflow
    and the director-based workflow. This example will use the director-based workflow,
    involving long-living components that can conduct FL task requests as they come
    in. This was chosen due to the desirability of having a persistent FL setup for
    deploying multiple projects; however, both workflows conduct the same core FL
    process and thus demonstrate similar performance.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'To help with model serialization in this case, we only aggregate the classification
    head weights, reconstructing the full model at runtime during training and validation
    (TensorFlow Hub caches the downloaded layers, so the download process only occurs
    once). We include the following functions in `sst_model.py` to aid with this modification:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Because OpenFL focuses on addressing the data silo case, the creation of the
    local datasets from the SST-2 data is slightly more involved than the TFF case.
    The objects needed to create the dataset will be implemented in a separate file
    named `sst_fl_dataset.py`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we include the necessary imports. The two OpenFL-specific objects we
    import are the `ShardDescriptor` object, which handles the dataset loading and
    sharding, and the `DataInterface` object, which handles access to the datasets:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Implementing ShardDescriptor
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We first implement the `SSTShardDescriptor` class. When this shard descriptor
    is created, we save the `rank` (client number) and `worldsize` (total number of
    clients) values, then load the training and validation datasets:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We implement the `ShardDescriptor` class functions to get the available dataset
    types (training and validation in this case) and the respective dataset/shard
    based on the rank of the client:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We also specify the properties of the specific dataset being used. Note that
    the sample shape is set to `1`. The preprocessor layer of the `SSTModel` allows
    us to pass in strings as input, which are treated as input vectors of type `tf.string`
    and length `1`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With this, the `SSTShardDescriptor` implementation is completed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Implementing DataInterface
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we implement the `SSTFedDataset` class as a subclass of `DataInterface`.
    This is done by implementing the shard descriptor getter and setter methods, with
    the setter method preparing the data to be provided to the training/validation
    FL tasks:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We also implement the API functions to grant dataset access and dataset size
    information (used during aggregation):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: With this, the local SST-2 datasets can be constructed and used.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Creating FLExperiment
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now focus on the actual implementation of the FL process within a new file,
    `fl_sim.py`. First, we import the necessary libraries – from OpenFL, we import
    the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '`TaskInterface`: Allows us to define our FL training and validation tasks for
    the model; the registered tasks are what the director instructs each envoy to
    conduct'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ModelInterface`: Allows us to convert our Keras model into the format used
    by OpenFL in the registered tasks'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Federation`: Manages information relating to the connection with the director'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FLExperiment`: Uses the `TaskInterface`, `ModelInterface`, and `Federation`
    objects to conduct the FL process'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The requisite imports are done as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we create the `Federation` object using the default `director` connection
    information:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then initialize the model with the associated optimizer and loss function
    – these objects are used by the OpenFL `KerasAdapter` to create the `ModelInterface`
    object. We call the model on a dummy Keras input in order to initialize all of
    the weights before passing the model to `ModelInterface`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we create a `TaskInterface` object and use it to register the training
    task. Note that including the optimizer in the decorator function of a task will
    result in the training dataset being passed to the task; otherwise, the validation
    dataset will be passed to the task:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Similarly, we register the validation task using the `TaskInterface` object.
    Note that we can collect the metrics generated by the `evaluate` function and
    return the values as a means of tracking performance:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can now load in the dataset using the `SSTFedDataset` class implemented
    earlier and create and start a new `FLExperiment` using the created `ModelInterface`,
    `TaskInterface`, and `SSTFedDatasets` objects:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Defining the configuration files
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last step is to create the configuration files used by `director` and `envoys`
    in order to actually load the data and start the FL process. First, we create
    `director_config` containing the following information:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This is saved in `director/director_config.yaml`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create the three `envoy` configuration files. The first file (`envoy_config_1.yaml`)
    contains the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The second and third `envoy` config files are the same, except with the values
    `rank_worldsize: 2, 3` and `rank_worldsize: 3, 3`, respectively. These config
    files, alongside all of the code files, are stored in the experiment directory.
    The directory structure should look like the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '`director`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`director_config.yaml`'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment`'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_1.yaml`'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_2.yaml`'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_3.yaml`'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sst_fl_dataset.py`'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sst_model.py`'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fl_sim.py (file with` `FLExperiment creation)`'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With everything set up, we can now perform FL with OpenFL.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Running the OpenFL example
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, start the director by running the following command from within the
    `director` folder (make sure OpenFL is installed in the working environment):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, run the following commands in separate terminals from the experiment
    directory:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Finally, start `FLExperiment` by running the `fl_sim.py` script. After the three
    rounds are completed, the aggregate model should achieve a validation accuracy
    of around 82%. Once again, the performance is nearly identical to the local training
    scenario.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Integrating IBM FL for SST-2
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'IBM FL uses a saved version of the model when performing FL. The following
    code (`create_saved_model.py`) initializes a model (calling the model on a dummy
    input to initialize the parameters) and then saves the model in the Keras `SavedModel`
    format for IBM FL to use:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Run this once to save the model into the folder named `sst_model_save_dir` –
    we will point IBM FL to load in the model saved in this directory.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Creating DataHandler
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we create a subclass of the IBM FL `DataHandler` class in charge of providing
    the training and validation data to the model – this subclass will load, preprocess,
    and store the SST datasets as class attributes. We first import the necessary
    libraries:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `init` function of this class loads the data info parameters, which are
    then used to load the correct SST-2 data partition:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We also implement the API function that returns the loaded datasets for use
    during training/validation:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Defining the configuration files
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next step is to create the configuration JSON files used when starting
    the aggregator and initializing the parties. The aggregation config first specifies
    the connection information it will use to communicate with the parties:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, we specify the fusion handler used for aggregation:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We also specify the hyperparameters related to both local training and aggregation.
    `perc_quorum` refers to the percentage of parties that must participate before
    aggregation can begin:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, we specify the IBM FL protocol handler to use:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This configuration is saved in `agg_config.json`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'We also create the base party configuration file used to conduct FL with the
    local data. We first specify the connection information of the aggregator and
    the party:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We then specify the data handler and the local training handler to use – this
    component trains the SST model using the model information and the local data:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The model format and information is then specified – this is where we point
    to the saved model created earlier:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Finally, we specify the protocol handler:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Creating IBM FL party
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With this, all that is left is the code that starts each party, saved in `fl_sim.py`.
    We first import the necessary libraries:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We include an `argparse` argument that allows for the party number to be specified
    – this is used to modify the base party configuration file in order to allow for
    distinct parties to be started from the same file:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Finally, we create and start a new `Party` object with the modified configuration
    information:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: With this, we can now begin performing FL using IBM FL.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Running the IBM FL example
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, start `aggregator` by running the following command:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'After the aggregator is finished setting up, type `START` and press *Enter*
    key to open the aggregator to receive incoming connections. You can then start
    three parties using the following commands in separate terminals:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Finally, type `TRAIN` into the aggregator window and press *Enter* key to begin
    the FL process. When three rounds are completed, you can type `SAVE` into the
    same window to save the latest aggregate model.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Flower for SST-2
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two main Flower components that must be incorporated on top of the existing
    local training code are the client and strategy subclass implementations. The
    client subclass implementation allows us to interface with Flower, with API functions
    that allow for model parameters to be passed between the clients and the server.
    The strategy subclass implementation allows us to specify the details of the aggregation
    approach performed by the server.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by writing the code to implement and start a client (stored in `fl_sim.py`).
    First, the necessary libraries are imported:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We add a command-line argument specifying the client ID in order to allow for
    the same client script to be reused for all three agents:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We then load in the SST-2 datasets:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Note that we use the client ID to get the respective shard from the training
    dataset.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the model and the associated optimizer and loss objects, making
    sure to call the model on a dummy input to initialize the weights:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Implementing the Flower client
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can now implement the Flower client object that will pass model parameters
    to and from the server. To implement a client subclass, we have to define three
    functions:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '`get_parameters(self, config)`: Returns the model parameter values'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fit(self, parameters, config)`: Sets the weights of the local model to the
    received parameters, performs local training, and returns the new model parameters
    alongside the dataset size and training metrics'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`evaluate(self, parameters, config)`: Sets the weights of the local model to
    the received parameters, then evaluates the model on validation/test data and
    returns the performance metrics'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using `fl.client.NumPyClient` as the superclass allows us to take advantage
    of the Keras model `get_weights` and `set_weights` functions that convert the
    model parameters into lists of NumPy arrays:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The `evaluate` function is also defined:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'With this client implementation, we can finally start the client using the
    default connection information with the following line:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Creating the Flower server
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The final step before running Flower is to create the script (`server.py`)
    that will start the Flower server. We begin with the necessary imports and the
    `MAX_ROUNDS` parameter:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Because we want to save the model after performing federated learning, we create
    a subclass of the flower FedAvg strategy and add a final step that saves the model
    at the last round during the aggregation phase:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'With this strategy, we can run the following line to start the server (passing
    the `MAX_ROUNDS` parameter through the `config` argument):'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We can now start the server and clients, allowing for FL to be performed using
    Flower.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Running the Flower example
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To start the server, first run the `server.py` script.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the three clients can then be started by running the following commands
    in separate terminal windows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The final aggregate model after FL will be saved in the `final_agg_sst_model`
    directory as a `SavedModel` object.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Integrating STADLE for SST-2
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: STADLE differs from the previously examined FL frameworks by providing a cloud-based
    platform (STADLE Ops) to handle the deployment of aggregators and management of
    the FL process. Because the deployment of the server side can be done through
    the platform, the client-side implementation is all that needs to be implemented
    for performing FL with STADLE. This integration is done by creating a client object
    that occasionally sends the local model and returns the aggregate model from the
    previous round. To do this, we need to create the agent configuration file and
    modify the local training code to interface with STADLE.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the configuration file for the agent as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Information on these parameters can be found at https://stadle-documentation.readthedocs.io/en/latest/documentation.html#configuration-of-agent.
    Note that the aggregator IP and registration port values listed here are placeholders
    and will be modified when connecting to the STADLE Ops platform.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we modify the local training code to work with STADLE. We first import
    the requisite libraries:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Once again, we add a command-line argument to specify which partition of the
    training data the agent should receive:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Next, we instantiate a `BasicClient` object – this is the STADLE client component
    that handles communication between the local training process and the aggregators
    on the server side. We use the configuration file defined earlier to create this
    client:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Finally, we implement the FL training loop. In each round, the client gets
    the aggregate model from the previous round (starting with the base model) and
    trains it further on the local data before sending it back to the aggregator through
    the client:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The `wait_for_sg_model` function returns the latest aggregate model from the
    server, and the `send_trained_model` function sends the locally trained model
    with the desired performance metrics to the server. More information on these
    integration steps can be found at https://stadle-documentation.readthedocs.io/en/latest/usage.html#client-side-stadle-integration.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Now that the client side has been implemented, we can use the STADLE Ops platform
    to start an aggregator and start an FL process.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Creating a STADLE Ops project
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, go to stadle.ai and create a new account. Once you are logged in, you
    should be directed to the project information page in STADLE Ops:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Project information page in STADLE Ops'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_08_03.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Project information page in STADLE Ops
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Create New Project**, then fill in the project information and click
    **Create Project**. The project information page should have changed to show the
    following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – New project added to the project information page'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_08_04.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – New project added to the project information page
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the plus icon under **Initiate Aggregator** to start a new aggregator
    for the project, then click **OK** on the confirmation prompt. You can now navigate
    to the **Dashboard** page on the left side, resulting in a page that looks like
    the following:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Dashboard page of STADLE Ops'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_08_05.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Dashboard page of STADLE Ops
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Replace the `aggr_ip` and `reg_port` placeholder parameter values in the `config_agent.json`
    file with the values under **IP Address to Connect** and **Port to** **Connect**,
    respectively.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: With this, we are now ready to begin the FL training process.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Running the STADLE example
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step is to send the base model object to the server, allowing it
    to in turn distribute the model to the training agents. This is done with the
    following command:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Once the command successfully runs, the **Base Model Info** section on the
    STADLE Ops dashboard should update to show the model information. We can now start
    the three agents by running the following commands:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: After three rounds, the agents will terminate and the final aggregate model
    will be displayed in the project dashboard, available for download in the Keras
    SavedModel format. The user guide located at [https://stadle.ai/user_guide/guide](https://stadle.ai/user_guide/guide)
    is recommended for more information on the various functionalities of the STADLE
    Ops platform.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the resulting aggregate models produced by each FL framework results
    in the same conclusion—the performance of the aggregate model essentially matches
    that of the centralized training model. As explained in the *Dataset distributions*
    section of [*Chapter 7*](B18369_07.xhtml#_idTextAnchor176), *Model Aggregation*,
    this is generally the expected result. The natural question to ask is how the
    performance is affected when the local datasets are not IID—this is the focal
    point of the next section.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Example – the federated training of an image classification model on non-IID
    data
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous example, we examined how a centralized deep learning problem
    could be converted into an FL analog by training multiple clients on disjoint
    subsets of the original training dataset (the *local datasets*) in an FL process.
    One key point of this local dataset creation was that the subsets were created
    by random sampling, leading to local datasets that were all IID under the same
    distribution as the original dataset. As a result, the similar performance of
    FedAvg compared to the local training scenario was expected – each client’s model
    essentially had the same set of local minima to move toward during training, making
    all local training beneficial for the global objective.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in [*Chapter 7*](B18369_07.xhtml#_idTextAnchor176), *Model Aggregation*,
    we explored how FedAvg was susceptible to the divergence in training objectives
    induced by severely non-IID local datasets. To explore the performance of FedAvg
    on varying non-IID severities, this example trains the VGG-16 model (a simple
    deep-learning-based image classification model) on constructed non-IID local datasets
    sampled from the CIFAR-10 dataset (located at [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)).
    CIFAR-10 is a well-known simple image classification dataset containing 60,000
    images separated into 10 different classes; the goal of models trained on CIFAR-10
    is to correctly predict the class associated with an input image. The relatively
    low complexity and ubiquity as a benchmark dataset make CIFAR-10 ideal for exploring
    the response of FedAvg to non-IID data.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: To avoid including redundant code samples, this section focuses on the key lines
    of code that allow FL to be performed on PyTorch models using non-IID local datasets.
    It is recommended that you go through the examples within the *Example – the federated
    training of an NLP model* section in this chapter prior to reading this section
    in order to understand the core components needed for each FL framework. The implementations
    for this example can be found in full at this book’s GitHub repository ([https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)
    tree/main/ch8/cv_code), for use as a reference.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: The key point of this example is determining how the non-IID datasets should
    be constructed. We will change the class label distributions of each local dataset
    by changing the number of images of each class included in the training dataset. For
    example, a dataset skewed toward cars and birds might have 5,000 images of cars,
    5,000 images of birds, and 500 images for every other class. By creating three
    disjointed subsets of the 10 classes and constructing local datasets skewed toward
    these classes, we produce three local datasets with non-IID severity proportional
    to the number of images included from the classes not selected.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Skewing the CIFAR-10 dataset
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first map the three class subsets to client IDs, and set the proportion
    of images to be taken from the original dataset for selected classes (`sel_count`)
    and the other classes (`del_count`):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We then sample the appropriate number of images from the original dataset,
    using the indices of the images in the dataset to construct the skewed CIFAR-10
    subset:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The skewed trainset is then used to create the skewed `trainloader` for local
    training. When we refer to biasing the training data going forward, this is the
    code that is run.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: We will now demonstrate how to use different FL frameworks to run this non-IID
    FL process. Please refer to the installation instructions and framework-specific
    implementations in the previous section, *Example – the federated training of
    an NLP model*, for the explanations of the basics omitted in this section.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Integrating OpenFL for CIFAR-10
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the Keras NLP example, we first create the `ShardDescriptor` and
    `DataInterface` subclasses for the non-IID CIFAR-10 datasets in `cifar_fl_dataset.py`.
    Only a few changes need to be made in order to accommodate the new dataset.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we modify the `self.data_by_type` dictionary to instead store the modified
    CIFAR datasets:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The `load_cifar_data` function loads in the training and test data using `torchvision`,
    then biases the training data based on the rank passed to the object.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the dimensions of a data element are now known (the size of CIFAR-10
    image), we also modify the shape properties with fixed values:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We then implement the `CifarFedDataset` subclass of the `DataInterface` class.
    No significant modifications are needed for this implementation; thus, we can
    now use the biased CIFAR-10 dataset with OpenFL.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'We now move to the actual FL process implementation (`fl_sim.py`). One key
    difference is the framework adapter that must be used to create the `ModelInterface`
    object from a PyTorch model:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The only other major change is modifying the train and validation functions
    passed to the `TaskInterface` object to mirror the PyTorch implementations of
    these functions from the local training code.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to create the configuration files used by the director and
    envoys. The only necessary change in the director config is the updated `sample_shape`
    and `target_shape` for the CIFAR-10 data:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: This is saved in `director/director_config.yaml`.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'The envoy configuration files require no changes outside of updating the object
    and filenames – the directory structure should look like the following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '`director`'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`director_config.yaml`'
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment`'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_1.yaml`'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_2.yaml`'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_3.yaml`'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cifar_fl_dataset.py`'
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fl_sim.py`'
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can refer to *Running the OpenFL example* in the *Integrating OpenFL for
    SST-2* section to run this example.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Integrating IBM FL for CIFAR-10
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that IBM FL requires a saved version of the model used during training.
    We first run the following code in `create_saved_model.py` to create the saved
    VGG-16 PyTorch model:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Next, we create the `DataHandler` subclass for the skewed CIFAR-10 datasets.
    The only core change is the modification of the `load_and_preprocess_data` function
    to instead load in the CIFAR-10 data and bias the training set.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create the configuration JSON files used when starting
    the aggregator and initializing the parties. No significant changes to the aggregator
    config (`agg_config.json`) are necessary, and the only core change in the party
    config is the modification of the model information to work with PyTorch:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The code in `fl_sim.py` responsible for starting up the parties can essentially
    remain unmodified due to the extensive use of the configuration files.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to *Running the IBM FL example* in the *Integrating IBM FL for
    SST-2* section to run this example.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Flower for CIFAR-10
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After loading in the CIFAR-10 data and biasing the training data, the core
    change needed for the Flower implementation is the `NumPyClient` subclass. Unlike
    the Keras example, the `get_parameters` and `set_parameters` methods rely on the
    PyTorch model state dictionaries and are a bit more involved:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: We modify the `fit` function to mirror the training code in the local training
    example and modify the evaluate function to similarly mirror the local training
    evaluation code. Note that we call `self.set_parameters(parameters)` in order
    to update the local model instance with the most recent weights.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'We also set the `grpc_max_message_length` parameter to 1 GB when starting the
    Flower client and server to accommodate the larger VGG16 model size. The client
    initialization function is now the following:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Finally, we modify the aggregator code in `server.py` – the custom strategy
    we used previously to save the aggregate model at the end of the last round needs
    to be modified to work with PyTorch models:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'With this strategy, we can run the following line to start the server (adding
    the `grpc_max_message_length` parameter here as well):'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Refer to *Running the Flower example* in the *Integrating Flower for SST-2*
    section to run this example.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Integrating STADLE for CIFAR-10
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first modify the `config_agent.json` config file to use the VGG16 model
    from the `torchvision` library:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'To integrate STADLE into the local training code, we initialize the `BasicClient`
    object and modify the training loop to send the local model every two local training
    epochs and wait for the new aggregate model:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Note
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: The code located at [https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)
    contains the full implementation of this integration example for reference. To
    start an aggregator and perform FL with the CIFAR-10 STADLE example, please refer
    to *Creating a STADLE Ops project* and *Running the STADLE example* in the *Integrating
    STADLE for* *SST-2* subsection.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Testing different levels of bias in the constructed local datasets should lead
    to the same conclusion stated in the *Dataset distributions* section of [*Chapter
    7*](B18369_07.xhtml#_idTextAnchor176), *Model Aggregation* for non-IID cases—as
    the non-IID severity increases, the convergence speed and model performance decrease.
    The goal of this section was to build off of the understanding of each FL framework
    from the SST-2 example, highlighting the key changes necessary to work with a
    PyTorch model on a modified dataset. Using this section alongside the code examples
    in [https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)
    should help in understanding this example integration.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered several FL frameworks through the context of two
    different examples. From the first example, you learned how a traditional centralized
    ML problem can be converted into the analogous FL scenario by separating the data
    into disjointed subsets. It is now clear that random sampling leads to local datasets
    that are IID, allowing FedAvg to reach the same level of performance as the centralized
    equivalent with any of the FL frameworks.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: In the second example, you learned one of the many ways a group of datasets
    can be non-IID (different class label distributions) and observed how different
    severities of non-IID datasets affect the performance of FedAvg. We encourage
    you to explore how alternative aggregation methods can improve on FedAvg in these
    cases.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Both examples also should have given you a solid understanding of the general
    trends when working with different FL frameworks; while the specific implementation-level
    details may change (due to the rapidly changing field), the core concepts and
    implementation details will remain fundamentals.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we continue our transition to the business application
    side of FL by taking a look at several case studies involving the application
    of FL to specific domains.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
