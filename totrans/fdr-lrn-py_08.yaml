- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Existing Federated Learning Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of this chapter is to introduce existing **federated learning**
    (**FL**) frameworks and platforms, applying each to federated learning scenarios
    involving toy **machine learning** (**ML**) problems. The platforms focused on
    in this chapter are Flower, TensorFlow Federated, OpenFL, IBM FL, and STADLE –
    the idea behind this selection was to help you by covering a breadth of existing
    FL platforms.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have a basic understanding of how to
    use each platform for FL, and you should be able to choose a platform based on
    its associated strengths and weaknesses for an FL application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to existing FL frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementations of an example NLP FL task on movie review dataset, using existing
    frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementations of example computer vision FL task with non-IID datasets, using
    existing frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the supplemental code files for this chapter in the book’s GitHub
    repository:'
  prefs: []
  type: TYPE_NORMAL
- en: https://github.com/PacktPublishing/Federated-Learning-with-Python
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You can use the code files for personal or educational purposes. Please note
    that we will not support deployments for commercial use and will not be responsible
    for any errors, issues, or damage caused by using the code.
  prefs: []
  type: TYPE_NORMAL
- en: Each implementation example in this chapter was run on an x64 machine running
    Ubuntu 20.04.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the training code for the NLP example requires the following
    libraries to run:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 (version ≥ 3.8)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (version ≥ 2.9.1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Hub (`pip` `install tensorflow-hub`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Datasets (`pip` `install tensorflow-datasets`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Text (`pip` `install tensorflow-text`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a GPU with the appropriate TensorFlow installation is recommended to save
    training time for the NLP example, due to the size of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the training code for the **non-IID** (**non-independent
    and identical distribution**) computer vision example requires the following libraries
    to run:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 (version ≥ 3.8)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch (version ≥ 1.9)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torchvision (version ≥ 0.10.0, tied to PyTorch version)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The installation instructions for each FL framework are listed in the following
    subsections.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Federated
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can install the following libraries to use TFF:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tensorflow_federated` (using the `pip install` `tensorflow_federated` command)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nest_asyncio` (using the `pip install` `nest_asyncio` command)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenFL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can install OpenFL using `pip` `install openfl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can build from source with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: IBM FL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Installing the locally hosted version of IBM FL requires the wheel installation
    file located in the code repository. To perform this installation, run the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Flower
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can install Flower using the `pip install` `flwr` command.
  prefs: []
  type: TYPE_NORMAL
- en: STADLE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can install the STADLE client-side library using the `pip install` `stadle-client`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to FL frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we introduce the FL frameworks and platforms to be used in the subsequent
    implementation-focused sections.
  prefs: []
  type: TYPE_NORMAL
- en: Flower
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flower ([https://flower.dev/](https://flower.dev/)) is an open source and ML
    framework-agnostic FL framework that aims to be accessible to users. Flower follows
    a standard client-server architecture, in which the clients are set up to receive
    the model parameters from the server, train on local data, and send the new local
    model parameters back to the server.
  prefs: []
  type: TYPE_NORMAL
- en: The high-level orchestration of the federated learning process is dictated by
    what Flower calls strategies, used by the server for aspects such as client selection
    and parameter aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: Flower uses **Remote Procedure Calls** (**RPCs**) in order to perform said orchestration
    through client-side execution from messages sent by the server. The extensibility
    of the framework allows researchers to experiment with novel approaches such as
    new aggregation algorithms and communication methods (such as model compression).
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Federated (TFF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TFF ([https://www.tensorflow.org/federated](https://www.tensorflow.org/federated))
    is an open source FL/computation framework built on top of TensorFlow that aims
    to allow researchers to easily simulate federated learning with existing TensorFlow/Keras
    models and training pipelines. It consists of the Federated Core layer, which
    allows for the implementation of general federated computations, and the Federated
    Learning layer, which is built on top and provides interfaces for FL-specific
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: TFF focuses on single-machine local simulations of FL, using wrappers to create
    TFF-specific datasets, models, and federated computations (core client and server
    computation performed during the FL process) from the standard TensorFlow equivalents.
    The focus on building everything from general federated computations allows researchers
    to implement each step as desired, allowing experimentation to be supported.
  prefs: []
  type: TYPE_NORMAL
- en: OpenFL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenFL ([https://github.com/intel/openfl](https://github.com/intel/openfl))
    is an open source FL framework developed by Intel, focused on allowing cross-silo
    privacy-preserving ML to be performed. OpenFL allows for two different workflows
    depending on the desired lifespan of the federation (where federation refers to
    the entire FL system).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the aggregator-based workflow, a single experiment and associated federated
    learning plan are sent from the aggregator to the participating *collaborators*
    (agents) to be run as the local training step of the FL process—the federation
    is stopped after the experiment is complete. In the director-based workflow, long-lived
    components are instead used to allow for experiments to be run on demand. The
    following diagram depicts the architecture and users for the director-based workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Architecture of director-based workflow (adapted from https://openfl.readthedocs.io/en/latest/source/openfl/components.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_08_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Architecture of director-based workflow (adapted from https://openfl.readthedocs.io/en/latest/source/openfl/components.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**Director Manager** oversees the running of experiments, working with long-lived
    **Envoy** components residing on the collaborator nodes to manage the short-lived
    components (collaborators + aggregator) for each experiment. In targeting the
    cross-silo data scenario, OpenFL applies a unique focus on managing data shards,
    including cases where data representations differ across silos.'
  prefs: []
  type: TYPE_NORMAL
- en: IBM FL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IBM FL is a framework that also focuses on enterprise FL. It follows a straightforward
    aggregator-party design, where some number of parties with local data collaborate
    with other parties by sending incremental model training results to the aggregator
    and working with the produced aggregate models (following standard client-server
    FL architecture). IBM FL has official support for a number of fusion (aggregation)
    algorithms and certain fairness techniques aimed at combatting bias—the details
    of these algorithms can be found at the repository located at https://github.com/IBM/federated-learning-lib.
    One specific goal of IBM FL is to be highly extensible, allowing users to easily
    make necessary modifications if specific features are desired. It also supports
    a Jupyter-Notebook-based dashboard to aid in orchestrating FL experiments.
  prefs: []
  type: TYPE_NORMAL
- en: STADLE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the previous frameworks, STADLE ([https://stadle.ai/](https://stadle.ai/))
    is an ML-framework-agnostic FL and distributed learning SaaS platform that aims
    to allow for the seamless integration of FL into production-ready applications
    and ML pipelines. The goal of STADLE is to minimize the amount of FL-specific
    code necessary for integration, making FL accessible to newcomers while still
    providing flexibility to those looking to experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the STADLE SaaS platform, users of varying technical abilities can collaborate
    on FL projects at all scales. Performance tracking and model management functionalities
    allow users to produce validated federated models with strong performance, while
    an intuitive configuration panel allows for detailed control over the federated
    learning process. STADLE uses a two-level component hierarchy that allows for
    multiple aggregators to operate in parallel, scaling to match demand. The following
    figure depicts the high-level architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – STADLE multi-aggregator architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_08_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – STADLE multi-aggregator architecture
  prefs: []
  type: TYPE_NORMAL
- en: Development of STADLE clients is streamlined with `pip` installation and an
    easy-to-understand configuration file, with several examples made publicly available
    for use as a reference on the different ways STADLE can be integrated into existing
    ML code.
  prefs: []
  type: TYPE_NORMAL
- en: PySyft
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While PySyft ([https://github.com/OpenMined/PySyft](https://github.com/OpenMined/PySyft))
    implementations are not included in this chapter due to ongoing changes in the
    codebase, it is still a major player in the privacy-preserving deep learning space.
    The core principle behind PySyft is to allow for the ability to perform computations
    over data stored on a machine without direct access to said data ever being given.
    This is accomplished by adding an intermediate layer between the user and the
    data location that sends computation requests to participating worker machines,
    returning the computed result to the user while maintaining the privacy of the
    data stored and used by each worker to perform the computation.
  prefs: []
  type: TYPE_NORMAL
- en: This general capability directly extends itself to FL, reworking each step of
    a normal deep learning training flow to be a computation over the model parameters
    and data stored at each worker (agent) participating in FL. To accomplish this,
    PySyft utilizes hooks that encapsulate the standard PyTorch/TensorFlow libraries,
    modifying the requisite internal functions in order to allow model training and
    testing to be supported as PySyft privacy-preserving computations.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the high-level ideas behind the FL frameworks have been explained,
    we move to the implementation-level details for their practical usage in two example
    scenarios. First, we look at how to modify the existing centralized training code
    for an NLP model to use FL.
  prefs: []
  type: TYPE_NORMAL
- en: Example – the federated training of an NLP model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first ML problem that will be converted into an FL scenario through each
    of the aforementioned FL frameworks will be a classification problem within the
    domain of NLP. At a high level, NLP refers to the intersection of computational
    linguistics and ML with an overarching goal of allowing computers to achieve some
    level of *understanding* from human language – the details of this understanding
    vary widely based on the specific problem being targeted.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will be performing sentiment analysis on movie reviews,
    classifying them as positive or negative. The dataset we will be using is the
    SST-2 dataset (https://nlp.stanford.edu/sentiment/), containing movie reviews
    in a string format and the associated binary labels 0/1 representing negative
    and positive sentiment, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The model we will use to perform binary classification is a pretrained BERT
    model with a custom classification head. The BERT model allows us to encode a
    sentence into a high-dimensional numerical vector, which can then be passed to
    the classification head to output the binary label prediction; more information
    on the BERT model can be found at https://huggingface.co/blog/bert-101\. We choose
    to use a pretrained model that has already learned how to produce general encodings
    for sentences after a significant amount of training, as opposed to performing
    said training from scratch. This allows us to focus training on the classification
    head to fine-tune the model on the SST-2 dataset, saving time while maintaining
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: We will now go through the local (centralized) training code that will be used
    as a base when showing how to use each of the FL frameworks, starting with the
    Keras model definition and dataset loader.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the sentiment analysis model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `SSTModel` object defined in the `sst_model.py` file is the Keras model
    we will be using for this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the requisite libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow Hub is used to easily download the pretrained BERT weights into a
    Keras layer. TensorFlow Text is used when loading in the BERT weights from TensorFlow
    Hub. TensorFlow Datasets will allow us to download and cache the SST-2 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the model and initialize the model layer objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `preprocessor` object takes the raw sentence input batches and converts
    them into the format used by the BERT model. We load the preprocessor and BERT
    layers from TensorFlow Hub, then initialize the dense layers that make up the
    classification head. We use the sigmoid activation function at the end to squash
    the output into the interval (0,1), allowing for comparison with the true labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then define the forward pass of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We apply leaky ReLU to the BERT output to add non-linearity before passing the
    output to the classification head layers.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the data loader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We also implement a function to load in the SST-2 dataset using the TensorFlow
    Datasets library. First, the training data is loaded and converted into a NumPy
    array for use during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We load the test data in a similar manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If `client_idx` and `num_clients` are specified, we return the respective partition
    of the training dataset – this will be used for performing FL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, we examine the code to perform local training, located in `local_training.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first import the requisite libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use the previously defined dataset loader (without splitting) to
    load in the train and test splits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now compile the model and begin training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we evaluate the model on the test split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The model should reach around 82% test accuracy after three epochs of training.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have gone through the local training code, we can examine how the
    code can be modified to use FL with each of the aforementioned FL frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting an FL training approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To demonstrate how FL can be applied to the SST model training scenario, we
    have to first split the original SST-2 dataset into disjoint subsets representing
    the local datasets in an FL application. To keep things simple, we will examine
    the case of three agents each training on separate thirds of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For now, these subsets are randomly sampled without replacement from the dataset
    – in the next section, *Federated training of an image classification model on
    non-IID data*, we examine the case where the local datasets are created from a
    biased sampling of the original dataset. Instead of locally training for three
    epochs, we will perform three rounds of FL with each local training phase training
    for one epoch on the local data. FedAvg will be used to aggregate the locally
    trained models at the end of each round. After these three rounds, the aforementioned
    validation metrics will be computed using the final aggregate model, allowing
    for comparisons to be drawn between the local training cases and the FL case.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating TensorFlow Federated for SST-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As previously mentioned, the **TensorFlow Federated** (**TFF**) framework was
    built on top of the TensorFlow and Keras deep learning libraries. The model implementation
    was done using Keras; as a result, the integration of TFF into the local training
    code is relatively straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to add the TFF-specific imports and FL-specific parameters
    prior to loading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'TFF allows us to simulate some number of agents by passing the appropriate
    number of datasets (local datasets) to the FL process. To split the SST-2 dataset
    into thirds after preprocessing, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have to wrap the Keras model using a TFF API function to easily create
    the respective `tff.learning.Model` object. We create a function that initializes
    the SST model and passes it along with the input spec (information on the size
    of each data element) to this API function, returning the result – TFF will use
    this function internally to create the model during the FL process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The TFF FedAvg process can then be created, using the `sst_model_fn` function
    along with the optimizers used to update the local models and the aggregate model.
    Using a learning rate of 1.0 for the server optimizer function allows for the
    new aggregate model to replace the old one at the end of each round (as opposed
    to computing a weighted average of the old and new models):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we initialize and run the federated learning process for 10 rounds.
    Each `fed_avg_process.next()` call simulates one round by performing local training
    with three models on the client datasets followed by aggregation using FedAvg.
    The resulting state after the first round is passed to the next call as the starting
    FL state for the round:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After the FL process is completed, we convert the final aggregate `tff.learning.Model`
    object back into the original Keras model format in order to compute the validation
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The final accuracy of the aggregate model should be around 82%.
  prefs: []
  type: TYPE_NORMAL
- en: From this, it should be clear that the TFF FedAvg results are nearly identical
    to those of the local training scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating OpenFL for SST-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that OpenFL supports two different workflows: the aggregator-based workflow
    and the director-based workflow. This example will use the director-based workflow,
    involving long-living components that can conduct FL task requests as they come
    in. This was chosen due to the desirability of having a persistent FL setup for
    deploying multiple projects; however, both workflows conduct the same core FL
    process and thus demonstrate similar performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To help with model serialization in this case, we only aggregate the classification
    head weights, reconstructing the full model at runtime during training and validation
    (TensorFlow Hub caches the downloaded layers, so the download process only occurs
    once). We include the following functions in `sst_model.py` to aid with this modification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Because OpenFL focuses on addressing the data silo case, the creation of the
    local datasets from the SST-2 data is slightly more involved than the TFF case.
    The objects needed to create the dataset will be implemented in a separate file
    named `sst_fl_dataset.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we include the necessary imports. The two OpenFL-specific objects we
    import are the `ShardDescriptor` object, which handles the dataset loading and
    sharding, and the `DataInterface` object, which handles access to the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Implementing ShardDescriptor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We first implement the `SSTShardDescriptor` class. When this shard descriptor
    is created, we save the `rank` (client number) and `worldsize` (total number of
    clients) values, then load the training and validation datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We implement the `ShardDescriptor` class functions to get the available dataset
    types (training and validation in this case) and the respective dataset/shard
    based on the rank of the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We also specify the properties of the specific dataset being used. Note that
    the sample shape is set to `1`. The preprocessor layer of the `SSTModel` allows
    us to pass in strings as input, which are treated as input vectors of type `tf.string`
    and length `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: With this, the `SSTShardDescriptor` implementation is completed.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing DataInterface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we implement the `SSTFedDataset` class as a subclass of `DataInterface`.
    This is done by implementing the shard descriptor getter and setter methods, with
    the setter method preparing the data to be provided to the training/validation
    FL tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We also implement the API functions to grant dataset access and dataset size
    information (used during aggregation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: With this, the local SST-2 datasets can be constructed and used.
  prefs: []
  type: TYPE_NORMAL
- en: Creating FLExperiment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now focus on the actual implementation of the FL process within a new file,
    `fl_sim.py`. First, we import the necessary libraries – from OpenFL, we import
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TaskInterface`: Allows us to define our FL training and validation tasks for
    the model; the registered tasks are what the director instructs each envoy to
    conduct'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ModelInterface`: Allows us to convert our Keras model into the format used
    by OpenFL in the registered tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Federation`: Manages information relating to the connection with the director'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FLExperiment`: Uses the `TaskInterface`, `ModelInterface`, and `Federation`
    objects to conduct the FL process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The requisite imports are done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the `Federation` object using the default `director` connection
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We then initialize the model with the associated optimizer and loss function
    – these objects are used by the OpenFL `KerasAdapter` to create the `ModelInterface`
    object. We call the model on a dummy Keras input in order to initialize all of
    the weights before passing the model to `ModelInterface`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a `TaskInterface` object and use it to register the training
    task. Note that including the optimizer in the decorator function of a task will
    result in the training dataset being passed to the task; otherwise, the validation
    dataset will be passed to the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we register the validation task using the `TaskInterface` object.
    Note that we can collect the metrics generated by the `evaluate` function and
    return the values as a means of tracking performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now load in the dataset using the `SSTFedDataset` class implemented
    earlier and create and start a new `FLExperiment` using the created `ModelInterface`,
    `TaskInterface`, and `SSTFedDatasets` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Defining the configuration files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last step is to create the configuration files used by `director` and `envoys`
    in order to actually load the data and start the FL process. First, we create
    `director_config` containing the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This is saved in `director/director_config.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create the three `envoy` configuration files. The first file (`envoy_config_1.yaml`)
    contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The second and third `envoy` config files are the same, except with the values
    `rank_worldsize: 2, 3` and `rank_worldsize: 3, 3`, respectively. These config
    files, alongside all of the code files, are stored in the experiment directory.
    The directory structure should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`director`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`director_config.yaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_1.yaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_2.yaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_3.yaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sst_fl_dataset.py`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sst_model.py`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fl_sim.py (file with` `FLExperiment creation)`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With everything set up, we can now perform FL with OpenFL.
  prefs: []
  type: TYPE_NORMAL
- en: Running the OpenFL example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, start the director by running the following command from within the
    `director` folder (make sure OpenFL is installed in the working environment):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run the following commands in separate terminals from the experiment
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Finally, start `FLExperiment` by running the `fl_sim.py` script. After the three
    rounds are completed, the aggregate model should achieve a validation accuracy
    of around 82%. Once again, the performance is nearly identical to the local training
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating IBM FL for SST-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'IBM FL uses a saved version of the model when performing FL. The following
    code (`create_saved_model.py`) initializes a model (calling the model on a dummy
    input to initialize the parameters) and then saves the model in the Keras `SavedModel`
    format for IBM FL to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Run this once to save the model into the folder named `sst_model_save_dir` –
    we will point IBM FL to load in the model saved in this directory.
  prefs: []
  type: TYPE_NORMAL
- en: Creating DataHandler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we create a subclass of the IBM FL `DataHandler` class in charge of providing
    the training and validation data to the model – this subclass will load, preprocess,
    and store the SST datasets as class attributes. We first import the necessary
    libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `init` function of this class loads the data info parameters, which are
    then used to load the correct SST-2 data partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We also implement the API function that returns the loaded datasets for use
    during training/validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Defining the configuration files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next step is to create the configuration JSON files used when starting
    the aggregator and initializing the parties. The aggregation config first specifies
    the connection information it will use to communicate with the parties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we specify the fusion handler used for aggregation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We also specify the hyperparameters related to both local training and aggregation.
    `perc_quorum` refers to the percentage of parties that must participate before
    aggregation can begin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we specify the IBM FL protocol handler to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This configuration is saved in `agg_config.json`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also create the base party configuration file used to conduct FL with the
    local data. We first specify the connection information of the aggregator and
    the party:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We then specify the data handler and the local training handler to use – this
    component trains the SST model using the model information and the local data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The model format and information is then specified – this is where we point
    to the saved model created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we specify the protocol handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Creating IBM FL party
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With this, all that is left is the code that starts each party, saved in `fl_sim.py`.
    We first import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We include an `argparse` argument that allows for the party number to be specified
    – this is used to modify the base party configuration file in order to allow for
    distinct parties to be started from the same file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we create and start a new `Party` object with the modified configuration
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: With this, we can now begin performing FL using IBM FL.
  prefs: []
  type: TYPE_NORMAL
- en: Running the IBM FL example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, start `aggregator` by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'After the aggregator is finished setting up, type `START` and press *Enter*
    key to open the aggregator to receive incoming connections. You can then start
    three parties using the following commands in separate terminals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Finally, type `TRAIN` into the aggregator window and press *Enter* key to begin
    the FL process. When three rounds are completed, you can type `SAVE` into the
    same window to save the latest aggregate model.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Flower for SST-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two main Flower components that must be incorporated on top of the existing
    local training code are the client and strategy subclass implementations. The
    client subclass implementation allows us to interface with Flower, with API functions
    that allow for model parameters to be passed between the clients and the server.
    The strategy subclass implementation allows us to specify the details of the aggregation
    approach performed by the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by writing the code to implement and start a client (stored in `fl_sim.py`).
    First, the necessary libraries are imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We add a command-line argument specifying the client ID in order to allow for
    the same client script to be reused for all three agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We then load in the SST-2 datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Note that we use the client ID to get the respective shard from the training
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the model and the associated optimizer and loss objects, making
    sure to call the model on a dummy input to initialize the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Implementing the Flower client
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can now implement the Flower client object that will pass model parameters
    to and from the server. To implement a client subclass, we have to define three
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`get_parameters(self, config)`: Returns the model parameter values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fit(self, parameters, config)`: Sets the weights of the local model to the
    received parameters, performs local training, and returns the new model parameters
    alongside the dataset size and training metrics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`evaluate(self, parameters, config)`: Sets the weights of the local model to
    the received parameters, then evaluates the model on validation/test data and
    returns the performance metrics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using `fl.client.NumPyClient` as the superclass allows us to take advantage
    of the Keras model `get_weights` and `set_weights` functions that convert the
    model parameters into lists of NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The `evaluate` function is also defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'With this client implementation, we can finally start the client using the
    default connection information with the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Creating the Flower server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The final step before running Flower is to create the script (`server.py`)
    that will start the Flower server. We begin with the necessary imports and the
    `MAX_ROUNDS` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we want to save the model after performing federated learning, we create
    a subclass of the flower FedAvg strategy and add a final step that saves the model
    at the last round during the aggregation phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'With this strategy, we can run the following line to start the server (passing
    the `MAX_ROUNDS` parameter through the `config` argument):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We can now start the server and clients, allowing for FL to be performed using
    Flower.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Flower example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To start the server, first run the `server.py` script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the three clients can then be started by running the following commands
    in separate terminal windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The final aggregate model after FL will be saved in the `final_agg_sst_model`
    directory as a `SavedModel` object.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating STADLE for SST-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: STADLE differs from the previously examined FL frameworks by providing a cloud-based
    platform (STADLE Ops) to handle the deployment of aggregators and management of
    the FL process. Because the deployment of the server side can be done through
    the platform, the client-side implementation is all that needs to be implemented
    for performing FL with STADLE. This integration is done by creating a client object
    that occasionally sends the local model and returns the aggregate model from the
    previous round. To do this, we need to create the agent configuration file and
    modify the local training code to interface with STADLE.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the configuration file for the agent as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Information on these parameters can be found at https://stadle-documentation.readthedocs.io/en/latest/documentation.html#configuration-of-agent.
    Note that the aggregator IP and registration port values listed here are placeholders
    and will be modified when connecting to the STADLE Ops platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we modify the local training code to work with STADLE. We first import
    the requisite libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we add a command-line argument to specify which partition of the
    training data the agent should receive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate a `BasicClient` object – this is the STADLE client component
    that handles communication between the local training process and the aggregators
    on the server side. We use the configuration file defined earlier to create this
    client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we implement the FL training loop. In each round, the client gets
    the aggregate model from the previous round (starting with the base model) and
    trains it further on the local data before sending it back to the aggregator through
    the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The `wait_for_sg_model` function returns the latest aggregate model from the
    server, and the `send_trained_model` function sends the locally trained model
    with the desired performance metrics to the server. More information on these
    integration steps can be found at https://stadle-documentation.readthedocs.io/en/latest/usage.html#client-side-stadle-integration.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the client side has been implemented, we can use the STADLE Ops platform
    to start an aggregator and start an FL process.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a STADLE Ops project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, go to stadle.ai and create a new account. Once you are logged in, you
    should be directed to the project information page in STADLE Ops:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Project information page in STADLE Ops'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_08_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Project information page in STADLE Ops
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Create New Project**, then fill in the project information and click
    **Create Project**. The project information page should have changed to show the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – New project added to the project information page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_08_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – New project added to the project information page
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the plus icon under **Initiate Aggregator** to start a new aggregator
    for the project, then click **OK** on the confirmation prompt. You can now navigate
    to the **Dashboard** page on the left side, resulting in a page that looks like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Dashboard page of STADLE Ops'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_08_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Dashboard page of STADLE Ops
  prefs: []
  type: TYPE_NORMAL
- en: Replace the `aggr_ip` and `reg_port` placeholder parameter values in the `config_agent.json`
    file with the values under **IP Address to Connect** and **Port to** **Connect**,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we are now ready to begin the FL training process.
  prefs: []
  type: TYPE_NORMAL
- en: Running the STADLE example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step is to send the base model object to the server, allowing it
    to in turn distribute the model to the training agents. This is done with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the command successfully runs, the **Base Model Info** section on the
    STADLE Ops dashboard should update to show the model information. We can now start
    the three agents by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: After three rounds, the agents will terminate and the final aggregate model
    will be displayed in the project dashboard, available for download in the Keras
    SavedModel format. The user guide located at [https://stadle.ai/user_guide/guide](https://stadle.ai/user_guide/guide)
    is recommended for more information on the various functionalities of the STADLE
    Ops platform.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the resulting aggregate models produced by each FL framework results
    in the same conclusion—the performance of the aggregate model essentially matches
    that of the centralized training model. As explained in the *Dataset distributions*
    section of [*Chapter 7*](B18369_07.xhtml#_idTextAnchor176), *Model Aggregation*,
    this is generally the expected result. The natural question to ask is how the
    performance is affected when the local datasets are not IID—this is the focal
    point of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Example – the federated training of an image classification model on non-IID
    data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous example, we examined how a centralized deep learning problem
    could be converted into an FL analog by training multiple clients on disjoint
    subsets of the original training dataset (the *local datasets*) in an FL process.
    One key point of this local dataset creation was that the subsets were created
    by random sampling, leading to local datasets that were all IID under the same
    distribution as the original dataset. As a result, the similar performance of
    FedAvg compared to the local training scenario was expected – each client’s model
    essentially had the same set of local minima to move toward during training, making
    all local training beneficial for the global objective.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in [*Chapter 7*](B18369_07.xhtml#_idTextAnchor176), *Model Aggregation*,
    we explored how FedAvg was susceptible to the divergence in training objectives
    induced by severely non-IID local datasets. To explore the performance of FedAvg
    on varying non-IID severities, this example trains the VGG-16 model (a simple
    deep-learning-based image classification model) on constructed non-IID local datasets
    sampled from the CIFAR-10 dataset (located at [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)).
    CIFAR-10 is a well-known simple image classification dataset containing 60,000
    images separated into 10 different classes; the goal of models trained on CIFAR-10
    is to correctly predict the class associated with an input image. The relatively
    low complexity and ubiquity as a benchmark dataset make CIFAR-10 ideal for exploring
    the response of FedAvg to non-IID data.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: To avoid including redundant code samples, this section focuses on the key lines
    of code that allow FL to be performed on PyTorch models using non-IID local datasets.
    It is recommended that you go through the examples within the *Example – the federated
    training of an NLP model* section in this chapter prior to reading this section
    in order to understand the core components needed for each FL framework. The implementations
    for this example can be found in full at this book’s GitHub repository ([https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)
    tree/main/ch8/cv_code), for use as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: The key point of this example is determining how the non-IID datasets should
    be constructed. We will change the class label distributions of each local dataset
    by changing the number of images of each class included in the training dataset. For
    example, a dataset skewed toward cars and birds might have 5,000 images of cars,
    5,000 images of birds, and 500 images for every other class. By creating three
    disjointed subsets of the 10 classes and constructing local datasets skewed toward
    these classes, we produce three local datasets with non-IID severity proportional
    to the number of images included from the classes not selected.
  prefs: []
  type: TYPE_NORMAL
- en: Skewing the CIFAR-10 dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first map the three class subsets to client IDs, and set the proportion
    of images to be taken from the original dataset for selected classes (`sel_count`)
    and the other classes (`del_count`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'We then sample the appropriate number of images from the original dataset,
    using the indices of the images in the dataset to construct the skewed CIFAR-10
    subset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: The skewed trainset is then used to create the skewed `trainloader` for local
    training. When we refer to biasing the training data going forward, this is the
    code that is run.
  prefs: []
  type: TYPE_NORMAL
- en: We will now demonstrate how to use different FL frameworks to run this non-IID
    FL process. Please refer to the installation instructions and framework-specific
    implementations in the previous section, *Example – the federated training of
    an NLP model*, for the explanations of the basics omitted in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating OpenFL for CIFAR-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the Keras NLP example, we first create the `ShardDescriptor` and
    `DataInterface` subclasses for the non-IID CIFAR-10 datasets in `cifar_fl_dataset.py`.
    Only a few changes need to be made in order to accommodate the new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we modify the `self.data_by_type` dictionary to instead store the modified
    CIFAR datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The `load_cifar_data` function loads in the training and test data using `torchvision`,
    then biases the training data based on the rank passed to the object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the dimensions of a data element are now known (the size of CIFAR-10
    image), we also modify the shape properties with fixed values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: We then implement the `CifarFedDataset` subclass of the `DataInterface` class.
    No significant modifications are needed for this implementation; thus, we can
    now use the biased CIFAR-10 dataset with OpenFL.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now move to the actual FL process implementation (`fl_sim.py`). One key
    difference is the framework adapter that must be used to create the `ModelInterface`
    object from a PyTorch model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The only other major change is modifying the train and validation functions
    passed to the `TaskInterface` object to mirror the PyTorch implementations of
    these functions from the local training code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to create the configuration files used by the director and
    envoys. The only necessary change in the director config is the updated `sample_shape`
    and `target_shape` for the CIFAR-10 data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: This is saved in `director/director_config.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The envoy configuration files require no changes outside of updating the object
    and filenames – the directory structure should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`director`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`director_config.yaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_1.yaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_2.yaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`envoy_config_3.yaml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cifar_fl_dataset.py`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fl_sim.py`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can refer to *Running the OpenFL example* in the *Integrating OpenFL for
    SST-2* section to run this example.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating IBM FL for CIFAR-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that IBM FL requires a saved version of the model used during training.
    We first run the following code in `create_saved_model.py` to create the saved
    VGG-16 PyTorch model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Next, we create the `DataHandler` subclass for the skewed CIFAR-10 datasets.
    The only core change is the modification of the `load_and_preprocess_data` function
    to instead load in the CIFAR-10 data and bias the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create the configuration JSON files used when starting
    the aggregator and initializing the parties. No significant changes to the aggregator
    config (`agg_config.json`) are necessary, and the only core change in the party
    config is the modification of the model information to work with PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: The code in `fl_sim.py` responsible for starting up the parties can essentially
    remain unmodified due to the extensive use of the configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to *Running the IBM FL example* in the *Integrating IBM FL for
    SST-2* section to run this example.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Flower for CIFAR-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After loading in the CIFAR-10 data and biasing the training data, the core
    change needed for the Flower implementation is the `NumPyClient` subclass. Unlike
    the Keras example, the `get_parameters` and `set_parameters` methods rely on the
    PyTorch model state dictionaries and are a bit more involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: We modify the `fit` function to mirror the training code in the local training
    example and modify the evaluate function to similarly mirror the local training
    evaluation code. Note that we call `self.set_parameters(parameters)` in order
    to update the local model instance with the most recent weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also set the `grpc_max_message_length` parameter to 1 GB when starting the
    Flower client and server to accommodate the larger VGG16 model size. The client
    initialization function is now the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we modify the aggregator code in `server.py` – the custom strategy
    we used previously to save the aggregate model at the end of the last round needs
    to be modified to work with PyTorch models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'With this strategy, we can run the following line to start the server (adding
    the `grpc_max_message_length` parameter here as well):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Refer to *Running the Flower example* in the *Integrating Flower for SST-2*
    section to run this example.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating STADLE for CIFAR-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first modify the `config_agent.json` config file to use the VGG16 model
    from the `torchvision` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'To integrate STADLE into the local training code, we initialize the `BasicClient`
    object and modify the training loop to send the local model every two local training
    epochs and wait for the new aggregate model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The code located at [https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)
    contains the full implementation of this integration example for reference. To
    start an aggregator and perform FL with the CIFAR-10 STADLE example, please refer
    to *Creating a STADLE Ops project* and *Running the STADLE example* in the *Integrating
    STADLE for* *SST-2* subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Testing different levels of bias in the constructed local datasets should lead
    to the same conclusion stated in the *Dataset distributions* section of [*Chapter
    7*](B18369_07.xhtml#_idTextAnchor176), *Model Aggregation* for non-IID cases—as
    the non-IID severity increases, the convergence speed and model performance decrease.
    The goal of this section was to build off of the understanding of each FL framework
    from the SST-2 example, highlighting the key changes necessary to work with a
    PyTorch model on a modified dataset. Using this section alongside the code examples
    in [https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)
    should help in understanding this example integration.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered several FL frameworks through the context of two
    different examples. From the first example, you learned how a traditional centralized
    ML problem can be converted into the analogous FL scenario by separating the data
    into disjointed subsets. It is now clear that random sampling leads to local datasets
    that are IID, allowing FedAvg to reach the same level of performance as the centralized
    equivalent with any of the FL frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: In the second example, you learned one of the many ways a group of datasets
    can be non-IID (different class label distributions) and observed how different
    severities of non-IID datasets affect the performance of FedAvg. We encourage
    you to explore how alternative aggregation methods can improve on FedAvg in these
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: Both examples also should have given you a solid understanding of the general
    trends when working with different FL frameworks; while the specific implementation-level
    details may change (due to the rapidly changing field), the core concepts and
    implementation details will remain fundamentals.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we continue our transition to the business application
    side of FL by taking a look at several case studies involving the application
    of FL to specific domains.
  prefs: []
  type: TYPE_NORMAL
