- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview of Privacy-Preserving Data Analysis and an Introduction to Differential
    Privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the concept of privacy in the context of big
    data, along with the associated risks. We will delve into privacy in data analysis,
    focusing on the trade-off between privacy and utility. Furthermore, we will investigate
    various privacy-preserving techniques, such as anonymization, k-anonymity, t-closeness,
    and ℓ-diversity, while also discussing their limitations. Later on, we will introduce
    one of the key privacy-enhancing approaches, known as differential privacy. We
    will provide a high-level overview of differential privacy, covering essential
    concepts such as privacy loss, privacy budgets, and differential privacy mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics covered in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Privacy in data analysis:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy in data analysis, the need for privacy in data analysis, and the objectives
    of privacy in data analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Privacy-preserving techniques:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigating various privacy-preserving techniques, including anonymization,
    k-anonymity, t-closeness, and ℓ-diversity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data aggregation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy attacks with data aggregation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools/framework to protect data privacy in SQL
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Privacy-enhancing technologies:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing differential privacy as a privacy-enhancing technology and federated
    learning and homomorphic encryption
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Differential privacy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep diving into differential privacy concepts, including privacy loss, privacy
    budgets, and differential privacy mechanisms
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy in data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Privacy in data analysis is a crucial aspect that ensures sensitive information
    about individuals is not disclosed or misused. It involves implementing measures
    such as data anonymization and encryption to protect the identity and personal
    details of individuals while still allowing for meaningful data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The need for privacy in data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many enterprises, social networking companies, e-commerce platforms, networking
    companies, taxi/cab aggregators, food delivery services, and government organizations,
    among others, gather and process vast amounts of data – both personal and non-personal
    – to derive insights using machine learning and AI techniques. The data collected
    by these entities encompasses a wide range of information, such as browsing history,
    purchase records, social network interactions, health data, location data, content
    consumption patterns, device information, and more. It is important to note that
    this data often contains sensitive personal information. Sharing or retaining
    this data for longer durations than necessary (considering data retention laws
    based on data categories) can pose privacy risks and may lead to non-compliance
    with privacy and legal regulations. As we discussed in the first chapter, some
    privacy risks/breaches are related to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stealing personal information**: This includes unauthorized access to individuals’
    credentials, such as credit card numbers, passwords, and other sensitive data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identity theft**: Personal identification details, such as Social Security
    numbers (in the US) or Aadhaar unique IDs (in India), names, bank information,
    biometric data, and driving licenses, can be targeted for identity theft'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discrimination and targeting individuals**: Certain types of data, such as
    medical records, can be exploited to discriminate against individuals based on
    their religious beliefs or can lead to consequences such as insurance rejection.
    Other examples include political posts, comments/opinions on social media, and
    more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address these concerns, privacy laws have been implemented worldwide. For
    instance, the **General Data Protection Regulation** (**GDPR**) in Europe and
    the **California Consumer Privacy Act** (**CCPA**) in the United States provide
    guidelines and regulations for the protection of sensitive data and address privacy-related
    breaches and issues.
  prefs: []
  type: TYPE_NORMAL
- en: It is crucial for organizations to be up to date with the practices of privacy
    in data analysis in order to be aware of privacy risks, comply with relevant privacy
    laws and regulations, and take necessary measures to protect individuals’ data
    privacy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy in data analysis** refers to the process of performing data analysis
    tasks while preserving the privacy and confidentiality of data. It involves applying
    various techniques and methodologies to extract meaningful insights from data
    without compromising the privacy rights of individuals whose data is being analyzed.
    The goal is to strike a balance between data utility and privacy, which is crucial
    in privacy data analysis. However, there are trade-offs associated with each approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High data privacy**: Emphasizing strong privacy measures may result in poor
    data utility. For instance, if sensitive personal data, such as Social Security
    numbers or medical records, is redacted or anonymized to safeguard privacy, it
    becomes challenging to link that data to other relevant data points. Consequently,
    the resulting insights may be less useful for analytical purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High data utility**: Prioritizing high data utility may lead to weaker privacy
    protection. In this scenario, the data remains highly useful and valuable for
    analysis. However, there may be inadequate protection of sensitive information,
    increasing the risk of re-identification or the unintended disclosure of personal
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Privacy versus utility trade-off](img/B16573_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Privacy versus utility trade-off
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Privacy versus utility trade-off](img/B16573_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Privacy versus utility trade-off
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: *Churi, Prathamesh & Pawar, Dr. Ambika & Moreno Guerrero, Antonio.
    (2021). A Comprehensive Survey on Data Utility and Privacy: Taking Indian Healthcare
    System as a Potential Case Study. Inventions. 6\.* *1-30\. 10.3390/inventions6030045.*'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the right balance between privacy and utility is a critical challenge
    of privacy in data analysis. Organizations must implement privacy-preserving techniques,
    such as anonymization, aggregation, and noise addition, to protect sensitive information
    while still enabling meaningful analysis. By carefully considering both privacy
    and utility aspects, it is possible to derive valuable insights from data while
    safeguarding individuals’ privacy rights.
  prefs: []
  type: TYPE_NORMAL
- en: Objectives of privacy in data analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The objectives of privacy in data analysis are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy preservation**: The primary focus is on safeguarding the sensitive
    information contained within the dataset, ensuring that individuals’ privacy is
    respected and protected. This involves preventing unauthorized access, disclosure,
    or the identification of individuals from the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data utility**: While preserving privacy, the analysis should still provide
    valuable and accurate results that maintain the usefulness and quality of the
    data. The challenge lies in finding methods that allow for effective analysis
    while minimizing the impact on data utility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk assessment**: Privacy in data analysis involves identifying and assessing
    potential privacy risks associated with the data being analyzed. This includes
    evaluating the likelihood of re-identification or the possibility of the unintended
    disclosure of sensitive information during the analysis process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anonymization techniques**: Various anonymization techniques are employed
    to remove or obfuscate identifying information from the dataset. This can include
    methods such as data generalization, suppression, noise addition, and data perturbation
    to protect individuals’ identities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy-preserving algorithms/techniques**: Specialized algorithms and methodologies
    are utilized to perform analysis tasks while minimizing privacy risks. These algorithms
    ensure that the results reveal aggregated and anonymized insights rather than
    individual-level details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulatory compliance and ethical considerations**: Privacy in data analysis
    adheres to relevant privacy laws and regulations to ensure compliance and ethical
    handling of sensitive data. Organizations must adhere to legal requirements and
    ethical guidelines to protect individuals’ privacy rights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By protecting privacy in data analysis techniques, organizations can gain valuable
    insights from sensitive data while respecting privacy constraints. This approach
    fosters trust among those individuals whose data is being analyzed and promotes
    responsible data-handling practices.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy-preserving techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s important to note that data privacy is a fundamental right, and individuals
    have the right to control how their personal information is collected, used, and
    shared. Moreover, as discussed above, a lack of data privacy protection can lead
    to serious consequences such as identity theft, financial fraud, and discrimination.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of data analysis, an adversary refers to an entity or party that
    actively seeks to gain unauthorized access to, or exploit, sensitive data, disrupt
    the data analysis process, or manipulate the results for their own benefit or
    malicious intent. Adversaries in data analysis can include individuals, organizations,
    or automated systems that aim to compromise the integrity, confidentiality, or
    availability of the data or the analysis process itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Organizations can implement various privacy-preserving techniques that allow
    for the analysis of data while still protecting the privacy of individuals. We’ll
    cover the following privacy-preserving techniques and associated privacy attacks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data anonymization techniques/algorithms:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-anonymity, ℓ-diversity, and t-closeness
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data aggregation and privacy attacks associated with data aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data anonymization and algorithms for data anonymization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the techniques for safeguarding privacy and complying with data protection
    regulations is data anonymization. By anonymizing data, organizations can mitigate
    the risks associated with the unauthorized disclosure and misuse of personal information
    while still preserving the utility and analytical value of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, datasets consist of three types of attributes – namely, key identifiers,
    quasi-identifiers, and sensitive attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Key identifiers**: These attributes uniquely identify individuals within
    the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quasi-identifiers**: Quasi-identifiers are indirect identifiers that, when
    combined with other attributes, can be used to identify individuals or uncover
    personal information. They provide hints or partial identification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensitive attributes**: These attributes contain sensitive personal data
    that needs to be protected to maintain privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s consider an example dataset from a hospital or medical clinic that collects
    and maintains patient data. Assume that the hospital collects various data for
    each patient, including name, date of birth, gender, postal code/zip code, height,
    weight, blood pressure, SpO2 levels, current medical conditions, last visit date,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some details about this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: The name serves as a key attribute since it uniquely identifies each person.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quasi-identifiers include attributes such as date of birth, gender, postal code,
    and height. Although knowing the date of birth alone may not be sufficient to
    identify an individual, combining it with other quasi-identifiers such as gender,
    postal code, and medical conditions might make it possible to identify the individual.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitive attributes encompass information related to the type of disease the
    patient is suffering from. Protecting this information is crucial to maintain
    the privacy of the patients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding the distinct types of attributes within a dataset, organizations
    can implement appropriate anonymization techniques to safeguard the privacy of
    individuals while still allowing for valuable analysis and research.
  prefs: []
  type: TYPE_NORMAL
- en: Now we know about the different types of attributes, let’s try data anonymization
    on a sample toy dataset and analyze the results.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an example dataset containing information about individuals’
    ages and medical conditions. The sensitive attribute in this case is the medical
    condition, which we want to protect while preserving the data’s utility.
  prefs: []
  type: TYPE_NORMAL
- en: '| Key Attribute | Quasi-Identifiers |  | Sensitive Attribute | Other Data …
    |'
  prefs: []
  type: TYPE_TB
- en: '| Name | Date of Birth | Gender | Zip Code | Height in cm | Disease | Other
    Data … |'
  prefs: []
  type: TYPE_TB
- en: '| John | 2000-09-15 | M | 90001 | 170 | COVID | …. |'
  prefs: []
  type: TYPE_TB
- en: '| Rosy | 2002-12-08 | F | 96162 | 165 | COVID |  |'
  prefs: []
  type: TYPE_TB
- en: '| Robin | 1945-07-24 | M | 92348 | 180 | CANCER |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hellen | 1950-01-13 | F | 95411 | 156 | CANCER |  |'
  prefs: []
  type: TYPE_TB
- en: '| Antonio | 1970-01-13 | M | 95416 | 180 | Fever | ….. |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Sample dataset
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, the data collected contains personal information such as names, dates
    of birth, and so on, so in order to protect individuals, let’s try the data anonymization
    technique and see whether it helps or not. Data anonymization means either removing
    or modifying the personally identifiable information so that others will not be
    able to detect individuals from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Let us try a simple solution, that is, removing the personal information (in
    this case, the name of the person) from the given data. The data looks like the
    following after removing the names from the data.
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Date of Birth | Gender | Zip Code | Height in cm | Diagnosed Disease
    | Other Data … |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2000-09-15 | M | 90001 | 170 | COVID | …. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2002-12-08 | F | 96162 | 165 | COVID |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 1945-07-24 | M | 92348 | 180 | CANCER |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 1950-03-13 | F | 95411 | 156 | CANCER |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 1970-01-13 | M | 95416 | 180 | Fever | ….. |'
  prefs: []
  type: TYPE_TB
- en: Table 3.2 – Dataset without names
  prefs: []
  type: TYPE_NORMAL
- en: Is the removal of the names sufficient to protect sensitive data? Can this data
    now be shared with others? Could anyone accurately identify the individual with
    cancer from this data?
  prefs: []
  type: TYPE_NORMAL
- en: Based on the preceding data table, it looks like people will not be able to
    find out because we have removed the name of the person from the data. This is
    the first impression from this data. Removing the names of individuals from a
    dataset is a step toward protecting sensitive data and preserving privacy.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is important to note that simply removing the name does not guarantee
    complete privacy protection. Other quasi-identifiers and sensitive attributes
    within the dataset could still potentially lead to the re-identification of individuals.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how an adversary would be able to identify individuals using this
    so-called anonymized dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Government organizations do a census data survey (population survey and voter
    list updates/additions, and so on) every 5 or 10 years, depending on a country’s
    regulations, and share it with other departments as appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: There is a possibility that the anonymized dataset (like the preceding data
    table) could be linked with publicly available datasets published by either government
    organizations or non-profit organizations and insights could be derived or individuals
    could be identified, that is, sensitive personal information could be found out.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample dataset of voter ID data from the government:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Date of Birth | Gender | Zip Code | Last Year Voted | Registration
    Date | Othe Data … |'
  prefs: []
  type: TYPE_TB
- en: '| John | 2000-09-15 | M | 90001 | 2020 | 2018 | …. |'
  prefs: []
  type: TYPE_TB
- en: '| Rosy | 2002-12-08 | F | 96162 | - | 2020 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Robin | 1945-07-24 | M | 92348 | 2020 | 1963 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hellen | 1950-03-13 | F | 92411 | 2020 | 1968 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Antonio | 1970-01-13 | M | 95416 | - | 1988 |  |'
  prefs: []
  type: TYPE_TB
- en: Table 3.3 – Sample voter ID dataset
  prefs: []
  type: TYPE_NORMAL
- en: It is quite possible to link both these datasets, that is, patients’ data with
    voters’ data (refer to the LINDDUN framework’s privacy threat linkage category,
    which was described in [*Chapter 1*](B16573_01.xhtml#_idTextAnchor015)) and find
    out who has cancer from a given zip/postal code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Linking two datasets](img/B16573_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Linking two datasets
  prefs: []
  type: TYPE_NORMAL
- en: It is very clear that Robin lives in postal code 92348, so an adversary could
    link voter ID and medical data records and identify that he suffers from cancer.
    Once the adversary finds the sensitive information in this case, that is, who
    has cancer, then possible discrimination as a side effect of this data privacy
    breach could be the rejection of medical insurance for Robin.
  prefs: []
  type: TYPE_NORMAL
- en: So, just removing one or two fields from the dataset is not enough to protect
    the sensitive information of individuals.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try another data anonymization technique called k-anonymity.
  prefs: []
  type: TYPE_NORMAL
- en: K-anonymity – data anonymization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Latanya Sweeney* and *Pierangela Samarati* introduced k-anonymity in 1998
    and it is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given person-specific field-structured data, produce a release of the data with
    scientific guarantees that the individuals who are the subjects of the data cannot
    be re-identified while the data remain practically useful. A release of data is
    said to have the k-anonymity property if the information for each person contained
    in the release cannot be distinguished from at least k-1 individuals whose information
    also appears in the release.
  prefs: []
  type: TYPE_NORMAL
- en: '*(**Source: Wikipedia)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to apply k-anonymity anonymization, one needs to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the quasi-identifier in the given dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generalize the attributes (instead of exact dates, use ranges) or suppress the
    attributes (instead of showing full zip codes show only partial ones) for at least
    k-1 records in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| Name | Date of Birth | Gender | Zip Code | Height in cm | Diagnosed Disease
    | Other Data… |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2000-09-15 | M | 90001 | 170 | COVID | …. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2002-12-08 | F | 96162 | 165 | COVID |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 1945-07-24 | M | 92348 | 180 | CANCER |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 1950-03-13 | F | 92411 | 156 | CANCER |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 1970-01-13 | M | 95416 | 180 | Fever |  |'
  prefs: []
  type: TYPE_TB
- en: Table 3.4 – Dataset without names
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the quasi-identifiers are zip code and date of birth. These
    are considered indirect identifiers as there could be individuals who have the
    same zip code and date of birth, but these two factors may not necessarily identify
    a specific individual. Once the quasi-identifiers are identified, either suppress
    them or generalize attributes. Let’s try the last three digits of the zip code
    suppressed with *** and generalize the date of birth using a range rather than
    the exact date of birth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated data (anonymized) looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Date of Birth | Gender | Zip Code | Height in cm | Diagnosed Disease
    | Other Data… |'
  prefs: []
  type: TYPE_TB
- en: '|  | 20-30 | M | 90*** | 170 | COVID | …. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 20-30 | F | 96*** | 165 | COVID |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 70-80 | M | 92*** | 180 | CANCER |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 70-80 | F | 92*** | 156 | CANCER |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 50-60 | M | 95*** | 180 | FEVER |  |'
  prefs: []
  type: TYPE_TB
- en: Table 3.5 – Dataset after generalization and suppression
  prefs: []
  type: TYPE_NORMAL
- en: 'Go through these questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Is this now sufficient to protect the sensitive data because we have anonymized
    the data (quasi-identifiers) using k-anonymity?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can the data now be shared with others?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Could anyone now find out exactly who is the person with cancer from this data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, it is still possible to identify the individual because, within
    the same age range (70-80), every person has cancer. This vulnerability is known
    as a homogeneity attack, which can occur in k-anonymity when a set of k records
    have exact sensitive values. Despite the data being k-anonymized, the sensitive
    value for the set of k records can still be accurately predicted due to the identical
    date range for the sensitive value “cancer” in our example.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now implement k-anonymity using Python for a sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Source code: K-Anonymity_Example.ipynb'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll create a synthetic dataset of medical records containing quasi-identifiers
    (age and gender) and a sensitive attribute (diagnosis):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we first define the parameters, including the desired number
    of records (`num_records`) and the desired k-anonymity level (`k`). Then, we generate
    a synthetic dataset by randomly assigning values to the quasi-identifiers (age
    and gender) and the sensitive attribute (diagnosis). After generating the dataset,
    we apply k-anonymity by grouping the records based on the quasi-identifiers and
    checking whether the group size is less than k. If the group size is less than
    k, we generalize the quasi-identifiers by applying a suitable generalization technique.
  prefs: []
  type: TYPE_NORMAL
- en: The `generalize()` function takes an attribute (e.g., age or gender) as input
    and applies a specific generalization technique. Here, we implement the generalization
    for the age attribute by dividing it into predefined age ranges (e.g., 0-20, 20-40,
    40-60, and 60-80). For the gender attribute, we generalize it as `"Other"`.
  prefs: []
  type: TYPE_NORMAL
- en: We can modify and extend this function based on the generalization requirements
    specific to the dataset. For instance, we may need to handle other attributes
    differently or define additional generalization rules for different quasi-identifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the anonymized dataset is saved to a file named `k_anonymized_dataset.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try another anonymization technique called ℓ-diversity and see whether
    it provides a solution to the privacy questions discussed in our examination of
    k-anonymity.
  prefs: []
  type: TYPE_NORMAL
- en: ℓ-diversity – data anonymization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ℓ-diversity is an approach that aims to mitigate the vulnerability of homogeneity
    attacks on anonymized data. It achieves this by ensuring that each sequence of
    quasi-identifiers is associated with a “well-represented” sensitive attribute.
    In other words, ℓ-diversity seeks to introduce diversity within a group of records
    that share the same quasi-identifiers, ensuring that multiple sensitive attribute
    values are present. By doing so, ℓ-diversity reduces the risk of accurately inferring
    sensitive information from the anonymized data.
  prefs: []
  type: TYPE_NORMAL
- en: Original dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Quasi-Identifiers | Sensitive Attribute |'
  prefs: []
  type: TYPE_TB
- en: '| Date of Birth | ZIP Code | Disease |'
  prefs: []
  type: TYPE_TB
- en: '| 2000-09-15 | 90001 | COVID |'
  prefs: []
  type: TYPE_TB
- en: '| 2002-12-08 | 96162 | COVID |'
  prefs: []
  type: TYPE_TB
- en: '| 1945-07-24 | 92348 | CANCER |'
  prefs: []
  type: TYPE_TB
- en: '| 1950-03-13 | 95411 | CANCER |'
  prefs: []
  type: TYPE_TB
- en: '| 1970-01-13 | 95416 | FEVER |'
  prefs: []
  type: TYPE_TB
- en: Table 3.6 – Original dataset
  prefs: []
  type: TYPE_NORMAL
- en: K-anonymization dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Quasi-Identifiers | Sensitive Attribute |'
  prefs: []
  type: TYPE_TB
- en: '| Date of Birth | ZIP Code | Disease |'
  prefs: []
  type: TYPE_TB
- en: '| 20-30 | 90*** | COVID |'
  prefs: []
  type: TYPE_TB
- en: '| 20-30 | 96*** | COVID |'
  prefs: []
  type: TYPE_TB
- en: '| 70-80 | 92*** | CANCER |'
  prefs: []
  type: TYPE_TB
- en: '| 70-80 | 92*** | CANCER |'
  prefs: []
  type: TYPE_TB
- en: '| 50-60 | 95*** | FEVER |'
  prefs: []
  type: TYPE_TB
- en: Table 3.7 – K-anonymization dataset
  prefs: []
  type: TYPE_NORMAL
- en: This data is not diverse when this data is shared with others.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this dataset ℓ-diverse (for ℓ=2 ) we need to introduce a unique disease
    for at least one entry in each group with the same date of birth and zip code.
    Here is an example of how we could modify the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ℓ -Diversity Dataset Quasi-Identifiers | Sensitive Attribute |'
  prefs: []
  type: TYPE_TB
- en: '| Date of Birth | Zip Code | Disease |'
  prefs: []
  type: TYPE_TB
- en: '| 20-30 | 90*** | COVID |'
  prefs: []
  type: TYPE_TB
- en: '| 70-80 | 92*** | CANCER |'
  prefs: []
  type: TYPE_TB
- en: '| 50-60 | 95*** | FEVER |'
  prefs: []
  type: TYPE_TB
- en: '| 20-30 | 96*** | COVIDFLU |'
  prefs: []
  type: TYPE_TB
- en: '| 70-80 | 92*** | CANCERHEART DISEASE |'
  prefs: []
  type: TYPE_TB
- en: Table 3.8– ℓ-diversity dataset (somewhat diverse)
  prefs: []
  type: TYPE_NORMAL
- en: The 20-30 age group with zip code 96*** has `Disease` changed to `FLU`. The
    70-80 age group with the zip code 92*** has `Disease` changed to `HEART DISEASE`
    for one record. Now, each set of similar dates of birth and zip codes has at least
    two unique diseases, which makes the dataset 2-diverse.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving ℓ-diversity may seem straightforward in smaller datasets, as demonstrated
    in our example. However, there are limitations to its applicability, particularly
    when dealing with large datasets that contain millions or billions of records.
    In such cases, achieving ℓ-diversity becomes challenging and may not be necessary
    in all scenarios. It is important to note that ℓ-diversity, while helpful, is
    not a comprehensive solution for protecting sensitive information. While it introduces
    diversity within groups of records, it does not guarantee complete privacy. Additional
    privacy-preserving techniques and measures may be required to provide stronger
    protection for sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now implement ℓ-diversity using Python for a sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: L-Diversity_Example.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement `ℓ-diversity` on the synthetic dataset we generated earlier.
    We’ll consider the diagnosis attribute as the sensitive attribute and ensure that
    each group has at least ℓ-distinctsensitive attribute values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We generate a synthetic dataset similar to before. Afterward, we group the
    records based on the quasi-identifiers (age and gender) and check whether the
    number of distinct sensitive attribute values (diagnosis) is less than “ℓ” . If
    the condition is met, we apply generalization or suppression to the sensitive
    attribute to ensure there are at least “ℓ” distinct values within the group. Note
    that in this example, the `generalize()` function is called for the diagnosis
    attribute. Finally, the anonymized dataset is saved to a file named `l_diversity_dataset.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `generalize()` function takes an attribute (e.g., diagnosis)
    as input and applies a specific generalization technique. Here, we demonstrate
    the generalization for the diagnosis attribute by mapping specific diagnoses to
    higher-level categories. For instance, diagnoses such as `"Fever"` and `"Obesity"`
    are generalized to the category `"Non-Chronic Condition"`, while `"Cancer"` and
    `"Covid"` are generalized to `"``Chronic Condition"`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s delve into another approach, known as t-closeness.
  prefs: []
  type: TYPE_NORMAL
- en: t-closeness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The t-closeness approach ensures that the distribution of sensitive attributes
    within a group of individuals is not significantly different from the distribution
    of the same attributes in the overall population. It aims to maintain a balance
    between privacy and data utility by setting a threshold value, denoted by `t`,
    which represents the maximum allowed difference between the distributions.
  prefs: []
  type: TYPE_NORMAL
- en: To satisfy t-closeness, a dataset should exhibit similar probability distributions
    of sensitive attributes (such as age, income, and gender) in each group of individuals
    compared to the overall dataset. If the difference between the group and overall
    distributions exceeds the specified t-value, additional anonymization measures
    may be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an example to illustrate t-closeness. Suppose we have a dataset
    containing information about individuals’ ages and medical conditions. The sensitive
    attribute in this case is the medical condition, which we want to protect while
    preserving the data’s utility.
  prefs: []
  type: TYPE_NORMAL
- en: 'We divide the dataset into groups based on a set of quasi-identifiers, such
    as age ranges and gender. For simplicity, let’s focus on the age range. We have
    two groups: group A (age range 30-40) and group B (age range 40-50).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To satisfy t-closeness, we need to compare the distribution of medical conditions
    within each group to the overall distribution in the dataset. Let’s assume the
    overall dataset has the following distribution of medical conditions: 60% cancer,
    30% diabetes, and 10% asthma.'
  prefs: []
  type: TYPE_NORMAL
- en: In group A, we find that the distribution of medical conditions is 70% cancer,
    20% diabetes, and 10% asthma. In group B, the distribution is 50% cancer, 30%
    diabetes, and 20% asthma.
  prefs: []
  type: TYPE_NORMAL
- en: To determine whether the dataset satisfies t-closeness, we calculate the differences
    between the group distributions and the overall distribution. Let’s assume we
    set the threshold value at t = 0.2 (20%).
  prefs: []
  type: TYPE_NORMAL
- en: In group A, the difference for cancer is 10% (70%–60%), which is within the
    t-value. For diabetes, the difference is 10% (20%–30%), also within the threshold.
    The same applies to asthma, with a difference of 0% (10%–10%).
  prefs: []
  type: TYPE_NORMAL
- en: In group B, the difference for cancer is 10% (50%–60%), within the t-value.
    For diabetes, the difference is 0% (30%–30%), and for asthma, it is 10% (20%–10%).
    Both differences are within the specified threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Since all the differences between the group distributions and the overall distribution
    are within the t-value, we can conclude that the dataset satisfies t-closeness
    for the given threshold.
  prefs: []
  type: TYPE_NORMAL
- en: This example demonstrates how t-closeness ensures that the distribution of sensitive
    attributes within each group is not significantly different from the distribution
    in the overall dataset, providing a higher level of privacy while maintaining
    the data’s utility.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to k-anonymity, t-closeness provides a more stringent privacy requirement
    as it considers the actual distributions of sensitive attributes rather than just
    the group size. However, achieving t-closeness while maintaining the data’s utility
    can be challenging. Various techniques and algorithms have been developed to strike
    a balance between privacy and utility when applying t-closeness in data anonymization
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now implement t-closeness using Python for a sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '`Source code:t_Closeness_Example.ipynb`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example of implementing t-closeness on the synthetic dataset
    we generated earlier. We’ll consider the diagnosis attribute as the sensitive
    attribute and ensure that the distribution of sensitive attribute values within
    each group does not deviate significantly from the overall distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we generate a synthetic dataset as before. We calculate the
    overall distribution of the sensitive attribute (diagnosis) in the dataset. Then,
    we group the records based on the quasi-identifiers (age and gender) and calculate
    the distribution of the sensitive attribute within each group. If the maximum
    divergence between the group’s distribution and the overall distribution exceeds
    the specified threshold, `t`, we apply generalization or suppression to the sensitive
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of k-anonymity, l-diversity, and t-closeness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Privacy Technique | Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '| k-anonymity |'
  prefs: []
  type: TYPE_TB
- en: Simplicity and ease of implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides a strong guarantee of identity privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective in preventing attribute linkage attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preserves data utility to a certain extent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Does not consider the distribution of sensitive attributes, which may lead to
    attribute disclosure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can result in excessive generalization, reducing data utility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitive attribute suppression may lead to information loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vulnerable to background knowledge attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ℓ -diversity |'
  prefs: []
  type: TYPE_TB
- en: Enhances k-anonymity by considering diversity within groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides better protection against attribute disclosure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows for fine-grained control over the diversity requirement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offers a trade-off between privacy and data utility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Generalization and suppression may lead to information loss and reduced data
    utility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of “ℓ” value may be subjective and challenging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Still vulnerable to background knowledge attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cannot guarantee protection against all types of attribute disclosure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| t-closeness |'
  prefs: []
  type: TYPE_TB
- en: Extends ℓ-diversity by considering the distribution of sensitive attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides statistical guarantees against attribute disclosure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considers both diversity and distribution, striking a balance between privacy
    and data utility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows for fine-grained control over the t-closeness threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Generalization and suppression may still result in information loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The determination of an appropriate t-closeness threshold can be challenging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical measures may require domain expertise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vulnerable to background knowledge attacks and correlation attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.9– Comparison of k-anonymity, ℓ-diversity, and t-closeness
  prefs: []
  type: TYPE_NORMAL
- en: This data provides a high-level overview of the strengths and weaknesses of
    each technique. The actual effectiveness and limitations of each technique can
    vary depending on the specific implementation, dataset characteristics, and privacy
    requirements. It’s important to carefully evaluate and select the most appropriate
    technique based on the specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore the data aggregation technique, which is another privacy-preserving
    technique. We will try implementing data aggregation and see whether aggregates
    solve the problem of predicting sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: Data aggregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen, data cannot be fully anonymized and remain useful at the same
    time. Let’s try aggregated datasets and see whether aggregated data will be useful
    for preserving privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Data aggregation refers to the process of combining and summarizing data from
    multiple sources or individuals to obtain a consolidated view or analysis. It
    involves gathering data points or records and transforming them into a more concise
    representation, often in the form of statistical measures or aggregated values.
    The objective of data aggregation is to extract meaningful insights and patterns
    from large datasets while reducing complexity and maintaining data privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Key aspects of data aggregation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different aggregation methods are employed based on the nature of the data and
    the desired outcomes. Common techniques include summing, averaging, counting,
    maximum/minimum values, percentiles, or other statistical measures. These methods
    help condense the data while preserving essential characteristics or patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Data aggregation can be performed at different levels of granularity, depending
    on the analysis requirements. It can involve aggregating data at the individual
    level, group level, geographic level, time intervals, or any other relevant grouping
    criteria. Adjusting the granularity allows for different perspectives and insights
    to be derived from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregated data provides a more manageable and concise representation of the
    original dataset, enabling effective analysis, visualization, and decision-making
    processes. It helps identify trends, patterns, correlations, or anomalies that
    may not be apparent when examining individual data points.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy attacks associated with data aggregation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data aggregation, while a useful technique for deriving insights from large
    datasets, can also introduce privacy risks. Here are some privacy attacks associated
    with data aggregation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Re-identification attacks**: Aggregated data, even when individual identifiers
    are removed or anonymized, can still be susceptible to re-identification attacks.
    By combining multiple aggregated datasets or leveraging external information,
    an attacker may be able to identify individuals within the aggregated data, compromising
    their privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference attacks**: Inference attacks occur when an adversary can infer
    sensitive information about individuals by analyzing the aggregated data. By identifying
    patterns, correlations, or statistical measures within the aggregated dataset,
    an attacker may be able to deduce private attributes or behaviors of individuals
    that were intended to remain confidential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Membership inference attacks**: Membership inference attacks aim to determine
    whether a specific individual’s data was included in the aggregated dataset. By
    leveraging statistical analysis techniques, an attacker can exploit patterns or
    characteristics in the aggregated data to infer the presence or absence of certain
    individuals, violating their privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute disclosure attacks**: Aggregated data may inadvertently disclose
    sensitive attributes or information about individuals, even if direct identifiers
    are removed. By analyzing the aggregated data and combining it with external knowledge,
    an attacker may be able to infer private attributes, preferences, or characteristics
    of individuals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Composition attacks**: Composition attacks exploit the combination of multiple
    aggregated datasets to reveal sensitive information. Even if individual datasets
    do not pose privacy risks on their own, the aggregation of multiple datasets may
    enable attackers to identify individuals or extract private information by cross-referencing
    data from different sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to strike a balance between data utility and privacy protection
    to mitigate the risks associated with privacy attacks during data aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through key privacy attacks with aggregated datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Differencing privacy attack with an aggregated dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A differencing privacy attack leverages the statistical differences between
    the outputs of different queries to infer sensitive information about individuals
    or their data points.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An adversary attempts to infer the salary information of individual employees
    by exploiting the outputs or queries that provide aggregate statistics about salaries.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a dataset containing the salary information of employees in a company.
    The dataset includes attributes such as employee name, age, and salary.
  prefs: []
  type: TYPE_NORMAL
- en: '**Toy dataset example (****original dataset):**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Employee Name | Age | Salary (USD K) |'
  prefs: []
  type: TYPE_TB
- en: '| John | 30 | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| Rosy | 35 | 300 |'
  prefs: []
  type: TYPE_TB
- en: '| Robin | 40 | 250 |'
  prefs: []
  type: TYPE_TB
- en: '| Hellen | 50 | 315 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.10 – Toy dataset example
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that aggregated data protects the privacy of the individual by
    default.
  prefs: []
  type: TYPE_NORMAL
- en: If the HR department in this company publishes an aggregated dataset on a weekly
    basis instead of sharing the original dataset, it can help mitigate the privacy
    risks associated with individual salary information. By providing only aggregate
    statistics, the HR department can protect the sensitive details of individual
    salaries while still providing useful insights to stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregated dataset –** **week 1:**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Average Age | Average Salary |'
  prefs: []
  type: TYPE_TB
- en: '| 38.75 | 266.25 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.11 – Aggregated dataset – week 1
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that, in the following week, an additional employee was hired. The raw
    data was then updated with the details of the new employee and the aggregated
    dataset was published.
  prefs: []
  type: TYPE_NORMAL
- en: '| Employee Name | Age | Salary (USD K) |'
  prefs: []
  type: TYPE_TB
- en: '| John | 30 | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| Rosy | 35 | 300 |'
  prefs: []
  type: TYPE_TB
- en: '| Robin | 40 | 250 |'
  prefs: []
  type: TYPE_TB
- en: '| Hellen | 50 | 315 |'
  prefs: []
  type: TYPE_TB
- en: '| Fedrick | 55 | 335 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.12 – Aggregated dataset – week 1
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregated dataset –** **week 2:**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Average Age | Average Salary |'
  prefs: []
  type: TYPE_TB
- en: '| 42 | 280 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.13 – Aggregated dataset – week 2
  prefs: []
  type: TYPE_NORMAL
- en: Since HR has not shared the actual dataset but the aggregates only, is this
    enough to protect the personal information? In this case, the salary of an individual
    is the personal information.
  prefs: []
  type: TYPE_NORMAL
- en: Anyone could guess the new employee’s salary who joined in week 2 because we
    have the average salary before and after the new employee joined.
  prefs: []
  type: TYPE_NORMAL
- en: New employee salary = (total employees * average salary) after they join – (total
    employees * average salary) before they joined = 5* 280 – 4 * 266.25 = 335
  prefs: []
  type: TYPE_NORMAL
- en: This is called a differencing attack, that is, combining multiple aggregate
    queries to obtain precise information about specific individuals. In this way,
    aggregated datasets have problems with differencing attacks, so it is not completely
    possible to protect the personal information of individuals just by aggregating
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstruction attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s consider a hypothetical dataset from which the following information
    can be inferred:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Three groups with a higher occurrence of the specified disease are identified,
    and data is collected regarding the symptoms and treatments related to that* *particular
    illness.”*'
  prefs: []
  type: TYPE_NORMAL
- en: Using this knowledge, the attacker narrows down the possibilities and identifies
    a potential candidate within the target age group. They compare the individual’s
    characteristics, symptoms, and treatment history, available in public sources,
    with the aggregated data to find a match.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging statistical techniques, machine learning algorithms, or domain-specific
    knowledge, the attacker refines their assumptions and gradually reconstructs the
    medical history of the targeted individual. They can uncover sensitive information,
    such as the specific disease, related comorbidities, previous treatments, and
    potential health risks.
  prefs: []
  type: TYPE_NORMAL
- en: This reconstruction attack demonstrates how an adversary can exploit aggregated
    data, external information, and statistical analysis to infer detailed individual-level
    information. By combining publicly available data, patterns observed in the aggregated
    statistics, and auxiliary knowledge, the attacker can breach the privacy of individuals
    and unveil sensitive personal information.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at another reconstruction attack on public data to understand the
    problem in detail with aggregated datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Database reconstruction attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed earlier, most countries conduct a census survey (a survey of their
    population) every 5 or 10 years and share the aggregated data with orginazations
    as well as in various government portals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through an example of a reconstruction attack with the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Age (years) | Gender | City | Education |'
  prefs: []
  type: TYPE_TB
- en: '| A | 15 | M | Bangalore | Under Grad |'
  prefs: []
  type: TYPE_TB
- en: '| B | 80 | M | Bangalore | Illiterate |'
  prefs: []
  type: TYPE_TB
- en: '| C | 25 | M | Bangalore | Graduated |'
  prefs: []
  type: TYPE_TB
- en: '| D | 45 | F | Bangalore | Graduated |'
  prefs: []
  type: TYPE_TB
- en: '| E | 40 | F | Bangalore | Graduated |'
  prefs: []
  type: TYPE_TB
- en: '| F | 20 | F | Bangalore | Graduated |'
  prefs: []
  type: TYPE_TB
- en: '| G | 15 | F | Bangalore | Graduated |'
  prefs: []
  type: TYPE_TB
- en: Table 3.14 – Reconstruction attack data
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that an intermediatory has access to the preceding original data but
    is only allowed to publish aggregated data publicly. The intermediatory publishes
    the following statistics dataset based on the original data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Gender | Count | Average Age | Median Age |'
  prefs: []
  type: TYPE_TB
- en: '| Male | 3 | 40 | 25 |'
  prefs: []
  type: TYPE_TB
- en: '| Female | 4 | 30 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 7 | 34.28 | 25 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.15 – Statistics dataset
  prefs: []
  type: TYPE_NORMAL
- en: Using the aggregated datasets only, is it possible to find out the approximate
    age of the individuals (consider the case of the male gender)?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to attempt to solve this using a SAT solver. A SAT solver is a program
    whose objective is to solve the Boolean satisfiability problem. I have used the
    Google open source OR-Tools package using Python to solve this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the preceding data, there are 3 men in the dataset with an average
    age of 40 and a median age of 25\. Assuming x, y, and z are the ages of the 3
    men, then y will be 25 (because the median age is 25). The average of x, y, and
    z is 40, so the sum of their ages will equal 120\. Assume that the maximum age
    a person will live is 110 years:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this way, one can solve the approximate value of the original dataset by
    using the aggregate dataset. That is why governments in some countries (such as
    India) don’t allow aggregated datasets to be published, publishing only raw counts.
  prefs: []
  type: TYPE_NORMAL
- en: '| Gender | Count |'
  prefs: []
  type: TYPE_TB
- en: '| Male | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Female | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 7 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.16 – Aggregate dataset
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating database reconstruction attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To mitigate reconstruction attacks and protect the privacy of data, several
    techniques can be employed, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Strong anonymization**: Apply robust anonymization techniques, such as generalization,
    suppression, or aggregation, to the released data. By transforming the data in
    a way that preserves privacy, it becomes more challenging for attackers to reconstruct
    individual records accurately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise addition**: Introduce controlled noise to the released data or queries
    to add an additional layer of privacy protection. By injecting random perturbations,
    the attacker’s ability to uncover the original data or infer precise information
    is diminished.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data perturbation**: Modify the data by perturbing or modifying certain attributes
    or values while maintaining the overall utility of the data. This can make it
    harder for attackers to reconstruct the original information accurately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query restriction**: Limit the types or number of queries that can be made
    to the data to reduce the exposure of sensitive information. By restricting access
    to certain queries or applying query-based privacy mechanisms, the risk of reconstruction
    attacks can be mitigated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools/frameworks to protect privacy with SQL queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Open Diffix** is an open source implementation of Diffix that enables privacy-preserving
    data analysis using SQL. It provides a solution for privacy-conscious organizations
    or researchers who need to analyze sensitive data while safeguarding the privacy
    of individuals. Open Diffix employs strong anonymization techniques to achieve
    this goal, ensuring that the results of database queries do not reveal sensitive
    information about individuals.'
  prefs: []
  type: TYPE_NORMAL
- en: Diffix acts as a SQL proxy between the client (adversary or benign) and the
    database. Diffix returns SQL query results and adds a minimal amount of noise.
    Open Diffix is designed to ensure that sensitive information about individuals
    cannot be inferred from the data released. The key idea behind Open Diffix is
    to introduce controlled noise into query responses or statistical aggregates to
    prevent the identification of individual records.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4– diffix system flow](img/B16573_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4– diffix system flow
  prefs: []
  type: TYPE_NORMAL
- en: 'Open Diffix has the following key features:'
  prefs: []
  type: TYPE_NORMAL
- en: Noise addition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s quickly review some more detail on these two features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Noise addition**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Open Diffix supports adding noise to query results. Noise can be added in different
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixed amount of noise**: In this case, a limited amount of noise is added
    to query results. This is called a privacy budget.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sticky noise**: Each query, when executed the first time, will have random
    noise added, but repeated execution of the same query will give the same results,
    that is, the same amount of noise will be added to the query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proportional noise**: Based on the actual data and distribution of data (time
    series data), the tool will add the corresponding noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suppression**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Open Diffix supports the suppression of certain data (for example, extreme values
    in the case of numbers, and certain text data as well) and returns the results
    of the SQL query. In this way, apart from adding noise, suppression will also
    help to prevent revealing extreme values, and in turn, helps to protect privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Sample use case using Open Diffix on Postgres DB
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s go through a sample database using Postgres DB, execute the SQL queries
    using Open Diffix, and learn how Open Diffix anonymizes and aggregates the results
    by adding noise to protect the privacy of individuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the steps we’ll work through in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: Database creation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: User role creation and grant access
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enabling Diffix on the DB tables to protect privacy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**DB creation**'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we will create two tables (`Employee` and `Department`) and a
    sample database (`sample_db`) as well.
  prefs: []
  type: TYPE_NORMAL
- en: '`sample_db`'
  prefs: []
  type: TYPE_NORMAL
- en: '`schema_owner,` `analyst_trusted, analyst_untrusted`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Employee, Department`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '| Column Name | Column Type | Description | PII Data | Data Masked |'
  prefs: []
  type: TYPE_TB
- en: '| DEPTNO | Integer (primary key) | Department number or ID | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| DEPTNAME | Text | Department name | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| MGRNO | Integer | Department head employee ID | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| MGRNAME | Text | Department head employee Name | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| MGREMAILID | Text | Department head employee Email ID | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| LOCATION | Text | Location of the department | No | No |'
  prefs: []
  type: TYPE_TB
- en: Table 3.17 - Creating a sample database
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '| Column Name | Column Type | Description | PII Data | Data Masked |'
  prefs: []
  type: TYPE_TB
- en: '| EMPNO | Integer (primary key) | Employee number or ID | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| ENAME | Text | Employee name | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| JOB | Text | Job title | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| EMAILID | Text | Employee email ID | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| MGR | Integer | Manager’s employee ID | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| HIREDATE | Date | Join date | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| DEPTNO | integer | Department number or ID | No | No |'
  prefs: []
  type: TYPE_TB
- en: Table 3.18 - Creating a sample database
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the tables are created, insert the sample data as follows and run a sample
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SELECT DEPTNO, DEPTNAME, MGRNO, MGRNAME, MGREMAILID,LOCATION` `FROM DEPARTMENT;`'
  prefs: []
  type: TYPE_NORMAL
- en: '| DEPTNO | DEPTNAME | MGRNO | MGRNAME | MGREMAILID | LOCATION |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Research and Development | 45 | Michael | o64jDLFOfpfCe1fENtwVSAAT |
    India |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | IT Services | 47 | Anna | i1ALN8ApLAaQrccuuGqAZgAP | India |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Sales and Marketing | 43 | Srinivas | yvGkdh5dKQcN41nXWyYOjgAM | India
    |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Accounts and Finance | 44 | Sankara | 4jDkdh5LAN841nX9yYOjQrc2 | India
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Human Resources | 43 | Meena | 2II8Wq_aJDy1hGuNdYMeKQAB | India |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Infrastructures | 42 | Deepa | o2xkrwTjAcHghtiNWX5zSgAF | India |'
  prefs: []
  type: TYPE_TB
- en: Table 3.19 - Inserting the sample data
  prefs: []
  type: TYPE_NORMAL
- en: '`SELECT EMPNO,ENAME,JOB,EMAILID,MGR,HIREDATE,DEPTNO FROM EMPLOYEE` `LIMIT 5;`'
  prefs: []
  type: TYPE_NORMAL
- en: '| EMPNO | ENAME | JOB | EMAILID | MGR | HIREDATE | DEPTNO |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Melissa | Tester | RGk3h5dKccN41nyYOj | 45 | 2020-01-17 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Lzenson | Cybersecurity Engineer | FJDy1hGuNdYMeKQAB | 43 | 2020-06-11
    | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Dyer | Program Manager | Hy1hGuNdYMeKQAthy | 43 | 2020-11-13 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Kesavan | Legal Assistant | HghtiNWX5zStsadgA1 | 42 | 2021-03-11 | 4
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Jonathan | Data Engineer | pfC3e1fE3Nt4wVSAer | 47 | 2021-10-06 | 6 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.20 - Inserting the sample data
  prefs: []
  type: TYPE_NORMAL
- en: '**User creation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Users can have one of the following access levels to a database – three types
    of users are supported in Diffix:'
  prefs: []
  type: TYPE_NORMAL
- en: '**direct**: Direct (non-anonymized) access to data. Restrictions listed in
    this document do not apply in direct mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**anonymized_trusted**: Anonymized access to data. Prevents accidental release
    of personal data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CREATE USER schema_owner WITH` `PASSWORD ''schema_owner'';`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CREATE USER analyst_trusted WITH` `PASSWORD ''analyst_trusted'';`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`CREATE USER analyst_untrusted WITH` `PASSWORD ''analyst_untrusted'';`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Granting access
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Grant access to these three users so that they can select the data and execute
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Defining roles
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Define a role for each user by calling Diffix’s mark role function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Direct access:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CALL` `diffix.mark_role(''schema_owner'', ''direct'');`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Trusted access:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CALL` `diffix.mark_role(''analyst_trusted'', ''anonymized_trusted'');`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Untrusted access:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CALL` `diffix.mark_role(''analyst_untrusted'', ''anonymized_untrusted'');`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Enabling Diffix on the tables
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Enable Diffix on the tables by executing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Run SQL queries and see how privacy is protected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute this query with different users (direct and anonymized trusted and
    untrusted users) and observe the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SELECT deptname, count(*), diffix.count_noise(*) FROM employee GROUP` `BY
    deptname`'
  prefs: []
  type: TYPE_NORMAL
- en: '**For** **direct users:**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Deptname | Count | Count_noise |'
  prefs: []
  type: TYPE_TB
- en: '| Learning and Development | 15959 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Sales and Marketing | 737 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Accounts and Finance | 573 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Human Resources | 435 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Infrastructures | 20448 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| IT Services | 21021 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.21 - Actual results for direct users
  prefs: []
  type: TYPE_NORMAL
- en: '`anonymized_trusted``anonymized_untrusted`**users***:*'
  prefs: []
  type: TYPE_NORMAL
- en: '| Deptname | Count | Count_noise |'
  prefs: []
  type: TYPE_TB
- en: '| Learning and Development | 15957 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Sales and Marketing | 736 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Accounts and Finance | 571 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Human Resources | 436 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Infrastructures | 20446 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| IT Services | 21018 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.22 - Protected results for anonymized users
  prefs: []
  type: TYPE_NORMAL
- en: In the case of direct users, Diffix will provide the actual results. For anonymized
    users, it provides the results by adding noise, so that an adversary will not
    be able to find the actual results.
  prefs: []
  type: TYPE_NORMAL
- en: Subqueries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Any SQL may be executed on the output of anonymizing subqueries as their output
    is no longer considered personal. The query results vary for direct users versus
    anonymized users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**For** **direct users**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Average |'
  prefs: []
  type: TYPE_TB
- en: '| 8578.1428571428571429 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.23 - Average for direct users
  prefs: []
  type: TYPE_NORMAL
- en: '**For** **anonymized_trusted** **and** **anonymized_untrusted** **users**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Average |'
  prefs: []
  type: TYPE_TB
- en: '| 8576.8571428571428571 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.24 - Average for anonymized users
  prefs: []
  type: TYPE_NORMAL
- en: Suppression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s consider a scenario where there are a total of 874 employees in an organization.
    Through queries on a specific table, it becomes apparent that only one employee
    was hired on that day. This observation raises privacy concerns, as without proper
    anonymization, if we possess knowledge of an individual’s hiring date, it is highly
    likely that we can extract additional information about that person by querying
    other tables within the database.
  prefs: []
  type: TYPE_NORMAL
- en: The absence of multiple employees hired on the same day in the table implies
    a potential vulnerability in privacy protection. If an attacker or unauthorized
    entity gains access to the hiring date of an individual, they could exploit this
    knowledge by performing queries on other interconnected tables to infer additional
    personal information. This could compromise the privacy and confidentiality of
    employees’ sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate this risk and preserve privacy, it is crucial to implement proper
    privacy preservation techniques such as suppression.
  prefs: []
  type: TYPE_NORMAL
- en: SQL query to find out the exact dates employees joined
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**For** **direct** **users**:'
  prefs: []
  type: TYPE_NORMAL
- en: '| HireDate | Count |'
  prefs: []
  type: TYPE_TB
- en: '| 2021-12-10 | 723 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020-03-03 | 98 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020-03-01 | 36 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021-01-13 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020-03-19 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.25 - Analysis of singular employee joining dates and privacy risks
  prefs: []
  type: TYPE_NORMAL
- en: From this result, there are only two dates on which a single employee joined
    the organization. This particular pattern poses a privacy risk, as individuals
    with knowledge of this information could potentially link it to other data sources,
    thereby uncovering additional personal details. To safeguard this sensitive information
    and prevent unauthorized inference, it is crucial to provide alternative results
    that differ from the actual ones for `anonymized_trusted` and `anonymized_untrusted`
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Diffix, a privacy-preserving technique, addresses this concern by ensuring that
    untrusted and anonymized users do not receive the actual query results. Instead,
    Diffix suppresses data where the result count is too small to reveal meaningful
    information. By doing so, it effectively conceals the precise details while still
    providing statistical insights and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '| HireDate | Count |'
  prefs: []
  type: TYPE_TB
- en: '| 2021-12-10 | 723 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020-03-03 | 96 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020-03-01 | 37 |'
  prefs: []
  type: TYPE_TB
- en: '| * | 18 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.26 - Suppressed data counts for employee hire dates
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding table, we see that almost all the data has been suppressed.
    Counts for only four dates are displayed. The * symbol represents the hire date
    with respect to individual employee and it is not displayed the exact hire data
    for the 18 employees because of the different hire dates which can revel the sensitive
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Diffix Elm automatically recognizes this and suppresses the dates, merging them
    into a single bin with the value *, which we call the suppression bin. Along with
    suppression, noise is included in the count as well.
  prefs: []
  type: TYPE_NORMAL
- en: By suppressing data with low result counts, Diffix limits the possibility of
    discovering sensitive information through queries. This approach safeguards the
    privacy of individuals and prevents potential linkage attacks, where attackers
    try to correlate multiple datasets to reveal personal details. Diffix’s mechanism
    of altering the query results without compromising statistical accuracy adds an
    extra layer of protection to the data.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we have explored different anonymization techniques and aggregated
    datasets, understanding the advantages and limitations of each approach. However,
    it is important to note that these techniques may not provide complete privacy
    protection for individuals. This realization has led to the development of a new
    technique known as differential privacy, which we will learn about in the upcoming
    section in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy-enhancing technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Privacy-enhancing technologies** (**PETs**) are a set of technologies and
    techniques that help protect sensitive information while still allowing useful
    analysis and processing of the data. Here is a high-level introduction to some
    of the commonly used PETs.'
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a technique that adds a certain amount of noise to a dataset to protect
    the privacy of individual records while still allowing for statistical analysis.
    Differential privacy ensures that any queries made on a dataset do not reveal
    information about specific individuals, making it a powerful tool for protecting
    privacy in large datasets. We will go through differential privacy in this chapter
    and the rest of the PETs in other, subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Federated learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a technique for training machine learning models on data that is distributed
    across multiple devices or servers, without the need to centralize the data. In
    federated learning, the model is trained locally on each device, and then the
    updated model is sent back to a central server for aggregation. This approach
    helps to protect the privacy of individual data while still allowing for useful
    analysis and processing. We will learn more about federated learning in [*Chapter
    7*](B16573_07.xhtml#_idTextAnchor141).
  prefs: []
  type: TYPE_NORMAL
- en: Secure multi-party computation (SMC)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a technique that enables multiple parties to compute a function on their
    respective inputs without revealing their inputs to each other. SMC can be used
    for a range of privacy-preserving computations, including secure voting, privacy-preserving
    data mining, and secure auctions. We will learn more about SMC in [*Chapter 9*](B16573_09.xhtml#_idTextAnchor204).
  prefs: []
  type: TYPE_NORMAL
- en: Homomorphic encryption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a technique that allows computation on encrypted data without decrypting
    it first. This means that data can be processed while still being protected, making
    homomorphic encryption a powerful tool for protecting privacy in data analysis.
    We will learn more about homomorphic encryption in [*Chapter 9*](B16573_09.xhtml#_idTextAnchor204).
  prefs: []
  type: TYPE_NORMAL
- en: Anonymization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the process of removing identifying information from a dataset, such
    as names, addresses, and other personally identifiable information. Anonymization
    helps to protect the privacy of individuals in the dataset while still allowing
    for useful analysis. We have learned about data anonymization already, in the
    previous section.
  prefs: []
  type: TYPE_NORMAL
- en: De-identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a technique for removing identifying information from a dataset, while
    still preserving the usefulness of the data for analysis. De-identification involves
    transforming data in a way that makes it difficult to link it back to an individual,
    but still maintains its statistical properties. One-way hashing is one of the
    techniques to implement the de-identification of data to preserve privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, PETs are an important set of tools for protecting privacy in data analysis,
    allowing organizations to balance the need for useful data analysis with the need
    to protect the privacy of individuals.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now learn more about differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Differential privacy is a concept in data privacy that provides a rigorous framework
    for quantifying and controlling the privacy guarantees of data analysis algorithms.
    It ensures that the presence or absence of an individual’s data does not significantly
    affect the outcome of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, differential privacy is defined using two key concepts: **sensitivity**
    and the **privacy budget**.'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity (Δf)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sensitivity measures the maximum amount that the output of a function f can
    change when a single data point is added to or removed from the dataset. It quantifies
    the impact of an individual’s data on the analysis outcome. The sensitivity is
    typically defined using a metric called the L1 or L2 norm.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy budget (ε)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The privacy budget, also known as the privacy parameter, controls the amount
    of privacy protection provided by a differentially private algorithm. A smaller
    value of ε indicates stronger privacy guarantees. It represents the maximum allowable
    probability that the algorithm’s output will change significantly when a single
    individual’s data is included or excluded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these concepts, we can define differential privacy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A mechanism or algorithm L, which is randomized, provides ε-differential privacy
    if there are any two adjacent datasets D1 and D2 differing by the data of just
    one person, and for any subset of outputs defined* *as S.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematical terms, differential privacy is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pr ( L[D1] ∈ S ) ≤ exp( ε) . Pr. ( L[D2] [[OMML-EQ-4]] S ) + δ
  prefs: []
  type: TYPE_NORMAL
- en: 'Where Pr is the probability and:'
  prefs: []
  type: TYPE_NORMAL
- en: D1 and D2 are the two datasets that differ by a single record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S is all subsets of the randomized algorithm/mechanism L, which is applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ε (epsilon) is a positive real number that controls the privacy loss or referred
    privacy budget.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ε determins much the algorithm can differ between the two databases and captures
    how much privacy loss there is when the algorithm runs on the database.
  prefs: []
  type: TYPE_NORMAL
- en: When ε is set to zero, the answers to both queries will closely resemble each
    other, leading to a reduction in privacy.
  prefs: []
  type: TYPE_NORMAL
- en: This inequality states that the probability of obtaining an output in the set
    S when the algorithm operates on dataset D1 is at most `exp(ε)` times the probability
    of obtaining the same output when the algorithm operates on a neighboring dataset
    D2.
  prefs: []
  type: TYPE_NORMAL
- en: The `exp(ε)` factor quantifies the privacy guarantee, where smaller values of
    ε provide stronger privacy protection.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are variations and extensions of the differential privacy definition
    that incorporate additional parameters, such as the number of queries or the privacy
    loss over multiple analyses.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy provides a rigorous and quantifiable approach to balancing
    privacy and utility, allowing for the analysis of datasets while minimizing the
    risk of privacy breaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Differential privacy has the following properties, which are quite useful in
    overall privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: Privacy guarantee
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Composability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Group privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robustness to auxiliary information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy guarantee
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differential privacy provides a formal guarantee that the output of a query
    does not reveal sensitive information about any individual in the dataset. This
    means that even if an attacker has access to all but one individual’s data, they
    will not be able to determine that individual’s information with high probability.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an example to illustrate privacy guarantees in differential
    privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a scenario where a company wants to release aggregate information
    about the average salary of its employees. However, they also want to protect
    the privacy of individual employees. To achieve this, they decide to use differential
    privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Using differential privacy, the company adds random noise to the data before
    releasing the aggregated result. Suppose the actual average salary of the employees
    is $50,000\. By applying differential privacy, the released result might be slightly
    perturbed, such as $50,150 or $49,850, to ensure privacy protection.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s assume an external attacker tries to identify the salary of a specific
    employee by analyzing the released aggregate information. Due to the added noise,
    the attacker cannot accurately determine the salary of any individual employee.
    The randomness introduced by differential privacy ensures that the released data
    does not reveal precise information about any individual’s salary.
  prefs: []
  type: TYPE_NORMAL
- en: By controlling the level of noise added (based on the epsilon parameter), the
    company can adjust the privacy guarantee. For example, if a smaller epsilon value
    is chosen, the added noise will be higher, providing a stronger privacy guarantee
    but potentially sacrificing some accuracy in the aggregated result.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, differential privacy guarantees that the individual salary
    information remains protected, even when aggregate statistics are released. It
    ensures that no external party can precisely identify the salary of any specific
    employee, thus preserving privacy while still allowing useful insights to be derived
    from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Randomized response
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differential privacy achieves its privacy guarantee by adding random noise to
    the output of a query. The amount of noise is calibrated to balance the privacy
    guarantee with the accuracy of the output.
  prefs: []
  type: TYPE_NORMAL
- en: Randomized response is a method that introduces randomness into the data collection
    process to protect individual privacy. Let’s explain this technique with another
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a survey is conducted to gather sensitive information about a population’s
    illegal activities. The goal is to estimate the percentage of people engaged in
    illegal activities without revealing specific individuals’ responses.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of directly asking individuals whether they participate in illegal activities,
    which may lead to reluctance or dishonesty due to privacy concerns, the surveyors
    employ the randomized response technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: Each participant is given a fair coin (equally likely to land on heads or tails).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The participant flips the coin in private without revealing the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the coin lands on heads, the participant truthfully answers the question
    about illegal activities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the coin lands on tails, the participant provides a random response, independent
    of their actual behavior. For example, they might say “yes” regardless of their
    involvement in illegal activities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By using this randomized response approach, the true response of any individual
    is masked by randomness. The probability of providing a true response or a random
    response is equal (50% in this example), making it difficult to infer an individual’s
    true behavior based on their response.
  prefs: []
  type: TYPE_NORMAL
- en: Now, when the survey results are analyzed, the researchers take into account
    the randomized responses and the overall statistics to estimate the percentage
    of people engaged in illegal activities. This estimation incorporates the randomness
    introduced by the randomized response technique, providing privacy protection
    for individuals while still allowing valuable insights to be derived from the
    aggregated data.
  prefs: []
  type: TYPE_NORMAL
- en: The randomized response technique is just one of the many methods used in differential
    privacy to ensure privacy guarantees. It effectively adds noise and uncertainty
    to individual responses, making it challenging to identify specific individuals
    and their sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: Composability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Composability is a key property of differential privacy, which refers to the
    ability to combine multiple differentially private mechanisms in a way that preserves
    the overall privacy guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: In differential privacy, a mechanism is said to be ε-differentially private
    if the probability of any two neighboring datasets producing the same output is
    at most eε times the probability of the output being produced by one of the datasets
    alone.
  prefs: []
  type: TYPE_NORMAL
- en: The composability property ensures that if we apply multiple ε-differentially
    private mechanisms sequentially, the overall privacy guarantee remains ε-differential
    private. That is, the total privacy loss due to the composition of multiple mechanisms
    is at most the sum of the individual privacy losses.
  prefs: []
  type: TYPE_NORMAL
- en: This property is important because it allows us to design complex systems that
    use differentially private mechanisms for different tasks and still provide strong
    privacy guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can use differentially private mechanisms to collect sensitive
    data from multiple sources and then aggregate the data to obtain useful insights
    while preserving privacy. Let’s now illustrate this with an example.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a company that collects customer data, including their purchase history.
    The company wants to provide personalized recommendations to customers while ensuring
    their privacy using differential privacy. They decide to apply differential privacy
    mechanisms to protect the data. First, the company uses differential privacy to
    generate personalized recommendations for each customer based on their purchase
    history. This analysis introduces random noise to the recommendation algorithm,
    ensuring that no individual’s purchase history is precisely revealed in the recommendations.
    Next, the company wants to perform a separate analysis to determine the average
    amount spent by customers in different age groups. They utilize the same differentially
    private mechanism to compute the average spend, incorporating random noise to
    protect individual spending information.
  prefs: []
  type: TYPE_NORMAL
- en: The key aspect of composability in differential privacy is that the privacy
    guarantees hold even when these analyses are combined. In this example, the privacy
    guarantees are preserved when both the personalized recommendation and the average
    spend analysis are performed on the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose an external attacker attempts to identify the purchase history or spending
    details of a specific customer. Due to the added noise and the composability property
    of differential privacy, the attacker cannot accurately distinguish the true individual
    data from the randomized data generated during the analysis. The privacy guarantees
    provided by differential privacy extend across multiple analyses, protecting individual
    privacy even when multiple queries are performed on the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Composability ensures that the privacy guarantees of differential privacy remain
    intact, allowing organizations to perform various analyses on sensitive data while
    maintaining a consistent level of privacy protection.
  prefs: []
  type: TYPE_NORMAL
- en: However, the composability property assumes that the differentially private
    mechanisms are independent and do not share any information. If the mechanisms
    share information or if there are correlations between them, then the overall
    privacy guarantee may be weaker than the sum of the individual privacy guarantees.
    Therefore, it is important to carefully design and analyze the composition of
    differentially private mechanisms to ensure that privacy is preserved.
  prefs: []
  type: TYPE_NORMAL
- en: Group privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key concepts in differential privacy is group privacy, which refers
    to the protection of privacy for groups of individuals, as opposed to individual
    privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Group privacy in differential privacy is achieved by ensuring that the results
    of data analysis do not reveal information about any specific individual or subgroup
    of individuals.
  prefs: []
  type: TYPE_NORMAL
- en: This is typically accomplished by adding noise to the data in such a way that
    the statistical properties of the data remain largely unchanged, but the specific
    details about any individual or subgroup are obscured.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose a researcher wants to analyze the average income of a certain
    demographic group. In a differentially private framework, the researcher would
    first add random noise to the income data of each individual in the group, such
    that the overall statistical properties of the data are preserved, but the income
    of any individual cannot be determined. The researcher can then calculate the
    average income of the group without revealing any individual’s income.
  prefs: []
  type: TYPE_NORMAL
- en: Group privacy in differential privacy is important because it ensures that even
    if an attacker has some additional information about a subset of individuals,
    they cannot use that information to learn anything about other individuals in
    the group.
  prefs: []
  type: TYPE_NORMAL
- en: This makes differential privacy a powerful tool for protecting the privacy of
    individuals and groups in data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In dataset terminology, privacy can be protected on not only one row but X
    number of rows in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Pr ( L[D1] ∈ S ) ≤ exp( X * ε) . Pr( L[D2] ∈ S ) + δ
  prefs: []
  type: TYPE_NORMAL
- en: X defines the number of persons (rows) in the dataset that need to be protected
    using differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness to auxiliary information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Robustness to auxiliary information is a key property of differential privacy
    that ensures that the privacy guarantees of the framework remain intact even if
    an attacker (adversary) has some additional information about the individuals
    whose data is being analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: In the privacy frameworks that we have studied, such as k-anonymity and ℓ-diversity,
    the privacy of individuals is protected by ensuring that each individual’s data
    is indistinguishable from that of at least k or ℓ-1 other individuals, respectively.
    However, these frameworks do not take into account any additional information
    that an attacker might have about the individuals. For example, an attacker might
    be able to infer an individual’s sensitive information based on their zip code
    or occupation, even if their data is anonymized according to these frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy, on the other hand, provides strong privacy guarantees
    even in the face of such auxiliary information. This is achieved by adding random
    noise to the data in such a way that the noise makes it impossible to determine
    with high confidence whether a particular individual’s data is included in the
    dataset or not, regardless of any additional information an attacker might have.
  prefs: []
  type: TYPE_NORMAL
- en: Even if an attacker knows the exact income of one individual in the group, the
    added noise makes it impossible to determine with high confidence whether that
    individual’s data is included in the dataset or not. Robustness to auxiliary information
    is an important property of differential privacy because it ensures that the privacy
    guarantees of the framework remain intact even in the face of sophisticated attacks
    by adversaries who might have access to additional information about the individuals
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Local and global differential privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differential privacy is a concept that helps to protect the privacy of individuals
    while still allowing useful analysis to be performed on data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Local differential privacy** (**LDP**) and **global differential privacy**
    (**GDP**) are two different approaches to achieving differential privacy. LDP
    is a method of differential privacy that involves adding noise to individual data
    points before they are shared. This means that each data point is perturbed before
    it is released, which helps to protect the privacy of individuals in the dataset.
    LDP is often used in situations where individuals are providing their own data,
    such as in mobile apps, and it ensures that the data is kept private even from
    the server.'
  prefs: []
  type: TYPE_NORMAL
- en: GDP, on the other hand, involves adding noise to the aggregate data. This means
    that privacy protection is applied to the overall results of an analysis rather
    than to individual data points. GDP is often used in situations where the data
    is collected centrally, such as in a hospital or government agency, and the privacy
    of the individuals in the dataset must be protected.
  prefs: []
  type: TYPE_NORMAL
- en: Both LDP and GDP have their own advantages and disadvantages. LDP is more effective
    at protecting the privacy of individual data points, but it can be less accurate
    when it comes to analyzing the data. GDP, on the other hand, is more accurate
    but may not provide as strong privacy protection for individual data points. Ultimately,
    the choice between LDP and GDP will depend on the specific situation and the trade-offs
    between accuracy and privacy that are required.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, we have explored the realm of privacy in data analysis and the
    importance of privacy-preserving techniques. We discussed concepts such as anonymization,
    k-anonymity, t-closeness, and ℓ-diversity, which play a crucial role in safeguarding
    privacy during data analysis. However, we also acknowledged the limitations of
    these techniques. Furthermore, we delved into a high-level overview of privacy-enhancing
    technologies, with a specific focus on differential privacy. We explored essential
    concepts such as privacy loss, privacy budgets, and differential privacy mechanisms,
    understanding how they contribute to privacy preservation. We also examined the
    implementation of differential privacy properties and their significance in ensuring
    privacy guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, our focus will shift toward differential privacy algorithms.
    We will get an overview of these algorithms and explore the importance of concepts
    such as sensitivity and clipping in the context of differential privacy. Additionally,
    we will delve into the generation of aggregates using differential privacy, understanding
    how this approach provides statistical insights while preserving privacy. By delving
    deeper into these topics, we will enhance our understanding of the practical applications
    of differential privacy and how it can be effectively implemented to balance privacy
    and data utility.
  prefs: []
  type: TYPE_NORMAL
