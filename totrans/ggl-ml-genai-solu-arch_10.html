<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer132" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-170"><a id="_idTextAnchor232" class="calibre6 pcalibre pcalibre1"/>8</h1>
<h1 id="_idParaDest-171" class="calibre5"><a id="_idTextAnchor233" class="calibre6 pcalibre pcalibre1"/>Hyperparameters and Optimization</h1>
<p class="calibre3">We introduced the concepts of hyperparameters and hyperparameter optimization (or tuning) in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>. In this chapter, we will dive into these concepts in more detail, and we will use Google Cloud products such as Vertex AI Vizier to define and run hyperparameter <span>tuning jobs.</span></p>
<p class="calibre3">Following our established pattern, we will begin by covering some prerequisites that are required for the hands-on activities in this chapter. Then, we cover some important basic concepts that relate to the content covered in this chapter, and finally, we perform hands-on activities that teach you how to apply those concepts in <span>real-world scenarios.</span></p>
<p class="calibre3">This chapter covers the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">Prerequisites and <span>basic concepts</span></li>
<li class="calibre8">What <span>are hyperparameters?</span></li>
<li class="calibre8"><span>Hyperparameter optimization</span></li>
<li class="calibre8">Hands-on: performing hyperparameter tuning in <span>Vertex AI</span></li>
</ul>
<p class="calibre3">Let’s begin by reviewing the prerequisites for <span>this chapter.</span></p>
<h1 id="_idParaDest-172" class="calibre5"><a id="_idTextAnchor234" class="calibre6 pcalibre pcalibre1"/>Prerequisites</h1>
<p class="calibre3">The steps in this section need to be completed before we can perform the primary activities in <span>this chapter.</span></p>
<h2 id="_idParaDest-173" class="calibre9"><a id="_idTextAnchor235" class="calibre6 pcalibre pcalibre1"/>Enabling the Artifact Registry API</h2>
<p class="calibre3">We’re going to create <a id="_idIndexMarker889" class="calibre6 pcalibre pcalibre1"/>Docker images in order to run our custom code in conjunction with the Google Cloud Vertex AI Vizier service. The Google Cloud Artifact Registry is a fully managed artifact repository that we can use for storing our container images. It can be<a id="_idIndexMarker890" class="calibre6 pcalibre pcalibre1"/> seen as the next generation of the <strong class="bold">Google Cloud Container Registry</strong> (<strong class="bold">GCR</strong>) that can be used to store artifacts such as Java JAR files, Node.js modules, Python wheels, Go modules, Maven artifacts, and npm packages (in addition to Docker images, which were already supported <span>in GCR).</span></p>
<p class="calibre3">To enable the Artifact Registry API, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to <strong class="bold">Google Cloud services menu</strong> → <strong class="bold">APIs &amp; Services</strong> → <span><strong class="bold">Library</strong></span></li>
<li class="calibre8">Search for <strong class="source-inline">Artifact Registry</strong> in the <span>search box.</span></li>
<li class="calibre8">Select the API in the list <a id="_idIndexMarker891" class="calibre6 pcalibre pcalibre1"/><span>of results.</span></li>
<li class="calibre8">On the page that displays information about the API, <span>click </span><span><strong class="bold">Enable</strong></span><span>.</span></li>
</ol>
<p class="calibre3">Next, let’s set up the required permissions for the steps in <span>this chapter.</span></p>
<h2 id="_idParaDest-174" class="calibre9"><a id="_idTextAnchor236" class="calibre6 pcalibre pcalibre1"/>Creating an AI/ML service account</h2>
<p class="calibre3">In <a href="B18143_06.xhtml#_idTextAnchor187" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 6</em></span></a>, we created a service<a id="_idIndexMarker892" class="calibre6 pcalibre pcalibre1"/> account for using Google Cloud’s data processing services. In this chapter, we will create a service account that will be used in our hyperparameter tuning job for administering resources in Google Cloud <span>Vertex AI.</span></p>
<p class="calibre3">Perform the following steps to create the required <span>service account:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to <strong class="bold">Google Cloud services menu</strong> →<strong class="bold"> IAM &amp; Admin</strong> → <span><strong class="bold">Service accounts</strong></span><span>.</span></li>
<li class="calibre8">Select <strong class="bold">Create </strong><span><strong class="bold">service account</strong></span><span>.</span></li>
<li class="calibre8">For the service account name, <span>enter </span><span><strong class="source-inline">ai-ml-sa</strong></span><span>.</span></li>
<li class="calibre8">Click <strong class="bold">Create </strong><span><strong class="bold">and Continue</strong></span><span>.</span></li>
<li class="calibre8">In the section titled <strong class="bold">Grant this service account access to project</strong>, add the roles shown in <span><em class="italic">Figure 8</em></span><span><em class="italic">.1</em></span><span>.</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer129">
<img alt="Figure 8.1: AI/ML service account permissions" src="image/B18143_08_1.jpg" class="calibre129"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 8.1: AI/ML service account permissions</p>
<ol class="calibre7">
<li value="6" class="calibre8"><span>Select </span><span><strong class="bold">Done</strong></span><span>.</span></li>
</ol>
<p class="calibre3">Our service account is now<a id="_idIndexMarker893" class="calibre6 pcalibre pcalibre1"/> ready to be used later in <span>this chapter.</span></p>
<p class="calibre3">Now that we’ve covered the prerequisites, let’s discuss some concepts that we need to understand before performing the hands-on activities in <span>this chapter.</span></p>
<h1 id="_idParaDest-175" class="calibre5"><a id="_idTextAnchor237" class="calibre6 pcalibre pcalibre1"/>Concepts</h1>
<p class="calibre3">This section describes concepts that underpin the practical activities we will cover in <span>this chapter.</span></p>
<h2 id="_idParaDest-176" class="calibre9"><a id="_idTextAnchor238" class="calibre6 pcalibre pcalibre1"/>Model evaluation metrics used in this chapter</h2>
<p class="calibre3">We’ve already discussed the topic of model evaluation metrics in previous chapters. We first introduced the concept in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, where we briefly discussed metrics such as the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) for <a id="_idIndexMarker894" class="calibre6 pcalibre pcalibre1"/>regression use cases and accuracy for classification use cases. In <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a>, we used functions in scikit-learn to calculate<a id="_idIndexMarker895" class="calibre6 pcalibre pcalibre1"/> some of these metrics for the models we created, and we suggested looking up additional metrics as a supplemental learning activity at the end of <span>that chapter.</span></p>
<p class="calibre3">In this chapter, we will train models for a classification use case, and we will introduce some additional metrics to evaluate our models. The main metric we will use is something called <strong class="bold">AUC ROC</strong>, which stands for <strong class="bold">area under the receiver operating characteristic curve</strong>. That sounds like a lot, but don’t worry, we will explain this metric in more detail in this section. In <a id="_idIndexMarker896" class="calibre6 pcalibre pcalibre1"/>order to do so, we need to first introduce some concepts and simpler metrics that are used in calculating the <span>AUC ROC.</span></p>
<p class="calibre3">Note that in a binary classification use case, the model needs to predict one of two possible outcomes for each data point in the dataset, true or false, also referred to as <strong class="bold">positive</strong> or <strong class="bold">negative</strong>. They are usually represented by 1 <span>and 0.</span></p>
<p class="calibre3">Models are rarely perfect, so they will sometimes make mistakes. Let’s take a look at the possible outcomes of a binary <a id="_idIndexMarker897" class="calibre6 pcalibre pcalibre1"/>classification <span>model’s predictions.</span></p>
<h3 class="calibre11">True positives, false positives, true negatives, and false negatives</h3>
<p class="calibre3">A binary classification model’s predictions generally have four <span>possible outcomes:</span></p>
<ul class="calibre16">
<li class="calibre8">When our model<a id="_idIndexMarker898" class="calibre6 pcalibre pcalibre1"/> predicts something to be true (or positive) and it really is true (or positive), we call<a id="_idIndexMarker899" class="calibre6 pcalibre pcalibre1"/> this a <strong class="bold">true </strong><span><strong class="bold">positive</strong></span><span> (</span><span><strong class="bold">TP</strong></span><span>)</span></li>
<li class="calibre8">When our model predicts <a id="_idIndexMarker900" class="calibre6 pcalibre pcalibre1"/>something to be true (or positive) but really it is<a id="_idIndexMarker901" class="calibre6 pcalibre pcalibre1"/> false (or negative), we call this a <strong class="bold">false </strong><span><strong class="bold">positive</strong></span><span> (</span><span><strong class="bold">FP</strong></span><span>)</span></li>
<li class="calibre8">When our model predicts something to be <a id="_idIndexMarker902" class="calibre6 pcalibre pcalibre1"/>false (or negative) and it really is false (or negative), we <a id="_idIndexMarker903" class="calibre6 pcalibre pcalibre1"/>call this a <strong class="bold">true </strong><span><strong class="bold">negative</strong></span><span> (</span><span><strong class="bold">TN</strong></span><span>)</span></li>
<li class="calibre8">When our model predicts<a id="_idIndexMarker904" class="calibre6 pcalibre pcalibre1"/> something to be false (or negative) but it is really true (or positive), we call <a id="_idIndexMarker905" class="calibre6 pcalibre pcalibre1"/>this a <strong class="bold">false </strong><span><strong class="bold">negative</strong></span><span> (</span><span><strong class="bold">FN</strong></span><span>)</span></li>
</ul>
<p class="calibre3">Let’s take a look at how these outcomes are related to each other in <span>more detail.</span></p>
<h3 class="calibre11">Confusion matrix</h3>
<p class="calibre3">The preceding concepts<a id="_idIndexMarker906" class="calibre6 pcalibre pcalibre1"/> can be represented visually in something called a confusion<a id="_idIndexMarker907" class="calibre6 pcalibre pcalibre1"/> matrix, which is <a id="_idIndexMarker908" class="calibre6 pcalibre pcalibre1"/>demonstrated in <span><em class="italic">Table 8.1</em></span><span>.</span></p>
<table class="no-table-style" id="table001-2">
<colgroup class="calibre12">
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
</colgroup>
<tbody class="calibre14">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold">Predicted negative</strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold">Predicted positive</strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span>Actual negative</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span>TN</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span>FP</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold">Actual positive</strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span>FN</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span>TP</span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 8.1: Confusion matrix</p>
<p class="calibre3">We can use the preceding concepts to calculate metrics that measure how well our models are performing when trying to accurately identify positive or negative data points in our dataset. We define these <span>metrics next.</span></p>
<h3 class="calibre11">True positive rate</h3>
<p class="calibre3">The <strong class="bold">true positive rate</strong> (<strong class="bold">TPR</strong>) represents a count of all of the data points in our dataset that our model correctly <a id="_idIndexMarker909" class="calibre6 pcalibre pcalibre1"/>predicted as positive compared against all of the data points in the dataset that are positive, including<a id="_idIndexMarker910" class="calibre6 pcalibre pcalibre1"/> any data points that our model erroneously predicted to be negative (i.e., our model said they were negative, even though they were positive, which means it was a <span>false negative).</span></p>
<p class="calibre3">The formula to calculate TPR is TPR = TP / (TP + <span>FN)</span></p>
<p class="calibre3">TPR is also<a id="_idIndexMarker911" class="calibre6 pcalibre pcalibre1"/> referred <a id="_idIndexMarker912" class="calibre6 pcalibre pcalibre1"/>to as <strong class="bold">recall</strong> <span>or </span><span><strong class="bold">sensitivity</strong></span><span>.</span></p>
<h3 class="calibre11">False positive rate</h3>
<p class="calibre3">The <strong class="bold">false positive rate</strong> (<strong class="bold">FPR</strong>) is the ratio of <a id="_idIndexMarker913" class="calibre6 pcalibre pcalibre1"/>false positives (the number of negative instances incorrectly predicted as positive by<a id="_idIndexMarker914" class="calibre6 pcalibre pcalibre1"/> our model) to the sum of false positives and true negatives (the number of negative instances correctly predicted by the model). In other words, it’s the proportion of actual negatives that are incorrectly identified <span>as positive.</span></p>
<p class="calibre3">The formula to calculate FPR is FPR = FP / (FP + <span>TN)</span></p>
<p class="calibre3">TPR is also<a id="_idIndexMarker915" class="calibre6 pcalibre pcalibre1"/> referred to as <span><strong class="bold">fall out</strong></span><span>.</span></p>
<h3 class="calibre11">True negative rate</h3>
<p class="calibre3">The <strong class="bold">true negative rate</strong> (<strong class="bold">TNR</strong>) is described<a id="_idIndexMarker916" class="calibre6 pcalibre pcalibre1"/> very similarly to how we described the TPR, just with positive and negative switched around. That is, the TNR represents a count of all of the data points in our dataset that our<a id="_idIndexMarker917" class="calibre6 pcalibre pcalibre1"/> model correctly predicted as negative compared against all of the data points in the dataset that are negative, including any data points that our model erroneously predicted to be positive (i.e., our model said they were positive, even though they were negative, which means it was a <span>false positive).</span></p>
<p class="calibre3">The formula to calculate TNR is TNR = TN / (TN + <span>FP)</span></p>
<p class="calibre3">TPR is also referred<a id="_idIndexMarker918" class="calibre6 pcalibre pcalibre1"/> to <span>as </span><span><strong class="bold">specificity</strong></span><span>.</span></p>
<h3 class="calibre11">False negative rate</h3>
<p class="calibre3">The <strong class="bold">false negative rate</strong> (<strong class="bold">FNR</strong>) is described very similarly to how we described the FPR, just with positive and negative<a id="_idIndexMarker919" class="calibre6 pcalibre pcalibre1"/> switched around. It is the ratio of wrongly predicted negative observations to the actual <a id="_idIndexMarker920" class="calibre6 pcalibre pcalibre1"/>positives. In other words, it’s the proportion of actual positives that are incorrectly identified <span>as negative.</span></p>
<p class="calibre3">The formula to calculate FNR is FNR = FN / (FN + <span>TP)</span></p>
<h3 class="calibre11">Precision</h3>
<p class="calibre3">Precision is the ratio of correctly predicted<a id="_idIndexMarker921" class="calibre6 pcalibre pcalibre1"/> positive observations to the total predicted positives. In other words, out of all the instances the model predicted as<a id="_idIndexMarker922" class="calibre6 pcalibre pcalibre1"/> positive, how many were <span>actually positive?</span></p>
<p class="calibre3">The formula to calculate FNR is P = TP / (TP + <span>FP)</span></p>
<p class="calibre3">When I first started learning all of this stuff, I wondered why there were so many different metrics for measuring slightly different aspects of binary classification <span>model performance.</span></p>
<p class="calibre3">There are at least a couple of reasons <span>for this:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Statistical basis</strong>: These metrics are natural outcomes from statistical analysis in binary classification <span>use cases</span></li>
<li class="calibre8"><strong class="bold">Trial and error</strong>: Each metric may be more important than the others, based on the desired outcomes of the <span>use case</span></li>
</ul>
<p class="calibre3">Consider the following example for <a id="_idIndexMarker923" class="calibre6 pcalibre pcalibre1"/>the second point. If you’re trying to predict credit card fraud, you may want to maximize the sensitivity of your model, which would reduce the number of false negatives as much as possible, even if that ends up causing more false positives. In other words, it’s better to accidentally flag a transaction as fraudulent even if it’s not fraudulent than to accidentally allow a fraudulent transaction <span>to occur.</span></p>
<p class="calibre3">On the other hand, if you’re creating a spam filter, you would probably prefer allowing a few spam emails to accidentally reach your inbox (false negatives) than having valid emails flagged as spam (<span>false positives).</span></p>
<p class="calibre3">The preceding metrics are often too simple to use independently, and you will therefore usually want to find a more complex combination of metrics that provides a more balanced outcome. Even in the credit card fraud use case, too many false positives would be disruptive and frustrating for credit card customers. The balance depends on the threshold you specify (between 0 and 1) for determining whether something is positive or negative. For example, a low threshold results in more positives, while a higher threshold results in more negatives. This brings us to more advanced metrics such as F1 score and AUC ROC, which we <span>describe next.</span></p>
<h3 class="calibre11">F1 score</h3>
<p class="calibre3">F1 score is<a id="_idIndexMarker924" class="calibre6 pcalibre pcalibre1"/> defined as the <strong class="bold">harmonic mean</strong> of precision<a id="_idIndexMarker925" class="calibre6 pcalibre pcalibre1"/> and recall, which is<a id="_idIndexMarker926" class="calibre6 pcalibre pcalibre1"/> calculated using the <span>following formula:</span></p>
<p class="calibre3">F1 = 2 * (precision * recall) / (precision + <span>recall)</span></p>
<p class="calibre3">The F1 score is particularly useful when you care more about the positive class and you want to balance precision <span>and recall.</span></p>
<h3 class="calibre11">AUC ROC</h3>
<p class="calibre3">To understand AUC ROC, let’s first break <a id="_idIndexMarker927" class="calibre6 pcalibre pcalibre1"/>down its name. The <strong class="bold">receiver operating characteristic</strong> (<strong class="bold">ROC</strong>) is a pretty fancy name for a curve that is generated by plotting the TPR against the FPR at various classification threshold<a id="_idIndexMarker928" class="calibre6 pcalibre pcalibre1"/> settings, as depicted in <span><em class="italic">Figure 8</em></span><span><em class="italic">.2</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer130">
<img alt="Figure 8.2: AUC ROC" src="image/B18143_08_2.jpg" class="calibre130"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 8.2: AUC ROC</p>
<p class="calibre3">The <strong class="bold">area under the curve</strong> (<strong class="bold">AUC</strong>) is a measure <a id="_idIndexMarker929" class="calibre6 pcalibre pcalibre1"/>of the entire two-dimensional area underneath the ROC curve from (0, 0) to (1, 1), as represented by the blue area in <span><em class="italic">Figure 8</em></span><em class="italic">.2</em>. The AUC provides an aggregate measure of performance across all possible classification thresholds, and the objective is to maximize the area under the<a id="_idIndexMarker930" class="calibre6 pcalibre pcalibre1"/> curve, so in the best possible scenario, the curve would stretch up into the top-left corner, filling up the <span>entire graph.</span></p>
<p class="calibre3">Let’s take a look at how to interpret the AUC ROC score values in <span>more detail:</span></p>
<ul class="calibre16">
<li class="calibre8">An AUC ROC score of 1.0 means that the model is able to perfectly distinguish between all the positive and the negative data points correctly, in which case it has no false negatives and no false positives (i.e., <span>no mistakes).</span></li>
<li class="calibre8">An AUC ROC score of 0.5 means that the model is not able to accurately distinguish between positive and negative data points and performs no better than <span>random guessing.</span></li>
<li class="calibre8">An AUC ROC score of less than 0.5 means that the model is performing worse than random guessing, predicting negatives as positives and positives <span>as negatives.</span></li>
</ul>
<p class="calibre3">Understanding these <a id="_idIndexMarker931" class="calibre6 pcalibre pcalibre1"/>metrics is important because they are generally what our ML algorithms are trying to optimize. In the next section, we will discuss <a id="_idIndexMarker932" class="calibre6 pcalibre pcalibre1"/>hyperparameters and hyperparameter tuning and we will see that these objective metrics form the fundamental goal of our <span>tuning jobs.</span></p>
<h1 id="_idParaDest-177" class="calibre5"><a id="_idTextAnchor239" class="calibre6 pcalibre pcalibre1"/>What are hyperparameters?</h1>
<p class="calibre3">As we discussed in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, hyperparameters are parameters that define aspects of how our model training jobs run. They are not<a id="_idIndexMarker933" class="calibre6 pcalibre pcalibre1"/> the parameters in the dataset from which our models learn but rather external configuration options related to how the model training process is executed. They influence how the resulting models perform and they represent higher-level properties of the model, such as its complexity or how quickly it <span>should learn.</span></p>
<p class="calibre3">The following are examples of hyperparameters that we’ve already discussed in <span>this book:</span></p>
<ul class="calibre16">
<li class="calibre8">In our <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a> discussion of hyperparameters, we covered examples such as learning rate and the number <span>of epochs</span></li>
<li class="calibre8">In <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a>, we configured the number of clusters as a hyperparameter for our K-means algorithm and we configured hyperparameters for our tree-based models, such as the maximum depth of <span>our trees</span></li>
<li class="calibre8">We talked about regularization in <a href="B18143_07.xhtml#_idTextAnchor215" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 7</em></span></a>, and regularization parameters are another example <span>of hyperparameters.</span></li>
</ul>
<p class="calibre3">There are many more types of hyperparameters for different kinds of algorithms, and we will encounter more as we progress through <span>this book.</span></p>
<h1 id="_idParaDest-178" class="calibre5"><a id="_idTextAnchor240" class="calibre6 pcalibre pcalibre1"/>Hyperparameter optimization</h1>
<p class="calibre3">How do we know what kinds of<a id="_idIndexMarker934" class="calibre6 pcalibre pcalibre1"/> hyperparameters to use and what their values should be? Hyperparameters can be chosen based on domain knowledge, experience, or trial and error, but to most efficiently choose the best hyperparameters, we can use a process called hyperparameter optimization, or hyperparameter tuning, which is <a id="_idIndexMarker935" class="calibre6 pcalibre pcalibre1"/>a systematic process that can be implemented via different mechanisms that we will discuss next. Ultimately, the goal of hyperparameter optimization is to tune the hyperparameters of a model to achieve the best performance as measured by running it against a validation set, which is a subset of our <span>source dataset.</span></p>
<h2 id="_idParaDest-179" class="calibre9"><a id="_idTextAnchor241" class="calibre6 pcalibre pcalibre1"/>Methods for optimizing hyperparameter values</h2>
<p class="calibre3">In <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, we described hyperparameter tuning mechanisms such as grid search, random search, and Bayesian <a id="_idIndexMarker936" class="calibre6 pcalibre pcalibre1"/>optimization, summarized here as a <span>quick refresher:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Grid search</strong>: This is an exhaustive<a id="_idIndexMarker937" class="calibre6 pcalibre pcalibre1"/> search of the entire hyperparameter space (i.e., it tries out every possible combination of all hyperparameter values). This is usually impractical and unnecessarily <span>computationally expensive.</span></li>
<li class="calibre8"><strong class="bold">Random search</strong>: The random search<a id="_idIndexMarker938" class="calibre6 pcalibre pcalibre1"/> approach uses a subsampling technique in which hyperparameter values are selected at random for each training job experiment. This will not result in all possible values of every hyperparameter being tested, but it can often be quite an efficient method for finding an effective set of <span>hyperparameter values.</span></li>
<li class="calibre8"><strong class="bold">Bayesian optimization</strong>: This uses an optimization algorithm, and it is something that is provided as a<a id="_idIndexMarker939" class="calibre6 pcalibre pcalibre1"/> managed service in Google Cloud <span>Vertex AI.</span></li>
</ul>
<p class="calibre3">The following are some additional hyperparameter tuning mechanisms that exist in <span>the industry:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Gradient-based optimization</strong>: This method uses Gradient Descent, which we’ve already covered in depth<a id="_idIndexMarker940" class="calibre6 pcalibre pcalibre1"/> earlier in this book. These methods are often used when training neural networks. We provide a separate section later in this book that describes how to train neural networks <span>in detail.</span></li>
<li class="calibre8"><strong class="bold">Evolutionary algorithms</strong>: These are <strong class="bold">population-based</strong> optimization algorithms loosely modeled on the process of evolutionary natural selection. The term “population-based” refers to<a id="_idIndexMarker941" class="calibre6 pcalibre pcalibre1"/> the practice of building a pool (or population) of potential candidates. In this context, each candidate in the population represents a different set of hyperparameters, and candidates are evaluated based on their validation performance. The best-performing ones are then selected to produce “offspring” for the next generation. These algorithms are also more likely<a id="_idIndexMarker942" class="calibre6 pcalibre pcalibre1"/> to be used for advanced use cases such as neural networks, where the hyperparameter search space can be large and complex, and it can be expensive to evaluate the performance of <span>individual solutions.</span></li>
<li class="calibre8"><strong class="bold">Automated machine learning</strong> (<strong class="bold">AutoML</strong>) <strong class="bold">systems</strong>: We discussed the process of AutoML in previous <a id="_idIndexMarker943" class="calibre6 pcalibre pcalibre1"/>chapters. It can be used to automate the entire ML lifecycle, including <span>hyperparameter tuning.</span></li>
</ul>
<p class="calibre3">In any case, the general tuning process works <span>as follows:</span></p>
<ol class="calibre7">
<li class="calibre8">Split your source dataset into <span>three subsets:</span><ol class="calibre77"><li class="alphabets"><strong class="bold">Training dataset</strong>: Used to train <span>the model</span></li><li class="alphabets"><strong class="bold">Validation dataset</strong>: Used to evaluate each combination of hyperparameters during the <span>tuning process</span></li><li class="alphabets"><strong class="bold">Test dataset</strong>: Used to test the <span>final model</span></li></ol></li>
<li class="calibre8">Select which type of machine learning model we want to create (e.g., linear regression, decision tree, neural network). This determines which specific hyperparameters can <span>be tuned.</span></li>
<li class="calibre8">Set an initial range or grid of hyperparameters and values. This can be based on domain knowledge or research or we could just start with a random broad range and refine it <span>over time.</span></li>
<li class="calibre8">Choose a method for searching through the model’s hyperparameter space (e.g., random search, <span>Bayesian optimization).</span></li>
<li class="calibre8">For each combination of hyperparameters, fit the model to the training data and evaluate its performance by testing it against the validation data and measuring the appropriate objective metrics for the chosen type of model (e.g., MSE for regression, AUC ROC for <span>binary classification).</span></li>
<li class="calibre8">Once all combinations have been evaluated, choose the combination of hyperparameter values that resulted in the best <span>model performance.</span></li>
<li class="calibre8">Train a final model<a id="_idIndexMarker944" class="calibre6 pcalibre pcalibre1"/> using those hyperparameters and test the resulting model using the <strong class="source-inline">test</strong> dataset to confirm the model’s ability to generalize to <span>unseen data.</span></li>
</ol>
<p class="calibre3">Note that finding the best set of hyperparameters and values could require iterating through the outlined steps hundreds or even thousands of times, which would be extremely time-consuming or potentially impossible to perform manually. This is why hyperparameter tuning jobs, which automate the steps, are <span>often required.</span></p>
<p class="calibre3">Now that we’ve covered many of the important theoretical concepts related to hyperparameter tuning, it’s time for us to shift our focus to the practical implementation of <span>these concepts.</span></p>
<h1 id="_idParaDest-180" class="calibre5"><a id="_idTextAnchor242" class="calibre6 pcalibre pcalibre1"/>Hands-on: performing hyperparameter tuning in Vertex AI</h1>
<p class="calibre3">Considering that Google Cloud Vertex AI <a id="_idIndexMarker945" class="calibre6 pcalibre pcalibre1"/>provides tools that make it easy for us to implement every step in the data science project lifecycle, this gives us the perfect environment to put our knowledge into practice and start implementing<a id="_idIndexMarker946" class="calibre6 pcalibre pcalibre1"/> hyperparameter tuning jobs. In fact, as we mentioned previously, Vertex AI provides a tool called Vizier that is specialized for the purpose of automating hyperparameter tuning jobs, which we will dive into in more <span>detail next.</span></p>
<h2 id="_idParaDest-181" class="calibre9"><a id="_idTextAnchor243" class="calibre6 pcalibre pcalibre1"/>Vertex AI Vizier</h2>
<p class="calibre3">Vertex AI Vizier is a service in Google Cloud that automates the hyperparameter tuning process that we outlined in the previous section of this chapter. In this section, we discuss some terminology used by the Vertex AI Vizier service and we describe some details on how it <a id="_idIndexMarker947" class="calibre6 pcalibre pcalibre1"/>works. Then, we will actually use it in our hands-on activities to implement hyperparameter <span>tuning jobs.</span></p>
<h3 class="calibre11">Vertex AI Vizier terminology</h3>
<p class="calibre3">Google Cloud uses some <a id="_idIndexMarker948" class="calibre6 pcalibre pcalibre1"/>terminology that is specific to the Vertex AI Vizier service. We will briefly describe some important terms here and relate them back to the generic concepts we covered earlier in <span>this chapter.</span></p>
<h4 class="calibre20">Studies, study configurations, and trials</h4>
<p class="calibre3">In Vertex AI Vizier, a <strong class="bold">study</strong> represents the overall objective we are trying to achieve and all of the steps and other details involved in <a id="_idIndexMarker949" class="calibre6 pcalibre pcalibre1"/>working towards that objective. For example, if we look at the general tuning process steps we outlined in the <em class="italic">Methods for optimizing hyperparameter values</em> section of this chapter, a study encapsulates all of those steps. A <strong class="bold">study configuration</strong> is the actual configuration object that <a id="_idIndexMarker950" class="calibre6 pcalibre pcalibre1"/>contains all of the details of our study, such as the objective metric that we want the study to <a id="_idIndexMarker951" class="calibre6 pcalibre pcalibre1"/>optimize, what parameters to test, and what kind of parameter search method <span>to use.</span></p>
<p class="calibre3">A <strong class="bold">trial</strong> is an individual<a id="_idIndexMarker952" class="calibre6 pcalibre pcalibre1"/> experiment in our study, or a single iteration in the tuning process (i.e., a single training and evaluation job that uses a specific set of hyperparameter values). A study<a id="_idIndexMarker953" class="calibre6 pcalibre pcalibre1"/> will run many trials when working toward our <span>specified objective.</span></p>
<p class="calibre3">After you create a study, Vertex AI Vizier will start running trials on its own. In each test, a different set of hyperparameters will be used. Vertex AI Vizier will keep track of the results of each run and use this knowledge to choose the best set of hyperparameters (it will automatically stop running trials when it has found the best set of hyperparameters). Vizier will also summarize all of the trials and rank them according to which ones performed best with regard to the objective metric. Then, we can train our ML model with the hyperparameters from the <span>top-ranking trial.</span></p>
<p class="calibre3">Now that we’ve gotten the terminology covered, let’s dive into the <span>hands-on activities!</span></p>
<h3 class="calibre11">Use case and dataset</h3>
<p class="calibre3">In this section, we will <a id="_idIndexMarker954" class="calibre6 pcalibre pcalibre1"/>develop an XGBoost model to detect credit card fraud using the <strong class="source-inline">Credit Card Fraud Detection</strong> dataset available on <span>Kaggle (</span><a href="https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud" class="calibre6 pcalibre pcalibre1"><span>https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud</span></a><span>).</span></p>
<h3 class="calibre11">Implementation</h3>
<p class="calibre3">We’re going to use Jupyter Notebook for the hands-on activities in this chapter, and we’re going to customize the <a id="_idIndexMarker955" class="calibre6 pcalibre pcalibre1"/>contents of the notebook, so we will use a <strong class="source-inline">user-managed</strong> notebook instance. We can use the same Vertex AI Workbench user-managed notebook instance that we created in <a href="B18143_07.xhtml#_idTextAnchor215" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 7</em></span></a>. Please open JupyterLab on that notebook instance. In the directory explorer on the left side of the screen, navigate to the <strong class="source-inline">Chapter-08</strong> directory and open the <strong class="source-inline">vizier-hpo.ipynb</strong> notebook. You can choose <strong class="source-inline">Python (Local)</strong> as the kernel. Again, you can run each cell in the notebook by selecting the cell and pressing <em class="italic">Shift</em> + <em class="italic">Enter</em> on your keyboard. In addition to the relevant code, the notebook contains markdown text that describes what the code <span>is doing.</span></p>
<h3 class="calibre11">How our hyperparameter tuning job works</h3>
<p class="calibre3">Using Vertex AI Vizier for hyperparameter<a id="_idIndexMarker956" class="calibre6 pcalibre pcalibre1"/> tuning involves several steps that we implement in the model. Let’s take a look at the salient steps in <span>the process:</span></p>
<ol class="calibre7">
<li class="calibre8">First, we create a training application, which consists of a Python script that trains our model with the given hyperparameters. This script must also track and report the performance of the model when testing it on the validation set so that Vertex AI Vizier can use those performance metrics to determine the best hyperparameters. For this reason, we use the <strong class="source-inline">cloudml-hypertune</strong> Python library in our code to periodically report the hyperparameter tuning metric back to <span>Vertex AI.</span></li>
<li class="calibre8">Next, we create a configuration object for the hyperparameter tuning job, which specifies the hyperparameters to tune and the range of their possible values to try, as well as the objective metric we want to optimize (in our case, we’re using AUC ROC, referred to simply as <strong class="source-inline">auc</strong> in the code). One important thing to note at this point is that the more hyperparameters we include, the more combinations of trials will need to be run. This could result in additional time and computing resources (and therefore cost) being needed for our tuning job. For this reason, it is best to use domain <a id="_idIndexMarker957" class="calibre6 pcalibre pcalibre1"/>knowledge wherever possible to determine which hyperparameters we want the tuning job to focus on. We can also use the <strong class="source-inline">maxTrials</strong> variable in the configuration for the hyperparameter tuning job to control the number <span>of trials.</span><p class="calibre3">Understandably, it’s not always possible to use domain knowledge to narrow the parameter search space, and we often will need to find a trade-off between the quality of our hyperparameter tuning job outputs and the time and costs required to run them. For example, running the tuning job for a very long time may get us as close as possible to finding the perfect set of hyperparameter values, but running it for a shorter time may get us results that are just good enough, depending on the needs of our <span>use case.</span></p></li>
<li class="calibre8">The final stage in our hyperparameter tuning implementation is to use the Vertex AI Vizier client library to submit the hyperparameter tuning job to Vertex AI, which then runs our training application with different sets of hyperparameter values and finds the <span>best ones.</span></li>
</ol>
<h3 class="calibre11">Using the results of our hyperparameter tuning job</h3>
<p class="calibre3">Of course, we’re not just running hyperparameter tuning jobs for fun (although it is also fun)! When our tuning job finds the<a id="_idIndexMarker958" class="calibre6 pcalibre pcalibre1"/> best set of hyperparameters, we will want to access and review them, and usually, we will want to then use them to train the final <a id="_idIndexMarker959" class="calibre6 pcalibre pcalibre1"/>version of <span>our model.</span></p>
<h4 class="calibre20">Accessing the results via the Google Cloud console</h4>
<p class="calibre3">When we run the<a id="_idIndexMarker960" class="calibre6 pcalibre pcalibre1"/> hyperparameter tuning job in the notebook, the output from our code will display a link that will enable us to view the status of the tuning job in the Google Cloud console. The most important thing to view at that link is the list of trials performed by our tuning job, which will look similar to <span><em class="italic">Figure 8</em></span><span><em class="italic">.3</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer131">
<img alt="Figure 8.3: Hyperparameter tuning trials" src="image/B18143_08_3.jpg" class="calibre131"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 8.3: Hyperparameter tuning trials</p>
<p class="calibre3">In the list of our hyperparameter<a id="_idIndexMarker961" class="calibre6 pcalibre pcalibre1"/> tuning trials in the Google Cloud console, we can see the AUC metric for each trial, as well as the related trial ID (depicted within the box on the left in <span><em class="italic">Figure 8</em></span><em class="italic">.3</em>), and we can also see the hyperparameter values that were used for each trial (depicted within the box on the right in <span><em class="italic">Figure 8</em></span><em class="italic">.3</em>). We can click the arrow symbol in the header of the <strong class="source-inline">auc</strong> column to sort that column by ascending or descending AUC score. In our case, we want to sort it in descending order because we want the maximum score to appear at the top. This then tells us which trial had the hyperparameters that resulted in the best-performing model. In <span><em class="italic">Figure 8</em></span><em class="italic">.3</em>, you may notice that at least the top five trials all have the same AUC score. This is common because there may be multiple different combinations of hyperparameter values that can result in the same metric score. You can use the arrow in the bottom-right of the screen to peruse through the pages of additional trials, and you will see other trials that resulted in lower <span>AUC scores.</span></p>
<h4 class="calibre20">Accessing the results programmatically</h4>
<p class="calibre3">While it’s useful and interesting<a id="_idIndexMarker962" class="calibre6 pcalibre pcalibre1"/> to view the results of our hyperparameter tuning jobs in the Google Cloud console, we likely won’t want to have to manually copy and paste them into the final training job to create our <span>resulting model.</span></p>
<p class="calibre3">Fortunately, we can access all of those details programmatically via the Vertex API, and we can use the Vertex client library to do that from our development environment. In our notebook, after the tuning job finishes, we can continue with the additional activities in that notebook, which will show you how to access and use the best set of hyperparameter values produced by our tuning job. We then use those hyperparameter values to train a new model in our notebook, and then we finally test that model against the test dataset and calculate and display the resulting final AUC score. Note that when I ran this, I got a ROC-AUC score of 0.9188, which is <span>pretty good!</span></p>
<p class="calibre3">Great job, you have now<a id="_idIndexMarker963" class="calibre6 pcalibre pcalibre1"/> learned quite a lot about the topic of hyperparameter tuning and you should be ready to start applying what you’ve learned to other types of ML problems. Let’s summarize what we’ve learned in <span>this chapter.</span></p>
<h1 id="_idParaDest-182" class="calibre5"><a id="_idTextAnchor244" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we dived deeper into the important concept of objective metrics in machine learning. We covered, in detail, many of the most popular metrics that are used for evaluating binary classification models, such as precision, recall, F1 score, and ROC AUC. We then moved on to discuss hyperparameter optimization, including some of the important theoretical information in this area, such as the different types of methods that can be used to search for the optimal set of hyperparameters and associated values. This also provided some insight into why it can be very difficult or even impossible to efficiently perform hyperparameter tuning manually due to the large number of trials that can <span>be required.</span></p>
<p class="calibre3">Next, we dived into the Google Cloud Vertex AI Vizier service, which can be used to automate the hyperparameter tuning process for us. We then performed hands-on activities in Jupyter Notebook on Vertex AI, and we used Vizier to automatically find the best set of hyperparameters for training a credit card fraud detection model <span>using XGBoost.</span></p>
<p class="calibre3">Next, we used the outputs of our hyperparameter tuning job to train a final version of our model, and we then evaluated that model against our <span>test dataset.</span></p>
<p class="calibre3">In the next chapter, begin exploring beyond the simpler machine-learning algorithms such as linear regression and decision trees, and delve into the realm of Artificial Neural Networks (ANNs). Let's move on and discover this fascinating category of concepts <span>and technologies.</span></p>
</div>
</div></body></html>