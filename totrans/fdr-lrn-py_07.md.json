["```py\npip install stadle-client\n```", "```py\ndef _average_aggregate(self,\n                           buffer: List[np.array],\n                           num_samples: List[int]) -> np.array:\n        \"\"\"\n        Given a list of models, compute the average model (FedAvg).\n        This function provides a primitive mathematical operation.\n        :param buffer: List[np.array] - A list of models to be aggregated\n        :return: np.array - The aggregated models\n        \"\"\"\n        denominator = sum(num_samples)\n        # weighted average\n        model = float(num_samples[0]) / denominator * buffer[0]\n        for i in range(1, len(buffer)):\n            model += float(num_samples[i]) / denominator * buffer[i]\n        return model\n```", "```py\nagg_model = ... # Get aggregate model – abstracted out of example\nmodel.load_state_dict(agg_model.state_dict())\nfor epoch in range(num_epochs):\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n```", "```py\nagg_model = ... # Get aggregated model from aggregator\nmodel.load_state_dict(agg_model.state_dict())\nagg_grad = None\ncurr_grad = None\ngamma = 0.9\nmu = 0.001\n```", "```py\ndef gamma_inexact_solution_found(curr_grad, agg_grad, gamma):\n    if (curr_grad is None):\n        return False\n    return curr_grad.norm(p=2) < gamma * agg_grad.norm(p=2)\n```", "```py\nmodel.train()\nwhile (not gamma_inexact_solution_found(curr_grad, agg_grad, gamma)):\n    total_grad = torch.cat([torch.zeros_like(param.data.flatten()) for param in model.parameters()])\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n```", "```py\n        curr_weights = torch.cat([param.data.flatten() for param in model.parameters()])\n        agg_weights = torch.cat([param.data.flatten() for param in agg_model.parameters()])\n        prox_term = mu * torch.norm(curr_weights - agg_weights, p=2)**2\n        loss += prox_term\n```", "```py\n        loss.backward()\n        grad = torch.cat([param.grad.flatten() for param in model.parameters()])\n        total_grad += grad\n        optimizer.step()\n```", "```py\n    if (agg_grad == None):\n        agg_grad = total_grad\n    curr_grad = total_grad\n```", "```py\ndef aggregate(parameter_vectors):\n    # Perform some form of aggregation\n    return aggregated_parameter_vector\n```", "```py\ndef geometric_median_aggregate(parameter_vectors, epsilon):\n    vector_shape = parameter_vectors[0].shape\n    vector_buffer = list(v.flatten() for v in parameter_vectors)\n    prev_median = np.zeros(vector_buffer[0].shape)\n    delta = np.inf\n    vector_matrix = np.vstack(vector_buffer)\n    while (delta > epsilon):\n        dists = np.sqrt(np.sum((vector_matrix - prev_median[np.newaxis, :])**2, axis=1))\n        curr_median = np.sum(vector_matrix / dists[:, np.newaxis], axis=0) / np.sum(1 / dists)\n        delta = np.linalg.norm(curr_median - prev_median)\n        prev_median = curr_median\n    return prev_median.reshape(vector_shape)\n```", "```py\ndef coordinate_median_aggregate(parameter_vectors):\n    return np.median(parameter_vectors, axis=0)\n```", "```py\ndef krum_aggregate(parameter_vectors, f, use_mean=False):\n    num_vectors = len(parameter_vectors)\n    filtered_size = max(1, num_vectors-f-2)\n    scores = np.zeros(num_vectors)\n    for i in range(num_vectors):\n        distances = np.zeros(num_vectors)\n        for j in range(num_vectors):\n            distances[j] = np.linalg.norm(parameter_vectors[i] - parameter_vectors[j])\n        scores[i] = np.sum(np.sort(distances)[:filtered_size])\n    if (use_mean):\n        idx = np.argsort(scores)[:filtered_size]\n        return np.mean(np.stack(parameter_vectors)[idx], axis=0)\n    else:\n        idx = np.argmin(scores)\n        return parameter_vectors[idx]\n```", "```py\nagg_model = ... # Get aggregated model from aggregator\nmodel.load_state_dict(agg_model.state_dict())\nfisher_info_params = ... # Initialize at start, then maintain to store past round parameters\nagg_fisher_info_params = ... # Separate aggregate Fisher information parameters from aggregate model parameters\n# Only consider other agents, and convert to PyTorch tensor\nagg_fisher_info_params = {k:torch.tensor(agg_fisher_info_params[k] - fisher_info_params[k]) for k in fisher_info_params.keys()}\n# Scaling parameter for FedCurv regularization term\nfedcurv_lambda = 1.0\ntotal_grad = {i:torch.zeros_like(param.data) for i,param in enumerate(model.parameters())}\n```", "```py\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        for i,param in enumerate(model.parameters()):\n            # Factor out regularization term to use saved fisher info parameters\n            reg_term = (param.data ** 2) * agg_fisher_info_params[f'fedcurv_u_{i}']\n            reg_term += 2 * param.data * agg_fisher_info_params[f'fedcurv_v_{i}']\n            reg_term += (agg_fisher_info_params[f'fedcurv_v_{i}'] ** 2) / agg_fisher_info_params[f'fedcurv_u_{i}']\n            loss += fedcurv_lambda * reg_term.sum()\n```", "```py\n        loss.backward()\n        for i,param in enumerate(model.parameters()):\n            total_grad[i] += param.grad\n        optimizer.step()\n```", "```py\nfor i,param in enumerate(model.parameters()):\n    fisher_info_params[f'fedcurv_u_{i}'] = (total_grad[i] ** 2).numpy()\n    fisher_info_params[f'fedcurv_v_{i}'] = ((total_grad[i] ** 2) * param.data).numpy()\n```"]