<html><head></head><body>
		<div id="_idContainer058">
			<h1 class="chapter-number"><a id="_idTextAnchor060"/>5</h1>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor061"/>Quantifying and Improving Data Properties</h1>
			<p>Procuring data in machine learning systems is a long process. So far, we have focused on data collection from source systems and cleaning noise from data. Noise, however, is not the only problem that we can encounter in data. Missing values or random attributes are examples of data properties that can cause problems with machine learning systems. Even the length of the input data can be problematic if it is outside of the <span class="No-Break">expected values.</span></p>
			<p>In this chapter, we will dive deeper into the properties of data and how to improve them. In contrast to the previous chapter, we will work on feature vectors rather than raw data. Feature vectors are already a transformation of the data and therefore, we can change properties such as noise or even change how the data <span class="No-Break">is perceived.</span></p>
			<p>We’ll focus on the processing of text, which is an important part of many machine learning algorithms nowadays. We’ll start by understanding how to transform data into feature vectors using simple algorithms such as bag of words. We will also learn about techniques to handle problems <span class="No-Break">in data.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Quantifying data properties for machine <span class="No-Break">learning systems</span></li>
				<li>Germinating noise – feature engineering in <span class="No-Break">clean datasets</span></li>
				<li>Handling noisy data – machine learning algorithms and <span class="No-Break">noise removal</span></li>
				<li>Eliminating attribute noise – a guide to <span class="No-Break">dataset refinement</span></li>
			</ul>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor062"/>Feature engineering – the basics</h1>
			<p>Feature engineering<a id="_idIndexMarker194"/> is the process of transforming raw data into vectors of numbers that can be used in machine learning algorithms. This process is structured and requires us to first select which feature extraction mechanism we need to use – which depends on the type of the task – and then configure the chosen feature extraction mechanism. When the chosen algorithm is configured, we can use it to transform the raw input data into a matrix of features – we call this process feature extraction. Sometimes, the data needs to be processed before (or after) the feature extraction, for example, by merging fields or removing noise. This process is called <span class="No-Break">data wrangling.</span></p>
			<p>The number of feature extraction mechanisms is large, and we cannot cover all of them. Not that we need to either. What we need to understand, however, is how the choice of feature extraction mechanism influences the properties of the data. We’ll dive much deeper into the process of feature engineering in the next chapter, but in this chapter, we will introduce a basic algorithm for textual data. We need to introduce it to understand how it impacts the properties of data and how to cope with the most common problems that can arise in the context of feature extraction, including dealing with “dirty” data that <span class="No-Break">requires cleaning.</span></p>
			<p>To understand this process, let us start with the first example of feature extraction from text using an algorithm called bag of words. Bag of words is a method that transforms a piece of text into a vector of numbers showing which words are part of that text. The words form the set of features – or columns – in the resulting dataframe. In the following code, we can see how feature extraction works. We have used the standard library from <strong class="source-inline">sklearn</strong> to create a bag-of-words <span class="No-Break">feature vector.</span></p>
			<p>In the following code fragment, we take two lines of C code – <strong class="source-inline">printf("Hello world!");</strong> and <strong class="source-inline">return 1</strong> – and then translate this into a matrix <span class="No-Break">of features:</span></p>
			<pre class="source-code">
# create the feature extractor, i.e., BOW vectorizer
# please note the argument - max_features
# this argument says that we only want three features
# this will illustrate that we can get problems - e.g. noise
# when using too few features<a id="_idTextAnchor063"/>
<strong class="bold">vectorizer = CountVectorizer(max_features = 3)</strong>
# simple input data - two sentences
sentence1 =<a id="_idTextAnchor064"/> 'printf("Hello world!");'
sentence2 =<a id="_idTextAnchor065"/> 'return 1'
# creating the feature vectors for the input data<a id="_idTextAnchor066"/>
X = vectorizer.fit_transform([sentence1, sentence2])
# creating the data frame based on the vectorized data
df_bow_sklearn = pd.DataFrame(X.toarray(),
                              columns=vectorizer.get_feature_names(),
                              index=[sentence1, sentence2])
# take a peek at the featurized data
df_bow_sklearn.head()</pre>			<p>The<a id="_idIndexMarker195"/> line in bold is a statement that creates an instance of the <strong class="source-inline">CodeVectorizer</strong> class, which transforms a given text into a vector of features. This includes the extraction of the features identified. This line has one parameter – <strong class="source-inline">max_features = 3</strong>. This parameter tells the algorithm that we only want three features. In this algorithm, the features are the words that are used in the input text. When we input the text to the algorithm, it extracts the tokens (words), and then for every line, it counts whether it contains these words. This is done in the statement <strong class="source-inline">X = vectorizer.fit_transform([sentence1, sentence2])</strong>. When the features are extracted, the resulting dataset looks <span class="No-Break">as follows:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Hello</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">printf</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">return</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">printf(“Hello world!”);</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">return 1</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">1</strong></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Extracted features create this dataset</p>
			<p>The first line in the table contains the index – the line that was input to the algorithm – and then <strong class="source-inline">1</strong> or <strong class="source-inline">0</strong> to show that the line contains the words in the vocabulary. Since we only asked for three features, the table has three columns – <strong class="source-inline">Hello</strong>, <strong class="source-inline">printf</strong>, and <strong class="source-inline">return</strong>. If we change the parameter of the <strong class="source-inline">CountVectorizer()</strong>, we’ll obtain the full list of tokens in these two lines, that is, <strong class="source-inline">hello</strong>, <strong class="source-inline">printf</strong>, <strong class="source-inline">return</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">world</strong></span><span class="No-Break">.</span></p>
			<p>For these<a id="_idIndexMarker196"/> two simple lines of C code, we get four features, which illustrates that this kind of feature extraction can quickly increase the size of the data. This leads us on to my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #30</p>
			<p class="callout">Balance the number of features with the number of data points. More features is not <span class="No-Break">always better.</span></p>
			<p>When creating feature vectors, it is important to extract meaningful features that can effectively distinguish between the data points. However, we should keep in mind that having more features will require more memory and can make the training slower. It is also prone to problems with missing <span class="No-Break">data points.</span></p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor067"/>Clean data</h1>
			<p>One of the <a id="_idIndexMarker197"/>most problematic aspects of datasets, when it comes to machine learning, is the presence of empty data points or empty values of features for data points. Let’s illustrate that with the example of the features extracted in the previous section. In the following table, I introduced an empty data point – the <strong class="source-inline">NaN</strong> value in the middle column. This means that the value does <span class="No-Break">not exist.</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table002-2">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Hello</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">printf</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">return</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">printf(“Hello world!”);</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">NaN</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">return 1</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">1</strong></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Extracted features with a NaN value in the table</p>
			<p>If we use this data as input to a machine learning algorithm, we’ll get an error message that the data contains empty values and that the model cannot be trained. That is a very accurate description of this problem – if there is a missing value, then the model does not know how to handle it and therefore it cannot <span class="No-Break">be trained.</span></p>
			<p>There are two strategies to cope with empty values in datasets – removing the data points or imputing <span class="No-Break">the values.</span></p>
			<p>Let’s start<a id="_idIndexMarker198"/> with the first strategy – removing the empty data points. The following script reads the data from our code reviews that we will use for <span class="No-Break">further calculations:</span></p>
			<pre class="source-code">
# read the file with gerrit code reviews
dfReviews = pd.read_csv('./gerrit_reviews.csv', sep=';')
# just checking that we have the right columns
# and the right data
dfReviews.head()</pre>			<p>The preceding fragment of code reads the file and displays its first 10 rows for us to inspect what the <span class="No-Break">data contains.</span></p>
			<p>Once we have the data in memory, we can check how many of the rows contain null values for the column that contains the actual line of code, which is named <strong class="source-inline">LOC</strong>. Then, we can also remove the rows/data points that do not contain any data. The removal of the data points is handled by the following line – <strong class="source-inline">dfReviews = dfReviews.dropna()</strong>. This statement removes the lines that are empty and keeps the result in the dataframe itself (the <span class="No-Break"><strong class="source-inline">inplace=True</strong></span><span class="No-Break"> parameter):</span></p>
			<pre class="source-code">
import numpy as np
# before we use the feature extractor, let's check if the data contains NANs
print(f'The data contains {dfReviews.LOC.isnull().sum()} empty rows')
# remove the empty rows
dfReviews = dfReviews.dropna()
# checking again, to make sure that it does not contain them
print(f'The data contains {dfReviews.LOC.isnull().sum()} empty rows')</pre>			<p>After these commands, our dataset is prepared to create the feature vector. We can use <strong class="source-inline">CountVectorizer</strong> to<a id="_idIndexMarker199"/> extract the features from the dataset, as in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# now, let's convert the code (LOC) column to the vector of features
# using BOW from the example above
vectorizer = CountVectorizer(min_df=2,
                             max_df=10)
dfFeatures = vectorizer.fit_transform(dfReviews.LOC)
# creating the data frame based on the vectorized data
df_bow_sklearn = pd.DataFrame(dfFeatures.toarray(),
                              columns=vectorizer.get_feature_names(),index=dfReviews.LOC)
# take a peek at the featurized data
df_bow_sklearn.head()</pre>			<p>This fragment creates the bag-of-words model (<strong class="source-inline">CountVectorizer</strong>) with two parameters – the minimum frequency of the tokens and the maximum frequency of the tokens. This means that the algorithm calculates the statistics of how frequently each token appears in the dataset and then chooses the ones that fulfill the criteria. In our case, the algorithm chooses the tokens that appear at least twice (<strong class="source-inline">min_df=2</strong>) and at most 20 <span class="No-Break">times (</span><span class="No-Break"><strong class="source-inline">max_df=20</strong></span><span class="No-Break">).</span></p>
			<p>The result of this code fragment is a large dataframe with 661 features extracted for each line of code in our dataset. We can check this by writing <strong class="source-inline">len(df_bow_sklearn.columns)</strong> after executing the preceding <span class="No-Break">code fragment.</span></p>
			<p>In order to <a id="_idIndexMarker200"/>check how to work with data imputation, let us open a different dataset and check how many missing data points we have per column. Let’s read the dataset that is named <strong class="source-inline">gerrit_reviews_nan.csv</strong> and list the number of missing values in that dataset using the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# read data with NaNs to a dataframe
dfNaNs = pd.read_csv('./gerrit_reviews_nan.csv', sep='$')
# before we use the feature extractor, let's check if the data contains NANs
print(f'The data contains {dfNaNs.isnull().sum()} NaN values')</pre>			<p>As a result of this code fragment, we get a list of columns with the number of missing values in them – the tail of this list is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
yangresourcesnametocontentmap     213
yangtextschemasourceset           205
yangtextschemasourcesetbuilder    208
yangtextschemasourcesetcache      207
yangutils                         185</pre>			<p>There are many missing values and therefore, we need to adopt another strategy than removing them. If we remove all these values, we get exactly 0 data points – which means that there is a NaN value in one (or more) of the columns for every(!) data point. So, we need to adopt another strategy – <span class="No-Break">imputation.</span></p>
			<p>First, we need to prepare the data for the imputer, which only works on the features. Therefore, we need to remove the index from <span class="No-Break">the dataset:</span></p>
			<pre class="source-code">
# in order to use the imputer, we need to remove the index from the data
# we remove the index by first re-setting it (so that it becomes a regular column)
# and then by removing this column.
dfNaNs_features = dfNaNs.reset_index()
dfNaNs_features.drop(['LOC', 'index'], axis=1, inplace=True)
dfNaNs_features.head()</pre>			<p>Then, we<a id="_idIndexMarker201"/> can create the imputer. In this example, I use one of the modern ones, which is based on training a classifier on the existing data and then using it to fill the data in the original dataset. The fragment of code that trains the imputer is <span class="No-Break">presented here:</span></p>
			<pre class="source-code">
# let's use iterative imputed to impute data to the dataframe
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
# create the instance of the imputer
imp = IterativeImputer(max_iter=3,
                       random_state=42,
                       verbose = 2)
# train the imputer on the features in the dataset
imp.fit(dfNaNs_features)</pre>			<p>The last line of the code fragment is the actual training of the imputer. After this, we can start making the imputations to the dataset, as in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# now, we fill in the NaNs in the original dataset
npNoNaNs = imp.transform(dfNaNs_features)
dfNoNaNs = pd.DataFrame(npNoNaNs)</pre>			<p>After this <a id="_idIndexMarker202"/>fragment, we have a dataset that contains imputer values. Now, we need to remember that these values are only estimations, not the real ones. This particular dataset illustrates this very well. When we execute the <strong class="source-inline">dfNoNaNs.head()</strong> command, we can see that some of the imputed values are negative. Since our dataset is the result of <strong class="source-inline">CountVectorizer</strong>, the negative values are not likely. Therefore, we could use another kind of imputer – <strong class="source-inline">KNNImputer</strong>. That imputer uses the nearest neighbor algorithm to find the most similar data points and fills in the missing data based on the values of the similar data points. In this way, we get a set of imputed values that have the same properties (e.g., no negative values) as the rest of the dataset. However, the pattern of the imputed values <span class="No-Break">is different.</span></p>
			<p>Therefore, here is my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #30</p>
			<p class="callout">Use KNNImputer for data where the similarity between data points is expected to <span class="No-Break">be local.</span></p>
			<p><strong class="source-inline">KNNImputer</strong> works well when there is a clear local structure in the data, especially when neighboring data points are similar in terms of the feature with missing values. It can be sensitive to the choice of the number of nearest <span class="No-Break">neighbors (</span><span class="No-Break"><strong class="source-inline">k</strong></span><span class="No-Break">).</span></p>
			<p><strong class="source-inline">IterativeImputer</strong> tends to perform well when there are complex relationships and dependencies among features in the dataset. It may be more suitable for datasets with missing values that are not easily explained by <span class="No-Break">local patterns.</span></p>
			<p>However, check whether the imputation method provides logical results for the dataset at hand, in order to reduce the risk <span class="No-Break">of bias.</span></p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor068"/>Noise in data management</h1>
			<p>Missing data and <a id="_idIndexMarker203"/>contradictory annotations are only one type of problem with data. In many cases, large datasets, which are generated by feature extraction algorithms, can contain too much information. Features can be superfluous and not contribute to the end results of the algorithm. Many machine learning models can deal with noise in the features, called attribute noise, but too many features can be costly in terms of training time, storage, and even data<a id="_idIndexMarker204"/> <span class="No-Break">collection itself.</span></p>
			<p>Therefore, we should also pay attention to the attribute noise, identify it, and then <span class="No-Break">remove it.</span></p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor069"/>Attribute noise</h1>
			<p>There<a id="_idIndexMarker205"/> are a few methods to reduce attribute noise in large datasets. One of these methods is an <a id="_idIndexMarker206"/>algorithm named the <strong class="bold">Pairwise Attribute Noise Detection Algorithm</strong> (<strong class="bold">PANDA</strong>). PANDA compares features pairwise and identifies which of them adds noise to the dataset. It is a very effective algorithm, but unfortunately very computationally heavy. If our dataset had a few hundred features (which is when we would really need to use this algorithm), we would need a lot of computational power to identify these features that bring in little to <span class="No-Break">the analysis.</span></p>
			<p>Fortunately, there are machine learning algorithms that provide similar functionality with little computational overhead. One of these algorithms is the random forest algorithm, which allows you to retrieve the set of feature importance values. These values are a way of identifying which features are not used in any of the decision trees in <span class="No-Break">this forest.</span></p>
			<p>Let us then see how to use that algorithm to extract and visualize the feature’s importance. For this example, we will use the data extracted from the Gerrit tool in <span class="No-Break">previous chapters:</span></p>
			<pre class="source-code">
# importing the libraries to vectorize text
# and to manipulate dataframes
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
# create the feature extractor, i.e., BOW vectorizer
# please note the argument - max_features
# this argument says that we only want three features
# this will illustrate that we can get problems - e.g. noise
# when using too few features
vectorizer = CountVectorizer()
# read the file with gerrit code reviews
dfReviews = pd.read_csv('./gerrit_reviews.csv', sep=';')</pre>			<p>In this dataset, we<a id="_idIndexMarker207"/> have two columns that we extract features from. The first is the <strong class="source-inline">LOC</strong> column, which we use to extract the features using <strong class="source-inline">CountVectorizer</strong> – just like in the previous example. These features will be our <strong class="source-inline">X</strong> values later for the training algorithm. The second column of interest is the <strong class="source-inline">message</strong> column. The <strong class="source-inline">message</strong> column is used to provide the <strong class="source-inline">decision</strong> class. In order to transform the text of the message, we use a sentiment analysis model to identify whether the message is positive <span class="No-Break">or negative.</span></p>
			<p>First, let’s extract the BOW features <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">CountVectorizer</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
# now, let's convert the code (LOC) column to the vector of features
# using BOW from the example above
vectorizer = CountVectorizer(min_df=2,
                             max_df=10)
dfFeatures = vectorizer.fit_transform(dfReviews.LOC)
# creating the data frame based on the vectorized data
df_bow_sklearn = pd.DataFrame(dfFeatures.toarray(),
                              columns=vectorizer.get_feature_names(),index=dfReviews.LOC)</pre>			<p>To transform the message into a sentiment, we can use an openly available model from the Hugging Face Hub. We need to install the relevant libraries using the following command: <strong class="source-inline">! pip install -q transformers</strong>. Once we have the libraries, we can start<a id="_idIndexMarker208"/> the <span class="No-Break">feature extraction:</span></p>
			<pre class="source-code">
# using a classifier from the Hugging Face hub is quite straightforward
# we import the package and create the sentiment analysis pipeline
from transformers import pipeline
# when we create the pipeline, and do not provide the model
# then the huggingface hub will choose one for us
# and download it
sentiment_pipeline = pipeline("sentiment-analysis")
# now we are ready to get the sentiment from our reviews.
# let's supply it to the sentiment analysis pipeline
lstSentiments = sentiment_pipeline(list(dfReviewComments))
# transform the list to a dataframe
dfSentiments = pd.DataFrame(lstSentiments)
# and then we change the textual value of the sentiment to
# a numeric one – which we will use for the random forest
dfSentiment = dfSentiments.label.map({'NEGATIVE': 0, 'POSITIVE': 1})</pre>			<p>The preceding code fragment uses the pre-trained model for the sentiment analysis and one from the standard pipeline – <strong class="source-inline">sentiment-analysis</strong>. The result is a dataframe that contains a positive or <span class="No-Break">negative sentiment.</span></p>
			<p>Now, we<a id="_idIndexMarker209"/> have both the <strong class="source-inline">X</strong> values – features extracted from the lines of code – and the predicted <strong class="source-inline">Y</strong> values – the sentiment from the review comment message. We can use these to create a dataframe that we can use as an input to the random forest algorithm, train the algorithm, and identify which features contributed the most to <span class="No-Break">the result:</span></p>
			<pre class="source-code">
# now, we train the RandomForest classifier to get the most important features
# Note! This training does not use any data split, as we only want to find
# which features are important.
X = df_bow_sklearn.drop(['sentiment'], axis=1)
Y = df_bow_sklearn['sentiment']
# import the classifier – Random Forest
from sklearn.ensemble import RandomForestClassifier
# create the classifier
clf = RandomForestClassifier(max_depth=10, random_state=42)
# train the classifier
# please note that we do not check how good the classifier is
# only train it to find the features that are important.
Clf.fit(X,Y)</pre>			<p>When the random forest model is trained, we can extract the list of <span class="No-Break">important features:</span></p>
			<pre class="source-code">
# now, let's check which of the features are the most important ones
# first we create a dataframe from this list
# then we sort it descending
# and then filter the ones that are not important
dfImportantFeatures = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance'])
# sorting values according to their importance
dfImportantFeatures.sort_values(by=['importance'],
                                ascending=False,
                                inplace=True)
# choosing only the ones that are important, skipping
# the features which have importance of 0
dfOnlyImportant = dfImportantFeatures[dfImportantFeatures['importance'] != 0]
# print the results
print(f'All features: {dfImportantFeatures.shape[0]}, but only {dfOnlyImportant.shape[0]} are used in predictions. ')</pre>			<p>This <a id="_idIndexMarker210"/>preceding code fragment selects the features with an importance of more than <strong class="source-inline">0</strong> and then lists them. We find that 363 out of 662 features are used in the predictions. This means that the remaining 270 are just the <span class="No-Break">attribute noise.</span></p>
			<p>We can also visualize these features using the <strong class="source-inline">seaborn</strong> library, as in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# we use matplotlib and seaborn to make the plot
import matplotlib.pyplot as plt
import seaborn as sns
# Define size of bar plot
# We make the x axis quite much larger than the y-axis since
# there is a lot of features to visualize
plt.figure(figsize=(40,10))
# plot seaborn bar chart
# we just use the blue color
sns.barplot(y=dfOnlyImportant['importance'],
            x=dfOnlyImportant.index,
            color='steelblue')
# we make the x-labels rotated so that we can fit
# all the features
plt.xticks(rotation=90)
# add chart labels
plt.title('Importance of features, in descending order')
plt.xlabel('Feature importance')
plt.ylabel('Feature names')</pre>			<p>This preceding code<a id="_idIndexMarker211"/> fragment results in the following diagram for <span class="No-Break">the dataset:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer053">
					<img alt="Figure 5.3 – Feature importance chart with numerous features" src="image/B19548_05_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Feature importance chart with numerous features</p>
			<p>Since <a id="_idIndexMarker212"/>there are so many features, the diagram gets very cluttered and challenging to read, so we can only visualize the top 20 features to understand which ones are really the <span class="No-Break">most important.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer054">
					<img alt="Figure 5.4 – Top 20 most important features in the dataset" src="image/B19548_05_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Top 20 most important features in the dataset</p>
			<p>The preceding code examples show that we can reduce the number of features by 41%, which is almost half of the features. The algorithm takes just a few seconds to find the most important features, which makes it the perfect candidate for reducing attribute noise in <span class="No-Break">the datasets.</span></p>
			<p class="callout-heading">Best practice #31</p>
			<p class="callout">Use the Random Forest classifier to eliminate unnecessary features, as it offers very <span class="No-Break">good performance.</span></p>
			<p>Although we do not <a id="_idIndexMarker213"/>really get information on how much noise the removed features contain, receiving information that they have no value for the prediction algorithm is sufficient. Therefore, I recommend using this kind of feature reduction technique in the machine learning pipeline in order to reduce the computational and storage needs of <span class="No-Break">our pipeline.</span></p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor070"/>Splitting data</h1>
			<p>For the<a id="_idIndexMarker214"/> process of designing machine learning-based software, another important property is to understand the distribution of data, and, subsequently, ensure that the data used for training and testing is of a <span class="No-Break">similar distribution.</span></p>
			<p>The distribution of the data used for training and validation is important as the machine learning models identify patterns and re-create them. This means that if the data in the training is not distributed in the same way as the data in the test set, our model misclassifies data points. The misclassifications (or mispredictions) are caused by the fact that the model learns patterns in the training data that are different from the <span class="No-Break">test data.</span></p>
			<p>Let us understand how splitting algorithms work in theory, and how they work in practice. <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.5</em> shows how the splitting works on a theoretical and <span class="No-Break">conceptual level:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer055">
					<img alt="Figure 5.5 – Splitting data into train and test sets﻿" src="image/B19548_05_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Splitting data into train and test sets</p>
			<p>Icons <a id="_idIndexMarker215"/>represent review comments (and discussions). Every icon symbolizes its own discussion thread, and each type of icon reflects different teams. The idea behind splitting the dataset is that the two sets are very similar, but not identical. Therefore, the distribution of elements in the training and test datasets needs to be as similar as possible. However, it is not always possible, as <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.5</em> shows – there are three out of four icons of one of the kinds in the training set and only one in the test set. When designing machine learning software, we need to take this aspect into consideration, even though it only relates to machine learning models. Our data processing pipeline should contain checks that provide the ability to understand whether the data is correctly distributed and, if not, we need to correct it. If we do not correct it, our system starts mispredicting. The change in the distribution of data over time, which is natural in machine learning-based systems, is called <span class="No-Break">concept drift.</span></p>
			<p>Let us use this in practice by calculating the distributions of the data in our Gerrit reviews dataset. First, we read the data, and then we use the <strong class="source-inline">sklearn</strong> <strong class="source-inline">train_test_split</strong> method to create a <span class="No-Break">random split:</span></p>
			<pre class="source-code">
# then we read the dataset
dfData = pd.read_csv('./bow_sentiment.csv', sep='$')
# now, let's split the data into train and test
# using the random split
from sklearn.model_selection import train_test_split
X = dfData.drop(['LOC', 'sentiment'], axis=1)
y = dfData.sentiment
# now we are ready to split the data
# test_size parameter says that we want 1/3rd of the data in the test set
# random state allows us to replicate the same split over and over again
X_train, X_test, y_train, y_test =
                train_test_split(X, y,
                                 test_size=0.33,
                                 random_state=42)</pre>			<p>In this code<a id="_idIndexMarker216"/> fragment, we separate the predicted values (<strong class="source-inline">y</strong>) from the predictor values (<strong class="source-inline">X</strong>) features. Then we use the <strong class="source-inline">train_test_split</strong> method to split the dataset into two – two-thirds of the data in the training set and one-third of the data in the test set. This 2:1 ratio is the most common, but we can also encounter a 4:1 ratio, depending on the application and <span class="No-Break">the dataset.</span></p>
			<p>Now that we have two sets of data, we should explore whether the distributions are similar. Essentially, we should do that for each feature and the predicted variable (<strong class="source-inline">y</strong>), but in our dataset, we have 662 features, which means that we would have to do as many comparisons. So, let us, for the sake of the example, visualize only one of them – the one that was deemed the most important in our previous example – <span class="No-Break"><strong class="source-inline">dataresponse</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
# import plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns
# we make the figure a bit larger
# and the font a bit more visible
plt.figure(figsize=(10,7))
sns.set(font_scale=1.5)
# here we visualize the histogram using seaborn
# we take only one of the variables, please see the list of columns
# above, or use print(X_train.columns) to get the list
# I chose the one that was the most important one
# for the prediction algorithm
sns.histplot(data=X_train['dataresponse'],
             binwidth=0.2)</pre>			<p>We will <a id="_idIndexMarker217"/>do the same for the test <span class="No-Break">set too:</span></p>
			<pre class="source-code">
plt.figure(figsize=(10,7))
sns.set(font_scale=1.5)
sns.histplot(data=X_test['dataresponse'],
             binwidth=0.2)</pre>			<p>These two fragments result in two histograms with the distribution for that variable. They are presented in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer056">
					<img alt="Figure 5.6 – Distribution of dataresponse feature in train and test set﻿" src="image/B19548_05_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Distribution of dataresponse feature in train and test set</p>
			<p>The <a id="_idIndexMarker218"/>train set’s distribution is on the left-hand side and the test set’s distribution is on the right-hand side. At first glance, the distributions show that there is only a single value - 0 value. Therefore, we need to explore the data manually a bit more. We can check the distribution by calculating the number of entities per value – 0 <span class="No-Break">and 1:</span></p>
			<pre class="source-code">
# we can even check the count of each of these values
X_train_one_feature = X_train.groupby(by='dataresponse').count()
X_train_one_feature
# we can even check the count of each of these values
X_test_one_feature = X_test.groupby(by='dataresponse').count()
X_test_one_feature</pre>			<p>From the preceding calculations, we find that there are 624 values of 0 and 5 values of 1 in the train set. We also find that there are 309 values of 0 and 1 value of 1 in the test set. These are not exactly the same ratio, but given the scale – the 0s are significantly more than the 1s – this does not have any impact on the machine <span class="No-Break">learning model.</span></p>
			<p>The features in our dataset should have the same distribution, but so do the <strong class="source-inline">Y</strong> values – the predicted variables. We can use the same technique to visualize the distribution between <a id="_idIndexMarker219"/>classes in the <strong class="source-inline">Y</strong> value. The following code fragment does <span class="No-Break">just that:</span></p>
			<pre class="source-code">
# we make the figure a bit larger
# and the font a bit more visible
plt.figure(figsize=(10,7))
sns.set(font_scale=1.5)
sns.histplot(data=y_train, binwidth=0.5)
sns.histplot(data=y_test,  binwidth=0.5)</pre>			<p>This code fragment generates two diagrams, which show what the difference between the two classes is. They are presented in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer057">
					<img alt="Figure 5.7 – Distribution of classes (0 and 1) in the training and test data" src="image/B19548_05_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Distribution of classes (0 and 1) in the training and test data</p>
			<p>The predicted <strong class="source-inline">Y</strong> variable 0s are the negative sentiment values while 1s are the positive ones. Although the scales on the <em class="italic">y</em> axis are different in both diagrams, the distributions are very similar – it is roughly 2:1 in terms of the number of negative (0) sentiments and positive (<span class="No-Break">1) sentiments.</span></p>
			<p>The classes are not balanced – the number of 0s is much larger than the number of 1s, but the distribution is the same. The fact that the classes are not balanced means that the model trained on this data is slightly biased towards the negative sentiment rather than the positive sentiment. However, this reflects the empirical observations that we make: in code reviews, the reviewers are more likely to comment on code that needs to be improved<a id="_idIndexMarker220"/> rather than on code that is <span class="No-Break">nicely written.</span></p>
			<p class="callout-heading">Best practice #32</p>
			<p class="callout">As much as possible, retain the original distribution of the data as it reflects the <span class="No-Break">empirical observations.</span></p>
			<p>Although we can balance the classes using undersampling, oversampling, or similar techniques, we should consider keeping the original distribution as much as we can. Changing the distribution makes the model “fairer” in terms of predictions/classifications, but it changes the nature of the <span class="No-Break">observed phenomena.</span></p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor071"/>How ML models handle noise</h1>
			<p>Reducing noise<a id="_idIndexMarker221"/> from datasets is a time-consuming task, and it is also a task that cannot be easily automated. We need to understand whether we have noise in the data, what kind of noise is in the data, and how to remove it. Luckily, most machine learning algorithms are pretty good at <span class="No-Break">handling noise.</span></p>
			<p>For example, the algorithm that we have used quite a lot so far – random forest – is quite robust to noise in datasets. Random forest is an ensemble model, which means that it is composed of several separate decision trees that internally “vote” for the best result. This voting process can therefore filter out noise and coalescence toward the pattern contained in <span class="No-Break">the data.</span></p>
			<p>Deep learning algorithms have similar properties too – by utilizing a number of small neurons, these networks are robust to noise in large datasets. They can coerce the pattern in <span class="No-Break">the data.</span></p>
			<p class="callout-heading">Best practice #33</p>
			<p class="callout">In large-scale software systems, if possible, rely on machine learning models to handle noise in <span class="No-Break">the data.</span></p>
			<p>It may sound like I’m proposing an easy way out, but I’m not. Manual cleaning of the data is crucial, but it is also slow and costly. Therefore, during operations in large-scale systems, it is better to select a model that is robust to noise in the data and at the same time uses cleaner data. Since manual noise-handling processes require time and effort, relying on them would introduce unnecessary costs for our <span class="No-Break">product operations.</span></p>
			<p>Therefore, it’s <a id="_idIndexMarker222"/>better to use algorithms that do that for us and therefore create products that are reliable and require minimal maintenance. Instead of costly noise-cleaning processes, it’s much more cost-efficient to re-train the algorithm to let it do the work <span class="No-Break">for you.</span></p>
			<p>In the next chapter, we explore data visualization techniques. These techniques help us to understand dependencies in the data and whether it exposes characteristics that can be learnt by the machine <span class="No-Break">learning models.</span></p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor072"/>References</h1>
			<ul>
				<li><em class="italic">Scott, S. and S. Matwin. Feature engineering for text classification. in </em><span class="No-Break"><em class="italic">ICML. 1999.</em></span></li>
				<li><em class="italic">Kulkarni, A., et al., Converting text to features. Natural Language Processing Recipes: Unlocking Text Data with Machine Learning and Deep Learning Using Python, 2021: </em><span class="No-Break"><em class="italic">p. 63-106.</em></span></li>
				<li><em class="italic">Van Hulse, J.D., T.M. Khoshgoftaar, and H. Huang, The pairwise attribute noise detection algorithm. Knowledge and Information Systems, 2007. 11: </em><span class="No-Break"><em class="italic">p. 171-190.</em></span></li>
				<li><em class="italic">Li, X., et al., </em><em class="italic">Exploiting BERT for end-to-end aspect-based sentiment analysis. arXiv preprint </em><span class="No-Break"><em class="italic">arXiv:1910.00883, 2019.</em></span></li>
				<li><em class="italic">Xu, Y. and R. Goodacre, On splitting training and validation set: a comparative study of cross-validation, bootstrap and systematic sampling for estimating the generalization performance of supervised learning. Journal of analysis and testing, 2018. 2(3): </em><span class="No-Break"><em class="italic">p. 249-262.</em></span></li>
				<li><em class="italic">Mosin, V., et al. Comparing Input Prioritization Techniques for Testing Deep Learning Algorithms. in 2022 48th Euromicro Conference on Software Engineering and Advanced Applications (SEAA). </em><span class="No-Break"><em class="italic">2022. IEEE.</em></span></li>
				<li><em class="italic">Liu, X.-Y., J. Wu, and Z.-H. Zhou, Exploratory undersampling for class-imbalance learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 2008. 39(2): </em><span class="No-Break"><em class="italic">p. 539-550.</em></span></li>
				<li><em class="italic">Atla, A., et al., Sensitivity of different machine learning algorithms to noise. Journal of Computing Sciences in Colleges, 2011. 26(5): </em><span class="No-Break"><em class="italic">p. 96-103.</em></span></li>
			</ul>
		</div>
	</body></html>