- en: Chapter 4. Generalized Linear Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For regression tasks where the goal is to predict a numerical output, such as
    a price or temperature, we've seen that linear regression can potentially be a
    good starting point. It is simple to train and easy to interpret even though,
    as a model, it makes strict assumptions about the data and the underlying target
    function. Before studying more advanced techniques to tackle regression problems,
    we'll introduce **logistic regression**. Despite its somewhat misleading name,
    this is actually our first model for performing classification. As we learned
    in [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    in classification problems, our output is qualitative and thus comprises a finite
    set of values, which we call classes. We'll begin by thinking about the binary
    classification scenario, where we are trying to distinguish between two classes,
    which we'll arbitrarily label as 0 and 1, and later on we'll extend this to distinguishing
    between multiple classes. Finally, we'll finish up by touching on additional regression
    methods, Poisson regression and Negative Binomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying with linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though we know classification problems involve qualitative outputs, it
    seems natural to ask whether we could use our existing knowledge of linear regression
    and apply it to the classification setting. We could do this by training a linear
    regression model to predict a value in the interval [0, 1], remembering that we've
    chosen to label our two classes as 0 and 1\. Then, we could apply a threshold
    to the output of our model in such a way that, if the model outputs a value below
    0.5, we would predict class 0; otherwise, we would predict class 1\.
  prefs: []
  type: TYPE_NORMAL
- en: The following graph demonstrates this concept for a simple linear regression
    with a single input feature X1 and for a binary classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![Classifying with linear regression](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Our output variable *y* is either 0 or 1, so all the data lies on two horizontal
    lines. The solid line shows the output of the model, and the dashed line shows
    the decision boundary, which arises when we put a threshold on the model's predicted
    output at the value 0.5\. Points to the left of the dashed line are predicted
    as belonging to class 0, and points to the right are predicted as belonging to
    class 1.
  prefs: []
  type: TYPE_NORMAL
- en: The model is clearly not perfect, but it does seem to correctly classify a large
    proportion of the data.
  prefs: []
  type: TYPE_NORMAL
- en: While a good approximation in this case, this approach doesn't feel right for
    a number of reasons. Firstly, although we know before hand that our output variable
    is limited to the interval [0, 1] because we have just two classes, the raw output
    from the linear regression predicts values outside this range. We can see this
    from the graph for values of input feature *X1* that are either very low or very
    high. Secondly, linear regression is designed to solve the problem of minimizing
    the MSE, which does not seem appropriate for us in this case. Our goal is really
    to find a way to separate the two classes, not to minimize the mean squared error
    against a line of best fit. As a consequence of this fact, the location of the
    decision boundary is very sensitive to the presence of high leverage points. As
    we discussed in [Chapter 2](part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 2. Tidying Data and Measuring Performance"), *Linear Regression*, high
    leverage points are points that lie far away from most of the data because they
    have extreme values for at least one of their input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot demonstrates the effect of high leverage points on our classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classifying with linear regression](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the data is exactly the same as before, except that we have added two
    new observations for class 1 that have relatively high values for feature *X1*
    and thus appear far to the right of the graph. Now ideally, because these two
    newly added observations are well into the area of the graph where we predict
    class 1, they should not impact our decision boundary so heavily. Due to the fact
    that we are minimizing the MSE, the old linear regression line (shown as a solid
    line) has now shifted to the right (shown as a dashed line). Consequently, the
    point at which our new linear regression line crosses 0.5 on the *y* axis has
    moved to the right. Thus, our decision boundary has noticeably moved to the right
    as a result of adding only two new points.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression addresses all of these points by providing an output that
    is bounded by the interval [0,1] and is trained using an entirely different optimization
    criterion from linear regression, so we are no longer fitting a function by minimizing
    the MSE, as we'll now see.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In logistic regression, input features are linearly scaled just as with linear
    regression; however, the result is then fed as an input to the **logistic function**.
    This function provides a nonlinear transformation on its input and ensures that
    the range of the output, which is interpreted as the probability of the input
    belonging to class 1, lies in the interval [0,1]. The form of the logistic function
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to logistic regression](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a plot of the logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to logistic regression](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: When *x = 0*, the logistic function takes the value 0.5\. As *x* tends to *+∞*,
    the exponential in the denominator vanishes and the function approaches the value
    1\. As *x* tends to *-∞*, the exponential, and hence the denominator, tends to
    move towards infinity and the function approaches the value 0\. Thus, our output
    is guaranteed to be in the interval [0,1], which is necessary for it to be a probability.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression belongs to a class of models known as **generalized linear
    models** (**GLMs**). Generalized linear models have three unifying characteristics.
    The first of these is that they all involve a linear combination of the input
    features, thus explaining part of their name. The second characteristic is that
    the output is considered to have an underlying probability distribution belonging
    to the family of exponential distributions. These include the normal distribution,
    the Poisson, and the binomial distribution. Finally, the mean of the output distribution
    is related to the linear combination of input features by way of a function, known
    as the **link function**. Let''s see how this all ties in with logistic regression,
    which is just one of many examples of a GLM. We know that we begin with a linear
    combination of input features, so for example, in the case of one input feature,
    we can build up an *x* term as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generalized linear models](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that, in the case of logistic regression, we are modeling the probability
    that the output belongs to class 1, rather than modeling the output directly as
    we were in linear regression. As a result, we do not need to model the error term
    because our output, which is a probability, directly incorporates the inherent
    randomness of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we apply the logistic function to this term in order to produce our model''s
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generalized linear models](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the left term tells us directly that we are computing the probability
    that our output belongs to class 1, based on our evidence of seeing the value
    of the input feature *X1*. For logistic regression, the underlying probability
    distribution of the output is the Bernoulli distribution. This is the same as
    the binomial distribution with a single trial, and is the distribution we would
    obtain in an experiment with only two possible outcomes having constant probability,
    such as a coin flip.
  prefs: []
  type: TYPE_NORMAL
- en: The mean of the Bernoulli distribution, *μy*, is the probability of the (arbitrarily
    chosen) outcome for success, in this case, class 1\. Consequently, the left-hand
    side in the previous equation is also the mean of our underlying output distribution.
    For this reason, the function that transforms our linear combination of input
    features is sometimes known as the **mean function**, and we just saw that this
    function is the logistic function for logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to determine the link function for logistic regression, we can perform
    some simple algebraic manipulations in order to isolate our linear combination
    of input features.
  prefs: []
  type: TYPE_NORMAL
- en: '![Generalized linear models](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The term on the left-hand side is known as the **log-odds** or **logit function**
    and is the link function for logistic regression. The denominator of the fraction
    inside the logarithm is the probability of the output being class 0 given the
    data. Consequently, this fraction represents the ratio of probability between
    class 1 and class 0, which is also known as the **odds ratio**:'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A good reference for logistic regression along with examples of other GLMs such
    as Poisson regression is *Extending the Linear Model with R*, *Julian J. Faraway*,
    *CRC Press*.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting coefficients in logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking at the right-hand side of the last equation, we can see that we have
    almost exactly the same form that we had for simple linear regression, barring
    the error term. The fact that we have the logit function on the left-hand side,
    however, means we cannot interpret our regression coefficients in the same way
    that we did with linear regression. In logistic regression, a unit increase in
    feature *Xi* results in multiplying the odds ratio by an amount,![Interpreting
    coefficients in logistic regression](img/00070.jpeg). When a coefficient *βi*
    is positive, then we multiply the odds ratio by a number greater than 1, so we
    know that increasing the feature *Xi* will effectively increase the probability
    of the output being labeled as class 1\.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, increasing a feature with a negative coefficient shifts the balance
    toward predicting class 0\. Finally, note that when we change the value of an
    input feature, the effect is a multiplication on the odds ratio and not on the
    model output itself, which we saw is the probability of predicting class 1\. In
    absolute terms, the change in the output of our model as a result of a change
    in the input is not constant throughout, but depends on the current value of our
    input features. This is, again, different from linear regression, where irrespective
    of the values of the input features, the regression coefficients always represent
    a fixed increase in the output per unit increase of an input feature.
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions of logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression makes fewer assumptions about the input than linear regression.
    In particular, the nonlinear transformation of the logistic function means that
    we can model more complex input-output relationships. We still have a linearity
    assumption, but in this case, it is between the features and the log-odds. We
    no longer require a normality assumption for residuals and nor do we need the
    homoscedastic assumption. On the other hand, our error terms still need to be
    independent. Strictly speaking, the features themselves no longer need to be independent
    but, in practice, our model will still face issues if the features exhibit a high
    degree of multicollinearity. Finally, we'll note that, just as with unregularized
    linear regression, feature scaling does not affect the logistic regression model.
    This means that centering and scaling a particular input feature will simply result
    in an adjusted coefficient in the output model, without any repercussions on the
    model performance. It turns out that, for logistic regression, this is the result
    of a property known as the **invariance property of maximum likelihood**. Maximum
    likelihood is the method used to select the coefficients and will be the focus
    of the next section. It should be noted, however, that centering and scaling features
    might still be a good idea if they are on very different scales. This is done
    to assist the optimization procedure during training. In short, we should turn
    to feature scaling only if we run into model convergence issues.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we studied linear regression, we found our coefficients by minimizing the
    sum of squared error terms. For logistic regression, we do this by maximizing
    the **likelihood** of the data. The likelihood of an observation is the probability
    of seeing that observation under a particular model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the likelihood of seeing an observation *X* for class 1 is simply
    given by the probability *P(Y=1|X)*, the form of which was given earlier in this
    chapter. As we only have two classes, the likelihood of seeing an observation
    for class 0 is given by *1 - P(Y=1|X)*. The overall likelihood of seeing our entire
    dataset of observations is the product of all the individual likelihoods for each
    data point as we consider our observations to be independently obtained. As the
    likelihood of each observation is parameterized by the regression coefficients
    *βi*, the likelihood function for our entire dataset is also, therefore, parameterized
    by these coefficients. We can express our likelihood function as an equation,
    as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, this equation simply computes the probability that a logistic regression
    model with a particular set of regression coefficients could have generated our
    training data. The idea is to choose our regression coefficients so that this
    likelihood function is maximized. We can see that the form of the likelihood function
    is a product of two large products from the two big *π* symbols. The first product
    contains the likelihood of all our observations for class 1, and the second product
    contains the likelihood of all our observations for class 0\. We often refer to
    the **log likelihood** of the data, which is computed by taking the logarithm
    of the likelihood function. Using the fact that the logarithm of a product of
    terms is the sum of the logarithm of each term we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can simplify this even further using a classic trick to form just a single
    sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To see why this is true, note that for the observations where the actual value
    of the output variable *y* is 1, the right term inside the summation is zero,
    so we are effectively left with the first sum from the previous equation. Similarly,
    when the actual value of *y* is 0, then we are left with the second summation
    from the previous equation. Understanding the form of the log likelihood is important,
    and we'll get some practice with this when we start working with R to train a
    logistic regression model in the next section. Note that maximizing the likelihood
    is equivalent to maximizing the log likelihood; both approaches will yield the
    same parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimation is a fundamental technique of parameter fitting,
    and we will encounter it in other models in this book. Despite its popularity,
    it should be noted that maximum likelihood is not a panacea. Alternative training
    criteria on which to build a model do exist, and there are some well-known scenarios
    under which this approach does not lead to a good model, as we shall see in subsequent
    chapters. Finally, note that the details of the actual optimization procedure,
    which finds the values of the regression coefficients for maximum likelihood,
    are beyond the scope of this book and in general, we can rely on R to implement
    this for us.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting heart disease
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll put logistic regression for the binary classification task to the test
    with a real-world dataset from the UCI Machine Learning Repository. This time,
    we will be working with the *Statlog (Heart) dataset*, which we will refer to
    as the *heart dataset* henceforth for brevity. The dataset can be downloaded from
    the UCI Machine Repository''s website at [http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29](http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29).
    The data contains 270 observations for patients with potential heart problems.
    Of these, 120 patients were shown to have heart problems, so the split between
    the two classes is fairly even. The task is to predict whether a patient has a
    heart disease based on their profile and a series of medical tests. First, we''ll
    load the data into a data frame and rename the columns according to the website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table contains the definitions of our input features and the
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Type | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `AGE` | Numerical | Age (years) |'
  prefs: []
  type: TYPE_TB
- en: '| `SEX` | Binary | Gender |'
  prefs: []
  type: TYPE_TB
- en: '| `CHESTPAIN` | Categorical | 4-valued chest pain type |'
  prefs: []
  type: TYPE_TB
- en: '| `RESTBP` | Numerical | Resting blood pressure (beats per minute) |'
  prefs: []
  type: TYPE_TB
- en: '| `CHOL` | Numerical | Serum cholesterol (mg/dl) |'
  prefs: []
  type: TYPE_TB
- en: '| `SUGAR` | Binary | Is the fasting blood sugar level > 120 mg/dl? |'
  prefs: []
  type: TYPE_TB
- en: '| `ECG` | Categorical | 3-valued resting electrocardiographic results |'
  prefs: []
  type: TYPE_TB
- en: '| `MAXHR` | Numerical | Maximum heart rate achieved (beats per minute) |'
  prefs: []
  type: TYPE_TB
- en: '| `ANGINA` | Binary | Was angina induced by exercise? |'
  prefs: []
  type: TYPE_TB
- en: '| `DEP` | Numerical | ST depression induced by exercise relative to rest |'
  prefs: []
  type: TYPE_TB
- en: '| `EXERCISE` | Ordered categorical | Slope of the peak exercise ST segment
    |'
  prefs: []
  type: TYPE_TB
- en: '| `FLUOR` | Numerical | The number of major vessels colored by fluoroscopy
    |'
  prefs: []
  type: TYPE_TB
- en: '| `THAL` | Categorical | 3-valued Thal |'
  prefs: []
  type: TYPE_TB
- en: '| `OUTPUT` | Binary | Presence or absence of a heart disease |'
  prefs: []
  type: TYPE_TB
- en: 'Before we train a logistic regression model for these data, there are a couple
    of preprocessing steps that we should perform. A common pitfall when working with
    numerical data is the failure to notice when a feature is actually a categorical
    variable and not a numerical variable when the levels are coded as numbers. In
    the heart dataset, we have four such features. The `CHESTPAIN`, `THAL`, and `ECG`
    features are all categorical features. The `EXERCISE` variable, although an ordered
    categorical variable, is nonetheless a categorical variable, so it will have to
    be coded as a factor as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    we saw how we can transform categorical features with many levels into a series
    of binary valued indicator variables. By doing this, we can use them in a model
    such as linear or logistic regression, which requires all the inputs to be numerical.
    As long as the relevant categorical variables in a data frame have been coded
    as factors, R will automatically apply a coding scheme when performing logistic
    regression. Concretely, R will treat one of the *k* factor levels as a reference
    level and create *k-1* binary features from the other factor levels. We'll see
    visual evidence of this when we study the summary output of the logistic regression
    model that we'll train.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we should observe that the `OUTPUT` variable is coded so that class 1
    corresponds to the absence of heart disease and class 2 corresponds to the presence
    of heart disease. As a final change, we''ll want to recode the `OUTPUT` variable
    so that we will have the familiar class labels of 0 and 1, respectively. This
    is done by simply subtracting `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Our data frame is now ready. Before we train our model, however, we will split
    our data frame into two parts, for training and testing, exactly as we did for
    linear regression. Once again, we''ll use an 85-15 split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have 230 observations in our training set and 40 observations in our
    test set. To train a logistic regression model in R, we use the `glm()` function,
    which stands for generalized linear model. This function can be used to train
    various generalized linear models, but we''ll focus on the syntax and usage for
    logistic regression here. The call is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the format is very similar to what we saw with linear regression.
    The first parameter is the model formula, which identifies the output variable
    and which features we want to use (in this case, all of them). The second parameter
    is the data frame and the final `family` parameter is used to specify that we
    want to perform logistic regression. We can use the `summary()` function to find
    out more about the model we just trained, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Assessing logistic regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The summary of the logistic regression model produced with the `glm()` function
    has a similar format to that of the linear regression model produced with the
    `lm()` function. This shows us that, for our categorical variables, we have one
    fewer binary features than the number of levels in the original variable, so for
    example, the three-valued `THAL` input feature produced two binary variables labeled
    `THAL6` and `THAL7`. We'll begin by looking first at the regression coefficients
    that are predicted with our model. These are presented with their corresponding
    **z-statistic**. This is analogous to the t-statistic that we saw in linear regression,
    and again, the higher the absolute value of the z-statistic, the more likely it
    is that this particular feature is significantly related to our output variable.
    The p-values next to the z-statistic express this notion as a probability and
    are annotated with stars and dots, as they were in linear regression, indicating
    the smallest confidence interval that includes the corresponding p-value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the fact that logistic regression models are trained with the maximum
    likelihood criterion, we use the standard normal distribution to perform significance
    tests on our coefficients. For example, to reproduce the p-value for the `THAL7`
    feature that corresponds to the listed z-value of 3.362, we can write the following
    (set the `lower.tail` parameter to `T` when testing negative coefficients):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An excellent reference for learning about the essential concepts of distributions
    in statistics is *All of Statistics*, *Larry Wasserman*, *Springer*.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the model summary, we see that `FLUOR`, `CHESTPAIN4`, and `THAL7` are
    the strongest feature predictors for heart diseases. A number of input features
    have relatively high p-values. This indicates that they are probably not good
    indicators of heart disease in the presence of the other features. We''ll stress
    once again the importance of interpreting this table correctly. The table does
    not say that heart age, for example, is not a good indicator for heart disease;
    rather, it says that, in the presence of the other input features, age does not
    really add much to the model. Furthermore, note that we almost definitely have
    some degree of collinearity in our features as the regression coefficient of age
    is negative, whereas we would expect that the likelihood of heart disease increases
    with age. Of course, this assumption is valid only in the absence of all other
    input features. Indeed, if we retrain a logistic regression model with only the
    `AGE` variable, we get a positive regression coefficient as well as a low p-value,
    both of which support our belief that the features are collinear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that the AIC value of this simpler model is higher than what we obtained
    with the full model, so we would expect this simple model to be worse.
  prefs: []
  type: TYPE_NORMAL
- en: Model deviance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand the remainder of the model summary, we need to introduce an important
    concept known as **deviance**. In linear regression, our residuals were defined
    simply as the difference between the predicted value and the actual value of the
    output that we are trying to predict. Logistic regression is trained using maximum
    likelihood, so it is natural to expect that an analogous concept to the residual
    would involve the likelihood. There are several closely-related definitions of
    the concept of deviance. Here, we will use the definitions that the `glm()` function
    uses in order to explain the model's output. The deviance of an observation can
    be computed as the -2 times the log likelihood of that observation. The deviance
    of a dataset is just the sum of all the observation deviances.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **deviance residual** of an observation is derived from the deviance itself
    and is analogous to the residual of a linear regression. It can be computed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model deviance](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For an observation *i*, *dr[i]* represents the deviance residual and *di* represents
    the deviance. Note that squaring a deviance residual effectively eliminates the
    sign function and produces just the deviance of the observation. Consequently,
    the sum of squared deviance residuals is the deviance of the dataset, which is
    just the log likelihood of the dataset scaled by the constant -2\. Consequently,
    maximizing the log likelihood of the data is the same as minimizing the sum of
    the squared deviance residuals, so our analogy with linear regression is complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to reproduce the results that are shown in the model summary, and
    to understand how deviance is computed, we''ll write some of our own functions
    in R. We''ll begin by computing the log likelihood for our dataset using the equation
    for the log likelihood that we saw earlier on in this chapter. From the equation,
    we''ll create two functions. The `log_likelihoods()` function computes a vector
    of log likelihoods for all the observations in a dataset, given the probabilities
    that the model predicts and the actual target labels, and `dataset_log_likelihood()`
    sums these up to produce the log likelihood of a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can use the definition of deviance to compute two analogous functions:
    `deviances()` and `dataset_deviance()`. The first of these computes a vector of
    observation deviances, and the second sums these up for the whole dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Given these functions, we can now create a function that will compute the deviance
    of a model. To do this, we need to use the `predict()` function in order to compute
    the model''s probability predictions for the observations in the training data.
    This works just as with linear regression, except that by default it returns probabilities
    on the logit scale. To ensure that we get actual probabilities, we need to specify
    the value of `response` for the `type` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether our function is working, let''s compute the model deviance,
    also known as the residual deviance, for our heart model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Reassuringly, this is the same value as that listed in our model summary. One
    way to evaluate a logistic regression model is to compute the difference between
    the model deviance and the deviance of the null model, which is the model trained
    without any features. The deviance of the null model is known as **null deviance**.
    The null model predicts class 1 via a constant probability, as it has no features.
    This probability is estimated via the proportion of the observations of class
    1 in the training data, which we can obtain by simply averaging the `OUTPUT` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we see that we have reproduced the value that R computes for us
    in the model summary. The residual deviance and null deviance are analogous to
    the **Residual Sum of Squares** (**RSS**) and the **True Sum of Squares** (**TSS**)
    that we saw in linear regression. If the difference between these two is high,
    the interpretation is similar to the notion of the residual sum of squares in
    linear regression *explaining away* the variance observed by the output variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with this analogy, we can define a **pseudo R2** value for our model
    using the same equation that we used to compute the R2 for linear regression,
    but substituting in the deviances. We implement this in R as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our logistic regression model is said to explain roughly 56 % of the null deviance.
    This is not particularly high; most likely, we don't have a rich enough feature
    set to make accurate predictions with a logistic model. Unlike linear regression,
    it is possible for the pseudo R2 to exceed 1, but this only happens under problematic
    circumstances where the residual deviance exceeds the null deviance. If this happens,
    we should not trust the model and proceed with feature selection methods, or try
    out alternative models.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the pseudo R2, we may also want a statistical test to check whether
    the difference between the null deviance and the residual deviance is significant.
    The absence of a p-value next to the residual deviance in the model summary indicates
    that R has not created any test. It turns out that the difference between the
    residual and null deviances is approximately and asymptotically distributed with
    a *χ2* (pronounced *CHI squared*) distribution. We'll define a function to compute
    a p-value for this difference, but this is only an approximation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need the difference between the null deviance and the residual deviance.
    We also need the degrees of freedom for this difference, which are computed simply
    by subtracting the number of degrees of freedom of our model from those of the
    null model. The null model only has an intercept, so the number of degrees of
    freedom is the total number of observations in our dataset minus 1\. For the residual
    deviance, we are computing a number of regression coefficients, including the
    intercept, so we need to subtract this number from the total number of observations.
    Finally, we use the `pchisq()` function to obtain a `p-value`, noting that we
    are creating an upper tail computation and hence need to set the `lower.tail`
    parameter to `FALSE`. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `p-value` that we obtain is tiny, so we feel certain that our model produces
    predictions that are better than average guessing. In our original model summary
    we also saw a summary of the deviance residuals. Using the definition of a deviance
    residual that we gave earlier, we''ll define a function to compute the vector
    of deviance residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use the `summary()` function on the deviance residuals that
    we obtain with our `model_deviance_residuals()` function to obtain a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we can verify that we obtain the correct result. Our model summary
    provides us with one final diagnostic fisher scoring iterations, which we have
    not yet discussed. This number is typically in the range of 4 to 8 and is a convergence
    diagnostic. If the optimization procedure that R uses to train the logistic model
    has not converged, we expect to see a number that is high. If this happens, our
    model is suspect and we may not be able to use it to make predictions. In our
    case, we are within the expected range.
  prefs: []
  type: TYPE_NORMAL
- en: Test set performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve seen how we can use the `predict()` function to compute the output of
    our model. This output is the probability of the input belonging to class 1\.
    We can perform binary classification by applying a threshold. We''ll do this with
    both our training and test data and compare them with our expected outputs to
    measure the classification accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The classification accuracies on the training and test sets are very similar
    and are close to 90 %. This is a very good starting point for a modeler to work
    from. The coefficients table in our model showed us that several features did
    not seem to be significant, and we also saw a degree of collinearity that means
    we could now proceed with variable selection and possibly look for more features,
    either through computation or by obtaining additional data about our patients.
    The pseudo R2 computation showed us that we did not explain enough of the deviance
    in our model, which also supports this.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization with the lasso
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter on linear regression, we used the `glmnet` package
    to perform regularization with ridge regression and the lasso. As we''ve seen
    that, it might be a good idea to remove some of our features, we''ll try applying
    lasso to our dataset and assess the results. First, we''ll train a series of regularized
    models with `glmnet()` and then we will use `cv.glmnet()` to estimate a suitable
    value for *λ*. Then, we''ll examine the coefficients of our regularized model
    using this *λ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that a number of our features have effectively been removed from the
    model because their coefficients are zero. If we now use this model to measure
    the classification accuracy on our training and test sets, we observe that in
    both cases, we get slightly better performance. Even if this difference is small,
    remember that we have achieved this using three fewer features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Classification metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we looked at the test set accuracy for our model, we know from [Chapter
    1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7 "Chapter 1. Gearing
    Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*, that the binary
    confusion matrix can be used to compute a number of other useful performance metrics
    for our data, such as precision, recall, and the *F* measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll compute these for our training set now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we used the trick of bracketing our assignment statements to simultaneously
    assign the result of an expression to a variable and print out the value assigned.
    Now, recall is the ratio of correctly identified instances of class 1, divided
    by the total number of observations that actually belong to class 1\. In a medical
    context such as ours, this is also known as **sensitivity**, as it is an effective
    measure of a model''s ability to detect or be sensitive to a particular condition.
    Recall is also known as the true positive rate. There is an analogous measure
    known as **specificity**, which is the false negative rate. This involves the
    mirror computation of recall for class 0, that is, the correctly identified members
    of class 0 over all the observations of class 0 in our dataset. In our medical
    context, for example, the interpretation of specificity is that it measures the
    model''s ability to reject observations that do not have the condition represented
    by class 1 (in our case, heart disease). We can compute the specificity of our
    model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In computing these metrics, we begin to see the importance of setting the threshold
    at `0.5`. If we were to choose a different threshold, it should be clear that
    all of the preceding metrics would change. In particular, there are many circumstances,
    our current medical context being a prime example, in which we may want to adjust
    our threshold to be biased towards identifying members of class 1\. For example,
    suppose our model was being used by a clinician to determine whether to have a
    patient undergo a more detailed and expensive examination for heart disease. We
    would probably consider that mislabeling a patient with a heart condition as healthy
    is a more serious mistake to make than asking a healthy patient to undergo further
    tests because they were deemed unhealthy. To achieve this bias, we could lower
    our classification threshold to `0.3` or `0.2`, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, what we would like is a visual way to assess the effect of changing
    the threshold on our performance metrics, and the precision recall curve is one
    such useful plot. In R, we can use the `ROCR` package to obtain precision-recall
    curves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can then plot the `perf` object to obtain our precision recall curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification metrics](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The graph shows us, for example, that, to obtain values of recall above 0.8,
    we'll have to sacrifice precision quite abruptly. To fine-tune our threshold,
    we'll want to see the individual thresholds that were used to compute this graph.
    A useful exercise is to create a data frame of cutoff values, which are the threshold
    values for which precision and recall change in our data, along with their corresponding
    precision and recall values. We can then subset this data frame to inspect individual
    thresholds that interest us.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we want to find a suitable threshold so that we have at
    least 90 % recall and 80 % precision. We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, a threshold of roughly 0.35 will satisfy our requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed that we used the `@` symbol to access some of the attributes
    of the `perf` object. This is because this object is a special type of object
    known as an S4 class. S4 classes are used to provide object-oriented features
    in R. A good reference to learn about S4 classes and object-orientated programming
    in R more generally is *Advanced R*, *Hadley Wickham*, *Chapman and Hall*.
  prefs: []
  type: TYPE_NORMAL
- en: Extensions of the binary logistic classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, the focus of this chapter has been on the binary classification task
    where we have two classes. We'll now turn to the problem of multiclass prediction.
    In [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    we studied the iris dataset where the goal is to distinguish between three different
    species of iris, based on features that describe the external appearance of iris
    flower samples. Before presenting additional examples of multiclass problems,
    we'll state an important caveat. The caveat is that several other methods for
    classification that we will study in this book, such as neural networks and decision
    trees, are both more natural and more commonly used than logistic regression for
    classification problems involving more than two classes. With that in mind, we'll
    turn to multinomial logistic regression, our first extension of the binary logistic
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose our target variable comprises *K* classes. For example, in the iris
    dataset, *K = 3*. **Multinomial logistic regression** tackles the multiclass problem
    by fitting *K-1* independent binary logistic classifier models. This is done by
    arbitrarily choosing one of the output classes as a reference class and fitting
    *K-1* regression models that compare each of the remaining classes to this one.
    For example, if we have two features, *X1* and *X2*, and three classes, which
    we could call 0, 1, and 2, we construct the following two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multinomial logistic regression](img/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we used class 0 as the baseline and built two binary regression models.
    In the first, we compared class 1 against class 0 and in the second, we compare
    class 2 against class 0\. Note that because we now have more than one binary regression
    model, our model coefficients have two subscripts. The first subscript identifies
    the model and the second subscript pairs the coefficient with a feature. For example,
    *β[12]* is the coefficient of feature *X[2]* in the first model. We can write
    a general expression for the probability that our combined model predicts class
    *k* when there are *K* classes in total, numbered from *0* to *K-1*, and class
    0 is chosen as the reference class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multinomial logistic regression](img/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The reader should verify that the sum of all the output class probabilities
    is 1, as required. This particular mathematical form of an exponential divided
    by a sum of exponentials is known as the **softmax** function. For our three-class
    problem discussed previously, we simply substitute *K=3* in the preceding equations.
    At this point, we should mention some important characteristics of this approach.
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, we are training one fewer models than the total number of classes
    in our output variable, and as a result, it should be easy to see that this approach
    does not scale very well when we have a large number of output classes from which
    to choose. The fact that we are building and training so many models also means
    that we tend to need a much larger dataset to produce results with reasonable
    accuracy. Finally, as we independently compare each output class to a reference
    class, we make an assumption, known as the **Independence of Irrelevant Alternatives**
    (**IIA**) assumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'The IIA assumption, in a nutshell, states that the odds of predicting one particular
    output class over another do not depend on whether we increase the number of possible
    output classes *k* by adding new classes. To illustrate this, suppose for simplicity
    that we model our iris dataset using multinomial logistic regression, and the
    odds of the output classes are 0.33 : 0.33 : 0.33 for the three different species
    so that every species is in a 1 : 1 ratio with every other species. The IIA assumption
    states that if we refit a model that includes samples of a new type of iris, for
    example, ensata (the Japanese iris), the odds ratio between the previous three
    iris species is maintained. A new overall odds ratio of 0.2 : 0.2 : 0.2 : 0.4
    between the four species (where the 0.4 corresponds to the ensata) would be valid,
    for example, because the 1 : 1 ratios between the old three species are maintained.'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting glass type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we''ll demonstrate how we can train multinomial logistic regression
    models in R by way of an example dataset. The data we''ll examine is from the
    field of forensic science. Here, our goal is to examine properties of glass fragments
    found in crime scenes and predict the source of these fragments, for example,
    headlamps. The *glass identification dataset* is hosted by the UCI Machine Learning
    Repository at [http://archive.ics.uci.edu/ml/datasets/Glass+Identification](http://archive.ics.uci.edu/ml/datasets/Glass+Identification).
    We''ll first load the data in a data frame, rename the columns using information
    from the website, and throw away the first column (a unique identifier for each
    sample), as this has been arbitrarily assigned and is not needed by our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll look at a table showing what each column in our data frame represents:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Type | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `RI` | Numerical | Refractive index |'
  prefs: []
  type: TYPE_TB
- en: '| `Na` | Numerical | Percentage of Sodium Oxide by weight |'
  prefs: []
  type: TYPE_TB
- en: '| `Mg` | Numerical | Percentage of Magnesium Oxide by weight |'
  prefs: []
  type: TYPE_TB
- en: '| `Al` | Numerical | Percentage of Aluminium Oxide by weight |'
  prefs: []
  type: TYPE_TB
- en: '| `Si` | Numerical | Percentage of Silicon Oxide by weight |'
  prefs: []
  type: TYPE_TB
- en: '| `K` | Numerical | Percentage of Potassium Oxide by weight |'
  prefs: []
  type: TYPE_TB
- en: '| `Ca` | Numerical | Percentage of Calcium Oxide by weight |'
  prefs: []
  type: TYPE_TB
- en: '| `Ba` | Numerical | Percentage of Barium Oxide by weight |'
  prefs: []
  type: TYPE_TB
- en: '| `Fe` | Numerical | Percentage of Iron Oxide by weight |'
  prefs: []
  type: TYPE_TB
- en: '| `Type` | Categorical | Type of glass (1: float processed building windows,
    2: nonfloat processed building windows, 3: float processed vehicle windows, 4:
    nonfloat processed vehicle windows, 5: containers, 6: tableware, 7: headlamps)
    |'
  prefs: []
  type: TYPE_TB
- en: 'As usual, we''ll proceed by preparing a training and test set for our glass
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to perform multinomial logistic regression, we will use the `nnet` package.
    This package also contains functions that work with neural networks, so we will
    revisit this package in the next chapter as well. The `multinom()`function is
    used for multinomial logistic regression. This works by specifying a formula and
    a data frame, so it has a familiar interface. In addition, we can also specify
    the `maxit` parameter that determines the maximum number of iterations for which
    the underlying optimization procedure will run. Sometimes, we may find that training
    a model returns an error to the effect that convergence was not reached. In this
    case, one possible approach is to increase this parameter and allow the model
    to train over a larger number of iterations. In doing so, however, we should be
    aware of the fact that the model may take longer to train:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our model summary shows us that we have five sets of coefficients. This is because
    our `TYPE` output variable has six levels, which is to say that we are choosing
    to predict one of six different sources of glass. There are no examples in the
    data where `Type` takes the value 4\. The model also shows us standard errors,
    but no significance tests. In general, testing for coefficient significance is
    a lot trickier than with binary logistic regression, and this is one of the weaknesses
    of this approach. Often, we resort to independently testing the significance of
    coefficients for each of the binary models that we trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'We won''t dwell on this any further, but will instead check the overall accuracy
    on our training data to give us a sense of the overall quality of fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Our training accuracy is 72 %, which is not especially high. Here is the confusion
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix reveals certain interesting facts. The first of these
    is that it seems that the model does not distinguish well between the first two
    classes, as many of the errors that are made involve these two. Part of the reason
    for this, however, is that these two classes are the most frequent in the data.
    The second problem that we are seeing is that the model never predicts class 3\.
    In fact, it completely confuses this class with the first two classes. The seven
    examples of class 6 are perfectly distinguished, and accuracy for class 7 is also
    near perfect, with only 1 mistake out of 25\. Overall, 72 % accuracy on training
    data is considered mediocre, but given the fact that we have six output classes
    and only 172 observations in our training data, this is to be expected with this
    type of model. Let''s repeat this for the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the confusion matrix paints a fairly similar picture to what
    we saw in training. Again, our model never predicts class 3 and the first two
    classes are still hard to distinguish. The number of observations in our test
    set is only 42, so this is very small. The test set accuracy is only 64 %, somewhat
    less than we saw in training. If our sample sizes were larger, we might suspect
    that our model suffers from overfitting, but in this case the variance of our
    test set performance is high due to the small sample size.
  prefs: []
  type: TYPE_NORMAL
- en: With multinomial logistic regression, we assumed that there was no natural ordering
    to the output classes. If our output variable is an ordinal, also known as an
    **ordered factor**, we can train a different model known as **ordinal logistic
    regression**. This is our second extension of the binary logistic regression model
    and is presented in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ordered factors are very common in a number of scenarios. For example, human
    responses to surveys are often on subjective scales with scores ranging from 1
    to 5 or using qualitative labels with an intrinsic ordering such as *disagree*,
    *neutral*, and *agree*. We can try to treat these problems as regression problems,
    but we will still face similar issues to those we experienced when treating the
    binary classification problem as a regression problem. Instead of trying to train
    *K-1* binary logistic regression models as multinomial logistic regression, ordinal
    logistic regression trains a single model with multiple thresholds on the output.
    In order to achieve this, it makes an important assumption known as the assumption
    of **proportional odds**. If we have *K* classes and want to put a threshold on
    the output of a single binary logistic regression model, we will need *K-1* thresholds
    or cutoff points. The proportional odds assumption is that, in the logit scale,
    all of these thresholds lie on a straight line. Put differently, the model uses
    a single set of *βi* coefficients determining the slope of the straight line,
    but there are *K-1* intercept terms. For a model with *p* features and an output
    variable with *K* classes numbered from 0 to *K-1*, our model predicts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ordinal logistic regression](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This assumption may be a little hard to visualize and is perhaps best understood
    by way of an example. Suppose that we are trying to predict the results of a survey
    on of public opinion about a particular government policy, based on demographic
    data about survey participants.
  prefs: []
  type: TYPE_NORMAL
- en: The output variable is an ordered factor that ranges from *strongly disagree*
    to *strongly agree* on a five-point scale (also known as a **Likert** scale).
    Suppose that *l[0]* is the log-odds of the probabilities of strongly disagreeing
    versus disagreeing or better, *l[1]* is the log-odds of the probabilities of disagreeing
    or strongly disagreeing versus at least being neutral, and so on until *l[3]*.
    These four log-odds *l[0]* to *l[3]* form an arithmetic sequence, which means
    that the distance between consecutive numbers is a constant.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though the proportional odds model is the most frequently cited logistic
    regression model that handles ordered factors, there are alternative approaches.
    A good reference that discusses the proportional odds model as well as other related
    models, such as the adjacent-category logistic model, is *Applied Logistic Regression
    Third Edition*, *Hosmer Jr.*, *Lemeshow*, and *Sturdivant*, published by *Wiley*.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting wine quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The dataset for our ordinal logistic regression example is the *wine quality
    dataset* from the *UCI Machine Learning Repository*. The observations in this
    dataset consist of wine samples taken from both red and white wines of the Portuguese
    Vinho Verde variety. The wine samples have been rated on a scale from 1 to 10
    by a number of wine experts. The goal of the dataset is to predict the rating
    that an expert will give to a wine sample, using a range of physiochemical properties,
    such as acidity and alcohol composition. The website is [https://archive.ics.uci.edu/ml/datasets/Wine+Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality).
    The data is split into two files, one for red wines and one for white wines. We
    will use the white wine dataset, as it contains a larger number of samples. In
    addition, for simplicity and because the distribution of wine samples by score
    is sparse, we will contract our original output variable to a three point scale
    from 0 to 2\. First, let''s load and process our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table shows our input features and output variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Type | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `fixed.acidity` | Numerical | Fixed Acidity (g(tartaric acid)/dm3) |'
  prefs: []
  type: TYPE_TB
- en: '| `volatile.acidity` | Numerical | Volatile acidity (g(acetic acid)/dm3) |'
  prefs: []
  type: TYPE_TB
- en: '| `citric.acid` | Numerical | Citric acid (g/dm3) |'
  prefs: []
  type: TYPE_TB
- en: '| `residual.sugar` | Numerical | Residual sugar (g/dm3) |'
  prefs: []
  type: TYPE_TB
- en: '| `chlorides` | Numerical | Chlorides (g(sodium chloride)/dm3) |'
  prefs: []
  type: TYPE_TB
- en: '| `free.sulfur.dioxide` | Numerical | Free Sulfur Dioxide (mg/dm3) |'
  prefs: []
  type: TYPE_TB
- en: '| `total.sulfur.dioxide` | Numerical | Total Sulfur Dioxide (mg/dm3) |'
  prefs: []
  type: TYPE_TB
- en: '| `density` | Numerical | Density (g/cm3) |'
  prefs: []
  type: TYPE_TB
- en: '| `pH` | Numerical | PH |'
  prefs: []
  type: TYPE_TB
- en: '| `sulphates` | Numerical | Sulphates (g(potassium sulphate)/dm3) |'
  prefs: []
  type: TYPE_TB
- en: '| `alcohol` | Numerical | Alcohol (% vol.) |'
  prefs: []
  type: TYPE_TB
- en: '| `quality` | Categorical | Wine quality (1 = Poor, 2 = Average, 3 = Good)
    |'
  prefs: []
  type: TYPE_TB
- en: 'First, we''ll prepare a training and test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll use the `polr()` function from the `MASS` package to train a proportional
    odds logistic regression model. Just as with the other model functions we have
    seen so far, we first need to specify a formula and a data frame with our training
    data. In addition, we must specify the `Hess` parameter to `TRUE` in order to
    obtain a model that includes additional information, such as standard errors on
    the coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model summary shows us that we have three output classes, and we have two
    intercepts. Now, in this dataset we have many wines that were rated average (either
    5 or 6) and as a result, this class is the most frequent. We''ll use the `table()`
    function to count the number of samples by the output score and then apply `prop.table()`
    to express these as relative frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Class 2, which corresponds to average wines, is by far the most frequent. In
    fact, a simple baseline model that always predicts this category would be correct
    74.6 % of the time. Let''s see whether our model does better than this. We''ll
    begin by looking at the fit on the training data and the corresponding confusion
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model performs only marginally better on the training data than our baseline
    model. We can see why this is the case—it predicts the average class (2) very
    often and almost never predicts class 1\. Repeating with the test set reveals
    a similar situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that our model is not a particularly good choice for this dataset.
    As we know, there are a number of possible reasons ranging from having chosen
    the wrong type of model to having insufficient features or the wrong kind of features.
    One aspect of the ordinal logistic regression model that we should always try
    to check is whether the proportional odds assumption is valid. There is no universally
    accepted way to do this, but a number of different statistical tests have been
    proposed in the literature. Unfortunately, it is very difficult to find reliable
    implementations of these tests in R. One simple test that is easy to do, however,
    is to train a second model using multinomial logistic regression. Then, we can
    compare the AIC value of our two models. Let''s do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The two models have virtually no difference in the quality of the fit. Let''s
    check their `AIC` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The AIC is lower in the multinomial logistic regression model, which suggests
    that we might be better off working with that model. Another possible avenue for
    improvement on this dataset would be to carry out feature selection. The `step()`
    function that we saw in the previous chapter, for example, also works on models
    trained with the `polr()` function. We'll leave this as an exercise for the reader
    to verify that we can, in fact, get practically the same level of performance
    by removing some of the features. Dissatisfied with the results of logistic regression
    on this latest dataset, we will revisit it in subsequent chapters in order to
    see whether more sophisticated classification models can do better.
  prefs: []
  type: TYPE_NORMAL
- en: Poisson regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another form of regression analysis is *Poisson* regression. This type of analysis
    is a generalized linear model or GLM, used to model *count* data.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the example in the previous section where wine samples had been rated
    (or ranked) on a scale from 1 to 10, count data is a (statistical) data type in
    which the observations can take only the non-negative integer values *{0, 1, 2,
    3, ...}*, and where these integers arise from *counting* rather than *ranking*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Poisson regression assumes the outcome of your analysis has a *Poisson distribution*
    – in that it expresses: the probability of a number of events occurring in a fixed
    interval of time if these events occur with a known average rate and independently
    of the time since the last event.'
  prefs: []
  type: TYPE_NORMAL
- en: An example model might be of the number of phone calls received by a software
    support center each hour. Predictors of the number of calls received include the
    number of days after a new version (of the software) has been released and the
    number of years the customer has been a user of the software (in keeping with
    our wine example, you could use Poisson regression to analyze the *number of bottles
    of wine sold during a month*, with perhaps *store location* and *the month of
    the year* as predictors).
  prefs: []
  type: TYPE_NORMAL
- en: A data scientist may also use a Poisson distribution for the number of events
    in other specified intervals, such as distance, area, or volume.
  prefs: []
  type: TYPE_NORMAL
- en: Negative Binomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Poisson regression assumes a (known) average, *Negative Binomial regression*
    is implemented using what is referred to as *maximum likelihood estimation*.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, although Poisson distribution assumes that the *mean and variance
    are the same*, sometimes data will show greater variability or *extra variation
    that is greater than the mean*. When this occurs, Negative Binomial regression
    is a better choice because of its greater flexibility in that regard.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, what if we consider that a university wants to predict the average
    number of days a student athlete may miss each year. Predictors (of the number
    of days of absence from class) include the type of sport the student athlete is
    a member of and their average GPA score. The variable **sport** is a four-level
    nominal variable indicating which sport the athlete participates in (in this case
    it's either `Football`, `Track`, `Field` `Hockey`, or `Volleyball`).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we profile our data, suppose we find the following statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We may look at the preceding statistics and see that the average numbers of
    days absent by sport seems to suggest that the variable sport is a good candidate
    for predicting the number of days absent because the mean value of the outcome
    appears to vary by the athlete chosen sport. However, looking at the standard
    deviations, we see that the variances within each type of sport are higher than
    the means within each level.
  prefs: []
  type: TYPE_NORMAL
- en: These are the conditional means and variances. These differences suggest that
    over-dispersion (greater variability) is present and that a Negative Binomial
    model would be perhaps more appropriate. You could still use Poisson regression,
    but the standard errors could be biased.
  prefs: []
  type: TYPE_NORMAL
- en: Negative Binomial regression takes advantage of one additional parameter (over
    Poisson regression) to fine-tune the variance independently from the mean (in
    this example it is the student athlete GPA score).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is the prototypical method for solving classification problems,
    just as linear regression was the prototypical example of a model to solve regression
    problems. In this chapter, we demonstrated why logistic regression offers a better
    way of approaching classification problems compared to linear regression with
    a threshold, by showing that the least squares criterion is not the most appropriate
    criterion to use when trying to separate two classes. We presented the notion
    of likelihood and its maximization as the basis for training a model. This is
    a very important concept that features time and again in various machine learning
    contexts. Logistic regression is an example of a generalized linear model. This
    is a model that relates the output variable to a linear combination of input features
    via a link function, which we saw was the logit function in this case. For the
    binary classification problem, we used R's `glm()` function to perform logistic
    regression on a real-world dataset and studied the model diagnostics to evaluate
    our model's performance. We discovered parallels with linear regression, in that
    the model produces deviance residuals that are analogous to least squared error
    residuals and that we can compute a pseudo R2 statistic that is analogous to the
    R2 statistic, which measures the goodness of fit in linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw that we can apply regularization techniques to logistic regression
    models. Our tour of binary classification with the logistic regression model ended
    by studying precision-recall curves in order to choose appropriate model thresholds,
    an exercise that is very important when the cost of misclassifying an observation
    is not symmetric for the two classes involved. We then investigated two possible
    extensions of the binary logistic regression model to handle outputs with many
    class labels. These were the multinomial logistic regression model and the ordinal
    logistic regression model, which can be useful when the output classes are ordered.
    Lastly, we touched on the use of Poisson regression and, for models with greater
    variability, Negative Binomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that logistic regression is not a great choice for multiclass settings
    in general. In the next chapter, we'll introduce neural networks, which are a
    nonlinear model used to solve both regression and classification problems. We'll
    also see how neural networks are able to handle multiple class labels in a natural
    way.
  prefs: []
  type: TYPE_NORMAL
