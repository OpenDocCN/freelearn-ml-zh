- en: Chapter 4. Generalized Linear Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章. 广义线性模型
- en: For regression tasks where the goal is to predict a numerical output, such as
    a price or temperature, we've seen that linear regression can potentially be a
    good starting point. It is simple to train and easy to interpret even though,
    as a model, it makes strict assumptions about the data and the underlying target
    function. Before studying more advanced techniques to tackle regression problems,
    we'll introduce **logistic regression**. Despite its somewhat misleading name,
    this is actually our first model for performing classification. As we learned
    in [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    in classification problems, our output is qualitative and thus comprises a finite
    set of values, which we call classes. We'll begin by thinking about the binary
    classification scenario, where we are trying to distinguish between two classes,
    which we'll arbitrarily label as 0 and 1, and later on we'll extend this to distinguishing
    between multiple classes. Finally, we'll finish up by touching on additional regression
    methods, Poisson regression and Negative Binomial regression.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测数值输出的回归任务，例如价格或温度，我们已经看到线性回归可能是一个好的起点。它易于训练且易于解释，尽管作为一个模型，它对数据和潜在的目标函数做出了严格的假设。在学习更高级的回归问题解决技术之前，我们将介绍**逻辑回归**。尽管其名称有些误导，但实际上这是我们第一个用于执行分类的模型。正如我们在[第一章](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "第一章. 准备预测建模")中学习的，“准备预测建模”，在分类问题中，我们的输出是定性的，因此由有限值的集合组成，我们称之为类别。我们将从考虑二元分类场景开始，我们试图区分两个类别，我们将任意地将它们标记为
    0 和 1，稍后我们将扩展到区分多个类别。最后，我们将简要介绍其他回归方法，泊松回归和负二项回归。
- en: Classifying with linear regression
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线性回归进行分类
- en: Even though we know classification problems involve qualitative outputs, it
    seems natural to ask whether we could use our existing knowledge of linear regression
    and apply it to the classification setting. We could do this by training a linear
    regression model to predict a value in the interval [0, 1], remembering that we've
    chosen to label our two classes as 0 and 1\. Then, we could apply a threshold
    to the output of our model in such a way that, if the model outputs a value below
    0.5, we would predict class 0; otherwise, we would predict class 1\.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们知道分类问题涉及定性输出，但似乎很自然地会问我们是否可以使用我们现有的线性回归知识并将其应用于分类场景。我们可以通过训练一个线性回归模型来预测区间
    [0, 1] 内的值来实现这一点，记住我们已经选择将两个类别标记为 0 和 1。然后，我们可以将阈值应用于我们模型的输出，这样，如果模型输出的值低于 0.5，我们就会预测类别
    0；否则，我们就会预测类别 1。
- en: The following graph demonstrates this concept for a simple linear regression
    with a single input feature X1 and for a binary classification problem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了对于具有单个输入特征 X1 的简单线性回归和二元分类问题，这一概念。
- en: '![Classifying with linear regression](img/00063.jpeg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![使用线性回归进行分类](img/00063.jpeg)'
- en: Our output variable *y* is either 0 or 1, so all the data lies on two horizontal
    lines. The solid line shows the output of the model, and the dashed line shows
    the decision boundary, which arises when we put a threshold on the model's predicted
    output at the value 0.5\. Points to the left of the dashed line are predicted
    as belonging to class 0, and points to the right are predicted as belonging to
    class 1.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标变量 *y* 要么是 0，要么是 1，因此所有数据都位于两条水平线上。实线表示模型的输出，虚线表示决策边界，它出现在我们将模型预测输出的阈值设置为
    0.5 时。虚线左侧的点被预测为属于类别 0，而右侧的点被预测为属于类别 1。
- en: The model is clearly not perfect, but it does seem to correctly classify a large
    proportion of the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型显然并不完美，但它似乎确实正确分类了大部分数据。
- en: While a good approximation in this case, this approach doesn't feel right for
    a number of reasons. Firstly, although we know before hand that our output variable
    is limited to the interval [0, 1] because we have just two classes, the raw output
    from the linear regression predicts values outside this range. We can see this
    from the graph for values of input feature *X1* that are either very low or very
    high. Secondly, linear regression is designed to solve the problem of minimizing
    the MSE, which does not seem appropriate for us in this case. Our goal is really
    to find a way to separate the two classes, not to minimize the mean squared error
    against a line of best fit. As a consequence of this fact, the location of the
    decision boundary is very sensitive to the presence of high leverage points. As
    we discussed in [Chapter 2](part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 2. Tidying Data and Measuring Performance"), *Linear Regression*, high
    leverage points are points that lie far away from most of the data because they
    have extreme values for at least one of their input features.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这种情况下这是一个很好的近似，但这种方法在许多方面感觉并不正确。首先，尽管我们事先知道我们的输出变量被限制在区间[0, 1]内，因为我们只有两个类别，但线性回归的原始输出预测的值超出了这个范围。我们可以从输入特征*X1*的值非常低或非常高的图中看到这一点。其次，线性回归旨在解决最小化均方误差的问题，这似乎不适合我们这种情况。我们的目标实际上是找到一种方法来分离两个类别，而不是最小化与最佳拟合线的均方误差。因此，决策边界的位置对高杠杆点的存在非常敏感。正如我们在[第2章](part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7
    "第2章。整理数据和衡量性能")“线性回归”中讨论的那样，高杠杆点是那些由于至少一个输入特征的极端值而远离大部分数据的点。
- en: 'The following plot demonstrates the effect of high leverage points on our classifier:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图展示了高杠杆点对我们分类器的影响：
- en: '![Classifying with linear regression](img/00064.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![使用线性回归进行分类](img/00064.jpeg)'
- en: Here, the data is exactly the same as before, except that we have added two
    new observations for class 1 that have relatively high values for feature *X1*
    and thus appear far to the right of the graph. Now ideally, because these two
    newly added observations are well into the area of the graph where we predict
    class 1, they should not impact our decision boundary so heavily. Due to the fact
    that we are minimizing the MSE, the old linear regression line (shown as a solid
    line) has now shifted to the right (shown as a dashed line). Consequently, the
    point at which our new linear regression line crosses 0.5 on the *y* axis has
    moved to the right. Thus, our decision boundary has noticeably moved to the right
    as a result of adding only two new points.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，数据与之前完全相同，只是我们为类别1添加了两个新的观测值，这两个观测值在特征*X1*上的值相对较高，因此出现在图表的右侧。现在理想情况下，因为这两个新添加的观测值已经处于我们预测类别1的图表区域，它们不应该对我们的决策边界产生太大的影响。由于我们正在最小化均方误差，旧的线性回归线（以实线表示）现在已经向右移动（以虚线表示）。因此，我们的新线性回归线在*y*轴上与0.5相交的点已经向右移动。因此，仅添加两个新点就明显将我们的决策边界向右移动。
- en: Logistic regression addresses all of these points by providing an output that
    is bounded by the interval [0,1] and is trained using an entirely different optimization
    criterion from linear regression, so we are no longer fitting a function by minimizing
    the MSE, as we'll now see.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归通过提供一个输出值在区间[0,1]内且使用与线性回归完全不同的优化标准进行训练，解决了所有这些问题，因此我们不再通过最小化均方误差来拟合函数，正如我们现在将看到的。
- en: Introduction to logistic regression
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归简介
- en: 'In logistic regression, input features are linearly scaled just as with linear
    regression; however, the result is then fed as an input to the **logistic function**.
    This function provides a nonlinear transformation on its input and ensures that
    the range of the output, which is interpreted as the probability of the input
    belonging to class 1, lies in the interval [0,1]. The form of the logistic function
    is as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，输入特征与线性回归一样进行线性缩放；然而，结果随后被作为输入传递给**逻辑函数**。这个函数对其输入进行非线性转换，并确保输出值的范围，即解释为输入属于类别1的概率，位于区间[0,1]内。逻辑函数的形式如下：
- en: '![Introduction to logistic regression](img/00065.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归简介](img/00065.jpeg)'
- en: 'Here is a plot of the logistic function:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是逻辑函数的图示：
- en: '![Introduction to logistic regression](img/00066.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归简介](img/00066.jpeg)'
- en: When *x = 0*, the logistic function takes the value 0.5\. As *x* tends to *+∞*,
    the exponential in the denominator vanishes and the function approaches the value
    1\. As *x* tends to *-∞*, the exponential, and hence the denominator, tends to
    move towards infinity and the function approaches the value 0\. Thus, our output
    is guaranteed to be in the interval [0,1], which is necessary for it to be a probability.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *x = 0* 时，逻辑函数取值为 0.5。当 *x* 趋向于 *+∞* 时，分母中的指数消失，函数趋近于值 1。当 *x* 趋向于 *-∞* 时，指数以及分母趋向于无限大，函数趋近于值
    0。因此，我们的输出保证在区间 [0,1] 内，这对于它作为一个概率是必要的。
- en: Generalized linear models
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义线性模型
- en: 'Logistic regression belongs to a class of models known as **generalized linear
    models** (**GLMs**). Generalized linear models have three unifying characteristics.
    The first of these is that they all involve a linear combination of the input
    features, thus explaining part of their name. The second characteristic is that
    the output is considered to have an underlying probability distribution belonging
    to the family of exponential distributions. These include the normal distribution,
    the Poisson, and the binomial distribution. Finally, the mean of the output distribution
    is related to the linear combination of input features by way of a function, known
    as the **link function**. Let''s see how this all ties in with logistic regression,
    which is just one of many examples of a GLM. We know that we begin with a linear
    combination of input features, so for example, in the case of one input feature,
    we can build up an *x* term as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归属于一类称为**广义线性模型**（**GLMs**）的模型。广义线性模型有三个统一的特点。第一个特点是它们都涉及输入特征的线性组合，从而解释了它们名称的一部分。第二个特点是输出被认为具有属于指数分布族的潜在概率分布。这些包括正态分布、泊松分布和二项分布。最后，输出分布的均值通过一个称为**连接函数**的函数与输入特征的线性组合相关联。让我们看看这一切如何与逻辑回归联系起来，逻辑回归只是许多广义线性模型（GLM）的例子之一。我们知道我们从一个输入特征的线性组合开始，例如，在只有一个输入特征的情况下，我们可以构建一个如下所示的
    *x* 项：
- en: '![Generalized linear models](img/00067.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型](img/00067.jpeg)'
- en: Note
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that, in the case of logistic regression, we are modeling the probability
    that the output belongs to class 1, rather than modeling the output directly as
    we were in linear regression. As a result, we do not need to model the error term
    because our output, which is a probability, directly incorporates the inherent
    randomness of our model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在逻辑回归的情况下，我们是在模拟输出属于类别 1 的概率，而不是像线性回归那样直接模拟输出。因此，我们不需要模拟误差项，因为我们的输出，即概率，直接包含了模型固有的随机性。
- en: 'Next, we apply the logistic function to this term in order to produce our model''s
    output:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将逻辑函数应用于这个项，以产生我们模型的输出：
- en: '![Generalized linear models](img/00068.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型](img/00068.jpeg)'
- en: Here, the left term tells us directly that we are computing the probability
    that our output belongs to class 1, based on our evidence of seeing the value
    of the input feature *X1*. For logistic regression, the underlying probability
    distribution of the output is the Bernoulli distribution. This is the same as
    the binomial distribution with a single trial, and is the distribution we would
    obtain in an experiment with only two possible outcomes having constant probability,
    such as a coin flip.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，左边的项直接告诉我们，我们正在根据我们看到的输入特征 *X1* 的值来计算输出属于类别 1 的概率。对于逻辑回归，输出的潜在概率分布是伯努利分布。这与单次试验的二项分布相同，是在只有两种可能结果且概率恒定的实验中获得的分布，例如抛硬币。
- en: The mean of the Bernoulli distribution, *μy*, is the probability of the (arbitrarily
    chosen) outcome for success, in this case, class 1\. Consequently, the left-hand
    side in the previous equation is also the mean of our underlying output distribution.
    For this reason, the function that transforms our linear combination of input
    features is sometimes known as the **mean function**, and we just saw that this
    function is the logistic function for logistic regression.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 伯努利分布的均值 *μy* 是（任意选择的）成功事件的概率，在本例中，为类别 1。因此，前一个方程的左侧也是我们潜在输出分布的均值。因此，将输入特征线性组合转换的函数有时被称为**均值函数**，我们刚刚看到这个函数是逻辑回归中的逻辑函数。
- en: Now, to determine the link function for logistic regression, we can perform
    some simple algebraic manipulations in order to isolate our linear combination
    of input features.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了确定逻辑回归的连接函数，我们可以进行一些简单的代数运算，以便隔离我们的输入特征线性组合。
- en: '![Generalized linear models](img/00069.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![广义线性模型](img/00069.jpeg)'
- en: 'The term on the left-hand side is known as the **log-odds** or **logit function**
    and is the link function for logistic regression. The denominator of the fraction
    inside the logarithm is the probability of the output being class 0 given the
    data. Consequently, this fraction represents the ratio of probability between
    class 1 and class 0, which is also known as the **odds ratio**:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的项被称为**对数几率**或**logit函数**，是逻辑回归的连接函数。对数中的分母是在给定数据的情况下输出为类别0的概率。因此，这个分数代表了类别1和类别0之间概率的比率，也称为**优势比**：
- en: Tip
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: A good reference for logistic regression along with examples of other GLMs such
    as Poisson regression is *Extending the Linear Model with R*, *Julian J. Faraway*,
    *CRC Press*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的良好参考书籍，以及泊松回归等其他广义线性模型（GLM）的示例，是《使用R扩展线性模型》，作者为*Julian J. Faraway*，出版社为*CRC
    Press*。
- en: Interpreting coefficients in logistic regression
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归系数的解释
- en: Looking at the right-hand side of the last equation, we can see that we have
    almost exactly the same form that we had for simple linear regression, barring
    the error term. The fact that we have the logit function on the left-hand side,
    however, means we cannot interpret our regression coefficients in the same way
    that we did with linear regression. In logistic regression, a unit increase in
    feature *Xi* results in multiplying the odds ratio by an amount,![Interpreting
    coefficients in logistic regression](img/00070.jpeg). When a coefficient *βi*
    is positive, then we multiply the odds ratio by a number greater than 1, so we
    know that increasing the feature *Xi* will effectively increase the probability
    of the output being labeled as class 1\.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 观察最后一个方程的右侧，我们可以看到，我们几乎有与简单线性回归完全相同的结构，只是没有误差项。然而，左侧有logit函数的事实意味着我们不能像线性回归那样解释我们的回归系数。在逻辑回归中，特征*Xi*的单位增加会导致优势比乘以一个量，![逻辑回归系数的解释](img/00070.jpeg)。当一个系数*βi*为正时，那么我们将优势比乘以一个大于1的数，因此我们知道增加特征*Xi*将有效地增加输出被标记为类别1的概率。
- en: Similarly, increasing a feature with a negative coefficient shifts the balance
    toward predicting class 0\. Finally, note that when we change the value of an
    input feature, the effect is a multiplication on the odds ratio and not on the
    model output itself, which we saw is the probability of predicting class 1\. In
    absolute terms, the change in the output of our model as a result of a change
    in the input is not constant throughout, but depends on the current value of our
    input features. This is, again, different from linear regression, where irrespective
    of the values of the input features, the regression coefficients always represent
    a fixed increase in the output per unit increase of an input feature.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，增加一个具有负系数的特征会将平衡偏向预测类别0。最后，请注意，当我们改变输入特征的值时，这种影响是对优势比进行乘法运算，而不是对模型输出本身进行运算，正如我们所看到的，这是预测类别1的概率。从绝对值的角度来看，我们模型输出的变化（由于输入的变化引起）并不是恒定的，而是取决于我们输入特征当前的值。这与线性回归不同，在线性回归中，无论输入特征的值如何，回归系数始终代表输入特征单位增加时输出增加的固定量。
- en: Assumptions of logistic regression
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归的假设
- en: Logistic regression makes fewer assumptions about the input than linear regression.
    In particular, the nonlinear transformation of the logistic function means that
    we can model more complex input-output relationships. We still have a linearity
    assumption, but in this case, it is between the features and the log-odds. We
    no longer require a normality assumption for residuals and nor do we need the
    homoscedastic assumption. On the other hand, our error terms still need to be
    independent. Strictly speaking, the features themselves no longer need to be independent
    but, in practice, our model will still face issues if the features exhibit a high
    degree of multicollinearity. Finally, we'll note that, just as with unregularized
    linear regression, feature scaling does not affect the logistic regression model.
    This means that centering and scaling a particular input feature will simply result
    in an adjusted coefficient in the output model, without any repercussions on the
    model performance. It turns out that, for logistic regression, this is the result
    of a property known as the **invariance property of maximum likelihood**. Maximum
    likelihood is the method used to select the coefficients and will be the focus
    of the next section. It should be noted, however, that centering and scaling features
    might still be a good idea if they are on very different scales. This is done
    to assist the optimization procedure during training. In short, we should turn
    to feature scaling only if we run into model convergence issues.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归对输入的假设比线性回归要少。特别是，逻辑函数的非线性变换意味着我们可以模拟更复杂的输入输出关系。我们仍然有一个线性假设，但在这个情况下，它是特征和对数几率之间的。我们不再需要残差的正态性假设，也不需要同方差性假设。另一方面，我们的误差项仍然需要是独立的。严格来说，特征本身不再需要是独立的，但在实践中，如果特征表现出高度的多重共线性，我们的模型仍然会面临问题。最后，我们注意到，就像未正则化的线性回归一样，特征缩放不会影响逻辑回归模型。这意味着对特定输入特征进行中心化和缩放将简单地导致输出模型中的调整系数，而不会对模型性能产生任何影响。实际上，对于逻辑回归来说，这是由于一个称为**最大似然不变性**的性质所导致的。最大似然是选择系数的方法，将在下一节中讨论。然而，需要注意的是，如果特征处于非常不同的尺度上，对特征进行中心化和缩放可能仍然是一个好主意。这是在训练过程中帮助优化过程。简而言之，我们只有在遇到模型收敛问题时才应该转向特征缩放。
- en: Maximum likelihood estimation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大似然估计
- en: When we studied linear regression, we found our coefficients by minimizing the
    sum of squared error terms. For logistic regression, we do this by maximizing
    the **likelihood** of the data. The likelihood of an observation is the probability
    of seeing that observation under a particular model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们学习线性回归时，我们通过最小化平方误差项的总和来找到我们的系数。对于逻辑回归，我们通过最大化数据的**似然性**来实现这一点。一个观察值的似然性是在特定模型下看到该观察值的概率。
- en: 'In our case, the likelihood of seeing an observation *X* for class 1 is simply
    given by the probability *P(Y=1|X)*, the form of which was given earlier in this
    chapter. As we only have two classes, the likelihood of seeing an observation
    for class 0 is given by *1 - P(Y=1|X)*. The overall likelihood of seeing our entire
    dataset of observations is the product of all the individual likelihoods for each
    data point as we consider our observations to be independently obtained. As the
    likelihood of each observation is parameterized by the regression coefficients
    *βi*, the likelihood function for our entire dataset is also, therefore, parameterized
    by these coefficients. We can express our likelihood function as an equation,
    as shown in the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，看到类别1的观察值*X*的似然性简单地由概率*P(Y=1|X)*给出，其形式在本章前面已经给出。因为我们只有两个类别，所以看到类别0的观察值的似然性由*1
    - P(Y=1|X)*给出。看到我们整个观察值数据集的总体似然性是所有单个数据点的似然性的乘积，因为我们认为我们的观察值是独立获得的。由于每个观察值的似然性由回归系数*βi*参数化，因此我们整个数据集的似然函数也由这些系数参数化。我们可以将我们的似然函数表示为一个方程，如下所示：
- en: '![Maximum likelihood estimation](img/00071.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![最大似然估计](img/00071.jpeg)'
- en: 'Now, this equation simply computes the probability that a logistic regression
    model with a particular set of regression coefficients could have generated our
    training data. The idea is to choose our regression coefficients so that this
    likelihood function is maximized. We can see that the form of the likelihood function
    is a product of two large products from the two big *π* symbols. The first product
    contains the likelihood of all our observations for class 1, and the second product
    contains the likelihood of all our observations for class 0\. We often refer to
    the **log likelihood** of the data, which is computed by taking the logarithm
    of the likelihood function. Using the fact that the logarithm of a product of
    terms is the sum of the logarithm of each term we can write:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个方程简单地计算了一个具有特定回归系数的逻辑回归模型可能生成我们的训练数据的概率。我们的想法是选择我们的回归系数，使得这个似然函数最大化。我们可以看到，似然函数的形式是两个大乘积的乘积，来自两个大的*π*符号。第一个乘积包含我们所有类别1观测值的似然，第二个乘积包含我们所有类别0观测值的似然。我们通常指的是数据的**对数似然**，它是通过对似然函数取对数来计算的。利用乘积的每个项的对数之和等于对数乘积的事实，我们可以写出：
- en: '![Maximum likelihood estimation](img/00072.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![最大似然估计](img/00072.jpeg)'
- en: 'We can simplify this even further using a classic trick to form just a single
    sum:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个经典的技巧进一步简化，形成一个单独的求和：
- en: '![Maximum likelihood estimation](img/00073.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![最大似然估计](img/00073.jpeg)'
- en: To see why this is true, note that for the observations where the actual value
    of the output variable *y* is 1, the right term inside the summation is zero,
    so we are effectively left with the first sum from the previous equation. Similarly,
    when the actual value of *y* is 0, then we are left with the second summation
    from the previous equation. Understanding the form of the log likelihood is important,
    and we'll get some practice with this when we start working with R to train a
    logistic regression model in the next section. Note that maximizing the likelihood
    is equivalent to maximizing the log likelihood; both approaches will yield the
    same parameters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这是为什么，请注意，对于实际输出变量*y*的值为1的观测值，求和中的右侧项为零，所以我们实际上只剩下前一个方程中的第一个求和。同样，当*y*的实际值为0时，我们剩下前一个方程中的第二个求和。理解对数似然的形式很重要，当我们开始使用R训练逻辑回归模型时，我们将对此进行一些练习。请注意，最大化似然等价于最大化对数似然；两种方法都将产生相同的参数。
- en: Maximum likelihood estimation is a fundamental technique of parameter fitting,
    and we will encounter it in other models in this book. Despite its popularity,
    it should be noted that maximum likelihood is not a panacea. Alternative training
    criteria on which to build a model do exist, and there are some well-known scenarios
    under which this approach does not lead to a good model, as we shall see in subsequent
    chapters. Finally, note that the details of the actual optimization procedure,
    which finds the values of the regression coefficients for maximum likelihood,
    are beyond the scope of this book and in general, we can rely on R to implement
    this for us.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计是参数拟合的基本技术，我们将在本书的其他模型中遇到它。尽管它很受欢迎，但应该注意的是，最大似然并不是万能的。确实存在可以构建模型的替代训练标准，并且有一些众所周知的情况，这种方法不会导致一个好的模型，正如我们在后续章节中将要看到的。最后，请注意，实际优化过程的细节，即找到回归系数的最大似然值，超出了本书的范围，通常我们可以依赖R为我们实现这一点。
- en: Predicting heart disease
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测心脏病
- en: 'We''ll put logistic regression for the binary classification task to the test
    with a real-world dataset from the UCI Machine Learning Repository. This time,
    we will be working with the *Statlog (Heart) dataset*, which we will refer to
    as the *heart dataset* henceforth for brevity. The dataset can be downloaded from
    the UCI Machine Repository''s website at [http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29](http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29).
    The data contains 270 observations for patients with potential heart problems.
    Of these, 120 patients were shown to have heart problems, so the split between
    the two classes is fairly even. The task is to predict whether a patient has a
    heart disease based on their profile and a series of medical tests. First, we''ll
    load the data into a data frame and rename the columns according to the website:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用UCI机器学习仓库的真实世界数据集来测试二分类任务的逻辑回归。这次，我们将使用*Statlog (Heart)数据集*，为了简便起见，我们将称之为*心脏数据集*。该数据集可以从UCI机器学习仓库的网站[http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29](http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29)下载。数据包含270个潜在心脏问题的患者观察结果。其中，120名患者被证实有心脏病，因此两个类别的分割相当均匀。任务是预测患者是否有心脏病，基于他们的个人资料和一系列医疗测试。首先，我们将数据加载到数据框中，并根据网站重命名列：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following table contains the definitions of our input features and the
    output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格包含了我们输入特征的定义和输出：
- en: '| Column name | Type | Definition |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 类型 | 定义 |'
- en: '| --- | --- | --- |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `AGE` | Numerical | Age (years) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `AGE` | 数值 | 年龄（年） |'
- en: '| `SEX` | Binary | Gender |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `SEX` | 二元 | 性别 |'
- en: '| `CHESTPAIN` | Categorical | 4-valued chest pain type |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `CHESTPAIN` | 分类 | 4种值的胸痛类型 |'
- en: '| `RESTBP` | Numerical | Resting blood pressure (beats per minute) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `RESTBP` | 数值 | 休息血压（每分钟跳动次数） |'
- en: '| `CHOL` | Numerical | Serum cholesterol (mg/dl) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `CHOL` | 数值 | 血清胆固醇（mg/dl） |'
- en: '| `SUGAR` | Binary | Is the fasting blood sugar level > 120 mg/dl? |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `SUGAR` | 二元 | 空腹血糖水平是否大于120 mg/dl？ |'
- en: '| `ECG` | Categorical | 3-valued resting electrocardiographic results |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `ECG` | 分类 | 3种值的静息心电图结果 |'
- en: '| `MAXHR` | Numerical | Maximum heart rate achieved (beats per minute) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `MAXHR` | 数值 | 达到的最大心率（每分钟跳动次数） |'
- en: '| `ANGINA` | Binary | Was angina induced by exercise? |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `ANGINA` | 二元 | 是否由运动引起心绞痛？ |'
- en: '| `DEP` | Numerical | ST depression induced by exercise relative to rest |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `DEP` | 数值 | 相对于休息时由运动引起的ST段压低 |'
- en: '| `EXERCISE` | Ordered categorical | Slope of the peak exercise ST segment
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `EXERCISE` | 有序分类 | 运动峰值ST段斜率 |'
- en: '| `FLUOR` | Numerical | The number of major vessels colored by fluoroscopy
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `FLUOR` | 数值 | 通过荧光透视术着色的主要血管数量 |'
- en: '| `THAL` | Categorical | 3-valued Thal |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `THAL` | 分类 | 3种值的Thal |'
- en: '| `OUTPUT` | Binary | Presence or absence of a heart disease |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `OUTPUT` | 二元 | 是否存在心脏病 |'
- en: 'Before we train a logistic regression model for these data, there are a couple
    of preprocessing steps that we should perform. A common pitfall when working with
    numerical data is the failure to notice when a feature is actually a categorical
    variable and not a numerical variable when the levels are coded as numbers. In
    the heart dataset, we have four such features. The `CHESTPAIN`, `THAL`, and `ECG`
    features are all categorical features. The `EXERCISE` variable, although an ordered
    categorical variable, is nonetheless a categorical variable, so it will have to
    be coded as a factor as well:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们为这些数据训练逻辑回归模型之前，有一些预处理步骤我们应该执行。当处理数值数据时，一个常见的陷阱是没有注意到当一个特征实际上是分类变量而不是数值变量时，当级别被编码为数字时。在心脏数据集中，我们有四个这样的特征。`CHESTPAIN`、`THAL`和`ECG`特征都是分类特征。`EXERCISE`变量，尽管是有序分类变量，但仍然是一个分类变量，因此它也必须被编码为因子：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    we saw how we can transform categorical features with many levels into a series
    of binary valued indicator variables. By doing this, we can use them in a model
    such as linear or logistic regression, which requires all the inputs to be numerical.
    As long as the relevant categorical variables in a data frame have been coded
    as factors, R will automatically apply a coding scheme when performing logistic
    regression. Concretely, R will treat one of the *k* factor levels as a reference
    level and create *k-1* binary features from the other factor levels. We'll see
    visual evidence of this when we study the summary output of the logistic regression
    model that we'll train.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一章](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7 "第一章.
    准备预测建模")《准备预测建模》中，我们看到了如何将具有多个级别的分类特征转换为一组二元值指示变量。通过这样做，我们可以使用它们在模型中，如线性或逻辑回归，该模型要求所有输入都是数值。只要数据框中的相关分类变量已被编码为因子，R
    在执行逻辑回归时会自动应用编码方案。具体来说，R 会将其中一个 *k* 个因子级别作为参考级别，并从其他因子级别创建 *k-1* 个二元特征。当我们研究我们将要训练的逻辑回归模型的摘要输出时，我们将看到这一点的视觉证据。
- en: 'Next, we should observe that the `OUTPUT` variable is coded so that class 1
    corresponds to the absence of heart disease and class 2 corresponds to the presence
    of heart disease. As a final change, we''ll want to recode the `OUTPUT` variable
    so that we will have the familiar class labels of 0 and 1, respectively. This
    is done by simply subtracting `1`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该注意到`OUTPUT`变量被编码，使得类别1对应于没有心脏病，类别2对应于存在心脏病。作为最后的改变，我们希望重新编码`OUTPUT`变量，以便我们将有熟悉的类别标签0和1，分别。这可以通过简单地减去`1`来完成：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our data frame is now ready. Before we train our model, however, we will split
    our data frame into two parts, for training and testing, exactly as we did for
    linear regression. Once again, we''ll use an 85-15 split:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据框现在已准备就绪。然而，在我们训练模型之前，我们将数据框分为两部分，用于训练和测试，正如我们在线性回归中所做的那样。再次，我们将使用85-15的分割：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We now have 230 observations in our training set and 40 observations in our
    test set. To train a logistic regression model in R, we use the `glm()` function,
    which stands for generalized linear model. This function can be used to train
    various generalized linear models, but we''ll focus on the syntax and usage for
    logistic regression here. The call is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在训练集中有230个观测值，在测试集中有40个观测值。要在R中训练逻辑回归模型，我们使用`glm()`函数，代表广义线性模型。此函数可用于训练各种广义线性模型，但在这里我们将关注逻辑回归的语法和用法。调用如下：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note that the format is very similar to what we saw with linear regression.
    The first parameter is the model formula, which identifies the output variable
    and which features we want to use (in this case, all of them). The second parameter
    is the data frame and the final `family` parameter is used to specify that we
    want to perform logistic regression. We can use the `summary()` function to find
    out more about the model we just trained, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，格式与我们之前看到的线性回归非常相似。第一个参数是模型公式，它标识输出变量以及我们想要使用的特征（在这种情况下，所有特征）。第二个参数是数据框，最后的`family`参数用于指定我们想要执行逻辑回归。我们可以使用`summary()`函数来了解更多关于我们刚刚训练的模型的信息，如下所示：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Assessing logistic regression models
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估逻辑回归模型
- en: The summary of the logistic regression model produced with the `glm()` function
    has a similar format to that of the linear regression model produced with the
    `lm()` function. This shows us that, for our categorical variables, we have one
    fewer binary features than the number of levels in the original variable, so for
    example, the three-valued `THAL` input feature produced two binary variables labeled
    `THAL6` and `THAL7`. We'll begin by looking first at the regression coefficients
    that are predicted with our model. These are presented with their corresponding
    **z-statistic**. This is analogous to the t-statistic that we saw in linear regression,
    and again, the higher the absolute value of the z-statistic, the more likely it
    is that this particular feature is significantly related to our output variable.
    The p-values next to the z-statistic express this notion as a probability and
    are annotated with stars and dots, as they were in linear regression, indicating
    the smallest confidence interval that includes the corresponding p-value.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`glm()`函数生成的逻辑回归模型摘要的格式与使用`lm()`函数生成的线性回归模型摘要的格式相似。这表明，对于我们的分类变量，我们比原始变量的级别数少一个二进制特征，例如，三值`THAL`输入特征生成了两个标记为`THAL6`和`THAL7`的二进制变量。我们将首先查看模型预测的回归系数。这些系数与它们的对应**z统计量**一起呈现。这与我们在线性回归中看到的t统计量类似，再次强调，z统计量的绝对值越高，这个特定特征与我们的输出变量显著相关的可能性就越大。z统计量旁边的p值以概率的形式表达这一概念，并用星号和点标注，就像在线性回归中一样，表示包含相应p值的最小置信区间。
- en: 'Due to the fact that logistic regression models are trained with the maximum
    likelihood criterion, we use the standard normal distribution to perform significance
    tests on our coefficients. For example, to reproduce the p-value for the `THAL7`
    feature that corresponds to the listed z-value of 3.362, we can write the following
    (set the `lower.tail` parameter to `T` when testing negative coefficients):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于逻辑回归模型是用最大似然准则训练的，我们使用标准正态分布对我们系数进行显著性测试。例如，为了重现对应于列出的z值为3.362的`THAL7`特征的p值（当测试负系数时，将`lower.tail`参数设置为`T`）：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: An excellent reference for learning about the essential concepts of distributions
    in statistics is *All of Statistics*, *Larry Wasserman*, *Springer*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 学习统计学中分布的基本概念的绝佳参考书籍是《All of Statistics》，作者Larry Wasserman，Springer出版社。
- en: 'From the model summary, we see that `FLUOR`, `CHESTPAIN4`, and `THAL7` are
    the strongest feature predictors for heart diseases. A number of input features
    have relatively high p-values. This indicates that they are probably not good
    indicators of heart disease in the presence of the other features. We''ll stress
    once again the importance of interpreting this table correctly. The table does
    not say that heart age, for example, is not a good indicator for heart disease;
    rather, it says that, in the presence of the other input features, age does not
    really add much to the model. Furthermore, note that we almost definitely have
    some degree of collinearity in our features as the regression coefficient of age
    is negative, whereas we would expect that the likelihood of heart disease increases
    with age. Of course, this assumption is valid only in the absence of all other
    input features. Indeed, if we retrain a logistic regression model with only the
    `AGE` variable, we get a positive regression coefficient as well as a low p-value,
    both of which support our belief that the features are collinear:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型摘要中，我们看到`FLUOR`、`CHESTPAIN4`和`THAL7`是心脏病最强的特征预测因子。许多输入特征具有相对较高的p值。这表明，在其他特征存在的情况下，它们可能不是心脏病的好指标。我们再次强调正确解释这个表的重要性。该表并没有说心脏年龄，例如，不是心脏病的好指标；相反，它表示，在其他输入特征存在的情况下，年龄实际上并没有给模型增加多少。此外，请注意，我们几乎肯定在我们的特征中存在一定程度的多重共线性，因为年龄的回归系数是负的，而我们会预期心脏病的发生概率随着年龄的增长而增加。当然，这个假设只在所有其他输入特征不存在的情况下才是有效的。事实上，如果我们只使用`AGE`变量重新训练逻辑回归模型，我们也会得到一个正的回归系数以及一个低的p值，这两个结果都支持我们的信念，即特征是共线的：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that the AIC value of this simpler model is higher than what we obtained
    with the full model, so we would expect this simple model to be worse.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个更简单模型的AIC值高于我们使用完整模型获得的结果，因此我们预计这个简单模型会更差。
- en: Model deviance
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型偏差
- en: To understand the remainder of the model summary, we need to introduce an important
    concept known as **deviance**. In linear regression, our residuals were defined
    simply as the difference between the predicted value and the actual value of the
    output that we are trying to predict. Logistic regression is trained using maximum
    likelihood, so it is natural to expect that an analogous concept to the residual
    would involve the likelihood. There are several closely-related definitions of
    the concept of deviance. Here, we will use the definitions that the `glm()` function
    uses in order to explain the model's output. The deviance of an observation can
    be computed as the -2 times the log likelihood of that observation. The deviance
    of a dataset is just the sum of all the observation deviances.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解模型总结的其余部分，我们需要引入一个称为**偏差**的重要概念。在线性回归中，我们的残差被定义为预测值与实际输出值之间的差异，这是我们试图预测的输出。逻辑回归使用最大似然进行训练，因此可以自然地预期，与残差类似的概念将涉及似然。偏差的概念有几个紧密相关的定义。在这里，我们将使用
    `glm()` 函数使用的定义来解释模型的输出。观察值的偏差可以计算为该观察值的 -2 倍对数似然。数据集的偏差仅仅是所有观察值偏差的总和。
- en: 'The **deviance residual** of an observation is derived from the deviance itself
    and is analogous to the residual of a linear regression. It can be computed as
    follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 观察值的**偏差残差**是从偏差本身派生出来的，类似于线性回归的残差。它可以按以下方式计算：
- en: '![Model deviance](img/00074.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![模型偏差](img/00074.jpeg)'
- en: For an observation *i*, *dr[i]* represents the deviance residual and *di* represents
    the deviance. Note that squaring a deviance residual effectively eliminates the
    sign function and produces just the deviance of the observation. Consequently,
    the sum of squared deviance residuals is the deviance of the dataset, which is
    just the log likelihood of the dataset scaled by the constant -2\. Consequently,
    maximizing the log likelihood of the data is the same as minimizing the sum of
    the squared deviance residuals, so our analogy with linear regression is complete.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于观察值 *i*，*dr[i]* 代表偏差残差，而 *di* 代表偏差。请注意，偏差残差的平方实际上消除了符号函数，只产生了观察值的偏差。因此，平方偏差残差的和就是数据集的偏差，这仅仅是数据集对数似然值的常数
    -2 倍。因此，最大化数据的对数似然与最小化平方偏差残差之和是相同的，所以我们的线性回归类比就完整了。
- en: 'In order to reproduce the results that are shown in the model summary, and
    to understand how deviance is computed, we''ll write some of our own functions
    in R. We''ll begin by computing the log likelihood for our dataset using the equation
    for the log likelihood that we saw earlier on in this chapter. From the equation,
    we''ll create two functions. The `log_likelihoods()` function computes a vector
    of log likelihoods for all the observations in a dataset, given the probabilities
    that the model predicts and the actual target labels, and `dataset_log_likelihood()`
    sums these up to produce the log likelihood of a dataset:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重现模型总结中显示的结果，并理解偏差是如何计算的，我们将使用 R 编写一些自己的函数。我们将从使用本章前面提到的对数似然方程计算数据集的对数似然值开始。从方程中，我们将创建两个函数。`log_likelihoods()`
    函数计算数据集中所有观察值的对数似然向量，给定模型预测的概率和实际的目标标签，而 `dataset_log_likelihood()` 函数将这些值加起来以产生数据集的对数似然：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we can use the definition of deviance to compute two analogous functions:
    `deviances()` and `dataset_deviance()`. The first of these computes a vector of
    observation deviances, and the second sums these up for the whole dataset:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用偏差的定义来计算两个类似函数：`deviances()` 和 `dataset_deviance()`。第一个函数计算观察值偏差的向量，第二个函数将这些值加起来以计算整个数据集的偏差：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Given these functions, we can now create a function that will compute the deviance
    of a model. To do this, we need to use the `predict()` function in order to compute
    the model''s probability predictions for the observations in the training data.
    This works just as with linear regression, except that by default it returns probabilities
    on the logit scale. To ensure that we get actual probabilities, we need to specify
    the value of `response` for the `type` parameter:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些函数，我们现在可以创建一个计算模型偏差的函数。为此，我们需要使用 `predict()` 函数来计算训练数据中观察值的模型概率预测。这与线性回归类似，但默认情况下它返回对数尺度上的概率。为了确保我们得到实际的概率，我们需要指定
    `type` 参数的 `response` 值：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To check whether our function is working, let''s compute the model deviance,
    also known as the residual deviance, for our heart model:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的函数是否正常工作，让我们计算我们的心脏模型中模型偏差，也称为残差偏差：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Reassuringly, this is the same value as that listed in our model summary. One
    way to evaluate a logistic regression model is to compute the difference between
    the model deviance and the deviance of the null model, which is the model trained
    without any features. The deviance of the null model is known as **null deviance**.
    The null model predicts class 1 via a constant probability, as it has no features.
    This probability is estimated via the proportion of the observations of class
    1 in the training data, which we can obtain by simply averaging the `OUTPUT` column:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 令人欣慰的是，这个值与我们在模型摘要中列出的相同。评估逻辑回归模型的一种方法是通过计算模型偏差与无特征模型偏差之间的差异，后者是在没有任何特征的情况下训练的模型。无特征模型的偏差被称为**零偏差**。由于没有特征，零模型通过一个恒定的概率预测类别1。这个概率是通过估计训练数据中类别1观察值的比例来估计的，我们可以通过简单地平均`OUTPUT`列来获得这个比例：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once again, we see that we have reproduced the value that R computes for us
    in the model summary. The residual deviance and null deviance are analogous to
    the **Residual Sum of Squares** (**RSS**) and the **True Sum of Squares** (**TSS**)
    that we saw in linear regression. If the difference between these two is high,
    the interpretation is similar to the notion of the residual sum of squares in
    linear regression *explaining away* the variance observed by the output variable.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们看到我们重现了R在模型摘要中为我们计算出的值。残差偏差和零偏差类似于我们在线性回归中看到的**残差平方和**（**RSS**）和**真实平方和**（**TSS**）。如果这两个值之间的差异很大，其解释与线性回归中残差平方和的概念类似，即通过输出变量的观察值来“解释”方差。
- en: 'Continuing with this analogy, we can define a **pseudo R2** value for our model
    using the same equation that we used to compute the R2 for linear regression,
    but substituting in the deviances. We implement this in R as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 继续这个类比，我们可以为我们的模型定义一个**伪R2**值，使用与计算线性回归R2相同的方程，但用偏差来替换。我们在R中这样实现：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our logistic regression model is said to explain roughly 56 % of the null deviance.
    This is not particularly high; most likely, we don't have a rich enough feature
    set to make accurate predictions with a logistic model. Unlike linear regression,
    it is possible for the pseudo R2 to exceed 1, but this only happens under problematic
    circumstances where the residual deviance exceeds the null deviance. If this happens,
    we should not trust the model and proceed with feature selection methods, or try
    out alternative models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的逻辑回归模型解释了大约56%的零偏差。这并不特别高；很可能是我们没有足够丰富的特征集来使用逻辑模型进行准确预测。与线性回归不同，伪R2可以超过1，但这只发生在残差偏差超过零偏差的困难情况下。如果发生这种情况，我们不应相信模型，并继续使用特征选择方法，或者尝试其他模型。
- en: Besides the pseudo R2, we may also want a statistical test to check whether
    the difference between the null deviance and the residual deviance is significant.
    The absence of a p-value next to the residual deviance in the model summary indicates
    that R has not created any test. It turns out that the difference between the
    residual and null deviances is approximately and asymptotically distributed with
    a *χ2* (pronounced *CHI squared*) distribution. We'll define a function to compute
    a p-value for this difference, but this is only an approximation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了伪R2之外，我们可能还想有一个统计检验来检查零偏差和残差偏差之间的差异是否显著。模型摘要中残差偏差旁边没有p值表示，R没有创建任何测试。实际上，残差偏差和零偏差之间的差异大约是渐近地以**χ2**（发音为*CHI
    squared*）分布分布的。我们将定义一个函数来计算这个差异的p值，但这只是一个近似。
- en: 'First, we need the difference between the null deviance and the residual deviance.
    We also need the degrees of freedom for this difference, which are computed simply
    by subtracting the number of degrees of freedom of our model from those of the
    null model. The null model only has an intercept, so the number of degrees of
    freedom is the total number of observations in our dataset minus 1\. For the residual
    deviance, we are computing a number of regression coefficients, including the
    intercept, so we need to subtract this number from the total number of observations.
    Finally, we use the `pchisq()` function to obtain a `p-value`, noting that we
    are creating an upper tail computation and hence need to set the `lower.tail`
    parameter to `FALSE`. The code is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要零偏差和残差偏差之间的差异。我们还需要这个差异的自由度，这可以通过简单地从我们的模型自由度中减去零模型自由度来计算。零模型只有一个截距，所以自由度是数据集中观察值的总数减
    1。对于残差偏差，我们正在计算包括截距在内的多个回归系数，因此我们需要从这个总数中减去这个数字。最后，我们使用 `pchisq()` 函数来获得 `p-value`，注意我们正在进行一个上尾计算，因此需要将
    `lower.tail` 参数设置为 `FALSE`。代码如下：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `p-value` that we obtain is tiny, so we feel certain that our model produces
    predictions that are better than average guessing. In our original model summary
    we also saw a summary of the deviance residuals. Using the definition of a deviance
    residual that we gave earlier, we''ll define a function to compute the vector
    of deviance residuals:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得的 `p-value` 非常小，所以我们确信我们的模型产生的预测比平均猜测更好。在我们的原始模型摘要中，我们还看到了偏差残差的摘要。使用我们之前给出的偏差残差定义，我们将定义一个函数来计算偏差残差的向量：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we can use the `summary()` function on the deviance residuals that
    we obtain with our `model_deviance_residuals()` function to obtain a table:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 `summary()` 函数对我们的 `model_deviance_residuals()` 函数获得的偏差残差进行总结，以获得一个表格：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once again, we can verify that we obtain the correct result. Our model summary
    provides us with one final diagnostic fisher scoring iterations, which we have
    not yet discussed. This number is typically in the range of 4 to 8 and is a convergence
    diagnostic. If the optimization procedure that R uses to train the logistic model
    has not converged, we expect to see a number that is high. If this happens, our
    model is suspect and we may not be able to use it to make predictions. In our
    case, we are within the expected range.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 再次验证，我们可以确认我们得到了正确的结果。我们的模型摘要还提供了一项最后的诊断：Fisher 分数迭代次数，我们尚未讨论。这个数字通常在 4 到 8
    之间，是一个收敛诊断。如果 R 用于训练逻辑模型的优化过程没有收敛，我们预计会看到一个较高的数字。如果发生这种情况，我们的模型可能是可疑的，我们可能无法用它来做出预测。在我们的情况下，我们处于预期的范围内。
- en: Test set performance
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试集性能
- en: 'We''ve seen how we can use the `predict()` function to compute the output of
    our model. This output is the probability of the input belonging to class 1\.
    We can perform binary classification by applying a threshold. We''ll do this with
    both our training and test data and compare them with our expected outputs to
    measure the classification accuracy:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用 `predict()` 函数来计算我们模型的输出。这个输出是输入属于类别 1 的概率。我们可以通过应用阈值来进行二元分类。我们将对训练数据和测试数据进行此操作，并将它们与我们的预期输出进行比较，以衡量分类准确率：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The classification accuracies on the training and test sets are very similar
    and are close to 90 %. This is a very good starting point for a modeler to work
    from. The coefficients table in our model showed us that several features did
    not seem to be significant, and we also saw a degree of collinearity that means
    we could now proceed with variable selection and possibly look for more features,
    either through computation or by obtaining additional data about our patients.
    The pseudo R2 computation showed us that we did not explain enough of the deviance
    in our model, which also supports this.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集上的分类准确率非常相似，接近 90 %。这对于模型构建者来说是一个非常好的起点。我们模型中的系数表显示，一些特征似乎并不显著，我们还发现了一定程度的共线性，这意味着我们现在可以继续进行变量选择，并可能通过计算或获取更多关于我们患者的数据来寻找更多特征。伪
    R2 的计算显示，我们没有充分解释模型中的偏差，这也支持了这一点。
- en: Regularization with the lasso
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lasso 正则化
- en: 'In the previous chapter on linear regression, we used the `glmnet` package
    to perform regularization with ridge regression and the lasso. As we''ve seen
    that, it might be a good idea to remove some of our features, we''ll try applying
    lasso to our dataset and assess the results. First, we''ll train a series of regularized
    models with `glmnet()` and then we will use `cv.glmnet()` to estimate a suitable
    value for *λ*. Then, we''ll examine the coefficients of our regularized model
    using this *λ*:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章关于线性回归的章节中，我们使用了`glmnet`包来进行岭回归和lasso的正则化。正如我们所见，移除一些特征可能是个不错的主意，因此我们将尝试将lasso应用于我们的数据集并评估结果。首先，我们将使用`glmnet()`训练一系列正则化模型，然后我们将使用`cv.glmnet()`来估计一个合适的*λ*值。然后，我们将使用这个*λ*来检查我们正则化模型的系数：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We see that a number of our features have effectively been removed from the
    model because their coefficients are zero. If we now use this model to measure
    the classification accuracy on our training and test sets, we observe that in
    both cases, we get slightly better performance. Even if this difference is small,
    remember that we have achieved this using three fewer features:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的一些特征已经有效地从模型中移除，因为它们的系数为零。如果我们现在使用这个模型来衡量训练集和测试集上的分类准确率，我们会观察到在两种情况下，我们得到了略微更好的性能。即使这个差异很小，记住我们是通过使用三个更少的特征来达到这个效果的：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Classification metrics
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类指标
- en: Although we looked at the test set accuracy for our model, we know from [Chapter
    1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7 "Chapter 1. Gearing
    Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*, that the binary
    confusion matrix can be used to compute a number of other useful performance metrics
    for our data, such as precision, recall, and the *F* measure.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们检查了模型的测试集准确率，但我们从[第1章](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "第1章. 准备预测建模")，“准备预测建模”中知道，二元混淆矩阵可以用来计算我们数据集的许多其他有用的性能指标，例如精确率、召回率和*F*度量。
- en: 'We''ll compute these for our training set now:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为我们的训练集计算这些值：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, we used the trick of bracketing our assignment statements to simultaneously
    assign the result of an expression to a variable and print out the value assigned.
    Now, recall is the ratio of correctly identified instances of class 1, divided
    by the total number of observations that actually belong to class 1\. In a medical
    context such as ours, this is also known as **sensitivity**, as it is an effective
    measure of a model''s ability to detect or be sensitive to a particular condition.
    Recall is also known as the true positive rate. There is an analogous measure
    known as **specificity**, which is the false negative rate. This involves the
    mirror computation of recall for class 0, that is, the correctly identified members
    of class 0 over all the observations of class 0 in our dataset. In our medical
    context, for example, the interpretation of specificity is that it measures the
    model''s ability to reject observations that do not have the condition represented
    by class 1 (in our case, heart disease). We can compute the specificity of our
    model as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了将赋值语句括起来的技巧，同时将表达式的结果赋给一个变量并打印出所赋的值。现在，召回率是正确识别的类别1实例与属于类别1的总观察值的比率。在我们这样的医疗背景下，这也被称为**灵敏度**，因为它是一个衡量模型检测或对特定条件敏感的有效指标。召回率也被称为真正率。有一个类似的度量称为**特异性**，它是假阴性率。这涉及到对类别0的召回率的镜像计算，即正确识别的类别0成员与我们的数据集中类别0的所有观察值的比率。在我们这样的医疗背景下，例如，特异性的解释是它衡量模型拒绝不具有类别1所代表条件（在我们的案例中，心脏病）的观察的能力。我们可以如下计算我们模型的特异性：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In computing these metrics, we begin to see the importance of setting the threshold
    at `0.5`. If we were to choose a different threshold, it should be clear that
    all of the preceding metrics would change. In particular, there are many circumstances,
    our current medical context being a prime example, in which we may want to adjust
    our threshold to be biased towards identifying members of class 1\. For example,
    suppose our model was being used by a clinician to determine whether to have a
    patient undergo a more detailed and expensive examination for heart disease. We
    would probably consider that mislabeling a patient with a heart condition as healthy
    is a more serious mistake to make than asking a healthy patient to undergo further
    tests because they were deemed unhealthy. To achieve this bias, we could lower
    our classification threshold to `0.3` or `0.2`, for example.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算这些度量时，我们开始看到将阈值设置为`0.5`的重要性。如果我们选择不同的阈值，很明显，所有先前的度量都会改变。特别是，有许多情况，我们当前的医疗环境就是一个很好的例子，我们可能希望调整我们的阈值，使其偏向于识别类别1的成员。例如，假设我们的模型被临床医生用来确定是否让患者进行更详细和昂贵的疾病检查。我们可能会认为将患有心脏病的患者误标为健康是一个比要求健康患者进行进一步测试以被视为不健康更严重的错误。为了实现这种偏差，我们可以将我们的分类阈值降低到`0.3`或`0.2`，例如。
- en: 'Ideally, what we would like is a visual way to assess the effect of changing
    the threshold on our performance metrics, and the precision recall curve is one
    such useful plot. In R, we can use the `ROCR` package to obtain precision-recall
    curves:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望有一种视觉方式来评估改变阈值对我们性能指标的影响，而精确度召回率曲线就是这样一种有用的图表。在R中，我们可以使用`ROCR`包来获取精确度召回率曲线：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can then plot the `perf` object to obtain our precision recall curve.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以绘制`perf`对象以获得我们的精确度召回率曲线。
- en: '![Classification metrics](img/00075.jpeg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![分类度量](img/00075.jpeg)'
- en: The graph shows us, for example, that, to obtain values of recall above 0.8,
    we'll have to sacrifice precision quite abruptly. To fine-tune our threshold,
    we'll want to see the individual thresholds that were used to compute this graph.
    A useful exercise is to create a data frame of cutoff values, which are the threshold
    values for which precision and recall change in our data, along with their corresponding
    precision and recall values. We can then subset this data frame to inspect individual
    thresholds that interest us.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示，例如，为了获得超过0.8的召回率值，我们可能不得不相当突然地牺牲精确度。为了微调我们的阈值，我们希望看到用于计算此图表的个别阈值。一个有用的练习是创建一个包含截止值的DataFrame，这些截止值是我们数据中精确度和召回率发生变化的阈值，以及它们对应的精确度和召回率值。然后我们可以从这个DataFrame中提取出我们感兴趣的个别阈值。
- en: 'For example, suppose we want to find a suitable threshold so that we have at
    least 90 % recall and 80 % precision. We can do this as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要找到一个合适的阈值，以便至少有90%的召回率和80%的精确度。我们可以这样做：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we can see, a threshold of roughly 0.35 will satisfy our requirements.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，大约0.35的阈值将满足我们的要求。
- en: Tip
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: You may have noticed that we used the `@` symbol to access some of the attributes
    of the `perf` object. This is because this object is a special type of object
    known as an S4 class. S4 classes are used to provide object-oriented features
    in R. A good reference to learn about S4 classes and object-orientated programming
    in R more generally is *Advanced R*, *Hadley Wickham*, *Chapman and Hall*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我们使用了`@`符号来访问`perf`对象的一些属性。这是因为这个对象是一种特殊类型的对象，称为S4类。S4类用于在R中提供面向对象的功能。关于S4类以及R中更广泛的面向对象编程的参考资料是*Advanced
    R*，*Hadley Wickham*，*Chapman and Hall*。
- en: Extensions of the binary logistic classifier
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二元逻辑分类器的扩展
- en: So far, the focus of this chapter has been on the binary classification task
    where we have two classes. We'll now turn to the problem of multiclass prediction.
    In [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    we studied the iris dataset where the goal is to distinguish between three different
    species of iris, based on features that describe the external appearance of iris
    flower samples. Before presenting additional examples of multiclass problems,
    we'll state an important caveat. The caveat is that several other methods for
    classification that we will study in this book, such as neural networks and decision
    trees, are both more natural and more commonly used than logistic regression for
    classification problems involving more than two classes. With that in mind, we'll
    turn to multinomial logistic regression, our first extension of the binary logistic
    classifier.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本章的重点一直集中在二元分类任务上，其中我们有两个类别。现在，我们将转向多类别预测问题。在[第1章](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "第1章。准备预测建模")“准备预测建模”中，我们研究了鸢尾花数据集，其目标是根据描述鸢尾花样本外部形态的特征来区分三种不同的鸢尾花种类。在介绍更多多类别问题的例子之前，我们将提出一个重要的警告。这个警告是，我们将在本书中研究的几种其他分类方法，如神经网络和决策树，在涉及两个以上类别的分类问题中，比逻辑回归更自然且更常用。考虑到这一点，我们将转向多项式逻辑回归，这是二元逻辑分类器的第一次扩展。
- en: Multinomial logistic regression
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多项式逻辑回归
- en: 'Suppose our target variable comprises *K* classes. For example, in the iris
    dataset, *K = 3*. **Multinomial logistic regression** tackles the multiclass problem
    by fitting *K-1* independent binary logistic classifier models. This is done by
    arbitrarily choosing one of the output classes as a reference class and fitting
    *K-1* regression models that compare each of the remaining classes to this one.
    For example, if we have two features, *X1* and *X2*, and three classes, which
    we could call 0, 1, and 2, we construct the following two models:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的目标变量包含*K*个类别。例如，在鸢尾花数据集中，*K = 3*。**多项式逻辑回归**通过拟合*K-1*个独立的二元逻辑回归分类模型来解决多类别问题。这是通过任意选择一个输出类别作为参考类别，并拟合*K-1*个回归模型来完成的，这些模型将每个剩余类别与这个类别进行比较。例如，如果我们有两个特征，*X1*和*X2*，以及三个类别，我们可以称之为0、1和2，我们将构建以下两个模型：
- en: '![Multinomial logistic regression](img/00076.jpeg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![多项式逻辑回归](img/00076.jpeg)'
- en: 'Here we used class 0 as the baseline and built two binary regression models.
    In the first, we compared class 1 against class 0 and in the second, we compare
    class 2 against class 0\. Note that because we now have more than one binary regression
    model, our model coefficients have two subscripts. The first subscript identifies
    the model and the second subscript pairs the coefficient with a feature. For example,
    *β[12]* is the coefficient of feature *X[2]* in the first model. We can write
    a general expression for the probability that our combined model predicts class
    *k* when there are *K* classes in total, numbered from *0* to *K-1*, and class
    0 is chosen as the reference class:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将类别0作为基线，并建立了两个二元回归模型。在第一个模型中，我们比较了类别1与类别0，在第二个模型中，我们比较了类别2与类别0。请注意，因为我们现在有多个二元回归模型，我们的模型系数有两个下标。第一个下标标识模型，第二个下标将系数与特征配对。例如，*β[12]*是第一个模型中特征*X[2]*的系数。我们可以为当总共有*K*个类别（编号从*0*到*K-1*，类别0被选为参考类别）时，我们的组合模型预测类别*k*的概率写一个通用表达式：
- en: '![Multinomial logistic regression](img/00077.jpeg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![多项式逻辑回归](img/00077.jpeg)'
- en: The reader should verify that the sum of all the output class probabilities
    is 1, as required. This particular mathematical form of an exponential divided
    by a sum of exponentials is known as the **softmax** function. For our three-class
    problem discussed previously, we simply substitute *K=3* in the preceding equations.
    At this point, we should mention some important characteristics of this approach.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应验证所有输出类概率之和为1，这是必需的。这种指数除以指数之和的特定数学形式被称为**softmax**函数。对于之前讨论的三个类别问题，我们只需在前面方程中将*K=3*代入即可。在此阶段，我们应该提到这种方法的一些重要特性。
- en: To begin with, we are training one fewer models than the total number of classes
    in our output variable, and as a result, it should be easy to see that this approach
    does not scale very well when we have a large number of output classes from which
    to choose. The fact that we are building and training so many models also means
    that we tend to need a much larger dataset to produce results with reasonable
    accuracy. Finally, as we independently compare each output class to a reference
    class, we make an assumption, known as the **Independence of Irrelevant Alternatives**
    (**IIA**) assumption.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们训练的模型数量比输出变量中类别的总数少一个，因此，当从大量可能的输出类别中进行选择时，这种方法扩展性并不好。我们构建和训练这么多模型的事实也意味着我们往往需要一个更大的数据集才能产生具有合理准确性的结果。最后，当我们独立地将每个输出类别与一个参考类别进行比较时，我们做出一个假设，称为**无关备选方案的独立性**（**IIA**）假设。
- en: 'The IIA assumption, in a nutshell, states that the odds of predicting one particular
    output class over another do not depend on whether we increase the number of possible
    output classes *k* by adding new classes. To illustrate this, suppose for simplicity
    that we model our iris dataset using multinomial logistic regression, and the
    odds of the output classes are 0.33 : 0.33 : 0.33 for the three different species
    so that every species is in a 1 : 1 ratio with every other species. The IIA assumption
    states that if we refit a model that includes samples of a new type of iris, for
    example, ensata (the Japanese iris), the odds ratio between the previous three
    iris species is maintained. A new overall odds ratio of 0.2 : 0.2 : 0.2 : 0.4
    between the four species (where the 0.4 corresponds to the ensata) would be valid,
    for example, because the 1 : 1 ratios between the old three species are maintained.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '简而言之，IIA假设指出，预测一个特定输出类别相对于另一个类别的概率，并不依赖于我们是否通过添加新类别来增加可能的输出类别数量*k*。为了说明这一点，假设为了简化，我们使用多项逻辑回归来模拟我们的鸢尾花数据集，输出类别的概率为0.33
    : 0.33 : 0.33，对于三种不同的物种，每种物种与其他物种的比例为1 : 1。IIA假设指出，如果我们重新拟合一个包含新类型鸢尾花样本（例如，日本鸢尾花ensata）的模型，那么前三个鸢尾花物种之间的概率比将保持不变。四个物种之间新的整体概率比为0.2
    : 0.2 : 0.2 : 0.4（其中0.4对应ensata）将是有效的，例如，因为旧的三种物种之间的1 : 1比例得到了保持。'
- en: Predicting glass type
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测玻璃类型
- en: 'In this section, we''ll demonstrate how we can train multinomial logistic regression
    models in R by way of an example dataset. The data we''ll examine is from the
    field of forensic science. Here, our goal is to examine properties of glass fragments
    found in crime scenes and predict the source of these fragments, for example,
    headlamps. The *glass identification dataset* is hosted by the UCI Machine Learning
    Repository at [http://archive.ics.uci.edu/ml/datasets/Glass+Identification](http://archive.ics.uci.edu/ml/datasets/Glass+Identification).
    We''ll first load the data in a data frame, rename the columns using information
    from the website, and throw away the first column (a unique identifier for each
    sample), as this has been arbitrarily assigned and is not needed by our model:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过一个示例数据集来展示如何在R中训练多项逻辑回归模型。我们将检查的数据来自法医学领域。在这里，我们的目标是检查犯罪现场发现的玻璃碎片特性，并预测这些碎片的来源，例如，前灯。*玻璃识别数据集*由UCI机器学习存储库托管，网址为[http://archive.ics.uci.edu/ml/datasets/Glass+Identification](http://archive.ics.uci.edu/ml/datasets/Glass+Identification)。我们首先将数据加载到数据框中，使用网站上的信息重命名列，并丢弃第一列（每个样本的唯一标识符），因为这已被任意分配，并且对我们模型来说不是必需的：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we''ll look at a table showing what each column in our data frame represents:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看一个表格，显示我们的数据框中每一列代表的内容：
- en: '| Column name | Type | Definition |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 类型 | 定义 |'
- en: '| --- | --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `RI` | Numerical | Refractive index |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `RI` | 数值 | 折射率 |'
- en: '| `Na` | Numerical | Percentage of Sodium Oxide by weight |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `Na` | 数值 | 按重量计算的氧化钠百分比 |'
- en: '| `Mg` | Numerical | Percentage of Magnesium Oxide by weight |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `Mg` | 数值 | 按重量计算的氧化镁百分比 |'
- en: '| `Al` | Numerical | Percentage of Aluminium Oxide by weight |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `Al` | 数值 | 按重量计算的氧化铝百分比 |'
- en: '| `Si` | Numerical | Percentage of Silicon Oxide by weight |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `Si` | 数值 | 按重量计算的氧化硅百分比 |'
- en: '| `K` | Numerical | Percentage of Potassium Oxide by weight |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `K` | 数值 | 按重量计算的氧化钾百分比 |'
- en: '| `Ca` | Numerical | Percentage of Calcium Oxide by weight |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `Ca` | 数值 | 按重量计算的氧化钙百分比 |'
- en: '| `Ba` | Numerical | Percentage of Barium Oxide by weight |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `Ba` | 数值 | 按重量计算的氧化钡百分比 |'
- en: '| `Fe` | Numerical | Percentage of Iron Oxide by weight |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `Fe` | 数值 | 按重量计算的氧化铁百分比 |'
- en: '| `Type` | Categorical | Type of glass (1: float processed building windows,
    2: nonfloat processed building windows, 3: float processed vehicle windows, 4:
    nonfloat processed vehicle windows, 5: containers, 6: tableware, 7: headlamps)
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| `Type` | 分类 | 玻璃类型（1：浮法加工建筑窗户，2：非浮法加工建筑窗户，3：浮法加工车辆窗户，4：非浮法加工车辆窗户，5：容器，6：餐具，7：车灯）
    |'
- en: 'As usual, we''ll proceed by preparing a training and test set for our glass
    data:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们将为玻璃数据准备训练集和测试集：
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, to perform multinomial logistic regression, we will use the `nnet` package.
    This package also contains functions that work with neural networks, so we will
    revisit this package in the next chapter as well. The `multinom()`function is
    used for multinomial logistic regression. This works by specifying a formula and
    a data frame, so it has a familiar interface. In addition, we can also specify
    the `maxit` parameter that determines the maximum number of iterations for which
    the underlying optimization procedure will run. Sometimes, we may find that training
    a model returns an error to the effect that convergence was not reached. In this
    case, one possible approach is to increase this parameter and allow the model
    to train over a larger number of iterations. In doing so, however, we should be
    aware of the fact that the model may take longer to train:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了执行多项逻辑回归，我们将使用`nnet`包。此包还包含与神经网络一起工作的函数，因此我们将在下一章再次讨论这个包。`multinom()`函数用于多项逻辑回归。它通过指定一个公式和一个数据框来实现，因此它有一个熟悉的界面。此外，我们还可以指定`maxit`参数，该参数确定底层优化过程将运行的最大迭代次数。有时，我们可能会发现训练模型返回一个错误，表明没有达到收敛。在这种情况下，一种可能的方法是增加这个参数，并允许模型在更多的迭代中进行训练。然而，在这样做的时候，我们应该意识到模型可能需要更长的时间来训练：
- en: '[PRE26]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Our model summary shows us that we have five sets of coefficients. This is because
    our `TYPE` output variable has six levels, which is to say that we are choosing
    to predict one of six different sources of glass. There are no examples in the
    data where `Type` takes the value 4\. The model also shows us standard errors,
    but no significance tests. In general, testing for coefficient significance is
    a lot trickier than with binary logistic regression, and this is one of the weaknesses
    of this approach. Often, we resort to independently testing the significance of
    coefficients for each of the binary models that we trained.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型摘要显示我们拥有五组系数。这是因为我们的`TYPE`输出变量有六个级别，也就是说我们选择预测六种不同的玻璃来源之一。数据中没有`Type`取值为4的例子。模型还显示了标准误差，但没有显著性测试。一般来说，测试系数显著性比二元逻辑回归要复杂得多，这也是这种方法的一个弱点。我们通常需要独立测试我们训练的每个二元模型的系数显著性。
- en: 'We won''t dwell on this any further, but will instead check the overall accuracy
    on our training data to give us a sense of the overall quality of fit:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会进一步探讨这个问题，而是会检查训练数据集的整体准确性，以了解整体拟合质量：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Our training accuracy is 72 %, which is not especially high. Here is the confusion
    matrix:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练准确率是72%，这并不特别高。以下是混淆矩阵：
- en: '[PRE28]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The confusion matrix reveals certain interesting facts. The first of these
    is that it seems that the model does not distinguish well between the first two
    classes, as many of the errors that are made involve these two. Part of the reason
    for this, however, is that these two classes are the most frequent in the data.
    The second problem that we are seeing is that the model never predicts class 3\.
    In fact, it completely confuses this class with the first two classes. The seven
    examples of class 6 are perfectly distinguished, and accuracy for class 7 is also
    near perfect, with only 1 mistake out of 25\. Overall, 72 % accuracy on training
    data is considered mediocre, but given the fact that we have six output classes
    and only 172 observations in our training data, this is to be expected with this
    type of model. Let''s repeat this for the test dataset:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵揭示了某些有趣的事实。首先，似乎模型在区分前两个类别方面做得不好，因为许多错误都涉及这两个类别。然而，部分原因是这两个类别在数据中是最频繁的。我们看到的第二个问题是模型从未预测类别3。事实上，它完全将这个类别与第一个两个类别混淆。类别6的七个例子被完美地区分开来，类别7的准确率也几乎是完美的，只有25个中的1个错误。总的来说，在训练数据上72%的准确率被认为是平庸的，但考虑到我们只有六个输出类别和172个训练数据观测值，这种情况是可以预料的。让我们为测试数据集重复这个过程：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As we can see, the confusion matrix paints a fairly similar picture to what
    we saw in training. Again, our model never predicts class 3 and the first two
    classes are still hard to distinguish. The number of observations in our test
    set is only 42, so this is very small. The test set accuracy is only 64 %, somewhat
    less than we saw in training. If our sample sizes were larger, we might suspect
    that our model suffers from overfitting, but in this case the variance of our
    test set performance is high due to the small sample size.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，混淆矩阵描绘了一幅与我们在训练中看到相当相似的图景。再次强调，我们的模型从未预测过类别3，前两个类别仍然难以区分。我们的测试集观测数只有42，这非常少。测试集准确率仅为64%，略低于我们在训练中看到的。如果我们的样本量更大，我们可能会怀疑我们的模型存在过拟合问题，但在这个案例中，由于样本量小，我们的测试集性能的方差很高。
- en: With multinomial logistic regression, we assumed that there was no natural ordering
    to the output classes. If our output variable is an ordinal, also known as an
    **ordered factor**, we can train a different model known as **ordinal logistic
    regression**. This is our second extension of the binary logistic regression model
    and is presented in the next section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在多项式逻辑回归中，我们假设输出类别没有自然顺序。如果我们的输出变量是序数，也称为**有序因素**，我们可以训练一个不同的模型，称为**有序逻辑回归**。这是我们二元逻辑回归模型的第二次扩展，将在下一节中介绍。
- en: Ordinal logistic regression
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列逻辑回归
- en: 'Ordered factors are very common in a number of scenarios. For example, human
    responses to surveys are often on subjective scales with scores ranging from 1
    to 5 or using qualitative labels with an intrinsic ordering such as *disagree*,
    *neutral*, and *agree*. We can try to treat these problems as regression problems,
    but we will still face similar issues to those we experienced when treating the
    binary classification problem as a regression problem. Instead of trying to train
    *K-1* binary logistic regression models as multinomial logistic regression, ordinal
    logistic regression trains a single model with multiple thresholds on the output.
    In order to achieve this, it makes an important assumption known as the assumption
    of **proportional odds**. If we have *K* classes and want to put a threshold on
    the output of a single binary logistic regression model, we will need *K-1* thresholds
    or cutoff points. The proportional odds assumption is that, in the logit scale,
    all of these thresholds lie on a straight line. Put differently, the model uses
    a single set of *βi* coefficients determining the slope of the straight line,
    but there are *K-1* intercept terms. For a model with *p* features and an output
    variable with *K* classes numbered from 0 to *K-1*, our model predicts:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有序因素在许多场景中非常常见。例如，人类对调查的回答通常是在1到5分的主观量表上，或者使用具有内在顺序的定性标签，如*不同意*、*中立*和*同意*。我们可以尝试将这些问题作为回归问题来处理，但我们仍然会面临我们在将二元分类问题作为回归问题处理时所遇到的问题。与尝试训练*K-1*个二元逻辑回归模型作为多项式逻辑回归不同，有序逻辑回归通过在输出上设置多个阈值来训练一个单一模型。为了实现这一点，它做出了一个重要的假设，即**比例优势假设**。如果我们有*K*个类别，并且想在单个二元逻辑回归模型的输出上设置阈值，我们需要*K-1*个阈值或截止点。比例优势假设是，在logit尺度上，所有这些阈值都位于一条直线上。换句话说，模型使用一组单一的*βi*系数来确定直线的斜率，但存在*K-1*个截距项。对于一个具有*p*个特征和*K*个类别输出变量（编号从0到*K-1*）的模型，我们的模型预测如下：
- en: '![Ordinal logistic regression](img/00078.jpeg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![有序逻辑回归](img/00078.jpeg)'
- en: This assumption may be a little hard to visualize and is perhaps best understood
    by way of an example. Suppose that we are trying to predict the results of a survey
    on of public opinion about a particular government policy, based on demographic
    data about survey participants.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这个假设可能有点难以可视化，也许通过一个例子来理解会更好。假设我们正在尝试根据调查参与者的人口统计数据来预测一项特定政府政策公众舆论调查的结果。
- en: The output variable is an ordered factor that ranges from *strongly disagree*
    to *strongly agree* on a five-point scale (also known as a **Likert** scale).
    Suppose that *l[0]* is the log-odds of the probabilities of strongly disagreeing
    versus disagreeing or better, *l[1]* is the log-odds of the probabilities of disagreeing
    or strongly disagreeing versus at least being neutral, and so on until *l[3]*.
    These four log-odds *l[0]* to *l[3]* form an arithmetic sequence, which means
    that the distance between consecutive numbers is a constant.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 输出变量是一个有序因素，它在五点量表上从 *强烈不同意* 到 *强烈同意* 范围内变化（也称为 **Likert** 量表）。假设 *l[0]* 是强烈不同意与不同意或更好之间的概率的对数几率，*l[1]*
    是不同意或强烈不同意与至少中立之间的概率的对数几率，依此类推，直到 *l[3]*。这四个对数几率 *l[0]* 到 *l[3]* 形成一个算术序列，这意味着连续数字之间的距离是一个常数。
- en: Note
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: Even though the proportional odds model is the most frequently cited logistic
    regression model that handles ordered factors, there are alternative approaches.
    A good reference that discusses the proportional odds model as well as other related
    models, such as the adjacent-category logistic model, is *Applied Logistic Regression
    Third Edition*, *Hosmer Jr.*, *Lemeshow*, and *Sturdivant*, published by *Wiley*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管比例优势模型是处理有序因素的逻辑回归模型中最常引用的模型，但还有其他方法。讨论比例优势模型以及其他相关模型，如相邻类别逻辑模型的好参考是 *《应用逻辑回归第三版》*，由
    *Hosmer Jr.*，*Lemeshow* 和 *Sturdivant* 撰写，并由 *Wiley* 出版。
- en: Predicting wine quality
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测葡萄酒质量
- en: 'The dataset for our ordinal logistic regression example is the *wine quality
    dataset* from the *UCI Machine Learning Repository*. The observations in this
    dataset consist of wine samples taken from both red and white wines of the Portuguese
    Vinho Verde variety. The wine samples have been rated on a scale from 1 to 10
    by a number of wine experts. The goal of the dataset is to predict the rating
    that an expert will give to a wine sample, using a range of physiochemical properties,
    such as acidity and alcohol composition. The website is [https://archive.ics.uci.edu/ml/datasets/Wine+Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality).
    The data is split into two files, one for red wines and one for white wines. We
    will use the white wine dataset, as it contains a larger number of samples. In
    addition, for simplicity and because the distribution of wine samples by score
    is sparse, we will contract our original output variable to a three point scale
    from 0 to 2\. First, let''s load and process our data:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们序对数逻辑回归示例的数据集是来自 *UCI 机器学习仓库* 的 *葡萄酒质量数据集*。该数据集中的观测值包括来自葡萄牙绿酒品种的红葡萄酒和白葡萄酒的酒样。这些酒样由多位葡萄酒专家按照1到10的评分标准进行评分。数据集的目标是使用一系列的物理化学特性，如酸度和酒精成分，来预测专家对酒样的评分。网站是
    [https://archive.ics.uci.edu/ml/datasets/Wine+Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)。数据被分为两个文件，一个用于红葡萄酒，一个用于白葡萄酒。我们将使用白葡萄酒数据集，因为它包含更多的样本。此外，为了简化，并且因为按评分分配的酒样分布稀疏，我们将原始输出变量缩减为从0到2的三点量表。首先，让我们加载数据并处理：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following table shows our input features and output variables:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了我们的输入特征和输出变量：
- en: '| Column name | Type | Definition |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 类型 | 定义 |'
- en: '| --- | --- | --- |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `fixed.acidity` | Numerical | Fixed Acidity (g(tartaric acid)/dm3) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| `固定酸度` | 数值 | 固定酸度（每立方分米苹果酸克数） |'
- en: '| `volatile.acidity` | Numerical | Volatile acidity (g(acetic acid)/dm3) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| `挥发酸度` | 数值 | 挥发酸度（每立方分米乙酸克数） |'
- en: '| `citric.acid` | Numerical | Citric acid (g/dm3) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `柠檬酸` | 数值 | 柠檬酸（每立方分米克数） |'
- en: '| `residual.sugar` | Numerical | Residual sugar (g/dm3) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `残糖` | 数值 | 残糖（每立方分米克数） |'
- en: '| `chlorides` | Numerical | Chlorides (g(sodium chloride)/dm3) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| `氯化物` | 数值 | 氯化物（每立方分米钠氯化物的克数） |'
- en: '| `free.sulfur.dioxide` | Numerical | Free Sulfur Dioxide (mg/dm3) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| `游离二氧化硫` | 数值 | 游离二氧化硫（每立方分米毫克数） |'
- en: '| `total.sulfur.dioxide` | Numerical | Total Sulfur Dioxide (mg/dm3) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| `总二氧化硫` | 数值 | 总二氧化硫（每立方分米毫克数） |'
- en: '| `density` | Numerical | Density (g/cm3) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| `密度` | 数值 | 密度（每立方厘米克数） |'
- en: '| `pH` | Numerical | PH |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| `pH` | 数值 | pH值 |'
- en: '| `sulphates` | Numerical | Sulphates (g(potassium sulphate)/dm3) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| `硫酸盐` | 数值 | 硫酸盐（每立方分米硫酸钾的克数） |'
- en: '| `alcohol` | Numerical | Alcohol (% vol.) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| `酒精` | 数值 | 酒精（体积百分比） |'
- en: '| `quality` | Categorical | Wine quality (1 = Poor, 2 = Average, 3 = Good)
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| `质量` | 分类 | 葡萄酒质量（1 = 差，2 = 一般，3 = 好） |'
- en: 'First, we''ll prepare a training and test set:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将准备训练集和测试集：
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we''ll use the `polr()` function from the `MASS` package to train a proportional
    odds logistic regression model. Just as with the other model functions we have
    seen so far, we first need to specify a formula and a data frame with our training
    data. In addition, we must specify the `Hess` parameter to `TRUE` in order to
    obtain a model that includes additional information, such as standard errors on
    the coefficients:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用来自 `MASS` 包的 `polr()` 函数来训练一个比例优势逻辑回归模型。就像我们迄今为止看到的其他模型函数一样，我们首先需要指定一个公式和一个包含我们的训练数据的
    data frame。此外，我们必须将 `Hess` 参数指定为 `TRUE` 以获得包含额外信息（如系数的标准误差）的模型：
- en: '[PRE32]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Our model summary shows us that we have three output classes, and we have two
    intercepts. Now, in this dataset we have many wines that were rated average (either
    5 or 6) and as a result, this class is the most frequent. We''ll use the `table()`
    function to count the number of samples by the output score and then apply `prop.table()`
    to express these as relative frequencies:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型摘要显示我们有三个输出类别，并且我们有两个截距。现在，在这个数据集中，我们有许多被评为平均（要么是 5 要么是 6）的葡萄酒，因此这个类别是最频繁的。我们将使用
    `table()` 函数按输出分数计算样本数量，然后应用 `prop.table()` 将这些表示为相对频率：
- en: '[PRE33]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Class 2, which corresponds to average wines, is by far the most frequent. In
    fact, a simple baseline model that always predicts this category would be correct
    74.6 % of the time. Let''s see whether our model does better than this. We''ll
    begin by looking at the fit on the training data and the corresponding confusion
    matrix:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 2，对应于平均葡萄酒，是最频繁的。事实上，一个总是预测这个类别的简单基线模型将有 74.6% 的时间是正确的。让我们看看我们的模型是否比这做得更好。我们将从查看训练数据上的拟合和相应的混淆矩阵开始：
- en: '[PRE34]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Our model performs only marginally better on the training data than our baseline
    model. We can see why this is the case—it predicts the average class (2) very
    often and almost never predicts class 1\. Repeating with the test set reveals
    a similar situation:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练数据上的模型表现仅略好于我们的基线模型。我们可以看到这是为什么——它经常预测平均类别（2），几乎从不预测类别 1。在测试集上重复此操作揭示了类似的情况：
- en: '[PRE35]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It seems that our model is not a particularly good choice for this dataset.
    As we know, there are a number of possible reasons ranging from having chosen
    the wrong type of model to having insufficient features or the wrong kind of features.
    One aspect of the ordinal logistic regression model that we should always try
    to check is whether the proportional odds assumption is valid. There is no universally
    accepted way to do this, but a number of different statistical tests have been
    proposed in the literature. Unfortunately, it is very difficult to find reliable
    implementations of these tests in R. One simple test that is easy to do, however,
    is to train a second model using multinomial logistic regression. Then, we can
    compare the AIC value of our two models. Let''s do this:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的模型并不是这个数据集的一个特别好的选择。正如我们所知，有许多可能的原因，从选择了错误的模型类型到特征不足或特征类型不正确。我们应该始终尝试检查有序逻辑回归模型的一个方面，即比例优势假设是否有效。没有普遍接受的方法来做这件事，但文献中已经提出了许多不同的统计测试。不幸的是，在
    R 中很难找到这些测试的可靠实现。然而，有一个简单的测试很容易做，那就是使用多项式逻辑回归训练第二个模型。然后，我们可以比较我们两个模型的 AIC 值。让我们这样做：
- en: '[PRE36]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The two models have virtually no difference in the quality of the fit. Let''s
    check their `AIC` values:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型在拟合质量上几乎没有差异。让我们检查它们的 `AIC` 值：
- en: '[PRE37]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The AIC is lower in the multinomial logistic regression model, which suggests
    that we might be better off working with that model. Another possible avenue for
    improvement on this dataset would be to carry out feature selection. The `step()`
    function that we saw in the previous chapter, for example, also works on models
    trained with the `polr()` function. We'll leave this as an exercise for the reader
    to verify that we can, in fact, get practically the same level of performance
    by removing some of the features. Dissatisfied with the results of logistic regression
    on this latest dataset, we will revisit it in subsequent chapters in order to
    see whether more sophisticated classification models can do better.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式逻辑回归模型的 AIC 值较低，这表明我们可能更擅长使用该模型。在这个数据集上改进的另一个可能途径是进行特征选择。例如，我们在上一章中看到的 `step()`
    函数也可以用于使用 `polr()` 函数训练的模型。我们将把这个作为读者的练习，以验证我们实际上可以通过删除一些特征来获得几乎相同水平的性能。对于这个最新数据集上逻辑回归的结果不满意，我们将在后续章节中重新审视它，以看看更复杂的分类模型是否能做得更好。
- en: Poisson regression
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泊松回归
- en: Another form of regression analysis is *Poisson* regression. This type of analysis
    is a generalized linear model or GLM, used to model *count* data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种回归分析形式是泊松回归。这种分析是一种广义线性模型或GLM，用于建模计数数据。
- en: Unlike the example in the previous section where wine samples had been rated
    (or ranked) on a scale from 1 to 10, count data is a (statistical) data type in
    which the observations can take only the non-negative integer values *{0, 1, 2,
    3, ...}*, and where these integers arise from *counting* rather than *ranking*.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一部分中葡萄酒样品按1到10的等级评分（或排名）的例子不同，计数数据是一种（统计）数据类型，其中观测值只能取非负整数值 *{0, 1, 2, 3,
    ...}*，并且这些整数是从计数而不是排名中产生的。
- en: 'Poisson regression assumes the outcome of your analysis has a *Poisson distribution*
    – in that it expresses: the probability of a number of events occurring in a fixed
    interval of time if these events occur with a known average rate and independently
    of the time since the last event.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 泊松回归假设你的分析结果具有泊松分布——即它表示：如果这些事件以已知的平均速率发生且与自上次事件以来经过的时间无关，则在固定时间间隔内发生一定数量事件的概率。
- en: An example model might be of the number of phone calls received by a software
    support center each hour. Predictors of the number of calls received include the
    number of days after a new version (of the software) has been released and the
    number of years the customer has been a user of the software (in keeping with
    our wine example, you could use Poisson regression to analyze the *number of bottles
    of wine sold during a month*, with perhaps *store location* and *the month of
    the year* as predictors).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能的模型可能是每小时软件支持中心接收的电话数量。接收电话数量的预测因素包括新版本（软件）发布后的天数和客户使用软件的年数（按照我们的葡萄酒例子，你可以使用泊松回归来分析一个月内销售的葡萄酒瓶数，可能以“商店位置”和“一年中的月份”作为预测因素）。
- en: A data scientist may also use a Poisson distribution for the number of events
    in other specified intervals, such as distance, area, or volume.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家还可能使用泊松分布来表示其他指定区间内事件的数量，例如距离、面积或体积。
- en: Negative Binomial regression
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负二项式回归
- en: While Poisson regression assumes a (known) average, *Negative Binomial regression*
    is implemented using what is referred to as *maximum likelihood estimation*.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然泊松回归假设一个（已知）的平均值，但负二项式回归是通过所谓的最大似然估计来实现的。
- en: Remember that, although Poisson distribution assumes that the *mean and variance
    are the same*, sometimes data will show greater variability or *extra variation
    that is greater than the mean*. When this occurs, Negative Binomial regression
    is a better choice because of its greater flexibility in that regard.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，尽管泊松分布假设**平均值和方差相同**，但有时数据会显示出更大的变异性或**大于平均值的额外变异**。当这种情况发生时，负二项式回归是一个更好的选择，因为它在这方面具有更大的灵活性。
- en: To illustrate, what if we consider that a university wants to predict the average
    number of days a student athlete may miss each year. Predictors (of the number
    of days of absence from class) include the type of sport the student athlete is
    a member of and their average GPA score. The variable **sport** is a four-level
    nominal variable indicating which sport the athlete participates in (in this case
    it's either `Football`, `Track`, `Field` `Hockey`, or `Volleyball`).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，如果我们考虑一个大学想要预测学生运动员每年可能缺席的平均天数。预测因素（缺席天数）包括学生运动员所属的体育类型和他们平均的GPA分数。变量**体育**是一个四级名义变量，表示运动员参加的体育项目（在这种情况下，它可以是“足球”、“田径”、“足球”或“排球”）。
- en: 'If we profile our data, suppose we find the following statistics:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析我们的数据，假设我们找到以下统计数据：
- en: '[PRE38]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We may look at the preceding statistics and see that the average numbers of
    days absent by sport seems to suggest that the variable sport is a good candidate
    for predicting the number of days absent because the mean value of the outcome
    appears to vary by the athlete chosen sport. However, looking at the standard
    deviations, we see that the variances within each type of sport are higher than
    the means within each level.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看前面的统计数据，并看到体育缺席的平均天数似乎表明变量“体育”是预测缺席天数的好候选，因为结果的平均值似乎会随着所选的运动员体育项目而变化。然而，当我们查看标准差时，我们发现每种体育类型内的方差高于每个级别内的平均值。
- en: These are the conditional means and variances. These differences suggest that
    over-dispersion (greater variability) is present and that a Negative Binomial
    model would be perhaps more appropriate. You could still use Poisson regression,
    but the standard errors could be biased.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是条件均值和方差。这些差异表明存在过度分散（更大的变异），并且负二项式模型可能更为合适。你仍然可以使用泊松回归，但标准误差可能会存在偏差。
- en: Negative Binomial regression takes advantage of one additional parameter (over
    Poisson regression) to fine-tune the variance independently from the mean (in
    this example it is the student athlete GPA score).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 负二项式回归利用了一个额外的参数（相对于泊松回归）来独立于均值（在此例中是学生运动员的GPA分数）调整方差。
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Logistic regression is the prototypical method for solving classification problems,
    just as linear regression was the prototypical example of a model to solve regression
    problems. In this chapter, we demonstrated why logistic regression offers a better
    way of approaching classification problems compared to linear regression with
    a threshold, by showing that the least squares criterion is not the most appropriate
    criterion to use when trying to separate two classes. We presented the notion
    of likelihood and its maximization as the basis for training a model. This is
    a very important concept that features time and again in various machine learning
    contexts. Logistic regression is an example of a generalized linear model. This
    is a model that relates the output variable to a linear combination of input features
    via a link function, which we saw was the logit function in this case. For the
    binary classification problem, we used R's `glm()` function to perform logistic
    regression on a real-world dataset and studied the model diagnostics to evaluate
    our model's performance. We discovered parallels with linear regression, in that
    the model produces deviance residuals that are analogous to least squared error
    residuals and that we can compute a pseudo R2 statistic that is analogous to the
    R2 statistic, which measures the goodness of fit in linear regression.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是解决分类问题的典型方法，就像线性回归是解决回归问题的典型例子一样。在本章中，我们通过展示最小二乘准则不是在尝试分离两个类别时最合适的准则，证明了逻辑回归与具有阈值的线性回归相比，提供了一种更好的处理分类问题的方法。我们介绍了似然的概念及其最大化作为训练模型的基础。这是一个非常重要的概念，在各种机器学习环境中反复出现。逻辑回归是广义线性模型的一个例子。这是一个通过链接函数将输出变量与输入特征的线性组合相关联的模型，我们在此例中看到了这是logit函数。对于二元分类问题，我们使用R的`glm()`函数在现实世界数据集上执行逻辑回归，并研究了模型诊断以评估我们的模型性能。我们发现与线性回归有相似之处，即模型产生偏差残差，类似于最小二乘误差残差，并且我们可以计算一个类似于R2统计量的伪R2统计量，该统计量衡量线性回归中的拟合优度。
- en: We also saw that we can apply regularization techniques to logistic regression
    models. Our tour of binary classification with the logistic regression model ended
    by studying precision-recall curves in order to choose appropriate model thresholds,
    an exercise that is very important when the cost of misclassifying an observation
    is not symmetric for the two classes involved. We then investigated two possible
    extensions of the binary logistic regression model to handle outputs with many
    class labels. These were the multinomial logistic regression model and the ordinal
    logistic regression model, which can be useful when the output classes are ordered.
    Lastly, we touched on the use of Poisson regression and, for models with greater
    variability, Negative Binomial regression.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到，我们可以将正则化技术应用于逻辑回归模型。我们通过研究精确率-召回率曲线来选择合适的模型阈值，结束了使用逻辑回归模型进行二元分类的旅行，这是一个在涉及的两个类别中，误分类观察值的成本不对称时非常重要的练习。然后，我们研究了两种可能的二元逻辑回归模型的扩展，以处理具有许多类别标签的输出。这些是多项式逻辑回归模型和有序逻辑回归模型，当输出类别有序时可能很有用。最后，我们简要提到了泊松回归的使用，以及对于具有更大变异性的模型，负二项式回归的使用。
- en: It turns out that logistic regression is not a great choice for multiclass settings
    in general. In the next chapter, we'll introduce neural networks, which are a
    nonlinear model used to solve both regression and classification problems. We'll
    also see how neural networks are able to handle multiple class labels in a natural
    way.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，逻辑回归在一般情况下并不是解决多类设置的最佳选择。在下一章中，我们将介绍神经网络，这是一种非线性模型，用于解决回归和分类问题。我们还将看到神经网络如何以自然的方式处理多个类别标签。
