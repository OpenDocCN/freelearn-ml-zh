<html><head></head><body>
		<div id="_idContainer081">
			<h1 id="_idParaDest-108"><a id="_idTextAnchor124"/>Chapter 6: Key Principles for Deploying Your ML System</h1>
			<p><a id="_idTextAnchor125"/>In this chapter, you will learn the fundamental principles for deploying <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models in production and implement the hands-on deployment of ML models for the business problem we have been working on. To get a comprehensive understanding and first-hand experience, we will deploy ML models that were trained and packaged previously (in <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines</em>, and <a href="B16572_05_Final_JM_ePub.xhtml#_idTextAnchor093"><em class="italic">Chapter 5</em></a>, <em class="italic">Model Evaluation and Packaging</em>) using the Azure ML service on two different deployment targets: an Azure container instance and a Kubernetes cluster.</p>
			<p>We will also learn how to deploy ML models using an open source framework called MLflow that we have already worked with. This will enable you to get an understanding of deploying ML models as REST API endpoints on diverse deployment targets using two different tools (the Azure ML service and MLflow). This will equip you with the skills required to deploy ML models for any given scenario on the cloud.</p>
			<p>In this chapter, we start by looking at how ML is different in research and production and continue exploring the following topics:<a id="_idTextAnchor126"/></p>
			<ul>
				<li>ML in research versus production</li>
				<li>Understanding the types of ML inference in production</li>
				<li>Going through the mapping infrastructure for your solution</li>
				<li>Hands-on deployment (for the business problem)</li>
				<li>Understanding the need for continuous integration and continuous deployment </li>
			</ul>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor127"/>ML in research versus production</h1>
			<p>ML in research <a id="_idIndexMarker434"/>is implemented with specific goals and priorities to improve the state of the art in the field, whereas the aim of ML in production is to optimize, automate, or augment a scenario or a business.</p>
			<p>In order to understand the deployment of ML models, let's start by comparing how ML is implemented in research versus production (in the industry). Multiple factors, such as performance, priority, data, fairness, and interpretability, as listed in <em class="italic">Table 6.1</em>, depict how deployments and ML work differently in research and production:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/Table_6.1.jpg" alt="Table 6.1 – ML in research and production&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 6.1 – ML in research and production</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor128"/>Data</h2>
			<p>In<a id="_idIndexMarker435"/> general, data in research projects is static because data scientists or statisticians are working on a set dataset and trying to beat the current state-of-the-art models. For example, recently, many breakthroughs in natural language processing models have been witnessed, for instance, with BERT from Google or XLNet from Baidu. To train these models, data was scraped and compiled into a static dataset. In the research world, to evaluate or benchmark the performance of the models, static datasets are used to evaluate the performance, as shown in <em class="italic">Table 6.2</em> (source: <a href="https://arxiv.org/abs/1906.08237">https://arxiv.org/abs/1906.08237</a>):</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/Table_6.2.jpg" alt="Table 6.2 – BERT versus XLNet performance (in research)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 6.2 – BERT versus XLNet performance (in research)</p>
			<p>For <a id="_idIndexMarker436"/>instance, we can compare the performance of two models by comparing their performance on a popular dataset called SQUAD (10,000+ QnA) version 1.1, on which BERT performs with 92.8% accuracy and XLNET with 94.0% accuracy. Likewise, data used in research for training and evaluating models is static, whereas data in production or in industrial use cases is dynamic and constantly changing as per the environment, operations, business, or users.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor129"/>Fairness</h2>
			<p>In <a id="_idIndexMarker437"/>real life, biased models can be costly. Unfair or biased decisions will lead to poor choices for business and operations. For ML models in production, it is important that decisions made are as fair as possible. It can be costly for the business if the models in production are not fair. For example, recently, Amazon made HR screening software that screens applicants based on their suitability for the job they applied for. ML specialists at Amazon discovered that male candidates were favored over female candidates (source: <a href="https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10">https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10</a>). This kind of system bias can be costly because, in Amazon's case, you can miss out on some amazing talent as a result of bias. Hence having fair models in production is critical and should be monitored constantly. In research, fair models are important as well but not as critical as in production or real life, and fairness is not critically monitored as in production. The goal in research is to beat the state of the art, and model fairness is a secondary goal.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor130"/>Interpretability</h2>
			<p>Model <a id="_idIndexMarker438"/>interpretability is critical in production in order to understand the correlation or causality between the ML model's decisions and its impact on the operations or business to optimize, augment, or automate a business or task at hand. This is not the case in research, where the goal is to challenge or beat the state-of-the-art results, and here the priority is better performance (such as accuracy, or other metrics). In the case of research, ML model interpretability is good to have but not mandatory. Typically, ML projects are more concerned with predicting outcomes than with understanding causality. ML models are great at finding correlations in data, but not causation. We strive not to fall into the pit of equating association with the cause in our ventures. Our ability to rely on ML is severely hampered as a result of this issue. This problem severely limits our ability to use ML <a id="_idIndexMarker439"/>to make decisions. We need resources that can understand the causal relationships between data and build ML solutions that can generalize well from a business viewpoint. Having the right model interpretability mechanisms can enhance our understanding of causality and enable us to craft ML solutions that generalize well and are able to handle previously unseen data. As a result, we can make more reliable and transparent decisions using ML.  </p>
			<p>In the case of production (in a business use case), a lack of interpretability is not recommended at all. Let us look at a hypothetical case. Let's assume you have cancer and have to choose a surgeon to perform your surgery. Two surgeons are available, one is human (with an 80% cure rate) and another is an AI black-box model (with a 90% cure rate) that cannot be interpreted or explain how it works, but it has a high cure rate. What would you choose? AI or a surgeon to cure cancer? It would be easier to replace the surgeon with AI if the model was not a black-box model. Though the AI is better than the surgeon, without understanding the model, decision, trust and compliance is an issue. Model interpretability is essential to make legal decisions. Hence, it is vital to have model interpretability for ML in production. We will learn more about this in later chapters.  </p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor131"/>Performance</h2>
			<p>When<a id="_idIndexMarker440"/> it comes to the performance of the ML models, the focus in research is to improve on the state-of-the-art models, whereas in production the focus is to build better models than simpler models that serve the business needs (<strong class="bold">state-of-the-art</strong> models are not the focus). </p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor132"/>Priority </h2>
			<p>In<a id="_idIndexMarker441"/> research, training the models faster and better is the priority, whereas in production faster inference is the priority as the focus is to make decisions and serve the business needs in real time.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor133"/>Understanding the types of ML inference in production</h1>
			<p>In the<a id="_idIndexMarker442"/> previous section, we saw the priorities of ML in research and production. To serve the business needs in production, ML models are inferred using various deployment targets, depending on the need. Predicting or making a decision using an ML model is called ML model inference. Let's explore ways of deploying ML models on different deployment targets to facilitate ML inference as per the business needs.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor134"/>Deployment targets</h2>
			<p>In this section, we will look at<a id="_idIndexMarker443"/> different types of deployment targets and why and how we serve ML models for inference in these deployment targets. Let's start by looking at a virtual machine or an on-premises server.</p>
			<h3>Virtual machines</h3>
			<p>Virtual machines<a id="_idIndexMarker444"/> can be<a id="_idIndexMarker445"/> on the cloud or on-premises, depending on the IT setup of a business or an organization. Serving ML models on virtual machines is quite common. ML models are served on virtual machines in the form of web services. The web service running on a virtual machine receives a user request (as an HTTP request) containing the input data. The web service, upon receiving the input data, preprocesses it in the required format to infer the ML model, which is part of the web service. After the ML model makes the prediction or performs the task, the output is transformed and presented in a user-readable format. Commonly <a id="_idIndexMarker446"/>into <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) or <strong class="bold">Extensible Markup language string</strong> (<strong class="bold">XML</strong>). Usually<a id="_idIndexMarker447"/>, a web service is served in the form of a REST API. REST API web services can be developed using multiple tools; for instance, FLASK or FAST API web application tools can be used to develop REST API web services using Python or Spring Boot in Java, or Plumber in R, depending on the need. A combination of virtual machines is used in parallel to scale and maintain the robustness of the web services.</p>
			<p>In order to<a id="_idIndexMarker448"/> orchestrate the traffic and to scale the machines, a load balancer is used to dispatch incoming requests to the virtual machines for ML model inference. This way, ML models are deployed on virtual machines on the cloud or on-premises to serve the business needs, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B16572_06_01.jpg" alt="Figure 6.1 – Deployment on virtual machines&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Deployment on virtual machines</p>
			<h3>Containers</h3>
			<p>Containers <a id="_idIndexMarker449"/>are a reliable way to run applications using the Linux OS with customized settings. A container<a id="_idIndexMarker450"/> is an application running with a custom setting orchestrated by the developer. Containers are an alternative and more resource-efficient way of serving models than virtual machines. They operate like virtual machines as they have their own runtime environment, which is isolated and confined to memory, the filesystem, and processes. </p>
			<p>Containers can be customized by developers to confine them to required resources such as memory, the filesystem, and processes, and the virtual machines are limited to such customizations. They are more flexible and operate in a modular way and hence provide more resource efficiency and optimization. They allow the possibility to scale to zero, as containers can be reduced to zero replicas and run a backup on request. This way, lower computation power consumption is possible compared to running web services on virtual machines. As a result of this lower computation power consumption, cost-saving on the cloud is possible.</p>
			<p>Containers<a id="_idIndexMarker451"/> present many advantages; however, one disadvantage can be the complexity required to work with containers, as it requires expertise.</p>
			<p>There are some differences in the way containers and virtual machines operate. For example, there can be multiple containers running inside a virtual machine that share the operating system and resources with the virtual machine, but the virtual machine runs its own resources and operating system. Containers<a id="_idIndexMarker452"/> can operate modularly, but virtual machines operate as single units. Docker is used to build and deploy containers; however, there are alternatives, such as Mesos and CoreOS rkt. A container is typically packaged with the ML model and web service to facilitate the ML inference, similar to how we serve the ML model wrapped in a web service in the virtual machine. Containers need to be orchestrated to be consumed by users. The orchestration of containers means the automation of the deployment, management, scaling, and networking of containers.  Containers are orchestrated using a container orchestration system such as Kubernetes. In the following diagram, we can see container orchestration with auto-scaling (based on the traffic of requests):</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B16572_06_02.jpg" alt="Figure 6.2 – Deployment on containers&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Deployment on containers</p>
			<h3>Serverless</h3>
			<p>Serverless computing, as the name<a id="_idIndexMarker453"/> suggests, does not<a id="_idIndexMarker454"/> involve a virtual machine or container. It eliminates infrastructure management tasks such as OS management, server management, capacity provisioning, and disk management. Serverless computing enables developers and organizations to focus on their core product instead of mundane tasks such as managing and operating servers, either on the cloud or on-premises. Serverless computing is facilitated by using cloud-native services.</p>
			<p>For instance, Microsoft Azure uses Azure Functions, and AWS uses Lambda functions to deploy serverless applications. The deployment for serverless applications involves submitting a collection of files (in the form of <strong class="source-inline">.zip</strong> files) to run ML applications. The .zip archive typically has a file with a particular function or method to execute. The zip archive is uploaded to the cloud platform using cloud services and deployed as a serverless application. The deployed application serves as an API endpoint to submit input to the serverless application serving the ML model.</p>
			<p>Deploying ML models using serverless applications can have many advantages: there's no need to install or upgrade dependencies, or maintain or upgrade systems. Serverless applications auto-scale on demand and are robust in overall performance. Synchronous (execution happens one after another in a single series, A-&gt;B-&gt;C-&gt;D) and asynchronous (execution happens in parallel or on a priority basis, not in order: A-&gt;C-&gt;D-&gt;B or A and B together in parallel and C and D in parallel) operations are both supported by serverless functions. However, there are some disadvantages, such as cloud resource availability such as RAM or disk space or GPU unavailability, which can be crucial requirements for running heavy models such as deep learning or reinforcement learning models. For example, we can hit the wall of resource limitation if we have deployed a model without using serverless operations. The model or application deployed will not auto-scale and thus limit the available computation power. If more users infer the model or application than the limit, we will hit the resource unavailability blocker. In the following diagram, we can see how<a id="_idIndexMarker455"/> traditional applications and serverless applications are developed: </p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B16572_06_03.jpg" alt="Figure 6.3: Traditional versus serverless deployments&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3: Traditional versus serverless deployments</p>
			<p>To develop serverless applications, the developer only has to focus on the application's logic and not worry about backend or security code, which is taken care of by the cloud services upon deploying serverless applications.</p>
			<h3>Model streaming</h3>
			<p>Model streaming is a<a id="_idIndexMarker456"/> method of <a id="_idIndexMarker457"/>serving models for handling streaming data. There is no beginning or end of streaming data. Every second, data is produced from thousands of sources and must be processed and analyzed as soon as possible. For example, Google Search results must be processed in real time. Model streaming is another way of deploying ML models. It has two main advantages over other model serving techniques, such as REST APIs or batch processing approaches. The first advantage is asynchronicity (serving multiple requests at a time). REST API ML applications are robust and scalable but have the limitation of being synchronous (they process requests <a id="_idIndexMarker458"/>from the client on a first come, first serve basis), which can lead to high latency and resource utilization. To cope with this limitation, stream processing is available. It is inherently asynchronous as the user or client does not have to coordinate or wait for the system to process the request.</p>
			<p>Stream processing <a id="_idIndexMarker459"/>is able to process asynchronously and serve the users on the go. In order to do so, stream processing uses a message broker to receive messages from the users or clients. The message <a id="_idIndexMarker460"/>broker allows the data as it comes and spreads the processing over time. The message broker decouples the incoming requests and facilitates communication between the users or clients and the service without being aware of each other's operations, as shown in figure 5.4. There are a couple of options for message streaming brokers, such as Apace Storm, Apache Kafka, Apache Spark, Apache Flint, Amazon Kinesis, and StreamSQL. The tool you choose is dependent on the IT setup and architecture. </p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B16572_06_04.jpg" alt="Figure 6.4 – Model streaming process &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Model streaming process </p>
			<p>The second advantage <a id="_idIndexMarker461"/>of stream processing is when multiple models are being inferred in an ML system. REST APIs are great for single-model or dual-model processing, but they manage to produce latency and use high amounts of computation of resources when multiple models need to be inferred, and on top of this they are limited to synchronous inference.</p>
			<p>In the case of multiple models, stream processing is a good option as all models and artifacts (code and files) needed to run the ML system can be packaged together and deployed on a stream processing engine (it runs on its own cluster of machines and manages resource allocation for distribut<a id="_idTextAnchor135"/>ing data processing).</p>
			<p>For example, let's look at the use case of an intelligent email assistant tasked to automate customer service, as shown in <em class="italic">Figure 5.4</em>. In order to automate replies to serve its users, the email assistant system performs multiple predictions using multiple models:</p>
			<ul>
				<li>Predict the class of the email, such as spam or accounts or renewal</li>
				<li>Intent recognition</li>
				<li>Sentiment prediction</li>
				<li>Answer/text generation</li>
			</ul>
			<p>These four models deployed on REST API endpoints will generate high latency and maintenance costs, whereas a streaming service is a good alternative as it can package and serve multiple models as one process and continuously serve user requests in the form of a stream. Hence in such cases, streaming is recommended over REST API endpoints.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor136"/>Mapping the infrastructure for our solution</h2>
			<p>In this section, we map <a id="_idIndexMarker462"/>infrastructural needs and deployment targets needed to address diverse business needs, as seen in <em class="italic">Table 6.3</em>: </p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/Table_6.3.jpg" alt="Table 6.3 – Mapping the infrastructure for ML solutions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 6.3 – Mapping the infrastructure for ML solutions</p>
			<p>Depending on <a id="_idIndexMarker463"/>your use case, it is recommended to select suitable infrastructure and deployment targets to serve ML models to generate business or operational impact.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor137"/>Hands-on deployment (for the business problem)</h1>
			<p>In this section, we will learn how<a id="_idIndexMarker464"/> to deploy solutions for the business problem we have been working on. So far, we have done data processing, ML model training, serialized models, and registered them to the Azure ML workspace. In this section, we will explore how inference is performed on the serialized model on a container and an auto-scaling cluster. These deployments will give you a broad understanding and will prepare you well for your future assignments.</p>
			<p>We will use Python as the primary programming language, and Docker and Kubernetes for building and deploying containers.  We will start with deploying a REST API service on an Azure container instance using Azure ML. Next, we will deploy a REST API service on an auto-scaling cluster using Kubernetes (for container orchestration) using Azure ML, and lastly, we will deploy on an<a id="_idIndexMarker465"/> Azure container instance using MLflow and an open source ML framework; this way, we will learn how to use multiple tools and deploy ML models on the cloud (Azure). Let's get started with deployment on <strong class="bold">Azure Container Instances</strong> (<strong class="bold">ACI</strong>). </p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor138"/>Deploying the model on ACI</h2>
			<p>To get <a id="_idIndexMarker466"/>started with deployment, go to the GitHub repository cloned <a id="_idIndexMarker467"/>on Azure DevOps previously (in <a href="B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, <em class="italic">Code Meets Data</em>), access the folder named <strong class="source-inline">06_ModelDeployment</strong>, and follow the implementation steps in the <strong class="source-inline">01_Deploy_model_ACI.ipynb</strong> notebook:</p>
			<ol>
				<li>We start by importing the required packages and check for the version of the Azure ML SDK, as shown in the following code:<p class="source-code">%matplotlib inline</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import azureml.core</p><p class="source-code">from azureml.core import Workspace</p><p class="source-code">from azureml.core.model import Model</p><p class="source-code"># display the core SDK version number</p><p class="source-code">print("Azure ML SDK Version: ", azureml.core.VERSION)</p><p>The preceding code will print the Azure ML SDK version (for example, <strong class="source-inline">1.10.0</strong>; your version may be different). </p></li>
				<li>Next, using<a id="_idIndexMarker468"/> the <strong class="source-inline">workspace</strong> function from the Azure ML SDK, we<a id="_idIndexMarker469"/> connect to the ML workspace and download the required serialized files and model trained earlier using the <strong class="source-inline">Model</strong> function from the workspace. The serialized <strong class="source-inline">scaler</strong> and <strong class="source-inline">model</strong> are used to perform inference or prediction. <strong class="source-inline">Scaler</strong> will be used to shrink the input data to the same scale of data that was used for model training, and the <strong class="source-inline">model</strong> file is used to make predictions on the incoming data:<p class="source-code">ws = <strong class="bold">Workspace</strong>.from_config()</p><p class="source-code">print(ws.name, ws.resource_group, ws.location, sep = '\n')</p><p class="source-code">scaler = <strong class="bold">Model</strong>(ws,'scaler').download(exist_ok=True)</p><p class="source-code">model = <strong class="bold">Model</strong>(ws,'support-vector-classifier').download(exist_ok=True)</p></li>
				<li>After the <strong class="source-inline">scaler</strong> and the <strong class="source-inline">model</strong> files are downloaded, the next step is to prepare the <strong class="source-inline">scoring</strong> file. The <strong class="source-inline">scoring</strong> file is used to infer the ML models in the containers deployed with the ML service in the Azure container instance and Kubernetes cluster. The <strong class="source-inline">scoring</strong> script takes input passed by the user and infers the ML model for prediction and then serves the output with the prediction to the <a id="_idIndexMarker470"/>user. It contains two primary functions, <strong class="source-inline">init()</strong> and <strong class="source-inline">run()</strong>. We start by<a id="_idIndexMarker471"/> importing the required libraries and then define the <strong class="source-inline">init()</strong> and <strong class="source-inline">run()</strong> functions:<p class="source-code">%%writefile score.py</p><p class="source-code">import json</p><p class="source-code">import numpy as np</p><p class="source-code">import os</p><p class="source-code">import pickle</p><p class="source-code">import joblib</p><p class="source-code">import onnxruntime</p><p class="source-code">import time</p><p class="source-code">from azureml.core.model import Model</p><p><strong class="source-inline">%%writefile score.py</strong> writes this code into a file named <strong class="source-inline">score.py</strong>, which is later packed as part of the ML service in the container for performing ML model inference. </p></li>
				<li>We define the <strong class="source-inline">init()</strong> function; it downloads the required models and deserializes them into variables to be used for the predictions:<p class="source-code">def <strong class="bold">init</strong>():</p><p class="source-code">    global model, scaler, input_name, label_name</p><p class="source-code">    scaler_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'scaler/2/scaler.pkl')</p><p class="source-code">    # deserialize the scalar file back into a variable to be used for inference</p><p class="source-code">    scaler = joblib.load(scaler_path)</p><p class="source-code">    </p><p class="source-code">model_onnx = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'support-vector-classifier/2/svc.onnx')</p><p class="source-code"># deserialize support vector classifer model </p><p class="source-code">    model = onnxruntime.InferenceSession(model_onnx, None)</p><p class="source-code">    input_name = model.get_inputs()[0].name</p><p class="source-code">    label_name = model.get_outputs()[0].name</p><p>Using <strong class="source-inline">onnxruntime</strong> we<a id="_idIndexMarker472"/> can deserialize the support vector classifier model. The <strong class="source-inline">InferenceSession()</strong> function is used for deserializing and serving the model for inference, and the <strong class="source-inline">input_name</strong> and <strong class="source-inline">label_name</strong> variables are loaded from the deserialized model.  </p></li>
				<li>In a<a id="_idIndexMarker473"/> nutshell, the <strong class="source-inline">init()</strong> function loads files (<strong class="source-inline">model</strong> and <strong class="source-inline">scaler</strong>) and deserializes and serves the model and artifact files needed for making predictions, which are used by the <strong class="source-inline">run()</strong> function as follows:<p class="source-code">def <strong class="bold">run</strong>(raw_data):</p><p class="source-code">                try: </p><p class="source-code">                    data = np.array(json.loads(raw_data)['data']).astype('float32')</p><p class="source-code">                    data = scaler.fit_transform(data. 	                    reshape(1, 7))</p><p class="source-code">                    # make prediction</p><p class="source-code">                    model_prediction = model.run([label_name], {input_name: data.astype(np.float32)})[0]</p><p class="source-code"># you can return any data type as long as it is JSON-serializable</p><p class="source-code">                 </p><p class="source-code">                except Exception as e:   </p><p class="source-code">                    model_prediction = 'error'</p><p class="source-code">                    </p><p class="source-code">                return model_prediction</p><p>The <strong class="source-inline">run()</strong> function takes raw incoming data as the argument, performs ML model inference, and returns the predicted result as the output. When called, the <strong class="source-inline">run()</strong> function receives the incoming data, which is sanitized and loaded into a variable for scaling. The incoming data is scaled using the scaler loaded previously in the <strong class="source-inline">init()</strong> function. Next, the model inference step, which is the key step, is performed by inferencing scaled data to the model, as shown previously. The prediction inferred<a id="_idIndexMarker474"/> from the model is then returned as the output. This way, the scoring file is written into <strong class="source-inline">score.py</strong> to be used for deployment.</p></li>
				<li>Next, we <a id="_idIndexMarker475"/>will proceed to the crucial part of deploying the service on an Azure container instance. For this, we define a deployment environment by creating an<a id="_idIndexMarker476"/> environment <strong class="bold">YAML</strong> (<strong class="bold">Yet Another Markup Language</strong>) file called <strong class="source-inline">myenv.yml</strong>, as shown in the following code. Using the <strong class="source-inline">CondaDependencies()</strong> function, we mention all the <strong class="source-inline">pip</strong> packages that need to be installed inside the Docker container that will be deployed as the ML service. Packages such as <strong class="source-inline">numpy</strong>, <strong class="source-inline">onnxruntime</strong>, <strong class="source-inline">joblib</strong>, <strong class="source-inline">azureml-core</strong>, <strong class="source-inline">azureml-defaults</strong>, and <strong class="source-inline">scikit-learn</strong> are installed inside the container upon triggering the environment file:<p class="source-code">from azureml.core.conda_dependencies import CondaDependencies </p><p class="source-code">myenv = <strong class="bold">CondaDependencies</strong>.create(pip_packages=["numpy", "onnxruntime", "joblib", "azureml-core", "azureml-defaults", "scikit-learn==0.20.3"])</p><p class="source-code">with open("myenv.yml","w") as f:</p><p class="source-code">    f.write(myenv.serialize_to_string())</p></li>
				<li>Next, we<a id="_idIndexMarker477"/> define the inference configuration by using the <strong class="source-inline">InferenceConfig()</strong> function, which <a id="_idIndexMarker478"/>takes <strong class="source-inline">score.py</strong> and the environment file as the arguments upon being called. Next, we call the <strong class="source-inline">AciWebservice()</strong> function to initiate the compute configuration ( <strong class="source-inline">cpu_cores</strong> and <strong class="source-inline">memory</strong>) in the <strong class="source-inline">aciconfig</strong> variable as follows:<p class="source-code">from azureml.core.model import InferenceConfig</p><p class="source-code">from azureml.core.environment import Environment</p><p class="source-code">myenv = Environment.from_conda_specification(name="myenv", file_path="myenv.yml")</p><p class="source-code">inference_config = <strong class="bold">InferenceConfig</strong>(entry_script="score.py", environment=myenv)</p><p class="source-code">from azureml.core.webservice import AciWebservice</p><p class="source-code">aciconfig = <strong class="bold">AciWebservice</strong>.deploy_configuration(cpu_cores=1, </p><p class="source-code">memory_gb=1, </p><p class="source-code">tags={"data": "weather"}, </p><p class="source-code">description='weather-prediction')</p></li>
				<li>Now we are <a id="_idIndexMarker479"/>all set to deploy the ML or web service on the ACI. We<a id="_idIndexMarker480"/> will use <strong class="source-inline">score.py</strong>, the environment file (<strong class="source-inline">myenv.yml</strong>), <strong class="source-inline">inference_config</strong>, and <strong class="source-inline">aci_config</strong> to deploy the ML or web service. We will need to point to the models or artifacts to deploy. For this, we use the <strong class="source-inline">Model()</strong> function to load the <strong class="source-inline">scaler</strong> and <strong class="source-inline">model</strong> files from the workspace and get them ready for deployment:<p class="source-code">%%time</p><p class="source-code">from azureml.core.webservice import Webservice</p><p class="source-code">from azureml.core.model import InferenceConfig</p><p class="source-code">from azureml.core.environment import Environment</p><p class="source-code">from azureml.core import Workspace</p><p class="source-code">from azureml.core.model import Model</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">model1 = Model(ws, 'support-vector-classifier')</p><p class="source-code">model2 = Model(ws, 'scaler')</p><p class="source-code">service = Model.<strong class="bold">deploy</strong>(workspace=ws, </p><p class="source-code">                       name='weatherprediction', </p><p class="source-code">                       models=[model1, model2], </p><p class="source-code">                       inference_config=inference_config, </p><p class="source-code">                       deployment_config=aciconfig)</p><p class="source-code">service.wait_for_deployment(show_output=True)</p></li>
				<li>After the models are mounted into variables, <strong class="source-inline">model1</strong> and <strong class="source-inline">model2</strong>, we proceed with deploying them as a web service. We use the <strong class="source-inline">deploy()</strong> function to deploy the mounted models as a web service on the ACI, as shown in the preceding code. This process will take around 8 minutes, so grab your popcorn and enjoy the service being deployed. You will see a message like this:<p class="source-code">Running..............................................................................</p><p class="source-code">Succeeded</p><p class="source-code">ACI service creation operation finished, operation "Succeeded"</p><p class="source-code">CPU times: user 610 ms, sys: 103 ms, total: 713 ms</p><p class="source-code">Wall time: 7min 57s</p><p>Congratulations! You have <a id="_idIndexMarker481"/>successfully deployed your first <a id="_idIndexMarker482"/>ML service using MLOps. </p></li>
				<li>Let's check out the workings and robustness of the deployed service. Check out the service URL and Swagger URL, as shown in the following code. You can use these URLs to perform ML model inference for input data of your choice in real time:<p class="source-code">print(service.scoring_uri)</p><p class="source-code">print(service.swagger_uri)</p></li>
				<li>Check for the deployed service in the Azure ML workspace.</li>
				<li>Now, we can test the service using the Azure ML SDK <strong class="source-inline">service.run()</strong> function by passing some input data as follows:<p class="source-code">import json</p><p class="source-code">test_sample = json.dumps({'data': [[34.927778, 0.24, 7.3899, 83, 16.1000, 1016.51, 1]]})</p><p class="source-code">test_sample = bytes(test_sample,encoding = 'utf8')</p><p class="source-code">prediction = <strong class="bold">service.run </strong>(input_data=test_sample)</p><p>The features in the input data are in this order: <strong class="source-inline">Temperature_C</strong>, <strong class="source-inline">Humidity</strong>, <strong class="source-inline">Wind_speed_kmph</strong>, <strong class="source-inline">Wind_bearing_degrees</strong>, <strong class="source-inline">Visibility_km</strong>, <strong class="source-inline">Pressure_millibars</strong>, and <strong class="source-inline">Current_weather_condition</strong>. Encode the input data in UTF-8 for smooth inference. Upon inferring the model using <strong class="source-inline">service.run()</strong>, the model returns a prediction of <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>. <strong class="source-inline">0</strong> means a clear sky and <strong class="source-inline">1</strong> means it will rain. Using this service, we can make weather predictions at the port of Turku as tasked in the business problem.</p></li>
				<li>The service<a id="_idIndexMarker483"/> we have deployed is a REST API web service that we can infer <a id="_idIndexMarker484"/>with an HTTP request as follows:<p class="source-code">import requests</p><p class="source-code">headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}</p><p class="source-code">if service.auth_enabled:</p><p class="source-code">    headers['Authorization'] = 'Bearer '+ service.get_keys()[0]</p><p class="source-code">elif service.token_auth_enabled:</p><p class="source-code">    headers['Authorization'] = 'Bearer '+ service.get_token()[0]</p><p class="source-code">scoring_uri = service.scoring_uri</p><p class="source-code">print(scoring_uri)</p><p class="source-code">response = requests.post(scoring_uri, data=test_sample, headers=headers)</p><p class="source-code">print(response.status_code)</p><p class="source-code">print(response.elapsed)</p><p class="source-code">print(response.json())</p><p>When a <strong class="source-inline">POST</strong> request is <a id="_idIndexMarker485"/>made by passing input data, the service returns the model prediction in the form of <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>. When you get such a prediction, your service is working and is robust enough to serve production needs. </p><p>Next, we will deploy the service on an auto-scaling cluster; this is ideal for production scenarios as the deployed service can auto-scale and serve user needs. </p></li>
			</ol>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor139"/>Deploying the model on Azure Kubernetes Service (AKS)</h2>
			<p>To get <a id="_idIndexMarker486"/>started with the deployment, go<a id="_idIndexMarker487"/> to the Git repository cloned on Azure DevOps in <a href="B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, <em class="italic">Code Meets Data</em>, access the <strong class="source-inline">06_ModelDeployment</strong> folder, and follow the implementation steps in the <strong class="source-inline">02_Deploy_model_AKS.ipynb</strong> notebook:</p>
			<ol>
				<li value="1">As we did in the previous section, start by importing the required packages, such as <strong class="source-inline">matplotlib</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">azureml.core</strong>, and the required functions, such as <strong class="source-inline">Workspace</strong> and <strong class="source-inline">Model</strong>, from <strong class="source-inline">azureml.core</strong>, as shown in the following code block:<p class="source-code">%matplotlib inline</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code"> </p><p class="source-code">import azureml.core</p><p class="source-code">from azureml.core import Workspace</p><p class="source-code">from azureml.core.model import Model</p><p class="source-code"># display the core SDK version number</p><p class="source-code">print("Azure ML SDK Version: ", azureml.core.VERSION)</p></li>
				<li>Print the <a id="_idIndexMarker488"/>version of the Azure <a id="_idIndexMarker489"/>ML SDK and check for the version (it will print, for example, <strong class="source-inline">1.10.0</strong>; your version may be different). Use the config file and <strong class="source-inline">Workspace</strong> function connect to your workspace, as shown in the following code block:<p class="source-code">ws = Workspace.from_config()</p><p class="source-code">print(ws.name, ws.resource_group, ws.location, sep = '\n')</p><p class="source-code">scaler = Model(ws,'scaler').download(exist_ok=True)</p><p class="source-code">model = Model(ws,'support-vector-classifier').download(exist_ok=True)</p></li>
				<li>Download the <strong class="source-inline">model</strong> and <strong class="source-inline">scaler</strong> files as we did previously. After the <strong class="source-inline">model</strong> and the <strong class="source-inline">scaler</strong> files are downloaded, the next step is to prepare the <strong class="source-inline">scoring</strong> file, which is used to infer the ML models in the containers deployed with the ML service. The <strong class="source-inline">scoring</strong> script takes an input passed by the user, infers the ML model for prediction, and then serves the output with the prediction to the user. We will start by importing the required libraries, as shown in the following code block:<p class="source-code">%%writefile score.py</p><p class="source-code">import json</p><p class="source-code">import numpy as np</p><p class="source-code">import os</p><p class="source-code">import pickle</p><p class="source-code">import joblib</p><p class="source-code">import onnxruntime</p><p class="source-code">import time</p><p class="source-code">from azureml.core.model import Model</p></li>
				<li>As we <a id="_idIndexMarker490"/>made <strong class="source-inline">score.py</strong> previously for ACI deployment, we will use the same file. It contains two primary functions, <strong class="source-inline">init()</strong> and <strong class="source-inline">run()</strong>. We define the <strong class="source-inline">init()</strong> function; it downloads <a id="_idIndexMarker491"/>the required models and deserializes them into variables to be used for predictions:<p class="source-code">def init():</p><p class="source-code">    global model, scaler, input_name, label_name</p><p class="source-code">    scaler_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'scaler/2/scaler.pkl')</p><p class="source-code">    # deserialize the model file back into a sklearn model</p><p class="source-code">    scaler = joblib.load(scaler_path)</p><p class="source-code">    </p><p class="source-code">    model_onnx = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'support-vector-classifier/2/svc.onnx')</p><p class="source-code">    model = onnxruntime.InferenceSession(model_onnx, None)</p><p class="source-code">    input_name = model.get_inputs()[0].name</p><p class="source-code">    label_name = model.get_outputs()[0].name</p></li>
				<li>As we did in the previous section on ACI deployment, by using <strong class="source-inline">onnxruntime</strong> package functions we can deserialize the support vector classifier model.The <strong class="source-inline">InferenceSession()</strong> function is used to deserialize and serve the model for inference, and the <strong class="source-inline">input_name</strong> and <strong class="source-inline">label_name</strong> variables are loaded from the <a id="_idIndexMarker492"/>deserialized model. In a nutshell, the <strong class="source-inline">init()</strong> function loads files (<strong class="source-inline">model</strong> and <strong class="source-inline">scaler</strong>), and<a id="_idIndexMarker493"/> deserializes and serves the model and artifact files needed for making predictions that are used by the <strong class="source-inline">run()</strong> function:<p class="source-code">def run(raw_data):</p><p class="source-code">                try: </p><p class="source-code">                    data = np.array(json.loads(raw_data)['data']).astype('float32')</p><p class="source-code">                    data = scaler.fit_transform(data.reshape(1, 7))</p><p class="source-code">                    # make prediction</p><p class="source-code">                    model_prediction = model.run([label_name], {input_name: data.astype(np.float32)})[0]</p><p class="source-code">                    # you can return any data type as long as it is JSON-serializable</p><p class="source-code">                 </p><p class="source-code">                except Exception as e:   </p><p class="source-code">                    model_prediction = 'error'</p><p class="source-code">                    </p><p class="source-code">                return model_prediction</p><p>We will use the same <strong class="source-inline">run()</strong> function previously used in the section <em class="italic">Deploying the model on ACI</em> for the AKS deployment. With this we can proceed to deploying the service on AKS. </p></li>
				<li>Next, we will proceed to the crucial part of deploying the service on <strong class="bold">Azure Kubernetes Service</strong>. Create an environment in which your model will be deployed using the <strong class="source-inline">CondaDependencies()</strong> function. We will mention all the required <strong class="source-inline">pip</strong> and <strong class="source-inline">conda</strong> packages to be installed inside the Docker container that will be deployed as the ML service. Packages such as <strong class="source-inline">numpy</strong>, <strong class="source-inline">onnxruntime</strong>, <strong class="source-inline">joblib</strong>, <strong class="source-inline">azureml-core</strong>, <strong class="source-inline">azureml-defaults</strong>, and <strong class="source-inline">scikit-learn</strong> are installed inside the container upon triggering the <strong class="source-inline">environment</strong> file. Next, use the publicly available container in the Microsoft Container Registry without any <a id="_idIndexMarker494"/>authentication. This container will install your environment and will be configured for deployment to your target AKS:<p class="source-code">from azureml.core import Environment</p><p class="source-code">from azureml.core.conda_dependencies import CondaDependencies </p><p class="source-code">conda_deps = CondaDependencies.create(conda_packages=['numpy','scikit-learn==0.19.1','scipy'], pip_packages=["numpy", "onnxruntime", "joblib", "azureml-core", "azureml-defaults", "scikit-learn==0.20.3"])</p><p class="source-code">myenv = Environment(name='myenv')</p><p class="source-code">myenv.python.conda_dependencies = conda_deps</p><p class="source-code"># use an image available in public Container Registry without authentication</p><p class="source-code">myenv.docker.base_image = "mcr.microsoft.com/azureml/o16n-sample-user-base/ubuntu-miniconda"</p></li>
				<li>Now, define the inference configuration by using the <strong class="source-inline">InferenceConfig()</strong> function, which takes <strong class="source-inline">score.py</strong> and the environment variable as the arguments upon being called:<p class="source-code">from azureml.core.model import InferenceConfig</p><p class="source-code">inf_config = InferenceConfig(entry_script='score.py', environment=myenv)</p></li>
				<li>Now we<a id="_idIndexMarker495"/> are all set to deploy the<a id="_idIndexMarker496"/> ML or web service on Azure Kubernetes Service (auto-scaling cluster). In order to do so, we will need to create an AKS cluster and attach it to the Azure ML workspace. Choose a name for your cluster and check if it exists using the <strong class="source-inline">ComputeTarget()</strong> function. If not, a cluster will be created or provisioned using the <strong class="source-inline">ComputeTarget.create()</strong> function. It takes a workspace object, <strong class="source-inline">ws</strong>; a service name; and a provisioning config to create the cluster. We use the default parameters for the provisioning config to create a default cluster:<p class="source-code">%%time</p><p class="source-code">from azureml.core.compute import ComputeTarget</p><p class="source-code">from azureml.core.compute_target import ComputeTargetException</p><p class="source-code">from azureml.core.compute import AksCompute, ComputeTarget</p><p class="source-code"># Choose a name for your AKS cluster</p><p class="source-code">aks_name = 'port-aks' </p><p class="source-code"># Verify that cluster does not exist already</p><p class="source-code">try:</p><p class="source-code">    aks_target = <strong class="bold">ComputeTarget</strong>(workspace=ws, name=aks_name)</p><p class="source-code">    print('Found existing cluster, use it.')</p><p class="source-code">except ComputeTargetException:</p><p class="source-code">    # Use the default configuration (can also provide parameters to customize)</p><p class="source-code">    prov_config = AksCompute.provisioning_configuration()</p><p class="source-code">    # Create the cluster</p><p class="source-code">    aks_target = <strong class="bold">ComputeTarget.create</strong>(workspace = ws, </p><p class="source-code">                                    name = aks_name, </p><p class="source-code">provisioning_configuration = prov_config)</p><p class="source-code">if aks_target.get_status() != "Succeeded":</p><p class="source-code">aks_target.wait_for_completion(show_output=True)</p><p>After <a id="_idIndexMarker497"/>creating a cluster, you <a id="_idIndexMarker498"/>will get the following message:</p><p class="source-code">Creating.................................................</p><p class="source-code">........................</p><p class="source-code">SucceededProvisioning operation finished, operation </p><p class="source-code">"Succeeded"</p><p>Congrats, you have successfully created a cluster!</p><p class="callout-heading">Note</p><p class="callout">If a cluster with the same AKS cluster name ( <strong class="source-inline">aks_name</strong> = <strong class="source-inline">port-aks</strong>) already exists, a new cluster will not be created. Rather, the existing cluster (named <strong class="source-inline">port-aks</strong> here) will be attached to the workspace for further deployments.</p></li>
				<li>Next, we proceed to the critical task of deploying the ML service in the Kubernetes cluster. In order to deploy, we need some prerequisites, such as mounting the models to deploy. We mount the models using the <strong class="source-inline">Model()</strong> function to load the <strong class="source-inline">scaler</strong> and <strong class="source-inline">model</strong> files from the workspace and get them ready<a id="_idIndexMarker499"/> for deployment, as<a id="_idIndexMarker500"/> shown in the following code:<p class="source-code">from azureml.core.webservice import Webservice, </p><p class="source-code">AksWebservice</p><p class="source-code"># Set the web service configuration (using default here)</p><p class="source-code">aks_config = AksWebservice.deploy_configuration()</p><p class="source-code">%%time</p><p class="source-code">from azureml.core.webservice import Webservice</p><p class="source-code">from azureml.core.model import InferenceConfig</p><p class="source-code">from azureml.core.environment import Environment</p><p class="source-code">from azureml.core import Workspace</p><p class="source-code">from azureml.core.model import Model</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">model1 = <strong class="bold">Model</strong>(ws, 'support-vector-classifier')</p><p class="source-code">model2 = <strong class="bold">Model</strong>(ws, 'scaler')</p></li>
				<li>Now we are all set to deploy the service on AKS. We deploy the service with the help of the <strong class="source-inline">Model.deploy()</strong> function from the Azure ML SDK, which takes the workspace object, <strong class="source-inline">ws</strong>; <strong class="source-inline">service_name</strong>; <strong class="source-inline">models</strong>; <strong class="source-inline">inference_config</strong>; <strong class="source-inline">deployment_config</strong>; and <strong class="source-inline">deployment_target</strong> as arguments upon being called:<p class="source-code">%%time</p><p class="source-code">aks_service_name ='weatherpred-aks'</p><p class="source-code">aks_service = <strong class="bold">Model.deploy</strong> (workspace=ws,</p><p class="source-code">                           name=aks_service_name,</p><p class="source-code">                           models=[model1, model2],</p><p class="source-code">                           inference_config=inf_config,</p><p class="source-code">                           deployment_config=aks_config,</p><p class="source-code">                           deployment_target=aks_target)</p><p class="source-code">aks_service.wait_for_deployment(show_output = True)</p><p class="source-code">print(aks_service.state)</p><p>Deploying <a id="_idIndexMarker501"/>the service will take<a id="_idIndexMarker502"/> approximately around 10 mins. After deploying the ML service, you will get a message like the following: </p><p class="source-code">Running........................ Succeeded AKS service creation operation finished, operation "Succeeded"</p><p>Congratulations! Now you have deployed an ML service on AKS. Let's test it using the Azure ML SDK.</p></li>
				<li>We use the <strong class="source-inline">service.run()</strong> function to pass data to the service and get the predictions, as follows:<p class="source-code">import json</p><p class="source-code">test_sample = json.dumps({'data': [[34.927778, 0.24, 7.3899, 83, 16.1000, 1016.51, 1]]})</p><p class="source-code">test_sample = bytes(test_sample,encoding = 'utf8')</p><p class="source-code">prediction = service.run(input_data=test_sample)</p></li>
				<li>The <a id="_idIndexMarker503"/>deployed service is a REST <a id="_idIndexMarker504"/>API web service that can be accessed with an HTTP request as follows: <p class="source-code">import requests</p><p class="source-code">headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}</p><p class="source-code">if service.auth_enabled:</p><p class="source-code">    headers['Authorization'] = 'Bearer '+ service.get_keys()[0]</p><p class="source-code">elif service.token_auth_enabled:</p><p class="source-code">    headers['Authorization'] = 'Bearer '+ service.get_token()[0]</p><p class="source-code">scoring_uri = service.scoring_uri</p><p class="source-code">print(scoring_uri)</p><p class="source-code">response = requests.post(scoring_uri, data=test_sample, headers=headers)</p><p class="source-code">print(response.status_code)</p><p class="source-code">print(response.elapsed)</p><p class="source-code">print(response.json())</p><p>When a <strong class="source-inline">POST</strong> request is made by passing input data, the service returns the model <a id="_idIndexMarker505"/>prediction in the form of <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>. When <a id="_idIndexMarker506"/>you get such a prediction, your service is working and is robust to serve production needs. The service scales from <strong class="source-inline">0</strong> to the needed number of container replicas based on the user's request traffic. </p></li>
			</ol>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor140"/>Deploying the service using MLflow </h2>
			<p>Lastly, let's do <a id="_idIndexMarker507"/>the deployment of an ML service on the deployment <a id="_idIndexMarker508"/>target (ACI) using MLflow to get hands-on experience with an open source framework. To get started, go to the Git repository cloned on Azure DevOps previously (in <a href="B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, <em class="italic">Code Meets Data</em>), access the folder named <strong class="source-inline">06_ModelDeployment</strong>, and follow the implementation steps in the <strong class="source-inline">02_Deploy_model_MLflow.ipynb</strong> notebook. Before implementing, it is recommended to read this documentation to understand the concepts behind the <strong class="source-inline">mlflow.azureml</strong> SDK: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models</a>.</p>
			<ol>
				<li value="1">We start by importing the required packages and check for the version of the Azure ML SDK, as shown in the following code block:<p class="source-code">import numpy as np</p><p class="source-code">import mlflow.azureml </p><p class="source-code">import azureml.core</p><p class="source-code"># display the core SDK version number</p><p class="source-code">print("Azure ML SDK Version: ", azureml.core.VERSION)</p></li>
				<li>Next, using the <strong class="source-inline">workspace</strong> function from the Azure ML SDK, we connect to the ML workspace and set the tracking URI for the workspace using <strong class="source-inline">set_tracking_uri</strong>:<p class="source-code">from azureml.core import Workspace</p><p class="source-code">from azureml.core.model import Model</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">print(ws.name, ws.resource_group, ws.location, sep = '\n')</p><p class="source-code">mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())</p></li>
				<li>Now go to<a id="_idIndexMarker509"/> the workspace and fetch the path to the <strong class="source-inline">mlflow</strong> model<a id="_idIndexMarker510"/> from the <strong class="source-inline">models</strong> or <strong class="source-inline">experiments</strong> section and set the path:<p class="source-code">from azureml.core.webservice import AciWebservice, Webservice</p><p class="source-code"># Set the model path to the model folder created by your run </p><p class="source-code">model_path = "model path"</p></li>
				<li>Now we are all set to deploy to the ACI using <strong class="source-inline">mlflow</strong> and the <strong class="source-inline">azureml</strong> SDK. Configure the ACI deployment target using the <strong class="source-inline">deploy_configuration</strong> function and deploy to the ACI using the <strong class="source-inline">mlflow.azureml.deploy</strong> function. The <strong class="source-inline">deploy</strong> function takes <strong class="source-inline">model_uri</strong>, <strong class="source-inline">workspace</strong>, <strong class="source-inline">model_name</strong>, <strong class="source-inline">service_name</strong>, <strong class="source-inline">deployment_config</strong>, and custom tags as arguments upon being called:<p class="source-code"># Configure </p><p class="source-code">aci_config = AciWebservice.deploy_configuration</p><p class="source-code">(cpu_cores=1, </p><p class="source-code">memory_gb=1, </p><p class="source-code">tags={'method' : 'mlflow'}, </p><p class="source-code">description='weather pred model',</p><p class="source-code">location='eastus2')</p><p class="source-code"># Deploy on ACI</p><p class="source-code">(webservice,model) = mlflow.azureml.deploy(model_uri=</p><p class="source-code">'runs:/{}/{}'.format(run.id, model_path), workspace=ws, </p><p class="source-code">model_name='svc-mlflow', service_name='port-weather-pred', deployment_config=aci_config, tags=None, mlflow_home=None, synchronous=True)</p><p class="source-code">webservice.wait_for_deployment(show_output=True)</p></li>
			</ol>
			<p>You will get a <a id="_idIndexMarker511"/>deployment succeeded message upon successful <a id="_idIndexMarker512"/>deployment. For more clarity on MLflow deployment, follow these examples: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models</a>.</p>
			<p>Congratulations! You have deployed ML models on diverse deployment targets such as ACI and AKS using <strong class="source-inline">azureml</strong> and <strong class="source-inline">mlflow</strong>. </p>
			<p>Next, we will focus on bringing the full capabilities of MLOps to the table using continuous integration and continuous deployment to have a robust and dynamically developing system in production.</p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor141"/>Understanding the need for continuous integration and continuous deployment</h1>
			<p><strong class="bold">Continuous integration</strong> (<strong class="bold">CI</strong>) and <strong class="bold">continuous deployment</strong> (<strong class="bold">CD</strong>) enable continuous delivery to the <a id="_idIndexMarker513"/>ML service. The goal is to maintain and version the source code <a id="_idIndexMarker514"/>used for model training, enable triggers to perform necessary jobs in parallel, build artifacts, and release them for deployment to the ML service. Several cloud vendors enable DevOps services that can be used for monitoring ML services, ML models in production, and orchestration with other services in the cloud.</p>
			<p>Using CI and CD, we <a id="_idIndexMarker515"/>can enable continuous learning, which is critical for the success of an ML system. Without continuous learning, an ML system is destined to end up as a failed <strong class="bold">PoC</strong> (<strong class="bold">Proof of Concept</strong>). We will delve into the concepts of CI/CD and implement hands-on CI and CD pipelines to see MLOps in play in the next chapter.</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor142"/>Summary</h1>
			<p>In this chapter, we have learned the key principles of deploying ML models in production. We explored the various deployment methods and targets and their needs. For a comprehensive understanding and hands-on experience, we implemented the deployment to learn how ML models are deployed on a diverse range of deployment targets such as virtual machines, containers, and in an auto-scaling cluster. With this, you are ready to handle any type of deployment challenge that comes your way. </p>
			<p>In the next chapter, we will delve into the secrets to building, deploying, and maintaining robust ML services enabled by CI and CD. This will enable the potential of MLOps! Let's delve into it.</p>
		</div>
	</body></html>