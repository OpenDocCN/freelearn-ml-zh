<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer493">
    <h1 class="chapterNumber">10</h1>
    <h1 class="chapterTitle" id="_idParaDest-233">Evaluating Model Performance</h1>
    <p class="normal">When only the wealthy could afford education, tests and exams were not used to evaluate students. Instead, tests evaluated the teachers for parents who wanted to know whether their children learned enough to justify the instructors’ wages. Obviously, this is different today. Now, such evaluations are used to distinguish between high-achieving and low-achieving students, filtering them into careers and other opportunities.</p>
    <p class="normal">Given the significance of this process, a great deal of effort is invested in developing accurate student assessments. Fair assessments have a large number of questions that cover a wide breadth of topics and reward true knowledge over lucky guesses. A good assessment also requires students to think about problems they have never faced before. Correct responses, therefore, reflect an ability to generalize knowledge more broadly.</p>
    <p class="normal">The process of evaluating machine learning algorithms is very similar to the process of evaluating students. Since algorithms have varying strengths and weaknesses, tests should distinguish between learners. It is also important to understand how a learner will perform on future data.</p>
    <p class="normal">This chapter provides the information needed to assess machine learners, such as:</p>
    <ul>
      <li class="bulletList">The reasons why predictive accuracy is not sufficient to measure performance, and the performance measures you might use instead</li>
      <li class="bulletList">Methods to ensure that the performance measures reasonably reflect a model’s ability to predict or forecast unseen cases</li>
      <li class="bulletList">How to use R to apply these more useful measures and methods to the predictive models covered in previous chapters</li>
    </ul>
    <p class="normal">Just as the best way to learn a topic is to attempt to teach it to someone else, the process of teaching and evaluating machine learners will provide you with greater insight into the methods you’ve learned so far.</p>
    <h1 class="heading-1" id="_idParaDest-234">Measuring performance for classification</h1>
    <p class="normal">In the previous <a id="_idIndexMarker1106"/>chapters, we measured classifier accuracy by dividing the number of correct predictions by the total number of predictions. This finds the proportion of cases in which the learner is correct, and the proportion of incorrect cases follows directly. For example, suppose that a classifier correctly predicted whether newborn babies were a carrier of a treatable but potentially fatal genetic defect in 99,990 out of 100,000 cases. This would imply an accuracy of 99.99 percent and an error rate of only 0.01 percent.</p>
    <p class="normal">At first glance, this appears to be an extremely valuable classifier. However, it would be wise to collect additional information before trusting a child’s life to the test. What if the genetic defect is found in only 10 out of every 100,000 babies? A test that invariably predicts no defect will be correct for 99.99 percent of all cases, but incorrect for 100 percent of the cases that matter most. In other words, even though the classifier is extremely accurate, it is not very useful for preventing treatable birth defects.</p>
    <div class="packt_tip">
      <p class="normal">This is one consequence of the <strong class="keyWord">class imbalance problem</strong>, which refers to the trouble associated with data having a large majority of records belonging to a single class.</p>
    </div>
    <p class="normal">Though there are many ways to measure a classifier’s performance, the best measure is always that which captures whether the classifier is successful at its intended purpose. It is crucial to define performance measures in terms of utility rather than raw accuracy. To this end, we will explore a variety of alternative performance measures derived from the confusion matrix. Before we get started, however, we need to consider how to prepare a classifier for evaluation.</p>
    <h2 class="heading-2" id="_idParaDest-235">Understanding a classifier’s predictions</h2>
    <p class="normal">The goal of <a id="_idIndexMarker1107"/>evaluating a classification model is to better understand how its performance will extrapolate to future cases. Since it is usually infeasible to test an unproven model in a live environment, we typically simulate future conditions by asking the model to classify cases in a dataset made of cases that resemble what it will be asked to do in the future. By observing the learner’s responses to this examination, we can learn about its strengths and weaknesses.</p>
    <p class="normal">Though we’ve <a id="_idIndexMarker1108"/>evaluated classifiers in prior chapters, it’s worth reflecting on the types of data at our disposal:</p>
    <ul>
      <li class="bulletList">Actual class values</li>
      <li class="bulletList">Predicted class values</li>
      <li class="bulletList">The estimated probability of the prediction</li>
    </ul>
    <p class="normal">The actual and predicted class values may be self-evident, but they are the key to the evaluation. Just like a teacher uses an answer key—a list of correct answers—to assess the student’s answers, we need to know the correct answer for a machine learner’s predictions. The goal is to maintain two vectors of data: one holding the correct or actual class values, and the other holding the predicted class values. Both vectors must have the same number of values stored in the same order. The predicted and actual values may be stored as separate R vectors or as columns in a single R data frame.</p>
    <p class="normal">Obtaining this data is easy. The actual class values come directly from the target in the test dataset. Predicted class values are obtained from the classifier built upon the training data, which is then applied to the test data. For most machine learning packages, this involves applying the <code class="inlineCode">predict()</code> function to a model object and a data frame of test data, such as <code class="inlineCode">predictions &lt;- predict(model, test_data)</code>.</p>
    <p class="normal">Until now, we have only examined classification predictions using these two vectors of data, but most models can supply another piece of useful information. Even though the classifier makes a single prediction about each example, it may be more confident about some decisions than others. </p>
    <p class="normal">For instance, a classifier may be 99 percent certain that an SMS with the words “free” and “ringtones” is spam, but only 51 percent certain that an SMS with the word “tonight” is spam. In both cases, the classifier classifies the message as spam, but it is far more certain about one decision than the other.</p>
    <figure class="mediaobject"><img alt="A picture containing diagram  Description automatically generated" src="../Images/B17290_10_01.png"/></figure>
    <p class="packt_figref">Figure 10.1: Learners may differ in their prediction confidence even when trained on the same data</p>
    <p class="normal">Studying <a id="_idIndexMarker1109"/>these internal prediction probabilities provides useful data for evaluating a model’s performance. If two models make the same number of mistakes, but one is more able to accurately assess its uncertainty, then it is a smarter model. It’s ideal to find a learner that is extremely confident when making a correct prediction, but timid in the face of doubt. The balance between confidence and caution is a key part of model evaluation.</p>
    <p class="normal">The function call to obtain the internal prediction probabilities varies across R packages. For most classifiers, the <code class="inlineCode">predict()</code> function allows an additional parameter to specify the desired type of prediction. To obtain a single predicted class, such as spam or ham, you typically set the <code class="inlineCode">type = "class"</code> parameter. To obtain the prediction probability, the <code class="inlineCode">type</code> parameter should be set to one of <code class="inlineCode">"prob"</code>, <code class="inlineCode">"posterior"</code>, <code class="inlineCode">"raw"</code>, or <code class="inlineCode">"probability"</code>, depending on the classifier used.</p>
    <div class="packt_tip">
      <p class="normal">All classifiers presented in this book can provide prediction probabilities. The correct setting for the <code class="inlineCode">type</code> parameter is included in the syntax box introducing each model.</p>
    </div>
    <p class="normal">For example, to output the predicted probabilities for the C5.0 classifier built in <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>, use the <code class="inlineCode">predict()</code> function with <code class="inlineCode">type = "prob"</code> as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> predicted_prob <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>credit_model<span class="hljs-punctuation">,</span> credit_test<span class="hljs-punctuation">,</span> type <span class="hljs-operator">=</span> <span class="hljs-string">"prob"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">To output the Naive Bayes predicted probabilities for the SMS spam classification model developed in <em class="chapterRef">Chapter 4</em>, <em class="italic">Probabilistic Learning – Classification Using Naive Bayes</em>, use <code class="inlineCode">predict()</code> with <code class="inlineCode">type = "raw"</code> as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sms_test_prob <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>sms_classifier<span class="hljs-punctuation">,</span> sms_test<span class="hljs-punctuation">,</span> type <span class="hljs-operator">=</span> <span class="hljs-string">"raw"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">In <a id="_idIndexMarker1110"/>most cases, the <code class="inlineCode">predict()</code> function returns a probability for each category of the outcome. For example, in the case of a two-outcome model like the SMS classifier, the predicted probabilities might be stored in a matrix or data frame, as shown here:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>sms_test_prob<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">              ham         spam
[1,] 9.999995e-01 4.565938e-07
[2,] 9.999995e-01 4.540489e-07
[3,] 9.998418e-01 1.582360e-04
[4,] 9.999578e-01 4.223125e-05
[5,] 4.816137e-10 1.000000e+00
[6,] 9.997970e-01 2.030033e-04
</code></pre>
    <p class="normal">Each line in this output shows the classifier’s predicted probability of spam and ham. According to probability rules, the sum of the probabilities across each row is 1 because these are mutually exclusive and exhaustive outcomes. For convenience, during the evaluation process, it can be helpful to construct a data frame collecting the predicted class, the actual class, and the predicted probability of the class level (or levels) of interest.</p>
    <p class="normal">The <code class="inlineCode">sms_results.csv</code> file available in the GitHub repository for this chapter is an example of a data frame in exactly this format and is built from the predictions of the SMS classifier built in <em class="chapterRef">Chapter 4</em>, <em class="italic">Probabilistic Learning – Classification Using Naive Bayes</em>. The steps required to construct this evaluation dataset have been omitted for brevity, so to follow along with the example here, simply download the file and load it into a data frame using the following command:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sms_results <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"sms_results.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting <code class="inlineCode">sms_results</code> data frame is simple. It contains four vectors of 1,390 values. One column contains values indicating the actual type of SMS message (spam or ham), another indicates the Naive Bayes model’s predicted message type, and the third and fourth columns indicate the probability that the message was spam or ham, respectively:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>sms_results<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  actual_type predict_type prob_spam prob_ham
1         ham          ham   0.00000  1.00000
2         ham          ham   0.00000  1.00000
3         ham          ham   0.00016  0.99984
4         ham          ham   0.00004  0.99996
5        spam         spam   1.00000  0.00000
6         ham          ham   0.00020  0.99980
</code></pre>
    <p class="normal">For these <a id="_idIndexMarker1111"/>six test cases, the predicted and actual SMS message types agree; the model predicted their statuses correctly. Furthermore, the prediction probabilities suggest that the model was extremely confident about these predictions because they all fall close to or are exactly 0 or 1.</p>
    <p class="normal">What happens when the predicted and actual values are further from 0 and 1? Using the <code class="inlineCode">subset()</code> function, we can identify a few of these records. The following output shows test cases where the model estimated the probability of spam as being between 40 and 60 percent:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>subset<span class="hljs-punctuation">(</span>sms_results<span class="hljs-punctuation">,</span> prob_spam <span class="hljs-operator">&gt;</span> <span class="hljs-number">0.40</span> <span class="hljs-operator">&amp;</span> prob_spam <span class="hljs-operator">&lt;</span> <span class="hljs-number">0.60</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">     actual_type predict_type prob_spam prob_ham
377         spam          ham   0.47536  0.52464
717          ham         spam   0.56188  0.43812
1311         ham         spam   0.57917  0.42083
</code></pre>
    <p class="normal">By the model’s own estimation, these were cases in which a correct prediction was virtually a coin flip. Yet all three predictions were wrong—an unlucky result. Let’s look at a few more cases where the model was wrong:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> head<span class="hljs-punctuation">(</span>subset<span class="hljs-punctuation">(</span>sms_results<span class="hljs-punctuation">,</span> actual_type <span class="hljs-operator">!=</span> predict_type<span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    actual_type predict_type prob_spam prob_ham
53         spam          ham   0.00071  0.99929
59         spam          ham   0.00156  0.99844
73         spam          ham   0.01708  0.98292
76         spam          ham   0.00851  0.99149
184        spam          ham   0.01243  0.98757
332        spam          ham   0.00003  0.99997
</code></pre>
    <p class="normal">These cases illustrate the important fact that a model can be extremely confident and yet it can still <a id="_idIndexMarker1112"/>be extremely wrong. All six of these test cases were spam messages that the classifier believed to have no less than a 98 percent chance of being ham.</p>
    <p class="normal">Despite such mistakes, is the model still useful? We can answer this question by applying various error metrics to this evaluation data. In fact, many such metrics are based on a tool we’ve already used extensively in previous chapters.</p>
    <h2 class="heading-2" id="_idParaDest-236">A closer look at confusion matrices</h2>
    <p class="normal">A <strong class="keyWord">confusion matrix</strong> is <a id="_idIndexMarker1113"/>a table that categorizes predictions according to whether they match the actual value. One of the table’s dimensions indicates the possible categories of predicted values, while the other dimension indicates the same thing for actual values. Although we have mostly worked with 2x2 confusion matrices so far, a matrix can be created for models that predict any number of class values. The following figure depicts the familiar confusion matrix for a two-class binary model, as well as the 3x3 confusion matrix for a three-class model.</p>
    <p class="normal">When the predicted value is the same as the actual value, this is a correct classification. Correct predictions fall on the diagonal in the confusion matrix (denoted by <strong class="screenText">O</strong>). The off-diagonal matrix cells (denoted by <strong class="screenText">X</strong>) indicate the cases where the predicted value differs from the actual value. These are incorrect predictions. Performance measures for classification models are based on the counts of predictions falling on and off the diagonal in these tables:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_10_02.png"/></figure>
    <p class="packt_figref">Figure 10.2: Confusion matrices count cases where the predicted class agrees or disagrees with the actual value</p>
    <p class="normal">The most<a id="_idIndexMarker1114"/> common performance measures consider the model’s ability to discern one class versus all others. The class of interest is known as the <strong class="keyWord">positive</strong> class, while all others are known as <strong class="keyWord">negative</strong>.</p>
    <div class="packt_tip">
      <p class="normal">The use of the terms positive and negative is not intended to imply any value judgment (that is, good versus bad), nor does it necessarily suggest that the outcome is present or absent (such as there being a birth defect versus there not being one). The choice of the positive outcome can even be arbitrary, as in cases where a model is predicting categories such as sunny versus rainy, or dog versus cat.</p>
    </div>
    <p class="normal">The relationship between positive class and negative class predictions can be depicted as a 2x2 confusion matrix that tabulates whether predictions fall into one of four categories:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">True positive</strong> (<strong class="keyWord">TP</strong>): Correctly classified as the class of interest</li>
      <li class="bulletList"><strong class="keyWord">True negative</strong> (<strong class="keyWord">TN</strong>): Correctly classified as not the class of interest</li>
      <li class="bulletList"><strong class="keyWord">False positive</strong> (<strong class="keyWord">FP</strong>): Incorrectly classified as the class of interest</li>
      <li class="bulletList"><strong class="keyWord">False negative</strong> (<strong class="keyWord">FN</strong>): Incorrectly classified as not the class of interest</li>
    </ul>
    <p class="normal">For the spam classifier, the positive class is spam, as this is the outcome we hope to detect. We then can imagine the confusion matrix as shown in <em class="italic">Figure 10.3</em>:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_10_03.png"/></figure>
    <p class="packt_figref">Figure 10.3: Distinguishing between positive and negative classes adds detail to the confusion matrix</p>
    <p class="normal">The confusion matrix presented in this way is the basis for many of the most important measures of model performance. In the next section, we’ll use this matrix to better understand exactly what is meant by accuracy.</p>
    <h2 class="heading-2" id="_idParaDest-237">Using confusion matrices to measure performance</h2>
    <p class="normal">With<a id="_idIndexMarker1115"/> the 2x2 confusion matrix, we can formalize our definition<a id="_idIndexMarker1116"/> of <strong class="keyWord">prediction accuracy</strong> (sometimes called the <strong class="keyWord">success rate</strong>) as:</p>
    <p class="center"><img alt="" src="../Images/B17290_10_001.png"/></p>
    <p class="normal">In this formula, the terms <em class="italic">TP</em>, <em class="italic">TN</em>, <em class="italic">FP</em>, and <em class="italic">FN</em> refer to the number of times the model’s predictions fell into each of these categories. The accuracy is therefore a proportion that represents the number of true positives and true negatives divided by the total number of predictions.</p>
    <p class="normal">The <strong class="keyWord">error rate</strong>, or <a id="_idIndexMarker1117"/>the proportion of incorrectly classified examples, is specified as:</p>
    <p class="center"><img alt="" src="../Images/B17290_10_002.png"/></p>
    <p class="normal">Notice that the error rate can be calculated as 1 minus the accuracy. Intuitively, this makes sense; a model that is correct 95 percent of the time is incorrect five percent of the time.</p>
    <p class="normal">An easy way to tabulate a classifier’s predictions into a confusion matrix is to use R’s <code class="inlineCode">table()</code> function. The command for creating a confusion matrix for the SMS data is shown as follows. The counts in this table could then be used to calculate accuracy and other statistics:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">,</span> sms_results<span class="hljs-operator">$</span>predict_type<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">      
        ham spam
  ham  1203    4
  spam   31  152
</code></pre>
    <p class="normal">If you would like to create a confusion matrix with more informative output, the <code class="inlineCode">CrossTable()</code> function in the <code class="inlineCode">gmodels</code> package offers a customizable solution. If you recall, we first used this function in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>. If you didn’t install the package at that time, you will need to do so using the <code class="inlineCode">install.packages("gmodels")</code> command.</p>
    <p class="normal">By <a id="_idIndexMarker1118"/>default, the <code class="inlineCode">CrossTable()</code> output includes proportions in each cell that indicate the cell count as a percentage of the table’s row, column, and overall total counts. The output also includes row and column totals. As shown in the following code, the syntax is similar to the <code class="inlineCode">table()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>gmodels<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> CrossTable<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">,</span> sms_results<span class="hljs-operator">$</span>predict_type<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The result is a confusion matrix with a wealth of additional detail:</p>
    <pre class="programlisting con"><code class="hljs-con">  Cell Contents
|-------------------------|
|                       N |
| Chi-square contribution |
|           N / Row Total |
|           N / Col Total |
|         N / Table Total |
|-------------------------|
 
Total Observations in Table:  1390 
 
                        | sms_results$predict_type 
sms_results$actual_type |       ham |      spam | Row Total | 
------------------------|-----------|-----------|-----------|
                    ham |      1203 |         4 |      1207 | 
                        |    16.128 |   127.580 |           | 
                        |     0.997 |     0.003 |     0.868 | 
                        |     0.975 |     0.026 |           | 
                        |     0.865 |     0.003 |           | 
------------------------|-----------|-----------|-----------|
                   spam |        31 |       152 |       183 | 
                        |   106.377 |   841.470 |           | 
                        |     0.169 |     0.831 |     0.132 | 
                        |     0.025 |     0.974 |           | 
                        |     0.022 |     0.109 |           | 
------------------------|-----------|-----------|-----------|
           Column Total |      1234 |       156 |      1390 | 
                        |     0.888 |     0.112 |           | 
------------------------|-----------|-----------|-----------|
</code></pre>
    <p class="normal">We’ve used <code class="inlineCode">CrossTable()</code> in several previous chapters, so by now, you should be familiar with its output. If you ever forget how to interpret the output, simply refer to the key (labeled <code class="inlineCode">Cell Contents</code>), which provides the definition of each number in the table cells.</p>
    <p class="normal">We can <a id="_idIndexMarker1119"/>use the confusion matrix to obtain the accuracy and error rate. Since accuracy is (TP + TN) / (TP + TN + FP + FN), we can calculate it as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-punctuation">(</span><span class="hljs-number">152</span> <span class="hljs-operator">+</span> <span class="hljs-number">1203</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span> <span class="hljs-punctuation">(</span><span class="hljs-number">152</span> <span class="hljs-operator">+</span> <span class="hljs-number">1203</span> <span class="hljs-operator">+</span> <span class="hljs-number">4</span> <span class="hljs-operator">+</span> <span class="hljs-number">31</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.9748201
</code></pre>
    <p class="normal">We can also calculate the error rate (FP + FN) / (TP + TN + FP + FN) as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-punctuation">(</span><span class="hljs-number">4</span> <span class="hljs-operator">+</span> <span class="hljs-number">31</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span> <span class="hljs-punctuation">(</span><span class="hljs-number">152</span> <span class="hljs-operator">+</span> <span class="hljs-number">1203</span> <span class="hljs-operator">+</span> <span class="hljs-number">4</span> <span class="hljs-operator">+</span> <span class="hljs-number">31</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.02517986
</code></pre>
    <p class="normal">This is the same as 1 minus accuracy:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-number">1</span> – <span class="hljs-number">0.9748201</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.0251799
</code></pre>
    <p class="normal">Although these calculations may seem simple, it is important to practice thinking about how the components of the confusion matrix relate to one another. In the next section, you will see how these same pieces can be combined in different ways to create a variety of additional performance measures.</p>
    <h2 class="heading-2" id="_idParaDest-238">Beyond accuracy – other measures of performance</h2>
    <p class="normal">Countless performance<a id="_idIndexMarker1120"/> measures have been developed and used for specific purposes in disciplines as diverse as medicine, information retrieval, marketing, and signal detection theory, among others. To cover all of them could fill hundreds of pages, which makes a comprehensive description infeasible here. Instead, we’ll consider only some of the most useful and most cited measures in machine learning literature.</p>
    <p class="normal">The <code class="inlineCode">caret</code> package by Max Kuhn includes functions for computing many such performance measures. This package provides tools for preparing, training, evaluating, and visualizing machine learning models and data; the name “caret” is a simplification of “classification and regression training.” Because it is also valuable for tuning models, in addition to its use here, we will also employ the <code class="inlineCode">caret</code> package extensively in <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>. Before proceeding, you will need to install the package using the <code class="inlineCode">install.packages("caret")</code> command.</p>
    <div class="note">
      <p class="normal">For more information on <code class="inlineCode">caret</code>, refer to <em class="italic">Building Predictive Models in R Using the caret Package, Kuhn, M, Journal of Statistical Software, 2008, Vol. 28</em> or the package’s very informative documentation pages at <a href="http://topepo.github.io/caret/index.html"><span class="url">http://topepo.github.io/caret/index.html</span></a></p>
    </div>
    <p class="normal">The <code class="inlineCode">caret</code> package <a id="_idIndexMarker1121"/>adds yet another function for creating a confusion matrix. As shown in the following command, the syntax is similar to <code class="inlineCode">table()</code>, but with a minor difference. Because <code class="inlineCode">caret</code> computes the measures of model performance that reflect the ability to classify the positive class, a <code class="inlineCode">positive</code> parameter should be specified. In this case, since the SMS classifier is intended to detect spam, we will set <code class="inlineCode">positive = "spam"</code> as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>caret<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> confusionMatrix<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>predict_type<span class="hljs-punctuation">,</span>
    sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">,</span> positive <span class="hljs-operator">=</span> <span class="hljs-string">"spam"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This results in the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">Confusion Matrix and Statistics
          Reference
Prediction  ham spam
      ham  1203   31
      spam    4  152
                                          
               Accuracy : 0.9748          
                 95% CI : (0.9652, 0.9824)
    No Information Rate : 0.8683          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.8825          
                                          
 Mcnemar’s Test P-Value : 1.109e-05       
                                          
            Sensitivity : 0.8306          
            Specificity : 0.9967          
         Pos Pred Value : 0.9744          
         Neg Pred Value : 0.9749          
             Prevalence : 0.1317          
         Detection Rate : 0.1094          
   Detection Prevalence : 0.1122          
      Balanced Accuracy : 0.9136          
                                          
       ‘Positive’ Class : spam    
</code></pre>
    <p class="normal">At the top of the <a id="_idIndexMarker1122"/>output is a confusion matrix much like the one produced by the <code class="inlineCode">table()</code> function, but transposed. The output also includes a set of performance measures. Some of these, like accuracy, are familiar, while many others are new. Let’s look at some of the most important metrics.</p>
    <h3 class="heading-3" id="_idParaDest-239">The kappa statistic</h3>
    <p class="normal">The <strong class="keyWord">kappa statistic</strong> (labeled <code class="inlineCode">Kappa</code> in the previous output) adjusts <a id="_idIndexMarker1123"/>the accuracy by accounting <a id="_idIndexMarker1124"/>for the possibility of a correct prediction by chance alone. This is especially important for datasets with a severe class imbalance because a classifier can obtain high accuracy simply by always guessing the most frequent class. The kappa statistic will only reward the classifier if it is correct more often than this simplistic strategy.</p>
    <div class="note">
      <p class="normal">There is more than one way<a id="_idIndexMarker1125"/> to define the kappa statistic. The most common method, described here, uses <strong class="keyWord">Cohen’s kappa coefficient</strong>, as described in the paper <em class="italic">A coefficient of agreement for nominal scales, Cohen, J, Education and Psychological Measurement, 1960, Vol. 20, pp. 37-46</em>.</p>
    </div>
    <p class="normal">Kappa values typically range from 0 to a maximum of 1, with higher values reflecting stronger agreement between the model’s predictions and the true values. It is possible to observe values less than 0 if the predictions are consistently in the wrong direction—that is, the predictions disagree with the actual values or are wrong more often than would be expected by random guessing. This rarely occurs for machine learning models and usually reflects a coding issue, which can be fixed by simply reversing the predictions.</p>
    <p class="normal">Depending on how a model is to be used, the interpretation of the kappa statistic might vary. One common interpretation is shown as follows:</p>
    <ul>
      <li class="bulletList">Poor agreement = less than 0.2</li>
      <li class="bulletList">Fair agreement = 0.2 to 0.4</li>
      <li class="bulletList">Moderate agreement = 0.4 to 0.6</li>
      <li class="bulletList">Good agreement = 0.6 to 0.8</li>
      <li class="bulletList">Very good agreement = 0.8 to 1.0</li>
    </ul>
    <p class="normal">It’s important to note that these categories are subjective. While “good agreement” may be more than<a id="_idIndexMarker1126"/> adequate for predicting <a id="_idIndexMarker1127"/>someone’s favorite ice cream flavor, “very good agreement” may not suffice if your goal is to identify birth defects.</p>
    <div class="note">
      <p class="normal">For more information on the previous scale, refer to <em class="italic">The measurement of observer agreement for categorical data, Landis, JR, Koch, GG. Biometrics, 1997, Vol. 33, pp. 159-174</em>.</p>
    </div>
    <p class="normal">The following is the formula for calculating the kappa statistic. In this formula, Pr(<em class="italic">a</em>) refers to the proportion of actual agreement and Pr(<em class="italic">e</em>) refers to the expected agreement between the classifier and the true values, under the assumption that they were chosen at random:</p>
    <p class="center"><img alt="" src="../Images/B17290_10_003.png"/></p>
    <p class="normal">These proportions are easy to obtain from a confusion matrix once you know where to look. Let’s consider the confusion matrix for the SMS classification model created with the <code class="inlineCode">CrossTable()</code> function, which is repeated here for convenience:</p>
    <pre class="programlisting con"><code class="hljs-con">                       | sms_results$predict_type 
sms_results$actual_type |       ham |      spam | Row Total | 
------------------------|-----------|-----------|-----------|
                    ham |      1203 |         4 |      1207 | 
                        |    16.128 |   127.580 |           | 
                        |     0.997 |     0.003 |     0.868 | 
                        |     0.975 |     0.026 |           | 
                        |     0.865 |     0.003 |           | 
------------------------|-----------|-----------|-----------|
                   spam |        31 |       152 |       183 | 
                        |   106.377 |   841.470 |           | 
                        |     0.169 |     0.831 |     0.132 | 
                        |     0.025 |     0.974 |           | 
                        |     0.022 |     0.109 |           | 
------------------------|-----------|-----------|-----------|
           Column Total |      1234 |       156 |      1390 | 
                        |     0.888 |     0.112 |           | 
------------------------|-----------|-----------|-----------| 
</code></pre>
    <p class="normal">Remember that the bottom value in each cell indicates the proportion of all instances falling into that cell. Therefore, to calculate the observed agreement Pr(<em class="italic">a</em>), we simply add the <a id="_idIndexMarker1128"/>proportion of all instances where the <a id="_idIndexMarker1129"/>predicted type and actual SMS type agree. </p>
    <p class="normal">Thus, we can calculate Pr(<em class="italic">a</em>) as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> pr_a <span class="hljs-operator">&lt;-</span> 0.865 <span class="hljs-operator">+</span> <span class="hljs-number">0.109</span>
<span class="hljs-operator">&gt;</span> pr_a
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.974
</code></pre>
    <p class="normal">For this classifier, the observed and actual values agree 97.4 percent of the time—you will note that this is the same as the accuracy. The kappa statistic adjusts the accuracy relative to the expected agreement, Pr(<em class="italic">e</em>), which is the probability that chance alone would lead the predicted and actual values to match, under the assumption that both are selected randomly according to the observed proportions.</p>
    <p class="normal">To find these observed proportions, we can use the probability rules we learned in <em class="chapterRef">Chapter 4</em>, <em class="italic">Probabilistic Learning – Classification Using Naive Bayes</em>. Assuming two events are independent (meaning one does not affect the other), probability rules note that the probability of both occurring is equal to the product of the probabilities of each one occurring. For instance, we know that the probability of both choosing ham is:</p>
    <p class="center">Pr(<em class="italic">actual_type is ham</em>) * Pr(<em class="italic">predicted_type is ham</em>)</p>
    <p class="normal">And the probability of both choosing spam is:</p>
    <p class="center">Pr(<em class="italic">actual_type is spam</em>) * Pr(<em class="italic">predicted_type is spam</em>)</p>
    <p class="normal">The probability that the predicted or actual type is spam or ham can be obtained from the row or column totals. For instance, Pr(<em class="italic">actual_type is ham</em>) = 0.868 and Pr(<em class="italic">predicted type is ham</em>) = 0.888.</p>
    <p class="normal">Pr(<em class="italic">e</em>) can be calculated as the sum of the probabilities that the predicted and actual values agree that the message is either spam or ham. Recall that for mutually exclusive events (events that cannot happen simultaneously), the probability of either occurring is equal to the sum of their probabilities. Therefore, to obtain the final Pr(<em class="italic">e</em>), we simply add both products, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> pr_e <span class="hljs-operator">&lt;-</span> 0.868 <span class="hljs-operator">*</span> <span class="hljs-number">0.888</span> <span class="hljs-operator">+</span> <span class="hljs-number">0.132</span> <span class="hljs-operator">*</span> <span class="hljs-number">0.112</span>
<span class="hljs-operator">&gt;</span> pr_e
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.785568
</code></pre>
    <p class="normal">Since Pr(<em class="italic">e</em>) is 0.786, by chance alone, we would expect the observed and actual values to agree about 78.6 percent <a id="_idIndexMarker1130"/>of the time.</p>
    <p class="normal">This <a id="_idIndexMarker1131"/>means that we now have all the information needed to complete the kappa formula. Plugging the Pr(<em class="italic">a</em>) and Pr(<em class="italic">e</em>) values into the kappa formula, we find:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> k <span class="hljs-operator">&lt;-</span> <span class="hljs-punctuation">(</span>pr_a <span class="hljs-operator">-</span> pr_e<span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span> <span class="hljs-punctuation">(</span><span class="hljs-number">1</span> <span class="hljs-operator">-</span> pr_e<span class="hljs-punctuation">)</span> 
<span class="hljs-operator">&gt;</span> k
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.8787494
</code></pre>
    <p class="normal">The kappa is about 0.88, which agrees with the previous <code class="inlineCode">confusionMatrix()</code> output from <code class="inlineCode">caret</code> (the small difference is due to rounding). Using the suggested interpretation, we note that there is very good agreement between the classifier’s predictions and the actual values.</p>
    <p class="normal">There are a couple of R functions for calculating kappa automatically. The <code class="inlineCode">Kappa()</code> function (be sure to note the capital “K”) in<a id="_idIndexMarker1132"/> the <strong class="keyWord">Visualizing Categorical Data</strong> (<strong class="keyWord">VCD</strong>) package uses a confusion matrix of predicted and actual values. After installing the package by typing <code class="inlineCode">install.packages("vcd")</code>, the following commands can be used to obtain kappa:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>vcd<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> Kappa<span class="hljs-punctuation">(</span>table<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">,</span> sms_results<span class="hljs-operator">$</span>predict_type<span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">            value     ASE     z Pr(&gt;|z|)
Unweighted 0.8825 0.01949 45.27        0
Weighted   0.8825 0.01949 45.27        0
</code></pre>
    <p class="normal">We’re interested in the unweighted kappa. The value of 0.88 matches what we computed manually.</p>
    <div class="packt_tip">
      <p class="normal">The weighted kappa is used when there are varying degrees of agreement. For example, using a scale of cold, cool, warm, and hot, the value of warm agrees more with hot than it does with the value of cold. In the case of a two-outcome event, such as spam and ham, the weighted and unweighted kappa statistics will be identical.</p>
    </div>
    <p class="normal">The <code class="inlineCode">kappa2()</code> function in<a id="_idIndexMarker1133"/> the <strong class="keyWord">Interrater Reliability</strong> (<code class="inlineCode">irr</code>) package can be used to calculate kappa from vectors of predicted and actual values stored in a data frame. After installing the package using <code class="inlineCode">install.packages("irr")</code>, the following commands can be used to obtain kappa:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>irr<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> kappa2<span class="hljs-punctuation">(</span>sms_results<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">2</span><span class="hljs-punctuation">])</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Cohen's Kappa for 2 Raters (Weights: unweighted)
 Subjects = 1390 
   Raters = 2 
    Kappa = 0.883 
        z = 33 
  p-value = 0
</code></pre>
    <p class="normal">The <code class="inlineCode">Kappa()</code> and <code class="inlineCode">kappa2()</code> functions report the same kappa statistic, so use whichever option <a id="_idIndexMarker1134"/>you are more comfortable <a id="_idIndexMarker1135"/>with.</p>
    <div class="packt_tip">
      <p class="normal">Be careful not to use the built-in <code class="inlineCode">kappa()</code> function. It is completely unrelated to the kappa statistic reported previously!</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-240">The Matthews correlation coefficient</h3>
    <p class="normal">Although <a id="_idIndexMarker1136"/>accuracy <a id="_idIndexMarker1137"/>and kappa have been popular measures of performance for many years, a third option has quickly become a de facto standard in the field of machine learning. Like both prior metrics, the <strong class="keyWord">Matthews correlation coefficient</strong> (<strong class="keyWord">MCC</strong>) is a single statistic intended to reflect the overall performance of a classification model. Additionally, the MCC is like kappa in that it is useful even in the case that the dataset is severely imbalanced—the types of situations in which the traditional accuracy measure can be very misleading.</p>
    <p class="normal">The MCC has grown in popularity due to its ease of interpretation, as well as a growing body of evidence suggesting that it performs better in a wider variety of circumstances than kappa. Recent empirical research has indicated that the MCC may be the best single metric for describing the real-world performance of a binary classification model. Other studies have identified potential circumstances in which the kappa statistic provides a misleading or incorrect depiction of model performance. In these cases, when the MCC and kappa disagree, the MCC metric tends to provide a more reasonable assessment of the model’s true capabilities.</p>
    <div class="note">
      <p class="normal">For more information on the relative advantages of the Matthews correlation coefficient versus kappa, see <em class="italic">The Matthews correlation coefficient (MCC) is more informative than Cohen’s kappa and brier score in binary classification assessment, Chicco D, Warrens MJ, Jurman G, IEEE Access, 2021, Vol. 9, pp. 78368-78381</em>. Alternatively, refer to <em class="italic">Why Cohen’s Kappa should be avoided as performance measure in classification, Delgado R, Tibau XA, PLoS One, 2019, Vol. 14(9):e0222916</em>.</p>
    </div>
    <p class="normal">Values <a id="_idIndexMarker1138"/>of the MCC <a id="_idIndexMarker1139"/>are interpreted on the same scale as Pearson’s correlation coefficient, which was introduced in <em class="chapterRef">Chapter 6</em>, <em class="italic">Forecasting Numeric Data – Regression Methods</em>. This ranges from -1 to +1, which indicate perfectly inaccurate and perfectly accurate predictions, respectively. A value of 0 indicates a model that performs no better than random guessing. As most MCC scores fall somewhere in the range of values between 0 and 1, some subjectivity is involved in knowing what is a “good” score. Much like the scale used for Pearson correlations, one potential interpretation is as follows:</p>
    <ul>
      <li class="bulletList">Perfectly incorrect = -1.0</li>
      <li class="bulletList">Strongly incorrect = -0.5 to -1.0</li>
      <li class="bulletList">Moderately incorrect = -0.3 to -0.5</li>
      <li class="bulletList">Weakly incorrect = -0.1 to 0.3</li>
      <li class="bulletList">Randomly correct = -0.1 to 0.1</li>
      <li class="bulletList">Weakly correct = 0.1 to 0.3</li>
      <li class="bulletList">Moderately correct = 0.3 to 0.5</li>
      <li class="bulletList">Strongly correct = 0.5 to 1.0</li>
      <li class="bulletList">Perfectly correct = 1.0</li>
    </ul>
    <p class="normal">Note that the worst-performing models fall in the middle of the scale. In other words, a model on the negative side of the scale (from perfectly to weakly incorrect) still performs better than a model predicting at random. For instance, even though the accuracy of a strongly incorrect model is poor, the predictions can simply be reversed to obtain the correct result.</p>
    <div class="packt_tip">
      <p class="normal">As with all such scales, these should be used only as rough guidelines. Furthermore, the key benefit of a metric like the MCC is not to understand a model’s performance in isolation, but rather, to facilitate performance comparisons across several models.</p>
    </div>
    <p class="normal">The MCC <a id="_idIndexMarker1140"/>can be computed from the confusion matrix for a binary classifier as shown in the following formula:</p>
    <p class="center"><img alt="" src="../Images/B17290_10_004.png"/></p>
    <p class="normal">Using <a id="_idIndexMarker1141"/>the confusion matrix for the SMS spam classification model, we obtain the following values:</p>
    <ul>
      <li class="bulletList">TN = 1203</li>
      <li class="bulletList">FP = 4</li>
      <li class="bulletList">FN = 31</li>
      <li class="bulletList">TP = 152</li>
    </ul>
    <p class="normal">The MCC can then be computed manually in R as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-punctuation">(</span><span class="hljs-number">152</span> <span class="hljs-operator">*</span> <span class="hljs-number">1203</span> <span class="hljs-operator">-</span> <span class="hljs-number">4</span> <span class="hljs-operator">*</span> <span class="hljs-number">31</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span>
    <span class="hljs-built_in">sqrt</span><span class="hljs-punctuation">((</span><span class="hljs-number">152</span> <span class="hljs-operator">+</span> <span class="hljs-number">4</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">*</span> <span class="hljs-punctuation">(</span><span class="hljs-number">152</span> <span class="hljs-operator">+</span> <span class="hljs-number">31</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">*</span> <span class="hljs-punctuation">(</span><span class="hljs-number">1203</span> <span class="hljs-operator">+</span> <span class="hljs-number">4</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">*</span> <span class="hljs-punctuation">(</span><span class="hljs-number">1203</span> <span class="hljs-operator">+</span> <span class="hljs-number">31</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.8861669
</code></pre>
    <p class="normal">The <code class="inlineCode">mltools</code> package by Ben Gorman provides an <code class="inlineCode">mcc()</code> function which can perform the MCC calculation using vectors of predicted and actual values. After installing the package, the following R code produces the same result as the calculation done by hand:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>mltools<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> mcc<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">,</span> sms_results<span class="hljs-operator">$</span>predict_type<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.8861669
</code></pre>
    <p class="normal">Alternatively, for a binary classifier where the positive class is coded as 1 and the negative class is coded as 0, the MCC is identical to the Pearson correlation between the predicted and actual values. We can demonstrate this using the <code class="inlineCode">cor()</code> function in R, after using <code class="inlineCode">ifelse()</code> to convert the categorical (<code class="inlineCode">"spam"</code> or <code class="inlineCode">"ham"</code>) values into binary (<code class="inlineCode">1</code> or <code class="inlineCode">0</code>) values as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> cor<span class="hljs-punctuation">(</span>ifelse<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>actual_type <span class="hljs-operator">==</span> <span class="hljs-string">"spam"</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">),</span>
      ifelse<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>predict_type <span class="hljs-operator">==</span> <span class="hljs-string">"spam"</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.8861669
</code></pre>
    <p class="normal">The fact that such an obvious classification performance metric was hiding in plain sight, as a simple adaptation of Pearson’s correlation introduced in the late 1800s, makes it quite remarkable that the MCC has only become popular in recent decades! Biochemist Brian W. Matthews <a id="_idIndexMarker1142"/>is responsible for popularizing this metric in 1975 for use in two-outcome classification problems and thus receives naming credit for this specific application. However, it seems quite likely that the metric was already used widely, even if it <a id="_idIndexMarker1143"/>had not garnered much attention until much later. Today, it is used across industry, academic research, and even as a benchmark for machine learning competitions. There may be no single metric that better captures the overall performance of a binary classification model. However, as you will soon see, a more in-depth understanding of model performance can be obtained using combinations of metrics.</p>
    <div class="packt_tip">
      <p class="normal">Although the MCC is defined here for binary classification, it is unclear whether it is the best metric for multi-class outcomes. For a discussion of this and other alternatives, see <em class="italic">A comparison of MCC and CEN error measures in multi-class prediction, Jurman G, Riccadonna S, Furlanello C, 2012, PLOS One 7(8): e41882</em>.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-241">Sensitivity and specificity</h3>
    <p class="normal">Finding a useful classifier often involves a balance between predictions that are overly conservative and overly aggressive. For example, an email filter could guarantee to eliminate every spam message by aggressively filtering nearly every ham message. On the other hand, to guarantee that no ham messages will be inadvertently filtered might require us to allow an unacceptable amount of spam to pass through the filter. A pair of performance measures captures this tradeoff: sensitivity and specificity.</p>
    <p class="normal">The <strong class="keyWord">sensitivity</strong> of<a id="_idIndexMarker1144"/> a <a id="_idIndexMarker1145"/>model (also called the <strong class="keyWord">true positive rate</strong>) measures the proportion of positive examples that were correctly classified. Therefore, as shown in the following formula, it is calculated as the number of true positives divided by the total number of positives, both those correctly classified (the true positives) and those incorrectly classified (the false negatives):</p>
    <p class="center"><img alt="" src="../Images/B17290_10_005.png"/></p>
    <p class="normal">The <strong class="keyWord">specificity</strong> of a <a id="_idIndexMarker1146"/>model (also called the <strong class="keyWord">true negative rate</strong>) measures <a id="_idIndexMarker1147"/>the proportion of negative examples that were correctly classified. As with sensitivity, this is computed as the number of true negatives divided by the total number of negatives—the true negatives plus the false positives.</p>
    <p class="center"><img alt="" src="../Images/B17290_10_006.png"/></p>
    <p class="normal">Given the confusion<a id="_idIndexMarker1148"/> matrix for the SMS classifier, we can easily calculate these measures by hand. Assuming that spam is a positive class, we can confirm that the numbers in the <code class="inlineCode">confusionMatrix()</code> output are correct. For example, the calculation for sensitivity is:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sens <span class="hljs-operator">&lt;-</span> 152 <span class="hljs-operator">/</span> <span class="hljs-punctuation">(</span><span class="hljs-number">152</span> <span class="hljs-operator">+</span> <span class="hljs-number">31</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> sens
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.8306011
</code></pre>
    <p class="normal">Similarly, for specificity, we can calculate:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> spec <span class="hljs-operator">&lt;-</span> 1203 <span class="hljs-operator">/</span> <span class="hljs-punctuation">(</span><span class="hljs-number">1203</span> <span class="hljs-operator">+</span> <span class="hljs-number">4</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> spec
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.996686
</code></pre>
    <p class="normal">The <code class="inlineCode">caret</code> package provides functions for calculating sensitivity and specificity directly from vectors of predicted and actual values. Be careful to specify the <code class="inlineCode">positive</code> or <code class="inlineCode">negative</code> parameter appropriately, as shown in the following lines:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>caret<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> sensitivity<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>predict_type<span class="hljs-punctuation">,</span> sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">,</span>
              positive <span class="hljs-operator">=</span> <span class="hljs-string">"spam"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.8306011
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> specificity<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>predict_type<span class="hljs-punctuation">,</span> sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">,</span>
              negative <span class="hljs-operator">=</span> <span class="hljs-string">"ham"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.996686
</code></pre>
    <p class="normal">Sensitivity and specificity range from 0 to 1, with values close to 1 being more desirable. Of course, it is important to find an appropriate balance between the two—a task that is often quite context-specific.</p>
    <p class="normal">For example, in this case, the sensitivity of 0.831 implies that 83.1 percent of the spam messages were correctly classified. Similarly, the specificity of 0.997 implies that 99.7 percent of non-spam messages were correctly classified, or alternatively, 0.3 percent of valid messages were rejected as spam. The idea of rejecting 0.3 percent of valid SMS messages may be unacceptable, or it may be a reasonable tradeoff given the reduction in spam.</p>
    <p class="normal">Sensitivity and specificity provide tools for thinking about such tradeoffs. Typically, changes are made to the model and different models are tested until you find one that meets a desired sensitivity and specificity threshold. Visualizations, such as those discussed later in this chapter, can also assist with understanding the balance between sensitivity and specificity.</p>
    <h3 class="heading-3" id="_idParaDest-242">Precision and recall</h3>
    <p class="normal">Closely related <a id="_idIndexMarker1149"/>to sensitivity and specificity are two other performance measures related to compromises made in classification: precision and recall. Used primarily in the context of information retrieval, these statistics are intended to indicate how interesting and relevant a model’s results are, or whether the predictions are diluted by meaningless noise.</p>
    <p class="normal">The <strong class="keyWord">precision</strong> (also known as the <strong class="keyWord">positive predictive value</strong>) is <a id="_idIndexMarker1150"/>defined as the proportion of positive predictions that are truly positive; in other words, when a model predicts the positive class, how often is it correct? A precise model will only predict the positive class in cases very likely to be positive. It will be very trustworthy.</p>
    <p class="center"><img alt="" src="../Images/B17290_10_007.png"/></p>
    <p class="normal">Consider what would happen if the model was very imprecise. Over time, the results would be less likely to be trusted. In the context of information retrieval, this would be analogous to a search engine like Google returning irrelevant results. Eventually, users would switch to a competitor like Bing. In the case of the SMS spam filter, high precision means that the model is able to carefully target only the spam while avoiding false positives in the ham.</p>
    <p class="normal">On the other <a id="_idIndexMarker1151"/>hand, <strong class="keyWord">recall</strong> is a<a id="_idIndexMarker1152"/> measure of how complete the results are. As shown in the following formula, this is defined as the number of true positives over the total number of positives. You may have already recognized this as the same as sensitivity; however, the interpretation differs slightly.</p>
    <p class="center"><img alt="" src="../Images/B17290_10_008.png"/></p>
    <p class="normal">A model with high recall captures a large portion of the positive examples, meaning that it has a wide<a id="_idIndexMarker1153"/> breadth. For example, a search engine with high recall returns a large number of documents <a id="_idIndexMarker1154"/>pertinent to the search query. Similarly, the SMS spam filter has high recall if the majority of spam messages are correctly identified.</p>
    <p class="normal">We can calculate precision and recall from the confusion matrix. Again, assuming that spam is a positive class, the precision is:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> prec <span class="hljs-operator">&lt;-</span> 152 <span class="hljs-operator">/</span> <span class="hljs-punctuation">(</span><span class="hljs-number">152</span> <span class="hljs-operator">+</span> <span class="hljs-number">4</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> prec
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.974359
</code></pre>
    <p class="normal">And the recall is:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> rec <span class="hljs-operator">&lt;-</span> 152 <span class="hljs-operator">/</span> <span class="hljs-punctuation">(</span><span class="hljs-number">152</span> <span class="hljs-operator">+</span> <span class="hljs-number">31</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> rec
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.8306011
</code></pre>
    <p class="normal">The <code class="inlineCode">caret</code> package can be used to compute either of these measures from vectors of predicted and actual classes. Precision uses the <code class="inlineCode">posPredValue()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>caret<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> posPredValue<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>predict_type<span class="hljs-punctuation">,</span> sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">,</span>
               positive <span class="hljs-operator">=</span> <span class="hljs-string">"spam"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.974359
</code></pre>
    <p class="normal">Recall uses the <code class="inlineCode">sensitivity()</code> function that we used earlier:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sensitivity<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>predict_type<span class="hljs-punctuation">,</span> sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">,</span>
              positive <span class="hljs-operator">=</span> <span class="hljs-string">"spam"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.8306011
</code></pre>
    <p class="normal">Like the tradeoff between sensitivity and specificity, for most real-world problems, it is difficult to build a model with both high precision and high recall. It is easy to be precise if you target only the low-hanging fruit—the easiest-to-classify examples. Similarly, it is easy for a model to have a high recall by casting a very wide net, meaning that the model is overly aggressive at identifying positive cases. In contrast, having both high precision and recall at the same time is very challenging. It is therefore important to test a variety of models in order to find the combination of precision and recall that meets the needs of your project.</p>
    <h3 class="heading-3" id="_idParaDest-243">The F-measure</h3>
    <p class="normal">A measure <a id="_idIndexMarker1155"/>of model performance <a id="_idIndexMarker1156"/>that combines precision and recall into a single number is known as the <strong class="keyWord">F-measure</strong> (also sometimes called the <strong class="keyWord">F</strong><sub class="subscript-bold" style="font-weight: bold;">1</sub><strong class="keyWord"> score</strong> or the <strong class="keyWord">F-score</strong>). The F-measure combines precision and recall using<a id="_idIndexMarker1157"/> the <strong class="keyWord">harmonic mean</strong>, a type of average that is used for rates of change. The harmonic mean is used rather than the more common arithmetic mean since both precision and recall are expressed as proportions between 0 and 1, which can be interpreted as rates. The following is the formula for the F-measure:</p>
    <p class="center"><img alt="" src="../Images/B17290_10_009.png"/></p>
    <p class="normal">To calculate the F-measure, use the precision and recall values computed previously:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> f <span class="hljs-operator">&lt;-</span> <span class="hljs-punctuation">(</span><span class="hljs-number">2</span> <span class="hljs-operator">*</span> prec <span class="hljs-operator">*</span> rec<span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span> <span class="hljs-punctuation">(</span>prec <span class="hljs-operator">+</span> rec<span class="hljs-punctuation">)</span> 
<span class="hljs-operator">&gt;</span> f
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.8967552
</code></pre>
    <p class="normal">This comes out exactly the same as using the counts from the confusion matrix:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> f <span class="hljs-operator">&lt;-</span> <span class="hljs-punctuation">(</span><span class="hljs-number">2</span> <span class="hljs-operator">*</span> <span class="hljs-number">152</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">/</span> <span class="hljs-punctuation">(</span><span class="hljs-number">2</span> <span class="hljs-operator">*</span> <span class="hljs-number">152</span> <span class="hljs-operator">+</span> <span class="hljs-number">4</span> <span class="hljs-operator">+</span> <span class="hljs-number">31</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> f
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.8967552
</code></pre>
    <p class="normal">Since the F-measure describes model performance in a single number, it provides a convenient, quantitative metric to compare several models directly. Indeed, the F-measure was once virtually a gold standard measure of a model of performance, but today, it seems to be much less widely used than it was previously. One potential explanation is that it assumes that equal weight should be assigned to precision and recall, an assumption that is not always valid, depending on the real-world costs of false positives and false negatives. Of course, it is possible to calculate F-scores using different weights for precision and recall, but choosing the weights can be tricky at best and arbitrary at worst. This being said, perhaps a more important reason why this metric has fallen out of favor is the adoption of methods that visually depict a model’s performance on different subsets of data, such as those described in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-244">Visualizing performance tradeoffs with ROC curves</h2>
    <p class="normal">Visualizations <a id="_idIndexMarker1158"/>are <a id="_idIndexMarker1159"/>helpful for understanding the performance of machine learning algorithms in greater detail. Where statistics such as sensitivity and specificity, or precision and recall, attempt to boil model performance down to a single number, visualizations depict how a learner performs across a wide range of conditions.</p>
    <p class="normal">Because learning algorithms have different biases, it is possible that two models with similar accuracy could have drastic differences in how they achieve their accuracy. Some models may struggle with certain predictions that others make with ease while breezing through cases that others struggle to get right. Visualizations provide a method for understanding these tradeoffs by comparing learners side by side in a single chart.</p>
    <p class="normal">The <strong class="keyWord">receiver operating characteristic</strong> (<strong class="keyWord">ROC</strong>) curve is<a id="_idIndexMarker1160"/> commonly used to examine the tradeoff between the detection of true positives while avoiding false positives. As you might suspect from the name, ROC curves were developed by engineers in the field of communications. Around the time of World War II, radar and radio operators used ROC curves to measure a receiver’s ability to discriminate between true signals and false alarms. The same technique is useful today for visualizing the efficacy of machine learning models.</p>
    <div class="note">
      <p class="normal">For more reading on ROC curves, see <em class="italic">An introduction to ROC analysis, Fawcett T, Pattern Recognition Letters, 2006, Vol. 27, pp. 861–874</em>.</p>
    </div>
    <p class="normal">The <a id="_idIndexMarker1161"/>characteristics of a typical ROC diagram are depicted in <em class="italic">Figure 10.4</em>. The ROC curve is drawn using the proportion of true positives on the vertical axis and the proportion of false positives on the horizontal axis. Because these values are equivalent to sensitivity and (1 – specificity), respectively, the diagram is also <a id="_idIndexMarker1162"/>known as a <strong class="keyWord">sensitivity/specificity plot</strong>.</p>
    <figure class="mediaobject"><img alt="Chart  Description automatically generated" src="../Images/B17290_10_07.png"/></figure>
    <p class="packt_figref">Figure 10.4: The ROC curve depicts classifier shapes relative to perfect and useless classifiers</p>
    <p class="normal">The points<a id="_idIndexMarker1163"/> comprising ROC curves indicate the true positive rate at varying false positive thresholds. To illustrate this concept, three hypothetical classifiers are contrasted in the previous plot. First, the <em class="italic">perfect classifier</em> has a curve that passes through the point at a 100 percent true positive rate and a 0 percent false positive rate. It is able to correctly identify all of the true positives before it incorrectly classifies any negative result. Next, the diagonal line from the bottom-left to the top-right corner of the diagram represents a <em class="italic">classifier with no predictive value</em>. This type of classifier detects true positives and false positives at exactly the same rate, implying that the classifier cannot discriminate between<a id="_idIndexMarker1164"/> the two. This is the baseline by which other classifiers may be judged. ROC curves falling close to this line indicate models that are not very useful. Lastly, most real-world classifiers are like the <em class="italic">test classifier</em>, in that they fall somewhere in the zone between perfect and useless.</p>
    <p class="normal">The best way to understand how the ROC curve is constructed is to create one by hand. The values in the table depicted in <em class="italic">Figure 10.5</em> indicate predictions of a hypothetical spam model applied to a test set containing 20 examples, of which six are the positive class (spam) and 14 are the negative class (ham).</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_10_08.png"/></figure>
    <p class="packt_figref">Figure 10.5: To construct the ROC curve, the estimated probability values for the positive class are sorted in descending order, then compared to the actual class value</p>
    <p class="normal">To create <a id="_idIndexMarker1165"/>the curves, the classifier’s predictions are sorted by the model’s estimated probability of the positive class, in descending order, with the largest values first, as shown in the table. Then, beginning at the plot’s origin, each prediction’s impact on the true positive rate and false positive rate results in a curve tracing vertically for each positive <a id="_idIndexMarker1166"/>example and horizontally for each negative example. This process can be performed by hand on a piece of graph paper, as depicted in <em class="italic">Figure 10.6</em>:</p>
    <figure class="mediaobject"><img alt="Chart  Description automatically generated" src="../Images/B17290_10_09.png"/></figure>
    <p class="packt_figref">Figure 10.6: The ROC curve can be created by hand on graph paper by plotting the number of positive examples versus the number of negative examples</p>
    <p class="normal">Note that the<a id="_idIndexMarker1167"/> ROC curve is not complete at this point, because the axes are skewed due to the presence of more than twice as many negative examples as positive examples in the test set. A simple fix for this is to scale the plot proportionally so that the two axes are equivalent in size, as depicted in <em class="italic">Figure 10.7</em>:</p>
    <figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" src="../Images/B17290_10_10.png"/></figure>
    <p class="packt_figref">Figure 10.7: Scaling the plot’s axes creates a proportionate comparison, regardless of the initial balance of positive and negative examples</p>
    <p class="normal">If we imagine that both the <em class="italic">x</em> and <em class="italic">y</em> axes now range from 0 to 1, we can interpret each axis as a percentage. The <em class="italic">y</em> axis is the number of positives and originally ranged from 0 to 6; after shrinking it to a scale from 0 to 1, each increment becomes 1/6. On this scale, we can think of the vertical coordinate of the ROC curve as the number of true positives divided by <a id="_idIndexMarker1168"/>the total number of positives, which is the true positive rate, or sensitivity. Similarly, the <em class="italic">x</em> axis measures the number of negatives; by dividing by the total number of negatives (14 in this example), we obtain the true negative rate, or specificity. </p>
    <p class="normal">The table in <em class="italic">Figure 10.8</em> depicts these calculations for all 20 examples in the hypothetical test set:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_10_11.png"/></figure>
    <p class="packt_figref">Figure 10.8: The ROC curve traces what happens to the model’s true positive rate versus the false positive rate, for increasingly large sets of examples</p>
    <p class="normal">An important <a id="_idIndexMarker1169"/>property of ROC curves is that they are not affected by the class imbalance problem in which one of the two outcomes—typically the positive class—is much rarer than the other. Many performance metrics, such as accuracy, can be misleading for imbalanced data. This is not the case for ROC curves, because both dimensions of the plot are based solely on rates <em class="italic">within</em> the positive and negative values, and thus the ratio <em class="italic">across</em> positives and negatives does not affect the result. Because many of the most important <a id="_idIndexMarker1170"/>machine learning tasks involve severely imbalanced outcomes, ROC curves are a very useful tool for understanding the overall quality of a model.</p>
    <h3 class="heading-3" id="_idParaDest-245">Comparing ROC curves</h3>
    <p class="normal">If ROC curves <a id="_idIndexMarker1171"/>are helpful for evaluating a single model, it is unsurprising that they are also useful for comparing across models. Intuitively, we know that curves closer to the top-left of the plot area are better. In practice, the comparison is often more challenging than this, as differences between curves are often subtle rather than obvious, and the interpretation is nuanced and specific to how the model is to be used.</p>
    <p class="normal">To understand<a id="_idIndexMarker1172"/> the nuances, let’s begin by considering what causes two models to trace different curves on the ROC plot. Beginning at the origin, the curve’s length is extended as additional test set examples are predicted to be positive. Because the <em class="italic">y</em> axis represents the true positive rate and the <em class="italic">x</em> axis represents the false positive rate, a steeper upward trajectory is an implicit ratio, implying that the model is better at identifying the positive examples without making as many mistakes. This is illustrated in <em class="italic">Figure 10.9</em>, which depicts the start of ROC curves for two imaginary models. For the same number of predictions—indicated by the equal lengths of the vectors emerging from the origin—the first model has a higher true positive rate and a lower false positive rate, which implies it is the better performer of the two:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_10_12.png"/></figure>
    <p class="packt_figref">Figure 10.9: For the same number of predictions, model 1 outperforms model 2 because it has a higher true positive rate</p>
    <p class="normal">Suppose we continue to trace the ROC curves for each of these two models, evaluating the model’s predictions on the entire dataset. In this case, perhaps the first model continues to outperform the second at all points on the curve, as shown in <em class="italic">Figure 10.10</em>. </p>
    <p class="normal">For all points on the curve, the first model has a higher true positive rate and a lower false positive rate, which means it is the better performer across the entire dataset:</p>
    <figure class="mediaobject"><img alt="Chart  Description automatically generated" src="../Images/B17290_10_13.png"/></figure>
    <p class="packt_figref">Figure 10.10: Model 1 consistently performs better than model 2, with a higher true positive rate and a lower false positive rate at all points on the curve</p>
    <p class="normal">Although<a id="_idIndexMarker1173"/> the second model is clearly inferior in the prior example, choosing the better performer is not always so easy. <em class="italic">Figure 10.11</em> depicts intersecting ROC curves, which suggests that neither model is the best performer for all applications:</p>
    <figure class="mediaobject"><img alt="Chart  Description automatically generated" src="../Images/B17290_10_14.png"/></figure>
    <p class="packt_figref">Figure 10.11: Both model 1 and model 2 are the better performers for different subsets of the data</p>
    <p class="normal">The point of intersection between the two ROC curves splits the plot into two areas: one in which the first model has a higher true positive rate and the other in which the opposite is true. So, how do we know which model is “best” for any given use case?</p>
    <p class="normal">To answer this question, when comparing two curves, it helps to understand that both models are trying to sort the dataset in order of the highest to the lowest probability that each example is of the positive class. Models that are better able to sort the dataset in this way will have ROC curves closer to the top-left of the plot.</p>
    <p class="normal">The first model in <em class="italic">Figure 10.11</em> jumps to an early lead because it was able to sort a larger number of positive examples to the very front of the dataset, but after this initial surge, the second model was able to catch up and outperform the other by slowly and steadily sorting positive examples in front of negative examples over the remainder of the dataset. Although the second model may have better overall performance on the full dataset, we tend to prefer models that perform better early—the ones that take the so-called “low-hanging fruit” in the dataset. The justification for preferring these models is that many real-world models are used only for action on a subset of the data.</p>
    <p class="normal">For example, consider <a id="_idIndexMarker1174"/>a model used to identify customers that are most likely to respond to a direct mail advertising campaign. If we could afford to mail all potential customers, a model would be unnecessary. But because we don’t have the budget to send the advertisement to every address, the model is used to estimate the probability that the recipient will purchase the product after viewing the advertisement. A model that is better at sorting the true most likely purchasers to the front of the list will have a greater slope early in the ROC curve and will shrink the marketing budget needed to acquire purchasers. In <em class="italic">Figure 10.11</em>, the first model would be a better fit for this task.</p>
    <p class="normal">In contrast to this approach, another consideration is the relative costs of various types of errors; false positives and false negatives often have a different impact in the real world. If we know that a spam filter or a cancer screening needs to target a specific true positive rate, such as 90 percent or 99 percent, we will favor the model that has the lower false positive rate at the desired levels. Although neither model would be very good due to the high false positive rate, <em class="italic">Figure 10.11</em> suggests that the second model would be slightly preferable for these applications.</p>
    <p class="normal">As these examples demonstrate, ROC curves allow model performance comparisons that also consider how the models will be used. This flexibility is appreciated over simpler numeric metrics like accuracy or kappa, but it may also be desirable to quantify the ROC curve in a single metric that can be compared quantitatively, much like these statistics. The next section introduces exactly this type of measure.</p>
    <h3 class="heading-3" id="_idParaDest-246">The area under the ROC curve</h3>
    <p class="normal">Comparing ROC curves<a id="_idIndexMarker1175"/> can be somewhat subjective and context-specific, so metrics that reduce performance to a single numeric value are always in demand to simplify and bring objectivity into the comparison. While it may be difficult to say what makes a “good” ROC curve, in general, we know that the closer the ROC curve is to the top-left of the plot, the better it is at identifying positive values. This can be measured using a statistic known as <a id="_idIndexMarker1176"/>the <strong class="keyWord">area under the ROC curve</strong> (<strong class="keyWord">AUC</strong>). The AUC treats the ROC diagram as a two-dimensional square and measures the total area under the ROC curve. The AUC ranges from 0.5 (for a classifier with no predictive value) to 1.0 (for a perfect classifier). A convention for interpreting AUC scores uses a system similar to academic letter grades:</p>
    <ul>
      <li class="bulletList"><strong class="screenText">A</strong>: Outstanding = 0.9 to 1.0</li>
      <li class="bulletList"><strong class="screenText">B</strong>: Excellent/Good = 0.8 to 0.9</li>
      <li class="bulletList"><strong class="screenText">C</strong>: Acceptable/Fair = 0.7 to 0.8</li>
      <li class="bulletList"><strong class="screenText">D</strong>: Poor = 0.6 to 0.7</li>
      <li class="bulletList"><strong class="screenText">E</strong>: No Discrimination = 0.5 to 0.6</li>
    </ul>
    <p class="normal">As with most scales like this, the levels may work better for some tasks than others; the boundaries across categories are naturally somewhat fuzzy.</p>
    <div class="packt_tip">
      <p class="normal">It is rare but possible for the ROC curve to fall below the diagonal, which causes the AUC to be less than 0.50. This means the classifier performs worse than random. Usually, this is caused by a coding error, because a model that consistently makes the wrong prediction has obviously learned something useful about the data—it is merely applying the predictions in the wrong direction. To fix this issue, confirm that the positive cases are coded correctly, or simply reverse the predictions such that when the model predicts the negative class, choose the positive class instead.</p>
    </div>
    <p class="normal">When the use of AUC started to become widespread, some treated it as a definitive measure of model performance, although unfortunately, this is not true in all cases. Generally speaking, higher AUC values reflect classifiers that are better at sorting a random positive example higher than a random negative example. However, <em class="italic">Figure 10.12</em> illustrates the important fact that two ROC curves may be shaped very differently, yet have an identical AUC:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_10_15.png"/></figure>
    <p class="packt_figref">Figure 10.12: ROC curves may have different performances despite having the same AUC</p>
    <p class="normal">Because the <a id="_idIndexMarker1177"/>AUC is a simplification of the ROC curve, the AUC alone<a id="_idIndexMarker1178"/> is insufficient to identify the “best” model for all use cases. The safest practice is to use the AUC in combination with a qualitative examination of the ROC curve, as described earlier in this chapter. If two models have an identical or similar AUC, it is usually preferable to choose the one that performs better early. Furthermore, even in the case that one model has a better overall AUC, a model that has a higher initial true positive rate may be preferred for applications that will use only a subset of the most confident predictions.</p>
    <h3 class="heading-3" id="_idParaDest-247">Creating ROC curves and computing AUC in R</h3>
    <p class="normal">The <code class="inlineCode">pROC</code> package <a id="_idIndexMarker1179"/>provides<a id="_idIndexMarker1180"/> an easy-to-use set of functions for creating ROC<a id="_idIndexMarker1181"/> curves and<a id="_idIndexMarker1182"/> computing the AUC. The <code class="inlineCode">pROC</code> website (at <a href="https://web.expasy.org/pROC/"><span class="url">https://web.expasy.org/pROC/</span></a>) includes a list of the full set of features, as well as several examples of visualization capabilities. Before continuing, be sure that you have installed the package using the <code class="inlineCode">install.packages("pROC")</code> command.</p>
    <div class="note">
      <p class="normal">For more information on the <code class="inlineCode">pROC</code> package, see <em class="italic">pROC: an open-source package for R and S+ to analyze and compare ROC curves, Robin, X, Turck, N, Hainard, A, Tiberti, N, Lisacek, F, Sanchez, JC, and Mueller M, BMC Bioinformatics, 2011, pp. 12-77</em>.</p>
    </div>
    <p class="normal">To <a id="_idIndexMarker1183"/>create<a id="_idIndexMarker1184"/> visualizations with <code class="inlineCode">pROC</code>, two vectors of data are needed. The first must contain the estimated probability of the positive class and the second must contain the predicted class values. </p>
    <p class="normal">For the SMS classifier, we’ll supply the estimated spam probabilities and the actual class labels to the <code class="inlineCode">roc()</code> function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>pROC<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> sms_roc <span class="hljs-operator">&lt;-</span> roc<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>prob_spam<span class="hljs-punctuation">,</span> sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Using the <code class="inlineCode">sms_roc</code> object, we can visualize the ROC curve with R’s <code class="inlineCode">plot()</code> function. As shown in the following command, many of the standard parameters for adjusting the plot can be used, such as <code class="inlineCode">main</code> (for adding a title), <code class="inlineCode">col</code> (for changing the line color), and <code class="inlineCode">lwd</code> (for adjusting the line width). The <code class="inlineCode">grid</code> parameter adds faint gridlines to the plot to aid readability, and the <code class="inlineCode">legacy.axes</code> parameter instructs <code class="inlineCode">pROC</code> to label the <em class="italic">x</em> axis as 1 – specificity, which is a popular convention because it is equivalent to the false positive rate:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> plot<span class="hljs-punctuation">(</span>sms_roc<span class="hljs-punctuation">,</span> main <span class="hljs-operator">=</span> <span class="hljs-string">"ROC curve for SMS spam filter"</span><span class="hljs-punctuation">,</span>
         Col <span class="hljs-operator">=</span> <span class="hljs-string">"</span><span class="hljs-string">blue"</span><span class="hljs-punctuation">,</span> lwd <span class="hljs-operator">=</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> grid <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">,</span> legacy.axes <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The result is an ROC curve for the Naive Bayes classifier and a diagonal reference line representing the baseline classifier with no predictive value:</p>
    <figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" src="../Images/B17290_10_16.png"/></figure>
    <p class="packt_figref">Figure 10.13: The ROC curve for the Naive Bayes SMS classifier</p>
    <p class="normal">Qualitatively, we can see that this<a id="_idIndexMarker1185"/> ROC curve appears to occupy the space in the top-left corner of the diagram, which suggests that it is closer to a perfect classifier than the dashed line representing a useless classifier.</p>
    <p class="normal">To compare<a id="_idIndexMarker1186"/> this model’s performance to other models making predictions on the same dataset, we can add additional ROC curves to the same plot. Suppose that we had also trained a k-NN model on the SMS data using the <code class="inlineCode">knn()</code> function described in <em class="chapterRef">Chapter 3</em>, <em class="italic">Lazy Learning – Classification Using Nearest Neighbors</em>. Using this model, the predicted probabilities of spam were computed for each record in the test set and saved to a CSV file, which we can load here. After loading the file, we’ll apply the <code class="inlineCode">roc()</code> function as before to compute the ROC curve, then use the <code class="inlineCode">plot()</code> function with the parameter <code class="inlineCode">add = TRUE</code> to add the curve to the previous plot:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sms_results_knn <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"sms_results_knn.csv"</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> sms_roc_knn <span class="hljs-operator">&lt;-</span> roc<span class="hljs-punctuation">(</span>sms_results<span class="hljs-operator">$</span>actual_type<span class="hljs-punctuation">,</span>
                       sms_results_knn<span class="hljs-operator">$</span>p_spam<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> plot<span class="hljs-punctuation">(</span>sms_roc_knn<span class="hljs-punctuation">,</span> col <span class="hljs-operator">=</span> <span class="hljs-string">"red"</span><span class="hljs-punctuation">,</span> lwd <span class="hljs-operator">=</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> add <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The resulting visualization has a second curve depicting the performance of the k-NN model making predictions on the same test set as the Naive Bayes model. The curve for k-NN is consistently lower, suggesting that it is a consistently worse model than the Naive Bayes approach:</p>
    <figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" src="../Images/B17290_10_17.png"/></figure>
    <p class="packt_figref">Figure 10.14: ROC curves comparing the performance of Naive Bayes (topmost curve) and k-NN (bottom curve) on the SMS test set</p>
    <p class="normal">To confirm this <a id="_idIndexMarker1187"/>quantitatively, we can use the <code class="inlineCode">pROC</code> package to calculate the AUC. To do so, we simply apply the package’s <code class="inlineCode">auc()</code> function to the <code class="inlineCode">sms_roc</code> object for each model, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> auc<span class="hljs-punctuation">(</span>sms_roc<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Area under the curve: 0.9836
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> auc<span class="hljs-punctuation">(</span>sms_roc_knn<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Area under the curve: 0.8942
</code></pre>
    <p class="normal">The AUC for the Naive Bayes SMS classifier is 0.98, which is extremely high and substantially better than the k-NN classifier’s AUC of 0.89. But how do we know whether the model is just as likely to perform well on another dataset, or whether the difference is greater than expected by chance alone? In order to answer such questions, we need to better understand how far we can extrapolate a model’s predictions beyond the test data. Such methods are described in the sections that follow.</p>
    <div class="packt_tip">
      <p class="normal">This was mentioned before, but is worth repeating: the AUC value alone is often insufficient for identifying a “best” model. In this example, the AUC does identify the better model because the ROC curves do not intersect—the Naive Bayes model has a better true positive rate at all points on the ROC curve. When ROC curves <em class="italic">do</em> intersect, the “best” model will depend on how the model will be used. Additionally, it is possible to combine learners with intersecting ROC curves into even stronger models using the techniques covered in <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>.</p>
    </div>
    <h1 class="heading-1" id="_idParaDest-248">Estimating future performance</h1>
    <p class="normal">Some R machine learning <a id="_idIndexMarker1188"/>packages present confusion matrices and performance measures during the model-building process. The purpose of these statistics is to provide insight into the <a id="_idIndexMarker1189"/>model’s <strong class="keyWord">resubstitution error</strong>, which occurs when the target values of training examples are incorrectly predicted, despite the model being trained on this data. This can be used as a rough diagnostic to identify obviously poor performers. A model that cannot perform sufficiently well on the data it was trained on is unlikely to do well on future data.</p>
    <p class="normal">The opposite is not true. In other words, a model that performs well on the training data cannot be assumed to perform well on future datasets. For example, a model that used rote memorization to perfectly classify every training instance with zero resubstitution error would be unable to generalize its predictions to data it has never seen before. For this reason, the error rate on the training data can be assumed to be optimistic about a model’s future performance.</p>
    <p class="normal">Instead of relying on resubstitution error, a better practice is to evaluate a model’s performance on data it has not yet seen. We used such a method in previous chapters when we split the available data into a set for training and a set for testing. In some cases, however, it is not always ideal to create training and test datasets. For instance, in a situation where you have only a small pool of data, you might not want to reduce the sample any further.</p>
    <p class="normal">Fortunately, as you will soon learn, there are other ways to estimate a model’s performance on unseen data. The <code class="inlineCode">caret</code> package we used to calculate performance measures also offers functions to estimate future performance. If you are following along with the R code examples and haven’t already installed the <code class="inlineCode">caret</code> package, please do so. You will also need to load the package to the R session using the <code class="inlineCode">library(caret)</code> command.</p>
    <h2 class="heading-2" id="_idParaDest-249">The holdout method</h2>
    <p class="normal">The <a id="_idIndexMarker1190"/>procedure of partitioning data into training and <a id="_idIndexMarker1191"/>test datasets that we used in previous chapters is known as the <strong class="keyWord">holdout method</strong>. As shown in <em class="italic">Figure 10.15</em>, the <strong class="keyWord">training dataset</strong> is <a id="_idIndexMarker1192"/>used to generate the model, which is then applied to the <strong class="keyWord">test dataset</strong> to <a id="_idIndexMarker1193"/>generate predictions for evaluation. Typically, about one-third of the data is held out for testing and two-thirds is used for training, but this proportion can vary depending on the amount of available data or the complexity of the learning task. To ensure that the training and test datasets do not have systematic differences, their examples are randomly divided into two groups.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_10_18.png"/></figure>
    <p class="packt_figref">Figure 10.15: The simplest holdout method divides the data into training and test sets</p>
    <p class="normal">For the holdout method to result in a truly accurate estimate of future performance, at no time should performance on the test dataset be allowed to influence the modeling process. In the words of Stanford professor and renowned machine learning expert Trevor Hastie, “<em class="italic">ideally the test set should be kept in a ‘vault,’ and be brought out only at the end of the data analysis</em>.” In other words, the test data should remain untouched aside from its one and only purpose, which is to evaluate a single, final model.</p>
    <div class="note">
      <p class="normal">For more information, see <em class="italic">The Elements of Statistical Learning (2nd edition), Hastie, Tibshirani, and Friedman (2009), p. 222</em>.</p>
    </div>
    <p class="normal">It is easy to unknowingly violate this rule and peek into the metaphorical “vault” when choosing one of several models or changing a single model based on the results of repeated <a id="_idIndexMarker1194"/>testing. For example, suppose we built several models on the training data and selected the one with the highest accuracy on the test data. In this case, because we have used the test dataset to cherry-pick the best result, the test performance is not an unbiased measure of future performance on unseen data</p>
    <div class="packt_tip">
      <p class="normal">A keen reader will note that holdout test data was used in previous chapters to both evaluate models and improve model performance. This was done for illustrative purposes but would indeed violate the rule stated previously. Consequently, the model performance statistics shown were not truly unbiased estimates of future performance on unseen data.</p>
    </div>
    <p class="normal">.</p>
    <p class="normal">To avoid this problem, it is better to divide the original data so that in addition to the training and test datasets, a <strong class="keyWord">validation dataset</strong> is available. The <a id="_idIndexMarker1195"/>validation dataset can be used for iterating and refining the model or models chosen, leaving the test dataset to be used only once as a final step to report an estimated error rate for future predictions. A typical split between training, test, and validation would be 50 percent, 25 percent, and 25 percent, respectively.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_10_19.png"/></figure>
    <p class="packt_figref">Figure 10.16: A validation dataset can be held out from training to select from multiple candidate models</p>
    <p class="normal">A simple method for creating holdout samples uses random number generators to assign records to partitions. This technique was first used in <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>, to create training and test datasets.</p>
    <div class="packt_tip">
      <p class="normal">If you’d like to follow along with the following examples, download the <code class="inlineCode">credit.csv</code> dataset from Packt Publishing’s website and load it into a data frame using the <code class="inlineCode">credit &lt;- read.csv("credit.csv", stringsAsFactors = TRUE)</code> command.</p>
    </div>
    <p class="normal">Suppose <a id="_idIndexMarker1196"/>we have a data frame named <code class="inlineCode">credit</code> with 1,000 rows of data. We can divide this into three partitions as follows. First, we create a vector of randomly ordered row IDs from 1 to 1,000 using the <code class="inlineCode">runif()</code> function, which, by default, generates a specified number of random values between 0 and 1. The <code class="inlineCode">runif()</code> function gets its name from the random uniform distribution, which was discussed in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>.</p>
    <p class="normal">The <code class="inlineCode">order()</code> function then returns a vector indicating the rank order of the 1,000 random numbers. For instance, <code class="inlineCode">order(c(0.5, 0.25, 0.75, 0.1))</code> returns the sequence <code class="inlineCode">4 2 1 3</code> because the smallest number (0.1) appears fourth, the second smallest (0.25) appears second, and so on:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> random_ids <span class="hljs-operator">&lt;-</span> order<span class="hljs-punctuation">(</span>runif<span class="hljs-punctuation">(</span><span class="hljs-number">1000</span><span class="hljs-punctuation">))</span>
</code></pre>
    <p class="normal">Next, the random IDs are used to divide the credit data frame into 500, 250, and 250 records comprising the training, validation, and test datasets:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_train <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span>random_ids<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">500</span><span class="hljs-punctuation">],</span> <span class="hljs-punctuation">]</span>
<span class="hljs-operator">&gt;</span> credit_validate <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span>random_ids<span class="hljs-punctuation">[</span><span class="hljs-number">501</span><span class="hljs-operator">:</span><span class="hljs-number">750</span><span class="hljs-punctuation">],</span> <span class="hljs-punctuation">]</span>
<span class="hljs-operator">&gt;</span> credit_test <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span>random_ids<span class="hljs-punctuation">[</span><span class="hljs-number">751</span><span class="hljs-operator">:</span><span class="hljs-number">1000</span><span class="hljs-punctuation">],</span> <span class="hljs-punctuation">]</span>
</code></pre>
    <p class="normal">One problem with holdout sampling is that each partition may have a larger or smaller proportion of some classes. In cases where one (or more) class is a very small proportion of the dataset, this can lead to it being omitted from the training dataset—a significant problem because the model cannot then learn this class.</p>
    <p class="normal">To reduce the chance of this occurring, a technique<a id="_idIndexMarker1197"/> called <strong class="keyWord">stratified random sampling</strong> can be used. Although a random sample should generally contain roughly the same proportion of each class value as the full dataset, stratified random sampling guarantees that the random partitions have nearly the same proportion of each class as the full dataset, even when some classes are small.</p>
    <p class="normal">The <code class="inlineCode">caret</code> package provides a <code class="inlineCode">createDataPartition()</code> function, which creates partitions based on stratified holdout sampling. The steps for creating a stratified sample of training and test data for the <code class="inlineCode">credit</code> dataset are shown in the following commands. To use the function, a vector of class values must be specified (here, <code class="inlineCode">default</code> refers to whether a loan went into default), in addition to a parameter, <code class="inlineCode">p</code>, which specifies the proportion of <a id="_idIndexMarker1198"/>instances to be included in the partition. The <code class="inlineCode">list = FALSE</code> parameter prevents the result from being stored as a list object—a capability that is needed for more complex sampling techniques, but is unnecessary here:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> in_train <span class="hljs-operator">&lt;-</span> createDataPartition<span class="hljs-punctuation">(</span>credit<span class="hljs-operator">$</span>default<span class="hljs-punctuation">,</span> p <span class="hljs-operator">=</span> <span class="hljs-number">0.75</span><span class="hljs-punctuation">,</span> <span class="hljs-built_in">list</span> <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit_train <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span>in_train<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
<span class="hljs-operator">&gt;</span> credit_test <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span><span class="hljs-operator">-</span>in_train<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">in_train</code> vector indicates the row numbers included in the training sample. We can use these row numbers to select examples for the <code class="inlineCode">credit_train</code> data frame. Similarly, by using a negative symbol, we can use the rows not found in the <code class="inlineCode">in_train</code> vector for the <code class="inlineCode">credit_test</code> dataset.</p>
    <p class="normal">Although it distributes the classes evenly, stratified sampling does not guarantee other types of representativeness. Some samples may have too many or too few difficult cases, easy-to-predict cases, or outliers. This is especially true for smaller datasets, which may not have a large enough portion of such cases to divide among training and test sets.</p>
    <p class="normal">In addition to potentially biased samples, another problem with the holdout method is that substantial portions of data must be reserved for testing and validating the model. Since this data cannot be used to train the model until its performance has been measured, the performance estimates are likely to be overly conservative.</p>
    <div class="packt_tip">
      <p class="normal">Since models trained on larger datasets generally perform better, a common practice is to retrain the model on the full set of data (that is, training plus test and validation) after a final model has been selected and evaluated.</p>
    </div>
    <p class="normal">A technique <a id="_idIndexMarker1199"/>called <strong class="keyWord">repeated holdout</strong> is sometimes used to mitigate the problems of randomly composed training datasets. The repeated holdout method is a special case of the holdout method that uses the average result from several random holdout samples to evaluate a model’s performance. As multiple holdout samples are used, it is less likely that the model is trained or tested on non-representative data. We’ll expand on this idea in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-250">Cross-validation</h2>
    <p class="normal">The<a id="_idIndexMarker1200"/> repeated holdout is the basis of a technique <a id="_idIndexMarker1201"/>known as <strong class="keyWord">k-fold cross-validation</strong> (<strong class="keyWord">k-fold CV</strong>), which has become the industry standard for estimating model performance. Rather<a id="_idIndexMarker1202"/> than taking repeated random samples that could potentially use the same record more than once, k-fold CV randomly divides the data into <em class="italic">k</em> separate random partitions<a id="_idIndexMarker1203"/> called <strong class="keyWord">folds</strong>.</p>
    <p class="normal">Although <em class="italic">k</em> can be set to any number, by far the most common convention is to use a 10-fold CV. Why 10 folds? The reason is that empirical evidence suggests that there is little added benefit to using a greater number. For each of the 10 folds (each comprising 10 percent of the total data), a machine learning model is built on the remaining 90 percent of the data. The fold’s 10 percent sample is then used for model evaluation. After the process of training and evaluating the model has occurred 10 times (with 10 different training/testing combinations), the average performance across all folds is reported.</p>
    <div class="packt_tip">
      <p class="normal">An extreme case of k-fold CV is <a id="_idIndexMarker1204"/>the <strong class="keyWord">leave-one-out method</strong>, which performs k-fold CV using a fold for each one of the data’s examples. This ensures that the greatest amount of data is used for training the model. Although this may seem useful, it is so computationally expensive that it is rarely used in practice.</p>
    </div>
    <p class="normal">Datasets for CV can be created using the <code class="inlineCode">createFolds()</code> function in the <code class="inlineCode">caret</code> package. Like stratified random holdout sampling, this function will attempt to maintain the same class balance in each of the folds as in the original dataset. The following is the command to create 10 folds, using <code class="inlineCode">set.seed(123)</code> to ensure the results are reproducible:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">123</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> folds <span class="hljs-operator">&lt;-</span> createFolds<span class="hljs-punctuation">(</span>credit<span class="hljs-operator">$</span>default<span class="hljs-punctuation">,</span> k <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The result of the <code class="inlineCode">createFolds()</code> function is a list of vectors storing the row numbers for each of the <code class="inlineCode">k = 10</code> requested folds. We can peek at the contents using <code class="inlineCode">str()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> str<span class="hljs-punctuation">(</span>folds<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">List of 10
 $ Fold01: int [1:100] 14 23 32 42 51 56 65 66 77 95 ...
 $ Fold02: int [1:100] 21 36 52 55 96 115 123 129 162 169 ...
 $ Fold03: int [1:100] 3 22 30 34 37 39 43 58 70 85 ...
 $ Fold04: int [1:100] 12 15 17 18 19 31 40 45 47 57 ...
 $ Fold05: int [1:100] 1 5 7 20 26 35 46 54 106 109 ...
 $ Fold06: int [1:100] 6 27 29 48 68 69 72 73 74 75 ...
 $ Fold07: int [1:100] 10 38 49 60 61 63 88 94 104 108 ...
 $ Fold08: int [1:100] 8 11 24 53 71 76 89 90 91 101 ...
 $ Fold09: int [1:100] 2 4 9 13 16 25 28 44 62 64 ...
 $ Fold10: int [1:100] 33 41 50 67 81 82 100 105 107 118 ...
</code></pre>
    <p class="normal">Here, we<a id="_idIndexMarker1205"/> see that the first fold is named <code class="inlineCode">Fold01</code> and stores 100 integers, indicating the 100 rows in the <code class="inlineCode">credit</code> data frame for the first fold. To create training and test datasets to build and evaluate a model, an additional step is needed. The following commands show how to create data for the first fold. We’ll assign the selected 10 percent to the test dataset and use the negative symbol to assign the remaining 90 percent to the training dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit01_test <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span>folds<span class="hljs-operator">$</span>Fold01<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
<span class="hljs-operator">&gt;</span> credit01_train <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span><span class="hljs-operator">-</span>folds<span class="hljs-operator">$</span>Fold01<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
</code></pre>
    <p class="normal">To perform the full 10-fold CV, this step would need to be repeated a total of 10 times, first building a model and then calculating the model’s performance each time. In the end, the performance measures would be averaged to obtain the overall performance. Thankfully, we can automate this task by applying several of the techniques we learned earlier.</p>
    <p class="normal">To demonstrate the process, we’ll estimate the kappa statistic for a C5.0 decision tree model of the credit data using 10-fold CV. First, we need to load some R packages: <code class="inlineCode">caret</code> (to create the folds), <code class="inlineCode">C50</code> (to build the decision tree), and <code class="inlineCode">irr</code> (to calculate kappa). The latter two packages were chosen for illustrative purposes; if you desire, you can use a different model or a different performance measure with the same series of steps:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>caret<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>C50<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>irr<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Next, we’ll create a list of 10 folds as we have done previously. As before, the <code class="inlineCode">set.seed()</code> function is used here to ensure that the results are consistent if the same code is run again:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">123</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> folds <span class="hljs-operator">&lt;-</span> createFolds<span class="hljs-punctuation">(</span>credit<span class="hljs-operator">$</span>default<span class="hljs-punctuation">,</span> k <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Finally, we will apply a series of identical steps to the list of folds using the <code class="inlineCode">lapply()</code> function. As shown in the following code, because there is no existing function that does exactly what <a id="_idIndexMarker1206"/>we need, we must define our own function to pass to <code class="inlineCode">lapply()</code>. Our custom function divides the <code class="inlineCode">credit</code> data frame into training and test data, builds a decision tree using the <code class="inlineCode">C5.0()</code> function on the training data, generates a set of predictions from the test data, and compares the predicted and actual values using the <code class="inlineCode">kappa2()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> cv_results <span class="hljs-operator">&lt;-</span> lapply<span class="hljs-punctuation">(</span>folds<span class="hljs-punctuation">,</span> <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-punctuation">{</span>
    credit_train <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span><span class="hljs-operator">-</span>x<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
    credit_test <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span>x<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
    credit_model <span class="hljs-operator">&lt;-</span> C5.0<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit_train<span class="hljs-punctuation">)</span>
    credit_pred <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>credit_model<span class="hljs-punctuation">,</span> credit_test<span class="hljs-punctuation">)</span>
    credit_actual <span class="hljs-operator">&lt;-</span> credit_test<span class="hljs-operator">$</span>default
    kappa <span class="hljs-operator">&lt;-</span> kappa2<span class="hljs-punctuation">(</span>data.frame<span class="hljs-punctuation">(</span>credit_actual<span class="hljs-punctuation">,</span> credit_pred<span class="hljs-punctuation">))</span><span class="hljs-operator">$</span>value
    <span class="hljs-built_in">return</span><span class="hljs-punctuation">(</span>kappa<span class="hljs-punctuation">)</span>
  <span class="hljs-punctuation">})</span>
</code></pre>
    <p class="normal">The resulting kappa statistics are compiled into a list stored in the <code class="inlineCode">cv_results</code> object, which we can examine using <code class="inlineCode">str()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> str<span class="hljs-punctuation">(</span>cv_results<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">List of 10
 $ Fold01: num 0.381
 $ Fold02: num 0.525
 $ Fold03: num 0.247
 $ Fold04: num 0.316
 $ Fold05: num 0.387
 $ Fold06: num 0.368
 $ Fold07: num 0.122
 $ Fold08: num 0.141
 $ Fold09: num 0.0691
 $ Fold10: num 0.381
</code></pre>
    <p class="normal">There’s just one more step remaining in the 10-fold-CV process: we must calculate the average of these 10 values. Although you will be tempted to type <code class="inlineCode">mean(cv_results)</code>, because <code class="inlineCode">cv_results</code> is not a numeric vector, the result would be an error. Instead, use the <code class="inlineCode">unlist()</code> function, which eliminates the list structure and reduces <code class="inlineCode">cv_results</code> to a numeric vector. From there, we can calculate the mean kappa as expected:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mean<span class="hljs-punctuation">(</span>unlist<span class="hljs-punctuation">(</span>cv_results<span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.2939567
</code></pre>
    <p class="normal">This <a id="_idIndexMarker1207"/>kappa statistic is relatively low, corresponding to “fair” on the interpretation scale, which suggests that the credit scoring model performs only marginally better than random chance. In <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>, we’ll examine automated methods based on a 10-fold CV, which can assist us with improving the performance of this model.</p>
    <p class="normal">Because CV provides a performance estimate from multiple test sets, we can also compute the variability in the estimate. For example, the standard deviation of the 10 iterations can be computed as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> sd<span class="hljs-punctuation">(</span>unlist<span class="hljs-punctuation">(</span>cv_results<span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.1448565
</code></pre>
    <p class="normal">After finding the average and standard deviation of the performance metric, it is possible to calculate a confidence interval or determine whether two models have a <strong class="keyWord">statistically significant</strong> difference in performance, which means that it is likely that the difference is real and not due to random variation.</p>
    <p class="normal">Unfortunately, recent research has demonstrated that CV violates assumptions of such statistical tests, particularly the need for the data to be drawn from independent random samples, which is clearly not the case for CV folds, which are linked to one another by definition.</p>
    <div class="note">
      <p class="normal">For a discussion of the limitations of performance estimates taken from 10-fold CV, see <em class="italic">Cross-validation: what does it estimate and how well does it do it?, Bates S, Hastie T, and Tibshirani R, 2022, https://arxiv.org/abs/2104.00673</em>.</p>
    </div>
    <p class="normal">More sophisticated variants of CV have been developed to improve the robustness of model performance estimates. One <a id="_idIndexMarker1208"/>such technique is <strong class="keyWord">repeated k-fold CV</strong>, which involves repeatedly applying k-fold CV and averaging the results. A common strategy is to perform 10-fold CV 10 times. Although computationally intensive, this provides an even more robust performance estimate than a standard 10-fold CV, as the performance is averaged over many more trials. However, it too violates statistical assumptions, and thus statistical tests performed on the results may be slightly biased.</p>
    <p class="normal">Perhaps the current gold standard for estimating model performance<a id="_idIndexMarker1209"/> is <strong class="keyWord">nested cross-validation</strong>, which literally performs k-fold CV within another k-fold CV process. This technique is described in <em class="chapterRef">Chapter 11</em>, <em class="italic">Being Successful with Machine Learning</em>, and is not only extremely computationally expensive but is also more challenging to implement and interpret. The upside of nested k-fold CV is that it produces truly<a id="_idIndexMarker1210"/> valid comparisons of model performance compared to standard k-fold CV, which is biased due to its violation of statistical assumptions. On the other hand, the bias caused by this issue seems to be less important for very large datasets, so it may still be reasonable—and remains a common practice—to use the confidence intervals or significance tests derived from the simpler CV approach to help identify the “best” model.</p>
    <h2 class="heading-2" id="_idParaDest-251">Bootstrap sampling</h2>
    <p class="normal">A slightly<a id="_idIndexMarker1211"/> less popular, but very important, alternative to k-fold CV is known as <strong class="keyWord">bootstrap sampling</strong>, the <strong class="keyWord">bootstrap</strong>, or <strong class="keyWord">bootstrapping</strong> for short. Generally <a id="_idIndexMarker1212"/>speaking, these refer to statistical methods that use random samples of data to estimate the properties of a larger set. When this principle is applied to machine learning model performance, it implies the creation of several randomly selected training and test datasets, which are then used to estimate performance statistics. The results from the various random datasets are then averaged to obtain a final estimate of future performance.</p>
    <p class="normal">So, what makes this procedure different from k-fold CV? Whereas CV divides the data into separate partitions in which each example can appear only once, the bootstrap allows examples to be selected <a id="_idIndexMarker1213"/>multiple times through a process of <strong class="keyWord">sampling with replacement</strong>. This means that from the original dataset of <em class="italic">n</em> examples, the bootstrap procedure will create one or more new training datasets that also contain <em class="italic">n</em> examples, some of which are repeated. </p>
    <p class="normal">The corresponding test datasets are then constructed from the set of examples that were not selected for the respective training datasets.</p>
    <p class="normal">In a bootstrapped dataset, the probability that any given instance is excluded from the training dataset is 36.8 percent. We can prove this mathematically by recognizing that each example has a 1/<em class="italic">n</em> chance of being sampled each time one of <em class="italic">n</em> rows is added to the training dataset. Therefore, to be in the test set, an example must <em class="italic">not</em> be selected <em class="italic">n</em> times. As the <a id="_idIndexMarker1214"/>chance of being chosen is 1/<em class="italic">n</em>, the chance of <em class="italic">not</em> being chosen is therefore 1 - 1/<em class="italic">n</em>, and the probability of going unselected <em class="italic">n</em> times is as follows:</p>
    <p class="center"><img alt="" src="../Images/B17290_10_010.png"/></p>
    <p class="normal">Using this formula, if the dataset to be bootstrapped contains 1,000 rows, the probability <a id="_idIndexMarker1215"/>of a random record being unselected is:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-punctuation">(</span><span class="hljs-number">1</span> <span class="hljs-operator">-</span> <span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-operator">/</span><span class="hljs-number">1000</span><span class="hljs-punctuation">))</span><span class="hljs-operator">^</span><span class="hljs-number">1000</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.3676954
</code></pre>
    <p class="normal">Similarly, for a dataset with 100,000 rows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-punctuation">(</span><span class="hljs-number">1</span> <span class="hljs-operator">-</span> <span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-operator">/</span><span class="hljs-number">100000</span><span class="hljs-punctuation">))</span><span class="hljs-operator">^</span><span class="hljs-number">100000</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.3678776
</code></pre>
    <p class="normal">And as <em class="italic">n</em> approaches infinity, the formula reduces to 1/<em class="italic">e</em>, as shown here:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-number">1</span> <span class="hljs-operator">/</span> <span class="hljs-built_in">exp</span><span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.3678794
</code></pre>
    <p class="normal">Given that the probability of being unselected is 36.8 percent, the probability of any instance being selected for the training dataset is 100 - 36.8 = 63.2 percent. In other words, the training data represents only 63.2 percent of available examples, some of which are repeated. In contrast with 10-fold CV, which uses 90 percent of examples for training, the bootstrap sample is less representative of the full dataset.</p>
    <p class="normal">Because a model trained on only 63.2 percent of the training data is likely to perform worse than a model trained on a larger training set, the bootstrap’s performance estimates may be substantially lower than what will be obtained when the model is later trained on the full dataset. </p>
    <p class="normal">A special case of bootstrapping, known as the <strong class="keyWord">0.632 bootstrap</strong>, accounts<a id="_idIndexMarker1216"/> for this by calculating the final performance measure as a function of performance on both the training data (which is overly optimistic) and the test data (which is overly pessimistic). The final error rate is then estimated as:</p>
    <p class="center"><img alt="" src="../Images/B17290_10_011.png"/></p>
    <p class="normal">One advantage of bootstrap sampling over CV is that it tends to work better with very <a id="_idIndexMarker1217"/>small datasets. Additionally, bootstrap sampling has <a id="_idIndexMarker1218"/>applications beyond performance measurement. In particular, in <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>, you will learn how the principles of bootstrap sampling can be used to improve model performance.</p>
    <h1 class="heading-1" id="_idParaDest-252">Summary</h1>
    <p class="normal">This chapter presented several of the most common measures and techniques for evaluating the performance of machine learning classification models. Although accuracy provides a simple method for examining how often a model is correct, this can be misleading in the case of rare events because the real-life importance of such events may be inversely proportional to how frequently they appear in the data.</p>
    <p class="normal">Some measures based on confusion matrices better capture a model’s performance as well as the balance between the costs of various types of errors. The kappa statistic and Matthews correlation coefficient are two more sophisticated measures of performance, which work well even for severely unbalanced datasets. Additionally, closely examining the tradeoffs between sensitivity and specificity, or precision and recall, can be a useful tool for thinking about the implications of errors in the real world. Visualizations such as the ROC curve are also helpful to this end.</p>
    <p class="normal">It is also worth mentioning that, sometimes, the best measure of a model’s performance is to consider how well it meets, or doesn’t meet, other objectives. For instance, you may need to explain a model’s logic in simple language, which would eliminate some models from consideration. Additionally, even if it performs very well, a model that is too slow or difficult to scale to a production environment is completely useless.</p>
    <p class="normal">Looking ahead to the chapters that follow, an obvious extension of <em class="italic">measuring</em> performance is finding ways to <em class="italic">improve</em> performance. As you continue in this book, you will apply many of the principles in this chapter while strengthening your machine learning abilities and adding more advanced skills. CV techniques, ROC curves, bootstrapping, and the <code class="inlineCode">caret</code> package will reappear regularly in the coming pages, as we build upon our work so far to investigate ways to make smarter models by systematically iterating, refining, and combining learning algorithms.</p>
    <h1 class="heading-1" id="_idParaDest-253">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>