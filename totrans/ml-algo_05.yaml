- en: Logistic Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'This chapter begins by analyzing linear classification problems, with particular
    focus on logistic regression (despite its name, it''s a classification algorithm)
    and stochastic gradient descent approaches. Even if these strategies appear too
    simple, they''re still the main choices in many classification tasks. Speaking
    of which, it''s useful to remember a very important philosophical principle: **Occam''s
    razor**. In our context, it states that the first choice must always be the simplest
    and only if it doesn''t fit, it''s necessary to move on to more complex models.
    In the second part of the chapter, we''re going to discuss some common metrics
    useful to evaluate a classification task. They are not limited to linear models,
    so we use them when talking about different strategies as well.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先分析线性分类问题，特别关注逻辑回归（尽管它的名字是分类算法）和随机梯度下降方法。即使这些策略看起来过于简单，但它们仍然是许多分类任务中的主要选择。说到这一点，记住一个非常重要的哲学原则是有用的：**奥卡姆剃刀**。在我们的语境中，它表明第一个选择必须是简单的，并且只有当它不适用时，才需要转向更复杂的模型。在章节的第二部分，我们将讨论一些用于评估分类任务的常用指标。它们不仅限于线性模型，因此我们在讨论不同的策略时也使用它们。
- en: Linear classification
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性分类
- en: 'Let''s consider a generic linear classification problem with two classes. In
    the following figure, there''s an example:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个具有两个类别的通用线性分类问题。在下面的图中，有一个例子：
- en: '![](img/1e3a5f30-f40e-4a6a-b4f8-dac180fb9041.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1e3a5f30-f40e-4a6a-b4f8-dac180fb9041.png)'
- en: 'Our goal is to find an optimal hyperplane, which separates the two classes.
    In multi-class problems, the strategy one-vs-all is normally adopted, so the discussion
    can be focused only on binary classifications. Suppose we have the following dataset:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到一个最优的超平面，它将两个类别分开。在多类问题中，通常采用一对一的策略，因此讨论可以仅集中在二分类上。假设我们有以下数据集：
- en: '![](img/c16cc001-8b7e-43d2-95c8-346bf6027dfc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c16cc001-8b7e-43d2-95c8-346bf6027dfc.png)'
- en: 'This dataset is associated with the following target set:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集与以下目标集相关联：
- en: '![](img/389065d9-fc7c-4077-ae7c-1dd305531b84.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/389065d9-fc7c-4077-ae7c-1dd305531b84.png)'
- en: 'We can now define a weight vector made of *m* continuous components:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义一个由 *m* 个连续分量组成的权重向量：
- en: '![](img/c623901b-3744-48c9-9c46-5cff7b1365bf.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c623901b-3744-48c9-9c46-5cff7b1365bf.png)'
- en: 'We can also define the quantity *z*:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以定义数量 *z*：
- en: '![](img/19cfa97f-4466-4d4c-a13d-48476b14bed3.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/19cfa97f-4466-4d4c-a13d-48476b14bed3.png)'
- en: 'If *x* is a variable, *z* is the value determined by the hyperplane equation.
    Therefore, if the set of coefficients *w* that has been determined is correct,
    it happens that:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *x* 是一个变量，*z* 是由超平面方程确定的值。因此，如果已经确定的系数集 *w* 是正确的，那么就会发生以下情况：
- en: '![](img/3a1dc317-1b6b-47d3-9317-eefa134e7787.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3a1dc317-1b6b-47d3-9317-eefa134e7787.png)'
- en: 'Now we must find a way to optimize *w*, in order to reduce the classification
    error. If such a combination exists (with a certain error threshold), we say that
    our problem is **linearly separable**. On the other hand, when it''s impossible
    to find a linear classifier, the problem is called **non-linearly separable**.
    A very simple but famous example is given by the logical operator `XOR`:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须找到一种方法来优化 *w*，以减少分类误差。如果存在这样的组合（具有一定的错误阈值），我们说我们的问题是**线性可分**的。另一方面，当无法找到线性分类器时，问题被称为**非线性可分**。一个简单但著名的例子是由逻辑运算符
    `XOR` 提供的：
- en: '![](img/1f827264-813e-45ec-99a0-1e4de9acffcd.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1f827264-813e-45ec-99a0-1e4de9acffcd.png)'
- en: As you can see, any line will always include a wrong sample. Hence, in order
    to solve this problem, it is necessary to involve non-linear techniques. However,
    in many real-life cases, we use linear techniques (which are often simpler and
    faster) for non-linear problems too, accepting a tolerable misclassification error.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，任何一行都可能包含一个错误的样本。因此，为了解决这个问题，有必要涉及非线性技术。然而，在许多实际情况下，我们也会使用线性技术（通常更简单、更快）来解决非线性问题，接受可容忍的错误分类误差。
- en: Logistic regression
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Even if called regression, this is a classification method which is based on
    the probability for a sample to belong to a class. As our probabilities must be
    continuous in *R* and bounded between (0, 1), it''s necessary to introduce a threshold
    function to filter the term *z*. The name logistic comes from the decision to
    use the sigmoid (or logistic) function:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 即使被称为回归，这实际上是一种基于样本属于某一类别的概率的分类方法。由于我们的概率必须在 *R* 中连续并且介于 (0, 1) 之间，因此有必要引入一个阈值函数来过滤
    *z* 这一项。这个名字“逻辑回归”来源于使用sigmoid（或逻辑）函数的决定：
- en: '![](img/b1d6d740-629e-4fac-a5a1-aae77e2a4178.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b1d6d740-629e-4fac-a5a1-aae77e2a4178.png)'
- en: 'A partial plot of this function is shown in the following figure:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的部分图示如下：
- en: '![](img/8de1bdb9-9e69-498f-af71-cf00358d5b99.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8de1bdb9-9e69-498f-af71-cf00358d5b99.png)'
- en: 'As you can see, the function intersects *x=0* in the ordinate 0.5, and *y<0.5*
    for *x<0* and *y>0.5* for *x>0*. Moreover, its domain is *R* and it has two asymptotes
    at 0 and 1\. So, we can define the probability for a sample to belong to a class
    (from now on, we''ll call them 0 and 1) as:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，函数在纵坐标0.5处与*x=0*相交，对于*x<0*，*y<0.5*，对于*x>0*，*y>0.5*。此外，其定义域为*R*，并且在0和1处有两个渐近线。因此，我们可以定义一个样本属于一个类（从现在开始，我们将它们称为0和1）的概率：
- en: '![](img/2746315b-8631-46d3-bd76-3c5c679ada8b.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2746315b-8631-46d3-bd76-3c5c679ada8b.png)'
- en: 'At this point, finding the optimal parameters is equivalent to maximizing the
    log-likelihood given the output class:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，找到最优参数等同于在给定输出类别的情况下最大化对数似然：
- en: '![](img/730cc66f-36d3-4675-93a3-4b72d0b4d8bb.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/730cc66f-36d3-4675-93a3-4b72d0b4d8bb.png)'
- en: 'Therefore, the optimization problem can be expressed, using the indicator notation,
    as the minimization of the loss function:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，优化问题可以用指示符号表示为损失函数的最小化：
- en: '![](img/5bb4b7d5-2946-434e-a49f-6ed10a250b80.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5bb4b7d5-2946-434e-a49f-6ed10a250b80.png)'
- en: 'If *y=0*, the first term becomes null and the second one becomes *log(1-x)*,
    which is the log-probability of the class 0\. On the other hand, if *y=1*, the
    second term is 0 and the first one represents the log-probability of *x*. In this
    way, both cases are embedded in a single expression. In terms of information theory,
    it means minimizing the cross-entropy between a target distribution and an approximated
    one:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*y=0*，第一个项变为零，第二个项变为*log(1-x)*，这是类别0的对数概率。另一方面，如果*y=1*，第二个项为零，第一个项代表*x*的对数概率。这样，两种情况都包含在一个单一的表达式中。从信息论的角度来看，这意味着最小化目标分布和近似分布之间的交叉熵：
- en: '![](img/884e7e28-7dc7-41c4-8679-a09be940169d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/884e7e28-7dc7-41c4-8679-a09be940169d.png)'
- en: In particular, if *log[2]* is adopted, the functional expresses the number of
    extra bits requested to encode the original distribution with the predicted one.
    It's obvious that when *J(w) = 0*, the two distributions are equal. Therefore,
    minimizing the cross-entropy is an elegant way to optimize the prediction error
    when the target distributions are categorical.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，如果采用*log[2]*，该泛函表示使用预测分布对原始分布进行编码所需的额外比特数。很明显，当*J(w) = 0*时，两个分布是相等的。因此，最小化交叉熵是优化预测误差的一种优雅方法，当目标分布是分类的。
- en: Implementation and optimizations
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现和优化
- en: 'scikit-learn implements the `LogisticRegression` class, which can solve this
    problem using optimized algorithms. Let''s consider a toy dataset made of 500
    samples:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn实现了`LogisticRegression`类，该类可以使用优化算法解决这个问题。让我们考虑一个由500个样本组成的玩具数据集：
- en: '![](img/19f5a3f5-26a1-4176-8967-bb277683369a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/19f5a3f5-26a1-4176-8967-bb277683369a.png)'
- en: 'The dots belong to the class 0, while the triangles belong to the class 1\.
    In order to immediately test the accuracy of our classification, it''s useful
    to split the dataset into training and test sets:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 点属于类别0，而三角形属于类别1。为了立即测试我们分类的准确性，将数据集分为训练集和测试集是有用的：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can train the model using the default parameters:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用默认参数来训练模型：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It''s also possible to check the quality through a cross-validation (like for
    linear regression):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过交叉验证（类似于线性回归）来检查质量：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The classification task has been successful without any further action (confirmed
    also by the cross-validation) and it''s also possible to check the resulting hyperplane
    parameters:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务在没有进一步操作的情况下成功（交叉验证也证实了这一点），还可以检查结果超平面参数：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the following figure, there's a representation of this hyperplane (a line),
    where it's possible to see how the classification works and what samples are misclassified.
    Considering the local density of the two blocks, it's easy to see that the misclassifications
    happened for outliers and for some borderline samples. The latter can be controlled
    by adjusting the hyperparameters, even if a trade-off is often necessary.  For
    example, if we want to include the four right dots on the separation line, this
    could exclude some elements in the right part. Later on, we're going to see how
    to find the optimal solution. However, when a linear classifier can easily find
    a separating hyperplane (even with a few outliers), we can say that the problem
    is linearly modelable; otherwise, more sophisticated non-linear techniques must
    be taken into account.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，展示了这个超平面（一条线），可以看到分类是如何工作的以及哪些样本被错误分类。考虑到两个块的区域局部密度，很容易看出错误分类发生在异常值和一些边界样本上。后者可以通过调整超参数来控制，尽管通常需要权衡。例如，如果我们想将分离线上的四个右点包括在内，这可能会排除右侧的一些元素。稍后，我们将看到如何找到最优解。然而，当一个线性分类器可以轻松地找到一个分离超平面（即使有一些异常值）时，我们可以说问题是可以线性建模的；否则，必须考虑更复杂的不线性技术。
- en: '![](img/739a41ee-7b3b-47fe-85a9-f56ccae642a0.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/739a41ee-7b3b-47fe-85a9-f56ccae642a0.png)'
- en: 'Just like for linear regression, it''s possible to impose norm conditions on
    the weights. In particular, the actual functional becomes:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性回归一样，可以对权重施加范数条件。特别是，实际的功能变为：
- en: '![](img/0a3158bb-aa68-46db-a804-22115f4b5b4e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0a3158bb-aa68-46db-a804-22115f4b5b4e.png)'
- en: The behavior is the same as explained in the previous chapter. Both produce
    a shrinkage, but *L1* forces sparsity. This can be controlled using the parameters penalty
    (whose values can be *L1* or *L2*) and *C*, which is the inverse regularization
    factor (1/alpha), so bigger values reduce the strength, while smaller ones (in
    particular less than 1) force the weights to move closer to the origin. Moreover,
    *L1* will prefer vertexes (where all but one components are null), so it's a good
    idea to apply `SelectFromModel` in order to optimize the actual features after
    shrinkage.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其行为与上一章中解释的相同。两者都产生收缩，但*L1*强制稀疏性。这可以通过参数penalty（其值可以是*L1*或*L2*）和*C*（正则化因子的倒数，1/alpha）来控制，较大的值会减少强度，而较小的值（特别是小于1的值）会迫使权重更靠近原点。此外，*L1*将优先考虑顶点（其中所有但一个分量都是零），因此在使用`SelectFromModel`来优化收缩后的实际特征是一个好主意。
- en: Stochastic gradient descent algorithms
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机梯度下降算法
- en: 'After discussing the basics of logistic regression, it''s useful to introduce
    the `SGDClassifier` class , which implements a very famous algorithm that can
    be applied to several different loss functions. The idea behind stochastic gradient
    descent is iterating a weight update based on the gradient of loss function:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了逻辑回归的基本原理之后，介绍`SGDClassifier`类是有用的，该类实现了一个非常著名的算法，可以应用于多个不同的损失函数。随机梯度下降背后的思想是基于损失函数梯度的权重更新迭代：
- en: '![](img/04ef7d5e-cb6d-4237-8ff5-47b22ec9da89.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/04ef7d5e-cb6d-4237-8ff5-47b22ec9da89.png)'
- en: However, instead of considering the whole dataset, the update procedure is applied
    on batches randomly extracted from it. In the preceding formula, *L* is the loss
    function we want to minimize (as discussed in [Chapter 2](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml),
    *Important Elements in Machine Learning*) and gamma (`eta0` in scikit-learn) is
    the learning rate, a parameter that can be constant or decayed while the learning
    process proceeds. The  `learning_rate` parameter can be also left with its default
    value (`optimal`), which is computed internally according to the regularization
    factor.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更新过程不是应用于整个数据集，而是应用于从中随机提取的批次。在先前的公式中，*L* 是我们想要最小化的损失函数（如[第2章](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml)中所述，*机器学习的重要元素*）和伽马（在scikit-learn中为`eta0`）是学习率，这是一个在学习过程中可以保持恒定或衰减的参数。`learning_rate`参数也可以保留其默认值（`optimal`），该值根据正则化因子内部计算。
- en: The process should end when the weights stop modifying or their variation keeps
    itself under a selected threshold. The scikit-learn implementation uses the `n_iter`
    parameter to define the number of desired iterations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当权重停止修改或其变化保持在所选阈值以下时，过程应该结束。scikit-learn实现使用`n_iter`参数来定义所需的迭代次数。
- en: 'There are many possible loss functions, but in this chapter, we consider only
    `log` and `perceptron`. Some of the other ones will be discussed in the next chapters.
    The former implements a logistic regression, while the latter (which is also available
    as the autonomous class `Perceptron`) is the simplest neural network, composed
    of a single layer of weights *w*, a fixed constant called bias, and a binary output
    function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多可能的损失函数，但在这章中，我们只考虑`log`和`感知器`。其他的一些将在下一章中讨论。前者实现逻辑回归，而后者（也作为自主类`感知器`可用）是最简单的神经网络，由一个权重层*w*、一个称为偏置的固定常数和一个二元输出函数组成：
- en: '![](img/e7494786-d0a8-4f97-be29-c494e584a643.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e7494786-d0a8-4f97-be29-c494e584a643.png)'
- en: 'The output function (which classifies in two classes) is:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输出函数（将数据分为两类）是：
- en: '![](img/e96697b4-9004-4ba9-b7b3-eb2160b6758c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e96697b4-9004-4ba9-b7b3-eb2160b6758c.png)'
- en: 'The differences between a `Perceptron` and a `LogisticRegression` are the output
    function (sign versus sigmoid) and the training model (with the loss function).
    A perceptron, in fact, is normally trained by minimizing the mean square distance
    between the actual value and prediction:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`感知器`和`逻辑回归`之间的区别在于输出函数（符号函数与Sigmoid函数）和训练模型（带有损失函数）。实际上，感知器通常通过最小化实际值与预测值之间的均方距离来训练：'
- en: '![](img/f26cedfe-5233-4554-a7fa-4aad05d3f1ed.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f26cedfe-5233-4554-a7fa-4aad05d3f1ed.png)'
- en: 'Just like any other linear classifier, a perceptron is not able to solve nonlinear
    problems; hence, our example will be generated using the built-in function `make_classification`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如任何其他线性分类器一样，感知器不能解决非线性问题；因此，我们的例子将使用内置函数`make_classification`生成：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this way, we can generate 500 samples split into two classes:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以生成500个样本，分为两类：
- en: '![](img/87b1ca33-88b8-421b-92c2-0f6588b7566c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/87b1ca33-88b8-421b-92c2-0f6588b7566c.png)'
- en: 'This problem, under a determined precision threshold, can be linearly solved,
    so our expectations are equivalent for both `Perceptron` and `LogisticRegression.`
    In the latter case, the training strategy is focused on maximizing the likelihood
    of a probability distribution. Considering the dataset, the probability of a red
    sample to belong to class 0 must be greater than 0.5 (it''s equal to 0.5 when
    *z = 0*, so when the point lays on the separating hyperplane) and vice versa.
    On the other hand, a perceptron will adjust the hyperplane so that the dot product
    between a sample and the weights would be positive or negative, according to the
    class. In the following figure, there''s a geometrical representation of a perceptron
    (where the bias is 0):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个确定的精度阈值下，这个问题可以线性解决，因此我们对`感知器`和`逻辑回归`的期望是等效的。在后一种情况下，训练策略集中在最大化概率分布的似然。考虑到数据集，红色样本属于类别0的概率必须大于0.5（当*z
    = 0*时等于0.5，因此当点位于分离超平面时），反之亦然。另一方面，感知器将调整超平面，使得样本与权重之间的点积根据类别为正或负。在以下图中，有一个感知器的几何表示（其中偏置为0）：
- en: '![](img/2d6b379f-a8a7-4bb2-acf5-22ce86e63b47.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2d6b379f-a8a7-4bb2-acf5-22ce86e63b47.png)'
- en: 'The weight vector is orthogonal to the separating hyperplane, so that the discrimination
    can happen only considering the sign of the dot product. An example of stochastic
    gradient descent with perceptron loss (without *L1*/*L2* constraints) is shown
    as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 权重向量与分离超平面正交，因此只有考虑点积的符号才能进行判别。以下是一个具有感知器损失（无*L1*/*L2*约束）的随机梯度下降的例子：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The same result can be obtained by directly using the `Perceptron` class:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接使用`Perceptron`类得到相同的结果：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finding the optimal hyperparameters through grid search
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过网格搜索寻找最佳超参数
- en: Finding the best hyperparameters (called this because they influence the parameters
    learned during the training phase) is not always easy and there are seldom good
    methods to start from. The personal experience (a fundamental element) must be
    aided by an efficient tool such as `GridSearchCV`, which automates the training
    process of different models and provides the user with optimal values using cross-validation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最佳超参数（之所以称为最佳超参数，是因为它们影响训练阶段学习的参数）并不总是容易的，而且很少有好的方法可以从中开始。个人经验（一个基本元素）必须由一个高效的工具如`GridSearchCV`来辅助，该工具自动化不同模型的训练过程，并使用交叉验证为用户提供最佳值。
- en: 'As an example, we show how to use it to find the best penalty and strength
    factors for a linear regression with the Iris toy dataset:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们展示如何使用它来找到使用Iris玩具数据集进行线性回归的最佳惩罚和强度因子：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It's possible to insert into the `param` dictionary any parameter supported
    by the model with a list of values. `GridSearchCV` will process in parallel and
    return the best estimator (through the instance variable `best_estimator_`, which
    is an instance of the same classifier specified through the parameter `estimator`).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将任何由模型支持的参数及其值列表插入到`param`字典中。`GridSearchCV`将并行处理并返回最佳估计器（通过实例变量`best_estimator_`，它是一个与通过参数`estimator`指定的相同分类器的实例）。
- en: When working with parallel algorithms, scikit-learn provides the `n_jobs` parameter,
    which allows us to specify how many threads must be used. Setting `n_jobs=multiprocessing.cpu_count()`
    is useful to exploit all CPU cores available on the current machine.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用并行算法时，scikit-learn提供了`n_jobs`参数，允许我们指定必须使用多少线程。将`n_jobs=multiprocessing.cpu_count()`设置为有效，可以充分利用当前机器上可用的所有CPU核心。
- en: 'In the next example, we''re going to find the best parameters of an `SGDClassifier`
    trained with perceptron loss. The dataset is plotted in the following figure:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将找到使用感知损失训练的`SGDClassifier`的最佳参数。数据集在以下图中展示：
- en: '![](img/b087a9da-4a5f-45c5-8f08-b261f8ca2522.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b087a9da-4a5f-45c5-8f08-b261f8ca2522.png)'
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Classification metrics
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类指标
- en: 'A classification task can be evaluated in many different ways to achieve specific
    objectives. Of course, the most important metric is the accuracy, often expressed as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类任务可以通过多种不同的方式来评估，以达到特定的目标。当然，最重要的指标是准确率，通常表示为：
- en: '![](img/d5f7e0c0-6444-481e-9354-56b0a8870ebb.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d5f7e0c0-6444-481e-9354-56b0a8870ebb.png)'
- en: 'In scikit-learn, it can be assessed using the built-in `accuracy_score()` function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，可以使用内置的`accuracy_score()`函数进行评估：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Another very common approach is based on zero-one loss function, which we saw
    in [Chapter 2](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml), *Important Elements
    in Machine Learning*, which is defined as the normalized average of *L[0/1]* (where
    *1* is assigned to misclassifications) over all samples. In the following example,
    we show a normalized score (if it''s close to 0, it''s better) and then the same
    unnormalized value (which is the actual number of misclassifications):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常常见的方法是基于零一损失函数，我们在[第2章](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml)，*机器学习的重要元素*中看到了它，定义为所有样本上*L[0/1]*（其中*1*分配给误分类）的归一化平均值。在以下示例中，我们展示了一个归一化分数（如果它接近0，则更好）以及相同的未归一化值（这是实际的误分类数量）：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'A similar but opposite metric is the **Jaccard similarity coefficient**, defined
    as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类似但相反的指标是**Jaccard相似系数**，定义为：
- en: '![](img/a3ca948b-c1fd-4bb1-80bb-74c2f00d8227.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a3ca948b-c1fd-4bb1-80bb-74c2f00d8227.png)'
- en: 'This index measures the similarity and is bounded between 0 (worst performances)
    and 1 (best performances). In the former case, the intersection is null, while
    in the latter, the intersection and union are equal because there are no misclassifications.
    In scikit-learn, the implementation is:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标衡量相似性，其值介于0（最差性能）和1（最佳性能）之间。在前者情况下，交集为零，而在后者情况下，交集和并集相等，因为没有误分类。在scikit-learn中的实现是：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'These measures provide a good insight into our classification algorithms. However,
    in many cases, it''s necessary to be able to differentiate between different kinds
    of misclassifications (we''re considering the binary case with the conventional
    notation: 0-negative, 1-positive), because the relative weight is quite different.
    For this reason, we introduce the following definitions:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标为我们提供了对分类算法的良好洞察。然而，在许多情况下，有必要能够区分不同类型的误分类（我们考虑的是具有传统记号的二元情况：0-负，1-正），因为相对权重相当不同。因此，我们引入以下定义：
- en: '**True positive**: A positive sample correctly classified'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性**：一个被正确分类的阳性样本'
- en: '**False positive**: A negative sample classified as positive'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**：一个被分类为正例的阴性样本'
- en: '**True negative**: A negative sample correctly classified'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性**：一个被正确分类的阴性样本'
- en: '**False negative**: A positive sample classified as negative'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**：一个被分类为负例的阳性样本'
- en: 'At a glance, false positive and false negative can be considered as similar
    errors, but think about a medical prediction: while a false positive can be easily
    discovered with further tests, a false negative is often neglected, with repercussions
    following the consequences of this action. For this reason, it''s useful to introduce
    the concept of a confusion matrix:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，假阳性和假阴性可以被认为是类似的错误，但考虑一下医学预测：虽然假阳性可以通过进一步的测试轻易发现，但假阴性往往被忽视，并随之产生后果。因此，引入混淆矩阵的概念是有用的：
- en: '![](img/43c6e2e9-0130-465f-a6fe-9f6061f95593.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/43c6e2e9-0130-465f-a6fe-9f6061f95593.png)'
- en: 'In scikit-learn, it''s possible to build a confusion matrix using a built-in
    function. Let''s consider a generic logistic regression on a dataset *X* with
    labels *Y*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，可以使用内置函数构建混淆矩阵。让我们考虑一个在数据集*X*上具有标签*Y*的通用逻辑回归：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we can compute our confusion matrix and immediately see how the classifier
    is working:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算我们的混淆矩阵，并立即看到分类器是如何工作的：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The last operation is needed because scikit-learn adopts an inverse axle. However,
    in many books, the confusion matrix has true values on the main diagonal, so I
    preferred to invert the axle.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的操作是必要的，因为scikit-learn采用了一个逆轴。然而，在许多书中，混淆矩阵的真实值位于主对角线上，所以我更喜欢反转轴。
- en: In order to avoid mistakes, I suggest you visit the page at [http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html,](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)
    and check for true/false positive/negative position.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免错误，我建议您访问[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)页面，并检查真实/假正/假负的位置。
- en: So we have five false negatives and two false positives. If needed, a further
    analysis can allow for the detection of the misclassifications to decide how to
    treat them (for example, if their variance overcomes a predefined threshold, it's
    possible to consider them as outliers and remove them).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有五个假阴性样本和两个假阳性样本。如果需要，进一步的分析可以检测到误分类，以决定如何处理它们（例如，如果它们的方差超过预定义的阈值，可以考虑它们为异常值并移除它们）。
- en: 'Another useful direct measure is:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的直接指标是：
- en: '![](img/4a213466-5665-4ee9-82b9-9045d6e38f8b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4a213466-5665-4ee9-82b9-9045d6e38f8b.png)'
- en: 'This is directly connected with the ability to capture the features that determine
    the positiveness of a sample, to avoid the misclassification as negative. In scikit-learn,
    the implementation is:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这直接关联到捕捉决定样本正性的特征的能力，以避免被错误地分类为负类。在scikit-learn中，实现方式如下：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If you don't flip the confusion matrix, but want to get the same measures, it's
    necessary to add the `pos_label=0` parameter to all metric score functions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想翻转混淆矩阵，但想要得到相同的指标，必须在所有指标评分函数中添加`pos_label=0`参数。
- en: 'The ability to detect true positive samples among all the potential positives
    can be assessed using another measure:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有潜在的阳性样本中检测真实阳性样本的能力可以使用另一个指标来评估：
- en: '![](img/e49c8eee-edf6-4a47-ba07-0bc279d0e30e.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e49c8eee-edf6-4a47-ba07-0bc279d0e30e.png)'
- en: 'The scikit-learn implementation is:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的实现方式如下：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It''s not surprising that we have a 90 percent recall with 96 percent precision,
    because the number of false negatives (which impact recall) is proportionally
    higher than the number of false positives (which impact precision). A weighted
    harmonic mean between precision and recall is provided by:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有90%的召回率和96%的精确度并不令人惊讶，因为假阴性（影响召回率）的数量与假阳性（影响精确度）的数量成比例较高。精确度和召回率之间的加权调和平均值由以下公式提供：
- en: '![](img/19659e54-38b5-474e-bad7-e3691a70c37e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/19659e54-38b5-474e-bad7-e3691a70c37e.png)'
- en: 'A beta value equal to 1 determines the so-called *F[1]* score, which is a perfect
    balance between the two measures. A beta less than 1 gives more importance to
    *precision* and a value greater than 1 gives more importance to *recall*. The
    following snippet shows how to implement it with scikit-learn:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当beta值等于1时，确定所谓的*F[1]*分数，这是两个指标之间的完美平衡。当beta小于1时，更重视*精确度*；当beta大于1时，更重视*召回率*。以下代码片段展示了如何使用scikit-learn实现它：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: For *F[1]* score, scikit-learn provides the function `f1_score()`, which is
    equivalent to `fbeta_score()` with `beta=1`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*F[1]*分数，scikit-learn提供了`f1_score()`函数，它与`fbeta_score()`函数的`beta=1`等价。
- en: The highest score is achieved by giving more importance to precision (which
    is higher), while the least one corresponds to a recall predominance. *F[Beta]*
    is hence useful to have a compact picture of the accuracy as a trade-off between
    high precision and a limited number of false negatives.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最高分是通过更重视精确度（精确度更高）来实现的，而最低分则对应着召回率占优。因此，*Beta* 系数有助于获得一个关于准确性的紧凑图景，该图景是高精确度和有限数量的假阴性之间的权衡。
- en: ROC curve
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC 曲线
- en: 'The **ROC curve** (or receiver operating characteristics) is a valuable tool
    to compare different classifiers that can assign a score to their predictions.
    In general, this score can be interpreted as a probability, so it''s bounded between
    0 and 1\. The plane is structured like in the following figure:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROC 曲线**（或接收者操作特征）是用于比较不同分类器（可以为它们的预测分配分数）的有价值工具。通常，这个分数可以解释为概率，因此它在 0 和
    1 之间。平面结构如下所示：'
- en: '![](img/9b0ea115-e411-434f-8ba3-5b13729c6527.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9b0ea115-e411-434f-8ba3-5b13729c6527.png)'
- en: 'The *x *axis represents the increasing false positive rate (also known as **specificity**),
    while the *y *axis represents the true positive rate (also known as **sensitivity**).
    The dashed oblique line represents a perfectly random classifier, so all the curves
    below this threshold perform worse than a random choice, while the ones above
    it show better performances. Of course, the best classifier has an ROC curve split
    into the segments [0, 0] - [0, 1] and [0, 1] - [1, 1], and our goal is to find
    algorithms whose performances should be as close as possible to this limit. To
    show how to create a ROC curve with scikit-learn, we''re going to train a model
    to determine the scores for the predictions (this can be achieved using the `decision_function()` or
    `predict_proba()` methods):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*x 轴*代表不断增加的假阳性率（也称为**特异性**），而 *y 轴*代表真正阳性率（也称为**灵敏度**）。虚线斜线代表一个完全随机的分类器，因此所有低于此阈值的曲线的性能都比随机选择差，而高于此阈值的曲线则表现出更好的性能。当然，最佳分类器的
    ROC 曲线将分为 [0, 0] - [0, 1] 和 [0, 1] - [1, 1] 这两个部分，我们的目标是找到性能尽可能接近这个极限的算法。为了展示如何使用
    scikit-learn 创建 ROC 曲线，我们将训练一个模型来确定预测的分数（这可以通过 `decision_function()` 或 `predict_proba()`
    方法实现）：'
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we can compute the ROC curve:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算 ROC 曲线：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is made up of the increasing true and false positive rates and the
    decreasing thresholds (which isn''t normally used for plotting the curve). Before
    proceeding, it''s also useful to compute the **area under the curve** (**AUC)**,
    whose value is bounded between 0 (worst performances) and 1 (best performances),
    with a perfectly random value corresponding to 0.5:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输出由不断增加的真正阳性和假阳性率以及不断降低的阈值（通常不用于绘制曲线）组成。在继续之前，计算**曲线下面积**（**AUC**）也是很有用的，其值介于
    0（最差性能）和 1（最佳性能）之间，完全随机值对应于 0.5：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We already know that our performances are rather good because the AUC is close
    to 1\. Now we can plot the ROC curve using matplotlib. As this book is not dedicated
    to this powerful framework, I''m going to use a snippet that can be found in several
    examples:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道我们的性能相当好，因为 AUC 接近 1。现在我们可以使用 matplotlib 绘制 ROC 曲线。由于这本书不是专门介绍这个强大框架的，我将使用可以在多个示例中找到的代码片段：
- en: '[PRE20]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The resulting ROC curve is the following plot:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 ROC 曲线如下所示：
- en: '![](img/ecd852a4-f726-422e-b401-97340022d9d9.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ecd852a4-f726-422e-b401-97340022d9d9.png)'
- en: As confirmed by the AUC, our ROC curve shows very good performance. In later
    chapters, we're going to use the ROC curve to visually compare different algorithms.
    As an exercise, you can try different parameters of the same model and plot all
    the ROC curves, to immediately understand which setting is preferable.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如 AUC 所证实，我们的 ROC 曲线显示出非常好的性能。在后面的章节中，我们将使用 ROC 曲线来直观地比较不同的算法。作为一个练习，你可以尝试同一模型的不同的参数，并绘制所有
    ROC 曲线，以立即了解哪种设置更可取。
- en: I suggest visiting [http://matplotlib.org](http://matplotlib.org), for further
    information and tutorials. Moreover, an extraordinary tool is Jupyter ([http://jupyter.org](http://jupyter.org)),
    which allows working with interactive notebooks, where you can immediately try
    your code and visualize in-line plots.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议访问 [http://matplotlib.org](http://matplotlib.org)，以获取更多信息和学习教程。此外，一个非凡的工具是
    Jupyter ([http://jupyter.org](http://jupyter.org))，它允许使用交互式笔记本，在那里你可以立即尝试你的代码并可视化内联图表。
- en: Summary
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: A linear model classifies samples using separating hyperplanes; hence, a problem
    is linearly separable if it's possible to find a linear model whose accuracy overcomes
    a predetermined threshold. Logistic regression is one of most famous linear classifiers,
    based on the principle of maximizing the probability of a sample belonging to
    the right class. Stochastic gradient descent classifiers are a more generic family
    of algorithms, determined by the different loss function that is adopted. SGD
    allows partial fitting, particularly when the amount of data is too huge to be
    loaded in memory. A perceptron is a particular instance of SGD, representing a
    linear neural network that cannot solve the `XOR` problem (for this reason, multi-layer
    perceptrons became the first choice for non-linear classification). However, in
    general, its performance is comparable to a logistic regression model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型使用分离超平面来对样本进行分类；因此，如果可以找到一个线性模型，其准确率超过一个预定的阈值，则问题线性可分。逻辑回归是最著名的线性分类器之一，其原理是最大化样本属于正确类的概率。随机梯度下降分类器是一组更通用的算法，由采用的损失函数的不同而决定。SGD允许部分拟合，尤其是在数据量太大而无法加载到内存中的情况下。感知器是SGD的一个特定实例，代表一个不能解决`XOR`问题的线性神经网络（因此，多层感知器成为了非线性分类的首选）。然而，在一般情况下，其性能与逻辑回归模型相当。
- en: All classifier performances must be measured using different approaches, in
    order to be able to optimize their parameters or to change them when the results
    don't meet our requirements. We discussed different metrics and, in particular,
    the ROC curve, which graphically shows how the different classifiers are performing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 所有分类器的性能都必须使用不同的方法进行衡量，以便能够优化它们的参数或在我们对结果不满意时更改它们。我们讨论了不同的指标，特别是ROC曲线，它以图形方式显示了不同分类器的性能。
- en: In the next chapter, we're going to discuss naive Bayes classifiers, which are
    another very famous and powerful family of algorithms. Thanks to this simple approach,
    it's possible to build spam filtering systems and solve apparently complex problems
    using only probabilities and the quality of results. Even after decades, it's
    still superior or comparable to much more complex solutions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论朴素贝叶斯分类器，这是另一组非常著名且强大的算法家族。得益于这种简单的方法，我们可以仅使用概率和结果质量来构建垃圾邮件过滤系统，并解决看似复杂的问题。即使经过几十年，它仍然优于或与许多更复杂的解决方案相当。
