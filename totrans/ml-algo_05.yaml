- en: Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter begins by analyzing linear classification problems, with particular
    focus on logistic regression (despite its name, it''s a classification algorithm)
    and stochastic gradient descent approaches. Even if these strategies appear too
    simple, they''re still the main choices in many classification tasks. Speaking
    of which, it''s useful to remember a very important philosophical principle: **Occam''s
    razor**. In our context, it states that the first choice must always be the simplest
    and only if it doesn''t fit, it''s necessary to move on to more complex models.
    In the second part of the chapter, we''re going to discuss some common metrics
    useful to evaluate a classification task. They are not limited to linear models,
    so we use them when talking about different strategies as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a generic linear classification problem with two classes. In
    the following figure, there''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e3a5f30-f40e-4a6a-b4f8-dac180fb9041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our goal is to find an optimal hyperplane, which separates the two classes.
    In multi-class problems, the strategy one-vs-all is normally adopted, so the discussion
    can be focused only on binary classifications. Suppose we have the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c16cc001-8b7e-43d2-95c8-346bf6027dfc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This dataset is associated with the following target set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/389065d9-fc7c-4077-ae7c-1dd305531b84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now define a weight vector made of *m* continuous components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c623901b-3744-48c9-9c46-5cff7b1365bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also define the quantity *z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19cfa97f-4466-4d4c-a13d-48476b14bed3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If *x* is a variable, *z* is the value determined by the hyperplane equation.
    Therefore, if the set of coefficients *w* that has been determined is correct,
    it happens that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a1dc317-1b6b-47d3-9317-eefa134e7787.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we must find a way to optimize *w*, in order to reduce the classification
    error. If such a combination exists (with a certain error threshold), we say that
    our problem is **linearly separable**. On the other hand, when it''s impossible
    to find a linear classifier, the problem is called **non-linearly separable**.
    A very simple but famous example is given by the logical operator `XOR`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f827264-813e-45ec-99a0-1e4de9acffcd.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, any line will always include a wrong sample. Hence, in order
    to solve this problem, it is necessary to involve non-linear techniques. However,
    in many real-life cases, we use linear techniques (which are often simpler and
    faster) for non-linear problems too, accepting a tolerable misclassification error.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even if called regression, this is a classification method which is based on
    the probability for a sample to belong to a class. As our probabilities must be
    continuous in *R* and bounded between (0, 1), it''s necessary to introduce a threshold
    function to filter the term *z*. The name logistic comes from the decision to
    use the sigmoid (or logistic) function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1d6d740-629e-4fac-a5a1-aae77e2a4178.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A partial plot of this function is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8de1bdb9-9e69-498f-af71-cf00358d5b99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the function intersects *x=0* in the ordinate 0.5, and *y<0.5*
    for *x<0* and *y>0.5* for *x>0*. Moreover, its domain is *R* and it has two asymptotes
    at 0 and 1\. So, we can define the probability for a sample to belong to a class
    (from now on, we''ll call them 0 and 1) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2746315b-8631-46d3-bd76-3c5c679ada8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, finding the optimal parameters is equivalent to maximizing the
    log-likelihood given the output class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/730cc66f-36d3-4675-93a3-4b72d0b4d8bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the optimization problem can be expressed, using the indicator notation,
    as the minimization of the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bb4b7d5-2946-434e-a49f-6ed10a250b80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If *y=0*, the first term becomes null and the second one becomes *log(1-x)*,
    which is the log-probability of the class 0\. On the other hand, if *y=1*, the
    second term is 0 and the first one represents the log-probability of *x*. In this
    way, both cases are embedded in a single expression. In terms of information theory,
    it means minimizing the cross-entropy between a target distribution and an approximated
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/884e7e28-7dc7-41c4-8679-a09be940169d.png)'
  prefs: []
  type: TYPE_IMG
- en: In particular, if *log[2]* is adopted, the functional expresses the number of
    extra bits requested to encode the original distribution with the predicted one.
    It's obvious that when *J(w) = 0*, the two distributions are equal. Therefore,
    minimizing the cross-entropy is an elegant way to optimize the prediction error
    when the target distributions are categorical.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation and optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'scikit-learn implements the `LogisticRegression` class, which can solve this
    problem using optimized algorithms. Let''s consider a toy dataset made of 500
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19f5a3f5-26a1-4176-8967-bb277683369a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The dots belong to the class 0, while the triangles belong to the class 1\.
    In order to immediately test the accuracy of our classification, it''s useful
    to split the dataset into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can train the model using the default parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s also possible to check the quality through a cross-validation (like for
    linear regression):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The classification task has been successful without any further action (confirmed
    also by the cross-validation) and it''s also possible to check the resulting hyperplane
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the following figure, there's a representation of this hyperplane (a line),
    where it's possible to see how the classification works and what samples are misclassified.
    Considering the local density of the two blocks, it's easy to see that the misclassifications
    happened for outliers and for some borderline samples. The latter can be controlled
    by adjusting the hyperparameters, even if a trade-off is often necessary.  For
    example, if we want to include the four right dots on the separation line, this
    could exclude some elements in the right part. Later on, we're going to see how
    to find the optimal solution. However, when a linear classifier can easily find
    a separating hyperplane (even with a few outliers), we can say that the problem
    is linearly modelable; otherwise, more sophisticated non-linear techniques must
    be taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/739a41ee-7b3b-47fe-85a9-f56ccae642a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just like for linear regression, it''s possible to impose norm conditions on
    the weights. In particular, the actual functional becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a3158bb-aa68-46db-a804-22115f4b5b4e.png)'
  prefs: []
  type: TYPE_IMG
- en: The behavior is the same as explained in the previous chapter. Both produce
    a shrinkage, but *L1* forces sparsity. This can be controlled using the parameters penalty
    (whose values can be *L1* or *L2*) and *C*, which is the inverse regularization
    factor (1/alpha), so bigger values reduce the strength, while smaller ones (in
    particular less than 1) force the weights to move closer to the origin. Moreover,
    *L1* will prefer vertexes (where all but one components are null), so it's a good
    idea to apply `SelectFromModel` in order to optimize the actual features after
    shrinkage.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After discussing the basics of logistic regression, it''s useful to introduce
    the `SGDClassifier` class , which implements a very famous algorithm that can
    be applied to several different loss functions. The idea behind stochastic gradient
    descent is iterating a weight update based on the gradient of loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04ef7d5e-cb6d-4237-8ff5-47b22ec9da89.png)'
  prefs: []
  type: TYPE_IMG
- en: However, instead of considering the whole dataset, the update procedure is applied
    on batches randomly extracted from it. In the preceding formula, *L* is the loss
    function we want to minimize (as discussed in [Chapter 2](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml),
    *Important Elements in Machine Learning*) and gamma (`eta0` in scikit-learn) is
    the learning rate, a parameter that can be constant or decayed while the learning
    process proceeds. The  `learning_rate` parameter can be also left with its default
    value (`optimal`), which is computed internally according to the regularization
    factor.
  prefs: []
  type: TYPE_NORMAL
- en: The process should end when the weights stop modifying or their variation keeps
    itself under a selected threshold. The scikit-learn implementation uses the `n_iter`
    parameter to define the number of desired iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many possible loss functions, but in this chapter, we consider only
    `log` and `perceptron`. Some of the other ones will be discussed in the next chapters.
    The former implements a logistic regression, while the latter (which is also available
    as the autonomous class `Perceptron`) is the simplest neural network, composed
    of a single layer of weights *w*, a fixed constant called bias, and a binary output
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7494786-d0a8-4f97-be29-c494e584a643.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output function (which classifies in two classes) is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e96697b4-9004-4ba9-b7b3-eb2160b6758c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The differences between a `Perceptron` and a `LogisticRegression` are the output
    function (sign versus sigmoid) and the training model (with the loss function).
    A perceptron, in fact, is normally trained by minimizing the mean square distance
    between the actual value and prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f26cedfe-5233-4554-a7fa-4aad05d3f1ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just like any other linear classifier, a perceptron is not able to solve nonlinear
    problems; hence, our example will be generated using the built-in function `make_classification`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this way, we can generate 500 samples split into two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87b1ca33-88b8-421b-92c2-0f6588b7566c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This problem, under a determined precision threshold, can be linearly solved,
    so our expectations are equivalent for both `Perceptron` and `LogisticRegression.`
    In the latter case, the training strategy is focused on maximizing the likelihood
    of a probability distribution. Considering the dataset, the probability of a red
    sample to belong to class 0 must be greater than 0.5 (it''s equal to 0.5 when
    *z = 0*, so when the point lays on the separating hyperplane) and vice versa.
    On the other hand, a perceptron will adjust the hyperplane so that the dot product
    between a sample and the weights would be positive or negative, according to the
    class. In the following figure, there''s a geometrical representation of a perceptron
    (where the bias is 0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d6b379f-a8a7-4bb2-acf5-22ce86e63b47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The weight vector is orthogonal to the separating hyperplane, so that the discrimination
    can happen only considering the sign of the dot product. An example of stochastic
    gradient descent with perceptron loss (without *L1*/*L2* constraints) is shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The same result can be obtained by directly using the `Perceptron` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Finding the optimal hyperparameters through grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding the best hyperparameters (called this because they influence the parameters
    learned during the training phase) is not always easy and there are seldom good
    methods to start from. The personal experience (a fundamental element) must be
    aided by an efficient tool such as `GridSearchCV`, which automates the training
    process of different models and provides the user with optimal values using cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we show how to use it to find the best penalty and strength
    factors for a linear regression with the Iris toy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It's possible to insert into the `param` dictionary any parameter supported
    by the model with a list of values. `GridSearchCV` will process in parallel and
    return the best estimator (through the instance variable `best_estimator_`, which
    is an instance of the same classifier specified through the parameter `estimator`).
  prefs: []
  type: TYPE_NORMAL
- en: When working with parallel algorithms, scikit-learn provides the `n_jobs` parameter,
    which allows us to specify how many threads must be used. Setting `n_jobs=multiprocessing.cpu_count()`
    is useful to exploit all CPU cores available on the current machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next example, we''re going to find the best parameters of an `SGDClassifier`
    trained with perceptron loss. The dataset is plotted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b087a9da-4a5f-45c5-8f08-b261f8ca2522.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Classification metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A classification task can be evaluated in many different ways to achieve specific
    objectives. Of course, the most important metric is the accuracy, often expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5f7e0c0-6444-481e-9354-56b0a8870ebb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In scikit-learn, it can be assessed using the built-in `accuracy_score()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Another very common approach is based on zero-one loss function, which we saw
    in [Chapter 2](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml), *Important Elements
    in Machine Learning*, which is defined as the normalized average of *L[0/1]* (where
    *1* is assigned to misclassifications) over all samples. In the following example,
    we show a normalized score (if it''s close to 0, it''s better) and then the same
    unnormalized value (which is the actual number of misclassifications):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A similar but opposite metric is the **Jaccard similarity coefficient**, defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3ca948b-c1fd-4bb1-80bb-74c2f00d8227.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This index measures the similarity and is bounded between 0 (worst performances)
    and 1 (best performances). In the former case, the intersection is null, while
    in the latter, the intersection and union are equal because there are no misclassifications.
    In scikit-learn, the implementation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'These measures provide a good insight into our classification algorithms. However,
    in many cases, it''s necessary to be able to differentiate between different kinds
    of misclassifications (we''re considering the binary case with the conventional
    notation: 0-negative, 1-positive), because the relative weight is quite different.
    For this reason, we introduce the following definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive**: A positive sample correctly classified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive**: A negative sample classified as positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negative**: A negative sample correctly classified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative**: A positive sample classified as negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At a glance, false positive and false negative can be considered as similar
    errors, but think about a medical prediction: while a false positive can be easily
    discovered with further tests, a false negative is often neglected, with repercussions
    following the consequences of this action. For this reason, it''s useful to introduce
    the concept of a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43c6e2e9-0130-465f-a6fe-9f6061f95593.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In scikit-learn, it''s possible to build a confusion matrix using a built-in
    function. Let''s consider a generic logistic regression on a dataset *X* with
    labels *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can compute our confusion matrix and immediately see how the classifier
    is working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The last operation is needed because scikit-learn adopts an inverse axle. However,
    in many books, the confusion matrix has true values on the main diagonal, so I
    preferred to invert the axle.
  prefs: []
  type: TYPE_NORMAL
- en: In order to avoid mistakes, I suggest you visit the page at [http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html,](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)
    and check for true/false positive/negative position.
  prefs: []
  type: TYPE_NORMAL
- en: So we have five false negatives and two false positives. If needed, a further
    analysis can allow for the detection of the misclassifications to decide how to
    treat them (for example, if their variance overcomes a predefined threshold, it's
    possible to consider them as outliers and remove them).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful direct measure is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a213466-5665-4ee9-82b9-9045d6e38f8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is directly connected with the ability to capture the features that determine
    the positiveness of a sample, to avoid the misclassification as negative. In scikit-learn,
    the implementation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If you don't flip the confusion matrix, but want to get the same measures, it's
    necessary to add the `pos_label=0` parameter to all metric score functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ability to detect true positive samples among all the potential positives
    can be assessed using another measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e49c8eee-edf6-4a47-ba07-0bc279d0e30e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The scikit-learn implementation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s not surprising that we have a 90 percent recall with 96 percent precision,
    because the number of false negatives (which impact recall) is proportionally
    higher than the number of false positives (which impact precision). A weighted
    harmonic mean between precision and recall is provided by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19659e54-38b5-474e-bad7-e3691a70c37e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A beta value equal to 1 determines the so-called *F[1]* score, which is a perfect
    balance between the two measures. A beta less than 1 gives more importance to
    *precision* and a value greater than 1 gives more importance to *recall*. The
    following snippet shows how to implement it with scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For *F[1]* score, scikit-learn provides the function `f1_score()`, which is
    equivalent to `fbeta_score()` with `beta=1`.
  prefs: []
  type: TYPE_NORMAL
- en: The highest score is achieved by giving more importance to precision (which
    is higher), while the least one corresponds to a recall predominance. *F[Beta]*
    is hence useful to have a compact picture of the accuracy as a trade-off between
    high precision and a limited number of false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: ROC curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **ROC curve** (or receiver operating characteristics) is a valuable tool
    to compare different classifiers that can assign a score to their predictions.
    In general, this score can be interpreted as a probability, so it''s bounded between
    0 and 1\. The plane is structured like in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0ea115-e411-434f-8ba3-5b13729c6527.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *x *axis represents the increasing false positive rate (also known as **specificity**),
    while the *y *axis represents the true positive rate (also known as **sensitivity**).
    The dashed oblique line represents a perfectly random classifier, so all the curves
    below this threshold perform worse than a random choice, while the ones above
    it show better performances. Of course, the best classifier has an ROC curve split
    into the segments [0, 0] - [0, 1] and [0, 1] - [1, 1], and our goal is to find
    algorithms whose performances should be as close as possible to this limit. To
    show how to create a ROC curve with scikit-learn, we''re going to train a model
    to determine the scores for the predictions (this can be achieved using the `decision_function()` or
    `predict_proba()` methods):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can compute the ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is made up of the increasing true and false positive rates and the
    decreasing thresholds (which isn''t normally used for plotting the curve). Before
    proceeding, it''s also useful to compute the **area under the curve** (**AUC)**,
    whose value is bounded between 0 (worst performances) and 1 (best performances),
    with a perfectly random value corresponding to 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We already know that our performances are rather good because the AUC is close
    to 1\. Now we can plot the ROC curve using matplotlib. As this book is not dedicated
    to this powerful framework, I''m going to use a snippet that can be found in several
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting ROC curve is the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecd852a4-f726-422e-b401-97340022d9d9.png)'
  prefs: []
  type: TYPE_IMG
- en: As confirmed by the AUC, our ROC curve shows very good performance. In later
    chapters, we're going to use the ROC curve to visually compare different algorithms.
    As an exercise, you can try different parameters of the same model and plot all
    the ROC curves, to immediately understand which setting is preferable.
  prefs: []
  type: TYPE_NORMAL
- en: I suggest visiting [http://matplotlib.org](http://matplotlib.org), for further
    information and tutorials. Moreover, an extraordinary tool is Jupyter ([http://jupyter.org](http://jupyter.org)),
    which allows working with interactive notebooks, where you can immediately try
    your code and visualize in-line plots.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A linear model classifies samples using separating hyperplanes; hence, a problem
    is linearly separable if it's possible to find a linear model whose accuracy overcomes
    a predetermined threshold. Logistic regression is one of most famous linear classifiers,
    based on the principle of maximizing the probability of a sample belonging to
    the right class. Stochastic gradient descent classifiers are a more generic family
    of algorithms, determined by the different loss function that is adopted. SGD
    allows partial fitting, particularly when the amount of data is too huge to be
    loaded in memory. A perceptron is a particular instance of SGD, representing a
    linear neural network that cannot solve the `XOR` problem (for this reason, multi-layer
    perceptrons became the first choice for non-linear classification). However, in
    general, its performance is comparable to a logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: All classifier performances must be measured using different approaches, in
    order to be able to optimize their parameters or to change them when the results
    don't meet our requirements. We discussed different metrics and, in particular,
    the ROC curve, which graphically shows how the different classifiers are performing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to discuss naive Bayes classifiers, which are
    another very famous and powerful family of algorithms. Thanks to this simple approach,
    it's possible to build spam filtering systems and solve apparently complex problems
    using only probabilities and the quality of results. Even after decades, it's
    still superior or comparable to much more complex solutions.
  prefs: []
  type: TYPE_NORMAL
